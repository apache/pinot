
@@106848669 @2248669/ <h> Two Ways To Remove Fisheye From GoPro Images <p> Earlier this week I did a post on the new lens profiles for GoPro cameras in Camera Raw 8.2 and Lightroom 5.2 . I mentioned in that post ( and have had several conversations with people since then ) that you can also reduce the fisheye effect using Photoshop CCs Adaptive Wide Angle filter . So I decided to put together a quick post showing the output of the Lens Correction and Adaptive Wide Angle filters side-by-side on the same image . <p> Check out the video see how to apply both techniques , and compare the output . <p> You can check out the final images here ( shrunk to 600+450 for the blog ) : 
@@106848671 @2248671/ <p> These two services enable you to quickly add Text-To-Speech or Speech-To-Text capability to any application . - What 's a better way to show them off than by updating my existing app to leverage the new speech services ? <p> I simply added the Text To Speech and Speech To Text services to my existing Healthcare QA application that runs on Bluemix : <p> IBM Bluemix Dashboard <p> These services are available via a REST API . Once youve added them- to your application , you can consume them easily within any of your applications . <p> I updated the code from my previous example- in 2 ways : 1 ) take advantage of the Watson Node.js Wrapper that makes interacting with Watson a lot easier and 2 ) to take advantage of these new services services . <h> Watson Node.js Wrapper <p> Using the Watson Node.js Wrapper , you can now easily instantiate Watson services in a single line of code . - For example : <p> The credentials come from your environment configuration , then you just create instances of whichever services that you want to consume @ @ @ @ @ @ @ @ @ @ service is now much simpler than the previous version . - When we want to submit a question to the Watson QA service , you can now simply call the " ask " method on the QA service instance . <p> Below is- my server-side code from app.js that accepts a POST submission from the browser , delegates- the question to Watson , and takes the result and renders HTML using a- Jade template. - See the Getting Started Guide for the Watson QA Service to learn more about the wrappers for Node or Java . <p> Compare this to the previous version , and you 'll quickly see that it is much simpler . <h> Speech Synthesis <p> At this point , we- already have a functional service that can take natural language text , submit it to Watson , - and return a search result as text . - The next logical step for me was to add speech synthesis using the Watson Text To Speech Service- ( TTS ) . - Again , the Watson Node Wrapper and Watsons REST services- make this task very simple . - @ @ @ @ @ @ @ @ @ @ src of an &lt;audio&gt; instance to the URL for the TTS service : <p> On the server you just need to synthesize the audio from the data in the URL query string . - Heres an example how to invoke the text to speech service directly from the Watson TTS sample app : <p> var textToSpeech = new **32;0;TOOLONG ; // handle get requests app.get ( ' /synthesize ' , function ( req , res ) // make the request to Watson to synthesize the audio file from the query text var transcript = **34;34;TOOLONG ; // set content-disposition header if downloading the // file instead of playing directly in the browser transcript.on ( 'response ' , function(response) **29;70;TOOLONG ; if ( req.query.download ) **36;101;TOOLONG ' = ' attachment ; filename=transcript.ogg ' ; ) ; // pipe results back to the browser as they come in from Watson transcript.pipe(res) ; ) ; <p> The Watson TTS service supports . ogg and . wav file formats . - I modified this sample is setup only with . ogg files . - On the client side , these are played using the @ @ @ @ @ @ @ @ @ @ were able to process natural language and generate speech , that last part of the solution is to recognize spoken input and turn it into text . - The Watson Speech To Text ( STT ) service handles this for us. - Just like the TTS service , the Speech To Text- service also has a sample app , complete with source code to help you get started . 123433 @qwx983433 <p> I still have trouble running the demo successfully in any browser ( I tried both Chrome and Firefox updated versions ) . <p> I do get the microphone notification icon but nothing gets transcribed once I speak . 123435 @qwx983435 <p> Hi Rabimba , can you elaborate ? What OS are you using , and do you get any error messages ? Can you check the JavaScript console for any error messages ? I just tested again , and it appears to be running for me OK in both Chrome and Firefox on OS X. Thanks <p> rabimba <p> Yeah . <p> I tried using it both in Ubuntu ( 12.10 ) and Windows 8.1 with both Chrome @ @ @ @ @ @ @ @ @ @ thanks . Ill have to test it in other OSes . I just tried again , and no problems here on OS X. No errors shown in that log either , just normal debug messages . Any chance your microphone is muted at the OS level ? <p> rabimba <p> That really should not be the case ! But I just tried it in my iMac and it worked like a charm : O ( in chrome ) 123435 @qwx983435 <p> Yeah , it should be identical across platforms . Not sure why its doing that , whether its a browser/platform issue , or issue in the code . <p> rabimba <p> I just checked it again in WIndows . Not working . 123435 @qwx983435 <p> New version posted . Should work in any browser supporting getUserMedia and . ogg for HTML5 Audio ( Chrome and Firefox ) . <p> rabimba <p> Is it the same url ? <p> I still ca n't get it to work in chrome/windows. 123435 @qwx983435 <p> I am running this without issue in Chrome on Windows . You sure your mic is working ? <p> @ @ @ @ @ @ @ @ @ @ to develop some tools using Watson . Do you know anyone interested in joining a startup ? <p> Peter <p> Thanks Andrew I really like this example , and was myself trying to combine these three fundamental Watson services into a similar demo . I have built apps leveraging the Q&amp;A , and then I played with the Speech services . You have combined all three very well into a really good app . But I have two issues . 1 . I demonstrated the Q&amp;A healthcare service to a Doctor and had the Doctor ask questions and the results were apparently less than usable . How can the results be improved and how can a user feedback to Watson to improve the accuracy ? Cognitive systems need some feedback so how is Watson getting this ? 2 . In my browser the mic does not record the speech very well . It is not capturing and converting what I say into the same question in text . It seems to be quite unreliable and inaccurate . Is this a fault of my browser , laptop mic or other ? @ @ @ @ @ @ @ @ @ @ But many thanks for pulling this demo together I really like it . Just a couple of things to build on . Thanks very much . <p> So , if a physician or patient trying to " test " the system asks " why does my back hurt ? " , Watson wont be able to answer that b/c it is not trained to answer that kind of situation . This free/demo service pulls from the open/online data sets , and is trained only for relatively simple questions . In order to have more meaningful interaction , the QA service needs to be trained on a more complex data set , and trained to understand different kinds of questions . This is all very possible , but would require engagement with IBM . Watson is being used in the medical field with much success , but those applications are with much larger data sets , with a more highly tuned/specialized algorithm . You can see more examples of this online , go to the " Healthcare " section at : LONG ... <p> 2 : I think its a laptop/mic @ @ @ @ @ @ @ @ @ @ to work pretty well , but the internal mic on my laptop does not work nearly as well . I 'm sure the team is working on ways to improve extraction and dealing with low signals background noise , but I cant say with any certainty b/c I am not involved with that team . <p> Brian L Donaldson <p> Hi Andrew , I would like to start working on a two app ideas using Watson with Real Estate data . Right now Watson Q&amp;A on Bluemix is only for Travel and Healthcare data . What steps can I take to get Real Estate data into Watson for my apps ? <p> Andrew I thought you might appreciate an ATTA-BOY of recognition for the superb work and your ( even better ) attitude of collaboration . Thanks for creating the videos and sharing the tools . I 'm a veteran web developer ( since 97 ) , but on my first efforts with Bluemix . Thanks for heading me on this journey with some enthusiasm and naive confidence ! 
@@106848672 @2248672/ <h> Lens Profile Correction for GoPro Cameras <p> Last week , new release-candidate versions of Camera Raw and Lightroom were posted to Adobe Labs that feature additional camera and lens support . I was extremely excited when I found out that one of the new camera profiles supported is the GoPro- Hero 3 . I 'm a huge fan of GoPro- cameras , and this means we now have more ways to get more creative with their usage . <p> I was recently thinking I absolutely love the shots I get off of the GoPro , but I wish there was an easy way to reduce the fisheye distortion . I wanted to try my hand at creating some aerial panoramas , but the distortion was causing issues . You can reduce the distortion using Photoshops Adaptive Wide Angle filter , but that can be tedious to get right . This release makes the process of reducing fisheye dead simple . <p> Reducing the lens distortion is now as simple as selecting the GoPro camera profile in Camera Raws lens correction settings , and you can use it to create @ @ @ @ @ @ @ @ @ @ see how to apply GoPro lens correction to both images and videos in Adobe Photoshop , Adobe Lightroom , and Adobe Bridge . <h> Images <p> Here are some side-by-side comparisons of before and after applying the GoPro lens correction . <h> Panoramas <p> I 've also been able to use this feature to create some awesome ( in my opinion ) aerial panoramas using the- DJI Phantom- quad-rotor remote controlled helicopter . The easiest way to create one of these panoramas is to select the images you want to merge inside of Adobe Bridge . Then right-click and select " Open in Camera Raw " , and then apply the lens correction to all of the images . Once you 've done that , keep the same images selected and go to Tools -&gt; Photoshop -&gt; Photomerge inside of Adobe Bridge . This will launch the Photomerge process inside Photoshop , and after a few minutes , you will have a nice panorama to work with . Take the generated panorama , turn it into a smart object , and then you can start applying other filters ( including Camera Raw @ @ @ @ @ @ @ @ @ @ results . <p> Here are a few panoramas Ive created . Click on any one of them to view an interactive panorama , where you can zoom into the full resolution of the image . <p> To make the panoramas interactive , I used Photoshops " Zoomify " export , combined with the Leaflet- mapping library for an interactive HTML experience . You can ignore the HTML it generates , but keep the images and XML configuration file . I then used this open source Zoomify Layer for Leaflet- to make the images fully interactive , without any plugin dependencies . You can pinch/zoom and pan the images , and they are loaded as individual tiles , so its a smooth experience for the end user . <h> Next Steps <p> Go get started , and have fun ! You can download Camera Raw 8.2 and Lightroom 5.2 release candidates from labs.adobe.com just make sure to get the correct Camera Raw plugin for your suite ( CS6 or CC ) . <p> Also , check out this video produced by Adobes own Russell Brown for additional information : 123433 @qwx983433 @ @ @ @ @ @ @ @ @ @ bought a GoPro . When it dawned on me that there might be profiles to correct the distortion , I was excited . After playing with it , I 've decided its not worth turning on Lens Profile Corrections as a default for my GoPro images . In many cases , for instance see the lower of the two original/corrected images in your post , fixing the horizon only creates scale errors in the horizontal , the building on the right looks wider than it is in real life . Do that to a person , and shell be very annoyed , and so probably will you be too . Its easier for the mind to live with the curved horizon than the unnatural look of people or other objects . For certain images , such as panoramas composed of many many images so the overlap is large and you 're only using the center of the field for unique content , then the profiles will certainly help . 123435 @qwx983435 <p> Yeah , I agree that I would n't turn it on by default . I think its great for dramatic effect @ @ @ @ @ @ @ @ @ @ mentioned ) , but there is some distortion , so use it wisely . You can achieve similar results without the edge distortion using the Adaptive Wide Angle filters , but I 've found it to be far more manual of a process . However , if you want to straighten things out without the distortion , its far worth the manual effort . <p> Ive been using GoPro cameras for a bit and have been trying to remove/reduce the fish eye effect in my videos . I 've seen your video ( and others ) on using the lens filter in Ps6 . As you can see in the screen shot below , I cant get the GoPro camera option to show up in the drop down menu . I 've installed all of the updates to no avail . I 'm sure there is something I 'm not doing correctly . Any help is greatly appreciated . <p> Thanks , Rich 123435 @qwx983435 <p> Did you download the update from Adobe Labs ( link above ) ? This is a beta/release candidate feature , and is not in a " normal " update @ @ @ @ @ @ @ @ @ @ Works great now . I guess I thought it was a Ps6 update and not a ACR update ( since I do n't  use ACR much ) . This is so much easier than " Adaptive Wide Angle " . Thanks for the assistance . You can go back to being a hero again ! <p> Excellent tutorial . Thank you . Do I understand correctly that you use Photoshop ( which version ? ) rather than ALR for correcting videos ? I use ALR , Adobe Photoshop Elements , and Adobe Premiere Elements but can not find the video functions there . <p> Brooke <p> Please excuse me if this is a silly question but I use my GoPro to study the physiology of marine mammals I have to calibrate my images to the camera and my computer so I know the size of the animals that I am shooting , Now if I used this method , would it change the image so much as to not be able to calibrate and record the size of the animals ? 123435 @qwx983435 <p> I honestly do n't  know the @ @ @ @ @ @ @ @ @ @ area of the image . However , since everything is being stretched uniformly , it might be OK . Do you have some kind of visual reference point inside of the image for comparison ? I think the best thing is to just try it and see how the measurements differ ( if at all ) . <p> Brooke <p> That is true I will have to compare a few images with each other . Thanks for your help ! <p> JH <p> A curvilinear lens as the one used on GoPro is the worst possible choice for measuring objects filmed . Even after the artificial compensation for the fish-eye effect the picture is basically useless unless you work with large margins of error . The " correction " introduces another error known as forced perspective . Basically you force something which the lens was looking at from the side ( because of the fish-eye lens in the GoPro ) into the perspective og the picture elements the lens was looking straight at . You want to look up the difference between curvilinear and rectilinear projection even the wiki is @ @ @ @ @ @ @ @ @ @ am looking to do some GoPro work covering golf courses . With little-to-no knowledge on GoPro devices , what is the best camera to go with the combination of affordability , quality and simplicity for a relative beginner who will be looking to learn more about video work ? Also , something that will effectively capture a golf course . <p> Thank you for any help ! <p> Justin <p> I 'd like to know if there 's a lens profile for the GoPro Hero 2 ? thanks <p> Scott Wilton <p> I did n't  find one but had pretty good luck using the Hero 3 White profile with 120 on the distorsion scale . Obviously you have to manually select this profile since it does n't  fit with the cameras metadata . <p> Jordan <p> I cant seem to figure out how to download the update for camera raw , so that i am able to select the GoPro option . Please help me ! 
@@106848674 @2248674/ <p> Its not every day that you get the opportunity to have your work showcased front and center on the main landing page for one of the largest companies in the world . Well , today is definitely my lucky day . I was interviewed last month about a drone-related project that I 've been working on that focuses on insurance use cases and safety/productivity improvement by using cognitive/artifical intelligence via IBM Watson . I knew it was going to be used for some marketing materials , but the last thing that I expected was to have my image right there on ibm.com . I see this as a tremendous honor , and am humbled by the opportunity and exposure . <p> here 's an interview that I recently did with IBM DeveloperWorks TV at the recent World of Watson conference . In it I discuss a project Ive been working on that analyzes drone imagery to perform automatic damage detection using the Watson Visual Recognition , and generates 3D models from the drone images using photogrammetry processes . The best part the entire thing runs in the cloud on IBM @ @ @ @ @ @ @ @ @ @ here on the blog - In fact , - I just did the math , and- its been over 7 months. - Lots of things have happened since , I 've moved to a new team within IBM , built new developertools , worked directly with clients- on their solutions , worked on a few high profile keynotes , built apps for kinetic motion and activity tracking , built a mobile client for a chat bot , and even completed some new drone projects . - Its been exciting to say the least , but the real reason I 'm writing this post is to share a few- of the public projects Ive been involved with from recent conferences . <p> I recently returned from Gartner Symposium and IBMs annual World of Watson conference , and- its been one of the busiest , yet most exciting span of two weeks Ive experienced- in quite a while . <p> At both events , we showed a project Ive been working on with IBMs Global Business Services team that focuses on the use of small consumer drones and drone imagery to transform Insurance use @ @ @ @ @ @ @ @ @ @ automatically detect roof damage , in conjunction with photogrammetry to create 3D reconstructions and generate measurements of afflicted areas to expedite and automate claims processing . <p> This application leverages many of the services IBM Bluemix has to offer on-demand CloudFoundry runtimes , a Cloudant NoSQL database , scalable Cloud Object Storage ( S3 compatible storage ) , and BareMetal servers on Softlayer . Bare Metal servers are *awesome* I have a dedicated server in the cloud that has 24 cores ( 48 threads ) , 64 GB RAM , RAID array of SSD drives , and 2 high end multi-core GPUs . Its taken my analysis processes from 2-3 hours on my laptop down to 10 minutes for photogrammetric reconstruction with Watson analysis . <p> Its been an incredibly interesting project , - and you can check it out yourself in the links below . <h> World of Watson <p> World of Watson was a whirlwind of the best kind I had the opportunity to join IBM SVP of Cloud , Robert LeBlanc , on stage as part of the the Cloud keynote at T-Mobile Arena ( a huge @ @ @ @ @ @ @ @ @ @ the drone/insurance demo , plus 2 more presentations , and an " ask me anything " session on the expo floor . <p> You can also check out my session " Elevate Your apps with IBM Bluemix " on UStream to see an overview in much more detail : <p> .. and that 's not all . I also finally got to see a complete working version of the Olympic Cycling teams training app on the expo floor , including cycling/biometric feedback , video , etc I worked with an IBM JStart team and wrote the video integration layer into for the mobile app using IBM Cloud Object Storage and Aspera for efficient network transmission . <h> Drones <p> On this project we 've been working with a partner DataWing , who provides drone image/data capture as a service . However , I 've also been flying and capturing my own data . The app can process virtually any images with appropriate metadata , but I 've been putting both the DJI Phantom and Inspire 1 to work , and they 're working fantastically . <p> MobileFirst Platform Foundation provides a middleware solution and SDK @ @ @ @ @ @ @ @ @ @ security through encryption , authentication- and handshaking to guarantee app authenticity , - provides facilities- to easily manage multiple versions of an app , notify and engage users , and , on top of everything else , provides operational analytics so that you can monitor the health of your overall system at any point in time . <p> As a mobile developer catering to the enterprise , it makes your life significantly- easier , and it supports any mobile development- paradigm- that you might want to target : Native platforms , hybrid Xamarin using C# , and hybrid Cordova platforms ( HTML/JS ) . <h> What 's new in the IBM MobileFirst Platform Foundation 8.0 Beta ? <p> The recently opened beta has some great new features , AND its now available as a service on Bluemix ( IBMs Cloud platform ) . - The beta program is intended to deliver the next generation of an open , integrated and comprehensive mobile app development platform redesigned for cloud agility , speed , and productivity , that enables enterprises to accelerate delivery of their mobile strategy . <p> In my last post @ @ @ @ @ @ @ @ @ @ language at IBM . - Upon further thought , I guess- its probably not a bad idea to re-post- more detail here too <p> If you did n't  see/hear it last week , IBM unveiled several projects to advance the Swift language for developers , which we think will have a profound impact on developers &amp; developer productivity in the years to come . You can view a replay of the IBM announcement- in the video embedded below , or just scroll down for direct links : <p> Here are quick links to each of the projects listed : <p> Kitura A light-weight web framework written in Swift , that allows you to build web services with complex routes , easily . Learn more 
@@106848675 @2248675/ <p> This post specifically covers- native iOS , though there are also Android and hybrid options available . This should have everything you need to get started . It covers all aspects- from creating the app , to updating the back end , to leveraging Cloudant storage , push notifications , and monitoring &amp; logging . <p> So , without further ado , let 's get started <h> Part 1 : Getting Started with Bluemix Mobile Services <p> In this first video I show how to create a new mobile app on Bluemix , connect to the cloud app instance , and implement- remote logging from the client application . This process is covered in more detail in the Getting Started docs , but below- are the basics from my experience . <p> Youll- first need to- sign into your Bluemix account . If you do n't  already have one , you can create a trial account- for free . Once you 're signed in , you just need to create a new mobile app instance . <p> The process is very simple , and there is a " wizard " @ @ @ @ @ @ @ @ @ @ to do is create a new app by clicking the big " Create an App " button on your bluemix dashboard . <p> Create a new app from IBM Bluemix Dashboard <p> Next , select which kind of app you 're going to create . For MBaaS , you 'll want to select the " Mobile " option . <p> Select the type of app <p> Next you 'll need to select your platform target . You can choose either " iOS , Android , Hybrid " , or the " iOS 8 beta " target . In this case I chose the iOS 8 beta , but the process is similar for both targets. - Hybrid apps are built leveraging the Apache Cordova- container . <p> Select your platform target <p> Next , just specify an app name and click " Finish " . <p> Give your app a name <p> Once your app is created , you will be presented with instructions how to connect the app in Xcode . I 'll get to that in a moment <p> Now that- your app has been created , you 'll be able to see @ @ @ @ @ @ @ @ @ @ of several components : a Node.js back-end instance , a Cloudant NoSQL database instance , an Advanced Mobile Access instance , and a Push instance . The Advanced Mobile Access component provides you with app analytics , user auth management , remote logging , and more . The Push component gives you the ability to manage and send push notifications ( either manually , or with a rest-based API ) . <p> You app has been created here are the components and the activity <p> Once your app has been created , you will need to setup the mobile app to connect to Bluemix to consume the services . Again , this is a very straightforward process . <p> The next step is to register your client application . Once your app is created , you will be presented with a screen to do this . If you do n't  complete it right away , you can always come back later and register an application . You 'll need to specify the Bundle I 'd and version of your app , then you can setup any authentication ( if you choose ) @ @ @ @ @ @ @ @ @ @ Once your app has been registered , you need to configure Xcode . Youll first need to create a new project in Xcode . There are two options for configuring your Xcode project : 1 ) automated installation using CocoaPods , or 2 ) manual installation . I used the CocoaPods installation simply because it is easier and manages dependencies for you . <p> If you are n't  familiar with CocoaPods , it is much like NPM CocoaPods is a dependency manager for Cocoa projects . It helps you configure- the Bluemix libraries and manages dependencies for you . <p> If you 've got Xcode up and running , then close it and install CocoaPods , if you do n't  already have it . Next open up a terminal/command prompt , go to the directory that contains your Xcode project and initialize CocoaPods- using the " setup " - command : <p> pod setup <p> This will create a new file called " podfile " . Open this file in any text editor and paste the following ( note : you can remove any lines that you do n't  want to @ @ @ @ @ @ @ @ @ @ ' # Copy the following list as is and # remove the dependencies you do not need pod ' IMFCore ' pod ' IMFGoogleAuthentication ' pod ' **25;173;TOOLONG ' pod ' IMFURLProtocol ' pod ' IMFPush ' pod ' CloudantToolkit ' <p> Save the changes to the " podfile " file , and close the text editor . Then go back to your command promprt/terminal- - and run the installation process : <p> pod install <p> Your project will be configured , and all dependencies will be downloaded automatically . Once this is complete , open up the newly created . xcworkspace file ( Xcode Workspace ) . <p> You have to initialize the Bluemix inside of your application to connect to the cloud service to be able to take advantage of any Bluemix features ( logging , data access , auth , etc ) . The best place to put this is inside of your AppDelegate.m- class- application **29;200;TOOLONG method because it is the first code that will be run within your application : <p> One of the first features I wanted to take advantage of was remote collection @ @ @ @ @ @ @ @ @ @ IMFLogger class , in much the same fashion as you do with OCLogger in MobileFirst Foundation server . Once great feature that requires almost no- additional configuration is the- **25;231;TOOLONG method , which automatically configures the Advanced Mobile Access component to collect information for all app crashes . <p> Next , launch your app in the iOS simulator , or on a device , and you 'll see everything come together . Log into your Bluemix dashboard , and you 'll be able to monitor app analytics and remote logs . <p> Note : If you experience any issues connecting to the Bluemix mobile app from within the iOS simulator , just clear the iOS Simulator by going to the menu command " iOS Simulator -&gt; Reset Content and Settings " , and everything should connect properly the next time you launch the app . <h> Part 2 : Configuring the Node.js Backend <p> In the next video , I demonstrate how to- grab the code for the backend Node.js application , create a git repository on IBM JazzHub , then pull the code for local development . <p> When the app @ @ @ @ @ @ @ @ @ @ link under the app name . Using this link , you can create a git repository for the backend code . <p> Add a git repository <p> Once your git repo has been created , you can check out the code using any Git client ( I used the CLI ) . You 'll need to use the " npm install " command to pull down all the app dependencies . The biggest thing you need to know is that it uses express.js for the web application framework. - - You can make any changes that you want , and they will be automatically deployed to your Bluemix server instance upon commit . Yes , this workflow is also configurable b/c this process may not work for everyone . <p> One other thing that you will need to watch out for if you are doing local development : You will want to wrap the following code on line 6 in a try/catch block , otherwise you will hit errors in the local environment which will prevent your app from launching locally : <h> Part 3 : Consuming Data from Cloudant <p> @ @ @ @ @ @ @ @ @ @ database . The Cloudant NoSQL database is a powerful solution that gives you remote storage , querrying , and client-side- data storage mechanisms with automatic online/offline synchronization , all with monitoring/analytics capabilities . <p> By default , objects within the Cloudant data store are treated as generic objects ( over-simplification : think of it is an extremely powerful JSON store in the cloud ) . However you can also serialize your objects to strong data types within the client code configuration . <p> In your AppDelegate class- application **29;258;TOOLONG method , you 'll also want to initialize the IMFDataManager class , which is the class used for interacting with all Cloudant data operations . <p> IMFDataManager *manager = IMFDataManager sharedInstance ; <p> In my sample , I setup the database manually with open permissions , but you 'll probably want something more secure . Once your database is created , you can create indexes , search for data , create data , etc <p> In the following code , I create a search index and query for data from the remote Cloudant database . You really only need to create the index @ @ @ @ @ @ @ @ @ @ this either through the mobile app code , or manually through the Cloudant databases web interface . I did this inline in the following code , just for the sake of simplicity : <h> Part 4 : Push Notifications <p> The IBM Bluemix mobile services app also contains a component for managing push notifications within your mobile applications . With this service , you can send push notifications to a specific device , a group of devices using tags , or all devices , and you can send push notifications either manually via the web interface , or as part of an automated workflow using the REST API . <h> Part 5 : Monitoring and Logging <p> Did I- mention that every action that you perform through Bluemix Mobile Services can be monitored ? Analytics are available for the Advanced Mobile Access component , the Cloudant NoSQL data store , and the Push Notifications service . In addition , you also have remote collection of client logs and crash reports . This provides- - unparalleled insight into- the health of your applications . 
@@106848676 @2248676/ <h> Time-Lapse Photography with Creative Cloud <p> In addition to my addiction to aerial photography , I 'm also fascinated by time-lapse photography . With time lapse photography , you set up your camera to take pictures on an interval . This could be every few seconds , every few minutes , every few hours , or heck , once a day . Its really up to you how you want to set up your shots and what you want to shoot . In any case , you can end up with a lot images each by itself could be great , but it only tells a limited story . - However , you can put all those images together in a sequence to create some truly amazing visuals . Subtle motion becomes pronounced , and you can clearly view the passage of time . Often , this ends up with an amazing visual story that would be hard to otherwise capture . <p> All that you need start diving into time-lapse photography is a camera that is capable of capturing images on an interval normally there is some kind @ @ @ @ @ @ @ @ @ @ image frequency and duration . Then , once you 've got your images , you can process them with Creative Cloud tools to bring out their full potential . <p> Here are two time-lapse sequences I created this weekone a snow storm , one a sunset . <p> Neither sequence required a lot of specialized or expensive equipment . I used a GoPro Hero 3 Black camera , set it on my window sill , and let it do its thing . ( I do want to upgrade to better gear , but this still works fantastically , and I love the GoPro . ) <p> GoPro Hero 3 Black Edition <p> The sunset was a ten second interval captured over about 2 hours and played back in 30 seconds . The snow storm was a 60 second interval captured over roughly 14 hours , played back in 40 seconds . <p> So , you 've captured the images , what next ? - <p> You can check out the video below , or read on for further explanation how I processed and assembled the images into a video sequence , complete @ @ @ @ @ @ @ @ @ @ putting everything together as a sequence , I wanted to enhance the photos to bring out as much detail as possible . here 's where Adobe Lightroom comes into the picture . I used Lightroom to import all of my photos , add them to a collection , and then perform bulk/batch processing to enhance all of the images . <p> Editing Photos with Lightroom <p> First , select an image to use as your baseline for adjustments . I would n't start with your darkest image , and I would n't start with your lightest either . I normally start somewhere in the middle . Select the image , and then switch over to the " Develop " module . I use the basic panel to make adjustments to this image . For the GoPro , I like to bring up the shadows and bring down the highlights to pull out details out . If I 'm shooting a landscape , I also like to bring up the clarity and maybe even the vibrance and saturation just do n't  over do it . You could also use one of Lightrooms presets if you @ @ @ @ @ @ @ @ @ @ extra careful that it is not too dark or too light b/c were going to apply these settings to all images in the sequence . <p> Lightroom Basic Panel <p> If you want to adjust hue , saturation or luminance of specific colors , you can do that within the HSL/Color/B&amp;W panel . Using this you can make specific colors more or less intense . - I normally try to tone down the yellows in my GoPro images after I 've increased overall saturation . <p> Since I used the GoPro , there is a lot of fisheye distortion from the lens the GoPro has a 2.77mm lens whichgives an ultra-wide 170 degree field of view . This makes for some awesome wide angle shots , but sometimes you do n't  want that extreme distortion . This is where lens correction gets really handy . Next , I opened up the Lens Correction panel . As soon as you check the " Enable Profile Corrections " checkbox , Lightroom should automatically select the GoPro Hero 3 Black Edition lens profile based upon metadata within the image . I did n't  want @ @ @ @ @ @ @ @ @ @ angle , so I turned down the distortion correction using the " Distortion " slider . <p> Lightroom Lens Correction <p> Once you have your baseline image the way you want it , you need to apply these settings to all of your images in the sequence . Just select them all , and then either click on the " Sync " button in the bottom right of the Develop module , or use the Settings -&gt; Synch Settings menu . This will apply you changes on this image to all of the images that were selected . This will happen automatically if you are using auto-sync. - You can learn more about synchronizing metadata between photos in the Lightroom documentation . <p> Next , be sure to view several images in your collection , the lightest to the darkest , and make sure they all look decent . If you need to make any changes because they are too light , or too dark , or do n't  have the right contrast , then now is your time to fix it . Once you 're happy with the images in @ @ @ @ @ @ @ @ @ @ exported as JPG with 100% quality at full resolution with sequential names . <p> Now we 've got a lot of processed images . What 's next ? We need to make a video ! <p> If you 're wondering how I got the motion in the time lapse sequence , no I did n't  have the camera moving . There are devices which make this possible , but I just used a video editing trick . The images are 12 MP , or 4000 by 3000 pixels . A " standard " HD video sequence is 1920 by 1080 pixels . The image below reflects this scale the red area represents the 4000 by 3000 still image , and the yellow represents the 1920 by 1080 video . <p> Video &amp; Image Size Comparison <p> You 'll notice that leaves us with a lot of room to zoom and pan around the image . I zoom into the image so that it fills the entire horizontal space within the video sequence you can zoom in more if you want . This leaves a fair amount of vertical content outside the clipping rectangle of @ @ @ @ @ @ @ @ @ @ by panning vertically within this area . - I just made the pan very slow and deliberate so it appears that there is constant motion of the camera throughout the entire video . <p> The final result is that the content in the video ( yellow area ) appears to move because the actual image sequence is moving relative to the video viewport. 123433 @qwx983433 <p> can you please explain how to do the zoom and pan movement over so many images ? Are you putting in motion key-frames for every image ? That would take hoursany way to automate the process ? The motion in your videos looks great . 123435 @qwx983435 <p> You import the images as a sequence , and they get treated like a single movie clip . Then , set a keyframe at the beginning , and a keyframe at the end and add movement between the two keyframes . You do n't  need a lot of movement , Just subtle movement over the duration seems to work pretty well . <p> I was wondering how to have adobe Premiere treat the Lightroom edited images @ @ @ @ @ @ @ @ @ @ them into Premiere I do n't  have the image sequence button , but a numbered stills button . When I click this and the first image in the folder it does n't  upload them all . And if I select all the images in the folder to upload I can not check the numbered stills button . Any suggestions ? 123435 @qwx983435 <p> Maybe it is getting confused by the format of your file names . Try having descriptive text , followed by the sequence number . Something like " MyPhotoShoot-1.jpg " . This tutorial was done with Premiere Pro I think you are using Premiere Elements , but the process should be similar . See how the files are named here : LONG ... <p> Rachel Assendelft <p> Thank You soooo much ! Changing the file names worked ! <p> Karl Leonard <p> How do you get the white balance to blend from daylight to night correctly using this method of synching an image somewhere in the middle ? Doing it as shown , the image in the middle has correct white balance , but all earlier images get @ @ @ @ @ @ @ @ @ @ get shifted progressively towards the blue . <p> cjromb <p> Thank you for taking the time to detail this out ! What a fantastic tutorial ! <p> Lisa <p> Dear Andrew , this exactly what i was looking for ! Thanks a lot for describing this in these clear steps ! <p> I love the tutorial for the photo edits , I made it exactly the same settings as you and my photos look wonderful ! Just about to try out the Premiere tuto , I just wished you had been more explicit in regards to how to make it zoom on the photo but I 'll work it out 
@@106848678 @2248678/ <h> Perspective Warp in Adobe Photoshop CC <p> Today new versions of Illustrator , InDesign , and Photoshop were released , and there are some amazing new features added for Creative Cloud members . I 'd like to take this opportunity to show off a really cool new feature in Adobe Photoshop CC Perspective Warp ! Perpsective warp was sneaked at Adobe MAX last year its a new way to manipulate your images by changing their three dimensional perspective . - You can manipulate parts of the image to give the appearance that the camera perspective changes , all without having to create a complex 3D model ! This filter can be used for changing entire images , and is especially useful when aligning perspectives while compositing content from multiple images . <p> here 's a quick video I put together showing how you can use it within your own Photoshop creations . <p> All that you have to do is select the layer that you want to manipulate , then select the Edit -&gt; Perspective Warp menu option. - Using Perspective Warp is a two-step process . <p> First you @ @ @ @ @ @ @ @ @ @ within your images to match areas or geometries of content within your image . This might be the sides of a building , or other areas that you do n't  want deformed . <p> Next , Switch " Warp " mode . - In warp mode you can drag the pins/vertices to warp the content within the image . You can also hold shift and click on a line to lock that line to a horizontal or vertical position . <p> Drag the perspective warping where you want it , and then commit the changes by clicking on the " Commit " check button . <p> Perspective Warp Toolbar <p> Now , let 's look at two scenarios where Ive used perspective warp ( see the video for step by step details ) . <p> The first example shows how you can change the perspective/focal area of the image . - In this case I applied perspective warp to the entire image . - I created planes to match the Adobe building in the center of the image , and then used Perspective Warp to shift the building focal area to @ @ @ @ @ @ @ @ @ @ a truck onto a street . - In the original images , the trucks perspective is close , but does not match the perspective of the street . - Using Perspective Warp , you can easily re-shape content to match perspectives within your composition . <p> Hi Andrew Im glad I happened upon this post . My art is 3 dimensional . Its made of thousands of colored paper discs . Light source plays a major role when documenting the work . Light source must come from the right . Problem is the image becomes trapezoidal . I need to convert the photographic documentation of the artwork to square . Obviously later I would crop out the background . Here is an example . Can you let me know if Perspective Warp would accomplish this ? Ive used Transform and Puppet Warp in the past and they are just too liquid and a novices nightmare to use for this purpose . LONG ... <p> http : **26;289;TOOLONG Escape <p> How to make this new feature available to CC version only ? Do I need to install a new version or we can just upgrade it 
@@106848679 @2248679/ <p> If you think that voice-driven apps are too complicated , or out of your reach , then I have great news for you : They are not ! Last week , IBM elevated- several IBM Watson voice services from Beta to General Availability that means you can use them reliably in your own systems too ! <p> Let 's examine the two parts of the system , and see what solutions IBM has available right now for you to take advantage of <p> Transcribe audible signal to text transcript <p> Part one of this equation is converting the audible signal into text that can be parsed and acted upon . The IBM Speech to Text service fits this bill perfectly , and can be called from any app platform that supports REST services which means just about anything . It could be from the browser , it could be from the desktop , and it could be from a native mobile app . The Watson STT service is very easy to use , you simply post a request to the REST API containing an audio file , and the @ @ @ @ @ @ @ @ @ @ what it is able to analyze from the audio file . With this API you do n't  have to worry about any of the transcription actions on your own no concern for accents , etc Let Watson do the heavy lifting for you . <p> Perform a system action by parsing text transcript <p> This one is perhaps not quite as simple because it is entirely subjective , and depends upon what you/your app is trying to do . You can parse the text transcript on your own , searching for actionable keywords , or you can leverage something like the IBM Watson Q&amp;A service , which enables natural language search queries to Watson data corpora . <p> Riding on the heels of the Watson language services promotion , I put together a sample application that enables a voice-driven app- experience on the iPhone , powered by both the Speech To Text and Watson Question &amp; Answer services , and have made the mobile app and Node.js middleware source code available on github . <p> The app communicates to the Speech to Text and Question &amp; Answer services through the @ @ @ @ @ @ @ @ @ @ Mobile Access service to provide operational analytics ( usage , devices , network utilization ) and remote log collection from the client app on the mobile devices . <p> For the Speech To Text service , the app records audio from the local device , and sends a WAV file to the Node.js in a HTTP post request . The Node.js tier then delegates to the Speech To Text service to provide transcription capabilities . The Node.js tier then formats the respons JSON object and returns the query to the mobile app . <p> For the QA service , the app makes an HTTP GET request ( containing the query string ) to the Node.js server , which delegates to the Watson QA natural language processing service to return search results . The Node.js tier then formats the respons JSON object and returns the query to the mobile app . <p> The general flow between these systems is shown in the graphic below : <p> IBM Watson Speech QA for iOS Logic Flow <h> Code Explained <p> The code for this example is really in 2- main areas : The @ @ @ @ @ @ @ @ @ @ but could also be done in Swift ) , and the application server/middleware implemented in Node.js . <h> Node.js Middleware <p> The server side JavaScript- code uses the- Watson Node.js Wrapper , which enables you- to- easily instantiate Watson services in- just a few short lines of code <p> The credentials come from your Bluemix environment configuration , then you just create instances of whichever services that you want to consume . <p> I implemented two methods in the Node.js application tier . The first accepts the audio input from the mobile client as an attachment to a HTTP POST request and returns a transcript from the Speech To Text- service : <p> Note : I am using the free/open Watson Healthcare data set . However- the Watson QA service can handle other data sets- these- require an engagement with IBM to train the Watson service to understand the desired data sets . <h> Native iOS Objective C <p> On the mobile side- were working with a native iOS application . My code is written in Objective C , however you could also implement this using Swift . I wont @ @ @ @ @ @ @ @ @ @ brevity , but you can access the client side code in the ViewController.m file . In particular , this is within the postToServer and requestQA methods . <p> You can see the flow of the application within the image below : <p> App Flow : User speaks , transcript displayed , results displayed <p> The native mobile app first captures audio input from devices microphone . This is then sent to the Node.js servers /transcribe method- as an attachment to- a HTTP POST request- ( postToServer method on line 191 ) . On the server side this delegates to the Speech To Test service as described above . Once the result is received on the client , the transcribed text is displayed in the UI and then a request is made to the QA service . <p> In the requestQA method , the mobile app makes a HTTP GET request to the Node.js apps /ask method ( as shown on line 257 ) . The Node.js app delegates to the Watson QA service as shown above . Once the results are returned to the client they are displayed within a @ @ @ @ @ @ @ @ @ @ Mobile Access <p> A few other things you may notice if you decide to peruse the native Objective-C code : <p> Within AppDelegate.m you will see calls to IMFClient , IMFAnalytics , and OCLogger classes . These enable operational analytics and log collection within the Advanced MobileAccess service . 
@@106848680 @2248680/ <h> Updated : Parallax Effects in Hybrid/Web Apps <p> A while back I wrote about adding parallax effects to your HTML/JS experiences to make them feel a bit richer and closer to a native experience . - I 've just added this subtle ( key word *subtle* ) effect to a new project and made a few changes I wanted to share here . <p> If you are wondering what I am talking about with " parallax effects " Parallax movement is where objects in the background move at a different rate than objects in the foreground , thus causing the perception- of depth . - Read more about it if you 're interested . <p> First , here 's a quick video of this latest app in action . - Its a hybrid MobileFirst app , but this technique could be used in any **35;317;TOOLONG web app experience . - The key is to keep it subtle and not too much " in your face " , and yes , it is very subtle in this video . - You have to watch closely . <p> The techniques that I wrote @ @ @ @ @ @ @ @ @ @ a bit more to cover more use cases . <p> This sets the background image and default position . - The distinct change here is that I set the background size to " auto " width and 120% height . - In this case , you can have a huge image that shrinks down to just slightly larger than the window size , or a small image that scales up to a larger window size . - This way you do n't  end up with seams in a repeated background or a background that is too big to highlight the parallax effect effectively . <p> In the requestAnimationFrame loop , it only applies changes *if* there are changes to apply . - This prevents needless calls to apply CSS even if the CSS styles had n't  changed . - In this , I also truncate- the numeric CSS string so that it is n't reapplying CSS if the position should shift by 0.01 pixels . Side note : If you are n't  using requestAnimationFrame for HTML animations , you should learn about- it . <p> If you used my old code @ @ @ @ @ @ @ @ @ @ work . - Not even a little bit . - This has that fixed ( see comments inline above ) . <p> This moves the background in CSS , which does n't  cause browser reflow operations , and moves the foreground content ( inside of a div ) using translate3d , which also does n't  cause browser reflow operations . - This helps keep animations smooth and the UX performing optimally . <p> I also added a global variable to turn parallax on and off very quickly , if you need it . <p> The result is a faster experience that is more efficient and less of a strain on CPU and battery . - Feel free to test this technique out on your own . <p> If you use the code above , you can modify the xMovement and yMovement variables to exaggerate the parallax effect . 
@@106848682 @2248682/ <h> Enabling Gestures in Edge Animate Compositions <p> Another request that I have gotten from some of our DPS customers is that theyd like to be able to implement gestures inside of the Edge Animate compositions that they are building for DPS publications . This includes double-tap gestures , swipe gestures , etc Out of the box , these gestures are n't  supported , but you can add them to any Edge Animate composition without too great of an effort . <p> Below is a quick video showing Edge Animate Compositions that are taking advantage of both double-tap and swipe gestures . - Note : I intended these to be used inside of DPS , but I show them in Safari on iOS. - These gestures override the default mobile browser behaviors . <p> As I mentioned above , this is n't something that is supported out of the box , but it is possible to add gesture features manually . <p> The links below are for the basic examples that I put in the video . - Both should work in desktop and mobile browsers : <p> For the @ @ @ @ @ @ @ @ @ @ anywhere on the stage ( the image area ) , and the animation will start again from the beginning . - For the swipe gesture , just perform a horizontal swipe in either direction with either your finger , or the mouse . <h> Gestures With Hammer.js <p> I leveraged the hammer.js JavaScript library to handle gesture detection since these gestures are n't  supported by default . - Hammer.js also enables other gestures , like long taps , pinch , rotate , etc - However , I 'm only showing double tap and swipe . - You can read more about hammer.js using the following links : <p> I used this exact setup procedure in both the double-tap and swipe examples . <p> To include this library , I first downloaded the hammer.js file , and saved it inside of the " edgeincludes " folder . <p> Next , you have to disable the web view/browser default double tap behavior , which is to zoom in when double tapped . - You can disable the zoom on double tap by adding a viewport metadata tag inside of the Edge Animate projects @ @ @ @ @ @ @ @ @ @ ( in this case " DoubleTap.html " , and add the following line to the &lt;head&gt; <p> Next , we have to add the code inside of the Edge Animate composition to enable the gesture behavior. - The first thing you have to do is include the hammer.js library . - In this case , I wanted to add the gestures to the compositions stage , instead of a particular element . - So , right-click on the stage in the Edge Animate Editor , then select - the " Open Actions for Stage " menu option . <p> This will open the actions for the Stage instance . - Next , click on the " + " icon and select " creationcomplete " . - This will create a function that gets invoked once the Stage instance has been created at runtime . <p> In that function , first we need to import the hammer.js library . - Edge Animate compositions include the- yepnope.js library , which is originally intended to detect if a browser includes a specific piece of functionality . - If not , then include a @ @ @ @ @ @ @ @ @ @ this case , I am passing it a blank test to force it to include the hammer.js library . - The following function forces loading of the hammer.js library . - Once the library has been loaded into memory , it triggers the " init " function : <p> In the init function , we grab a reference to the stages element ( div ) , then use hammer.js to add our gesture event handlers : <p> Now , we need to start looking at the individual examples <h> Double Tap Gestures <p> In the double tap example , we have a simple timeline animation that plays sequentially . - At the end of the sequence the animation is stopped by a simple sym.stop() function call . - Heres a quick preview of the setup in Edge Animate : <p> To add the double tap gesture , all you have to do is add a hammer.js event for " doubletap " . - In that event handler , were just calling sym.play(0) , which restarts playback from the beginning of the composition . The full code for the creationcomplete event @ @ @ @ @ @ @ @ @ @ needed to add the double-tap gesture to the composition stage instance : <h> Swipe Gestures <p> In the swipe gestures example , we have a simple timeline animation that plays sequentially . - However , at the end of each slide transition , playback is stopped by a simple sym.stop() function call . - Whenever we perform a swipe action , were either just playing forward , or playing in reverse until the next slide animation stops . - Heres a quick preview of the setup in Edge Animate , note the stop points highlighted by the arrows : <p> To add the swipe gestures , all you have to do is add a hammer.js event for " swipeleft " or " swiperight " . - In those event handlers , were just calling sym.play() or sym.playReverse() , depending whether it was a left or right swipe . - These play actions progress to the next animation sequence . The full code for the creationcomplete event is shown below . - This is all that is needed to add the swipe gesture to the composition stage instance : <p> With @ @ @ @ @ @ @ @ @ @ conflicts on mobile devices . - If you run into this and you do not want the page to scroll , the scroll action can be prevented by capturing the touchstart event , and canceling the default behavior. - I did n't  add this , just because I wanted to keep this example very simple . <p> Sorry , that link is n't working for me I get a 404 error , so I 'm not sure what issue you are running into . <p> http : //partycrashers.co.nz Karuna <p> it works fine locally . here 's my index.html Any help you can provide would be great ! <p> EDGY <p> window.jQuery document.write ( " ) 123436 @qwx983436 <p> Looks like the blog system removed the code you posted . Try putting it in a gist for sharing : http : //gist.github.com/ <p> Jose Andrade <p> This does n't  seem to work in Edge Animate CC . Thoughts ? <p> Fang Zhou <p> Animation published with CC used in DPS works on some iPads and not the others And the the animation made with CC affects functionality of animation in neighboring articles and @ @ @ @ @ @ @ @ @ @ ? Right now , we are just using Edge animate 1.5 to publish the animations , it seems to work fine . <p> Nicholas Jacobs <p> What about pinch and zoom on an object ? Any suggestions ? <p> Nicholas Jacobs <p> I want to zoom in on the whole page , because I 'm making a custom app within Phone Gap Build , and have everything working EXCEPT the pinch zoom . HELP ? <p> Yolande Sukal <p> I 'm a designer with beginner knowledge of Adobe Edge and HMTL 5 . The scrolling worked great in a widget published for iBooks . Just had to remember to put the hammer.js in the published . wdgt file . Thank you ! <p> dylangraft <p> Ive run into the drag event conflicts that you were talking about inside of DPS . When you swipe horizontally , the animation plays but DPS also tries to pull the next article over at the same time . I would like to keep this from happening I 'm brand new to working in edge so any help would be greatly appreciated . <p> rubens <p> Hi , @ @ @ @ @ @ @ @ @ @ events that I can use in my ipad . I export html from to Edge Animate , and I insert in Indesign to digital publishes ( DPS ) . <p> I would like to know if there is a way to make the scroll smooth . I mean , if you give a little push with your finger and it just scrolls by itself and goes slower at the stop ( don-t stop suddenly like in the example ) . Is this possible ? <p> this is great ! i 'm building an app inside animate and want to use swipe left , right , up and down for navigation , do you think ill be able to implement this using hammer ? 123435 @qwx983435 <p> Actually the latest version of edge animate has native support for swipe gestures . You wo n't even need hammer.js for basic swipes you would need it for other features though , like pinch zoom or 2 finger tap <p> nathaniel harman <p> that 's cool , what about swipe up and down though ? <p> Nicolas Celers Noeud Pap <p> Hi Andrew <p> nice tuts ! @ @ @ @ @ @ @ @ @ @ swip , DPS want to change page . <p> is it possible to have more information about : " the scroll action can be prevented by capturing the touchstart event , and canceling the default behavior " cause i do n't  understand very well what i have to do <p> thanks alot <p> Leonardo Alannis <p> Andrew , I can not thank you enough for this tutorial . You really helped me add the " fuck-yeah ! " factor to my website . I 've given you a shout out on my blog ( **29;354;TOOLONG ) . Grazie , Gracias , Danke , Merci ! <p> http : //www.bebdesigner.com Bertrand Berlureau <p> Hi , did the gestures can be combined in the same project ? Swipe + long tap on a same scene for example ? 123435 @qwx983435 <p> Yes , they can be combined . <p> http : //www.bebdesigner.com Bertrand Berlureau <p> Ok , thanks for your answer , I will test it as soon as possible . Many thanks for the tutorial ! 
@@106848683 @2248683/ <p> A few weeks ago , a fellow Adobe colleague showed me a DPS publication that had an amazing design . All of the content looked great by itself , but what really made parts of it " pop " was that in certain areas there was a 3D parallax effect , which made it feel like you were looking into an image that had depth . You could rotate the device and see what 's hiding behind a person , or around the corner of a building . <p> Heres what I mean on the surface the image looked static , but as I rotated it , elements shifted to give the illusion of depth . The background and foreground elements all moved at different rates : <p> 3D Parallax Effects on a Device <p> I thought this was an incredible example of added interactivity and immersive- experiences , and its not really that difficult to implement. - In fact , I put together this tutorial to show exactly how you can create these types of effects in your own compositions . <p> To create this kind of an @ @ @ @ @ @ @ @ @ @ break apart an image into layers note : - you may need to synthesize edges so that there is an overlap in all transparent areas . Then you need to add interactivity in HTML . Align those images so that their default state looks just like the still image , then move the images based upon the device orientation . I move the foreground one way , keep the middle content more or less stationary , and move the background content the opposite direction ( all based upon which way you are rotating the mobile device ) . Since this is all HTML , you can take this content and use it on the web , or import it into Adobe InDesign to export a DPS digital publication . <h> Step 1 : Create Layered Images <p> You can either create your own layers , or break apart an existing image into layers so that each individual layer can be placed over top each other to form a seamless composition . In this case , I separated the strawberries , the rows of plants , my daughter , and the @ @ @ @ @ @ @ @ @ @ I added logic to support both landscape and portrait orientation . <p> Be sure to add both of those JavaScript snippets inside of the creationComplete event for the Stage . - I also over-exaggerated the movement in the timeline. - I think it would look better with slightly less ( more subtle ) movement . <p> At this point , you could publish the composition and use it on the web there 's nothing stopping you at all . In fact , you can check it out here , just load it on an iPad and rotate the device to see the effect . However , please keep in mind that 1 ) I have n't added a preloader , 2 ) the assets are non-optimized and are all retina size , 3 ) I do n't  have it auto scaling for the viewport size , so it will only look right on a retina iPad , and 4 ) I have only tested this on an iPad no other devices . <p> Note : You could also do this without using Edge Animate , but you 'd have to hand code the @ @ @ @ @ @ @ @ @ @ InDesign/DPS Composition <p> To include this in a DPS publication , all that you need to do is export an Animate Deployment Package ( . oam file ) from Adobe Edge Animate . You can then just drag and drop this into InDesign for inclusion in a DPS publication . <p> If you are n't  already a member of Creative Cloud , join today to take advantage of all of our creative tools ! <p> Update : After publishing this I realized that the movement of the plants should actually be reversed . - If you view this link , you 'll see the updated motion- ( which looks more realistic ) , but I cant update the video that 's already been published . <p> Do you know if there 's a way to determine if the viewer is using a tablet or mobile device as compared to a desktop ? So that the parallax will work if its using the excellerator or a mouse . Perhaps by using some kind of " if " statement in the code ? 123435 @qwx983435 <p> Yes . You could use JS to detect if @ @ @ @ @ @ @ @ @ @ already supports this . If device motion events are n't  supported , use mouse interactions . Or , you could just use mouse events , which you could block by intercepting touch events . <p> Sam Totman <p> Great tutorial and project file was a big help as well . This was just what I was looking for . Is it necessary to disable the touch commands ? I used this technique for a cover page of a folio , but at a lost as to how to get my users from the cover page to the next article without a swipe gesture . I tried a button but no luck . I 'm sure its an easy fix but for the life of me Im not having any success . Thanks <p> caffeineandpixels <p> I render all of my frames for 3d lenticular projects out of After Effects . Can this work with individual frames , or an animated gif ? This would be a great way to soft-proof lenticular . An app would be very useful . Each of my layers have displacement maps applied , so they 're not @ @ @ @ @ @ @ @ @ @ have volume . Like this LONG ... 123435 @qwx983435 <p> You could create an image sequence on the timeline ( each frame has a different image ) , and scrub through the sequence using the JS device motion events , similar to what 's done in this example . Though , this would be pretty heavy on file size . If it can be broken into layers and use the layered technique it would be lighter , but that 's not always an option . Standard image sequences in InDesign/DPS do n't  support device motion , and you ca n't scrub through the frames of a GIF . <p> caffeineandpixels <p> Not sure what you mean . As I said , I 'm not a programmer so code is foreign to me . Also not sure why you 'd use a page layout program like Indesign when you have After Effects . When I create DVD covers I have every element on a separate layer ( sometimes hundreds of layers ) with extra image n the sides for the 3d offsetting . Individual layers can have depth maps applied , which is great for selling @ @ @ @ @ @ @ @ @ @ render individual frames or a Quicktime . I normally render frames , bring them into PS and layer it out to create the animated gifs to show customers before going to a hard proof . Any way you could create a Photoshop Action or droplet or something to make it more like an app so non-programmers could do this easier ? Its a fantastic idea and I see a lot of potential use for it . I 'd pay for an App that did it . LONG ... 123435 @qwx983435 <p> OK I thought you meant for use in DPS , since that 's what I originally had posted about . This wo n't work inside a GIF , as there is no API . However , you can take those frames and put them into an animation composition using Edge Animate , so it would work in HTML using the device motion JavaScript APIs . Let me put together a sample , and I 'll post back later . <p> benedikt sk <p> hello , thank you for the tut , that is awsome . I have one question , does it also @ @ @ @ @ @ @ @ @ @ the retina dimension oam file into a retina indesign file it only displays the animation in the wrong scale , only the upper right part of the animation . My next question is , if its possible to autoplay this animation , so that you do n't  have to click before the parralex starts ? <p> And is it possible to swipe down to the next page ? <p> I would really apppreciate your answer . Thank you very very much . <p> crom <p> hi , is very good , i test your exemple , on my ipad mini but i note work in full screen how make full screen please ? <p> crom <p> hi , its very fantastic , i try it , but i do n't  know how to make full screen on mini ipad to look exemple . <p> crom <p> how to make exmple on full screen , please on ipad mini . <p> volwin <p> Hi Andrew , thanks for this inspiring tutorial . I build a iPad page with it and it works perfect . But I realized , when I turn @ @ @ @ @ @ @ @ @ @ landscape orientation the animation also runs reversed and that is really killing the 3D effect . Is there any easy solutionfor this problem ? <p> I accurately used your code and it basically works for both orientation . But elements that move to the right at the 0- orientation , move to the left at the 180- orientation . So is there a way to get elements behave exactly the same way for both orientation seen from viewers perspective ? <p> Yes , I understand . As far as I can see , this does n't  solve the problem of the fact that in the moment the iPad screen content flips to the new orientation , the direction parameters work the wrong way round . But I really do n't  want to bother you with this anymore . I try to find a solution and will post it when I have it . Many Thx . <p> Shiggy Diggy <p> Hi andrew ! Love this tutorial and I plan on using it on my dps school project , I have question thought , will this effect work on the 1st @ @ @ @ @ @ @ @ @ @ convert some animation to symbol , and it did n't  work on ipad orientation . it just play by it self . please help me .. i do n't  understand script . thanks a lot sir . <p> http : //www.gidf.de Ken <p> Hey Andrew , thank you so much for this tutorial , i really love the effect . Its Amazing . One Question : The Parallax-Effect works on the Ipad , everything looks nice , but i cant swipe to the next site , or put some Content in the foreground of the embedded html File in the DPS . How can i add this Effect in my Magazin , with the Possibility to swipe to the next site ? 
@@106848684 @2248684/ <h> " What is PhoneGap ? " &amp; Other Common Questions <p> While looking at the analytics for my blog , I 've recently started to see a lot of search phrases similar to " what is phonegap ? " , " how does a phonegap app look ? " , " how to get started in phonegap ? " , among many , many others . - In this post , I hope to she 'd some light on some basic questions to help you understand and start working with PhoneGap . <p> In case you do n't  feel like reading the whole thing , here are quicklinks to each question : <h> What is PhoneGap ? <p> PhoneGap is an application framework that enables you to build natively installed applications using HTML and JavaScript. - The easiest way to think of PhoneGap is a web view container that is 100% width and 100% height , with a JavaScript programming interface that allows you to access underlying operating system features . - You build your user interface using traditional web development skills ( HTML , CSS , &amp; JavaScript ) @ @ @ @ @ @ @ @ @ @ application ecosystems and devices . - When packaged for deployment , the PhoneGap application is a binary distributable file that can be distributed by the " normal " application marketplaces ( iTunes , Google App Market , Amazon Market , etc ) . <h> How does a PhoneGap application typically look ? <p> Since the UI rendering engine is the mobile devices web browser , PhoneGap applications can literally look like anything . - You can use standard HTML &amp; CSS to make it look like a normal web page , you can use a UI framework like jQuery UI , Kendo UI , Sencha , - Twitter Bootstrap , or Skeleton- ( or any other HTML/CSS/JS user interface framework ) . You can also use CSS styles/themes to make your web content look like native apps , such as- iUI- to mimic iOS or Android , or- bbUI - to mimic BlackBerry . <p> PhoneGap applications can have static UIs based on normal HTML , or can have dynamic &amp; interactive experiences developed using JavaScript. - It depends upon the specific application , user experience design , target audience @ @ @ @ @ @ @ @ @ @ will appear . <p> PhoneGap applications can use pinch/zoom gestures to zoom in &amp; out , or you can lock the viewport scale using the viewport metadata tag . - You can have the page scroll using normal browser behaviors , or you can use a library like iScroll to enable touch-based scrolling of specific container elements . <p> There really are lots of ways to create a user interface with HTML , CSS &amp; JavaScript , so there really is n't any " typical " look . - If you do not apply any CSS styles at all , then all user interface elements will use the operating system/browser default for that specific platform . - This includes buttons , links , and color/highlight states . - This behaves in the exact same manner as the operating systems default web browser . <h> How do I get started in PhoneGap ? <p> Getting started in PhoneGap is easy . - For 90% of a PhoneGap application , all you need is a text editor . - PhoneGap also integrates with device-specific development environments very easily . - You can view @ @ @ @ @ @ @ @ @ @ the links below : <p> When developing PhoneGap applications , just keep in mind that you are running code inside of a web browser instance . - You develop your applications using HTML and JavaScript , not native code , so you do n't  need anything special . - In fact , I personally do most of my development on the desktop using an HTML editor and the Chrome browser. - When I need device-specific functionality , or I need to test on a device , then I switch over the the device-specific environments . <h> How do you debug PhoneGap applications ? <p> Debugging PhoneGap applications can sometimes be the trickiest part of development . - If you are testing on a physical device , you cant always get access to JavaScript exceptions when they happen . - There are a few strategies for debugging PhoneGap applications . <h> Develop as much as possible on the desktop browser <p> Since PhoneGap applications are written with HTML , CSS , and JavaScript , you can develop most of them using any HTML editor and debug them within a desktop web @ @ @ @ @ @ @ @ @ @ ( including Chrome , IE , Firefox , Opera and Safari ) provide rich debugging features . In the developer tools for the browsers , you can inspect HTML DOM elements , inspect CSS styles , set breakpoints in JavaScript , and introspect into memory &amp; JavaScript variables . - You can learn more about the desktop browser development tools at : <p> Once you build the main aspects of your application using desktop tools , you can switch over to a device-specific environment to add device-specific behavior and integrate with PhoneGap APIs . <p> It is imperative that you test your applications on actual devices ! - Actual devices will have different runtime performance than desktop browsers and simulators , and may unearth different bugs/issues including API differences and different UX scenarios . <h> Debug With debug.phonegap.com <p> PhoneGap provides a hosted service that allows you to perform remote , on-device debugging through- debug.phonegap.com. - This uses the Weinre ( Web Inspector Remote ) debugging tool to allow you to remotely inspect the DOM , resource loading , network usage , timeline , and console output . - If @ @ @ @ @ @ @ @ @ @ , this will look very familiar . - You will not be able to set breakpoints on the mobile device , but it is certainly better than nothing at all . <h> Remote Web Inspector Through iOS 5 <p> There is a little known undocumented API introduced in iOS5 that allows you to perform remote debugging through the iOS5 Simulator. - You just need to enable remote debugging <p> Then launch the application in the desktop iOS Simulator . Once the app is running , open a local Safari instance to : http : //localhost:9999/ . This will launch the remote debugger , complete with breakpoints and script introspection . <h> More Debugging Info <h> How do you architect PhoneGap applications ? <p> You generally architect PhoneGap applications the same way that you create mobile web experiences . The difference is that the initial HTML assets are available locally , instead of on a remote server . - The PhoneGap application loads the initial HTML , which can then request resources from a server , or from the local environment . - Since PhoneGap is based in a browser , @ @ @ @ @ @ @ @ @ @ to behave . - You can load multiple pages ; however , keep in mind that once you load/unload a page you may lose any data that is stored in memory via JavaScript. - PhoneGap also supports the single-page web experience model . I strongly suggest using the single-page architecture approach . <h> Single-Page Architecture <p> A single-page architecture refers to the practice of having a single HTML page that dynamically updates based upon data and/or user input . - You can think of this as closer to a true client/server architecture where there is a client application ( written with HTML &amp; JS ) and a separate server structure for serving data . - All client-side application logic resides in JavaScript. - The client application may request data and update its views without reloading the current web page . <p> Using a Single-Page architecture allows you to maintain data in-memory , in JavaScript , which allows you to have a stateful , yet dynamic user interface . - You can read more about single-page architectures at : - LONG ... <h> How do you get PhoneGap apps on devices @ @ @ @ @ @ @ @ @ @ deployed using the same guidelines for native applications for each given platform . - You must follow the rules of each hardware platform/vendor , and there is no way to get around that . - - You can compile the executables for each platform yourself using each platforms specific build process , or you can use build.phonegap.com to compile them for you . - build.phonegap.com is a hosted service that will compile platform-specific application distributable files for you . - In either case , the output of the build process is a platform-specific binary file : IPA for iOS , APK for Android , etc - You can read more about distributing to various application ecosystems , and each systems signing/certificate requirements at : <h> What is the difference between PhoneGap and AIR ? <p> The most fundamental differences between PhoneGap and AIR is that you develop AIR applications using tools rooted in the Flash Platform ( Flex , Flash , ActionScript , MXML ) , and you develop PhoneGap applications using HTML , CSS , &amp; JavaScript. - AIR applications use the AIR runtime , which allows you to @ @ @ @ @ @ @ @ @ @ expected behavior across all supported platforms . - PhoneGap applications run inside of the native web browser component for each supported platform . - For this reason , a PhoneGap codebase may behave slightly different between separate platforms , and you will need to account for this during your development efforts . <p> Air applications can be built for iOS , Android , BlackBerry Playbook , and the desktop ( mac and windows ) , with future support for Windows Metro ( Windows 8 mobile interface ) . You can read more about AIRs supported platforms at : - LONG ... <p> ActionScript has strongly-typed objects and supports classical inheritance programming models . AIR applications can also be built using the Flex framework , which allows you to rapidly build enterprise-class applications . - Components in AIR applications are logical objects that have behaviors , properties , and a graphics context . <p> JavaScript-based applications support prototypal inheritance , and have numerous open-source frameworks/tools that can be used . - HTML/JS applications are all visualized through HTML DOM elements . - HTML interfaces can be created through basic string concatenation @ @ @ @ @ @ @ @ @ @ really just creating DOM elements that have properties and styles . <p> There are some fundamental difference in the syntax of building these applications , however the basic concepts of interactive design and interactive development are identical . - Both platforms have valid strengths , which I could write about ad nauseum I 'll save that for another post . <p> This one article is even better than your own " PhoneGap Explained Visually " I publicly said was the best of the best . So I wish I had read this one BEFORE we began our dev . Thanks , A.R . ( Again , I disagree strongly with the so-called " single-page architecture " , what a code mess that ought to be ! ) 
@@106848685 @2248685/ <h> On-Device Debugging With PhoneGap &amp; iOS 6 <p> Want to debug your PhoneGap apps , complete with breakpoints , DOM &amp; CSS inspection , profiling , and more ? - This is all possible with the- PhoneGap Emulator , - which allows you to leverage Chromes Developer Tools inside of the desktop Chrome browser ( covered in detail here ) . - However , did you also know that you can have a rich debugging/development experience in an app that is actually running on a device ? <p> Since the release of iOS 6 last Summer , weve all had the ability to debug PhoneGap apps while they are running on external iOS devices , or inside of the iOS simulator . Im surprised how often I hear that people are not aware of this feature . With iOS 6 you can use Safaris developer Tools to connect to any HTML content on the device , either in the mobile Safari browser , or inside of a web view . PhoneGap apps fall into that second category , they are based upon iOS system web view . <p> @ @ @ @ @ @ @ @ @ @ iOS in the video below : <p> In order to take advantage of this , you 'll first have to enable the remote web inspector for Safari on iOS . Just follow the instructions for " Debugging Web Content on iOS " from Apple be sure not to skip the " Enable Web Inspector on iOS " - section , which is hidden by default . - You have to enable this in iOS Settings in order for the desktop Safari Browser to be able to connect to any web content on the mobile device . <p> Unfortunately , this is only available for PhoneGap on iOS devices at this time . Android enables remote debugging inside of the Chrome browser , however that is n't enabled for PhoneGap apps *yet*. - Whenever Google enables Chrome for web views inside of apps , its on ! 123433 @qwx983433 <p> We use this at times but it is REALLY difficult to get right . Safar is very fickle with large JS files and crashes often . <p> You also can not see the very beginning of your app startup because you have to @ @ @ @ @ @ @ @ @ @ on device can be very difficult and painful at times . 123436 @qwx983436 <p> True , the startup debugging can be tricky b/c you 'll have to reconnect to Safari . However I have not run into the crashing issue , and I 've got a few very complicated apps . How big are your " large " files ? <p> http : //mobileicecube.com/ Ville <p> Thanks so much ! I was aware I could do this with the simulator but never figured out how to enable it on the device . So useful that I can finally properly debug my app with the camera on device . <p> ahmad kas <p> Hi Andrew , May I aks you something , I was upload an app using phonegap API to download and store the file on to iPad but apple rejected the app because this : <p> We found that your app does not follow the iOS Data Storage Guidelines , which is required per the App Store Review Guidelines . <p> In particular , we found that on launch and/or content download , your app stores 8.89 MB . To check @ @ @ @ @ @ @ @ @ @ iOS Data Storage Guidelines indicate that only content that the user creates using your app , e.g. , documents , new files , edits , etc. , should be backed up by iCloud . <p> Temporary files used by your app should only be stored in the /tmp directory ; please remember to delete the files stored in this location when the user exits the app . <p> Data that can be recreated but must persist for proper functioning of your app or because customers expect it to be available for offline use should be marked with the " do not back up " attribute . For NSURL objects , add the **28;385;TOOLONG attribute to prevent the corresponding file from being backed up . For CFURLRef objects , use the corresponding **29;415;TOOLONG attribute . <p> For more information , please see Technical Q&amp;A 1719 : How do I prevent files from being backed up to iCloud and iTunes ? . <p> What 's should I do ? I have set BackupWebStorage to " none " on config.xml and reupload but apple still rejected the app . <p> please help I do @ @ @ @ @ @ @ @ @ @ <p> Thanks . 123436 @qwx983436 <p> What are you storing ? That does seem like an awful lof content to put into storage just to access content . If you need to store files locally , you can use LONG ... 0 , onSuccess , onError ) ; from the file API to get access to temporary storage . More detail at : LONG ... <p> Tom Wilson <p> I 've found that if you put an alert at the start of your javascript , the code will be delaying in executing , then you can start the debugger and after you click the alert , your code continues to execute and you can then continue to debug . <p> http : //www.garthwaite.info Paulo <p> I also had difficulty debugging startup code . <p> I found that it helped to set a breakpoint in the startup code and then go to the console in Web Inspector and enter : window.location.reload() <p> Jan Becicka <p> I tried it on my iPad and it does not work . I 'm able to inspect pages on Mobile Safari , but not real device . I @ @ @ @ @ @ @ @ @ @ , but when I run it on real device I see only " No Inspectable Applications " Any idea what I 'm doing wrong ? Thanks 
@@106848686 @2248686/ <h> Who Uses PhoneGap/Apache Cordova ? <p> I see questions and comments all the time with the general sentiment " it looks nice , but who really uses PhoneGap/Apache Cordova ? " . There is no way to create a definitive list of everyone who uses it , but the general answer is " more people than you think " . Here are a few organizations that you might recognize who are using either PhoneGap or Apache Cordova in their cross-platform mobile solutions and/or tools . ( PhoneGap is a distribution of Apache Cordova ) <h> Microsoft <p> Microsoft is involved with core Apache Cordova development ( specifically for the Windows Phone platform ) . - Not only are staff from Microsoft committers for the core Apache Cordova project , Microsoft has also used PhoneGap on public mobile applications that target multiple platforms . - This includes the XBox-Live integrated gaming application Halo Waypoint , for both iOS and Android . - Check out Halo Waypoint in the video below , it looks awesome : <h> Adobe <p> I think most people already know how deeply involved Adobe is @ @ @ @ @ @ @ @ @ @ In late 2011 , Adobe acquired Nitobi , the creators of PhoneGap , and contributed PhoneGap to the Apache Software Foundation as the Apache Cordova project . - Adobe has resources dedicated to furthering PhoneGap and is dedicated to the success of the platform . - Not only are we helping develop and mature PhoneGap/Apache Cordova , we also build some of our own applications with it . - ( Maybe I 'll be able to talk about those some day . ) <h> Zynga <h> Logitech <p> Logitech- used PhoneGap to develop the Logitech Squeezebox Controller application , which uses your home wifi connection to control a Squeezebox Internet radio device from your smart phone . - You can read more about this application on the PhoneGap application showcase , or download it now for iOS or Android . <p> Still not sure if anyone uses PhoneGap ? What about these , among many others ? 123433 @qwx983433 <p> I prefer indies app , big companies do not always produce great apps But I understand that showing those names can reassure . For what its worth , Apple featured one @ @ @ @ @ @ @ @ @ @ French TV ad for iPad : https : //www.youtube.com/watch ? v=hi6s7G3SIno The app is the first one , TEDxParis. 123436 @qwx983436 <p> Thanks for the input Thomas , and thanks for sharing the video . I did not know that TEDx was using PhoneGap . There are lots of indies using PhoneGap too I just do n't  know the vast majority of them . <p> Thomas <p> And just to be clear , only TEDxParis and TEDxConcorde apps are using PhoneGap . I do n't  know for the official TED app + others TEDx apps <p> http : //jrgalia.com JR Galia <p> This was introduced to me by my friend . Then I look to apps developed by Apache Cordova viola ! ! ! Great list . <p> I will try this one for my next project . <p> No Way <p> Wikipedia app feels amateurish . Havent seen any phonegap apps that would perform well . It lags on my iphone 4s &amp; 5 . And its even worse on android devices . The only reason why this crap is used is because its cheaper to target multiple platforms . You get what you pay for . 
@@106848687 @2248687/ <p> This new feature enables you to be able to dramatically scale content for special effects , or convert low resolution content to HD or even Ultra-HD formats without sacrificing quality . <p> Check out the video below to see it in action . I 've included two examples of using the Detail Preserving Upscale to convert to 4K Ultra HD . One old/historical video upscaled to 4K , and one is a 1080p video upscaled to 4K . In both of the examples , you will see that there is better contrast and less pixelation in the final upscaled result . <p> Here are some screen grabs from the editing process . The center ( small ) image is a still from the original source clip at 100% size . - The image on the left is resized using After Effects normal scale transform , and the image on the right uses the new Detail Preserve Upscale effect . - If you look closely , you can see that the image on the left has more pixelation , and there is less contrast in the face . - With @ @ @ @ @ @ @ @ @ @ darker , and you do n't  have pixelation/distortion . <p> Detail Preserve Scale Comparison ( scaled to 850% size ) <p> If you 'd like to see the final result , check out the JFK video below . You 'll just need to set YouTube settings to " Original " resolution to see the full size videos , though you 'll need a 4K monitor to see the video at full resolution . 
@@106848689 @2248689/ <p> In January 2012 , I started the year with a post on multi-screen applications developed with PhoneGap . In that post , I describe an approach for creating mobile applications that extend the app experience onto a second screen where the mobile device drives the content on that second screen essentially having an " external monitor " capability on a mobile device . <p> Mobile App Drives External Screen <p> Now , I 'm going to turn things around I 've been experimenting with a few ideas of connected secondary-experience applications , and I figured this would be a great way to come full circle and end 2012 . I see the secondary app experience as having huge potential for our connected/media-centric world . The secondary app experience is different in that the application is your " second screen " , perhaps a companion to something else that you are doing . For example , the secondary screen is a mobile application that augments the experience of watching television . Perhaps it is a mobile application that augments the experience of playing a video game , along the same concept @ @ @ @ @ @ @ @ @ @ platform . The key element is that the mobile application is not only an augmented experience to the television-based content , but that it is also updated in real time as you watch the program , or as you play the game . <p> External Screen Drives Mobile App <p> In this post I 'll show a proof-of-concept second screen experience where the content of a mobile PhoneGap application is being driven by an external source ( a video ) using- audio watermarks . In this case , the mobile application is the " second screen " , and your TV is the primary screen . I 'd also like to emphasize that this is just a proof of concept the methods and code in this example are not yet suitable for a production-quality use case for reasons I 'll describe below , but are a great starting point for further exploration . <h> The Concept <p> Let 's start with the core concept : a synchronized experience between a content source ( TV or other ) and a mobile application . Since we are talking about TV or media-based content , you cant @ @ @ @ @ @ @ @ @ @ synchronization between the media source and the mobile app . This just would n't be possible due to legacy TV hardware , and the fact that there is no way to digitally synchronize the content . However , TVs are great at producing sounds , and it is very possible to use sound-based cues to invoke actions within a mobile application . <p> Now let 's focus on audio watermarks : Audio watermarks are markers embedded within an audio signal . They may be either human-imperceptible- or within the range of human hearing . In general , humans can hear frequencies between 20Hz and 20kHz , with that range decreasing with age . While we may not be able to hear the markers , mobile devices are able to detect them . When these markers are " heard " by your device , they can invoke an action within your application . <p> Next , Let 's take a look at my proof of concept application . The proof of concept exemplifies a mobile application themed with content from the HBO series Game of Thrones , - synchronized- with the opening scene from @ @ @ @ @ @ @ @ @ @ in the video , the content within the mobile applications is updated to show details about each location . In a nutshell : <p> Proof of Concept <p> In the video below , you can see the proof of concept in action . It shows the synchronization between the video and the PhoneGap-based application , with a brief description from yours truly . <h> The Application Implementation <p> There are several ways that you can do audio watermarks . The most basic of which is to embed a single tone in the audio stream , and check for the presence of that tone . The first thing that I started exploring was how to identify the dominant frequency of a sound . A quick Google search- yielded- an answer in the first result . This post not only describes how to detect the dominant sound frequency on iOS , but also has a downloadable project in github that you can use to get started . No exaggeration , I had this project up and running within minutes . It operates kind of like a guitar tuner the application detects the @ @ @ @ @ @ @ @ @ @ UI . <p> At a much later time , I also discovered this sample project from Apple , which also demonstrates how to detect frequencies from an audio stream ( used for the frequency waveform visualization ) . This will be useful for maturing the concepts shown here . <h> Creating Watermarks <p> Once I had the sample native iOS project up and running , I started exploring inaudible audio " tones " and testing what the devices could- accurately- detect . I initially started using tones above the 20kHz frequency range , so that humans would not be able to hear the watermark . Tones in the range of 20-22kHz worked great for the iPhone , but I quickly realized that the iPad microphone was incapable of detecting these watermarks , so I dropped down to the 18-20kHz range , which the iPad was able to pick up without any problems . Most adults wont be able to hear these frequencies , but small children may hear them , and they may drive your pets crazy . <p> The first thing I did was create " pure " audio @ @ @ @ @ @ @ @ @ @ , create a new waveform , then go to the " Effects " menu and select " Generate Tones " . From here , you can create audio tones at any frequency . Just specify your frequency and the tone duration , and hit " OK " . I used 3-second tones to make sure that the tone was long enough to be- perceived- by the device . <p> Generate Tones in Adobe Audition <p> I did this for tones in the range of 18-22kHz , and saved each in a separate wav file . Some of which you can find in the GitHub sample . These files were used for testing , and were embedded in the final video . <p> To embed the audio watermarks in the video , I fired up Adobe Premiere- and started adding the inaudible tones at specific points in time within the video . <p> Audio Tones in Adobe Premiere <p> By playing specific tones at specific times , you can synchronize events within your application to those specific tones . This means that you can reliably synchronize in-app content with video content @ @ @ @ @ @ @ @ @ @ of concept implementation . These watermarks worked great locally , but would n't work in a real-world solution as-is . I also ran into a few major issues when embedding the watermarks See the " lessons learned " below for details . <h> The PhoneGap Implementation <p> The next logical step was to take the native code example and turn it into a PhoneGap native plugin so that it can be used within a PhoneGap application . I stripped out the native user interface and exposed an API that would allow the PhoneGap/JavaScript content to register to listen for specific frequencies . If these frequencies are registered as the dominant frequency of a sound , the native plugin invokes the JavaScript callback JavaScript function that is mapped to that particular frequency . Using this approach , a unique JavaScript function can be assigned to each frequency . <p> The final step was to build a user interface using HTML , CSS , &amp; JavaScript that could respond to the audio watermarks . This was the easy part . First , I created a basic project that showed the reception and handling @ @ @ @ @ @ @ @ @ @ actual application content themed around the Game Of Thrones video . <p> Audio Watermark Enabled Applications <h> The Final Product <p> You can view the completed project , the sample tones , and the video containing the embedded tones on GitHub . <h> Lessons Learned <p> This was a really fun- experiment , and I definitely learned a lot while doing it . Below are just a few of my findings : <p> Dominant Frequency <p> Dominant Frequency watermarks are not the way to go in a real-world solution for many reasons . The main reason being that the watermark has to be the loudest and most predominant frequency in the captured audio spectrum . If there is a lot of other audio content , such as music , sound effects , talking , etc , then the watermark has to be louder than all of the other content , otherwise it will not be detectable . This alone is problematic . If you are normalizing or compressing your audio stream , this can cause even more problems . A multi-frequency watermark that is within the audible range , but @ @ @ @ @ @ @ @ @ @ High-Frequency Watermarks <p> High-frequency watermarks are also problematic . High-pitch frequencies may be beyond the capabilities of hardware devices . Speakers may have problems playing these frequencies , or microphones may have problems detecting these frequencies , as I discovered with the iPad . High-pitch frequencies also may have issues when encoding your media . Many compression formats/codecs will remove frequencies that are beyond human hearing , thus removing your watermarks . Without those watermarks , there can be no synchronization of content . <p> Time-Duration or Sequential Tones <p> The current implementation only detects for a dominant frequency , without a duration . If that frequency is encountered , it triggers the listening JavaScript function regardless of how long the sound was actually being played . All of my experimental tones lasted 3 seconds , so I could ensure it played long enough to be detected . However , I noticed that some of my- frequency- listeners would be triggered if I slid my mouse across the desk . While the action of moving my mouse across my desk was very brief and I could not hear it , @ @ @ @ @ @ @ @ @ @ detect . This triggered some of the frequencies that the app was listening for . If there was a minimum duration for the watermark frequency , this- erroneous- triggering of the event would not have occurred . You could also prevent misfires of audio watermarks by requiring specific series of tones in a sequence to trigger the action . <h> Media Production and Encoding <p> If you are using audio frequencies that are near the upper-range of human hearing , you have to be careful when you encode your media content . If the " inaudible " sound waveforms are over-amplified and are clipped , it has the potential to cause an extremely unpleasant high frequency noise that you can hear . I strongly- recommend- that you do not do this I learned this from experience . <p> Additionally , if you are using high-frequency tones , be careful if you transcode between 16 and 32 bit formats or if you transcode sample rates . Transcoding between 16/32 bit depth or between sample rates can cause the inaudible sounds to become audible with very unpleasant artifacts . I found that @ @ @ @ @ @ @ @ @ @ Premiere , the export format , and the source waveform all had the exact same bit depth ( 32 ) and sample rate ( 41000kHz ) . <h> Findings <p> From this exploration , some reading , and a lot of trial and error , I think it would be better to have a multi-frequency watermark for a minimum duration . Rather than having one specific frequency that dominates the audio sample , the application would detect for elevated levels of specific frequencies for a minimum period of time . This way the watermark frequencies do n't  have to overpower any other frequencies , and the watermark frequencies can be within the normal range of human hearing without being noticed . This also gives you the ability to have significantly more watermarks by using combinations of frequencies . Since the watermark tones would be within the normal range of human hearing , you also would be better able to rely on common hardware to be able to- accurately- detect those watermarks . <h> Conclusion <p> The main conclusion : not only is it really cool to control your mobile app @ @ @ @ @ @ @ @ @ @ for connected experiences . There are already TV shows and apps out in the real world employing the audio-watermarking technique to achieve a synchronized multi-screen experience . My guess is that you will start to see more of these experiences in the not-so-distant future . This is an inexpensive low-fi solution that has potential to work extremely well , and has applications far beyond just the synchronization of an app content with a TV show . <p> Targeted advertising : Imagine you are using an app while in a retail store &amp; you receive advertisements just be being in the store . The watermarks could be embedded within the music playing in the store . <p> Product placement : Imagine that you area watching a movie , and your favorite actor is drinking your favorite soda you look down at your device , and you also see an advertisement for that same brand of soda . <p> Museums : Imagine you have a mobile app for your favorite museum . While in the museum , there is an audio track describing the exhibits , or just playing background music . @ @ @ @ @ @ @ @ @ @ details about that exhibit , all triggered by the sound being played within the museum . <p> The applications of audio watermarking are only limited by our imaginations . This is a low-cost solution that could enable connected experiences pretty much everywhere that you go . The goal of this experiment was to see if these techniques are possible within PhoneGap apps , and yes , they are . <p> While PhoneGap is a multi-platform solution , you may have noticed that this proof of concept is iOS only . I 'm planning on developing this idea further on iOS , and if successful , I 'll considering porting it to other platforms . 123433 @qwx983433 <p> Thanks for sharing this project on the second screen experience . Was wondering whether the technology is matured , to trigger the second screen app , based on capturing the video source from the camera of the interested content ( high quality ) , and do some sort of image recognition with a baseline video source . Then it will be more original rather than ingesting a foreign audio element . <p> Just a Thought @ @ @ @ @ @ @ @ @ @ Youd have to fire up the camera and point your device at the video source though I wonder how many people would actually do that . The audio watermark technique is less intrusive to how someone interacts with the device/application . <p> http : //blendedmarket.com Nate <p> Really great info . I had a few ideas that you may have already thought up . What if you used the actual audio from the video as the watermark ? Couldnt you tell the application to listen for when the movie track audio plays a particular part ? For example , this audio is present ( the actor says " I 'll be back . " ) and then you call the javascript ? Basically use cue points in the sources audio itself seems like an easy solution . Perhaps its to complex of a signal ? <p> How much battery is your " listening " function using up ? Could you make a passive app out of this , ( one that runs persistently ? ) <p> Either way thank you so much this is a great process you have been working @ @ @ @ @ @ @ @ @ @ the apps that use audio watermarks thatve seen do n't  update in real time . I know the fingerprinting algorithms are pretty complex , and my assumption is that they are computationally expensive I do n't  know if it is even possible to have a non-intrusive audio fingerprint algorithm in a background thread , but I want to find out whenever I have the time . That would be an ideal solution to tie into existing media . I 've stumbled across some open source audio watermark algorithms here , which I plan to look at further : http : **36;446;TOOLONG The audio watermark approach is definitely easier to get up and running , and is simpler from a technical perspective , so I went that route first . <p> Regarding the battery , I do n't  have any hard metrics , but I have certainly noticed that it eats the battery much faster than when I 'm not running this app. iOS restricts your apps , and I do n't  think they allow a persistent app , but I think it could be done on Android w/o much effort . <p> @ @ @ @ @ @ @ @ @ @ a bit last year , attempting to use Morse Code at high frequency in the home to control a tablet app . That eventually became OpenMic and the very low cost RTMFP:http : //vimeo.com/24023295 <p> For my project , the catch was that broadcast is almost never " live " , within the UK there could be a difference of 17 minutes lag time between the studio and a person at home depending on their region . <p> So for my purposes it would n't do the trick , but it was certainly a lot of fun trying ! 123436 @qwx983436 <p> Very cool ! Thanks for sharing Mark ! <p> Don Holloway <p> Fantastic post , thanks ! This is brilliant , I will share my progress trying to get some spread spectrum type of encode / decode thing going . <p> Tahir Alvi <p> Nice example &amp; Thanks for code . <p> http : //www.appliness.com Michael Chaize <p> Nice trick ! <p> Here is another project that used this trick . I 'm sure you 'll love the content . They use sound fingerprints to synchronize Youtube videos and an iPhone @ @ @ @ @ @ @ @ @ @ Game of Thrones in this article . I have recently , for the very first time , started to hear an audio watermarking on this show that I 've been hearing on other networks , most notably CBS . And now I 'm noticing that HBO has also added this audio watermarking to other shows and programs as well . Its supposed to be inaudible , but for people that can hear it , its extremely annoying , to the point where I wont watch shows containing it . There 's a lot of information , and even some videos that contain the sound , which sounds like an old cell phone going off every 4.5 seconds , a sequence of 6 or 7 tones in rapid succession , at this URL : http : **39;511;TOOLONG . Once you start hearing it , its impossible not to hear it , and drives you crazy . 123436 @qwx983436 <p> Hah , Thankfully , I have not noticed it in any shows I watch . Horray for loud music destroying my hearing ! <p> Hugh McManigal <p> Andrew , For quite some time we have @ @ @ @ @ @ @ @ @ @ behold I find your work . Have there been any developments in the past few months on this front . Are there any apps you know of that are utilizing this method in the ways you addressed and that you find impressive ? Thank you 123436 @qwx983436 <p> I have n't spent much time on this since my initial prototype . However , I know there are apps in the wild that use this technique . Many primetime TV shows have watermarked content , for example : http : //www.intonow.com/ci <p> Pascal R <p> Thank you for sharing your research . I would like to know more about the implementation of multi-frequency watermark since i 'm not an expert of sound . Can you refer me literature on the subject ? Do you have a sound sample using this technique ? <p> Christian Aar++ Rasmussen <p> Hi Andrew , I 've recently begun looking into 2nd screen technology for a personal project and during my research I stumbled across your work . First of all , thank you for sharing your research and source code . Its been a huge help for some 
@@106848691 @2248691/ <p> The first is- app versioning ; MobileFirst Foundation- tracks each version of an app that you deploy , and gives you the ability- govern or restrict access to specific platforms and versions. - App versioning applies to all apps , native or hybrid , on any platform that MobileFirst Foundation- supports . The second is Direct Update , which allows you to push new HTML/CSS/JavaScript ( web ) resources to a MobileFirst- hybrid app . Direct Update only applies to hybrid apps , but it works for- any platform that MobileFirst- supports . <h> App Version Management <p> When you deploy an app to the MobileFirst Foundation server , the server- will automatically track versions based on the version number specified in you **27;552;TOOLONG file . <p> Set Application Version <p> When you load the MobileFirst Foundation Server Console , you 'll be able to view all of the deployed app platforms and versions . <p> The screenshot below shows a hybrid app deployed for both Android and iOS- platforms . You would also be able to see the exact same version and platform information for native apps that @ @ @ @ @ @ @ @ @ @ MobileFirst Console ( click to enlarge ) <p> Youll notice in the MobileFirst console that next to each platform/version- you can set the status for that version . This makes it possible to set notification messages for- users on specific platforms and versions , or even restrict access to specific platforms and versions . <p> For example , look at the screenshot above Version 1.0 on Android is active . Version 1.2 on iOS is active . Version 1.1 on iOS is notifying , and Version 1.0 on iOS is disabled . <p> There are 3 statuses that can be set for each platform and version combination. : Active , Active Notifying , and Access Disabled . <p> Set Platform/Version Status <p> When you set the status of a platform/version , this status is only for that specific platform/version pair . This enables you to selectively notify users of specific versions , or even block access to specific versions if they are outdated and no longer supported . <p> " Active " means that the application is active . Services to this version will operate normally , and no messages @ @ @ @ @ @ @ @ @ @ Notifying " means that the application is active , services will continue to work , but a message will be presented to the user when the app becomes active , or when a service request is made to the MF server . <p> Setting Active Notification Message <p> This can be used to send any text-based message to the app users . This could be a deprecation notice , service maintenance notice , or any other general notice . <p> Within the app , the user will see a message when the app becomes active , or when a request is made to the server . This message can be dismissed , and the app functionality is not impacted in any way . <p> In-App Active Notification Experience <p> " Access- Disabled " means that access to the application is disabled . In this state , a notification message will be presented to the user , and access from the app version will be disabled . The user will also be presented with an " Upgrade " button , which will redirect the user to any URL , which presumably @ @ @ @ @ @ @ @ @ @ <p> Setting Disabled Status <p> In this state , the app will not be granted access to the MobileFirst/Worklight server . So , if your app requests- data from a data adapter , all requests to the adapter from this platform/version will be blocked . If- your app initialization code is inside of the Worklight clients connect:onSuccess handler , then this can- prevent your app from loading at all . <p> In-App Disabled Experience <p> Again , When you set the status of a platform/version , this status is only for that specific platform/version pair . <p> Direct Update is a feature for MobileFirst hybrid apps , which enables- you to push updated app- content ( HTML , CSS , &amp; JavaScript ) without the user having to deploy a new version of the app through the app store . <p> Direct Update is considered an additional security feature b/c it enforces users to use the latest version of the application code. - However , when an app uses Direct Update , it *only* updates the web resources. - No native changes or version # changes will be applied . @ @ @ @ @ @ @ @ @ @ this will bypass the- Apples app store approval process. - You should not overhaul the entire UI and break Apples- Human Interaction Guidelines , otherwise you could be kicked out of the app store . <p> Direct Update User Experience <p> By default , the updates user experience is a modal overlay that shows download and installation progress . The updaters UX can be configured to use silent updates that do not block the users experience , can be a completely custom user experience , or can be disabled altogether. - Updates can also be paused or resumed using the JavaScript API so that it does not block the user from performing a critical task , however this would require a custom UI the default UI does not enable pause/resume . <p> Updates in the current version of Worklight ( 6.2 ) are complete updates containing the entire application ( www ) code , however- MobileFirst Foundation- 6.3 ( coming this month ) will have a Differential Direct Update feature- that includes- only- the changed files . More detail will be posted once this is available . <p> Direct Update @ @ @ @ @ @ @ @ @ @ hybrid apps to update automatically . <p> For more information on Direct Update , be sure to check out these additional resources : 
@@106848693 @2248693/ <p> This is more than just " Cloud Services " which more generally refer to a scalable virtual cluster- of computing or storage resources . - Bluemix is IBMs suite of cloud service offerings , and covers lots of use cases : <p> Bluemix is an open-standards , cloud-based platform for building , managing , and running apps of all types , such as web , mobile , big data , and smart devices . Capabilities include Java , mobile back-end development , and application monitoring , as well as features from ecosystem partners and open source " all provided as-a-service in the cloud . <p> Why is it a hot topic ? - MBaaS- enables growth of mobile applications- with seamless ( and virtually endless ) scalability , all without having to manage individual systems for the application server , database , identify management , push notifications , or platform-specific services . <p> Ive been writing a lot about IBM MobileFirst lately for a seamless API to deliver mobile apps to multiple platforms ; though it has been- in the context of an on-premise installation . - However @ @ @ @ @ @ @ @ @ @ MobileFirst features are available as- MBaaS services on IBM Bluemix ? <p> Mobile Data The mobile data service includes- a- NOSQL database ( powered by IBM Cloudant ) , file storage- capabilities , and appropriate management and analytics features to measure the number of calls , storage usage , time/activity , and OS distribution . <p> Push Notifications The push notification service allows you to easily push data to the right people at the right time on- either Apple APNS or Google GCM platforms all with a single API. - Notifications can be sent by either an app or backend system , and can be sent to a single device , or a group of devices based on their tags/subscriptions. - Of course , with appropriate analytics for monitoring activity , distribution , and engagement . <p> Many of these are the exact same features that you can host in your own on-premise IBM MobileFirst Platform Foundation server the difference is that you do n't  have to maintain the infrastructure . - You can scale as needed through the Bluemix cloud offering . 
@@106848694 @2248694/ <p> The IBM WatsonG Question Answer ( QA ) service provides an API that give you the power of the IBM Watson cognitive computing system . With this service , you can connect to Watson , pose questions in natural language , and receive responses that you can use within your application . <p> Just click on the microphone button , allow access to the system mic , and start talking . - Just a warning , lots of background noise might interfere with the APIs ability to recognize &amp; generate a meaningful transcript . <p> This demo uses the Watson For Healthcare data set , which contains information- from HealthFinder.gov , the CDC , National Hear Lung , and Blood Institute , - National Institute of Arthritis and Musculoskeletal and Skin Diseases , National Institute of Diabetes and Digestive and Kidney Diseases , National Institute of Neurological Disorders and Stroke , and Cancer.gov. - Just know that this is a beta service/data set implementing Watson for your own enterprise- solutions requires system training and algorithm development for Watson to be able to understand your data . <p> Using @ @ @ @ @ @ @ @ @ @ , like : <p> What is X ? <p> What causes X ? <p> What is the treatment for X ? <p> What are the symptoms of X ? <p> Am I at risk of X ? <p> Procedure questions , like : <p> What should I expect before X ? <p> What should I expect after X ? <p> General health auestions , like : <p> What are the benefits of taking aspirin daily ? <p> Why do I need to get shots ? <p> How do I know if I have food poisoning ? <p> Or , action-related questions , like : <p> How can I quit smoking ? <p> What should I do if my child is obese ? <p> What can I do to get more calcium ? <p> Watson services are exposed through a RESTful API , and can easily be integrated into an existing application . - For example , here 's a snippet demonstrating how- you can consume the Watson QA service inside of a Node.js app : <p> Hooking into the Web Speech API is just as easy ( assuming you 're using a @ @ @ @ @ @ @ @ @ @ demo using Chrome on OS X ) . On the client side , you just need need to create a SpeechRecognition instance , and add the appropriate event handlers . <p> To make your app talk back to you ( synthesize speech ) , you just need to- create a new SpeechSynthesisUtterance object , and pass it into the **30;581;TOOLONG function . You can add event listeners to handle speech events , if needed . 
@@106848696 @2248696/ <h> My Workflow for Developing PhoneGap Applications <p> I am asked all the time " How do I get started developing PhoneGap applications ? " . My normal answer is to advise people to check out the PhoneGap Getting Started Guides , which provide a great starting point for every platform . However after further thought , I 'm not sure this is always what people are asking . Rather than " how do I get started ? " , I think people are often looking for insight into the workflow for developing PhoneGap applications . Everything from tools to developer flow , to getting the app on devices . The Getting Started Guides are essential for setting up the initial project structure , but once you get that setup , you might be wondering " what do I do next ? " . In this post , I 'll try to she 'd some light on the workflow and tools that I use when developing PhoneGap applications . <h> Know What You 're Going To Build Before You Build It <p> First and foremost it is essential to have at least some @ @ @ @ @ @ @ @ @ @ you build it . If you just start hacking things together without a plan , the final result is seldomly great . Complete ( pixel perfect ) UI/UX mockups are fantastic , but you do n't  have to have a fully polished design and screen flow . Just having wireframes/sketches are a great start . Heck , even a sketch on a napkin is better than starting with nothing . <p> The UX design/wireframes help you understand what you application should be doing from the users perspective , which in turn helps you make decisions on how you tackle a project . This can be purely from a HTML level , helping you figure out how you should position DOM elements and/or content . Or , it can help you gauge your projects technical complexity How many " moving parts " do you have , how much of the app is dynamic or asynchronus , or how do different visual elements need to work together ? You can leverage this design/mockup to analyze the needs of your application and determine if a particular framework/development methodology is a best fit ( @ @ @ @ @ @ @ @ @ @ , Angular.js , etc ) . <p> When working with a designer , I use Adobes Creative Suite Tools for pretty much everything wireframes , UI/UX designs , chopping up assets , etc I 'm currently working on a project that was designed by the talented- Joni from Adobe XD . Joni designed everything in Creative Suite , and I 'm using Photoshop to view screen flows and extract UI assets for the actual implementation . <p> UI Mockups in PhotoshopScreen Flow in Photoshop <p> Note : This app will also be free and open source as a sample/learning resource for PhoneGap , including all of the design assets I 'll follow up with another post on this later , once the app is- available- in the app stores . <p> If you are n't  a " graphics person " , or do n't  have creative suite , there are a bunch of other tools that you can use for wireframing and/or sketching ( but cmon , Creative Cloud is only $50 a month ) . Here are several Ive used with great success , but this is not a comprehensive list at @ @ @ @ @ @ @ @ @ @ for OS X. This is fantastic for wireframing or documenting screen flows . In fact , the screen flow image shown in Photoshop above was originally composed in Omnigraffle , using the mockups created in Photoshop . <p> Visio- A powerful drag &amp; drop wireframing/design tool for Windows much like OmniGraffle , but for windows . <p> PowerPoint- or Keynote- - These are n't  just for presentations . They can be really useful for putting together screen flow diagrams , or annotating images/content . <p> Often people like to sketch out ideas &amp; wireframes on their tablets , here are a few tools that I use for that : <p> iBrainstorm A great app for collaboratively taking notes and sketching . I 'm partial to this one b/c used to be on the dev team , and I wrote a good chunk of the graphics sketching logic . <p> There are a bunch of other tablet sketching apps out there , but I have n't used most of them . <h> Coding Environment <p> Coding environments are a tricky subject . There is no single solution that meets the exact wants and @ @ @ @ @ @ @ @ @ @ , some people chose large-scale IDEs , some people use designer-centric tools , and many of these choices are- dependant- upon which operating system you use or your background as a designer or developer . Since PhoneGap applications are really just editing HTML , CSS &amp; JavaScript , you can use whatever editor you want . In fact , I know a several people that use vim as their primary editor . <h> Large-Scale IDEs <p> I 'm a bigger fan of of using a complete IDE ( integrated development environment ) than I am of a lightweight editor , simply b/c IDEs tend to have hooks into more features/languages , etc I know people complain about startup time , but there is no startup time if you leave it open all the time . <p> There are a few catches when talking about IDEs with PhoneGap . The first is that if you want to deploy anything locally to devices ( without using PhoneGap Build ) , you have to delpoy using the IDE for the particular platform that you are- targeting . That means Xcode for iOS , Eclipse @ @ @ @ @ @ @ @ @ @ etc However if you wish , you can use your editor of choice , and just use the IDE to deploy to devices locally . You can even share source code across several IDE installations using symlinks ( which I describe here ) . I very often use this type of a configuration to share code between Xcode , Eclipse , and WebStorm . <p> My preference for coding PhoneGap applications is to use- WebStorm by JetBrains . WebStorm has great code-hinting ( even for your own custom JS and 3rd party libraries ) , great refactoring , hooks into Git , CVS , or SVN repositories , and is a very mature IDE . <p> WebStorm IDE <p> I tend to use this as my primary coding tool , then switch to Eclipse or Xcode when I want to locally deploy to a device for testing . When using PhoneGap Build to simplify cross-platform compilation , I just push the code to git , then recompile via PhoneGap Build . <p> I 'm not a fan of Xcodes HTML/JS editing , and havent found an HTML/JS plugin for Eclipse that @ @ @ @ @ @ @ @ @ @ use Visual Studio . <h> Lightweight Editors <p> I 'm a bigger fan of larger IDEs than lightweight editors , but- Adobe Edge Code ( also known as Brackets ) is a great lightweight editor for quick edits. - Edge Code/Brackets is an open source HTML/JS editor that supports live editing in the browser and inline editors for CSS styles , without leaving your HTML files . If you tried Edge Code Preview 1 , but werent sold on it , you should try Edge Code Preview 2 . The team has come a long way very quickly . Its fast , easy to use , and there is a- plugin to tie it into PhoneGap Build . I sometimes use this for quick edits . <p> There are tons of other lightweight editors out there , and everyone has their favorite . As long as you 're happy with the tool , and it can edit text ( HTML , CSS , JS ) files , you can use it to build PhoneGap applications . <h> Designer-Friendly Editors <p> I 'm not necessarily the primary target for Dreamweaver , but it has @ @ @ @ @ @ @ @ @ @ environment plus a WYSIWYG editor for HTML experiences . It also features PhoneGap Build integration directly inside the coding environment . If you 're used to Dreamweaver for creating web experiences , you can continue to use it and target mobile apps as well . <p> Adobe Dreamweaver <h> Debugging Environments <p> Yes , that is plural Debugging Environments . Due to the cross-platform nature and PhoneGaps leveraging of native web views for each platform , debugging PhoneGap applications can sometimes be tricky . Here are some tips that will make this significantly easier . <h> The PhoneGap Emulator <p> The PhoneGap Emulator is my primary development/debugging tool for all PhoneGap apps . It is a browser-based emulator leveraging the Google Chrome browser and the Ripple Emulation Environment . The PhoneGap Emulator runs inside of Google Chrome , and provides emulation of PhoneGaps core APIs . Since it is built on top of Chrome , it enables you to leverage Chromes Developer Tools , which in my opinion are second to none for web/application development . This is a highly-productive developer environment . <p> PhoneGap Emulator in Google Chrome <p> here 's @ @ @ @ @ @ @ @ @ @ First , - this combination enables you to emulate most core PhoneGap APIs without leaving the desktop environment . It enables you to test various APIs including geolocation ( with simulated locations ) , device events ( deviceready , back , etc ) , sensor events ( accelerometer , compass ) , and even let 's you test with different device aspect ratios all without having to push anything to an actual device . This saves a lot of time in development iterations . You can read about the supported Ripple emulator features here . <p> Second , Chromes Developer Tools are awesome . Here are just a few things that you can do while developing/debugging your app , live within the emulation environment : <p> Analyze all resources consumed by your app , via the resources panel . This includes all scripts , images , html files , cookies , etc it even includes insight into any local data stored via PhoneGaps local storage database ( WebSQL implementation ) . <p> View/query all local databases within your app . You can write your own queries to view/alter data in the @ @ @ @ @ @ @ @ @ @ its not immediately intuitive . <p> Debug JavaScript with the Scripts/Sources Panel . You can set breakpoints in JS execution , inspect &amp; alter values in JS objects in-memory , and view details and line numbers for any exceptions that occur . <p> Use the console to monitor console.log() statements , inspect properties of objects in memory , or execute arbitrary JavaScript whenever you want . <p> The PhoneGap Emulator enables developers to be extremely productive with development , however I can not emphasize enough that on-device testing is critical for having a successful app . On-device testing can expose performance problems or browser rendering variances that you may not notice in the emulator environment . <h> On-Device Remote Debugging <p> As I mentioned above , on-device testing is critical for successful applications . iOS and BlackBerry have an advantage over other platforms b/c the latest developer tools allow you to remotely debug content live on a device . <p> Since the release of iOS 6 , you can debug content in the iOS simulator using Safaris Developer Tools . Safaris developer tools give you many of the same debugging @ @ @ @ @ @ @ @ @ @ Debugging with Weinre <p> Not every platform supports live remote debugging , especially older versions . Weinre ( pronounced winery ) is a remote web inspector that allows you to inspect/edit DOM and CSS elements on remote devices . Basically , you include some JavaScript in your app , and it communicates back to a server that will tell you what 's happening inside of the app running on the mobile device . It wo n't give you full debugging capabilities like JS breakpoints and memory inspection , but its better than nothing . You can use Weinre by setting up your own instance , or by leveraging debug.phonegap.com . <p> Weinre for On-Device Debugging <h> When All Else Fails <p> If you 're still debugging your apps , and the solutions mentioned above do n't  work , you can always resort to plain-old " alert() " statements to pop up debug messages , or use " console.log() " statements to write to system logs . <p> On Android , all- console.log ( ' ... ' ) ; - messages will appear as printouts in the command-line tool- logcat , which is bundled @ @ @ @ @ @ @ @ @ @ plugin . <p> On BlackBerry , all- console.log ( ' ... ' ) ; - are printed to the BlackBerrys Event Log . The Event Log can be accessed by pressing- ALT + LGLG . <p> On iOS , all- console.log ( ' ... ' ) ; - are output to the Xcode Debug Area console . <h> Building PhoneGap Apps <p> The PhoneGap getting started guides will point you to the right direction for getting started with a particular platform . If you are just targeting iOS , you can use Xcode for building . If you are just targeting Android , you can use Eclipse , etc It is all very easy to get up and running . <p> However , this process gets much more complicated when targeting multiple platforms at once . When I have to do this , PhoneGap Build becomes really , really handy . <p> PhoneGap Build allows you to either upload your code , or point to a Git repository . PhoneGap Build will then pull your code and build for 7 different platforms , without you having to do anything special @ @ @ @ @ @ @ @ @ @ do is install the cloud-compiled binaries on your device . You can do this by copying/pasting a URL to the binaries , or by capturing a QR code that will directly link to the compiled- application- binary . <p> One other advantage of PhoneGap build is that it let 's designers/developers build mobile applications without having to install any developer tools . If you want to compile a PhoneGap app for iOS , but are on Windows just use PhoneGap build and you wo n't need Xcode or a Mac . <h> PhoneGap UI/Development Frameworks <p> Probably the most common PhoneGap question that I get asked is " what MVC/development framework should I use ? " . If you 've been waiting for me to answer this , do n't  hold your breath . It is impossible to be prescriptive and say that one solution fits all use cases for every- developer . <p> When people ask me this , I like to paraphrase- Brian Leroux from the PhoneGap team : " Use HTML , it works really well . " <p> I think people often overlook the fact that PhoneGaps UI renderer @ @ @ @ @ @ @ @ @ @ HTML/CSS content can be rendered as your applications user interface . This could be something incredibly simple , like text on the screen , or it could be incredibly creative or complex . The important factor is that you need to focus on a quality user experience . If you 're worried about your UX , and are worried that Apple may reject your app , then read this article where I explain Apple rejections in detail . <p> HTML/JS developers come from many different backgrounds , with varying degrees of programming expertise . Some frameworks appeal to some people , other frameworks appeal to other people . There also seem to be new UI &amp; architectural frameworks popping up every week . It would be a disservice to all people who use PhoneGap for us to proclaim that we should only use one singe framework . <p> There are lots , and lots , and lots more options out in the HTML/JS development world I 'm not even taking into account JavaScript generating tools and languages like CoffeeScript , TypeScript , or others 123433 @qwx983433 <p> Wow , great detailed post @ @ @ @ @ @ @ @ @ @ developers go through when setting up their dev environment ( and testing/debugging ) . Personally , I am a fan of lightweight editors but there 's so really good stuff in here that I 'm certainly going to trial . <p> Let 's see how long it is before someone comments to suggest VIM <p> Lisa <p> You 're not the first to give a thumbs up for Adobe 's Creative Suite Tools and you certainly wont be the last . Thanks for sharing your workflow Andrew . <p> Anhsirk Dasarp <p> Good One ! Kudos ! ! <p> http : //www.cynergy.com michael wolf <p> i 'd add to the above that while the phone gap emulator is sweet , also remember that your plugins wo n't run in browser , as while the emulator will mock in interactions w/ cordova , you should come up w/ a process of mocking in your plugins. 123436 @qwx983436 <p> Thanks Mike , that 's a good point . The emulator will not emulate the behavior of native plugins . You can specify a return/callback value for PhoneGap.exec methods which the emulator does n't  know how to handle , but @ @ @ @ @ @ @ @ @ @ behavior of those plugins . <p> http : //www.asyncdev.net Romain <p> I was about to blog about how Ive successfully used event routing to decouple plugins from the rest of the codebase . <p> It makes it quite easy to have two implementations , for example a real in-app purchase plugin ( that talks to the App Store ) , and a fake one using a confirm dialog box ( " was the purchase successful ? " ) . Then the build simply includes the right JS file at build time ( browser build ? ios build ? ) and the event router does the rest . <p> Are you willing to share an example of one of your finished PhoneGap-based apps using this workflow , so folks can see a finished product ? <p> The example apps posted on PhoneGaps site are a little janky , and I wonder whether that 's a result of how those apps were built , or an indication of the softwares limitations . Thanks ! 123436 @qwx983436 <p> Hi Jim , here are a few that Ive developed &amp; released as open source . @ @ @ @ @ @ @ @ @ @ then again , I have never considered myself an amazing designer . However , the same workflow has applied . Its important to keep in mind that the UI rendering layer in PhoneGap applications is a system web view from the underlying operating system . On the latest and greatest devices , the web view is very performant , however on older devices web views can be slower . Your UI is limited to what can be done within the web view , and different platforms have different runtime performance for HTML-based content . With that said , there are lots of very good PhoneGap or HTML-based apps out there , and you can create very highly customized UIs with HTML , very easily . However , you must keep mobile performance in mind from the very beginning . A bit of over-generalization , but I would not consider your experience with existing apps a limitation of the software , rather the approach to how the apps were built . <p> However , a rant about PhoneGap is due : the Getting Started guide is pathetic . There should be @ @ @ @ @ @ @ @ @ @ a new project or a small app that does that . <p> http : //n2d.co Richard Bennett <p> Good article . Been developing in PhoneGap for the better part of a year myself . I 've found too many incompatibilities with JS-based frameworks to recommend any I 've always ended up re-writing everything myself in the end . <p> A question I have though is about touch event libraries . Any suggestions regarding those ? <p> I used to use QUOjs , but the site for it has since been taken down , and I 'm now looking at Hammer.js . <p> Thoughts ? 123436 @qwx983436 <p> Pretty much any JS framework will work with PhoneGap b/c its just a web view . For touch events , I generally write my own handlers , however hammer.js looks like it can make handling multiple touch events easier . If you are just trying to detect tap ( vs click ) , Zepto.js has a " tap " handler that seems to work well . <p> pavunkumar <p> Your post was really useful ! ! ! i am just having one query . Hi I @ @ @ @ @ @ @ @ @ @ Now I have selected phonegap for doing the same for cross platform devices like Anroid , BlackBerry , iOS , Windows and etc . Actually I have used following things in my j2me app . Storage , Cache , Https Communications . I want to know whether above all feature could be implemented in phonegap environment without any serious conflict and especially with regard to security related things . . 123436 @qwx983436 <p> Yes , all of these can be implemented inside of a PhoneGap application without any implications . PhoneGap leverages the system web view for each platform supported , so it can leverage any HTML spec/api/feature that is supported by the operating system . <p> Hi great post . We are soon to begin developing mobile apps using Phonegap , but are deciding on what actual physical devices we need to buy for testing . What do you currently use ? You mention above that performance can vary , and can be especially a problem on older devices , so from that point of view would you suggest buying an iphone 4 and 5 to test on ? @ @ @ @ @ @ @ @ @ @ guess that maybe you can use this instead of an iphone 4 to keep costs down ? For android I assume you would have a device for 2.3 and 4 , and perhaps different screen sizes ? And then there are the tablets ! Do you test on these ? No point testing on an ipad mini separately I guess ( same as ipad 2 right ) , but for android I assume we would need to test on 7 ? and 10 ? . Arghhhh , what does everyone else do in these circumstances ? 123436 @qwx983436 <p> I use older hardware , running the most prevalent operating systems . For iOS , I have an iPhone 4 ( not 4s ) , and an iPad 2 , both on iOS 6 . For Android , I have a Motorola Atrix running 2.3 , a Nexus 7 tablet running 4.2 , a Samsung Galaxy Tab running 3.0 , and a Kindle Fire running whatever version amazon uses . Only the Nexus 7 is the " latest and greatest " flavor of Android . My general philosophy is that if @ @ @ @ @ @ @ @ @ @ it will be even faster on newer hardware . However , I always test on the latest hardare whenever I can get my hands on it , just to make sure it runs as expected . I 've even gone into the retail stores and installed my app on brand new devices , just to see how it runs . <p> Robin <p> Im surprised you left out jqMobi from your list of UI frameworks . I have tried Sencha Touch , JQuery Mobile and Dojo Mobile . jqMobis performance is streaks ahead of these . A lot of the bad rap that PhoneGap gets is due to a bad or underperforming JS framework . JQuery Mobile is horrible when using transitions or even when it comes to general performance like scrolling etc . <p> I almost went native after my frustration with JQuery Mobile drove me to the edge of sanity . <p> So if anyone wants to start off on the right foot , jqMobi is the way to go . Their UI is ugly , but very easy to change to any style you want . They should @ @ @ @ @ @ @ @ @ @ groups ; Core UI functionality styles and visual styles that can dropped completely without breaking the framework . <p> Jeremy Colton <p> Hi Robin , <p> I am having trouble using jqmobi with Phonegap . Can you please describe your exact setup ? <p> Although that 's true , it does not help a bit in Phonegap workflow because the system browser used in WebView is not Chrome , to the date of this comment . Should Android change it to Chrome , 80% of all the issues I 've come across in phonegap would solve itself magically . <p> What do you do about design differences between the OSs . For instance the common upper left back button for Apple and the left/right swiping of Android . I 'm guessing you 'd need to set styles for each OS if you wanted to comply with each design principle ? Plus how do you handle things like when you hit the end of the page , with Apple bouncing and android showing a light . I have n't tried phonegap but have really wanted to give it a go . Was curious what people did @ @ @ @ @ @ @ @ @ @ just build one style and just throw out the design principles that make each OS unique . <p> How about a support on mobile payment gateway with this phonegap ? Does it supports all the credit card and paypal implementation or i need to use a third party sdk for it . 123436 @qwx983436 <p> PhoneGap by itself does not support a mobile payment gateway since they are different per platform . However , you can leverage native plugins to hook into in app purchasing or other third party SDKs for payment processing . <p> http : //www.onegeek.com.au Matt <p> Hi Andrew , <p> Thanks for the great write-up , this is one of the better round-ups out there . I 'm new to all of this , so finding the emulator was a God-send . <p> That being said , I 'm still finding the development-test cycle quite a challenge at the moment though , as I 'm writing an App that needs to capture Camera input , and it does n't  appear as though the Ripple Emulator can emulate the Camera . ( Using the Chrome console inspecting the navigator object @ @ @ @ @ @ @ @ @ @ thoughts on how to tackle this would be greatly appreciated ( currently trying to build Ripple from source as it appears to have mentions of camera APIs in it ) ? 123436 @qwx983436 <p> Hi Matt , You should be able to use **27;613;TOOLONG inside of the Ripple emulator ( I am able to use it in my test projects ) . You can also use on-device debugging if you are using iOS , which can be really helpful . <p> meme <p> hey Andrew how can i get the executable file of phonegap project using just command line waiting your reply <p> http : //omnispear.com Jared Armstrong <p> You want to look into Cordova cli which requires node.jd for command line interaction . You can then launch x code to load and run it in your actual phone . <p> Thanks for this great post . <p> George <p> Can we use Ripple Emulator for Phongap plugin based app development . I have tried the https : **38;642;TOOLONG project . But in the Ripple Emulator its giving me missing **33;682;TOOLONG error . 123436 @qwx983436 <p> Ripple will not allow @ @ @ @ @ @ @ @ @ @ call an API that it does not support , it will give you an error message , and will allow you to specify a response value . However , it does not mimic the actual native implementation . <p> Antonio Bray <p> Andrew , great post . Did you release the workflow above for the mockup including the finished phonegap source ? <p> Elvis Almenar <p> Only I want to sayWHAT A GREAT POST <p> Megalomaniacs4u <p> What Getting Started guides in the phonegap docs ? there are NO getting starting guides worthy of the name on the phonegap site . Your excellent article is the closest so far to being one . <p> Visva lingam <p> Superb Article really helps Thanks buddy . <p> Mat+as Stanislavsky <p> What a great post , thank you for showing to the rest of us a little bit of enlightment , <p> Regards from Argentina , <p> Mat+as ! <p> Ankur <p> Thanks . The article was really fantastic . <p> However , I am using AngularJS for application development and contains ng-view which loads pages dynamically based upon AngularJS routing . It @ @ @ @ @ @ @ @ @ @ Android device . Can you guide me on that ? It is actually driving me crazy . <p> Marc Sch <p> I think your architectural patterns should include YUI <p> Andrea Bonapersona <p> Great post ! I 'd add just a few things for debugging : <p> Adobe Edge Inspect ( formerly Adobe Shadow ) should let you inspect a phonegap application from desktop browser . ( Couldnt get it to work , does n't  work on chrome 30+ , doc is poor about phonegap and talks only about browser debugging ) <p> JSHybugger let 's you inspect a phonegap app from your chrome browser . Only android and not free ( 10 days trial ) , but worked for me easy to setup . <p> Andrea Bonapersona <p> You can still take a look into scaffolding with yeoman ? There should be basic phonegap scaffolding <p> ruan <p> You should probably also check out http : **26;717;TOOLONG it has a designer and very functional code editor built-in , and has a mobile build process included as well so you do n't  have to upload to PG Build . Most notably , @ @ @ @ @ @ @ @ @ @ 10MB limit PG Build imposes . <p> OJayBee <p> I found this really handy along with the getting started guides in phonegap : <p> Looking for something like this for a long time . My concern was the hassle of updating project to committing changes to Github , then upgrading the the code at build , then scanning the barcode and then installing and testing the app . Then Brackets and its integration to the Phonegap build came like a blessing . I followed the article LONG ... on how to integrate the brackets editor to phonegap build . <p> Everything goes fine and I can see my app , on the build , here in the Brackets . But none of the options in the Phonegap menu item works for me . <p> I am posting the snapshots . Plz help . <p> Julian GOnzalez <p> Thank you very much Andrew ! <p> http : **29;745;TOOLONG Normski <p> Excellent article , I 've been toying with using PhoneGap for sometime but now finally taking the plunge after reading your article . Thanks . <p> Ezequiel <p> Hi , Andrew ! @ @ @ @ @ @ @ @ @ @ is fantastic . <p> I decided to take a look at backbone.js from your link , but it seems to be broken . <p> hello , firstly i must say it is really a very well written and highly descriptive article . This post is useful for the cross platform developers . Any developer can refer this post to plan out their work strategy it will really give them benefit for sure . <p> If I 'm developing the PhoneGap application using CLI with nodejs &amp; sublimetext , do I still need Eclipse ? in other words only the Android SDK is enough right ? <p> http : **29;776;TOOLONG Flak DiNenno <p> That 's correct . Eclipse is just an IDE you could use instead of Sublime . And if you were building a native app in Java , Eclipse comes with everything you need in a Java IDE configured by default ( although you could also configure Eclipse to work without Java and just PHP for instance ) . Node.js provides the framework and utilities for converting your web app ( html , js , css ) to a phonegap-android stub @ @ @ @ @ @ @ @ @ @ to an ADK . <p> jackbrucesimpson <p> Did you end up releasing the educational app for PhoneGap by any chance ? <p> http : //www.benwagner.net Yankees9920 <p> How do you set up the emulator . Do you just point it to the base www folder where you develop , or do you have to run " cordova prepare " and point it to the **27;807;TOOLONG folder ? Do you just run a webserver pointed to the www directory ? <p> When trying to point to the base www folder it seems to be unable to find cordova.js . <p> Michelle Diamond <p> Good article . Thanks . I want to ask if it is possible to build a site in muse just for the basic framework and main visuals and then work into it using foundation ? Developer will more than likely be using angular for the functionality . <p> ArunJayapal <p> I want to knowam I really tied to " PhoneGap Build " in order to create my native binaries for multiple platforms ? I do n't  want to rely on another service to get this donewhat are my @ @ @ @ @ @ @ @ @ @ <p> Great post . <p> Jeremy <p> What about ReactJS baby ! 123435 @qwx983435 <p> Look at the date on this post . I wrote it before react existed ! <p> Greg <p> ANDREW : " Note : This app will also be free and open source as a sample/learning resource for PhoneGap , including all of the design assets " I 'll follow up with another post on this later , once the app is available in the app stores . " <p> Do you have a link to this yet ? I have followed up looking for the sample code/assets to be posted so I can better learn how this is all put together <p> Saroj Naagar <p> One of my friends working in Root Info Solutions , a uk based company as Sr. PhoneGap App Developers . He show me their some apps Developed using PhoneGap frameworks . Apps are really awesome . If somebody want to build these kinds Awesome apps can visit here . LONG ... 
@@106848698 @2248698/ <h> Monthly Archives : April 2014 <p> Every year the town I live in has a weekend-long spring festival . There are rides for the kids , live music , beer , and lots of food . This year I have a great view overlooking the carnival area , so I decided to do a time-lapse video capturing all of the activity . The trucks pulled in before I got to the office on Thursday morning , but I managed to capture most of the set up , all the way until the trucks drove away on Sunday night . <p> I set up two GoPro cameras . One was a stock GoPro Hero 3+ Black edition capturing 7MP narrow FOV stills every 60 seconds . The other was a GoPro Hero 3 Black with a " flat " lens capturing 5MP stills every 60 seconds . Unfortunately the 3+ stopped recording after about 24 hours I 'm not sure if the camera over heated , had a bug in the firmware ( I realized I 'm 1 version back from the latest ) , or if my memory card had @ @ @ @ @ @ @ @ @ @ from this camera . The backup camera kept running all 4 days and captured the entire festival . <p> Assembling this was simple I imported the images as image sequences in Adobe Premiere , arranged them on the timeline , cut out the night sequences ( there was almost no activity during them ) , added some transitions , titles , and color correction ( contrast and saturation ) , then added some background music . - I added slow zooming and panning to each of the shots to add drama , which helped make things a lot more interesting . <p> Ever since Photoshop introduced 3D printer support- Ive been hooked on 3D printing . Some of my recent experiments included text extrusions , phone cases , and a dragon . While these are cool , I still wanted to kick it up a notch or two . Those models were all printed using plastic materials , and I 've been wanting to try out a few other other materials , in particular metal . So in honor of Photoshop World coming up next week , I decided to design @ @ @ @ @ @ @ @ @ @ steel , and- it turned out AMAZING ! Far better than I had hoped . <p> I chose stainless steel because I wanted a minimalistic/industrial look for the belt buckle . The process was actually quite easy . I took a vector Photoshop logo in Illustrator , copied the PS and the square border , and pasted it into Photoshop . Then I extruded it into a 3D object , added a flattened cube on the back , and used cylinder and sphere primitives to create the belt loop and pin . Seriously , this was the complete process . I 'm not over-simplifying things . Check out the video below to see a timelapse recreating this model entirely in Photoshop . <p> When you are creating 3D models , just be sure to pay attention to the objects physical dimensions in the 3D Properties Coordinates panel . - Set the units to a physical unit of measurement ( I chose Centimeters ) , and create your objects using the exact physical print size . - Also , its important to know the physical characteristics and limitations of your target materials @ @ @ @ @ @ @ @ @ @ embossing depth , clearance , etc- Design within these parameters for best results . <p> 3D Properties Coordinates <p> Once you 're ready to print , just select your print target and material in the 3D Print settings panel . Then send it to print using the 3D-&gt;3D Print menu . - If you 're using the- Shapeways- 3D printing service you 'll then be redirected to upload your model and complete the print order . <p> 3D Print Properties <p> A few days later , your print will arrive in your mailbox . However , do not forget to double check your print volume/dimensions after you 've uploaded your STL to Shapeways . I noticed that some of the minimum thickness checks made the pin too thick , so I generated the STL file for a plastic material , then chose the metal material when actually ordering the print through Shapeways . <p> I 'm really happy with how this turned out , and yes , I 'm wearing this belt buckle right now . <p> All of this was created in its entirety using Creative Cloud . Join now if you have n't already become @ @ @ @ @ @ @ @ @ @ World next week in Atlanta , stop by to check it out and learn more ! <p> 3D printing has the potential to completely change how people create physical objects . It enables faster prototyping and iteration in design and manufacturing , enables new forms of artistry and jewelry , and even has applications in medicine . - If you have n't checked out 3D printing yet , you really should do yourself a favor and give it a few minutes of your time . 
@@106848699 @2248699/ <h> About <p> Andrew Trice is a Developer- Advocate with IBM . Andrew brings to the table more than a decade of experience designing , implementing , and delivering rich applications for the web , desktop , and mobile devices . He is an experienced architect , team leader , accomplished speaker , and published author , specializing in object oriented principles , mobile development , realtime data systems , GIS , and data visualization . <p> I have read your article and about using edge animate project and injecting them into div on another page . It came up while I was researching about loading edge animate projects after an ajax request . Would you know how and where I would need to edit my *edgePreload.js to make it work after ajax and not page load . <p> kai ettrup <p> Hi Andrew , <p> Can you tell me if it is really not possible to create a phonegap app that are larger than 9.5 Mb ? ( or what to do to get above the limit ) <p> Thanks in advance ! <p> Regards , kai 123436 @ @ @ @ @ @ @ @ @ @ over 10 MB on the PhoneGap Build service . I know there was talk about changing this , but I do n't  know where that currently stands . If you are using local dev tools and packaging yourself , you can definitely have a larger size than 10 MB . For example , this PhoneGap app is 1.5 Gigs : LONG ... <p> Paul Davis <p> Andrew , do you know of any progress on a phonegap plugin for Bluetooth LE for the iphone ? Apple published a ios sample back in November which works well allowing you to send data between two iphones , but I do n't  know IOS and am struggling to write my own plugin using the apple btle sample . If someone who know IOS took a look at it I 'm sure it would n't be that hard to convert to work with phonegap . Any suggestions ? <p> I do n't  know of one off the top of my head , but I 'd be surprised if someone has n't  already published an implementation . <p> Paul Davis <p> Thanks Andrew . There is n't an existing @ @ @ @ @ @ @ @ @ @ size of the project what I would really like to do is just strip out the user interface components in the apple Bluetooth Low Energy IOS sample found at : LONG ... to send data between two iphones . So I would just use the objective-c code to transfer the data and manage the bluetooth framework coordination and HTML5/javascript to do the rest . The net is I 'm not very good with objective-c and cant figure out how to strip out the UI code from the apple sample and since BTLE is hot and has so many potential use cases this functionality would be huge . <p> Jonathan <p> Hi Andrew <p> Can you advise a consultant/company that can help improve the perf of my phonegap app ? Thanks Jonathan <p> Screen Flow in Photoshop Note : This app will also be free and open source as a sample/learning resource for PhoneGap , including all of the design assets " I 'll follow up with another post on this later , once the app is available in the app stores . <p> Iam looking for the above assets ( app @ @ @ @ @ @ @ @ @ @ checked Github . Might you send a link if Assets , if have ? <p> btw GREAT resource 123436 @qwx983436 <p> Those assets havent been shared yet . However , if you want to just have some nice looking UI elements for your PhoneGap app , try checking out http : //topcoat.io/ <p> way2tour <p> I created one app with phonegap . In that app am using plguins related to android .. How can i use that same www folder for all other platform . help me <p> Will I be able to built someting on this code , and then distribute the result under any CC-License or even the WTFPL ? <p> Of course , If you like I will include your name in the Sourcecode . <p> Kind regards , Daniel <p> Gruff McGruff <p> Hello Andrew , I watched your ATL Meetup presentation on phone gap and really liked it . I am a Javascript junkie that has created an open-source , requireJS-inspired development tool called structreJS ( **29;836;TOOLONG ) . For phonegap-style and single-page apps it really helps to make controller modularization easy . I also @ @ @ @ @ @ @ @ @ @ modules and others can use them by simply adding them to their projects manifest . If you get a chance please check out my projects website or give it a spin LONG ... <p> tiptronic <p> You have a minor problem with your site-output : <p> Warning : fileexists() function.file-exists : openbasedir restriction in effect . LONG ... is not within the allowed path(s) : ( D : ; C:php5 ; C:Temp ; C:WindowsTemp ) in LONG ... on line 253 <p> August Conte <p> Hello Andrew , I read with interest your post on using photoshop zoomify images , which was very helpful . Do you know of any plugins which will permit markups to be added to the images ? <p> any help appreciated . thank you , August Conte <p> veerabhadra <p> how to connect facebook. if i want any plugins or permissions <p> http : **26;867;TOOLONG Pedro Zaffuto <p> Hello Andrew , nice to meet your blog ! great pleasure to feel in touch with another IBMer outside Argentina . <p> Godai Aoki <p> Hi Andrew-san . Your " IBM-Watson-Speech-QA-iOS " app is great . So @ @ @ @ @ @ @ @ @ @ users ( I 'm Japanese ) . So I localized this app ( I added translation function english to japanese and vice versa ) . May I publish this localized app on github ? 123435 @qwx983435 <p> Hi Godai , Yes , feel free to publish in github and share . Can you fork my repo , and then commit your code in your fork so that it maintains a link back to the original project ? <p> Godai Aoki <p> Thank you Andrew-san ! I forked your project.Ill update it later . <p> Tama Handika <p> Hello Mr. Andrew . First of all , thanks a lot for writing this amazing tips : " My Workflow for Developing PhoneGap Applications " . I have a question that need to be verified by experts like you . But please do n't  feel obligate to answer this <p> I want to create a real time location based android apps that will be running on the background . The apps will give a custom notification when the user/devices location detected in certain point . Is it possible to accomplish this on hybrid apps @ @ @ @ @ @ @ @ @ @ found out there is a Cordova plugin for both of notification and GPS . However there is some points that i 'm not sure : <p> for my phonegap application can you please help me out . I am new to programming <p> Thanks in advance .. : ) <p> Haranath Gnana <p> Hi Andrew , I 'm building a healthcare app leveraging Watson api and would like to chat with you on a couple things . Pls ping me if we can connect . Thanks much ! Haranath <p> Patrick Wong <p> Hi Andrew I have tried Watson Spoken Health QA app on chrome for osx as well as Safari , but I can not get it to work . When I click the ASK button , nothing happens . Please help me ! 123436 @qwx983436 <p> Hi Patrick , The Healthcare QA service is unfortuantely no longer available through IBM Bluemix . It has been retired , and replaced with newer , more advanced functionality . There 's currently not a sample data corpus that I can easily point to for this sample . I should probably take it offline , @ @ @ @ @ @ @ @ @ @ <p> Was interested in your fresh food app , any issues with users that you know of ? Do you ever provide services to edit the fresh food app to use the functionality but in a different type of app . We need a login screen that after login will display specific filters based on user type . 
@@106848700 @2248700/ <h> 3D Printing A Custom Belt Buckle With Adobe Photoshop <p> Ever since Photoshop introduced 3D printer support- Ive been hooked on 3D printing . Some of my recent experiments included text extrusions , phone cases , and a dragon . While these are cool , I still wanted to kick it up a notch or two . Those models were all printed using plastic materials , and I 've been wanting to try out a few other other materials , in particular metal . So in honor of Photoshop World coming up next week , I decided to design and print a Photoshop themed belt buckle in raw stainless steel , and- it turned out AMAZING ! Far better than I had hoped . <p> I chose stainless steel because I wanted a minimalistic/industrial look for the belt buckle . The process was actually quite easy . I took a vector Photoshop logo in Illustrator , copied the PS and the square border , and pasted it into Photoshop . Then I extruded it into a 3D object , added a flattened cube on the back , and used @ @ @ @ @ @ @ @ @ @ pin . Seriously , this was the complete process . I 'm not over-simplifying things . Check out the video below to see a timelapse recreating this model entirely in Photoshop . <p> When you are creating 3D models , just be sure to pay attention to the objects physical dimensions in the 3D Properties Coordinates panel . - Set the units to a physical unit of measurement ( I chose Centimeters ) , and create your objects using the exact physical print size . - Also , its important to know the physical characteristics and limitations of your target materials , including minimum wall thickness , minimum wire thickness , embossing depth , clearance , etc- Design within these parameters for best results . <p> 3D Properties Coordinates <p> Once you 're ready to print , just select your print target and material in the 3D Print settings panel . Then send it to print using the 3D-&gt;3D Print menu . - If you 're using the- Shapeways- 3D printing service you 'll then be redirected to upload your model and complete the print order . <p> 3D Print Properties <p> A few @ @ @ @ @ @ @ @ @ @ . However , do not forget to double check your print volume/dimensions after you 've uploaded your STL to Shapeways . I noticed that some of the minimum thickness checks made the pin too thick , so I generated the STL file for a plastic material , then chose the metal material when actually ordering the print through Shapeways . <p> I 'm really happy with how this turned out , and yes , I 'm wearing this belt buckle right now . <p> All of this was created in its entirety using Creative Cloud . Join now if you have n't already become a member ! If you 're going to be at- Photoshop World next week in Atlanta , stop by to check it out and learn more ! <p> 3D printing has the potential to completely change how people create physical objects . It enables faster prototyping and iteration in design and manufacturing , enables new forms of artistry and jewelry , and even has applications in medicine . - If you have n't checked out 3D printing yet , you really should do yourself a favor and give it a few minutes @ @ @ @ @ @ @ @ @ @ I need a 3D printing-ready file for a belt buckle that 's a lot like this one , except much simpler ( circular , plain , no logo in front ) . Would you be willing to prep the file for me ? I 'm happy to pay for your time , and can provide all measurements . Thank you ! 
@@106848703 @2248703/ <h> Live Editing Server Files with Edge Code/Brackets <p> If you have n't already heard of Adobe Edge Code or Brackets , it is a new HTML/CSS/JS code editor that enables a rapid development cycle by connecting directly to your browser for automatic real-time updates to the HTML content running inside the browser . This promotes rapid development , and allows you to focus on writing your code rather than worrying about all the steps required to switch between your editor and browser and step through development and- debugging- iterations . <p> In most contexts we talk about the live HTML/JS/CSS development perspective on the client side , but I 'd also like to highlight that- Adobe Edge Code- and/or- Brackets- can be used to rapidly develop dynamic server-side web code too . Check out the screencast below to see live editing of dynamically generated HTML content using the latest Brackets release ( Sprint 23 ) . <p> This streamlined workflow can be incredibly powerful , especially for rapid prototyping scenarios . You just focus on writing code and creating an experience . Let the edtior offload the work of updating @ @ @ @ @ @ @ @ @ @ on the task at hand . <p> To enable the Edge Code/Brackets preview on a server , just go into " File-&gt;Project Settings " , and then specify a " Preview Base URL " string . The preview base URL will be prefixed to the name of the file that you are editing , and will be used for the browsers URL in the live connection . So , if you are editing " index.html " with a preview base URL of http : //tricedesigns.com , brackets will launch the live connection with the URL " http : **29;895;TOOLONG " . <p> However , keep in mind that Edge Code &amp; Brackets primary use case is editing HTML/JS/CSS , not server-side languages . The editor wont have code hinting for your server-side code , unless you are using an extension that enables code hinting ( with the exception of NodeJS because JavaScript is supported ) . The current build of Brackets also changes the live preview URL when you change files . So , if you are editing " index.cfm " , and switch to " content.cfm " in the @ @ @ @ @ @ @ @ @ @ to " http : //urlprefix/content.cfm " . If the files are intended to be separate , this works great . However , if content.cfm is n't a standalone file , the live preview wont be as valuable . Meaning : if content.cfm is actually an included file inside of index.cfm , the live preview of content.cfm may not work the way you want it to . In either case , this can still be a very valuable tool with rapid prototyping . <p> Speaking of extensions Edge Code/Brackets is built with an extensible plugin architecture . Since the editor itself and all of the extensions are written with HTML/JS , you can easily extend and customize the tool to add any functionality that you want . <p> if you 're wondering about the difference between the names Adobe Edge Code- and- Brackets , - Adobe Edge Code- is Adobes distribution of the Brackets editor . Adobe Edge Code has scheduled releases , and includes useful extensions and tie-ins to other Adobe services ( for example , Adobe Web Fonts and more ) . Brackets- is the open source project for the core @ @ @ @ @ @ @ @ @ @ cycle , and is rapidly evolving . Essentially , Brackets is the " engine " that powers Adobe Edge Code . <p> I chose a simple ColdFusion- demonstration because I already had an existing code sample I could reuse , however this will work with PHP , NodeJS , or other server-side scripting libraries . The latest builds of Brackets even have- NodeJS built directly into it . <p> Submlime can do similar things , so can WebStorm and others . However , I think Brackets has the easiest to use implementation . Also , Brackets has some really cool live edit features in development for inline client and server debugging : http : //www.youtube.com/watch ? v=ukpoPF61W5A <p> This feature seems to assume that you are possibly starting off a project and not maintaining it . <p> TexMate simply refreshes the active browser visible tab . For me this works the best , it can be local , remote , static or Dynamic . <p> I have the same problems with all the other " live " previews . <p> http : //www.vusisindane.com/ Vusi Sindane <p> Have you figured out @ @ @ @ @ @ @ @ @ @ Patterson <p> I ended up using Live reload and my workflow now involves more Server Side coding so I use PHP Storm . <p> naman34 <p> I 'm not being able to do this at all with Node.js projects <p> naman34 <p> Now that I think about it , its probably not possible . If I use templates , then brackets obviously can not figure out how which URL to load by seeing the file I 'm working on . That said , there should be some way of me telling brackets which URL should open on which files , even if that is a manual process . 
@@106848705 @2248705/ <h> Photoshop Zoomify in HTML , powered by Leaflet <p> Ive been spending a lot of time with Photoshop recently Whether it has been retouching video or images , creating panoramas , or working with my aerial photos , it has been a lot of fun . One thing that I 've been doing is exporting really large images to the web . So far this has been a very manual process Export from Photoshop using Zoomify . Then , since the default Zoomify renderer uses Flash ( and I want this consumable on mobile devices ) , take the Zoomify image tiles , and put them into a custom-coded HTML experience using the Leaflet tile- engine with a custom tile layer . <p> Leaflet is normally used for web-based mapping , but it is a perfect solution for rendering image tiles on the web . It already has touch and mouse interactions , inertial scrolling , progressive viewing , and a comprehensive API that can be extended if you so choose . <p> I 've done this enough times that I figured " There has to be an easier way @ @ @ @ @ @ @ @ @ @ Zoomify template that allows you to export from Photoshops Zoomify feature directly to HTML , leveraging the Leaflet engine . All of the code and installation instructions are below in this post . Check out the video below to see it in action : <h> Samples <p> Here are few samples from the Zoomify output ; both are the compositions that I showed in the video above . Use the mouse or touch interactions to pan and zoom on each of them . <p> The first is an export from a 10MP aerial panorama ( 4340+2325 pixels ) , which was created by stitching together multiple images captured with a GoPro camera and remote controlled helicopter . <p> The second example is a massive 139MP composite image ( 14561+9570 pixels ) . I created this by stitching together 48 10mp images in Photoshop . Its not perfect , but shows how far you can zoom into an image some images had different exposures , some were out of focus , there is still some perspective warp , and I definitely have some bad stitching seams . This image is so @ @ @ @ @ @ @ @ @ @ and filled up all hard disk space with the memory swap file when creating it ( I had over 100 Gigs of free space ) ! <p> Extract the zip file and copy the following files to Photoshops Presets/Zoomify directory . On OS X , with the default configuration , these files should be located in /Applications/Adobe Photoshop CC/Presets/Zoomify/ <p> L.TileLayer.Zoomify.js <p> Zoomify Leaflet HTML.zvt <p> leaflet.css <p> leaflet.js <p> Restart Photoshop . <p> When you have a file open that you want to export , choose File -&gt; Export -&gt; Zoomify- <p> Then select the " Zoomify Leaflet HTML " template that should now be in the list . Select an output location , base name , and image options , and hit " OK " . Ignore the browser width and height , since the template ignores these . Instead , it takes 100% of the width and height of the browser window . <p> - This will generate all of the image tiles and the HTML structure . From here , do whatever you want with it You can modify it , put it on a server @ @ @ @ @ @ @ @ @ @ something like the image below . You will have a folder that contains the generated HTML file , the Leaflet JS and CSS files , and a directory that contains the generated tiles and appropriate XML metadata. 123433 @qwx983433 <p> thanks for that stuff would be nice to have a 360 degree endless scrolling template , do you think that 's possible ? <p> magen1 <p> I love you <p> Steflp G " <p> Hello , I want to use you leaflet.js + zoomify with several layers ( I ve " zoomifided " several maps on a town in differents century and i want to put them on one page . Can you help me in js + code please Thanks a lot <p> Evgeniy Arhipov ( Jonarhipov ) <p> Hello ! Could you help me ? I try to locate my image to the 0,0 in coordinate system of leaflet , but cant . I write all parametres in my script . Please watch . <p> Evgeniy Arhipov ( Jonarhipov ) <p> marker stands in 0,0 coordinates <p> http : //www.martinzimelka.com Martin Zimelka <p> Thank you very much ! Just @ @ @ @ @ @ @ @ @ @ this ? <p> Jerome <p> Hi . Do I need to download leaflet to make this work online or can I just export with zoomify photoshop using the script and just upload to my server ? <p> cellebrek <p> just THANK YOU ! <p> Rob Edgcumbe <p> Thank you for this very helpful way to make use of the Zoomify output . Just what I was looking for . Two questions : How do I edit the name in the Leaflet footer that appears on the page and how did you embed it within an existing page ? I am making the page pop open in a new tab but would like to embed it in blog posts . Many thanks . 
@@106848706 @2248706/ <p> A while back I wrote about adding parallax effects to your HTML/JS experiences to make them feel a bit richer and closer to a native experience . - I 've just added this subtle ( key word *subtle* ) effect to a new project and made a few changes I wanted to share here . <p> If you are wondering what I am talking about with " parallax effects " Parallax movement is where objects in the background move at a different rate than objects in the foreground , thus causing the perception- of depth . - Read more about it if you 're interested . <p> First , here 's a quick video of this latest app in action . - Its a hybrid MobileFirst app , but this technique could be used in any **35;926;TOOLONG web app experience . - The key is to keep it subtle and not too much " in your face " , and yes , it is very subtle in this video . - You have to watch closely . <p> The techniques that I wrote about in the previous post still apply Ive @ @ @ @ @ @ @ @ @ @ . <p> This sets the background image and default position . - The distinct change here is that I set the background size to " auto " width and 120% height . - In this case , you can have a huge image that shrinks down to just slightly larger than the window size , or a small image that scales up to a larger window size . - This way you do n't  end up with seams in a repeated background or a background that is too big to highlight the parallax effect effectively . <p> In the requestAnimationFrame loop , it only applies changes *if* there are changes to apply . - This prevents needless calls to apply CSS even if the CSS styles had n't  changed . - In this , I also truncate- the numeric CSS string so that it is n't reapplying CSS if the position should shift by 0.01 pixels . Side note : If you are n't  using requestAnimationFrame for HTML animations , you should learn about- it . <p> If you used my old code and were holding the device upside down , @ @ @ @ @ @ @ @ @ @ . - This has that fixed ( see comments inline above ) . <p> This moves the background in CSS , which does n't  cause browser reflow operations , and moves the foreground content ( inside of a div ) using translate3d , which also does n't  cause browser reflow operations . - This helps keep animations smooth and the UX performing optimally . <p> I also added a global variable to turn parallax on and off very quickly , if you need it . <p> The result is a faster experience that is more efficient and less of a strain on CPU and battery . - Feel free to test this technique out on your own . <p> If you use the code above , you can modify the xMovement and yMovement variables to exaggerate the parallax effect . <p> The first is- app versioning ; MobileFirst Foundation- tracks each version of an app that you deploy , and gives you the ability- govern or restrict access to specific platforms and versions. - App versioning applies to all apps , native or hybrid , on any platform that MobileFirst @ @ @ @ @ @ @ @ @ @ allows you to push new HTML/CSS/JavaScript ( web ) resources to a MobileFirst- hybrid app . Direct Update only applies to hybrid apps , but it works for- any platform that MobileFirst- supports . <h> App Version Management <p> When you deploy an app to the MobileFirst Foundation server , the server- will automatically track versions based on the version number specified in you **27;963;TOOLONG file . <p> Set Application Version <p> When you load the MobileFirst Foundation Server Console , you 'll be able to view all of the deployed app platforms and versions . <p> The screenshot below shows a hybrid app deployed for both Android and iOS- platforms . You would also be able to see the exact same version and platform information for native apps that leverage IBM MobileFirst Foundation . <p> Managing Versions in the MobileFirst Console ( click to enlarge ) <p> Youll notice in the MobileFirst console that next to each platform/version- you can set the status for that version . This makes it possible to set notification messages for- users on specific platforms and versions , or even restrict access to specific platforms @ @ @ @ @ @ @ @ @ @ screenshot above Version 1.0 on Android is active . Version 1.2 on iOS is active . Version 1.1 on iOS is notifying , and Version 1.0 on iOS is disabled . <p> There are 3 statuses that can be set for each platform and version combination. : Active , Active Notifying , and Access Disabled . <p> Set Platform/Version Status <p> When you set the status of a platform/version , this status is only for that specific platform/version pair . This enables you to selectively notify users of specific versions , or even block access to specific versions if they are outdated and no longer supported . <p> " Active " means that the application is active . Services to this version will operate normally , and no messages will be presented to the user . <p> " Active Notifying " means that the application is active , services will continue to work , but a message will be presented to the user when the app becomes active , or when a service request is made to the MF server . <p> Setting Active Notification Message <p> This can be @ @ @ @ @ @ @ @ @ @ . This could be a deprecation notice , service maintenance notice , or any other general notice . <p> Within the app , the user will see a message when the app becomes active , or when a request is made to the server . This message can be dismissed , and the app functionality is not impacted in any way . <p> In-App Active Notification Experience <p> " Access- Disabled " means that access to the application is disabled . In this state , a notification message will be presented to the user , and access from the app version will be disabled . The user will also be presented with an " Upgrade " button , which will redirect the user to any URL , which presumably will be for an updated version of the app . <p> Setting Disabled Status <p> In this state , the app will not be granted access to the MobileFirst/Worklight server . So , if your app requests- data from a data adapter , all requests to the adapter from this platform/version will be blocked . If- your app initialization code is @ @ @ @ @ @ @ @ @ @ can- prevent your app from loading at all . <p> In-App Disabled Experience <p> Again , When you set the status of a platform/version , this status is only for that specific platform/version pair . <p> Direct Update is a feature for MobileFirst hybrid apps , which enables- you to push updated app- content ( HTML , CSS , &amp; JavaScript ) without the user having to deploy a new version of the app through the app store . <p> Direct Update is considered an additional security feature b/c it enforces users to use the latest version of the application code. - However , when an app uses Direct Update , it *only* updates the web resources. - No native changes or version # changes will be applied . However , it should not be abused . In particular this will bypass the- Apples app store approval process. - You should not overhaul the entire UI and break Apples- Human Interaction Guidelines , otherwise you could be kicked out of the app store . <p> Direct Update User Experience <p> By default , the updates user experience is a modal @ @ @ @ @ @ @ @ @ @ UX can be configured to use silent updates that do not block the users experience , can be a completely custom user experience , or can be disabled altogether. - Updates can also be paused or resumed using the JavaScript API so that it does not block the user from performing a critical task , however this would require a custom UI the default UI does not enable pause/resume . <p> Updates in the current version of Worklight ( 6.2 ) are complete updates containing the entire application ( www ) code , however- MobileFirst Foundation- 6.3 ( coming this month ) will have a Differential Direct Update feature- that includes- only- the changed files . More detail will be posted once this is available . <p> Direct Update can also be disabled if you do n't  want your hybrid apps to update automatically . <p> For more information on Direct Update , be sure to check out these additional resources : <p> Im excited to finally announce that I have embarked upon a new adventure ! - Today marks my first day as a MobileFirst Developer Advocate for IBM @ @ @ @ @ @ @ @ @ @ It is very similar to what I was doing back at Adobe focusing on developers . - I 'm excited to engage with the development community around building mobile apps and leveraging cloud services to meet critical business needs . - I 'll be focused on IBMs MobileFirst- platform , including- Worklight- a platform for building and delivering mobile applications leveraging- Apache Cordova- ( PhoneGap ) , and Bluemix IBMs scalable cloud computing platform , which can be used for everything from hosted services , " big data " , security , back-ends for mobile apps , Java , - node.js , ruby , and much , much more Seriously , check out everything that Bluemix- has to offer . <p> It is my mission to help you , the developer or business decision maker be successful , and now I have access to IBMs tools , knowledge and services to back me up ! <p> Will I still be building apps and services ? <p> YES stay tuned for more info <p> Will I still be helping you build apps , and writing about development tools , paradigms , and best @ @ @ @ @ @ @ @ @ @ make the right decisions and be successful <p> Will I see you at the next development conference , hackathon , or meetup ? <p> YES , and I cant wait to show you everything IBM has to offer . - <p> Will I still be flying drones ? <p> Of course ! However , I wont be blogging about drones and creative tools quite so- much. - Follow me on- Flickr to see images from my latest flights , and feel free to ask me questions . <p> I had a great run with Adobe , and am thankful for all of the opportunities while there . - I worked on many- amazing projects , - worked- with a lot of great ( and very , very smart ) people , and was able to continually push the envelope on- both the development and creative/media sides . For which- I am grateful . <p> I 've just wrapped up my presentations for this years DevNexus event in Atlanta it has been a great event , filled with tons of information on web , mobile , and back-end development . I had @ @ @ @ @ @ @ @ @ @ and one a mobile frameworks panel . <p> Below are my presentations . - I did n't  record them this time , since they were being recorded by the conference organizers , so expect to see a video once they 're released . <p> Just press the space bar , or use the arrow keys to view the presentation in your browser. 
@@106848708 @2248708/ <h> GeoPix : A sample iOS app powered by IBM MobileFirst for Bluemix <p> In this post I 'd like to show a fairly simple application that I put together which shows off some of the rich capabilities for IBM MobileFirst for Bluemix that you get out of the box All with an absolute minimal amount of your own developer effort . - Bluemix , of course , being IBMs platform as a service offering . <p> GeoPix is a sample application leveraging IBM MobileFirst for Bluemix to capture data and images on a mobile device , persist that data locally ( offline ) , and replicate that data to the cloud . Since its built with IBM MobileFirst , we get lots of things out of the box , including operational analytics , user authentication , and much more . <p> ( full source code at the bottom of this post ) <p> Heres what the application currently- does : <p> User can take a picture or select an image from the device <p> App captures geographic location when the image is captured <p> App saves both the image @ @ @ @ @ @ @ @ @ @ . <p> App uses asynchronous replication to automatically save any data in local store up to the remote store whenever the network is available <p> Oh yeah , cant forget , the user auth- is via Facebook <p> MobileFirst provides all the analytics we need . - Bluemix provides the cloud based server and Cloudant- NoSQL data store . <p> All captured data is available on a web based front-end powered by Node.js <p> In this sample I 'm using everything but the Push Notifications service . - Im- using user authentication , the Cloudant DB ( offline/local store and remote/cloud store ) , and the node.js backend. - You get the operational analytics automatically . <h> Capturing Images <p> Capturing images from the device is also very straightforward . - In the app I leverage Apples- UIImagePickerController- to allow the user to either upload an existing image or capture a new image . - See the presentImagePicker and- **29;992;TOOLONG below . All of this standard practice using Apples developer SDK : <h> Persisting Data <p> If you notice in the- **29;1023;TOOLONG method above , there is a call to- the @ @ @ @ @ @ @ @ @ @ data locally and rely on Cloudants replication to automatically save data from the local data store up to the Cloudant NoSQL database . - This is powered by the iOS 8 Data service from Bluemix . <p> The first thing that we will need to do is initialize the local and remote data stores . Below you can see my init method from my DataManager class . In this , you can see the local data store is initialized , then the remote data store is initialized . If either data store already exists , the existing store will be used , otherwise it is created . <p> Once the data stores are created , you can see that the replicate method is invoked . - This starts up the replication process to automatically push changesfrom the local data store to the remote data store " in the cloud " . <p> Therefore , if you 're collecting data when the app is offline , then you have nothing to worry about . - All of the data will be stored locally and pushed up to the cloud whenever you 're back @ @ @ @ @ @ @ @ @ @ - When using replication with the Cloudant SDK , you just have to start the replication process and let it do its thing fire and forget . <p> In my replicate function , I setup- CDTPushReplication for pushing changes to the remote data store . - You could also setup two-way replication to automatically pull new changes from the remote store . <p> Once weve setup the remote and local data stores and setup replication , we now are ready to save the data the were capturing within our app . <p> Next is my saveImage withLocation method . - Here you can see that it creates a new- **26;1054;TOOLONG object ( this is a generic object for the Cloudant NoSQL database ) , and populates it with the location data and timestamp. - It then creates a jpg image from the UIImage ( passed in from the UIImagePicker above ) and adds the jpg as an attachment to the document revision . - Once the document is created , it is saved to the local data store . - We then let replication take care of persisting this data @ @ @ @ @ @ @ @ @ @ query data from either the remote or local data stores , we can just use the performQuery method on the data store . Below you can see a method for retrieving data for all of the images in the local data store . <p> At this point we 've now captured an image , captured the geographic location , saved that data in our local offline store , and then use- replication to save that data up to the cloud whenever it is available . <p> AND <p> We did all of this without writing a single line of server-side logic . - Since this is built on top of MobileFirst for Bluemix , all the backend infrastructure is setup for us , and we get operational analytics to monitor everything that is happening . <p> With the operational analytics we get : <p> App usage <p> Active Devices <p> Network Usage <p> Authentications <p> Data Storage <p> Device Logs ( yes , complete debug/crash logs from devices out in the field ) <p> Push Notification Usage <h> Sharing on the web <p> Up until this point we have n't had to @ @ @ @ @ @ @ @ @ @ on Bluemix comes with a Node.js server . - We might as well take advantage of it . <p> The Node.js back end comes preconfigured to leverage the- express.js- framework for building web applications . - I added the- jade template engine and Leaflet for web-mapping , and was able to crank this out ridiculously quickly . <p> The first thing we need to do is make sure - we have our configuration variables for accessing the Cloudant service from our node app. - These are environment vars that you get automatcilly if you 're running on Bluemix , but you need to set these for your local dev environment : <p> Next you 'll se the logic for querying the Cloudant data store and preparing the data for our UI templates . You can customize this however you want caching for performance , refactoring for abstraction , or whatever you want . All interactions with Cloudant are powered by the Cloudant Node.js Client 
@@106848709 @2248709/ <h> 360 Spherical Panoramas The Creation of Planet Vegas <p> here 's a fun tutorial for one of the last Friday afternoons of 2013 the creation of 360 degree panoramas , or " planets " as some like to call them . - If you 're not quite sure what I 'm talking about , check out the image below : <p> Planet Vegas <p> A panorama is a wide-angle view , usually captured with either a special lens , or by stitching together multiple images to create the wide angle view . - A 360 degree panorama is a representation of the wide angle view into a sphere . <p> This image was created in Photoshop by taking multiple images which were captured by GoPro attached to a remote controlled helicopter , stitching them together to create a wide angle panorama , then creating a 360 panorama from the wide angle panorama . <p> Check out the video below to see a timelapse for the creation of this image . Details below <p> Pretty cool , right ? - I had a great time putting this together . - Just ask @ @ @ @ @ @ @ @ @ @ that I have been completely obsessed with this . Its not a new technique This has been around for years , but its one not everyone knows off of the top of their head . - You should know how to do it too ! <p> here 's how you go about creating a 360 degree spherical panorama : <h> Step 1 : Start with the image - <p> You do n't  have to have a massive panorama . You can use any image that you want . Though , I 've had the best results by applying this to panoramas. - You could also crop an image to be much wider than it is tall . - If you want to learn how to create panoramas , check out these tutorials : <p> I did a manual process that is a variation of these The original images I had were from a GoPro Hero 3 camera . - I applied lens profile correction in Lightroom , then loaded the images into Photoshop as separate layers of the same PSD composition . - Then I used auto-align layers and auto-blend layers to @ @ @ @ @ @ @ @ @ @ 3 : Rotate and Crop <p> You 'll want to rotate and crop the image so that the horizon is level , and you get rid of missing areas within the image . If you 're missing part of the image , but you do n't  want to crop any more , you can use content aware fill or the clone stamp to synthesize the missing parts . <p> Rotating and cropping in Photoshop <h> Step 4 : Get Rid of Seams <p> You do n't  necessarily need a full view of 360 degrees to create these types of images , but you will get the best results if the image does n't  have any glaring seams . The way that I usually go about doing this is by splitting the image in half , and swapping the two sides so that the seam is in the middle of the image , with some overlap . <p> Work in progress removing seams <p> Then I use Photoshops clone stamp , brushes , or content aware fill to get rid of any glaring visual seams so that it is one complete image . @ @ @ @ @ @ @ @ @ @ of any whitespace introduced from the overlapping . <h> Step 5 : Make the image a square <p> Just go to Image-&gt;Image Size and make the width and the height of the image the exact same value ( you 'll need to unlink width and height ) . - This will stretch your image out vertically do n't  worry , the stretching is OK and is part of the process . <p> " Squareifying " the image <h> Step 6 : Flip the image <p> Next , you 'll want flip the image so that it is upside down . - You can do this either by going to **29;1082;TOOLONG 180 , or **27;1113;TOOLONG Vertical . <p> Flipped Image <h> Step 7 : Apply Polar Coordinates Distortion <p> Make sure that your image has been flattened or that all layers are within a smart object , so you are applying this filter to only one layer . - Select the target layer , then go to **28;1142;TOOLONG Coordinates . - Make sure that the " Rectangular to Polar " option is selected , and hit " OK " . <p> Polar Coordinates @ @ @ @ @ @ @ @ @ @ the distortion filter , the content in the top 25-30% of your image will be stretched out , and the bottom 25-30% will be pinched/squeezed , so just be aware that this will happen . - You will want to play around with this feature with variations of image cropping to get a feel for what it will do to your images . <h> Step 8 : Polish your image <p> At this point , you will now have a 360 spherical panorama , but you might not be 100% happy with the output . - If you do n't  like how the distortion was applied , go back and change cropping and try again . If you want to make the colors " pop " , try applying Camera RAW as a filter . - Youll also notice that the corners of the image are partially transparent and stretched badly you can crop this area , retouch the area , or layer assets within a composition . - This is really up to you as the creator of the composition . Whatever you do , take advantage of Photoshop @ @ @ @ @ @ @ @ @ @ enhance your creativity . 
@@106848710 @2248710/ <h> Adding Dimensionality To An Edge Animate Composition <p> Lately Ive been spending a lot more time working with- Adobe Edge Animate , - Adobe InDesign , and Adobe DPS . If you are n't  familiar with these tools , Adobe Edge Animate is a tool that enables the creation of animated or interactive HTML content , Adobe InDesign is a desktop publishing design tool , and Adobe DPS is Adobes Digital Publishing Suite , which is used for creating digital publications from InDesign everything from digital magazines , catalogs , corporate publications , education , and more . <p> So , you might be wondering , how does Edge Animate fall into this grouping ? Well From Edge Animate you can export compositions into a . oam package , which can be imported directly into InDesign for use with a web content overlay . You can read more about this process on Adobe Developer Connection . <p> I was recently asked by a customer " does Edge Animate support 3D transforms ? " . Unfortunately , at this time 3D transforms are not supported in the timeline editor @ @ @ @ @ @ @ @ @ @ JavaScript . Here are some examples showing how to integrate CSS3 3D transforms with Edge Animate compositions : <p> These can be great additions to the interactive experience , but I also wanted to share that you do n't  always need 3D transforms to add dimensionality to an interactive experience . By leveraging 2D translation , scaling , and opacity you can easily create interactive experiences that have a feeling of depth . <p> Let 's take a look at a quick example . The image below is from screenshots of an Edge Animate Composition that I put together . On the left-hand side there is an anatomical illustration . On the right-hand side , that illustration has been broken out into separate layers , with emphasis placed on the topmost layer . <p> Just click or tap on the image to see an animation that transforms the illustration on the left to multi-layered cutaway on the right hand side . <p> So , while this animation does n't  leverage any actual three dimensional elements , it leverages those 2D transforms to visually create a sense of depth . here 's how @ @ @ @ @ @ @ @ @ @ . The bottom-most image shows the skeletal structure and body outline . The middle image shows parts of the digestive system , and the top-most image shows another layer of major organs . The top 2 images have transparency so that they do not completely hide content from the underlying layers . <p> The default state is that all of these images are aligned so that they appear as a single image . <p> Once you click/tap the image , a set of two-dimensional- animations take place providing a sense of depth and emphasizing the top layer . The underlying layers have both a scale and opacity change . The bottom layers are smaller , and less opaque . The underlying layers also have a two dimensional ( top/left ) transform . In this example , I 've tried to align both the scale and top/left transforms to correspond with a 3D point of origin to the left side . <p> Edge Animate Anatomy Composition <p> This technique provides the illusion of three-dimensional depth , even though we are n't  actually performing any kind of translation , rotation , or deformation @ @ @ @ @ @ @ @ @ @ be implemented- completely- with the timeline . So , you do n't  have to be a programmer to add a dimensional feeling to you Edge Animate compositions . This effect was achieved simply by using the timeline editor and visual workspace . <p> You can preview this animation in a new window , or download the full source using the links below : 123433 @qwx983433 <p> 3D transforms are superior to 2D even if the effect that you want can be done entirely in 2D , even if the desired effect has no perspective nor 3D depth effect whatsoever . Why ? Because most modern browsers leverage the 3D acceleration hardware in the GPU to perform 3D transforms , but do not for 2D transforms . <p> Since this hardware is designed to give visually smooth results for modern 3D games with scenes and effects orders of magnitude more complex than you 'd likely want to do on a web site or DPS project , it has more than enough oomph to provide a much smoother response for even relatively low-powered portable devices that have GPUs , and also take @ @ @ @ @ @ @ @ @ @ tasks smoother and more responsive as well . <p> In short , by using 3D transforms even to perform 100% 2D animations , you offload much of the animation computation load to the GPU where it belongs . Not so with 2D transforms . Those are handled by the graphics hardware blitter accelerator engines , which are in general nowhere near as powerful and well optimized , and require much more CPU intervention . <p> The next Edge Animate should default to doing 3D transforms for this reason . It should even convert existing 2D transforms into equivalent 3D in any new loaded project unless told in a Preference not to do so . In fact , they should probably strip out the 2D animation code entirely , except for the purpose of converting to 3D . If you really want to do CSS3 2D transform animation for some unfathomable reason , you could use the older versions . 
@@106848712 @2248712/ <h> Color Key/Green Screen Video Techniques with Creative Cloud <p> In my last post , I talked about masks in After Effects , specifically the new motion tracking feature for rigid masks . In this post , Im again focusing on video composition , but instead of compositing using masks , I 'm going to talk about keying . <p> Keying is a technique for selectively removing areas of a video based on content inside of that video , so that there is transparency . - With this transparency , you can add layers and special effects to your video compositions . - There is color keying , which removes pixels with specific colors from a video , luminance keying ( luma key ) , which removes pixels based on brightness value , track matte key , and more - all of which are just different methods of removing pixels from a video and adding transparency . <p> If you 've ever wondered how news programs get the weather person to appear in front of an animated weather map , they 're just using color keying/green screen techniques . This is when @ @ @ @ @ @ @ @ @ @ a green background , and the green background is removed using color keying. - Then you 're just left with the weather person , which can easily be composited/overlaid on top of the weather map . <p> Check out the video below to see two examples of keying in action ( scroll down to jump directly to the video ) The first example shows how to use a green screen/color key technique in Adobe Premiere to overlay a subject on a background video . <p> Attribution : Dog Video , Beach Video- Youll get better results with higher quality video I just used these for simplicity . The dog video is licensed under Creative- Commons- with attribution , and the beach video is a- preview file from Pond5 . <p> The second example shows usage of color keying to overlay an explosion over a more complicated scene in After Effects complete with motion tracking , and additional masking to make the explosion look like smoke is billowing down the streets between the buildings . <p> Now , check out the full video to see how to apply these techniques . You @ @ @ @ @ @ @ @ @ @ Cloud . 
@@106848713 @2248713/ <h> Multi-Screen iOS Apps with PhoneGap <p> Did you know that apps built on top of iOS can have a multi-screen workflow ? For example in Keynote , you can have an external screen show a presentation while you control it on your iOS device . In the- Jimi Hendrix app , you can view the audio player on an external screen , and in- Real Racing HD , you can view the game on an external screen while the iOS device becomes your controller . ( among others ) <p> Real Racing HD <p> This is all made possible by the UIWindow and UIScreen APIs in iOS . Even better , on the iPad 2 and iPhone 4Gs , this can be done wirelessly using Airplay with an Apple TV device . On other iOS devices , you can have a second screen using a VGA output . <p> One of the benefits of using a cross platform solution like PhoneGap or Flex/Air is that you can build apps with an easier to use/more familiar paradigm . - However , cross platform runtimes do n't  always offer access @ @ @ @ @ @ @ @ @ @ Out of the box , PhoneGap apps are confined to a single screen . - You can use screen mirroring to mirror content on an external screen , but you cant have a second screen experience . - Its a good thing you can write native plugins/extensions to enable native functionality within your applications . <h> ExternalScreen Native Plugin For PhoneGap <p> I recently did exactly that I created a PhoneGap native plugin that enables second screen capability for PhoneGap applications . - The plugin listens for external screen connection notifications , and if an additional screen is available , it creates a new UIWebView for HTML-based content in the external screen complete with functions for injecting HTML , JavaScript , or URL locations . <h> Why ? <p> You might be wondering " Why ? " you would want this plugin within PhoneGap - this plugin enables the multi-screen experiences described in the apps mentioned above . - They extend the interactions and capabilities of the mobile hardware . - With this PhoneGap native plugin , you can create rich multi-screen experiences with the ease of HTML and JavaScript. @ @ @ @ @ @ @ @ @ @ apps that you can build with this approach ( scroll down for source code ) : <h> Fleet Manager <p> Let 's first consider a simple Fleet Manager application which allows you monitor vehicles in a mobile app. - This is a similar concept which Ive used in previous examples . - The basic functionality allows you to see information on the tablet regarding your fleet . - What if this app connected to a larger screen and was able to display information about your vehicles for everyone to see ? - Watch the video below to see this in real life . <p> This application example is powered by Google Maps , and all of the data is randomly generated on the client . <h> Law Enforcement <p> Let 's next consider a mobile law enforcement application application which gives you details to aid in investigations and- apprehension- of criminals . - Let 's pretend that you are a detective who is searching for a fugitive , and you walk into a crowded bar near the last known location of that fugitive . - You connect to the bars Apple TV on @ @ @ @ @ @ @ @ @ @ of the suspect , then say " Have you seen this person ? " . - This could be incredibly powerful . - Check out- the video below to see a prototype in real life . <p> This law enforcement demo scenario is a basic application powered by the FBIs most wanted RSS data feeds . <h> Tip Of The Iceberg <p> There are lots of use cases where a second screen experience could be beneficial and create a superior product or application . - Using PhoneGap allows you to build those apps faster &amp; with the ease of HTML and JavaScript , using traditional web development paradigms . <p> The PhoneGap native plugin is written in Objective C , with a JavaScript interface to integrate with the client application . PhoneGap plugins are actually very easy to develop . - Basically , you have to write the native code class , write a corresponding JS interface , and add a mapping in your PhoneGap.plist file to expose the new functionality through PhoneGap. - There is a great reference on the PhoneGap wiki for native plugins- which includes architecture &amp; @ @ @ @ @ @ @ @ @ @ of those plugins. - - Here are quick links to the iOS-specific native plugin content authoring and installation . <p> The ExternalScreen plugin creates a UIWebView for the the external screen , and exposes methods for interacting with the UIWebView. - Note : This is just a normal UIWebView , it does not have support for all PhoneGap libraries just a standard HTML container . <p> You can read up on multi-screen programming at iOS from these useful tutorials : <h> PGExternalScreen.h <p> The header file shows the method signatures for the native functionality . - The corresponding PGExternalScreen.m contains all of the actual code to make it all work . - Note : If you are using ARC ( Automatic Reference Counting ) , you will need to remove the retain/release calls in PGExternalScreen.m . <p> I 'm running your basic sample on iPhone4S , and an AppleTV on same network , but the UIScreen count is always 1 . I can share photos to the tv tho . Is there anything I 'm missing ? maybe a setting somewhere ? <p> Alan <p> I sorted it out . Works great @ @ @ @ @ @ @ @ @ @ a second screen is in place . Have you run into this or found a solution ? 123436 @qwx983436 <p> What issue ar you seeing ? You wont be able to set input focus on the second screen . <p> Brent <p> I 'm seeing a similar issue . I 've got two screens , #1 is on an iPad , #2 is on an external screen . If I init the second screen and add in an input field on screen #1 the keyboard does not appear . If I remove the second screen the keyboard works fine . <p> Brent <p> Figured it out , in PGExternalScreen.m , ( void ) attemptSecondScreenView , the second last line " externalWindow makeKeyAndVisible ; " can be commented out / removed . Since you cant interact with the window , there 's no point in grabbing the interaction . 123436 @qwx983436 <p> Awesome , thanks for sharing ! <p> http : //www.andrewthorp.com Andrew Thorp <p> Working great in loading the content , but external assets ( css , images , etc ) do not seem to be appearing correctly . <p> I am using @ @ @ @ @ @ @ @ @ @ it . If i copy them to inline styles they work , but the urls to images do not work ( assets/img/img-here.png ) . <p> Any ideas ? <p> http : //www.andrewthorp.com/ Andrew Thorp <p> Followup to the previous question , I am trying to load a local html file . <p> This is a HTML file that needs to have a background image and needs to be able to execute jQuery . Can I not use external assets in a local html file that is loaded via loadHTMLResource() ; <p> Thanks ! 123436 @qwx983436 <p> Hi Andrew , I think it might be a bug in the plugin I 'll have to look into it further . This is an issue w/ the root for the UIWebView . Ill look into it and get back to you . <p> Yeah , I do understand that it is returning what is supported by the screen , but its odd that if I do n't  set it ( just use your plugin code ) it seems to use the screens native resolution , which is much larger than 720+480 . <p> It @ @ @ @ @ @ @ @ @ @ so I know it CAN feed it out at 16:9 . <p> The **31;1172;TOOLONG method . Wouldnt it be more useful if that sent a notification back to the JavaScript that I could listen for ? <p> If the display is disconnected and reconnected mid application , I am not aware of it in the app itself . Am I supposed to test every second to see if there is a second screen ? Or am I missing something ? <p> When I first load the application , whether there is a second screen or not , I see : " External Screen Notification Handlers initialized . " <p> However , if I plug my second screen in after that , I see a white screen with " loading " on it , but the resultHandler is never fired again . <p> Thanks ! 123436 @qwx983436 <p> You can add a call to self writeJavascript : @ " put javascript here " ; inside of both **31;1205;TOOLONG and **34;1238;TOOLONG , and it will do this for you . If I have time , I can add them later this week @ @ @ @ @ @ @ @ @ @ to get started w/o waiting on me . The writeJavascript function will execute any JS string within the PhoneGap web view . <p> http : //www.pixelagent.co.uk Nigel <p> Is there an issue with the makeKeyAndVisible when using this plugin ? Once synced the keyboard on my ipad wont load . 123436 @qwx983436 <p> Ill have to test it here and let you know I have n't noticed any errors before . Ill double check , and see if I can find anything . <p> Blake <p> Are you sure this supports AirPlay ? Ive been playing around with it but cant seem to get AirPlay to connect . So far it only works over HDMI+VGA. 123436 @qwx983436 <p> Yes it supports Airplay , but you need to be sure that the devices supports Airplay mirroring This works on iPhone4S , iPad2+ , but not on iPad 1 or iPhone 4 and earlier b/c those devices do not support Airplay mirroring . <p> grim <p> Anyone give this a shot with a newer version of Phonegap ? I had some issues with the name changes from PG to CDV in the plugins @ @ @ @ @ @ @ @ @ @ do n't  seem to work . Awesome plugin any target version of Phonegap you 'd recommend ? 123436 @qwx983436 <p> Yes , this does work with newer versions of PhoneGap . All you need to do is update the class name references . You can use it on any version of PhoneGap after 1.3 . Just make sure that the device you are using supports AirPlay mirroring . The iPhone 4 ( and earlier ) , and iPad 1 do not support it . <p> http : //www.creeostudio.it NoriSte <p> Hi Andrew , do you know if something similar will be possible with AIR ? I believe we cant but do you know if is Adobe planning to support it ? It could be very useful . Now I 've to create a native app only on selected smarttvs http : //www.youtube.com/watch ? v=0e8cmy1Vmic 123436 @qwx983436 <p> I am not actively engaged with the AIR roadmap , so I cant say with any certainty . However , I am not aware of any plans . You could create an ANE that does exactly what this PhoneGap native plugin does ( or just @ @ @ @ @ @ @ @ @ @ a web view , and not Flash/AIR content in the second screen . <p> This is beyond epic . I was waiting for someone to create this plugin . Ca n't wait to use this ! 123436 @qwx983436 <p> Thanks ! Feel free to share whatever you create ! <p> http : //www.appelsiini.net/ Mika Tuupola <p> Excellent stuff . I have been working with couple of similar projects . However these are HTML5 + JS applications and not PhoneGap . Content on bigger screens is controlled by iPad websockets . This is not screen mirroring although some views look almost the same . Here is video of the first version of the app ( ignore the cheesy music ) . <p> Both devices are ready and i can use them to watch f.e. image from iphone on atv and mac . <p> But when i test you example i get the error : SUCCESS : NO or External Webview Unavailable <p> What do i wrong <p> Greets from germany <p> Kevin <p> http : //www.appzer.de Kevin <p> today i could test it with a vga cable , this works fine , @ @ @ @ @ @ @ @ @ @ mac ) <p> Kevin 123436 @qwx983436 <p> Make sure that Airplay mirroring is enabled . 123436 @qwx983436 <p> Make sure that you have screen mirroring turned " ON " . See details here : http : **29;1274;TOOLONG IF you do n't  have mirroring turned " ON " , it will not recognize the second screen , even though AirPlay is enabled . The really odd part is that you are n't  really mirroring your screen in this case . <p> peter <p> great work ! " just can not get the initial viewport of the appletv to fullscreen without displaying a black border around the content of loadhtmlresource. after a video file has been played once , the viewport would stick to fullscreen. any ideas , how set the initial view to true fullscreen ? <p> The possibilities for this are insane , and myself and I 'm sure the rest of the community would LOVE the change to play with it . <p> Murat <p> Hi , <p> Is there a way as the same of the ipad screen to transfer full hd TV ? Could you give an example @ @ @ @ @ @ @ @ @ @ need to do anything special if you want screen mirroring . Just write your app normally without using the UIScreen API . Then , connect to an AppleTV device and mirror your main screen onto the TV . Details here : http : **29;1305;TOOLONG <p> Murat <p> Thanks for the answer but in this wise resolution falls and not the full screen . Is there any way about to get HD image on the TV ? 123435 @qwx983435 <p> Older apple TV devices only support up to 720p , newer devices can support 1080p . No Apple TV devices , and most TVs are not capable of displaying the full retina resolution of the iPad ( 2048 x 1536 ) . <p> Yes I Ca n't the screen to full size . This is limited to ipad Screen . <p> If you have an example to Xcode file and you share to me I 'm so glad 123435 @qwx983435 <p> Are you just mirroring your screen ? When you say you " cant make it to full size , this is limited to ipad screen " , it sounds like @ @ @ @ @ @ @ @ @ @ let 's you treat the TV screen as a completely separate screen . If you are mirroring only , make sure you have this plugin configured correctly . <p> Murat <p> Yes i am use to mirroring screen , I think i ca n't activate this plugin , I found the exam to the this solution i am proceed on this issue . 
@@106848714 @2248714/ <h> UX &amp; Mobile Apps Leveraging the IBM MobileFirst Platform <p> When you are developing a mobile app ( or website , or mobile web , or TV app , etc .. ) - you should always ask yourself " What kind of an impact does this have on the end user ? " It does n't  matter whether you are creating enterprise apps or games , or anything in between . Every development decision that you make should be weighed upon its impact to the overall impact- it has on the end user . Simply put : if your app sucks , nobody is going to want to use it . <p> When building mobile apps using IBM MobileFirst you have two options for a user interface layer ; you can write a native app , or you can write a hybrid app using HTML , CSS , &amp; JavaScript . <p> So , what kind of an impact does the addition of IBM MobileFirst have on the app ? <p> NONE , granted the apps UX can- vary depending upon whether you are developing a native app @ @ @ @ @ @ @ @ @ @ is a platform that consists of a server tier and client-side SDK . If you are developing a native app , the SDK/API provides access to MobileFirst platform features , like user authentication , app version management , data access through adapters , encrypted storage , unified push notification , remote log collection , and more . If you are developing a hybrid app , the apps UX must be developed complete inside of the web view container . The MobileFirst- Foundation SDK provides additional functionality just like mentioned above for the native SDK , plus a few classes that enable native dialogs and a few native UI elements , but for the most part , there is very , very minimal impact on- the users experience . <p> When building any kind of mobile app , regardless of whether it is native or hybrid , you need to pay attention to what the user experiences . Are you following human interaction or design guidelines for the platform ? Are you forcing your user to go through unnecessary or redundant steps ? Are you making forms- more complex than they @ @ @ @ @ @ @ @ @ @ a simple interaction ? Can things be simplified ? <p> The IBM MobileFirst Platform does not add any additional overhead for UX processes . <p> The IBM MobileFirst platform can be used to develop native apps on iOS , Android , Windows Phone , or Java ME platforms . Follow native coding conventions and UX guidelines for each individual platform . Make sure you follow these guidelines , otherwise your app may feel alien within the ecosystem , or may be rejected from app store approval altogether . <p> If you are using the HTML/CSS/JavaScript approach ( leveraging the- Apache Cordova- container ) , then you really want to focus on the users experience inside of the HTML container . You want to make sure the UI feels like " an app " , not like " a web page " . There are many client-side frameworks that help address this need . Feel free to use any of them , or roll your own just keep the UX/human interaction guidelines in mind . <p> For both native and hybrid approaches , you also want to consider impacts of perceived @ @ @ @ @ @ @ @ @ @ deliver the appearance that the app is fast and responsive , instead of sluggish or locked while waiting to perform an action . Perceived performance improvements can be achieved simply by providing instant feedback , performing animations during an asynchronous request , or preemptive tasking . Do n't  miss this post , where I go into perceived performance in mobile apps in great detail . <p> Experience Manager apps for marketers , this will be the first mobile app development platform that brings teams together to drive efficiency in app development , improve collaboration , and deliver better app experiences . 
@@106848715 @2248715/ <p> A while back I wrote about adding parallax effects to your HTML/JS experiences to make them feel a bit richer and closer to a native experience . - I 've just added this subtle ( key word *subtle* ) effect to a new project and made a few changes I wanted to share here . <p> If you are wondering what I am talking about with " parallax effects " Parallax movement is where objects in the background move at a different rate than objects in the foreground , thus causing the perception- of depth . - Read more about it if you 're interested . <p> First , here 's a quick video of this latest app in action . - Its a hybrid MobileFirst app , but this technique could be used in any **35;1336;TOOLONG web app experience . - The key is to keep it subtle and not too much " in your face " , and yes , it is very subtle in this video . - You have to watch closely . <p> The techniques that I wrote about in the previous post still apply Ive @ @ @ @ @ @ @ @ @ @ . <p> This sets the background image and default position . - The distinct change here is that I set the background size to " auto " width and 120% height . - In this case , you can have a huge image that shrinks down to just slightly larger than the window size , or a small image that scales up to a larger window size . - This way you do n't  end up with seams in a repeated background or a background that is too big to highlight the parallax effect effectively . <p> In the requestAnimationFrame loop , it only applies changes *if* there are changes to apply . - This prevents needless calls to apply CSS even if the CSS styles had n't  changed . - In this , I also truncate- the numeric CSS string so that it is n't reapplying CSS if the position should shift by 0.01 pixels . Side note : If you are n't  using requestAnimationFrame for HTML animations , you should learn about- it . <p> If you used my old code and were holding the device upside down , @ @ @ @ @ @ @ @ @ @ . - This has that fixed ( see comments inline above ) . <p> This moves the background in CSS , which does n't  cause browser reflow operations , and moves the foreground content ( inside of a div ) using translate3d , which also does n't  cause browser reflow operations . - This helps keep animations smooth and the UX performing optimally . <p> I also added a global variable to turn parallax on and off very quickly , if you need it . <p> The result is a faster experience that is more efficient and less of a strain on CPU and battery . - Feel free to test this technique out on your own . <p> If you use the code above , you can modify the xMovement and yMovement variables to exaggerate the parallax effect . <p> I 've just wrapped up my presentations for this years DevNexus event in Atlanta it has been a great event , filled with tons of information on web , mobile , and back-end development . I had 3 sessions on PhoneGap One intro , one advanced , and one a @ @ @ @ @ @ @ @ @ @ - I did n't  record them this time , since they were being recorded by the conference organizers , so expect to see a video once they 're released . <p> Just press the space bar , or use the arrow keys to view the presentation in your browser . <p> I 've recently had several conversations with PhoneGap users around processes for automating the compilation of PhoneGap apps . - This could be either in automated tasks using- Grunt , - Ant , - Maven , or any other task manager , or could be in continuous integration environments like Jenkins CI . <p> If you 're interested in this , here are a few options First of all , PhoneGap Build has a REST API . You can use this to programmatically create new projects , update projects , trigger new builds ( even just for specific platforms ) , etc This can integrate with your build scripts and tie into any workflow . <p> If you 're using GitHub , it is possible to tie into hooks triggering PhoneGap Build to recompile every time you commit your code . - Heres @ @ @ @ @ @ @ @ @ @ just use this service which is already setup : - http : //autobuild.monkeh.me/- ( from the same author ) - - Just be careful with your user/pass in plain text . Update : You can also use the- Autobuild service using a clientID variable instead of sending through username and password details via HTTP . <p> If you are n't  using PhoneGap Build , you 're not out of luck . - All PhoneGap CLI- commands are based on scripts , which themselves can be scripted. - You could use ANTs exec command , the Maven exec plugin , Grunt exec or Grunt shell plugins , Jenkins execute shell , or any other task runner to manually invoke the PhoneGap CLI . You just need to make sure all your environment and path variables are correct to access SDKs and required programs . However , there 's one caveat iOS builds require Xcode/Apple developer tools , which have to be run on a Mac . <p> I was - searching the web earlier this week for an older presentation from a few months back , and just happened to stumble across my @ @ @ @ @ @ @ @ @ @ like the videos were posted in November , but I 'm just seeing them now . I had two sessions : - Designing and Architecting PhoneGap and Mobile Web Apps and- Getting Started with PhoneGap and Cross-Platform Mobile Development , and if you werent able to attend them , you 're still in luck ! Here are the videos from those sessions : <h> Designing and Architecting PhoneGap and Mobile Web Apps <p> Tired of Hello World ? In this session , we explore best practices to build real-world PhoneGap applications . We investigate the Single Page Architecture , HTML templates , effective Touch events , performance techniques , modularization and more . We also compare and contrast the leading JavaScript and Mobile Frameworks . This session is a must If you plan to build a PhoneGap application that has more than a couple of screens . <h> Getting Started with PhoneGap and Cross-Platform Mobile Development <p> Unfortunately , I ran into network issues which prevented some of my samples from working in this one , but you 'll still be able to get the point . <p> HTML has emerged as a @ @ @ @ @ @ @ @ @ @ application development . In this session , you learn how to leverage your existing HTML and JavaScript skills to build cross-platform mobile applications , how to access the device features ( camera , accelerometer , contacts , file system , etc ) using JavaScript APIs , and how to package your HTML application as a native app for distribution through the different app stores . <p> Second , a video of my presentation on " Architectural Considerations for PhoneGap and Mobile Web Apps " has been published by the Atlanta HTML5 meetup group . - Check it out in the video below , and if you 're in the Atlanta area , be sure to check out the meetup group ! - Heres the presentation description : <p> Tired of Hello World ? In this session , we explore best practices to build real-world PhoneGap applications . We investigate the Single Page Architecture , HTML templates , effective Touch events , performance techniques , modularization and more . We also compare and contrast the leading JavaScript and Mobile Frameworks . This session is a must If you plan to build a @ @ @ @ @ @ @ @ @ @ . 
@@106848716 @2248716/ <h> Monthly Archives : May 2013 <p> The update to Creative Cloud that is coming in June is loaded with awesome tools and incredible new features . I recently demonstrated shake reduction in Photoshop , which can greatly enhance photos that are blurred from a shaky camera , but that 's not the only great update coming in June . Another feature that I wanted to highlight is the Sound Remover- process in Adobe Audition CC . <p> The Sound Remover enables you to select specific sound frequencies and patterns , and remove them from a sound file/composition . Imagine that you have a great recording which was ruined by a cell phone ringing , birds chirping in the background , or someone slamming a door . Now it is possible to easily remove those specific frequencies and patterns without losing or damaging the entire audio file . Check out the video below for an example . <p> In a nutshell , the process is this : <p> In the Spectral Frequency Display , use the paintbrush selection tool to select the frequencies and patterns you want to remove . @ @ @ @ @ @ @ @ @ @ Reduction feature coming to Adobe Creative Cloud members this June in- Photoshop CC , it is a new filter that will analyze your photos to detect and correct blurring due to a shaky camera . Yes you read that correctly , it can make a shaky/blurry photo LESS shaky/blurry . Its an awesome new feature that can be used to enhance photos under low light conditions or taken with slow shutter speeds . Check out the official teaser video below : <p> Like any other curious individual , I wanted to test this out on my own , with my own photographs . So , I broke out my camera , set it to have a slow shutter speed ( for intentionally blurry photos ) , and started walking around and taking pictures . I was completely blown away by the outcome . <p> Let 's take a look at the results <p> In all of the following examples , the image on the left is the original . The image on the right is the enhanced/corrected image . The only changes that I have made are camera shake correction and @ @ @ @ @ @ @ @ @ @ touchup/enhancement or alteration techniques . Its amazing how much detail was able to be pulled out of the original blurred images . <p> First , let 's take a look at an office building . The enhanced image has much more detail , including brick textures , more readable/discernable content , and while it has some minor artifacts , it is a huge improvement . <p> Click to view full size image <p> Now , let 's look a little bit closer If you 've already glanced at the image below , you 'll see that I was able to extract the license plate from a car that just happened to be passing at the time I took the photo . Seriously , this is real . All of those " image enhancement " sequences that you 've seen in crime and sci-fi movies and TV shows ( which youve probably rolled your eyes at ) are now a reality . Still do n't  believe me ? Take a look at this video I put together for a closer look . <p> Note : I edited this down to the " short version " , @ @ @ @ @ @ @ @ @ @ and permutations of the shake reduction tools to achieve the final result . <p> That 's not all , I was able to extract detail about other items in the photograph . For example , You can now see enough detail to identify the light bulb in the street lamp . <p> You can even make out the faint outline of the bicycle and arrow on the " bike lane " street sign behind the car . <p> Pretty cool , right ? Now , let 's take a look at another example . Again , the blurry image on the left is the *original* . <p> Click to view full size image <p> You can see that the enhanced image is sharper and has more detail . Let 's look a bit closer <p> In the original image , did you see that guy in the blue shirt standing walking towards the white van ? Yeah , neither did I : <p> Or , could you tell what kind of cars were in the parking lot ? I know I could n't in the original , but I can make a much better @ @ @ @ @ @ @ @ @ @ let 's take a look at one more . In the enhanced photo on the right , you can see much more detail everything from the brick patterns , to street signs , to details on people and objects . <p> Click to view full size image <p> While you still ca n't read the entire law office sign , you can certainly- see more detail in both the sign and the surroundings . <p> . <p> You can also make out much more detail on people , doorways to buildings , flowers , and leaves on the trees . <p> Again , - The only changes that I have made are camera shake correction and minor color/contrast corrections . There have been no other modifications made to these images . <p> All of my test images have fairly significant shake blurs . Due to the extreme amount of blur , there are some visual artifacts . Any images that have a less extreme blur will have fewer artifacts , with enhanced clarity . <p> You can reduce artifacts and improve the final output quality by experimenting with the Camera Shake @ @ @ @ @ @ @ @ @ @ blur , and adjust trace bounds , smoothing , and artifact- suppression- for each . You 'll likely need to experiment with the settings and try different permutations to achieve optimal results , but you 'll be amazed by the outcome . <p> Camera Shake Reduction Options <p> Its important to understand that while it can " work miracles " on some images , the Shake Reduction feature will not correct all blur in all types of images . If the blurring is because your image is simply out of focus , this probably wont help . Again , it is intended to correct blurs caused by a shaky camera situation . <p> If you 're wondering how to get this feature , just go over to creative.adobe.com and become a Creative Cloud member today ! Photoshop CC will be available in June , and you 'll be notified . <p> One of my first tasks upon downloading Android Studio was to get a PhoneGap app up and running in it . here 's how to get started . Note : I used PhoneGap 2.7- to create a new project with- the latest stable release @ @ @ @ @ @ @ @ @ @ the CLI create ) to import an already-existing PhoneGap application . Be sure to backup your existing project before doing so , just in case you have issues ( Android Studio is still in beta/preview ) . <p> Next , you 'll have to select the directory to import . Choose the directory for the PhoneGap project you just created via the command line tools . <p> Once you click " OK " , you will proceed through several steps of the import wizard . On the next screen , make sure that " Create project from existing sources " is selected , and click the " Next " button . <p> You will next specify a project name and project location . Make sure that the project location is the same as the location you selected above ( and used in the PhoneGap command line tools ) . I noticed that the default setting was to create a new directory , which you do not want . Once you 've verified the name and location , click " Next " . <p> On the next step , leave the default settings @ @ @ @ @ @ @ @ @ @ . <p> Android Studio should now open the full IDE/editor . You can just double click on a file in the " Project " tree to open it . <p> To run the project , you can either go to the " Run " menu and select " Run project name " , or click on the " Run " green triangle icon . <p> This will launch the application in your configured environment ( either emulator or on a device ) . You can see the new PhoneGap application running in the Android emulator in the screenshot below . If you 'd like to change your " Run " configuration profile , go to the " Run " menu and select " Edit Configurations " , and you can create multiple launch configurations , or modify existing launch configurations . <p> here 's a great video introduction to Adobe Creative Cloud . I strongly- recommend- taking a moment to watch it if you have any questions about Creative Cloud , even if you 're already are a member . - Its only two and a half minutes , so it wo n't take @ @ @ @ @ @ @ @ @ @ week , you may not have noticed that the Adobe Edge Web Fonts got a huge upgrade too ! - Its now easier than ever to browse web fonts and include them into your own HTML experiences . All for free , with no Creative Cloud membership required ! <p> Adobe Edge Web Fonts <p> Check out the video below to see the new interface in action : <p> Also shown in the video is- Adobe Edge Code- for live editing/previewing HTML in the browser. 
@@106848718 @2248718/ <h> Pushing Data to a PhoneGap Web View <p> In PhoneGap 1.5 , the naming conventions were changed to use the Apache Cordova naming conventions . - EX : PGPlugin is now CDVPlugin , phonegap.js is now cordova.js , etc - If you are running into issues , please be sure to check the PhoneGap version and appropriate naming conventions . <p> ORIGINAL POST : <p> In my last post , which gave a- crash course in PhoneGap Native Plugins , I discussed a scenario where you could use a native plugin and the writeJavascript function to " push " data into the UI/web view layer from the native code layer . - To elaborate on this scenario , I put together a sample application that demonstrates this concept in action . <p> I set up the sample application so that there is an extremely basic HTML user interface . - In that user interface , you can click a link which executes native code to start a timer ( in the native code layer ) . - Once the timer is started , the user interface will be @ @ @ @ @ @ @ @ @ @ every timer event , the- user interface will be updated with the current time , as it is provided by the native code layer . - In this case , I am using a timer to simulate an external thread of execution , or stream of input from some other source ( device or stream ) . - Take a look at the video below to see it in action : <p> Next , let 's examine how it works . <p> The HTML interface is very simple . - There is an &lt;a&gt; link , which calls the " doStart() " JavaScript function , an &lt;h4&gt; element to display status , and an &lt;h2&gt; element to display the content received from the native code layer . When the " doStart() " function is invoked , it calls the SamplePlugin JavaScript class " start() " function , passing a reference to the " successCallback() " function , which will get executed when a callback is received from the native layer . - The " updateContent() " function will just set the innerHTML value of the " output " element to the @ @ @ @ @ @ @ @ @ @ file , which is the JavaScript component of the native plugin , you can see that there is a SamplePlugin class instance that has a " start() " function . - The " start() " function invokes the PhoneGap.exec() command to invoke native code . <p> In the native code , there is a basic class that extends the PGPlugin class . - When the user invokes the " start() " function in JavaScript , the native " start " function will be invoked . - In this function , output is appended to the console using NSLog , and a NSTimer instance is started . - Once the timer is started , a success callback will be sent back to the HTML/JavaScript layer . <p> Now , the timer is executing indefinitely in the native code . - You will notice that in the timer handler " onTick " function , a message is written to the console , and a JavaScript string is created and routed to the UI layer using writeJavascript. - In this case , the JavaScript string just contains the current date as a string @ @ @ @ @ @ @ @ @ @ is executed , it will invoke the updateContent() JavaScript function , which will then update the HTML user interface . <p> While this example is extremely basic , you could use this exact same approach to push more complex data to the UI layer in real time . - This data could be coming from network activity , Bluetooth connections , or physical devices that are connected to the mobile device . 123433 @qwx983433 <p> Hello Andrew , this is a nice post and just touches what I am intended to ask . I have already posted the question on StackOverflow , but seems no ones responding . The problem in a few words is to **initiate a request from native and take it to JS** . In your example above you have to first click on the text " Start " and then the " exec " is called further calling the execute() on the native side . This is known to everyone , but how to do vice-versa is my question . 
@@106848719 @2248719/ <h> Why Cross Platform Mobile Development ? <p> Perhaps you have heard of the topic " cross platform development " , but are n't  really sure what it is , or you are n't  sure why you would want to use cross-platform technologies . If this is the case , then this post is especially for you . Ill try to she 'd some light onto what it is , and why you would want to use cross-platform development strategies . <h> What is cross-platform development ? <p> Cross platform development is a concept in computer software development where you write application code once , and it runs on multiple platforms . This is very much inline with the " write once , run everywhere " concept pioneered in the 90s , and brought to a mainstream reality with Flash in the browser , and AIR on the desktop . The standard evolution of technology has been to make everything faster , smaller , and more portable , and it is only natural that that this concept has now come into the mobile development world . In mobile scenarios , @ @ @ @ @ @ @ @ @ @ &amp; technology that allows the application to be deployed and distributed across multiple disparate platforms/operating systems/devices . <p> In case you 're wondering why I offered 2 cross platform technologies , that is because Adobe will soon have 2 cross-platform product offerings . - Adobe has entered an- agreement to purchase Nitobi , the creators of PhoneGap . <h> Adobe AIR <p> Adobe AIR is a cross-platform technology with roots in the Flash Player and the AIR desktop runtime. - AIR allows you to build cross-platform mobile applications using ActionScript and the open source Flex framework . - AIR apps can be built from the Flash Professional timeline-based design/animation tool , Flash Builder ( an Eclipse-based development environment ) , or other open source solutions using the freely available AIR SDK. - Applications developed with Adobe AIR can target desktop platforms ( Mac , &amp; Windows ) , smart phone and tablet platforms ( iOS , Android , BlackBerry , soon Windows ) , and even smart televisions . <h> PhoneGap ( Apache Callback ) <p> PhoneGap is an open source cross platform technology with roots in the HTML world . @ @ @ @ @ @ @ @ @ @ that consumes 100% of the available width &amp; 100% of the available height , taking advantage of web browsers on each platform . - PhoneGap offers a JavaScript to native bridge that enables you to build natively-installed applications using HTML and JavaScript , using the native bridge to interact with the device hardware/APIs. - Note : PhoneGap is also being submitted to the Apache Foundation as the Apache Callback project . <h> More Devices , Less Code <p> The driving factor behind cross-platform technologies is that you will be able to use those technologies to target more devices &amp; platforms , with writing a minimal amount of source code . - There are many advantages with this approach . - Here are a few of the major reasons <h> Lower Barrier of Entry <p> Generally speaking , development with HTML &amp; JavaScript or Flex &amp; ActionScript is easier than developing with Objective-C or Java . - Due to the ease of use of the development tooling and familiarity of the languages , cross platform technologies lower the technical barriers which may have prevented adoption of native development . - This @ @ @ @ @ @ @ @ @ @ not previously have been able to , and also enables your team to focus on what matters the application ; not the skills required to develop on multiple disparate platforms . <h> Reduce the Number of Required Skills for the Development Team <p> Native development on multiple platforms requires your development team to learn Objective C for iOS applications , Java for Android applications , Silverlight for Windows Phone applications , etc - Finding all of these skills in a single developer is nearly impossible . - Using cross-platform development technologies , your team only needs to be proficient with one language/skillset. - Knowledge of the native development paradigms and languages are always a plus , but are no longer a requirement . - - Many developers transitioning from web development already know either Flex/ActionScript and/or HTML/JavaScript , and making the transition from web to mobile development will not be a major undertaking . <h> Reduced Development &amp; Long Term Maintenance Costs <p> Cross-platform mobile applications can originate from a single codebase , which requires a single development skillset. - You do n't  need to have staff for each individual @ @ @ @ @ @ @ @ @ @ codebase can cover all target platforms . - Having a single codebase also reduces long term maintenance costs . - You no longer need to have bug tracking for X number of codebases , and do not need to maintain a larger staff to support each platform . - Did I also mention that you have one codebase to maintain ? <p> Having a single codebase does n't  reduce the need for QA/testing on each target platform nothing can get rid of this . - It is absolutely imperative that you test your codebase on physical devices for all platforms that you intend to support . - Emulators and Simulators can go a long way during development , but they will undoubtedly not cover all scenarios possible on a physical device , and they will not have the same runtime performance as a physical device . <h> Play the Strengths of a Technology <p> Some technologies make tasks easier than others . - For example , programmatic drawing and data visualization are very easy using Flex &amp; ActionScript. - Developing equivalent experiences in native code can be significantly more complex @ @ @ @ @ @ @ @ @ @ the language to their fullest potential , to your advantage- that 's why they exist . <p> Mobile application development is a field that is getting diverse and the user base for mobile apps is ever expanding . In this tech savvy era cross platform has become the first choice of developers because it gives so many nice facility to the developers like easy marketing , maintenance &amp; deployment becomes easy , good look and feel and most important it reduce development cost . <p> With the mobile industry taking over the world in the past few years , there has been a steady and continuous demand for Smartphone applications.Phonegap suggests the ability of applications to support multiple platforms or various operating systems . This revolutionary innovation helped businesses across the globe to ensure more users are engaged by providing them access to apps that can run on both iOS and Android platforms.Developing apps that are compatible with all the major operating systems is the best option to go for . <p> http : //www.mobilepundits.com/ alva christi <p> Depending on the type of applications being developed , the number of platforms @ @ @ @ @ @ @ @ @ @ communication app or a weather app would be developed so that users of multiple platforms can benefit from it , but a specific app need not be developed to support all the major operating systems . So , the benefits associated with cross platform mobile app development are many . It allows fast and easy marketing as mobile caters to a vast customer base.I believe phonegap is the perfect platform for mobile app developers who want to start with mobile application development and can leverage their existing skills on a common platform rather than with a device-specific compiled LONG ... 
@@106848725 @2248725/ <p> First , you can make so-so video look great with a few simple color correction techniques . Second , a video is only as good as its audio , so you need solid audio to keep viewers engaged . - Hopefully this post helps you improve your videos with simple steps on both of these topics . <p> To give you an idea what I 'm talking about , check out this before and after video . Its the exact same clip played twice . - The first run through is just the raw video straight from the camera and mic. - Colors do n't  " pop " , its a little grainy , and the audio is very quiet . - The second run through has color correction applied to enhance the visuals , and also has processed audio to enhance tone , increase volume , and clean up artifacts . <p> Let 's first look at color correction . - Below you can see a " before " and " after " still showing the effects of color correction . - The background is darker and has less @ @ @ @ @ @ @ @ @ @ are warmer . <p> Before and After Color Correction <p> The visual treatment was achieved using two simple effects in Adobe Premiere Pro . - First I used the Fast Color Corrector to adjust the input levels . - By bringing up the black and gray input levels , the background became darker , and it reduced grain in the darker areas . - Then , I applied the " Warm Overall " Lumetri- effect to make the video feel warmer this enhances the reds to add warmth to the image . <p> You can get by with a mediocre video with good audio , but nobody wants to sit through a nice looking video with terrible audio . Here are three simple tips for Adobe Audition to help improve your audio , and hopefully keep viewers engaged . <p> In this case , I thought the audio was too quiet and could be difficult to understand . - My goal was to enhance audio volume and dynamics to make this easier to hear . <p> I first used Dynamics Processing to create a noise gate . This process removes @ @ @ @ @ @ @ @ @ @ louder sounds , and generally cleaner audio . - You could also use Noise Reduction or the Sound Remover effects the effect that works best will depend on your audio source . <p> Dynamics Processing ( Noise Gate ) in Adobe Audition <p> Next I used the 10-band graphic equalizer to enhance sounds in specific frequency ranges . - I brought up mid-range sounds to give more depth to the audio track . <p> 10 Band EQ in Adobe Audition <p> Finally , I used the Multiband Compressor to enhance the dynamic range of the audio . - Quieter sounds were brought up and louder sounds were brought down to create more level audio that is easier to hear and understand . - However , be careful not to make your audio too loud when using the compressor ! - If you 've ever been watching TV and the advertisements practically blow out your eardrums , this is because of overly compressed audio . <p> Multi-band Compressor in Adobe Audition <p> Want to learn more ? - Do n't  miss the Creative Cloud Learn resources to learn more about all of @ @ @ @ @ @ @ @ @ @ everyone ! - If you are n't  already a member , join Creative Cloud today to access all Adobe media production tools . 
@@106848726 @2248726/ <h> Category Archives : IBM <p> Its not every day that you get the opportunity to have your work showcased front and center on the main landing page for one of the largest companies in the world . Well , today is definitely my lucky day . I was interviewed last month about a drone-related project that I 've been working on that focuses on insurance use cases and safety/productivity improvement by using cognitive/artifical intelligence via IBM Watson . I knew it was going to be used for some marketing materials , but the last thing that I expected was to have my image right there on ibm.com . I see this as a tremendous honor , and am humbled by the opportunity and exposure . <p> here 's an interview that I recently did with IBM DeveloperWorks TV at the recent World of Watson conference . In it I discuss a project Ive been working on that analyzes drone imagery to perform automatic damage detection using the Watson Visual Recognition , and generates 3D models from the drone images using photogrammetry processes . The best part the entire thing runs @ @ @ @ @ @ @ @ @ @ a while since I 've posted here on the blog - In fact , - I just did the math , and- its been over 7 months. - Lots of things have happened since , I 've moved to a new team within IBM , built new developertools , worked directly with clients- on their solutions , worked on a few high profile keynotes , built apps for kinetic motion and activity tracking , built a mobile client for a chat bot , and even completed some new drone projects . - Its been exciting to say the least , but the real reason I 'm writing this post is to share a few- of the public projects Ive been involved with from recent conferences . <p> I recently returned from Gartner Symposium and IBMs annual World of Watson conference , and- its been one of the busiest , yet most exciting span of two weeks Ive experienced- in quite a while . <p> At both events , we showed a project Ive been working on with IBMs Global Business Services team that focuses on the use of small consumer drones and drone @ @ @ @ @ @ @ @ @ @ by leveraging IBM Watson to automatically detect roof damage , in conjunction with photogrammetry to create 3D reconstructions and generate measurements of afflicted areas to expedite and automate claims processing . <p> This application leverages many of the services IBM Bluemix has to offer on-demand CloudFoundry runtimes , a Cloudant NoSQL database , scalable Cloud Object Storage ( S3 compatible storage ) , and BareMetal servers on Softlayer . Bare Metal servers are *awesome* I have a dedicated server in the cloud that has 24 cores ( 48 threads ) , 64 GB RAM , RAID array of SSD drives , and 2 high end multi-core GPUs . Its taken my analysis processes from 2-3 hours on my laptop down to 10 minutes for photogrammetric reconstruction with Watson analysis . <p> Its been an incredibly interesting project , - and you can check it out yourself in the links below . <h> World of Watson <p> World of Watson was a whirlwind of the best kind I had the opportunity to join IBM SVP of Cloud , Robert LeBlanc , on stage as part of the the Cloud keynote at @ @ @ @ @ @ @ @ @ @ people ) to show off the drone/insurance demo , plus 2 more presentations , and an " ask me anything " session on the expo floor . <p> You can also check out my session " Elevate Your apps with IBM Bluemix " on UStream to see an overview in much more detail : <p> .. and that 's not all . I also finally got to see a complete working version of the Olympic Cycling teams training app on the expo floor , including cycling/biometric feedback , video , etc I worked with an IBM JStart team and wrote the video integration layer into for the mobile app using IBM Cloud Object Storage and Aspera for efficient network transmission . <h> Drones <p> On this project we 've been working with a partner DataWing , who provides drone image/data capture as a service . However , I 've also been flying and capturing my own data . The app can process virtually any images with appropriate metadata , but I 've been putting both the DJI Phantom and Inspire 1 to work , and they 're working fantastically . <p> MobileFirst Platform Foundation provides @ @ @ @ @ @ @ @ @ @ mobile apps easier , improves- security through encryption , authentication- and handshaking to guarantee app authenticity , - provides facilities- to easily manage multiple versions of an app , notify and engage users , and , on top of everything else , provides operational analytics so that you can monitor the health of your overall system at any point in time . <p> As a mobile developer catering to the enterprise , it makes your life significantly- easier , and it supports any mobile development- paradigm- that you might want to target : Native platforms , hybrid Xamarin using C# , and hybrid Cordova platforms ( HTML/JS ) . <h> What 's new in the IBM MobileFirst Platform Foundation 8.0 Beta ? <p> The recently opened beta has some great new features , AND its now available as a service on Bluemix ( IBMs Cloud platform ) . - The beta program is intended to deliver the next generation of an open , integrated and comprehensive mobile app development platform redesigned for cloud agility , speed , and productivity , that enables enterprises to accelerate delivery of their mobile strategy . @ @ @ @ @ @ @ @ @ @ for IBM InterConnect IBMs largest conference of the year . With over 20,000 attendees , it was a fantastic event that covered everything from technical details for developers to forward-looking strategy and trends for C-level executives . IBM also made some big announcements for developers OpenWhisk serverless computing and bringing the Swift language to the server just to name a few . Both of these are exciting new initiatives- that offer radical changes &amp; simplification to developer workflows . <p> It was a busy week to say the least lots of presentations , a few labs , and even a role in the main stage Swift keynote . You can expect to find more detail on each of these here on the blog in the days/weeks to come . <p> For starters , here are two " lightning talks " I presented in the InterConnect Dev@ developer zone : <h> Smarter apps with Cognitive Computing <p> This session introduces the concept of cognitive computing , and demonstrates how you can use cognitive services in your own mobile apps . - If you are n't  familiar with cognitive computing , then @ @ @ @ @ @ @ @ @ @ The Future of Cognitive Computing . <p> In the presentation below , I show two apps leveraging services on Bluemix , IBMs Cloud computing platform , and the iOS SDK for Watson . <p> Actually , I 'm using two Watson SDKs The older Speech SDK for iOS , and the new iOS SDK. - I 'm using the older speech SDK in one example because it supports continuous listening for Watson Speech To Text , which is currently still in development for the new SDK . <h> Redefining your personal mobile expression with on-body computing <p> My second presentation highlighted how we can use on-body computing devices to change how we interact with systems and data . - For example , we can use a luxury smart watch ( ex : Apple Watch ) to consume and engage with data in more efficient and more personal ways . - Likewise , we can also use smart/wearable peripherals devices to access and act on data in ways that were- never possible- before . <p> For example , determining gestures or biometric status based upon patterns in raw data transmitted by the on-body @ @ @ @ @ @ @ @ @ @ IBM Wearables SDK. - The IBM Wearables SDK provides a consistent interface/abstraction layer for interacting with wearable sensors . - This allows you to focus on building your apps that interact with the data , rather thank learning the ins &amp; outs of a new device-specific SDK . <p> The wearables SDK also users data interpretation algorithms to enable you to define gestures or patterns in the data , and use those patterns to act upon events when they happen without additional user interaction . - For example : you can determine if someone falls down , you can determine when someone is raising their hand , you can determine anomalies in heart rate or skin temperature , and much more . - The system is capable of learning patterns for any type of action or virtually any data being submitted to the system . - Sound interesting ? - Then check it out here . <p> I also had some other- sessions- on integrating drones with cloud services , integrating weather services in your mobile apps , and more . - I 'll be sure to post updates for this- @ @ @ @ @ @ @ @ @ @ you 'll find the session on drones + cloud especially interesting I know I did . 
@@106848728 @2248728/ <h> Apple WWDC Recap for Mobile Devs <p> I 'm sure you 've already heard Apples big announcements from the- annual- Worldwide Developer Conference- this week . - I was lucky enough to snag a ticket in Apples lottery and got to check it all out in person . There were lots of great sessions , with tons of content . - Here are the highlights as I saw them from a mobile developers perspective *not* from the general consumer point of view . - For the most part , I think this years announcements highlighted the evolution and maturity of existing products and projects no new amazing breakthoughs , but definitely steps in the right direction . <p> If you have n't seen them already , the Keynote and the Platforms State of the Union videos cover most of the- announcements , but not in complete detail . Just be warned , the Keynote is loaded with product marketing fluff , not just developer topics . - Once you- get to " weve got one more thing " you can turn off the Keynote the Apple Music announcement has pretty much @ @ @ @ @ @ @ @ @ @ <h> Swift 2.0 <p> There was a tremendous emphasis on the Swift language at this years WWDC event . - There was the announcement that Swift is going to be open sourced , plus many language enhancements , and nearly every piece of sample code that was shown was written in Swift . - It is very clear that Swift is Apples direction moving forward . <p> I think the open souring of Swift is a big deal b/c it opens up the language for use beyond just iOS and OSX applications . - Think about it Perhaps another platform might adopt Switft to develops apps ( Windows ? ) , or let 's hypothetically say you really like Node.js on the backend b/c its the same language as your web front end ( JavaScript , that is ) . What if you are developing native apps , and youd like to write your back end in the same language as the front end mobile client , or what if you want an ECMAScript inspired language that is more structured than Node , with real Object Oriented or functional programming constructs @ @ @ @ @ @ @ @ @ @ multi-threaded ) ? - Swift is your answer . I 'm willing to bet that we will see server-side Swift not long after it is open sourced . - Let 's just hope that Swift is opened in the truest sense you know , actually accepting input and contributions from external parties . <p> The Swift language itself has also evolved quite significantly . - Better error handling , protocol extensions , and improved performance are a great start . - Heck , if I understood one of the speakers correctly , its now even faster than Objective C at runtime in some cases . <p> Want to learn more about Swift ? - Check out these session videos from WWDC ( requires Safari ) : <h> OS- Improvements <p> New versions of both OS X and iOS were announced and released to - developers OS X El Capitan and iOS 9 respectively . - Both seem to be incremental updates of the previous OSes . New apps , new features , etc for the end users . - Not necessarily significant changes for developers . - If you 're a graphics programmer @ @ @ @ @ @ @ @ @ @ low level graphics/gpu API ) , but if you 're not a graphics guru , you probably wont even know its there . <h> iPad Multitasking <p> The new iOS 9 **25;1373;TOOLONG mode for iPad is going to be a great addition which brings the iPad even closer to being a full laptop replacement . - Having the ability to have multiple apps open next to each other will improve the- iPads- " get $h1t done " ability . - Youll have to ensure that youve- authored your apps to leverage adaptive layouts , but that 's pretty much all that you need to do to take advantage of iPad Multitasking . <p> These videos will get you going in the right direction for iOS multitasking and adaptive layouts : <h> App Thinning <p> The new " App Thinning " features in Xcode 7/iOS 9 are also a great addition . - Currently if you build an iOS app it gets bundled with lots of resources that may never be used depending on the type of device . - App thinning introduces three concepts that help minimize the footprint and increase the @ @ @ @ @ @ @ @ @ @ Demand Resources , and Bitcode. - According to the presenters , these can decrease the download/installed size of your apps quite significantly . <p> App Slicing- is a new feature that creates variants of your app executable depending on the device that you are downloading the app to . So , if your app does n't  use @3x graphics , or does n't  use the arm7s architecture on a particular device , then they wont be downloaded. - Likewise , if your device- does- leverage- those assets , then the other smaller scale assets and non-used binaries wont be downloaded . <p> App Slicing from iOS Docs <p> On Demand Resources- give you the ability to download specific sets of resources from the app store as they are needed . - They are still hosted by the app store , but not part of the initial download. - Let 's say you are building a platform game . - Initially the shell/navigation assets will be downloaded. - While the app is running you 'll be able to download assets for level 1 , level 2 , level 3 , etc incrementally as @ @ @ @ @ @ @ @ @ @ up ODR resources to conserve space using a least-recently-used cleanup routine . <p> Bitcode is an intermediate representation of a compiled program . Apps you upload to iTunes Connect that contain bitcode will be compiled and linked on the App Store . Including bitcode will allow Apple to re-optimize your app binary in the future without the need to submit a new version of your app to the store . <p> Bitcode enables the app store to re-compile your code to take advantage of new LLVM optimizations without you even having to recompile and upload a new application binary . <h> UI Testing <p> The new UI testing features in Xcode 7 look pretty awesome as far as automated UI testing goes . - It enables you to record/playback steps and generated UI unit tests all from within Xcode. - What 's even better , it enables you to set breakpoints within your tests , so you can debug why your tests might be failing , or you can set breakpoints inside of your app , and the automated testing stops at the breakpoints and allows you to step through code @ @ @ @ @ @ @ @ @ @ not miss the session on UI Testing in Xcode 7 if you have any ( even remote ) interest in automated UI testing , it looks pretty darn useful . <h> Improved Search and Deep Linking <p> Improved search functionality was also announced for both iOS and OS X. - This improves the search functionality , and also enables your apps to index their content , so using the device search enables you to search for information hosted *inside* of the app. - To complement the enhanced search , there are also features that better facilitate deep linking into your app. - This enables apps to be launched directly into the appropriate content/context with greater ease . - I need to look into this more , but it sounded interesting <h> watchOS 2 <p> Last , but by certainly no means least , the announcement of watchOS 2- looks like a massive leap forward for developing for the Apple Watch . <p> WatchOS 2 brings us the ability to execute code natively on the- Apple Watch , not just in the WatchKit extension running on your iPhone , brings us @ @ @ @ @ @ @ @ @ @ to network connectivity if your phone is not connected , - support for multimedia , - and direct access to hardware sensors . - If you 're wondering what " watch complications " are , they are the widgets on the watch face that enable you to display customized information . <p> WatchOS Complications <p> You should definitely check out the videos on developing for the Apple Watch if you have any interest in- watchOS : <p> There are also new APIs , enhanced features in CloudKit , MapKit , HomeKit , Core Motion , Core Location , updates to Apple Pay , security updates , networking updates , and lots more . - Be sure to check out the complete list of WWDC videos- for more . <p> There was so much to absorb , I 'm sure I missed something , so feel free to point anything out that I 've overlooked ! 
@@106848729 @2248729/ <h> Sound Remover in Adobe Audition CC <p> The update to Creative Cloud that is coming in June is loaded with awesome tools and incredible new features . I recently demonstrated shake reduction in Photoshop , which can greatly enhance photos that are blurred from a shaky camera , but that 's not the only great update coming in June . Another feature that I wanted to highlight is the Sound Remover- process in Adobe Audition CC . <p> The Sound Remover enables you to select specific sound frequencies and patterns , and remove them from a sound file/composition . Imagine that you have a great recording which was ruined by a cell phone ringing , birds chirping in the background , or someone slamming a door . Now it is possible to easily remove those specific frequencies and patterns without losing or damaging the entire audio file . Check out the video below for an example . <p> In a nutshell , the process is this : <p> In the Spectral Frequency Display , use the paintbrush selection tool to select the frequencies and patterns you want to remove . 
@@106848730 @2248730/ <p> I recently gave a presentation at IBM Insight on Cognitive Computing in mobile apps . - I showed two apps : one that uses Watson natural language processing to perform search queries , and another that uses Watson translation and speech to text services to take text in one language , translate it to another language , then even- have the app play back the spoken audio in the translated language . - Its this second app that I want to highlight today . <p> In fact , it gets much cooler than that . - I had an idea : " What if we hook up an OCR ( optical character recognition ) engine to the translation services ? " That way , you can take a picture of something , extract the text , and translate it . - It turns out , its not that hard , and I was able to put together this sample app in just under two days . - Check out the video below to see it in action . <p> To be clear , I ended up using a @ @ @ @ @ @ @ @ @ @ . This is not based on any of the work IBM research is doing with OCR or natural scene OCR , and should not be confused with any IBM OCR work . - This is basic OCR and works best with dark text on a light background . <p> The Tesseract engine let 's you pass in an image , then handles the OCR operations , returning you a collection of words that it is able to extract from that image . - Once you have the text , you can do whatever you want from it . <p> So , here 's where Watson Developer Cloud Services come into play . First , I used the Watson Language Translation Service to perform the translation . - When using this service , I make a request to my- Node.js app running on IBM Bluemix ( IBMs cloud platform ) . - The Node.js app acts as a facade and delegates to- the Watson service for the actual translation . <p> You can check out a sample on the web here : <p> Translate english to : <p> On the mobile client , @ @ @ @ @ @ @ @ @ @ something with the response . The example below uses the IMFResourceRequest API to make a request to the server ( this can be done in either Objective C or Swift ) . IMFResourceRequest is the MobileFirst wrapper for networking requests that enables the MobileFirst/Mobile Client Access service to capture operational analytics for every request made by the app . <p> Once you receive the result from the server , then you can update the UI , make a request to the speech to text service , or pretty much anything else . <p> To generate audio using the Watson Text To Speech service , you can either use the Watson Speech SDK , or you can use the Node.js facade again to broker requests to the Watson Speech To Text Service . In this sample I used the Node.js facade to generate Flac audio , which I played in the native iOS app using the open source Origami Engine library that supports Flac audio formats . <p> You can preview audio generated using the Watson Text To Speech service using the embedded audio below . Note : In this sample @ @ @ @ @ @ @ @ @ @ work in browsers that support OGG . <p> On the native iOS client , I download the audio file and play it using the Origami Engine player . This could also be done with the Watson iOS SDK ( much easier ) , but I wrote this sample before the SDK was available . <p> Cognitive computing is all about augmenting the experience of the user , and enabling the users to perform their duties more efficiently and more effectively . The Watson language services enable any app to greater facilitate communication and broaden the reach of content across diverse user bases . You should definitely check them out to see how Watson services can benefit you . <h> MobileFirst <p> So , I mentioned that this app uses IBM MobileFirst offerings on Bluemix . In particular I am using the Mobile Client Access service to collect logs and operational analytics from the app . This let 's you capture logs and usage metrics for apps that are live " out in the wild " , providing insight into what people are using , how they 're using it , and the @ @ @ @ @ @ @ @ @ @ <h> Source <p> You can access the sample iOS client and Node.js code at https : **38;1400;TOOLONG . Setup instructions are available in the readme document . I intend on updating this app with some more translation use cases in the future , so be sure to check back ! 
@@106848731 @2248731/ <p> I think this camera is awesome ! Its really pretty amazing While it might seem " bare bones " without a view finder or zoom capabilities , it is very rugged , it takes great still photos ( 12MP ) , and even better video ( WVGA at 240 FPS all the way to 4K at 15 FPS ) . It excels in bright light and action shots , and is a very versatile and impressive camera . You can also use the GoPro app- on your iOS , Android , or Windows Phone device as a remote viewfinder and controller , which is very cool. - Though the GoPro is not without its quirks/annoyances . <h> Bottom Line : <p> Awesome camera. - I want multiple of them . For video , either 1080p or 720p video is more than adequate for most circumstances . Audio capture is okay , but its better if you have an external mic and can sync audio in post-production . While 4K video is seriously awesome , it kills the battery very quickly , has HUGE file sizes , and is n't @ @ @ @ @ @ @ @ @ @ there is significant fisheye distortion from the 170 degree lens . While this looks fantastic in a lot of cases , sometimes we do n't  want it Luckily we can get rid of the fisheye distortion with Photoshop ! <p> The camera has some learning curve , but the output is very good , and it is very portable and compact ( though , you may end up with more accessories that take up more space ) . <h> Notes : <p> Here are some notes about the camera , mostly focused on using it as a lightweight travel camera : <h> Images &amp; Videos Produced : <p> Very good quality <p> Images and videos look great in decent light , but the camera does n't  function well in very low-light situations . <h> Pros : <p> Wide angle lens great for capturing landscapes or presentation stage/projector ( You only need the camera to be about 6-10 feet away for a decent stage coverage area ) <p> Great in both indoor and outdoor environments <p> Small , very portable <h> Cons : <p> No view finder/lcd screen , though you @ @ @ @ @ @ @ @ @ @ wall charger included , however you can charge it by plugging the USB adapter into an iPad ( or other USB ) power supply <p> Relatively short battery life when capturing video <p> Video files are huge ( though it is because they are such high quality video ) <h> Quirks &amp; Annoyances : <p> If you unplug it from your computer without ejecting , it will lock up the camera . You have to remove the memory card , then remove the battery and reinstall both before the camera will reboot . <p> You can record video while plugged into a power source However , you MUST start recording before you plug it in , otherwise it wo n't let you record while plugged into computer or wall . <p> If you improperly shut down the camera , the most recently recorded video may be corrupt . This can happen if the camera is dropped or jarred from a crash . If this happens , just re-insert the memory card , then reboot the camera . It will go into recover mode , and fix the corrupted file . If @ @ @ @ @ @ @ @ @ @ able to recover the video from the memory card , but there is no guarantee this has happened to me . <p> The camera does not automatically turn off . You must manually power off the camera when not in use . This can kill your battery unexpectedly if you do n't  power off , however you can also change this behavior in the GoPro settings . <p> With the battery pack connected and the " frame " mount connected , you might get a " hum " in the audio you can reduce this in Adobe Audition , but if you have an external mic , then you do n't  have to worry . I 'm still not 100% sure if it was the camera/backpack , or something else in the room that caused the audio interference . <p> Let the camera finish whatever it is doing when you are done recording . Do n't  force a power-off , otherwise you might lose whatever is in memory buffer on the camera I know this from experience &amp; recently lost about 20 minutes of video . <p> Opening the waterproof case @ @ @ @ @ @ @ @ @ @ . Once you know how to do it , its really easy . <p> Although the camera is supposed to support 64 Gig memory cards , I strongly advise not using anything greater than 32 Gigs . I ran into some issues where the GoPro corrupted a 64 Gig Sandisk memory card to the point that it was unrecognizable by a computer or by the camera . After scouring the web and contacting technical support , I had to replace the card . It turns out the particular camera firmware version was not compatible with the file format for my memory card . Since switching to 32 Gig cards , I have n't had any problems . <h> Charge Time : <p> Approximately 2 hours for a full charge from completely dead . This will take longer with battery BacPac yes , this was very unscientific , as were my battery tests <h> Battery Tests : <p> 720p , 60FPS , WiFi Disabled : Approx 1 hour ( 2 iterations ) <p> 720p , 60FPS , WiFi Disabled , with battery BacPac : Approx 90 mins ( though the BacPac was @ @ @ @ @ @ @ @ @ @ life . <p> These are more than sufficient battery times for my normal circumstances . <h> Essential Accessories : <p> Micro SD Memory Card Required , it does not come with one . I reccommend 32 Gig cards , and get multiple cards so you can quickly swap them in and out . <p> Tripod mount- ( if you want it on a tripod . ) There is only the custom GoPro mount in the package , however you can use part of the package display as an inexpensive stand ) <p> " The Frame " mount- The only mounting option that comes with it is the waterproof enclosure . This is n't great for presentations b/c you get better audio quality by just having the camera exposed . This also comes with a much needed lens cap . <p> Battery BacPac- -- Extend the life of the GoPro with a secondary battery , although if you use this , you cant attach the LCD back . <p> Some sort of bag to put everying in . There are a lot of parts and accessories , although not all of them @ @ @ @ @ @ @ @ @ @ accessories : <p> These would be nice to have , but you can get by without them : <p> Wall Charger- I use an iPad power supply just plug in the USB adapter to the iPad " wall wart " . <p> LCD Back Panel I did not get one of these , I was fine just pointing it and shooting . However , you can not use this with the battery BacPac . <h> Side by Side Comparison : <p> Here is a sample where I was recording one of Raymond Camdens presentations . Compared with a Sony Bloggie ( at exact same distance ) . Bloggie feels " closer " , but has worse video quality , worse in poor lighting , and does n't  capture the full experience of the presentation . This is without any color correction or post processing other than resizing the image : <h> Other Sample Images : <h> Conclusion : <p> My wishlist for the GoPro is that I wish it took higher than 12 MP photos . - 12 MP is actually great , but *I NEEDZ MOAR P1XELS* . I @ @ @ @ @ @ @ @ @ @ with a non-fisheye lens , but otherwise I love this camera . Yes , I would buy another one in a hearbeat ! 123433 @qwx983433 <p> Hey thanks for a great review . Just bought one and looking forward to trying it out just wondering how much video you get for your 32GB card . What runs out first battery or memory ? ? 123435 @qwx983435 <p> Battery will definitely die before the memory card . I do n't  remember exact numbers , but I think an hour of 1080p video is about 10 gigs . <p> Jeff Marshall <p> I have an LCD bakpac from the previous model Gopro , but it gets warm and kills the Gopro3 Silver battery fast . Any ideas why ? Do I need to upgrade my LCD ? It is n't a touch screen as far as I know . <p> Johan Fechter <p> Hi there , my GoPro Hero3 has sound but no video , I do n't  know what I did wrong but can not fix it . Could you please advice ! 123435 @qwx983435 <p> Sorry , I 've never seen that @ @ @ @ @ @ @ @ @ @ another computer ? It could be a computer related issue . <p> Tim Waldron <p> Thanks for the info Andrew . One question , what Frame Rate , Resolution and Wide/medium/narrow do you use when filming your presentations ? 123435 @qwx983435 <p> 1080p @ 30 fps is usually sufficient . 1080 gives you high enough resolution . I would n't go over 30 fps b/c file sizes become massive . Its not for action sports , so that 's OK . <p> http : //blogs.netapp.com/tim Tim Waldron <p> Thanks Andrew . <p> Harrison Earl <p> Hi Andrew I was just wondering as I 'm going on a trip to Switzerland soon and doing loads of activities with my Gopro what the best battery saving option is ? Gopro recomend 1080p 30fps but I was wondering weather it was better to keep the camera turned on and just film in short burst , film in short bursts but turn camera off in between bursts or just keep the camera rolling the whole time ? Thank you 123435 @qwx983435 <p> Get extra batteries . If the camera is running , it is eating battery life @ @ @ @ @ @ @ @ @ @ eat the battery faster if you 're capturing either images or video . I have 4 batteries for mine , so I can easily swap them out when one dies . AT 1080p , you 'll only get from 60-90 minutes of video . It will definitely last longer if you cut it off when you 're not using it , but I would spend the extra little bit of money to have both backup batteries and memory cards for redundancy . <p> Harrison Earl <p> Thank you : 3 the only thing I 'm scared of os it running out while I 'm rafting or cannyon like in the water and not being able to change battery , any tips ? Or is this just a just go for it and see how long it lasts lol hahha 123435 @qwx983435 <p> Just go for it ! I have used this surfing , and I only recorded when I was actually paddling for or riding a wave , I turned it off in between to save battery , and it lasted a few hours . If you 're rafting , you can record when you 're going @ @ @ @ @ @ @ @ @ @ . Bring some backups in a waterproof bag/case though , so you can swap out while you 're taking a break from rafting . <p> Harrison Earl <p> Thank you very much ! I 'm looking forward to it <p> Bhavik <p> Hi Andrew , <p> For a rafting trip would you recommend shooting video at 1080P , 30 fps or 1080P 60 fps ? I know the file size for 60 fps will be largerbut is the battery life also affected when shooting at 60 fps ? I read that 60 fps might be good when editing the video and making slow motion videosbut I am not really sure if it will make a big difference for me when I am rafting . <p> Also , have you tried the simulataneous video and photo mode . Does that eat more battery too when compared to just video ? 123435 @qwx983435 <p> 60 FPS will eat the battery faster , but the footage will be better for action shots smoother and you can slow it down without it feeling unnatural . If you 're just going to post on facebook or save it @ @ @ @ @ @ @ @ @ @ you want pro-quality results and editing , I would go for 60 fps . I do not use the video + photo mode at the same time . My assumption is that it would eat the battery faster , but I have not thoroughly tested it . I have heard that when using video+photo mode the photos do n't  look quite as good as they do normally . <p> Jasmine McKeever <p> Great explanationsI experienced my videos deleting just like you said so they 're possibly corrupted ? I lost half of my footage from today and am extremely disappointed so could you tell me what to do to try to get it back ? 123435 @qwx983435 <p> Google " GoPro footage recovery " there are a bunch of articles out there . There are 2 approaches , the first is to reboot the gopro into recovery mode ( does n't  always work ) , and the second is to use recovery software to extract the un-indexed footage data from the memory card . I used a free/open source program to do it , but unfortunately I do n't  remember the @ @ @ @ @ @ @ @ @ @ know this is late but in response to your request for MOAR P1XELS there 's a reason for that . Video and stills have different output needs . If you do the calculations , 12MP is 4000+3000. 4000 is very close to 4K , the maximum resolution of the video and that 's no co-incidence , 4K Gopro video is effectively 24/25/30 still photos every second with the top and bottom chopped off . Of course , a camera can always take a higher bunch of pixels and reduce them , giving mega mega pixels for stills and matching standards for video . The Canon DSLRs do this , for example , but this reduction needs a lot of processing power to not look terrible , with aliasing and moire and all those horrible artifacts . The early Canons ( and current prosumer models ) , before they had better processors and realised that a significant number of their DSLR cameras were being sold to video dudes , had awful aliasing and moire . The filmmaker had to make sure nobody was wearing a checked shirt at a particular distance , had @ @ @ @ @ @ @ @ @ @ had to put up with glistening aliased awfulness on distant leaves . In fact , in some video resolutions put out by the Gopro you can see this where the Gopro has reduced this 4kx3k image sensor output to a smaller resolution . It actually looks worse in resolutions , even lower ones , that for whatever reason ( high frame rates ) require more processing power . So , I 'd rather have the Gopro working optimally for video with photos being a nice bonus . Your mileage may differ . On the bright side , Moores law means we will have more processing power for our buck ( and its sometimes easy to forget a Gopro costs in the low hundreds as compared to the thousands for a half-decent semi pro video setup ) so Gopro boffins may help out here . On the other hand , I 'd personally prefer it if they put more of the processing , design and hardware budget into improved video , providing a larger sensor , better low-light performance , less rolling shutter ( that wobbling effect that 's always there but most noticable @ @ @ @ @ @ @ @ @ @ ( shutter speeds , which are not user adjustable ) and slower frame rates . Or a better interface . Although the interface has improved a lot from 3 to 4 , it would be nice to be able to read it in anything other than full sunlight and an 18 year olds 20/20 vision <p> Lord Haw Haw <p> Oh , in case anyone is wondering about Bhaviks question below , 60 FPS at 1080p usually looks better , sometimes even if you edit it back to 30FPS . This is n't always true , and can seem counter intuitive , but the rolling shutter/jello effect is less ( plus you always have the option of half speed slomo just about anything looks better in slomo : - ) ) . This distortion effect is awful when it appears . You can see it in videos taken whilst driving , if the Gopro is being jostled or is n't stabilised and the cliche rolling shutter look is when you record telephone poles going past a train window . They will look slanted forward ( or is is backward on a gopro @ @ @ @ @ @ @ @ @ @ read all at once , the top of the pole ( or is the bottom ? I do n't  remember as I have this on a couple of my cameras ) is read first so its at one position when recorded , where the bottom has moved on by the time its image is captured . I can live with the telephone pole thing most of the time ( although I hate it ) but rolling shutter can really ruin footage , making the image look like its wobbling . I realise I possibly do this more than many people ( its my job ) but I try to sort out some sort of stabilisation or at least a solid mount for any Gopro footage . Its also responsible for making the propellors on aircraft look like they 're melting when the speed of the prop and the shutter speed combine with the sensor readout in a particular way . Of course , as the Gopro has a relatively small sensor ( which is one of the reasons you do n't  have to bother with focus combined with the wide angle @ @ @ @ @ @ @ @ @ @ would be much worse if the sensor was made larger and everything else remained the same the distance between the top and the bottom of the bit of silicon and chemicals that captures the image is comparitively small , compared to pro cameras . It might seem strange for me to be using Gopros when there are so many ( more expensive ) cameras I can use but , despite my whining over the last couple of posts , they are amazing . Especially for the price . And because they are so cheap they are easier to risk . Plus the waterproof housing , small size ( good for 360/VR stuff ) , lack of attention seeking , high resolutions etc . <p> Jacob Thompson <p> Get good footage in low light conditions with the Sidekick the perfect GoPro Light 
@@106848732 @2248732/ <h> Mobile ( 3G ) vs . WIFI Network Detection with Adobe AIR <p> Did you know that you can determine whether your mobile device is on wifi or your mobile data connection when using Adobe AIR for mobile ? To be honest , I was n't aware of it either until yesterday when my friend and former colleague Brian OConnor pointed me towards a recent tweet from @adobecookbook that showed an example how to do it . <p> These networking APIs have been around since AIR 2.0 , but I 've seldom had the need to dig into them for the desktop . Now that AIR for mobile is widely available , this can be critically important for your applications . For example , what if you want to minimize network usage while on the mobile network ? This may even prevent you being chastised for eating up expensive mobile bandwidth . <p> Using the NetworkInfo class findInterfaces() method , you can retrieve a list of all network interfaces on your computer/device . If you iterate through these , you can see which are active , what their IP and @ @ @ @ @ @ @ @ @ @ they are using . here 's a quick example ( code below the video ) : <p> Network Detection on AIR Mobile <p> I know that video is a little hard to see , so here are some screen captures . First , a capture showing the mobile network connection active : <p> Mobile Network Active <p> Next , a capture showing the WIFI network active : <p> WIFI Network Active <p> Basically , I just have a list that shows all of the network interfaces . When the app loads , or whenever the network connection changes , the content of that list is updated to reflect the current state of the network interfaces . If you want to determine whether you are on wifi , you can compare the name of the active network interface to see if it contains the string " wifi " , as shown in the Adobe Cookbook . <p> One thing not to forget : You must make sure that your application has been provisioned to allow access to network interfaces ! You 'll just need to uncomment the Android permissions in your app.xml for @ @ @ @ @ @ @ @ @ @ does not work on iOS . The **38;1440;TOOLONG ( ) method is not implemented . For now , you can only do this on Android . <p> http : //www.innovatology.nl Jon <p> Works on Blackberry PlayBook too ! 123436 @qwx983436 <p> Great ! I have n't tested it on the playbook yet . However , I found out the hard way that it definitely does not work on iOS . Just be sure to use NetworkInfo.isSupported to determine if it will work on your target platform . <p> Mike Lyons <p> I would love to find a way to do this from the browser without AIR but it seems impossible at the moment . 123436 @qwx983436 <p> You are correct , the NetworkInterface classes are AIR-specific and will not be supported in the browser . I am not aware of any plan to bring these features to the Flash Player . <p> As I mentioned above : " Just a forewarning this does not work on iOS . The **38;1480;TOOLONG ( ) method is not implemented . " You could write a native extension to access this information on iOS , @ @ @ @ @ @ @ @ @ @ iOS . <p> However , the code i have posted above does work on iOS ! ! Its true that you simply can not instanciate with " new NetworkInfo() ; " . But by using the getDefinitionByName method , you can call the findInterfaces() method and it is definately working , i 'm using this code in my project for an iOS project 123436 @qwx983436 <p> Oh , OK . Thanks for letting me know ! I must have misread , and did n't  realize that you actually had it working . I 'd be careful that it could stop working if there are any changes to the flash player or AIR . <p> devilonbike <p> Is there any way to determine available wifi and to connect to them using as3 
@@106848733 @2248733/ <p> IBMs Watson Developer Cloud speech services just got a whole lot easier for mobile developers . - I myself just learned about these two , and cant wait to integrate them into my own mobile applications . <p> The Watson Speech to Text and Text to Speech services are now available in both native iOS and Android SDKs , making it even easier to integrate language services into your apps . <p> This post is inspired by all the comments Ive seen this week about JS in the enterprise . I would have never imagined- this 10- years ago , but JavaScript is now pretty much ubiquitous . Here are a few reasons why you need to paying attention to JavaScript if you are n't  already , and why you should definitely not- write it off . <p> First , I think one of the major reasons for JavaScripts ubiquity is that- JavaScript is approachable . It is relatively easy for beginners to learn JavaScript , and powerful enough for advanced users to build complex and reliable systems . <p> Second , why you need to pay attention @ @ @ @ @ @ @ @ @ @ JavaScript to develop on virtually any platform : client side applications , server side logic , embedded chips/IoT devices , manage build scripts and dependencies , and more . <p> This does n't  mean you 'll use the exact same code in every case , rather that you can use the same skill set JavaScript Development to deliver solutions across multiple paradigms . <h> The Client Side <p> JavaScript can be used to power client side apps/user interfaces , and user interactions on numerous platforms and devices . <h> Web <p> Of course JavaScript powers the web , this is a given . JavaScript is the primary scripting language for all web browsers . I wo n't focus on this much b/c its already well known . <h> Mobile <p> JavaScript can also be used to power mobile applications that are natively installed on a device . <p> Apache Cordova/PhoneGap- You can build natively installed apps with web technology using PhoneGap or Cordova . PhoneGap is Adobes branded distribution of Cordova , but from the developers perspective , they are basically the same thing . Your app runs within a webview on @ @ @ @ @ @ @ @ @ @ the same way you you build a dynamic web application . Your user interface- is implemented in HTML , styled with CSS , and all interactivity is created with JavaScript . <p> React Native- JavaScript powered web apps do n't  just have to be inside of a a web view . The React Native framework gives developers the ability to write their application using JavaScript and declarative UI elements , and results in a native application running on the mobile device . The logic is interpreted JavaScript at runtime , but everything that the user- interacts with ( all UI elements ) is 100% native , providing a very high quality user experience , and it is now available for both iOS and Android applications . <p> Unity 3D You can even develop rich &amp; immersive mobile 3D simulation or gaming experience , entirely powered by JavaScript using the Unity 3D engine . **These can be web , desktop , or mobile , but is often used in mobile gaming . <h> Desktop <p> Yup , desktop apps are not left out of the mix . Most desktop solutions fall @ @ @ @ @ @ @ @ @ @ end results is a web view that has access to lower level APIs , whose content is developed with web based technology . <h> The Server Side <p> Most obviously- Node.js a JavaScript runtime buit on Chromes V8 JavaScript Engine has made huge inroads into server side development and the enterprise . Node.js , powered by frameworks like express.js or loopback.io makes server side development and complex enterprise apps with JavaScript possible . <p> Here are some stats that- show the magnitude of growth and adoption for Node.js/npm.js alone . NPM- stats currently shows a total of- 186,946 packages available for download , 94,978,032 package downloads in the last day , and 2,451,734,737 package downloads in the last month . <p> This does n't  mean that JavaScript is the best language at everything . It- also does n't  meant that you can take a single piece of source- code and run it in every device/context imaginable . <p> It means that you can use your skills in JavaScript to develop for just about any kind of device/context out there . Its not going to be write once , run everywhere @ @ @ @ @ @ @ @ @ @ learn once , write everywhere . <p> Node.js is an incredible tool for rapidly building highly performant and scalable back end systems , and you develop it using a familiar core language that most front-end developers are already accustomed to , JavaScript. - This acquisition is positioned to greatly enhance Node.js in the enterprise , and StrongLoops offerings will be integrated into IBM Bluemix , IBM MobileFirst , and WebSphere . <p> Even though the acquisition is still " hot off of the presses " , - you can start using these tools together- today : <p> If you have n't heard about StrongLoop 's LoopBack framework , it enables you to easily connect and expose your data as REST services . It provides the ability to visually create data models in a graphical ( or command line ) interface , which are used to automatically generate REST APIs " thus generating CRUD operations for your REST services tier , without having to write any code . <p> Why is this important ? <p> It makes API development easier and drastically reduces time from concept to implementation . - If you @ @ @ @ @ @ @ @ @ @ definitely check it out . - You can build API layers for your apps literally in minutes . - Check out the video below for a quick introduction : <p> Again , be sure to check out these posts that detail the integration steps so you can start using these tools together today : <p> That title get your attention ? - Yes , it really read " Adaptive- mobile- apps that- change based on personal context " with near real-time rules application , without much extra development effort . - If that sounds interesting to you , or like a product you might want to use within your own apps , then you might want to check out this site where you can get involved in the products development : - http : **35;1520;TOOLONG <p> IBM is looking for your input on creating these types of mobile app experiences. - User experiences within a single app that can be- dramatically different per user based on location , past behavior , profile information , social media activity , - and so much more . - With this behavior being driven @ @ @ @ @ @ @ @ @ @ app to the app store . <h> How it works for your customer <p> Consider this scenario : <p> Jon and Andrea download the mobile app for S&amp;W , a retailer known for its attention to providing great customer service . Over the next month , Jon and Andrea use the app to browse and discover content and merchandise differently . <p> Jon primarily navigates to sports related content for his favorite teams to find gear and clothes for travel to his favorite team 's games . Andrea scours the app for sales and fashion trends and usually ends up following her favorite designers . <p> Andrea and Jon go to a baseball game together . She 's never enjoyed watching it , so she opens up the S&amp;W app to entertain herself , and her app 's navigation quickly steers her through Spring fashion articles . <p> Jon however , wants to replace the hat he 's worn the last three times the team lost , and since he 's in the stadium , his S&amp;W app opens right up to the team 's gear page . The app @ @ @ @ @ @ @ @ @ @ to get to an S&amp;W store . <h> How it works for the dev team <p> Consider another scenario : <p> One of the developers on the team , George , sets up the system and application . He then gives access to Janet who is responsible for the customer experience . <p> Janet writes rules defining how the application could adapt and become more personalized based on inputs like , social media , geolocation , app usage , or customer information data . <p> Once Janet has built out her rules , she simply hits Submit ' and can immediately see her clever interactions reflected in the mobile application without having to involve the development team . <p> Analytics let Janet know which adaptations are working best , and helps her find new opportunities to optimize the app 's user experience . <p> Were not talking about a content management system , or translation based on locale , instead a rules-driven product that can adapt literally every aspect of your app : - customize the user interface , enable or disable different features , customized messaging and notifications , @ @ @ @ @ @ @ @ @ @ context . - This can be used to present- contextually relevant information , drive adoption , provide more/less data depending on your physical context , and so much more . <p> It wont be tied to a specific UI framework , wont be tied to a specific content management system , is n't attempting to re-create Google Now or Apple Proactive Assistance . - Rather , a set of tools and a rules engine that enable you to customize and tailor the app experience to the individual user . <p> Last week I had the opportunity to present to a great audience at- the- MoDev DC meetup group on " Smarter Apps with Cognitive Computing " . - In this session I focused on how you can create a voice-driven experience in your mobile apps . I gave an introduction to IBM Bluemix and IBM Watson services ( particularly the Watson language services ) , and demonstrated how you can integrate them into your native iOS apps . I also covered IBM MobileFirst for operational analytics and- remote logging to provide insight into your apps performance once it goes live . @ @ @ @ @ @ @ @ @ @ the video below : 
@@106848734 @2248734/ <h> Tag Archives : Photography <p> Last week I had the opportunity to present an incredibly fun topic to the DC/MD/VA Creative Professionals user group GoPro Cameras , Quadcopters , and Adobe Creative Cloud . - Thanks to everyone who attended . - This topic is a personal interest of mine , and I had a great time . - There were great questions and great conversations all around . <p> For those who werent able to attend , here 's a video of the full 2-hour presentation . - The audio quality is n't perfect , but you can still catch most of it : <p> FPV : KumbaCam- Great for a remote viewfinder , though the GoPro feed flickers when in time lapse photography mode . - I put it on a tripod at eye level so I can quickly glance between LOS and FPV viewing ( FPV = First Person View ) . I use this as a remote viewfinder , not a primary flight mechanism , and never go beyond line-of-sight . <p> Without the gimbal and FPV , you 'll get about 12-15 mins of battery per @ @ @ @ @ @ @ @ @ @ get about 7-8 minutes per flight I 'm currently researching options to extend battery life &amp; flight time . <p> You can definitely get bigger copters with a heavier lift capacity , but this configuration is great for getting started , and is designed specifically for the GoPro . Then , use Creative Cloud to polish your images and video . <p> here 's a quick video I put together showing how to create incredible panorama images in ten easy steps , using Adobe Photoshop CC. - I captured this one on Monday in a sunrise session- in Richmond , VA with a DJI Phantom and GoPro . <p> and the ten steps are : <p> Go get some awesome shots ! Just be sure that you 're in the same location and there is overlap between each shot <p> This week I was in the video studio recording some content for Adobe Inspire magazine on creative uses of quadcopters , GoPros and Creative Cloud for aerial photography and videography . Adobe Inspire is a great place to get new ideas or learn tips and tricks with Adobe Creative Cloud tools . Subscribe @ @ @ @ @ @ @ @ @ @ app to check it out . <p> My series will be released in early 2014 , but here are a few teasers to whet the appetite . I had a blast with this shoot , and cant wait for the articles and videos to be released radio-controlled aerial photography is my latest hobby/obsession . <p> In the studio : <p> Video from the shoot : Skies above the Adobe office in San Francisco . Captured with a DJI Phantom and GoPro : <p> If you look really closely , you can see us standing on top of the parking garage to the back left of the Baker &amp; Hamilton sign . <p> Subscribe- today or download the- iOS app- to be notified once this is live in Inspire . Now , go get creative and do amazing things ! <p> Ive been spending a lot of time with Photoshop recently Whether it has been retouching video or images , creating panoramas , or working with my aerial photos , it has been a lot of fun . One thing that I 've been doing is exporting really large images to the @ @ @ @ @ @ @ @ @ @ process Export from Photoshop using Zoomify . Then , since the default Zoomify renderer uses Flash ( and I want this consumable on mobile devices ) , take the Zoomify image tiles , and put them into a custom-coded HTML experience using the Leaflet tile- engine with a custom tile layer . <p> Leaflet is normally used for web-based mapping , but it is a perfect solution for rendering image tiles on the web . It already has touch and mouse interactions , inertial scrolling , progressive viewing , and a comprehensive API that can be extended if you so choose . <p> I 've done this enough times that I figured " There has to be an easier way " and there definitely is . I 've created a new Zoomify template that allows you to export from Photoshops Zoomify feature directly to HTML , leveraging the Leaflet engine . All of the code and installation instructions are below in this post . Check out the video below to see it in action : <h> Samples <p> Here are few samples from the Zoomify output ; both are the compositions that @ @ @ @ @ @ @ @ @ @ or touch interactions to pan and zoom on each of them . <p> The first is an export from a 10MP aerial panorama ( 4340+2325 pixels ) , which was created by stitching together multiple images captured with a GoPro camera and remote controlled helicopter . <p> The second example is a massive 139MP composite image ( 14561+9570 pixels ) . I created this by stitching together 48 10mp images in Photoshop . Its not perfect , but shows how far you can zoom into an image some images had different exposures , some were out of focus , there is still some perspective warp , and I definitely have some bad stitching seams . This image is so huge that I actually maxed out the system RAM , and filled up all hard disk space with the memory swap file when creating it ( I had over 100 Gigs of free space ) ! <p> Extract the zip file and copy the following files to Photoshops Presets/Zoomify directory . On OS X , with the default configuration , these files should be located in /Applications/Adobe Photoshop CC/Presets/Zoomify/ <p> L.TileLayer.Zoomify.js @ @ @ @ @ @ @ @ @ @ Photoshop . <p> When you have a file open that you want to export , choose File -&gt; Export -&gt; Zoomify- <p> Then select the " Zoomify Leaflet HTML " template that should now be in the list . Select an output location , base name , and image options , and hit " OK " . Ignore the browser width and height , since the template ignores these . Instead , it takes 100% of the width and height of the browser window . <p> - This will generate all of the image tiles and the HTML structure . From here , do whatever you want with it You can modify it , put it on a server , or anything else . the file output will look something like the image below . You will have a folder that contains the generated HTML file , the Leaflet JS and CSS files , and a directory that contains the generated tiles and appropriate XML metadata . <p> I 've posted about GoPro cameras here before I love mine b/c it is so versatile . You can mount it on just @ @ @ @ @ @ @ @ @ @ the air . There are tons of accessories for mounting it all kinds of different ways . However , the cost of these mounts can quickly add up . The adhesive mounts are essentially 1-use mounts . Once you stick them to something , they are firmly attached , and can not necessarily be easily reused multiple times . If you want a reusable mount , GoPro has their own clamp mount , but is also a bit pricey if you have other accessories . here 's how you can create an inexpensive and extremely versatile clamp mount . <p> I already had a frame mount , and a tripod mount for the GoPro , so I already had half of the equation . ( Yes , if you have to go out and purchase both of these , its just as much as GoPros clamp mount , but this approach is far more versatile when you have all of them . ) <p> To turn this into a clamp mount , you just need to add the clamp and a mounting point , which can be purchased at a hardware @ @ @ @ @ @ @ @ @ @ clamp ; any size will do , you just need one big enough to put a 1/4 inch bolt through the handle . Generally , the bigger the clamp , the stronger the spring will be to hold it closed . The one I used is pretty big you do n't  necessarily need one this big . You 'll also need a 1/4 inch bolt , some washers , and some 1/4 inch nuts . I used a thumb screw with two wing nuts so that it is easy to adjust in any situation without any tools . <p> Clamp Mount Parts <p> Just drill a hole through the clamp handle big enough to fit the 1/4 inch thumb screw . Put a washer on either side , and tighten it with one of the wing nuts . Then put the other wing nut in the opposite direction , followed by the tripod mount . Use this second wing nut to tighten the tripod mount in the correct position . Next , just attach the " arms " that come with the GoPro , and attach the camera inside of the @ @ @ @ @ @ @ @ @ @ clamp mount for the GoPro . This will also work with the waterproof enclosure . <p> DIY Clamp Mount <p> Now , get out there , capture some great images and videos , and then use Creative Cloud to bring out the best in them . You can do everything from lens profile correction , horizon correction , color correction , and much , much , more on both images and videos with Photoshop , Lightroom , and all of the tools that Creative Cloud offers . 
@@106848736 @2248736/ <h> Automating PhoneGap Builds <p> I 've recently had several conversations with PhoneGap users around processes for automating the compilation of PhoneGap apps . - This could be either in automated tasks using- Grunt , - Ant , - Maven , or any other task manager , or could be in continuous integration environments like Jenkins CI . <p> If you 're interested in this , here are a few options First of all , PhoneGap Build has a REST API . You can use this to programmatically create new projects , update projects , trigger new builds ( even just for specific platforms ) , etc This can integrate with your build scripts and tie into any workflow . <p> If you 're using GitHub , it is possible to tie into hooks triggering PhoneGap Build to recompile every time you commit your code . - Heres an example of it in action , or you can just use this service which is already setup : - http : //autobuild.monkeh.me/- ( from the same author ) - - Just be careful with your user/pass in plain text . Update : You @ @ @ @ @ @ @ @ @ @ instead of sending through username and password details via HTTP . <p> If you are n't  using PhoneGap Build , you 're not out of luck . - All PhoneGap CLI- commands are based on scripts , which themselves can be scripted. - You could use ANTs exec command , the Maven exec plugin , Grunt exec or Grunt shell plugins , Jenkins execute shell , or any other task runner to manually invoke the PhoneGap CLI . You just need to make sure all your environment and path variables are correct to access SDKs and required programs . However , there 's one caveat iOS builds require Xcode/Apple developer tools , which have to be run on a Mac . 123433 @qwx983433 <p> Hi Andy . Thanks for mentioning the Autobuild service . The link seems a little messed up ( I think something went wrong with a double-quote mark somewhere ) . <p> Its also possible to use the Autobuild service using a clientID variable instead of sending through username and password details via HTTP ( albeit behind the scenes in Github ) should the user wish to do that @ @ @ @ @ @ @ @ @ @ PhoneGap . Ray Camden provides a lot of good input and has caught my attention with his blog post . <p> Denis Bulichenko <p> Hi Andy ! Thanks for mentioning our blog post with step by step Jenkins configuration guide Before automatic builds working on many external projects was a complete mess . Developers spent so much time building small changes / adding test accounts / dealing with a build outside of the office . Now we start every project with configuring automatic builds system . <p> thenewmr <p> Thanks for this post Andy . Exactly what I was looking for . Particularly the link to the blog post . <p> Atul <p> Thank you for this post , Andy . In my own experiments I have found that even the simplest of Phonegap CLI commands , e.g. phonegap -v fail when issued from an automated script though they work perfectly well when the same script is run in an interactive bash shell . I was wondering if you might be able to tell me why that happens . 
@@106848738 @2248738/ <p> Another request that I have gotten from some of our DPS customers is that theyd like to be able to implement gestures inside of the Edge Animate compositions that they are building for DPS publications . This includes double-tap gestures , swipe gestures , etc Out of the box , these gestures are n't  supported , but you can add them to any Edge Animate composition without too great of an effort . <p> Below is a quick video showing Edge Animate Compositions that are taking advantage of both double-tap and swipe gestures . - Note : I intended these to be used inside of DPS , but I show them in Safari on iOS. - These gestures override the default mobile browser behaviors . <p> As I mentioned above , this is n't something that is supported out of the box , but it is possible to add gesture features manually . <p> The links below are for the basic examples that I put in the video . - Both should work in desktop and mobile browsers : <p> For the double tap example , just perform a @ @ @ @ @ @ @ @ @ @ area ) , and the animation will start again from the beginning . - For the swipe gesture , just perform a horizontal swipe in either direction with either your finger , or the mouse . <h> Gestures With Hammer.js <p> I leveraged the hammer.js JavaScript library to handle gesture detection since these gestures are n't  supported by default . - Hammer.js also enables other gestures , like long taps , pinch , rotate , etc - However , I 'm only showing double tap and swipe . - You can read more about hammer.js using the following links : <p> I used this exact setup procedure in both the double-tap and swipe examples . <p> To include this library , I first downloaded the hammer.js file , and saved it inside of the " edgeincludes " folder . <p> Next , you have to disable the web view/browser default double tap behavior , which is to zoom in when double tapped . - You can disable the zoom on double tap by adding a viewport metadata tag inside of the Edge Animate projects html file . - Open your projects @ @ @ @ @ @ @ @ @ @ , and add the following line to the &lt;head&gt; <p> Next , we have to add the code inside of the Edge Animate composition to enable the gesture behavior. - The first thing you have to do is include the hammer.js library . - In this case , I wanted to add the gestures to the compositions stage , instead of a particular element . - So , right-click on the stage in the Edge Animate Editor , then select - the " Open Actions for Stage " menu option . <p> This will open the actions for the Stage instance . - Next , click on the " + " icon and select " creationcomplete " . - This will create a function that gets invoked once the Stage instance has been created at runtime . <p> In that function , first we need to import the hammer.js library . - Edge Animate compositions include the- yepnope.js library , which is originally intended to detect if a browser includes a specific piece of functionality . - If not , then include a JS library so substitute that missing feature @ @ @ @ @ @ @ @ @ @ a blank test to force it to include the hammer.js library . - The following function forces loading of the hammer.js library . - Once the library has been loaded into memory , it triggers the " init " function : <p> In the init function , we grab a reference to the stages element ( div ) , then use hammer.js to add our gesture event handlers : <p> Now , we need to start looking at the individual examples <h> Double Tap Gestures <p> In the double tap example , we have a simple timeline animation that plays sequentially . - At the end of the sequence the animation is stopped by a simple sym.stop() function call . - Heres a quick preview of the setup in Edge Animate : <p> To add the double tap gesture , all you have to do is add a hammer.js event for " doubletap " . - In that event handler , were just calling sym.play(0) , which restarts playback from the beginning of the composition . The full code for the creationcomplete event is shown below . - This is @ @ @ @ @ @ @ @ @ @ the composition stage instance : <h> Swipe Gestures <p> In the swipe gestures example , we have a simple timeline animation that plays sequentially . - However , at the end of each slide transition , playback is stopped by a simple sym.stop() function call . - Whenever we perform a swipe action , were either just playing forward , or playing in reverse until the next slide animation stops . - Heres a quick preview of the setup in Edge Animate , note the stop points highlighted by the arrows : <p> To add the swipe gestures , all you have to do is add a hammer.js event for " swipeleft " or " swiperight " . - In those event handlers , were just calling sym.play() or sym.playReverse() , depending whether it was a left or right swipe . - These play actions progress to the next animation sequence . The full code for the creationcomplete event is shown below . - This is all that is needed to add the swipe gesture to the composition stage instance : <p> With the swipe gestures , you can get @ @ @ @ @ @ @ @ @ @ you run into this and you do not want the page to scroll , the scroll action can be prevented by capturing the touchstart event , and canceling the default behavior. - I did n't  add this , just because I wanted to keep this example very simple . <p> Lately Ive been spending a lot more time working with- Adobe Edge Animate , - Adobe InDesign , and Adobe DPS . If you are n't  familiar with these tools , Adobe Edge Animate is a tool that enables the creation of animated or interactive HTML content , Adobe InDesign is a desktop publishing design tool , and Adobe DPS is Adobes Digital Publishing Suite , which is used for creating digital publications from InDesign everything from digital magazines , catalogs , corporate publications , education , and more . <p> So , you might be wondering , how does Edge Animate fall into this grouping ? Well From Edge Animate you can export compositions into a . oam package , which can be imported directly into InDesign for use with a web content overlay . You can read more @ @ @ @ @ @ @ @ @ @ was recently asked by a customer " does Edge Animate support 3D transforms ? " . Unfortunately , at this time 3D transforms are not supported in the timeline editor . However , you can add 3D transformations programmatically with JavaScript . Here are some examples showing how to integrate CSS3 3D transforms with Edge Animate compositions : <p> These can be great additions to the interactive experience , but I also wanted to share that you do n't  always need 3D transforms to add dimensionality to an interactive experience . By leveraging 2D translation , scaling , and opacity you can easily create interactive experiences that have a feeling of depth . <p> Let 's take a look at a quick example . The image below is from screenshots of an Edge Animate Composition that I put together . On the left-hand side there is an anatomical illustration . On the right-hand side , that illustration has been broken out into separate layers , with emphasis placed on the topmost layer . <p> Just click or tap on the image to see an animation that transforms the illustration on the @ @ @ @ @ @ @ @ @ @ <p> So , while this animation does n't  leverage any actual three dimensional elements , it leverages those 2D transforms to visually create a sense of depth . here 's how it works : <p> First , there are 3 images . The bottom-most image shows the skeletal structure and body outline . The middle image shows parts of the digestive system , and the top-most image shows another layer of major organs . The top 2 images have transparency so that they do not completely hide content from the underlying layers . <p> The default state is that all of these images are aligned so that they appear as a single image . <p> Once you click/tap the image , a set of two-dimensional- animations take place providing a sense of depth and emphasizing the top layer . The underlying layers have both a scale and opacity change . The bottom layers are smaller , and less opaque . The underlying layers also have a two dimensional ( top/left ) transform . In this example , I 've tried to align both the scale and top/left transforms to correspond with a @ @ @ @ @ @ @ @ @ @ Edge Animate Anatomy Composition <p> This technique provides the illusion of three-dimensional depth , even though we are n't  actually performing any kind of translation , rotation , or deformation on a three dimensional coordinate system . AND this can be implemented- completely- with the timeline . So , you do n't  have to be a programmer to add a dimensional feeling to you Edge Animate compositions . This effect was achieved simply by using the timeline editor and visual workspace . <p> You can preview this animation in a new window , or download the full source using the links below : 
@@106848739 @2248739/ <h> Review : Beholder Lite Gimbal For DJI Phantom Quadcopter <p> I recently purchased a Beholder Lite camera gimbal for my DJI Phantom quadcopter , and I am very pleased with it . Bottom line for those that do n't  want to read this entire post The Beholder Lite is hard to beat for the price , as long as you have some time to tune the gimbal. - Output video is very steady when flying reasonably , and still images are far more crisp than they are without a gimbal . The final output is not quite as good as the- H3-2D Zenmuse , but it is still much more than acceptable. - Plus , you can buy almost 4 Beholder Lites for the cost of one Zenmuse . However , when flying aggressively , there is a lot of vibration . Read more for an explanation , pros &amp; cons , plus tips for set up and some sample videos . <p> DJI Phantom with Beholder Lite Gimbal <h> Background <p> Um What is a camera gimbal ? <p> First , a gimbal is : " a- @ @ @ @ @ @ @ @ @ @ a single axis . A set of three gimbals , one mounted on the other with- orthogonal- pivot axes , may be used to allow an object mounted on the innermost gimbal to remain independent of the rotation of its support . " <p> Basic Gimbal ( photo : Wikipedia ) <p> A camera gimbal is a set of these gimbal supports that enables the cameras movement to be independent from the support structure . In this case , the camera gimbal allows the cameras orientation to be independent from the orientation of the helicopter . Check out this video for more detail : <h> The Beholder Lite Gimbal <p> The Beholder Lite gimbal is a direct-drive brushless gimbal . This means : 1 ) the motors are brushless motors , and 2 ) that the motors directly drive the support arms for the gimbal ; there are no servo arms or other moving parts in the gimbal assembly . Brushless motors are faster than traditional/brushed servo motors , so they offer a smoother response and better stabilization . I have used the Phantom with no gimbal , as well @ @ @ @ @ @ @ @ @ @ gimbal is by far the best quality . <p> I mentioned that I am very happy with this gimbal , though it has not been without its own hiccups . <h> Pros : <p> Very stable footage ****once properly installed and tuned**** <p> Still images are more crisp <p> Videos are far more stable , though aggressive flying will cause significant vibration ( this happens with all gimbals , though some more than others ) <p> Very affordable compared to similar gimbals on the market . <h> Cons : <p> Installation instructions are not very good . They are based on pictures only , and do not clearly identify motor orientation or wiring . I originally had the motors in the wrong direction , and the motor polarity reversed . This third-party post ( with video ) was very helpful for installation . Ive provided additional installation details at the bottom of this post . <p> The gimbal is marketed as " ready to go " you just assemble it and start flying . Once assembled , I had lots of vibration to the point of being unusable . I @ @ @ @ @ @ @ @ @ @ manually change settings before I could get some decent footage . This was both physical ( in the mounting ) , and in software configuration . I had to adjust the output gains before the gimbal would provide stable footage ( details below ) . <p> The vibration absorbing mounts are too soft . You will get vibration from the camera oscillating below the copter . Luckily this can be corrected but sticking some foam ear plugs inside the vibration mounts ( see details below ) . <p> Both aggressive flying and high-wind environments will cause additional vibration in both the copter and gimbal , so keep those in mind when you are filming. - <h> Sample Video Footage <p> I still do n't  feel like I have the gimbal settings ( gains ) 100% dialed-in , but I have it close enough to get some high quality footage . I have found that the 1080p@60fps video mode on the GoPro camera has the best results . 2.7K@30fps has too much rolling shutter effect for the footage to be really usable . I also have not balanced the propellors on @ @ @ @ @ @ @ @ @ @ . Here are a few samples from the camera/gimbal combination . All of this footage was captured with a GoPro Hero 3 Black Edition . <p> 1080p@60fps with Post Processing via Adobe Premiere : I still think I can get it more stable by changing a few editing parameters and changing the sequence of my scaling and effects , but I am very happy with this output . <p> 2.7K@30fps with 10% Warp Stabilization The footage is far better than it is without a gimbal , but there 's still some rolling shutter effect evident ( capturing Tonys hex liftoff ) . <p> Aggressive Flight : This footage is completely raw , without any post-processing or stabilization . This shows output when flying aggressively . This was *very* aggressive flight full speed in ATTI mode . I recommend that you *never* attempt to shoot professional footage flying like this : <p> Pay particular attention to motor orientation and wiring . The motors look symmetrical , but their weight and operation is not . The pitch motor should have the wire coming out the side that is closest to the copter . @ @ @ @ @ @ @ @ @ @ side closer to the camera . If you have your motor polarity reversed , you 'll know b/c the gimbal will vibrate back and forth . This wont hurt it , and you can just reverse the wires easily if this happens . In the picture below you can see my motor orientation : <p> Motor Orientation and Wiring <p> Very important : make sure your gimbal is balanced . If the power is off , the GoPro should not fall in any direction . It should just stay where it is . Having it perfectly balanced is key to having smooth footage . <p> The rubber vibration dampeners are too soft for the gimbal . If you fly it " stock " , you will get a TON of vibration from the gimbal oscillating under the copter . Use some soft foam earplugs , roll them up , and put them inside of the dampeners . I used the cheap foam orange ones you get from any drug store , and they work great . I put one in each of the 6 vibration dampeners . Another picture below , @ @ @ @ @ @ @ @ @ @ dampeners : <p> Not entirely necessary , but I also put Moongel in between the gimbal and the copter body to absorb vibrations , and I also put it on top of the gimbals gyro board to absorb vibrations that may affect gimbal orientation ( see picture below ) . <p> Beholder Lite on DJI Phantom <p> Do not get the white wires too close to the power wires or to the motors . They are very light weight , and can get interference from power wires , motors , and motor wires . You will get inexplicable vibrations if this occurs . <p> One last thing If you use zip ties on the gimbal for extra safety , keep them very loose . If you compress the dampeners , the bottom plate will hit the controller board , and cause significant interference with gimbal operation . Everything will be fine one second , then completely out of control the next . <h> Battery Life <p> Before having a gimbal , I could easily get 12+ minutes of flight time per battery . With a servo-based gimbal , I could @ @ @ @ @ @ @ @ @ @ Lite , I get a max of 8 minutes per battery . I set a timer for 7 minutes , and be sure to bring it down as soon as the time goes off . However , this seems comparable to battery life with other brushless gimbals that friends/coworkers use . The decreased battery life is due to additional weight of the gimbal , plus the additional power consumption from the gimbal motors . <h> What Next ? <p> Use Creative Cloud to process all of your video and images to make them the best they can be ! - Here are some very useful posts for processing your content with this configuration ( or similar configurations ) 123433 @qwx983433 <p> Andrew this is a wonderful review ! This gimbal appears to be a great alternative . I did , however just splurge for the Zenmuse . It was such a pleasure to meet you and fly next to you at Photoshop World last month in Vegas . Seems we are all Phantom addicts so quickly ! 123435 @qwx983435 <p> Thanks Randy ! Great to meet you at Photoshop World @ @ @ @ @ @ @ @ @ @ obsession . I 'm always looking at new ways to tweak the output &amp; copter performance . 123435 @qwx983435 <p> also , the zenmuse is easier to get up and running quickly . I spent a lot of time tuning the beholder lite <p> Jim Southard <p> Andrew : Are you using FPV through your Gopro camera ? I was curious as to how you were framing your shots during your videos . It seems at times there was quite a bit of distance from where you would be standing . Thx . Jim . 123435 @qwx983435 <p> I currently do not have a FPV setup , All of my work to date has been line of sight , with a little bit of guess work for camera position . However , I just ordered a FPV system , which will hopefully be here by the end of the week . I 'll let you know how it goes . <p> Jim Southard <p> Andrew : Which FPV system did you choose ? I have heard mixed feelings about runing FPV through the Gopro vs. using a seperate FPV camera for safety @ @ @ @ @ @ @ @ @ @ KumbaCam setup LONG ... Its not a top of the line FPV system , but it does the trick for my needs . I just use it as a view finder , and it pulls the video from the GoPro . I 've used it to proerply frame both video and still images , though it blinks every time the GoPro takes a picture . If i were using this as my main way of flying the copter , I would probably want an external camera that wo n't blink . I also need to upgrade to clover-leaf antennas b/c it loses signal or gets interference as you are flying . <p> Jim Southard <p> I will check that out . Are you using the manual tilt control and seventh channel with your beholder light ? I turned on the gimbal control on the phantom and the beholder went to about 30 degrees down at power up instead of being level . The phantom software shows output when I manually turn the seventh channel but the gimbal just stays at 30 degees and does not move . Any suggestions ? I would @ @ @ @ @ @ @ @ @ @ by tilting the gimbal down . Thx. 123435 @qwx983435 <p> I modded the remote to add a knob to control pitch , following this example : http : **31;1557;TOOLONG Just be *really* careful not to damage the board when removing the old pot . For the gimbal settings in the Naza controller software , I have the Gimbal on , with the following : <p> Pitch : Max : 1000 , Center : 0 , Min -1000 <p> Roll : Max : 0 , Center : 0 , Min 0 <p> Automatic Control Gain : Pitch : 0 Roll : 0 <p> Manual Control Speed : 100 <p> Jim Southard <p> I ordered a control nob to fit the top hole on the back of the radio . When I use a screw driver to move the switch , nothing happens . The gibal just stays at about 30 degrees down and works when i move the quad . If the software is turned off , the gibal sits level and works properly when I move the quad . I am woundering if my wiring to the naza is wrong @ @ @ @ @ @ @ @ @ @ the wiring . Thx . <p> Jim Southard <p> Andrew : I ended up getting it to work . I got james over at Rebel to provide a fix below . <p> A quick fix to that is to change two parameters in GUI . Swap rc-channel number between pitch and roll . Then connect manual pitch(f1)to gnd and roll pins on gimbal controller . <p> Sean Lynes <p> Jim , I 'm confused . They now include a connector that would go from the 3 connections ( red red black ) on the receiver to two other connectors . One with Red and blackand the other with just a red wire . I tried using a simple servo connector as the FPV guys says and shows . Ive tried F1 and F2 , ive set my gains to the ones above , and ive tried connecting to the pins both ways . SHEESH ! The FPV guy contradicts himself over and over Nobody shows a simple video how to setup the phantom with the assistant and where the connects goes exactly . NOTE : I have the new phantom that @ @ @ @ @ @ @ @ @ @ tightening props . So the instructions did n't  match and I had some trouble early on . <p> Sean Lynes <p> Here is a pic of the connectorand of course nothing about this in the manual , videos , anything ! <p> Jim Southard <p> I used one cable on the F1 and one on the F2 . The one with the 3 wires I have going to the gimbal . What I ended up doing with trial and error was only having one of the red wires pluged in to the gimbal for for tilt . Basically I have the plug sidways and only one wire plugged in on the gimbal three post terminal . This really only matters for the manual tilt . The gimbal works without this plug anyway . You will need to turn on the gimbal in the NAZA as well . <p> Sean Lynes <p> Yes , that worked . Sheesh what a pain . I found a reference to a similar approach earlier . Thanks ! <p> Jim Southard <p> Sean : The gimbal will work without anything but power from the quad @ @ @ @ @ @ @ @ @ @ is the tilt feature . The gimbal likes to vibrate quite a bit and you will need to play around with the settings in the gimbal GUI board as Andrew indicated . His numbers did not work for mine but were a good starting point . I am still tweaking vibrations . I think the quality of the parts seem good but the gimbal has been a far cry from plug and play . <p> Nick Staib <p> Hi Andrew . This is a beautifully organised Beholder Lite resource . Thank you for compiling and sharing <p> I have just moved mine to an F550 and agree 100% with the need for foam dampeners in the suspension . I think they would have worked better in compression . <p> I notice a kind of vibey high speed vibration in mine at rest . More so with 3S power than 2S power . I 'm assuming this means I need to dable in the dark arts of PID tuning ! <p> Anyway you can share the settings that worked best for you ? Do I need to reduce gains ? <p> Cheers @ @ @ @ @ @ @ @ @ @ question . I see that you advocate " The roll motor should have the wire coming out the side closer to the camera . " This looks counter-intuitive as the wire is then being moved around as well , and from what I can see contradicts both the manual ( albeit not very clear pics ) and also the way that Bo seems to have set his up on the links you kindly provided . <p> Can I ask if your recommendation is based on testing both positions and then realising that your way works better ? <p> Thanks again , Nick 123435 @qwx983435 <p> Thanks Nick ! If you 're getting that high speed vibration at rest , you 'll need to adjust your PID gains no question . That 's exactly the same issue i was having . As soon as I attached power , i could hear it vibrating , and if the camera was on , there was a ton of jello . <p> I got that motor orientation from somewhere online I cant recall exactly where , but I went through lots of permutations of trial and error @ @ @ @ @ @ @ @ @ @ I did notice that the motor connection wires are in opposite directions where they attach to the board ( note , you can also reverse polarity in the software config ) , but it is working great for me . I honestly think it could work well either way , b/c I saw the biggest improvements after I started tuning via the software interface . <p> Heres what I am using for gains : <p> Pitch Motor : P = 12 , I = 7.722 , D = 16.602 , PWM% = 26 <p> Roll Motor : P = 18 , I = 5 , D = 100 , PWM% = 28.8 <p> Yes , the roll dampening is set at 100 ( the max ) . For roll , this is very far from the default setting I 've spent hours tuning this , and its pretty solid now . here 's an example recorded with these settings : http : //www.youtube.com/watch ? v=6OX9cZX3KKk <p> Note : when you change gains settings , be prepared for the gimbal to have a panic inducing seizure <p> Good luck , and have fun @ @ @ @ @ @ @ @ @ @ addictive hobby ) <p> Nick Staib <p> That is great info Andrew . I 've now downloaded the gimbal " assistant software " and the associated windows drivers but struggling to read my existing pid gain settings so that I can see what they are and edit them . Nothing is easy , eh but then if it were we would quickly tire of it all Thanks again ! Nick 123435 @qwx983435 <p> Awesome , glad you got it worked out ! Yeah , I noticed massive improvements after changing the PID settings . I 'm much happier with it now . <p> Jeff <p> Hello Andrew . <p> I 'm wondering if you could she 'd some light on how exactly I 'm supposed to use the " balance " set screws ? ? The manual does n't  do a very good job of showing me what to do with these . <p> Thanks ! Jeff 123435 @qwx983435 <p> I just adjusted the vertical positioning of the arm which connects to the camera until it was balanced . You can move that arm up or down on the motor . It was a lot @ @ @ @ @ @ @ @ @ @ it until the the gimbal no longer favored any direction when the camera is attached . When it is balanced , if you leave the camera at about a 45 degree angle , it will stay there . If it is not balanced , when you put the camera at about 45 degrees , it will fall forward or backwards . The gimbal does n't  really have a way to balance for the roll direction . Youd have to do add weight to one side until its balanced ( I did n't  have to do this ) . <p> boik glad , I did this .. With all the financial stress these years , I really hope all of you will give it a chance . onpq <p> http : @ @ @ @ @ @ @ @ @ @ the Beholder Lite as the best cheaper gimbal on the market now ? Or are there any other new models in a similar price point that are better ? <p> Thanks ! If I were to do it again , I would just buy the Zenmuse . The Zenmuse has dropped in price dramatically since I wrote this . Originally it was $750 , now its only around $350 . I am happy with the Beholder Lite , but it has required significant manual tuning to get solid performance out of it . The Zenmuse also has a proactive response ( its controlled by the copters IMU ) , and the Beholder is reactive ( it is controlled by its own gyro/accelerometer ) . Thus , the zenmuse gives you better performance , with far less maintenance . Though , One HUGE advantage of the Beholder is that it is made out of aluminum which is far less brittle than the Zenmuses plastic . I had a very hard crash 2 weeks ago , which ripped the gimbal from the copter it bent the gimbal arms , but I was @ @ @ @ @ @ @ @ @ @ . The Zenmuse would not have survived the impact of that crash . If there were a Beholder Lite that was controlled by the onboard IMU on the copter , I 'd take that in a heartbeat . <p> http : **25;1590;TOOLONG Zach <p> Great stuff , Andrew . I 'm getting started with a Phantom 2 , Zenmuse gimbal , GoPro Hero 3 Black . Can you describe your . mp4 ingest workflow with Premiere Pro ? I 've had mixed results and have read elsewhere that the GoPro Studio software can be useful for file conversion prior to importing into PP . <p> This is a great insightful article in my point of opinion ! I like the way you have constructed the points about Gimbal Technology . I have a couple of questions for you . <p> 1 ) Does Dji Phantom Series Quadcopters are ideal for Gimbal or is there any other inexpensive alternatives ? <p> 2 ) You mentioned that Gimbal can reduce the battery life , but is there any way how we can enjoy Gimbal while enjoying maximum battery life ? or at-least some techniques to @ @ @ @ @ @ @ @ @ @ UAV Drone which has less flight time than Dji Phantom Vision 2+ which flight time is considerably low . <p> Anyways , thanks in advance and just bookmarked this site . All the best and cant wait to check your new posts on Quadcopters <p> Andrew , it is an excellent review ! This type of gimbal seems to be a fine alternative . Does this Beholder Lite Gimbal fit the brand-new DJI Phantom 3 Professional LONG ... <p> Arran <p> I 'm upgrading to a new quadcopter soon , I 've had issues with my old Phantom though so i 'm reluctant to buy another one . <p> Have you tried DjI Inspire 1 ? It is much better than the Phantom 3 which was quite common and popular due to its affordability but for film makers , the Inspire 1 is the real deal and I 've found it as a recommended drone by these guys . <p> Gimbals are a great piece of innovation and it is going well with drones . As a professional photographer myself , I use many different gimbals and lens filters on my DJI Phantom quadcopter @ @ @ @ @ @ @ @ @ @ review , it was spot-on and very informative . Keep up the nice work Andrew . Will follow your future articles as well . 
@@106848741 @2248741/ <h> 3D Printing in Adobe Photoshop CC <p> The latest release of Photoshop has some amazing new features , one of which is 3D printer support . The new 3D printer support makes printing your 3D models easier , regardless of whether you modeled it within Photoshop or some other 3D modeling tool . - Photoshop will even inspect your 3D models for water tightness and generate support scaffolding to ensure a high quality 3D print . <p> Photoshop now supports local 3D printers from Makerbot- and- 3D Systems , but if you do n't  have a 3D printer , do n't  worry , you can still print 3D objects ! Through a partnership between Adobe and Shapeways , you can now package 3D prints and send them to Shapeways 3D printing service directly from Photoshop . <p> I started out by creating a few simple models and downloading existing models from the web . Check out the video below to learn more and see 3D features within Photoshop in action . <p> All of my 3D printing so far has been through the Shapeways service . This service @ @ @ @ @ @ @ @ @ @ gives you immediate feedback whether or not you need to make any changes . Be sure to pay attention to the details of Shapeways materials , since they have different characteristics , minimum thicknesses , and associated cost . Once your object is printed , it will be mailed right to your doorstep . <p> My first example is a 3D printed name plate , modeled entirely within Photoshop. - I took a text layer , extruded it into a 3D object , then added a cube ( stretched to the size of the text ) as a base . Just request a 3D print , and out comes a nice 3D model . This one is the " Coral Red Strong &amp; Flexible Polished " material from Shapeways . Check out the video above for details on how I created this . <p> " Adobe " 3D Printed Name Plate <p> My second example is a dragon , which I downloaded from a free 3D models site . This model was not created in Photoshop . I believe this model was originally intended for video games or renderings , @ @ @ @ @ @ @ @ @ @ Photoshop can read many common 3D model formats ( . obj , .3ds , . dae , etc ) , and makes the printing process simple , regardless of where the model was created . <p> I had to go through a few iterations to find a material and size that was actually printable because the model is very intricate and delicate , but I was finally able print it using the " White Strong &amp; Flexible " material . <p> 3D Printed Dragon <p> If you 're wondering " did those cost a fortune " , the answer is NO ! - The cost depends on type and amount of material you 've selected . The dragon was $32.65 , and the " Adobe " letters were $24.90 USD , including shipping . <p> Ready to create your own 3D prints yet ? - Check out the videos below to learn more about 3D printing with Adobe Photoshop CC. <h> 3D Printing in Photoshop Series <p> If you 're already a member of Creative Cloud , then you have everything you need to create your own 3D prints with Photoshop . Just @ @ @ @ @ @ @ @ @ @ ! If you 're not already a member of Creative Cloud , then become a member today ! 
@@106848742 @2248742/ <h> Category Archives : Photoshop <p> Adobe has introduced a bunch of new features in the Photoshop CC releases over the course of the year . One feature that I knew about , but had n't  used much myself was the new " preserve details " upsampling algorithm when resizing images , which was introduced in the spring . - Well , I was just looking into resizing one of my aerial photos of San Francisco for printing , and I was blown away at the output quality when upsampling my image . <p> Aerial Panorama of San Francisco <p> I stitched together the panorama from 6 or 7 images captured with a GoPro and DJI Phantom quadcopter ( details here ) . The original panorama image size was- 4746+1706 pixels I wanted to see how big I could make it for large-scale printing , and I upscaled the image by 469% to 22256+8000 using Photoshops image resizing features . The results are very impressive . <p> Here are a few side by side comparisons of different resampling/upscaling algorithms , the new " Preserve Details " algorithm is the @ @ @ @ @ @ @ @ @ @ the output of the new " preserve details " algorithm produced very good/high quality results . You can adjust the " reduce noise " slider to tweak the resizing algorithms output to minimize visual artifacts that may come from the up-sampling process . <p> You can check out the final output by interacting with the image below . - You can zoom in and pan around just by interacting with the image directly . When you zoom in really far , you can see its not going to create details that werent there in the original picture , but you 'll quickly see this can be immensely useful for upscaling smaller images for printing or other uses . - As a simple game , see if you can spot the Adobe San Francisco office ! <p> Check out more details on the Preserve Details upscaling feature in the video below , from Adobe TV : <p> I 'm testing out something new Im calling it " Photoshop Friday " , where every Friday , optimistically assuming that I do n't  have deadlines looming , I 'm going to try and do something fun @ @ @ @ @ @ @ @ @ @ ago I did some " crystal ball " compositions , and today Ive put together what I 'm calling Sky Aquariums. - Check out the timelapse video below <p> If you 're wondering how I created the timelapse , this technique for interval-based screen grabs works beautifully . - Just run this as a shell script from the directory where you want the screen capture images to be stored ( OSX only ) : <p> Last week I had the opportunity to feed my obsession with aerial photography and participate in Russell BrownsTop Gun Flight Training- workshop at Photoshop World , and it was an awesome experience ! Many thanks to Russell for putting together this incredible workshop . There 's nothing like a giant room full of RC photography enthusiasts , with multiple copters flying around all over the place , coupled with a wealth of Photoshop and video processing knowledge ( links below for all of this content ) ! <p> There are few places that have as dramatic architecture and landscape as Vegas , and this also made for some great- photos . <p> Aerial shot over the Luxor and @ @ @ @ @ @ @ @ @ @ an intro from DJI , who sponsored the event and donated copters for workshop participants to use . - If you have n't seen their copters/footage yet , you should check out the short video below to see what 's possible with todays radio controlled aircraft . <p> Next , we broke off into teams to cover basic flight training principles everything from safety ( the most important thing of all ) , basic takeoff and landing procedures , flight modes and capabilities , to techniques for panoramas and dramatic shots . <p> Once everyone had a chance to fly the copters and get acclimated , we regrouped to learn how to work with your aerial footage. - Do not miss the videos from Russell Brown and Colin Smith in the links below . These are must-watch content for any aerial photographer , cinematographer , or GoPro enthusiast and contain a wealth of information ! <p> Finally , the day ended with a flying competition The person who could fly through 4 hula hoops without knocking anything over , without crashing , and land in a controlled fashion walked home with a @ @ @ @ @ @ @ @ @ @ in 26 seconds ! This was not an easy task . <p> Some days you are just meant to be creative I think yesterday was that day for me . I saw that Erik Johansson released a new composition , and it sparked a wave of creativity within me . I discovered Eriks work via Adobe Max this year his presentation was awesome and very inspiring . Seriously , do yourself a favor and go watch it . <p> I digress I 've been doing a lot of aerialphotography lately , and after seeing Eriks latest composition I thought to myself : why not take some of my aerial photos and start altering reality ? My immediate idea was to create a surrealistic composition where you are looking down through a crystal ball . Next thing you know , this happened <p> Here are a few iterations from the creative spark . I think v.2 ( without the flames is my favorite ) : <p> Crystal Ball v.1Crystal Ball v.2Crystal Ball v.2 with Flames <p> Attribution : <p> The aerial image was mine , captured with a DJI Phantom and GoPro- camera . Check out the original image on Flickr. 
@@106848743 @2248743/ <p> I covered techniques for making your apps feel like " apps " , not " web pages " . - You can read more about these techniques and useful libraries in my recent post on Multi-Device Best Practices . - Note : That post contains references to both Flex and HTML/JS/CSS tools . - In this presentation I focused only on the HTML/JS/CSS tools . <p> In this presentation , I covered PhoneGap Build , a cloud-based compilation tool for PhoneGap apps , and- debug.phonegap.com for remote application debugging . - I also covered iWebinspector for debugging PhoneGap experiences inside of the iOS Simulator . <p> Let 's also not forget the real-world companies that have invested in PhoneGap/Apache Cordova , including Wikipedia , Facebook , Salesforce , IBM , and others . - You can read more about these companies from my recent post " Who Uses PhoneGap " . <p> I stumbled upon a really odd bug in my current project , which I can only attribute to the WebKit browser engine , since I was able to recreate this in a UIWebView on iOS ( in @ @ @ @ @ @ @ @ @ @ the common engine in both . Its a bizarre issue that is really easy to fix , but was dumbfounding since the user interface was not displaying what I was seeing in the WebKit debugging tools . I figured I 'd share , in case anyone else runs into the same issue . <h> The problem : <p> I have a horizontal slider ( custom HTML/JS ) component , and content within a separate HTML &lt;span&gt; should be updated when the slider value changes . The JavaScript seemed to be working properly , I could see console.log output that showed events were being dispatched , but the UI would n't display what I was seeing in the debugging tools . Instead , the UI would update sporadically with a value , but not consistently , and not for every time that I updated the HTML DOM . <h> The solution : <p> Set the CSS " display " property on the &lt;span&gt; element to " inline-block " , and everything works properly . This was a really strange issue since I could see the HTML DOM updates , but the actual UI @ @ @ @ @ @ @ @ @ @ seem to be making any sense , check it out in the video below . - Keep an eye on the HTML DOM structure , as well as the rendered output . <p> Unfortunately , it took way longer than I would have hoped to fix such a seemingly simple issue . - Hopefully this saves you some time if you run into it too ! <p> I am happy to announce the US Census Browser version 2.0 ! - Back in December of 2011 , I released the US Census Browser- as an open source- application intended to demonstrate principles for enterprise-class data visualization and applications developed with web standards . - This version has some fairly substantial changes See the video below to check out features in the latest version : <p> Version 2.0 of the US Census Browser has some substantial changes , including : <p> Completely new &amp; redesigned UI layer , using app-UI. - app-UI is an open source framework for application view-navigators that mimic native mobile applications . - Using the app-UI SplitViewNavigator , the US Census Browser now supports both landscape and portrait @ @ @ @ @ @ @ @ @ @ Map using OpenLayers. - Users of the Census Browser maxed out my Google Maps account ! - That is 25,000 map loads within a 24 hour period ! WOW ! I switched to the free Open Street Maps solution , which does n't  have any usage/bandwidth limitations . - With this change I was also able to add interactive maps . <p> Updated to Twitter Bootstrap 2.0. - The app is now using new UI styles and components which are now available in Twitter Bootstrap version 2.0 <p> I 'd like to take a moment and introduce app-UI , a new open source application framework that I 've been working on . <p> app-UI is a collection of reusable " application container " user interface components that may be helpful to web and mobile developers for creating interactive applications using HTML and JavaScript , especially those targeting mobile devices . app-UI is a continual work in progress it was born out of the necessity to have rich &amp; native-feeling interfaces in HTML/JS experiences , and it works great with PhoneGap applications ( http : //www.phonegap.com ) . app-UI can easily be styled/customized @ @ @ @ @ @ @ @ @ @ that things will change as the project is improved and matured this is a beta/early prototype . <p> All of app-UI was created using HTML , CSS , &amp; JavaScript . All animations are rendered using CSS3 translate3d , so that they are hardware accelerated ( where supported ) . app-UI works well on iOS , Android and BlackBerry browsers ( others not tested ) , and works well on the latest releases of most desktop browsers ( I know it does not work on old versions of IE ) . <h> Why ? <p> You might be wondering " why create this ? " when there are other open source alternatives like jQuery Mobile . The primary motivation for creating app-UI was to have reusable application containers that are highly performant , and do not force any prescriptive development paradigms . With respect to animations/transitions , app-UI outperforms the alternatives , particularly on mobile devices . <p> app-UI can be used with many different existing frameworks app-UI only requires jQuery as a solution accelerator framework . It will work with existing UI widget frameworks ( jQuery UI , Twitter @ @ @ @ @ @ @ @ @ @ templating frameworks ( Moustache , Knockout , Handlebars , etc ) . <h> Application Containers <p> app-UI currently has three application containers , and at this time it is not intended to be a complete UI widget framework . <p> Please see the " samples " directory for usage scenarios there is no documentation yet . <h> ViewNavigator <p> The ViewNavigator component allows you to create mobile experiences with an easily recognizable mobile UI paradigm . You use this to push &amp; pop views from the stack . <h> SplitViewNavigator <p> The SplitViewNavigator component allows you to create tablet experiences with an easily recognizable mobile UI paradigm . The SplitViewNavigator allows you to have side-by-side content in the landscape orientation , and the sidebar is hidden in portrait orientation . <h> SlidingView <p> The SlidingView allows content to slide to the side using a horizontal swipe gesture , revealing a navigation container " underneath " . This is very similar to the behavior in Facebooks iPad application . <p> I see questions and comments all the time with the general sentiment " it looks nice , but who really uses @ @ @ @ @ @ @ @ @ @ create a definitive list of everyone who uses it , but the general answer is " more people than you think " . Here are a few organizations that you might recognize who are using either PhoneGap or Apache Cordova in their cross-platform mobile solutions and/or tools . ( PhoneGap is a distribution of Apache Cordova ) <h> Microsoft <p> Microsoft is involved with core Apache Cordova development ( specifically for the Windows Phone platform ) . - Not only are staff from Microsoft committers for the core Apache Cordova project , Microsoft has also used PhoneGap on public mobile applications that target multiple platforms . - This includes the XBox-Live integrated gaming application Halo Waypoint , for both iOS and Android . - Check out Halo Waypoint in the video below , it looks awesome : <h> Adobe <p> I think most people already know how deeply involved Adobe is with PhoneGap , but I 'll try to recap quickly - In late 2011 , Adobe acquired Nitobi , the creators of PhoneGap , and contributed PhoneGap to the Apache Software Foundation as the Apache Cordova project . - Adobe @ @ @ @ @ @ @ @ @ @ the success of the platform . - Not only are we helping develop and mature PhoneGap/Apache Cordova , we also build some of our own applications with it . - ( Maybe I 'll be able to talk about those some day . ) <h> Zynga <h> Logitech <p> Logitech- used PhoneGap to develop the Logitech Squeezebox Controller application , which uses your home wifi connection to control a Squeezebox Internet radio device from your smart phone . - You can read more about this application on the PhoneGap application showcase , or download it now for iOS or Android . <p> Still not sure if anyone uses PhoneGap ? What about these , among many others ? 
@@106848744 @2248744/ <p> Push notifications , love them or hate them , are everywhere and there 's no getting around it . Push notifications are short messages that can be sent to mobile devices regardless of whether the apps are actually running . They can be used to send reminders , drive engagement with the mobile app , notify completion of long running processes , and more . Push notifications- send information to you in real time , rather than you having to request that information . <p> If you are building a back-end infrastructure to manage your applications data , and you want to leverage push notifications , then guess what ? You also have to build the hooks to manage subscription and distribution of push notifications for each platform . <p> The- unified- push notification API allows you to develop your app against a single API , yet deliver push notifications to multiple platforms , and it works with both hybrid ( HTML/CSS/JS ) apps , as well as native apps . <p> You will still have to build the logic to subscribe devices for messaging , and dispatch push @ @ @ @ @ @ @ @ @ @ once against the unified API not once for each platform . <p> The apps that I showed in the video above are sample apps taken straight from the IBM MobileFirst platform developer guide for iOS and Android , and can be accessed in their entirety ( with both client and server code ) using the links below : <p> On the client app , you 'll need to subscribe for messages from the event source . See the hybrid or native code- linked to above for syntax and examples . <p> Once your clients are subscribed , you can use a single server-side implementation to distribute messages to client apps . Below is an- excerpt from the sample application which demonstrates sending a push notification to all devices for a particular user ( on any platform ) : <p> From- the MobileFirst console , you will be able to monitor and manage event sources , platforms , and the devices that are consuming push notifications . <p> Push Notifications on the MobileFirst Console <p> If you were wondering , yes , these can be cloud-hosted on IBM BlueMix and yes , @ @ @ @ @ @ @ @ @ @ in your data center . - You have the option to configure- your physical or cloud servers however you want . <p> When you are developing a mobile app ( or website , or mobile web , or TV app , etc .. ) - you should always ask yourself " What kind of an impact does this have on the end user ? " It does n't  matter whether you are creating enterprise apps or games , or anything in between . Every development decision that you make should be weighed upon its impact to the overall impact- it has on the end user . Simply put : if your app sucks , nobody is going to want to use it . <p> When building mobile apps using IBM MobileFirst you have two options for a user interface layer ; you can write a native app , or you can write a hybrid app using HTML , CSS , &amp; JavaScript . <p> So , what kind of an impact does the addition of IBM MobileFirst have on the app ? <p> NONE , granted the apps UX can- @ @ @ @ @ @ @ @ @ @ or a hybrid app . <p> IBM MobileFirst Platform Foundation- is a platform that consists of a server tier and client-side SDK . If you are developing a native app , the SDK/API provides access to MobileFirst platform features , like user authentication , app version management , data access through adapters , encrypted storage , unified push notification , remote log collection , and more . If you are developing a hybrid app , the apps UX must be developed complete inside of the web view container . The MobileFirst- Foundation SDK provides additional functionality just like mentioned above for the native SDK , plus a few classes that enable native dialogs and a few native UI elements , but for the most part , there is very , very minimal impact on- the users experience . <p> When building any kind of mobile app , regardless of whether it is native or hybrid , you need to pay attention to what the user experiences . Are you following human interaction or design guidelines for the platform ? Are you forcing your user to go through unnecessary or redundant @ @ @ @ @ @ @ @ @ @ need to be ? Are you forcing multiple taps for a simple interaction ? Can things be simplified ? <p> The IBM MobileFirst Platform does not add any additional overhead for UX processes . <p> The IBM MobileFirst platform can be used to develop native apps on iOS , Android , Windows Phone , or Java ME platforms . Follow native coding conventions and UX guidelines for each individual platform . Make sure you follow these guidelines , otherwise your app may feel alien within the ecosystem , or may be rejected from app store approval altogether . <p> If you are using the HTML/CSS/JavaScript approach ( leveraging the- Apache Cordova- container ) , then you really want to focus on the users experience inside of the HTML container . You want to make sure the UI feels like " an app " , not like " a web page " . There are many client-side frameworks that help address this need . Feel free to use any of them , or roll your own just keep the UX/human interaction guidelines in mind . <p> For both native and hybrid @ @ @ @ @ @ @ @ @ @ performance vs actual raw processing- - power . These techniques deliver the appearance that the app is fast and responsive , instead of sluggish or locked while waiting to perform an action . Perceived performance improvements can be achieved simply by providing instant feedback , performing animations during an asynchronous request , or preemptive tasking . Do n't  miss this post , where I go into perceived performance in mobile apps in great detail . <p> The first is- app versioning ; MobileFirst Foundation- tracks each version of an app that you deploy , and gives you the ability- govern or restrict access to specific platforms and versions. - App versioning applies to all apps , native or hybrid , on any platform that MobileFirst Foundation- supports . The second is Direct Update , which allows you to push new HTML/CSS/JavaScript ( web ) resources to a MobileFirst- hybrid app . Direct Update only applies to hybrid apps , but it works for- any platform that MobileFirst- supports . <h> App Version Management <p> When you deploy an app to the MobileFirst Foundation server , the server- will automatically track @ @ @ @ @ @ @ @ @ @ file . <p> Set Application Version <p> When you load the MobileFirst Foundation Server Console , you 'll be able to view all of the deployed app platforms and versions . <p> The screenshot below shows a hybrid app deployed for both Android and iOS- platforms . You would also be able to see the exact same version and platform information for native apps that leverage IBM MobileFirst Foundation . <p> Managing Versions in the MobileFirst Console ( click to enlarge ) <p> Youll notice in the MobileFirst console that next to each platform/version- you can set the status for that version . This makes it possible to set notification messages for- users on specific platforms and versions , or even restrict access to specific platforms and versions . <p> For example , look at the screenshot above Version 1.0 on Android is active . Version 1.2 on iOS is active . Version 1.1 on iOS is notifying , and Version 1.0 on iOS is disabled . <p> There are 3 statuses that can be set for each platform and version combination. : Active , Active Notifying , and Access Disabled @ @ @ @ @ @ @ @ @ @ status of a platform/version , this status is only for that specific platform/version pair . This enables you to selectively notify users of specific versions , or even block access to specific versions if they are outdated and no longer supported . <p> " Active " means that the application is active . Services to this version will operate normally , and no messages will be presented to the user . <p> " Active Notifying " means that the application is active , services will continue to work , but a message will be presented to the user when the app becomes active , or when a service request is made to the MF server . <p> Setting Active Notification Message <p> This can be used to send any text-based message to the app users . This could be a deprecation notice , service maintenance notice , or any other general notice . <p> Within the app , the user will see a message when the app becomes active , or when a request is made to the server . This message can be dismissed , and the app functionality @ @ @ @ @ @ @ @ @ @ Notification Experience <p> " Access- Disabled " means that access to the application is disabled . In this state , a notification message will be presented to the user , and access from the app version will be disabled . The user will also be presented with an " Upgrade " button , which will redirect the user to any URL , which presumably will be for an updated version of the app . <p> Setting Disabled Status <p> In this state , the app will not be granted access to the MobileFirst/Worklight server . So , if your app requests- data from a data adapter , all requests to the adapter from this platform/version will be blocked . If- your app initialization code is inside of the Worklight clients connect:onSuccess handler , then this can- prevent your app from loading at all . <p> In-App Disabled Experience <p> Again , When you set the status of a platform/version , this status is only for that specific platform/version pair . <p> Direct Update is a feature for MobileFirst hybrid apps , which enables- you to push updated app- content ( @ @ @ @ @ @ @ @ @ @ having to deploy a new version of the app through the app store . <p> Direct Update is considered an additional security feature b/c it enforces users to use the latest version of the application code. - However , when an app uses Direct Update , it *only* updates the web resources. - No native changes or version # changes will be applied . However , it should not be abused . In particular this will bypass the- Apples app store approval process. - You should not overhaul the entire UI and break Apples- Human Interaction Guidelines , otherwise you could be kicked out of the app store . <p> Direct Update User Experience <p> By default , the updates user experience is a modal overlay that shows download and installation progress . The updaters UX can be configured to use silent updates that do not block the users experience , can be a completely custom user experience , or can be disabled altogether. - Updates can also be paused or resumed using the JavaScript API so that it does not block the user from performing a critical task , @ @ @ @ @ @ @ @ @ @ does not enable pause/resume . <p> Updates in the current version of Worklight ( 6.2 ) are complete updates containing the entire application ( www ) code , however- MobileFirst Foundation- 6.3 ( coming this month ) will have a Differential Direct Update feature- that includes- only- the changed files . More detail will be posted once this is available . <p> Direct Update can also be disabled if you do n't  want your hybrid apps to update automatically . <p> For more information on Direct Update , be sure to check out these additional resources : <h> IBM MobileFirst Platform Foundation <p> IBM MobileFirst Platform Foundation- ( formerly known as Worklight- Foundation ) is a platform for building mobile applications for the enterprise . - It is a suite- of tools and services available either on-premise or in the cloud , which enable you to rapidly build , administer , and monitor secure applications . <p> The MobileFirst Platform Foundation consists- of : <p> MobileFirst Server the middleware tier that provides a gateway between back-end systems and services and the mobile client applications . - The server enables application @ @ @ @ @ @ @ @ @ @ push notification management ( streamlined API for all platforms ) , consolidated logging , and app/services- analytics . For development purposes , the MobileFirst server is available as either part of the MobileFirst Studio ( discussed below ) , or as command line tools . <p> The server-side API enables you to expose data adapters to your mobile applications these adapters could be consuming data from SQL databases , REST or SOAP Services , or JMS data sources . The Server side API also provides a built-in security framework , unified push notifications ( across multiple platforms ) , and data **26;1646;TOOLONG services . You can leverage the server-side API in JavaScript , or dig deeper and use the Java implementation . <p> The client-side API is available for native iOS ( Objective-C ) , native Android ( Java ) , J2ME , C# native Windows Phone ( C# ) , and JavaScript for cross-platform hybrid OR mobile-web applications . For the native implementations , this includes user authentication , encrypted storage , push notifications , - logging , geo-notifications , data access , and more . - For hybrid @ @ @ @ @ @ @ @ @ @ plus cross-platform- native UI components and- platform specific application skinning . - With the hybrid development approach , you can even push updates to your applications that are live , out on devices , without having to push an update through an app store . - Does the hybrid approach leverage Apache Cordova ? - YES . <p> MobileFirst Studio - an optional all-inclusive development environment for developing enterprise apps on the MobileFirst platform . - This is based on the Eclipse platform , and includes an integrated server , development environment , facilities to create and test all data adapters/services , a browser-based hybrid app simulator , and the ability to generate platform-specific applications for deployment . - However , using the studio- is not required ! Try to convince a native iOS ( Xcode ) developer that they have to use Eclipse , and tell me- how that goes for you - If- you do n't  want to use the- all-inclusive studio , no problem . - You can use the command line tools ( CLI ) . - The CLI provides a command line interface for managing @ @ @ @ @ @ @ @ @ @ encrypted JSON store , and more . <p> MobileFirst Console the console provides a dashboard and management portal for everything happening within your MobileFirst applications . - You can view which APIs and adapters have been deployed , set app notifications , - manage or disable your apps , report on connected devices and platforms , monitor push notifications , view analytics information for all- services and adapters exposed through the MobileFirst- server , and manage- remote collection of client app logs . - All together , an extremely powerful set of features for monitoring and managing your applications . <h> MobileFirst Platform Application Scanning <p> MobileFirst Platform Application Scanning is set of tools- that can scan your JavaScript , HTML , Objective-C , or Java code for security vulnerabilities and coding best practices . - Think of it as a security layer in your software development lifecycle . <h> MobileFirst Quality Assurance <p> MobileFirst Quality Assurance- is a set of tools and features to help provide quality assurance to your mobile applications . - It includes automated crash analytics , user feedback and sentiment analysis , in-app bug reporting @ @ @ @ @ @ @ @ @ @ and- more . <p> So , is MobileFirst/Worklight- just for hybrid ( HTML/JS ) apps ? You tell me if you need clarification- more information , please re-read this post and follow all the links . - 
@@106848745 @2248745/ <h> Category Archives : Adobe <p> Mark your calendars ! Next Thursday , August 18th at 9:00AM to 10:30 AM ( PDT GMT-7 ) , the evangelism team will be hosting a live Q&amp;A chat . Come join us to learn more about Flex and AIR for mobile , and bring your questions with you ! <p> This session will be Q&amp;A only . There will be no demos , no slides , no speaking just pure Q&amp;A through Adobe Connect ( all you need is a web browser or the mobile app ) . Whether you are actively involved in mobile development , or about to start , join us and bring your questions ! <p> On Thursday morning , just go to http : //flex.org/ask and join us in the chat . I hope to see you there ! <p> How : We will have at least five evangelists in the Connect session to answer questions . This is a bit of a science experiment so we 'll see how it goes ! We will allow as many people in as we can handle , then we @ @ @ @ @ @ @ @ @ @ event will run for 90 minutes so you can come and go as you want . There are no presentations and no demos . It 's purely a Q&amp;A session . <p> Why : For many of us , building apps for mobile devices is a very dramatic shift . We 've never before had to deal with touch interfaces , varying screen densities , and input from the GPS and accelerometer . At the same time , we have to be much more conscience than ever before about resources , because our apps run on much slower CPUs with far less memory than we are used to and on OSes that will shut down your app if it crosses the line ! It 's not a trivial transition ! <p> Our team has been building real apps with Flex for several months and we have learned a lot . This is an opportunity for us to share that knowledge in an informal setting . We 've never attempted this before , but if it works well , we 'll do it often . If it 's a bust , @ @ @ @ @ @ @ @ @ @ few things to keep in mind for the chat : <p> Questions should be as specific as possible and limited to Flex on Mobile . <p> We ca n't promise that every question will be answered , but we 'll do our best . <p> We ca n't debug your code , that 's your job ! However , we can direct you to good online resources . <p> Keep it friendly and G-rated . We do n't want to see you on webcam ! <p> No flaming ! We 're here to answer specific questions , not to debate you about technology . <p> If you had n't  heard yet , Beta 2 of AIR 3.0 and Flash Player 11 are now availabe on Adobe Labs . The AIR 3.0 beta release is sporting some great new features , including hardware accelerated video playback for mobile , iOS background audio , android licensing support , front-facing camera support , encrypted local storage for mobile , H.264 software encoding for desktop applications , and last , but not least , captive runtime support for desktop and Android applications . <p> @ @ @ @ @ @ @ @ @ @ is , then I 'll try to explain Currently all AIR applications that are deployed on the desktop and in Android require the 3rd-party Adobe AIR runtime . If you are familiar with the process for developing mobile AIR applications for Apples iOS devices , then you may already know that these applications do n't  require the 3rd-party runtime ; they are completely self-contained applications . These AIR applications for iOS already take advantage of the captive runtime . All necessary components of the AIR framework are bundled into a self-contained , compiled distributable application that has no dependence upon other frameworks . <p> With AIR 3.0 , you will have the option to bundle the AIR framework into your applications to eliminate the 3rd-party dependency . However , one thing to keep in mind is that you can only export mac application packages on Macs and Windows EXEs on Windows . You ca n't target native installers or bundled runtimes for cross-platform development . You can only have a single app that targets both platforms if you export a . AIR file ( which requires the 3rd-party AIR runtime ) . @ @ @ @ @ @ @ @ @ @ with Molehill for mobile ( hardware accelerated 3D graphics ) , and I think it is something that many of you will be very excited about ( see video below ) . Everyone thinks that 3D is just for games , but its not 3D graphics can be used for complex data visualizations , scientific modeling , or a whole host of other subject areas . <p> I started down the path of creating 3D charts that are capable of rendering on mobile , using the Away3D engine . This is really just exploratory work , and I have it rendering multiple axes with 250 spheres ( individual data points ) , which is about 70K-90K polygons drawn onscreen at any given time . Check out the preview below : <p> Molehill ( Stage3D ) on Mobile <p> This preview is running on a Motorola Atrix , and Samsung Galaxy Tab 10.1 with a prerelease AIR runtime more devices will coming soon . Source code will be available once molehill for mobile has been released . 
@@106848746 @2248746/ <h> Featured PhoneGap Apps ! <p> Its always exciting to see what people in the PhoneGap community area up to . - Seeing featured apps in both iTunes and Google Play that are built with PhoneGap is fantastic . - Keep up the great work everyone ! <p> Ive checked Healthtapp app , and granted its better than an average PhoneGap app , it still suffers from occasional hiccups on displaying lists , constantly reloading pages ( I wonder why app would n't cache pages when I go back to them ) . <p> I 've tested on iPod touch 4th gen which is pretty average performance wise these days . App experience gets better when you use it on higher performance devices . <p> Vikas <p> Andrew any other App you are aware of which has been featured by Apple and built on Phonegap <p> Enrico Bertelletti <p> Digipea your favourite places http : //www.digipea.com is featured in both Italian and Spanish appStores ( lifestyle what 's hot section ) since his first version launch ( december 2012 ) . Built with Google Web Toolkit and Phonegap <p> Dan Fanelli <p> @ @ @ @ @ @ @ @ @ @ for you to check out links to its Android and Apple store downloads are here http : **30;1674;TOOLONG I used JQueryMobile for the UI and PhoneGap for the backend , thanks ! ! ! 
@@106848747 @2248747/ <h> PhoneGap Exploration Realtime Hardware Communication <p> Recently Ive undertaken some explorations with fellow evangelist- Kevin Hoyt , trying to determine how far we can push PhoneGap- applications with devices and physical computing . Turns out , you can push things really far and now I 'm delighted to share one of the experiments that we 've been pursuing . <p> Ive been asked on more than one occasion , can you access Bluetooth devices in PhoneGap applications . The answer is YES , you can . There is not a specific " Bluetooth API " in PhoneGap , however you can create native plugins to access any native library . Basically , with a native plugin , you create the native interface ( written in the native language ) , and a JavaScript interface . The native and JavaScript interfaces can leverage the PhoneGap native to JavaScript bridge for bidirectional communication . <p> In this exploration , we researched whether or not you can use a pressure sensitive stylus with a PhoneGap application . Again , the answer is YES , you can . Check out the video below to @ @ @ @ @ @ @ @ @ @ use of a TenOne Pogo Connect Stylus inside of a PhoneGap application . <p> Note : This is not connected to Project Mighty- in any way Kevin and I started exploring completely separately from the big announcements at MAX . <p> The Pogo Connect stylus leverages Bluetooth 4 Smart ( low energy ) connectivity to communicate with the device , and provides pressure sensitivity and a physical button for the user to interact with . The JavaScript interface does n't  interact directly with with the Bluetooth connection . Instead , I leveraged TenOnes Pogo Connect SDK and created a JavaScript bridge layer to delegate pen interaction from the SDK to the JavaScript layer . <p> There were definitely a few tricks to get this working . First , the SDK is designed to accept touch input at the native layer , and determine whether or not that touch is from the pen . When using the SDK ( in Objective-c ) , you are supposed to implement the touchesBegan , touchesMoved , touchesEnded , and touchesCancelled functions for a view : <p> The first catch with implementing this inside of @ @ @ @ @ @ @ @ @ @ handlers of a web view on iOS . Luckily , there is another way ! I leveraged a UIGestureRecognizer instance to intercept the touch input that is received by the web view , determine if the touches were from the PogoConnect Stylus , and if so , then delegate that input back to the JS layer . <p> UIGestureRecognizer is normally used as a base class for creating custom gestures in iOS applications . If has everything you need to handle touch input , and it gives you that information without having to subclass an actual view . This means you can attach it to any UIView instance . Since you do n't  have to subclass a view to use this , this plugin can be implemented in a PhoneGap native plugin without having to modify *any* code inside the PhoneGap framework . <p> So , here 's how I did it Once the PhoneGap plugin is initialized on load , I create a gesture recognizer instance and attach it to the PhoneGap applications web view . Whenever touch input is received by the gesture recognizer , that input is passed @ @ @ @ @ @ @ @ @ @ on the web view to pass that information to the JavaScript layer in real time as it is received . <p> On the JavaScript layer , stylus/pen input is dispatched to the application as custom events on the window.document object . To subscribe to pen input in JavaScript , you just add event listeners for the Pogo Connect events that I defined in the native plugins JavaScript file . <p> In JavaScript , you can do whatever you want within your application once you receive this information . <p> I created two sample applications to test this functionality . The first is a very basic app that simply outputs the pressure as text which follows the pen tip . The intent with this app was really just to prove the concept and determine if it was actually possible to receive and respond to information from the stylus . <p> Basic Pressure Sensitivity Detection in PhoneGap <p> Once I proved it could be done , the next logical step was to create an app that actually takes advantage of the pressure sensitivity information . So , I made a sketching app @ @ @ @ @ @ @ @ @ @ off by expanding on the drawing logic from my Lil Doodle PhoneGap application . This uses a requestAnimationFrame interval to render content in an HTML Canvas element using a " brush image " technique . Next , I added logic to vary the opacity and stroke size based on the pressure information received from the pen plugin , and a few other options to change the pen tip/brush shape and color . <p> The pressure sensitive stylus gives a few interesting interactions that you would n't get without the hardware : <p> First , the obvious , you know the pressure being applied to the pen tip , and the app can respond accordingly . <p> Next , you have an extra input method . The button on the pen allows you to interact with the device without having to actually touch the device . I used the button in two ways : first , if the button is pressed and held , the pen erases instead of draws . Second , if you double-tap the pen button , it brings up the drawing options . These options are where I @ @ @ @ @ @ @ @ @ @ <p> Third , the plugin provides bidirectional communication with the Stylus . When you change the pen color , the LED on the pen will display the selected color for a few seconds . <p> Note : This is iOS only The third-party PogoConnect SDK is for iOS devices only . This example will also ONLY work if you have the PogoConnect Stylus . It does not support other stylus devices or finger-only drawing . <p> I am trying to use Simon Mac Donalds latest bluetooth plugin ( for PG vesion 2.2.0 ) with Phonegap 3.0 . I could enable the bluetooth , but all the other functions are not working properly . It might be something different in the latest cordova-3.0.0.jar . If you find anything about bluetooth plugin in Phonegap 3.0 can you please post it here ? Cheers , Florin . 
@@106848748 @2248748/ <h> Category Archives : Drones <p> here 's an interview that I recently did with IBM DeveloperWorks TV at the recent World of Watson conference . In it I discuss a project Ive been working on that analyzes drone imagery to perform automatic damage detection using the Watson Visual Recognition , and generates 3D models from the drone images using photogrammetry processes . The best part the entire thing runs in the cloud on IBM Bluemix . <p> Its been a while since I 've posted here on the blog - In fact , - I just did the math , and- its been over 7 months. - Lots of things have happened since , I 've moved to a new team within IBM , built new developertools , worked directly with clients- on their solutions , worked on a few high profile keynotes , built apps for kinetic motion and activity tracking , built a mobile client for a chat bot , and even completed some new drone projects . - Its been exciting to say the least , but the real reason I 'm writing this post is to share a @ @ @ @ @ @ @ @ @ @ recent conferences . <p> I recently returned from Gartner Symposium and IBMs annual World of Watson conference , and- its been one of the busiest , yet most exciting span of two weeks Ive experienced- in quite a while . <p> At both events , we showed a project Ive been working on with IBMs Global Business Services team that focuses on the use of small consumer drones and drone imagery to transform Insurance use cases . In particular , by leveraging IBM Watson to automatically detect roof damage , in conjunction with photogrammetry to create 3D reconstructions and generate measurements of afflicted areas to expedite and automate claims processing . <p> This application leverages many of the services IBM Bluemix has to offer on-demand CloudFoundry runtimes , a Cloudant NoSQL database , scalable Cloud Object Storage ( S3 compatible storage ) , and BareMetal servers on Softlayer . Bare Metal servers are *awesome* I have a dedicated server in the cloud that has 24 cores ( 48 threads ) , 64 GB RAM , RAID array of SSD drives , and 2 high end multi-core GPUs . Its taken @ @ @ @ @ @ @ @ @ @ to 10 minutes for photogrammetric reconstruction with Watson analysis . <p> Its been an incredibly interesting project , - and you can check it out yourself in the links below . <h> World of Watson <p> World of Watson was a whirlwind of the best kind I had the opportunity to join IBM SVP of Cloud , Robert LeBlanc , on stage as part of the the Cloud keynote at T-Mobile Arena ( a huge venue that seats over 20,000 people ) to show off the drone/insurance demo , plus 2 more presentations , and an " ask me anything " session on the expo floor . <p> You can also check out my session " Elevate Your apps with IBM Bluemix " on UStream to see an overview in much more detail : <p> .. and that 's not all . I also finally got to see a complete working version of the Olympic Cycling teams training app on the expo floor , including cycling/biometric feedback , video , etc I worked with an IBM JStart team and wrote the video integration layer into for the mobile app using IBM @ @ @ @ @ @ @ @ @ @ <h> Drones <p> On this project we 've been working with a partner DataWing , who provides drone image/data capture as a service . However , I 've also been flying and capturing my own data . The app can process virtually any images with appropriate metadata , but I 've been putting both the DJI Phantom and Inspire 1 to work , and they 're working fantastically. 
@@106848749 @2248749/ <h> Tag Archives : HTML5 <p> Ive been spending a lot of time with Photoshop recently Whether it has been retouching video or images , creating panoramas , or working with my aerial photos , it has been a lot of fun . One thing that I 've been doing is exporting really large images to the web . So far this has been a very manual process Export from Photoshop using Zoomify . Then , since the default Zoomify renderer uses Flash ( and I want this consumable on mobile devices ) , take the Zoomify image tiles , and put them into a custom-coded HTML experience using the Leaflet tile- engine with a custom tile layer . <p> Leaflet is normally used for web-based mapping , but it is a perfect solution for rendering image tiles on the web . It already has touch and mouse interactions , inertial scrolling , progressive viewing , and a comprehensive API that can be extended if you so choose . <p> I 've done this enough times that I figured " There has to be an easier way " and there definitely @ @ @ @ @ @ @ @ @ @ you to export from Photoshops Zoomify feature directly to HTML , leveraging the Leaflet engine . All of the code and installation instructions are below in this post . Check out the video below to see it in action : <h> Samples <p> Here are few samples from the Zoomify output ; both are the compositions that I showed in the video above . Use the mouse or touch interactions to pan and zoom on each of them . <p> The first is an export from a 10MP aerial panorama ( 4340+2325 pixels ) , which was created by stitching together multiple images captured with a GoPro camera and remote controlled helicopter . <p> The second example is a massive 139MP composite image ( 14561+9570 pixels ) . I created this by stitching together 48 10mp images in Photoshop . Its not perfect , but shows how far you can zoom into an image some images had different exposures , some were out of focus , there is still some perspective warp , and I definitely have some bad stitching seams . This image is so huge that I actually @ @ @ @ @ @ @ @ @ @ hard disk space with the memory swap file when creating it ( I had over 100 Gigs of free space ) ! <p> Extract the zip file and copy the following files to Photoshops Presets/Zoomify directory . On OS X , with the default configuration , these files should be located in /Applications/Adobe Photoshop CC/Presets/Zoomify/ <p> L.TileLayer.Zoomify.js <p> Zoomify Leaflet HTML.zvt <p> leaflet.css <p> leaflet.js <p> Restart Photoshop . <p> When you have a file open that you want to export , choose File -&gt; Export -&gt; Zoomify- <p> Then select the " Zoomify Leaflet HTML " template that should now be in the list . Select an output location , base name , and image options , and hit " OK " . Ignore the browser width and height , since the template ignores these . Instead , it takes 100% of the width and height of the browser window . <p> - This will generate all of the image tiles and the HTML structure . From here , do whatever you want with it You can modify it , put it on a server , or anything else @ @ @ @ @ @ @ @ @ @ below . You will have a folder that contains the generated HTML file , the Leaflet JS and CSS files , and a directory that contains the generated tiles and appropriate XML metadata . <p> One criticism of PhoneGap apps that I sometimes hear is that they often do n't  have " standard " features from the native operating system . - Little things , like iOSs ability to scroll a container back to the top , just by tapping on the operating systems status bar . These types of features are not hard to add to a PhoneGap application , at all . This is more of an " attention to detail " issue , not something that the platform ca n't do . <p> Since this is n't a feature that is applicable on all platforms , and it can vary per PhoneGap app implementation , it is not part of the core PhoneGap/Cordova download . - However , this can be very easily added via a native plugin. - Native plugins enable you to access native code , or augment the capabilities of PhoneGap for a particular platform . @ @ @ @ @ @ @ @ @ @ today , you wo n't see this yet because I literally *just* submitted it to Apple . <p> So how did I do this ? <p> It was n't hard . The first thing to do is check and see if there was an existing native plugin that has already been created by someone in the PhoneGap/Cordova community . - It turns out , Greg- pointed out- one that already existed . - Since this plugin was built targeting an older version of PhoneGap , and my project was built using PhoneGap 3.0 , I had a few minor updates . - Though , I was able to get everything all set up in a very short period of time . <p> Then , in your PhoneGap application , you just have to add an event listener for the " statusTap " event , which is triggered when the user taps on the operating systems status bar . - It is literally this simple : <p> This just shows an alert that the status bar was tapped . If you want to animate specific containers , you have to do this manually @ @ @ @ @ @ @ @ @ @ do . here 's an excerpt that I used from the Halloween app , using jQuery syntax : <p> With Halloween less than a month away , I figured its about time to update my " Instant Halloween " PhoneGap sound effects app . I 'm happy to say that the latest version is now out for both iOS and Android . It has a few new sounds , a new UI style , and has been updated for iOS 7 . I also updated the low latency audio plugin- so it now supports PhoneGap 3.0 method signatures and supports the command line tooling for installation . <p> This app is fantastic for scaring people Just hook it up to really loud speakers , and start playing the sounds to your hearts content . Its got everything from background/ambient loops to maniacal laughter , screams , ghosts , zombies , and other spooky sound effects . <p> It is available now , for FREE , for both iOS ( 5.0+ ) and Android ( 4.0+ ) . <p> - <p> So what has changed in this version ? <p> First , I @ @ @ @ @ @ @ @ @ @ most part , this is a non-issue . PhoneGap apps are based on web standards , and HTML/JS/CSS work pretty much everywhere . However , you do have to account for a few minor changes . One is that the OS status bar now sits over top of the application . You 'll need to update your UI on iOS 7 , so there are no UI issues . Check out this post from Christophe Coenraets for details regarding creating PhoneGap apps for iOS 7 . <p> iOS 7 also introduces some new UI design paradigms and guidelines . I simplified the user interface , got rid of all textures , and tried to make things as simple and minimalistic and native-feeling as possible . I also got rid of iScroll for touch-based scrolling both the iOS and Android versions now use native inertial based scrolling from the operating system . This is the reason that the new Android version is only Android 4.0 and later , but it is also the reason that the app feels much closer to a fully native experience . <p> I updated the low latency @ @ @ @ @ @ @ @ @ @ were two parts to this : First , there is the updated method signature on the native interfaces . I just took the old plugin , and updated it for the new method signature . The new method signature- was actually introduced a while back , but I never updated the plugin for it. - Second , I added the appropriate XML metadata to enable CLI-based installation of the plugin for both iOS and Android . Take a look at the PhoneGap documentation for details on creating PhoneGap native plugins- and plugin.xml. - Also check out this tutorial from Holly Schinsky for help creating native plugins for Android . <p> In the process , I also ran into an unexpected issue with Android deployment Back in the spring I had a corrupted hard drive . I was able to recover *most* of my data , and I thought I had all of my app signing keys . It turns out that for Android , your apps must not only have the same signing keys , but also the same key store when you sign the APK files , or else @ @ @ @ @ @ @ @ @ @ an update ; it must be a new app . It turns out that I had recovered the key , but not the keystore . So , I had no choice but to distribute it as a new app . <p> Go download the free apps , get the source code , and start building your own PhoneGap apps today ! <p> CSS regions is a revolutionary- CSS specification draft- that allows a deeper separation of concerns in the way designers and developers structure their content and layout . They can now manage the way content should flow across different regions of the page design ( hence the name CSS Regions ) separately from the content itself . Then content can now be made to flow in different chains of regions , typically laid out differently for a mobile , tablet or desktop/laptop use . <p> Reflow now supports CSS Regions in the user interface/design surface , and Adobe Edge Code now supports CSS Regions in code hinting . Check out the video below to see CSS Regions in Reflow in action : <p> About a year ago I released @ @ @ @ @ @ @ @ @ @ with PhoneGap . The Fresh Food Finder provides an easy way to search for farmers markets near your current location , based on the farmers markets listings from the USDA . This app has seen a lot of popularity lately , so I 'm working on a new version for all platforms with a better data feed , better UI , and overall better UX unfortunately , that version is n't ready yet . However , I have been able to bring it to an additional platform this week : Firefox OS ! <p> Fresh Food Finder on iOS , Firefox OS , &amp; Android <p> PhoneGap support is coming for Firefox OS , and in preparation I wanted to become familiar with the Firefox OS development environment and platform ecosystem . So I ported the Fresh Food Finder , minus the specific PhoneGap API calls . The best part ( and this really shows the power of web-standards based development ) is that I was able to take the existing PhoneGap codebase , and turn it into a Firefox OS app AND submit it to the Firefox Marketplace in under 24 @ @ @ @ @ @ @ @ @ @ progress on Firefox OS support in the Cordova project , and it will be available on PhoneGap.com once its actually released . <p> Basically , I commented out the PhoneGap-specific API calls , added a few minor bug fixes , and added a few Firefox-OS specific layout/styling changes ( just a few minor things so that my app looked right on the device ) . Then you put in a mainfest.webapp configuration file , package it up , then submit it to the app store . Check it out in the video below to see it in action , running on a Firefox OS device <p> The phone I am using is a Geeksphone Firefox OS developer device . Its not a production/consumer model , so there were a few hiccups using it , but overall it was a good experience . Also , many thanks to Jason Weathersby from Mozilla for helping me get the latest device image running on my phone . <p> You can learn more about getting started with Firefox OS development here : 
@@106848750 @2248750/ <h> Tag Archives : adobe <p> Adobe MAX 2011 is rapidly approaching , are you ready ? Next week will bring some exciting announcements , so be sure to watch the MAX Keynotes steamed live online if you are n't  able to be there in person ( Monday AND Tuesday ) . Definitely do not miss them . <p> Adobe MAX 2011 <p> For those of you who will be able to attend , do n't  forget to get the MAX companion application , available for iOS and Android at- http : **26;1706;TOOLONG . The MAX Companion is an application that enables MAX attendees to check their session schedules and access general information about the MAX 2011 conference being held October 1-5 , 2011 in Los Angeles , CA . <p> MAX Companion <p> If you are interested in HTML5 , or Flex/AIR for mobile , also do n't  forget to stop by and check out my labs &amp; sessions : <p> Stay up to date on the latest developments in HTML , CSS , and javascript . Learn how to use HTML5 to add interactivity , motion , @ @ @ @ @ @ @ @ @ @ with the fundamentals of HTML5 and how it can be used today for both designers and developers . Get tips , tricks , and best practices from experts , while learning how to apply Adobes tools into your evolving workflow . Discover what can be done with next-generation web standards to create powerfully engaging websites that can transcend screens of all sizes . <p> Can you feel the excitement ? ! ? ! ? ! Adobe has just announced the availability of AIR 3 and Flash Player 11 for early October ! This release will bring a wave of change to the Internet and development tooling as you know them . From console-quality hardware accelerated 3D , to AIR captive runtime , to native extensions Adobe tools will enable the next wave of innovation in games , rich media , and multi-device applications . In this release there are a lot of new features to get excited about . <p> Flash Player 11 and AIR 3 will be publicly available in early October . Flash Builder and Flex will offer support for the new features in an upcoming release before @ @ @ @ @ @ @ @ @ @ lot of news and new content at MAX , and in the meantime , you can download the Flash Player and AIR betas on Labs , and start checking out some of the amazing content that 's already been built by developers ! <p> I awoke this morning to discover that Machinarium , originally developed as a Flash game and recompiled for iOS using Adobe AIR for mobile , is the #1 paid iPad game in Apples App Store , and the #2 overall paid iPad application . - Adobe Tools make great apps for mobile devices . <p> On the official Adobe Flex Team blog , there is a great post by Andrew Shorten discussing the future direction of Flex . I highly recommend taking a moment to read it . In that post Andrew points out where Flex is , and where Flex is heading . One thing I want to re-emphasize is that mobile is the next big thing . <p> It has been proclaimed many times , in many publications that mobile devices ( tablets and smartphones ) are the future of computing . This is @ @ @ @ @ @ @ @ @ @ of the catches with this growth is that each platform has its own development tooling and language . Wouldnt it be nice if you could just use one programming model &amp; technology stack ? Even better , would n't it be nice if you could use that same programming model &amp; technology stack to also develop applications for the web and the desktop ? Wow , what if you could even share code libraries across mobile , web , and desktop applications ? That would be awesome . <p> Wait you can already do that ! <p> Adobe Flex is the best tool for creating cross-platform , rich experiences in mobile , desktop , and web applications . <p> That is awesome . <p> One of the biggest enhancements introduced with Flash Builder 4.5.1 was the inclusion of mobile tooling . These tools allow you to easily create rich experiences targeting a variety of mobile devices iOS , Android , BlackBerry Playbook . All of which are natively installed applications that can be shared by the standard distribution models : App Store , Enterprise distribution , etc <p> The best part @ @ @ @ @ @ @ @ @ @ programming skills to develop and deploy for these platforms . You will need to learn about the app ecosystems , platform signing and deployment procedures , and device specifics ( soft keyboards , hardware buttons , etc .. ) . However , you can still develop these applications quickly and easily using Flex , ActionScript , and AIR APIs . One code base ; multiple platforms ; lots of devices . Did you know that you could even take that same code and deploy it to a TV ? Even better , there are great new advancements in the Flex/AIR mobile tooling waiting just over the horizon . <p> We 're continuing to focus on runtime performance , native extensions , new components , declarative skinning , adding more platforms and improving tooling workflows , such that in our next major release timeframe we expect that the need to build a fully-native application will be reserved for a small number of use cases . <p> The growth of the mobile market and the challenge of building out applications that work on a range of different form-factors and platforms present us @ @ @ @ @ @ @ @ @ @ new audience of developers , while continuing to be relevant for existing Flex developers who are extending their applications to mobile . <p> Flex &amp; AIR for mobile allow you to use the same enterprise class tooling to build cross platform mobile applications . You can still use existing framework components , existing open source libraries , the strongly typed programming language , automated ASUnit testing , build scripts , and many other features that Flex offers , and you can now target mobile devices . 
@@106848751 @2248751/ <p> Last week I was in good ole Las Vegas for IBM InterConnect IBMs largest conference of the year . With over 20,000 attendees , it was a fantastic event that covered everything from technical details for developers to forward-looking strategy and trends for C-level executives . IBM also made some big announcements for developers OpenWhisk serverless computing and bringing the Swift language to the server just to name a few . Both of these are exciting new initiatives- that offer radical changes &amp; simplification to developer workflows . <p> It was a busy week to say the least lots of presentations , a few labs , and even a role in the main stage Swift keynote . You can expect to find more detail on each of these here on the blog in the days/weeks to come . <p> For starters , here are two " lightning talks " I presented in the InterConnect Dev@ developer zone : <h> Smarter apps with Cognitive Computing <p> This session introduces the concept of cognitive computing , and demonstrates how you can use cognitive services in your own mobile apps . @ @ @ @ @ @ @ @ @ @ then I strongly recommend that you check out this post : The Future of Cognitive Computing . <p> In the presentation below , I show two apps leveraging services on Bluemix , IBMs Cloud computing platform , and the iOS SDK for Watson . <p> Actually , I 'm using two Watson SDKs The older Speech SDK for iOS , and the new iOS SDK. - I 'm using the older speech SDK in one example because it supports continuous listening for Watson Speech To Text , which is currently still in development for the new SDK . <h> Redefining your personal mobile expression with on-body computing <p> My second presentation highlighted how we can use on-body computing devices to change how we interact with systems and data . - For example , we can use a luxury smart watch ( ex : Apple Watch ) to consume and engage with data in more efficient and more personal ways . - Likewise , we can also use smart/wearable peripherals devices to access and act on data in ways that were- never possible- before . <p> For example , determining gestures or biometric @ @ @ @ @ @ @ @ @ @ on-body devices . - For this , I leveraged the new IBM Wearables SDK. - The IBM Wearables SDK provides a consistent interface/abstraction layer for interacting with wearable sensors . - This allows you to focus on building your apps that interact with the data , rather thank learning the ins &amp; outs of a new device-specific SDK . <p> The wearables SDK also users data interpretation algorithms to enable you to define gestures or patterns in the data , and use those patterns to act upon events when they happen without additional user interaction . - For example : you can determine if someone falls down , you can determine when someone is raising their hand , you can determine anomalies in heart rate or skin temperature , and much more . - The system is capable of learning patterns for any type of action or virtually any data being submitted to the system . - Sound interesting ? - Then check it out here . <p> I also had some other- sessions- on integrating drones with cloud services , integrating weather services in your mobile apps , and @ @ @ @ @ @ @ @ @ @ this- content- I make them publicly available . - I think you 'll find the session on drones + cloud especially interesting I know I did . <p> IBM Watson services , which are based on machine learning algorithms , - give you the ability- to handle unstructured data , like text analysis or- translation , speech processing , and more . - This makes consumption , mining , or responding to unstructured data or " dark data " faster , more efficient , and more powerful than ever . <p> The new Watson iOS SDK- provides- developers with an API- to simplify integration of the- Watson Developer Cloud services into their mobile apps , including the Dialog , Language Translation , Natural Language Classifier , Personality Insights , Speech To Text , Text to Speech , Alchemy Language , or Alchemy Vision services " all of which are available today , and can now be integrated with just a few lines of code . <p> The- Watson iOS SDK makes integration with Watson services significantly *really* easy . For example , if you want to take advantage of the Language @ @ @ @ @ @ @ @ @ @ instance . Once the translation service is setup , then you 'll be able to leverage translation capabilities within your mobile app : <p> Be sure to check out the sample 's readme for additional detail and setup instructions . As with all of the Watson services , You must have a service instance properly configured , with authentication credentials- in order to be able to consume it within your app . <p> I recently gave a presentation at IBM Insight on Cognitive Computing in mobile apps . - I showed two apps : one that uses Watson natural language processing to perform search queries , and another that uses Watson translation and speech to text services to take text in one language , translate it to another language , then even- have the app play back the spoken audio in the translated language . - Its this second app that I want to highlight today . <p> In fact , it gets much cooler than that . - I had an idea : " What if we hook up an OCR ( optical character recognition ) engine to the @ @ @ @ @ @ @ @ @ @ a picture of something , extract the text , and translate it . - It turns out , its not that hard , and I was able to put together this sample app in just under two days . - Check out the video below to see it in action . <p> To be clear , I ended up using a version of the open source Tesseract OCR engine targeting iOS . This is not based on any of the work IBM research is doing with OCR or natural scene OCR , and should not be confused with any IBM OCR work . - This is basic OCR and works best with dark text on a light background . <p> The Tesseract engine let 's you pass in an image , then handles the OCR operations , returning you a collection of words that it is able to extract from that image . - Once you have the text , you can do whatever you want from it . <p> So , here 's where Watson Developer Cloud Services come into play . First , I used the Watson Language Translation Service @ @ @ @ @ @ @ @ @ @ , I make a request to my- Node.js app running on IBM Bluemix ( IBMs cloud platform ) . - The Node.js app acts as a facade and delegates to- the Watson service for the actual translation . <p> You can check out a sample on the web here : <p> Translate english to : <p> On the mobile client , you just make a request to your service and do something with the response . The example below uses the IMFResourceRequest API to make a request to the server ( this can be done in either Objective C or Swift ) . IMFResourceRequest is the MobileFirst wrapper for networking requests that enables the MobileFirst/Mobile Client Access service to capture operational analytics for every request made by the app . <p> Once you receive the result from the server , then you can update the UI , make a request to the speech to text service , or pretty much anything else . <p> To generate audio using the Watson Text To Speech service , you can either use the Watson Speech SDK , or you can use the Node.js @ @ @ @ @ @ @ @ @ @ Text Service . In this sample I used the Node.js facade to generate Flac audio , which I played in the native iOS app using the open source Origami Engine library that supports Flac audio formats . <p> You can preview audio generated using the Watson Text To Speech service using the embedded audio below . Note : In this sample I 'm using the OGG file format ; it will only work in browsers that support OGG . <p> On the native iOS client , I download the audio file and play it using the Origami Engine player . This could also be done with the Watson iOS SDK ( much easier ) , but I wrote this sample before the SDK was available . <p> Cognitive computing is all about augmenting the experience of the user , and enabling the users to perform their duties more efficiently and more effectively . The Watson language services enable any app to greater facilitate communication and broaden the reach of content across diverse user bases . You should definitely check them out to see how Watson services can benefit you . <h> @ @ @ @ @ @ @ @ @ @ IBM MobileFirst offerings on Bluemix . In particular I am using the Mobile Client Access service to collect logs and operational analytics from the app . This let 's you capture logs and usage metrics for apps that are live " out in the wild " , providing insight into what people are using , how they 're using it , and the health of the system at any point in time . <h> Source <p> You can access the sample iOS client and Node.js code at https : **38;1734;TOOLONG . Setup instructions are available in the readme document . I intend on updating this app with some more translation use cases in the future , so be sure to check back ! <p> Last week I attended IBM Insight in Las Vegas . It was a great event , with tons of great information for attendees . I had- a few sessions on mobile applications . In particular , my dev@Insight session on Wearables powered by IBM MobileFirst was recorded . You can check it out here : <h> Key takeaways from the session : <p> Wearables are the most personal @ @ @ @ @ @ @ @ @ @ be notified of information , search/consume data , or even collect environmental data for reporting or actionable analysis . <p> Regardless of whether developing for a peripheral device like the Apple Watch or Microsoft Band , or a standalone device like Android Wear , you are developing an app that runs in an environment that mirrors that of a a native app . So , the fundamental development principles are exactly the same . You write native code , that uses standard protocols and common conventions to interact with the back-end . <p> Caveat to #1 : You user interface is much smaller . You should design the user interface and services to acomodate for the reduced amount of information that can be displayed . <p> You can share code across both the phone/tablet and watch/wearable experience ( depending on the target device ) . <p> Using IBM MobileFirst you can easily expose data , add authentication , and capture analytics for both the mobile and wearable solutions . <p> You may have heard a lot of buzz coming out of IBM lately about Cognitive Computing , and you might @ @ @ @ @ @ @ @ @ @ about ? " - You may have heard of- services for data and predictive analytics , services for natural language text processing , services for sentiment analysis , services understand speech and translate languages , but its sometimes hard to see the forest through the trees . <p> I highly recommend taking a moment to watch this video that introduces Cognitive Computing from- IBM : <p> Cognitive computing systems learn and interact naturally with people to extend what either humans or machine could do on their own . <p> They help human experts make better decisions by penetrating the complexity of Big Data . <p> Cognitive systems are often based upon massive sets of data and powerful analytics algorithms that detect- patterns and concepts that can be turned into actionable information for the end users . - Its not " artificial intelligence " in the sense that the services/machines act upon their own ; rather a system that provides the user tools or information that enables them to make better decisions . <p> The benefits of cognitive systems in a nutshell : <p> They augment the users experience <p> They @ @ @ @ @ @ @ @ @ @ complex information easier to understand <p> They enable you to do things you might not otherwise be able to do <p> Curious where this will lead ? - Now take a moment and watch this video talking about the industry-transforming opportunities that Cognitive Computing is already beginning to bring to life " <p> So , why is the " mobile guy " - talking about Cognitive Computing ? <p> First , its because Cognitive Computing is big I mean , really , really big . - Cognitive systems are literally transforming industries and providing powerful analytics and insight into the hands of both experts and " normal people " . - When I say " into the hands " , I again- mean this literally ; much of this cognitive ability is being delivered to those end users through their mobile devices . <p> Last , and this is purely just- personal opinion , I see the mobile MobileFirst offerings themselves as providing somewhat of cognitive service for developing mobile apps . - If you look at it from the operational analytics perspective , you have an immediate insight and @ @ @ @ @ @ @ @ @ @ would never have seen otherwise . - You can know what types of devices are hitting your system , what services- are being used , how long things are taking , and detect issues , all without any additional development efforts on your end . Its not predictive analytics , but sure is helpful and gets us moving in the right direction . 
@@106848752 @2248752/ <p> This post is inspired by all the comments Ive seen this week about JS in the enterprise . I would have never imagined- this 10- years ago , but JavaScript is now pretty much ubiquitous . Here are a few reasons why you need to paying attention to JavaScript if you are n't  already , and why you should definitely not- write it off . <p> First , I think one of the major reasons for JavaScripts ubiquity is that- JavaScript is approachable . It is relatively easy for beginners to learn JavaScript , and powerful enough for advanced users to build complex and reliable systems . <p> Second , why you need to pay attention , JavaScript is everywhere . <p> You can now use JavaScript to develop on virtually any platform : client side applications , server side logic , embedded chips/IoT devices , manage build scripts and dependencies , and more . <p> This does n't  mean you 'll use the exact same code in every case , rather that you can use the same skill set JavaScript Development to deliver solutions across multiple paradigms . @ @ @ @ @ @ @ @ @ @ power client side apps/user interfaces , and user interactions on numerous platforms and devices . <h> Web <p> Of course JavaScript powers the web , this is a given . JavaScript is the primary scripting language for all web browsers . I wo n't focus on this much b/c its already well known . <h> Mobile <p> JavaScript can also be used to power mobile applications that are natively installed on a device . <p> Apache Cordova/PhoneGap- You can build natively installed apps with web technology using PhoneGap or Cordova . PhoneGap is Adobes branded distribution of Cordova , but from the developers perspective , they are basically the same thing . Your app runs within a webview on the mobile device , and you build your user interface the same way you you build a dynamic web application . Your user interface- is implemented in HTML , styled with CSS , and all interactivity is created with JavaScript . <p> React Native- JavaScript powered web apps do n't  just have to be inside of a a web view . The React Native framework gives developers the ability to write their @ @ @ @ @ @ @ @ @ @ in a native application running on the mobile device . The logic is interpreted JavaScript at runtime , but everything that the user- interacts with ( all UI elements ) is 100% native , providing a very high quality user experience , and it is now available for both iOS and Android applications . <p> Unity 3D You can even develop rich &amp; immersive mobile 3D simulation or gaming experience , entirely powered by JavaScript using the Unity 3D engine . **These can be web , desktop , or mobile , but is often used in mobile gaming . <h> Desktop <p> Yup , desktop apps are not left out of the mix . Most desktop solutions fall into a category similar to Apache Cordova , where the end results is a web view that has access to lower level APIs , whose content is developed with web based technology . <h> The Server Side <p> Most obviously- Node.js a JavaScript runtime buit on Chromes V8 JavaScript Engine has made huge inroads into server side development and the enterprise . Node.js , powered by frameworks like express.js or loopback.io makes @ @ @ @ @ @ @ @ @ @ . <p> Here are some stats that- show the magnitude of growth and adoption for Node.js/npm.js alone . NPM- stats currently shows a total of- 186,946 packages available for download , 94,978,032 package downloads in the last day , and 2,451,734,737 package downloads in the last month . <p> This does n't  mean that JavaScript is the best language at everything . It- also does n't  meant that you can take a single piece of source- code and run it in every device/context imaginable . <p> It means that you can use your skills in JavaScript to develop for just about any kind of device/context out there . Its not going to be write once , run everywhere , rather in the words of the React.js team : learn once , write everywhere . 
@@106848753 @2248753/ <h> IBM MobileFirst &amp; Remote Client Side Logging in Mobile Apps <p> One of the many popular feature of IBM MobileFirst SDK is the ability to capture client-side- logs from mobile devices out in the wild in a central location ( on the server ) . - That means you can capture information from devices *after* you have deployed your app into production . - If you are trying to track down or recreate bugs , this can be incredibly helpful . Let 's say that users on iOS 7.0 , specifically on iPhone 4- models are having an issue . - You can capture device logs at this level of granularity ( or at a much broader scope , if you choose ) . <p> The logging classes in the MobileFirst Platform Foundation are similar in concept to- Log4J. - You have logging classes that you can use to write out- trace , debug , info , log , warn , fatal , or error messages . - You can also optionally specify a package name , which is used to identify which code module the debug statements are @ @ @ @ @ @ @ @ @ @ be able to see if the log message is coming from a user authentication manager , a data receiver , a user interface view , or any other class based upon how you setup your loggers. - Once the log file reaches the specified buffer size , it will automatically be sent to the server . <p> On the server you can setup log profiles that- determine the level of granularity of messages that are captured on the server . - Let 's say you have 100,000 devices consuming- your app. - You can configure the profiles to collect error or fatal messages for every app instance . - However , you probably do n't  want to capture complete device logs for every app instance ; You can setup the log profiles to only capture complete logs for a specific set of devices . <p> As an- example , take a look at the screenshot below to see how you can setup log collection profiles : <p> Configuring Log Profiles on the MobileFirst Server <p> When writing your code , you just need to create a logger instance , then write @ @ @ @ @ @ @ @ @ @ might want a trace statement , vs. a log statement , vs. a debug statement , etc Here is the usage level- guidance from the docs : <p> Then on the server , you can go into the analytics dashboard and access complete logs for a device , or search through all client-side logs with the ability to filter on application name , app versions , log levels , package name , environment , device models , and OS versions within an optional date range , and with the ability to search for keywords in the log message . 
@@106848755 @2248755/ <h> Unified Multi-Platform Push Notifications with IBM MobileFirst <p> Push notifications , love them or hate them , are everywhere and there 's no getting around it . Push notifications are short messages that can be sent to mobile devices regardless of whether the apps are actually running . They can be used to send reminders , drive engagement with the mobile app , notify completion of long running processes , and more . Push notifications- send information to you in real time , rather than you having to request that information . <p> If you are building a back-end infrastructure to manage your applications data , and you want to leverage push notifications , then guess what ? You also have to build the hooks to manage subscription and distribution of push notifications for each platform . <p> The- unified- push notification API allows you to develop your app against a single API , yet deliver push notifications to multiple platforms , and it works with both hybrid ( HTML/CSS/JS ) apps , as well as native apps . <p> You will still have to build the logic to @ @ @ @ @ @ @ @ @ @ , but you 'll only have to do it once against the unified API not once for each platform . <p> The apps that I showed in the video above are sample apps taken straight from the IBM MobileFirst platform developer guide for iOS and Android , and can be accessed in their entirety ( with both client and server code ) using the links below : <p> On the client app , you 'll need to subscribe for messages from the event source . See the hybrid or native code- linked to above for syntax and examples . <p> Once your clients are subscribed , you can use a single server-side implementation to distribute messages to client apps . Below is an- excerpt from the sample application which demonstrates sending a push notification to all devices for a particular user ( on any platform ) : <p> From- the MobileFirst console , you will be able to monitor and manage event sources , platforms , and the devices that are consuming push notifications . <p> Push Notifications on the MobileFirst Console <p> If you were wondering , yes , these can @ @ @ @ @ @ @ @ @ @ also be installed on-premise on your own server in your data center . - You have the option to configure- your physical or cloud servers however you want . 
@@106848756 @2248756/ <p> This post is inspired by all the comments Ive seen this week about JS in the enterprise . I would have never imagined- this 10- years ago , but JavaScript is now pretty much ubiquitous . Here are a few reasons why you need to paying attention to JavaScript if you are n't  already , and why you should definitely not- write it off . <p> First , I think one of the major reasons for JavaScripts ubiquity is that- JavaScript is approachable . It is relatively easy for beginners to learn JavaScript , and powerful enough for advanced users to build complex and reliable systems . <p> Second , why you need to pay attention , JavaScript is everywhere . <p> You can now use JavaScript to develop on virtually any platform : client side applications , server side logic , embedded chips/IoT devices , manage build scripts and dependencies , and more . <p> This does n't  mean you 'll use the exact same code in every case , rather that you can use the same skill set JavaScript Development to deliver solutions across multiple paradigms . @ @ @ @ @ @ @ @ @ @ power client side apps/user interfaces , and user interactions on numerous platforms and devices . <h> Web <p> Of course JavaScript powers the web , this is a given . JavaScript is the primary scripting language for all web browsers . I wo n't focus on this much b/c its already well known . <h> Mobile <p> JavaScript can also be used to power mobile applications that are natively installed on a device . <p> Apache Cordova/PhoneGap- You can build natively installed apps with web technology using PhoneGap or Cordova . PhoneGap is Adobes branded distribution of Cordova , but from the developers perspective , they are basically the same thing . Your app runs within a webview on the mobile device , and you build your user interface the same way you you build a dynamic web application . Your user interface- is implemented in HTML , styled with CSS , and all interactivity is created with JavaScript . <p> React Native- JavaScript powered web apps do n't  just have to be inside of a a web view . The React Native framework gives developers the ability to write their @ @ @ @ @ @ @ @ @ @ in a native application running on the mobile device . The logic is interpreted JavaScript at runtime , but everything that the user- interacts with ( all UI elements ) is 100% native , providing a very high quality user experience , and it is now available for both iOS and Android applications . <p> Unity 3D You can even develop rich &amp; immersive mobile 3D simulation or gaming experience , entirely powered by JavaScript using the Unity 3D engine . **These can be web , desktop , or mobile , but is often used in mobile gaming . <h> Desktop <p> Yup , desktop apps are not left out of the mix . Most desktop solutions fall into a category similar to Apache Cordova , where the end results is a web view that has access to lower level APIs , whose content is developed with web based technology . <h> The Server Side <p> Most obviously- Node.js a JavaScript runtime buit on Chromes V8 JavaScript Engine has made huge inroads into server side development and the enterprise . Node.js , powered by frameworks like express.js or loopback.io makes @ @ @ @ @ @ @ @ @ @ . <p> Here are some stats that- show the magnitude of growth and adoption for Node.js/npm.js alone . NPM- stats currently shows a total of- 186,946 packages available for download , 94,978,032 package downloads in the last day , and 2,451,734,737 package downloads in the last month . <p> This does n't  mean that JavaScript is the best language at everything . It- also does n't  meant that you can take a single piece of source- code and run it in every device/context imaginable . <p> It means that you can use your skills in JavaScript to develop for just about any kind of device/context out there . Its not going to be write once , run everywhere , rather in the words of the React.js team : learn once , write everywhere . <p> What I 'm about to show you might seem like science fiction from the future , but I can assure you it is not . Actually , every piece of this is available for you to use as a service . - Today . <p> Yesterday- Twilio , an IBM partner whose services are available- via- @ @ @ @ @ @ @ @ @ @ video chat as a service . - This makes live video very easy to integrate into your native mobile or web based applications , and gives you the power to do some very cool things . For example , what if you could add video chat capabilities between your mobile and web clients ? Now , what if you could take things a step further , and add IBM Watson cognitive computing capabilities to add real-time transcription and analysis ? <p> Jeff and Damion did an awesome job showing of both the new video service and the power of IBM Watson . I can also say first-hand that the new Twilio video services are- pretty easy to integrate into your own projects ( I helped them integrate these services into the native iOS client ( physicians app ) - shown in the demo ) ! - You just pull in the SDK , add your app tokens , and instantiate a video chat . - Jeff is pulling the audio stream from the WebRTC client and pushing it up to Watson in real time for the transcription and sentiment analysis @ @ @ @ @ @ @ @ @ @ of speaking at ApacheCon- in Austin , TX on the topic of data management for apps that work as well offline as they do online . - This is an important topic for mobile apps , since , as we all painfully know already , there is never a case when you are always online on your mobile devices . - There always ends up being a time when you need your device/app , but you cant get online to get the information you need . - Well , this does n't  always have to be the case . There are strategies you can employ to build apps that work just as well offline as they do online , and the strategy- I 'd like to highlight today is based upon data management using the IBM Cloudant NoSQL database as a service , which is based upon Apache CouchDB . <p> here 's a link to the presentation slides ( built using reveal.js ) just use the space bar to advance the presentation slides : <p> The " couch " in CouchDB is actually an acronym for Cluster of Unreliable Commodity Hardware @ @ @ @ @ @ @ @ @ @ of replication , which in the most basic of terms means that - data is shared between multiple sources . - Replication is used to share information between nodes of the cluster , which provides for cluster reliability and fault tolerance . <p> Cloudant is a- clustered NoSQL- database services that provides an extremely powerful and searchable data store . - It is- designed to power- the web and mobile apps , and all information is exposed via REST services . Since the IBM Cloudant service is based on CouchDB ( and not so coincidentally , IBM is a major contributor to the CouchDB project ) , replication is also core the the Cloudant service . <p> With replication , you only have to write your data/changes to a single node in the cluster , and replication takes care of propagating- these changes across the cluster . <p> If you are building apps for the web or mobile , there are options to extend the data replication locally either on the device or in the browser. - This means that you can have a local data store that automatically pushes @ @ @ @ @ @ @ @ @ @ and it can be done either via native languages , or using JavaScript . <p> If you want to have local replication in either a web or hybrid ( Cordova/PhoneGap ) app , you can use PouchDB. - PouchDB- is a local JavaScript database modeled- after CouchDB and implements that CouchDB replication API. - So , you can store your data in the browsers local storage , and those changes will automatically be replicated to the remote Cloudant store . - This works in the browser , in a hybrid ( web view ) app , or even inside of a Node.js instance . Granted , if you 're in-browser you 'll need to leverage the HTML5 cache to have your app cached locally . <p> If you are building a native app , do n't  worry , you can take advantage of the Cloudant Sync API to leverage- the local data store with replication . - This is available for iOS and- Android , and implements the CouchDB replication API . <p> The sample app that I showed in the presentation is a native iOS application based on the GeoPix MobileFirst @ @ @ @ @ @ @ @ @ @ - The difference is that in this case I showed it using the Cloudant Sync API , instead of the MobileFirst data wrapper classes , even though it was pointing at the exact same Cloudant database instance . - You can see a video of the app in action below . <p> All that you have to do is create a local data store instance , and then use replication to synchronize data between the local store and a remote store . <p> Replication be either one-way ( push or pull ) , or two-way . - So , any changes between the local and remote stores are replicated across the cluster . - Essentially , the local data store just becomes a node in the cluster . - This provides complete access to the local data , even if there is no network available . - Just save your data to the local store , and replication takes care of the rest . <p> In the native Objective-C code , you just need to setup the CDTDatastore manager , and initialize your datastore instance . <p> Once your datastore @ @ @ @ @ @ @ @ @ @ local store . - In this case I am creating a generic data object ( basically - like a JSON object ) , and creating a document containing this data . - A document is a record within the data store . <p> You can add attachments to the document or modify the document as your app needs . - In the code below , I add a JPG atttachment to the document . <p> Replication is a fire-and-forget process . - You simply need to initialize the replication process , and any changes to the local data store will be replicated to the remote store automatically when the device is online . <p> By assigning a replicator delegate class ( as shown above ) , your app can monitor and respond to changes in replication state . - For example , you can update status if replication is in progress , complete , or if an error condition was encountered . <p> If you want to access data from the local store , it is always available within the app , regardless of whether or not the device has @ @ @ @ @ @ @ @ @ @ method will return all documents within the local data store . <p> These two services enable you to quickly add Text-To-Speech or Speech-To-Text capability to any application . - What 's a better way to show them off than by updating my existing app to leverage the new speech services ? <p> I simply added the Text To Speech and Speech To Text services to my existing Healthcare QA application that runs on Bluemix : <p> IBM Bluemix Dashboard <p> These services are available via a REST API . Once youve added them- to your application , you can consume them easily within any of your applications . <p> I updated the code from my previous example- in 2 ways : 1 ) take advantage of the Watson Node.js Wrapper that makes interacting with Watson a lot easier and 2 ) to take advantage of these new services services . <h> Watson Node.js Wrapper <p> Using the Watson Node.js Wrapper , you can now easily instantiate Watson services in a single line of code . - For example : <p> The credentials come from your environment configuration , then you just @ @ @ @ @ @ @ @ @ @ . <h> QA Service <p> The code for consuming a service is now much simpler than the previous version . - When we want to submit a question to the Watson QA service , you can now simply call the " ask " method on the QA service instance . <p> Below is- my server-side code from app.js that accepts a POST submission from the browser , delegates- the question to Watson , and takes the result and renders HTML using a- Jade template. - See the Getting Started Guide for the Watson QA Service to learn more about the wrappers for Node or Java . <p> Compare this to the previous version , and you 'll quickly see that it is much simpler . <h> Speech Synthesis <p> At this point , we- already have a functional service that can take natural language text , submit it to Watson , - and return a search result as text . - The next logical step for me was to add speech synthesis using the Watson Text To Speech Service- ( TTS ) . - Again , the Watson Node Wrapper and @ @ @ @ @ @ @ @ @ @ On the client side you just need to set the src of an &lt;audio&gt; instance to the URL for the TTS service : <p> On the server you just need to synthesize the audio from the data in the URL query string . - Heres an example how to invoke the text to speech service directly from the Watson TTS sample app : <p> var textToSpeech = new **32;1774;TOOLONG ; // handle get requests app.get ( ' /synthesize ' , function ( req , res ) // make the request to Watson to synthesize the audio file from the query text var transcript = **34;1808;TOOLONG ; // set content-disposition header if downloading the // file instead of playing directly in the browser transcript.on ( 'response ' , function(response) **29;1844;TOOLONG ; if ( req.query.download ) **36;1875;TOOLONG ' = ' attachment ; filename=transcript.ogg ' ; ) ; // pipe results back to the browser as they come in from Watson transcript.pipe(res) ; ) ; <p> The Watson TTS service supports . ogg and . wav file formats . - I modified this sample is setup only with . ogg files . - @ @ @ @ @ @ @ @ @ @ HTML5 &lt;audio&gt; tag . <h> Speech Recognition <p> Now that were able to process natural language and generate speech , that last part of the solution is to recognize spoken input and turn it into text . - The Watson Speech To Text ( STT ) service handles this for us. - Just like the TTS service , the Speech To Text- service also has a sample app , complete with source code to help you get started . <p> A while back I wrote about adding parallax effects to your HTML/JS experiences to make them feel a bit richer and closer to a native experience . - I 've just added this subtle ( key word *subtle* ) effect to a new project and made a few changes I wanted to share here . <p> If you are wondering what I am talking about with " parallax effects " Parallax movement is where objects in the background move at a different rate than objects in the foreground , thus causing the perception- of depth . - Read more about it if you 're interested . <p> First , here 's a quick @ @ @ @ @ @ @ @ @ @ a hybrid MobileFirst app , but this technique could be used in any **35;1913;TOOLONG web app experience . - The key is to keep it subtle and not too much " in your face " , and yes , it is very subtle in this video . - You have to watch closely . <p> The techniques that I wrote about in the previous post still apply Ive just added a bit more to cover more use cases . <p> This sets the background image and default position . - The distinct change here is that I set the background size to " auto " width and 120% height . - In this case , you can have a huge image that shrinks down to just slightly larger than the window size , or a small image that scales up to a larger window size . - This way you do n't  end up with seams in a repeated background or a background that is too big to highlight the parallax effect effectively . <p> In the requestAnimationFrame loop , it only applies changes *if* there are changes to apply @ @ @ @ @ @ @ @ @ @ if the CSS styles had n't  changed . - In this , I also truncate- the numeric CSS string so that it is n't reapplying CSS if the position should shift by 0.01 pixels . Side note : If you are n't  using requestAnimationFrame for HTML animations , you should learn about- it . <p> If you used my old code and were holding the device upside down , it would n't work . - Not even a little bit . - This has that fixed ( see comments inline above ) . <p> This moves the background in CSS , which does n't  cause browser reflow operations , and moves the foreground content ( inside of a div ) using translate3d , which also does n't  cause browser reflow operations . - This helps keep animations smooth and the UX performing optimally . <p> I also added a global variable to turn parallax on and off very quickly , if you need it . <p> The result is a faster experience that is more efficient and less of a strain on CPU and battery . - Feel free to test this @ @ @ @ @ @ @ @ @ @ the code above , you can modify the xMovement and yMovement variables to exaggerate the parallax effect . 
@@106848757 @2248757/ <h> Monthly Archives : March 2013 <p> Next week I 'll be representing Adobe at GDC 2013 , and demonstrating how Adobe Creative Cloud , - PhoneGap , and PhoneGap Build- can be great tools for building casual gaming experiences. - In preparation , I 've been working on a gaming sample application that shows off the potential of HTML games packaged with PhoneGap . <p> and now I 'd like to introduce you to PhoneGap Legends . PhoneGap Legends is a fantasy/RPG themed demo that leverages HTML DOM animation techniques and targets webkit browsers . I was able to get some really outstanding performance out of this example , so be sure to check out the video and read the details below . The name " PhoneGap Legends " does n't  mean anything ; I just thought it sounded videogame-ish and appropriately fitting . <p> PhoneGap Legends <p> This game demo is an infinitely-scrolling top-view RPG themed game that is implemented entirely in HTML , CSS , and JavaScript . There is a scrolling background , enemy characters , HUD overlays , and of course , our " protagonist " hero @ @ @ @ @ @ @ @ @ @ mechanics like interacting with sprites . <p> Again , I was able to get some *really outstanding performance* out of this sample , so I wanted to share , complete with source code , which you 'll find further in this post ( and I encourage you to share it too ) . Take a look at the video below to see the game in action on a variety of devices . Every single bit of this is rendered completely with HTML , CSS , and JavaScript there are no native portions of the application . <p> Update 3/24 : - If you 'd like to test this out on your own devices , you can now access it online at- LONG ... - However , it will still only work on webkit browsers ( Chrome , Safari , Android , iOS , etc ) , and is optimized for small-device screens . - If you attempt to use this on a very large screen , you 'll probably see some sprite clipping . <p> Disclaimer : This sample app is by no means a complete game or complete game engine . Ive @ @ @ @ @ @ @ @ @ @ application with game-themed content , but it still needs additional game mechanics . I also wrote this code in about 2 days it needs some additional cleanup/optimization before use in a real-world game . <h> Source <p> Full source code for this demo application is available on GitHub . This code is provided as-is : - https : **37;1950;TOOLONG . I will be making a few updates over the next few days in preparation for GDC next week , but for the most part , it is stable . <p> In general , the DOM is as shallow as possible for achieving the desired experience . All " sprites " , or UI elements are basic DOM nodes with a fixed position and size . - All DOM elements have an absolute position at 0,0 and leverage translate3d for their x/y placement . - This is beneficial for 2 reasons : 1 ) It is hardware accelerated , and 2 ) there are very , very few reflow operations . - Since all the elements are statically positioned and of a fixed size , browser reflow operations are at an @ @ @ @ @ @ @ @ @ @ series of tiles that are repeated during the walk/movement sequence : <p> Sprite Sheet for Background Tiles <p> In the CSS styles , each tile is 256+256 square , with a background style that is defined for each " type " of tile : <p> The content displayed within each of the " sprite " DOM elements is applied using sprite sheets and regular CSS background styles . Each sprite sheet contains multiple images , the background for a node is set in CSS , and the position for each image is set using the " background-position " css style . - For example , the walking animation for the hero character is applied just by changing the CSS style that is applied to the " hero " &lt;div&gt; element . <p> Sprite Sheet for Hero <p> There is a sequence of CSS styles that are used to define each state within the walking sequence : <p> This game demo extensively uses translate3d for hardware accelerated composition . - However , note that the 3d transforms are all applied to relatively small elements , and are not nested . All @ @ @ @ @ @ @ @ @ @ texture size across all platforms ( 1024+1024 ) , and since it uses sprite sheets and reusable CSS styles , there are relatively few images to load into memory or upload to the GPU . <h> Attribution <p> The following Creative Commons assets were used in the creation of this sample app and the accompanying video : <p> Want to debug your PhoneGap apps , complete with breakpoints , DOM &amp; CSS inspection , profiling , and more ? - This is all possible with the- PhoneGap Emulator , - which allows you to leverage Chromes Developer Tools inside of the desktop Chrome browser ( covered in detail here ) . - However , did you also know that you can have a rich debugging/development experience in an app that is actually running on a device ? <p> Since the release of iOS 6 last Summer , weve all had the ability to debug PhoneGap apps while they are running on external iOS devices , or inside of the iOS simulator . Im surprised how often I hear that people are not aware of this feature . With iOS 6 @ @ @ @ @ @ @ @ @ @ HTML content on the device , either in the mobile Safari browser , or inside of a web view . PhoneGap apps fall into that second category , they are based upon iOS system web view . <p> You can check out a preview of remote debugging on iOS in the video below : <p> In order to take advantage of this , you 'll first have to enable the remote web inspector for Safari on iOS . Just follow the instructions for " Debugging Web Content on iOS " from Apple be sure not to skip the " Enable Web Inspector on iOS " - section , which is hidden by default . - You have to enable this in iOS Settings in order for the desktop Safari Browser to be able to connect to any web content on the mobile device . <p> Unfortunately , this is only available for PhoneGap on iOS devices at this time . Android enables remote debugging inside of the Chrome browser , however that is n't enabled for PhoneGap apps *yet*. - Whenever Google enables Chrome for web views inside of apps , its @ @ @ @ @ @ @ @ @ @ up a few points from my last post about Adobe Ideas &amp; Adobe Creative Cloud . I promise , I 'll get back to some technical &amp; development topics in my next post . I 'm just having too much fun with this right now <p> here 's another composition that I put together using Adobe Ideas , following the same workflow in my previous post : I sketched this composition in Adobe Ideas , then leveraged Adobe Creative Cloud to continue the editing process on the desktop . <h> Video : Adobe Ideas Composition <p> Disclaimer : I am not an artist or illustrator . - I primarily focus on writing software the art stuff is just for fun . <p> Now , I want to elaborate a bit more about the Creative Cloud workflow . Creative Cloud is not just a subscription to all of Adobes creative and Edge tools . Its also not just a " hard drive in the sky " that synchronizes your creative assets across your devices . It is both of these , and much more . <p> We 've already covered that Creative Cloud empowers the @ @ @ @ @ @ @ @ @ @ You can create content using the Adobe Touch apps , and pull that into the desktop tools for refinement . For example , let 's take the sketch that I created in the video above If I want to isolate and extract individual assets , I can do that very easily using the desktop tools : <p> Isolating Layers in Adobe Illustrator <p> I can also pull these layers into Photoshop for further enhancement . In this case , color correction , and rearranging some content ( oh look , more elephants ! ) . <p> Editing in Adobe Photoshop <p> Now , let 's consider workflow beyond just creation of content . What about collaboration and feedback ? Adobe Creative Cloud empowers collaborative experiences too . Everything from simple sharing , to comments/feedback , even having the ability to break down and view individual layers of PSD files . Check out the video below for some more detail : <h> Video : Adobe Creative Cloud Workflow <p> You can share any of your Creative Cloud assets by email or simple URLs , where that content and workflow can be consumed on @ @ @ @ @ @ @ @ @ @ ? Just send them a link to the composition work in progress , and they can leave comments whenever they want . <p> Sharing and Collaboration with Adobe Creative Cloud <p> In fact , If you 're interested , you can check this composition out for yourself . Here are the assets that I used to create this composition , which Ive shared publicly . Although , I disabled the ability to download the original files . You can toggle PSD layers , or leave comments ( just be nice ) : <p> here 's a quick video introduction I put together for Adobe Edge Inspect , a *free* tool that enables synchronized browsing and debugging of HTML content between desktop and mobile devices . Its definitely worth checking out , if you are n't  using it already . <p> Adobe Edge Inspect enables synchronized browsing across desktop and mobile devices . By pairing your devices with Edge- Inspect- running on your desktop , any content you view in the desktop browser will be synched to all paired mobile devices . <p> Adobe Edge Inspect also makes it easy for you to @ @ @ @ @ @ @ @ @ @ With just one button click , you can capture screenshots from all connected devices , complete with information about each device ( OS , device name , screen resolution , etc ) . <p> Screen Captures <p> Did I mention Adobe Edge Inspect is also free ? Its part of the free tier of Adobe Creative Cloud . <p> In this post I 'm changing things up a bit Rather than focusing just on developer-centric topics , I 've decided to show a little more creative workflow using Adobe Ideas . - Adobe Ideas is an awesome sketching app for the iPad . You can sketch whatever you want , whenever you want . It is all saved as vector content that you can then pull into Adobe Illustrator to integrate into your desktop workflow using Adobe Creative Cloud. - Take a look at the video below to see it in action : <p> Once you save your sketch , it automatically gets synched with your Creative Cloud account . You can then pull the composition directly into Illustrator and use it within other creative suite tools . <p> here 's the output @ @ @ @ @ @ @ @ @ @ You can export as raster ( png , jpg , etc ) or vector ( ai , svg , etc ) formats , so it can be a great tool for enabling you to hand-sketch high- quality- graphical assets for your creative workflow . 
@@106848758 @2248758/ <h> Tag Archives : pltaform <p> here 's a quick preview of my new HTML5/PhoneGap data vizualization app . Once released , this will be available on multiple platforms , in multiple app stores , AND it will be completely open source . - Just waiting on app store approvals <p> You can expect a full writeup on how this was built after it is released . <p> In this series , I 'd like to highlight Adobes mobile offerings , and the variety of platforms that can be supported from a single codebase. - - I wrote a simple URL Monitoring utility application that will check the status of various HTTP endpoints using the AIR URLMonitor class . - I now have that application running on 3 OS platforms , in 4 ecosystems ( 5th coming soon ) , supporting phone and tablet interfaces , and all with a single codebase <p> Many Apps , One Platform <p> here 's the official app description that I 've been using : <p> URL Monitor is a simple diagnostic application that will allow you to quickly and easily monitor the status of various URL @ @ @ @ @ @ @ @ @ @ and add it to the list . A polling HTTP request will be made every 10 seconds to determine the availability of a given endpoint . HTTP codes 200 , 202 , 204 , 205 and 206 will be identified as a success with a green check . All other HTTP codes will indicate a problem as a red X. To remove a listing , simply perform a horizontal swipe on a given row to reveal a delete button . All monitoring is paused when the application is in the background . <p> Flex/AIR on Devices : From Flash Builder to BlackBerry App World ( coming soon ) <p> What 's really interesting about this is the activity that I have seen on each market . - The application is free on each market , and became available over the last few weeks . - I have done absolutely no- promotion- of it This is the first announcement that I have made . <p> As of 9/15/2011 , the application has been live in Apples store for 3 weeks , and has had 554 downloads ; it has been live in @ @ @ @ @ @ @ @ @ @ had 20 installes ( only 11 active ) ; it has been in the Amazon market for 2 weeks , with 0 installs ( yes , that is a zero ) ; and surprisingly , it has been on the Nook store since Sept 1 , and has had 816 downloads ! - Now , that was unexpected . <p> Once you are a registered developer , you will have access to tools that enable you to put your apps onto a nook device . - First , you must download and install the Nook SDK add-on to the Android SDK . <p> Nook SDK Setup <p> This will download and install the Nook SDK. - Once installed , the installation process will need to restart ADB. - Click " YES " and allow it to proceed through the restart process . <p> Nook SDK Setup - Complete <p> Once you are a registered developer , you will have tools accessible to enable " Developer Mode " on your device , which allows you to provision the device for development . - Go to the " Developer Mode " section @ @ @ @ @ @ @ @ @ @ and serial numbers for your dev devices . - From here you will be able to download a provisioning file . <p> Nook Developer Provisioning <p> The provisioning file will allow you to deploy applications directly to the nook via USB. - - Attach the Nook Color device to your computer via a USB connection . - The Nook Color device will show up as a device in Finder. - Copy the provision.cmd file that you downloaded from the " Developer Mode " provisioning portal directly into the root of the Nook Color device . <p> Once ADB has restarted , use the " adb devices " command to list all connected Android devices ( The Nook Color device must still be connect via USB , even though it has been unmounted ) . <p> . /adb devices- <p> You should see your device listed in the output : <p> ADB - View Device <p> Now that you are able to view your Nook Color device using ADB , Flash Builder will also be able to deploy to it . - You will be able to Run/Debug directly from Flash @ @ @ @ @ @ @ @ @ @ - - This follows the normal Flash Builder debug process . <p> However , there is one trick you may notice that if you go to the " apps " menu on the Nook color , you wo n't see any of your applications deployed via USB. - Do n't  fret they actually are on your device . - There is just a trick to view them . - From the " apps " menu , click on the " archived " button . - A popup will be displayed here . - Now , press and hold the " volume up " button . - When the speaker appears on the screen ( continue holding the volume button ) , tap on the speaker . - When you tap on the speaker , the " extras " screen is displayed . - ( You can release the volume button now. ) - The " extras " screen will list all of your manually installed applications , and you will be able to re-launch your installed apps . <p> From here , you can generate your release-build APK as you normally @ @ @ @ @ @ @ @ @ @ generated and tested , you 're ready to prepare it for the Nook Store . - Sign into the Nook Developer Portal , and go to the " Applications " section . - Click on the " Add New Application " button . <p> First , you will need to enter the primary metadata for the application ( name , type , price , version , etc ) . <p> Nook Portal - New Application <p> Once you have completed the basic information , click on " Save and Continue " , and you will be redirected to the " Description &amp; EULA " tab . - Enter an application description and an optional license agreement ( I left this blank b/c I do not have any special terms for my app ) . <p> Nook Portal - App Description <p> When ready , click " Save and Continue " to enter the " Keywords and Category " tab . - On this screen , enter descriptive keywords and categories to help categorize your application within the Nook store . <p> Nook Portal - Keywords &amp; Category <p> When you have @ @ @ @ @ @ @ @ @ @ Continue " to proceed to the " Icons &amp; Screenshots " screen . - Here you will need to set the application icon and add application screenshots . <p> Nook Portal - Media &amp; Images <p> Now this is where the application approval process is a little different from other app stores . - You need to click " Send for Application Approval " , and the application metadata must be approved before you can upload a binary APK file . <p> Once the application metadata is approved ( this took about a week for my app ) , you will be able to upload the APK binary . <p> Nook Portal - Upload Binary <p> After you have uploaded and submitted the binary APK file , the binary file will need to be approved before it is actually available within the Nook store . - The binary approval took another week+ , so the overall approval process took over 2 weeks . - Once approved , your application will appear in the nook store , and will be ready for public consumption ! <p> First , navigate to- https @ @ @ @ @ @ @ @ @ @ Android " link . Once there , you will need to walk through the full registration process to create your account . <p> Amazon Appstore Landing Page <p> Once you click on the " Get Started " button , you will be guided through the Appstore registration process . Once that is complete , you will be directed to the Appstore developer portal home page . - From here , just click on " Add a New App " . <p> Amazon Appstore Home <p> Next , you will being the App upload wizard . - First , you will need to enter primary metadata for the application , including a title , form factor , supported languages , and contact information . - Once you have entered this information , click on the " Save " button . <p> Amazon Appstore App Metadata <p> Next , you need to enter merchandising information . - This includes the app category , keywords , a description , price , and release/availability dates . - Once you have completed all of your merchandising information , click on the " Save " button @ @ @ @ @ @ @ @ @ @ Next , you will have to specify content rating information . - Just fill out the information about your content , and click on the " Save " button to proceed . - I did n't  run into any content rating issues in the Amazon Appstore , like I did with the iOS App Store . <p> Amazon Appstore App Content Ratings <p> Next , upload multimedia that will be associated with the application . - This includes application icons ( note : they must match , event though this screenshot does n't  show it I was rejected b/c of this ) , and actual screenshots of the application . <p> Amazon Appstore App Multimedia <p> Scroll down to see more of the " Multimedia Content " form . - You will also be able to enter promotional images and promotional video assets for your application . - Once you have uploaded all necessary multimedia , click the " Done " button . <p> Amazon Appstore App Promotional Media <p> Now you are ready to upload your APK Binary . - Follow the instructions for uploading an APK file . @ @ @ @ @ @ @ @ @ @ the file that was uploaded . <p> Amazon Appstore App Binary <p> Finally , click on the " Submit App " button to submit your application for approval . - There is an approval process for the Amazon Appstore similar to Apples , and my application was live in less than a week from submission . <p> Once you have successfully uploaded the application APK , you will be prompted with the details about your application . - Click " Save " to being entering application metadata information . <p> Android Market APK Uploaded <p> You will need to upload at least 2 screenshots of your application , as well as an application icon for use within the Android Market . <p> Android Market APK Metadata <p> Next , scroll down to the " Listing Details " area and begin entering metadata for your application . - This includes an application title and description , as well as release notes , classifications , and promotional text . <p> Android Market APK Metadata - Description <p> Next , scroll down to the " Publishing Options " screen , where you can @ @ @ @ @ @ @ @ @ @ and available markets . <p> Once you have completed the form in its entirety , scroll back up to the top of the page and click on the " Save " button to save content , or " Publish " to save and publish the application to the Android Market . - Once you have successfully published it , your application will show up in your home dashboard , as shown below . <p> Android Market APK App Ready <p> That 's it ! - The Android Market makes it extremely easy to publish your application . - My application started showing up in the market after about 30 minutes from pressing the " Submit " button . 
@@106848759 @2248759/ <h> MegaList jQuery Plugin <p> Ive been working on lots of different projects lately . - On several of them , I 've had the need for a reusable list component . - In some cases , it needed to handle a large data set , in others it just needed to be self-contained and easy to use . - Out of these projects came MegaList : a reusable list component for jQuery , which Ive released as open source on Github . <p> MegaList is a jQuery plugin that creates a touch-enabled list component , capable of very large datasets , complete with data virtualization . It was originally intended for touch-enabled devices , however it also works in many desktop browsers . <p> For performance optimizations , the list component uses data virtualization techniques , so there are never more list elements in the HTML DOM than what is currently visible on the screen . As the user scrolls through content , the list updates the DOM elements accordingly . This makes scrolling lists of thousands of items extremely fluid . - This works in a very similar @ @ @ @ @ @ @ @ @ @ . <p> You can employ the list component using one of two approaches . - One option is to declare the list structure in HTML markup , another option is to specify a dataProvider array , from which the list will create DOM elements . <h> Why ? <p> Sometimes you need a pre-built list that you can reuse . Sometimes you need to scroll through big data sets , and other times you just need component logic kept away from your app logic . It does n't  fit every scenario , but it certainly fits a few . <p> Data virtualization techniques allow you to quickly scroll through massive lists , without performance degradation . However , if your app really has 100K list items to scroll through , you should fire your UX designer . <h> Samples <p> View the " samples " directory to see the scrollable list component in action . All samples are interactive , and scrollable via touch or mouse events , with function event handlers . <p> Each of these examples can be scrolled using either the mouse or finger , and just @ @ @ @ @ @ @ @ @ @ selection handler ( alert message ) . On the desktop , you can also scroll with the scrollbar . Note : I originally intended this for mobile on the desktop , I 've only tested in Chrome and Safari . <p> Simple List Created With Inline LI Elements This is a basic example with a list of 50 LI elements . <p> Note : These inline/embedded examples are contained inside of iframes if you mouse-up outside of the iframe , the iframe contents wont receive the event . If MegaList is used in a page , without a wrapping iframe , you do n't  run into this issue . Follow the " View Sample " links above to see them without the iframe issue . <h> Observations <p> Contrary to my expectations , using CSS3 translate3d is actually slower than using CSS top/left when placing the virtualized content . If you enable CSS3 translate3d and set backface visibility , there is an extremely noticeable performance degradation on both desktop and mobile devices . <p> I 've experimented with lots of permutations to get the best performance possible . I 'm not finished yet @ @ @ @ @ @ @ @ @ @ performance of DOM manipulation by removing elements from the DOM , manipulating them , then re-adding them . This is what is done within the updateLayout() method . The &lt;ul&gt; is removed from the DOM , &lt;li&gt; elements are added or removed , and then the &lt;ul&gt; is added back to the DOM . You may see a flicker on rare occasions , but I did n't  find this overly intrusive . <p> For small data sets , this may not be much advantage you can get better performance by just using something like iScroll in a &lt;div&gt; containing a &lt;ul&gt; . With large data sets , this is definitely faster . <p> The more complex the HTML inside of your label function , the slower the animation will be . <h> Download <p> The full source code for this component is available on Github . Check out the landing page for API documentation and samples . 
@@106848760 @2248760/ <h> Status Tap/Scroll To Top in PhoneGap Apps on iOS <p> One criticism of PhoneGap apps that I sometimes hear is that they often do n't  have " standard " features from the native operating system . - Little things , like iOSs ability to scroll a container back to the top , just by tapping on the operating systems status bar . These types of features are not hard to add to a PhoneGap application , at all . This is more of an " attention to detail " issue , not something that the platform ca n't do . <p> Since this is n't a feature that is applicable on all platforms , and it can vary per PhoneGap app implementation , it is not part of the core PhoneGap/Cordova download . - However , this can be very easily added via a native plugin. - Native plugins enable you to access native code , or augment the capabilities of PhoneGap for a particular platform . <p> If you download the app from the app store today , you wo n't see this yet because I literally *just* submitted it @ @ @ @ @ @ @ @ @ @ ? <p> It was n't hard . The first thing to do is check and see if there was an existing native plugin that has already been created by someone in the PhoneGap/Cordova community . - It turns out , Greg- pointed out- one that already existed . - Since this plugin was built targeting an older version of PhoneGap , and my project was built using PhoneGap 3.0 , I had a few minor updates . - Though , I was able to get everything all set up in a very short period of time . <p> Then , in your PhoneGap application , you just have to add an event listener for the " statusTap " event , which is triggered when the user taps on the operating systems status bar . - It is literally this simple : <p> This just shows an alert that the status bar was tapped . If you want to animate specific containers , you have to do this manually yourself via JavaScript . Again , that is n't hard to do . here 's an excerpt that I used from the Halloween app , @ @ @ @ @ @ @ @ @ @ at strictFail LONG ... <p> at Object.write LONG ... <p> at XMLParser.feed LONG ... <p> at ElementTree.parse LONG ... <p> at Object.exports.XML LONG ... <p> at **42;1989;TOOLONG LONG ... <p> at LONG ... <p> at LONG ... <p> at ChildProcess.exithandler ( childprocess.js:635:7 ) <p> I was able to install from Andrew . It works like a charm . The only thing I had an issue with was removing and adding **26;2033;TOOLONG . It caused some weirdness for me . I 've tried using that technique before and also had issues . It 's not a huge deal though since it still scrolls up without doing that . Thanks ! 
@@106848761 @2248761/ <h> Tag Archives : creative cloud <p> Last week , new release-candidate versions of Camera Raw and Lightroom were posted to Adobe Labs that feature additional camera and lens support . I was extremely excited when I found out that one of the new camera profiles supported is the GoPro- Hero 3 . I 'm a huge fan of GoPro- cameras , and this means we now have more ways to get more creative with their usage . <p> I was recently thinking I absolutely love the shots I get off of the GoPro , but I wish there was an easy way to reduce the fisheye distortion . I wanted to try my hand at creating some aerial panoramas , but the distortion was causing issues . You can reduce the distortion using Photoshops Adaptive Wide Angle filter , but that can be tedious to get right . This release makes the process of reducing fisheye dead simple . <p> Reducing the lens distortion is now as simple as selecting the GoPro camera profile in Camera Raws lens correction settings , and you can use it to create some @ @ @ @ @ @ @ @ @ @ how to apply GoPro lens correction to both images and videos in Adobe Photoshop , Adobe Lightroom , and Adobe Bridge . <h> Images <p> Here are some side-by-side comparisons of before and after applying the GoPro lens correction . <h> Panoramas <p> I 've also been able to use this feature to create some awesome ( in my opinion ) aerial panoramas using the- DJI Phantom- quad-rotor remote controlled helicopter . The easiest way to create one of these panoramas is to select the images you want to merge inside of Adobe Bridge . Then right-click and select " Open in Camera Raw " , and then apply the lens correction to all of the images . Once you 've done that , keep the same images selected and go to Tools -&gt; Photoshop -&gt; Photomerge inside of Adobe Bridge . This will launch the Photomerge process inside Photoshop , and after a few minutes , you will have a nice panorama to work with . Take the generated panorama , turn it into a smart object , and then you can start applying other filters ( including Camera Raw ) @ @ @ @ @ @ @ @ @ @ . <p> Here are a few panoramas Ive created . Click on any one of them to view an interactive panorama , where you can zoom into the full resolution of the image . <p> To make the panoramas interactive , I used Photoshops " Zoomify " export , combined with the Leaflet- mapping library for an interactive HTML experience . You can ignore the HTML it generates , but keep the images and XML configuration file . I then used this open source Zoomify Layer for Leaflet- to make the images fully interactive , without any plugin dependencies . You can pinch/zoom and pan the images , and they are loaded as individual tiles , so its a smooth experience for the end user . <h> Next Steps <p> Go get started , and have fun ! You can download Camera Raw 8.2 and Lightroom 5.2 release candidates from labs.adobe.com just make sure to get the correct Camera Raw plugin for your suite ( CS6 or CC ) . <p> Also , check out this video produced by Adobes own Russell Brown for additional information : <p> I have @ @ @ @ @ @ @ @ @ @ , I am completely addicted . Perhaps " obsessed " might be the proper term . Luckily for me , it is a great use of Creative Cloud , so I can get away with it ! What is this new obsession , you ask ? Aerial Photography . <p> However , I 'm not flying airplanes or riding in helicopters . In fact , my feet are n't  even leaving the ground at all just my camera . It all started a few weeks ago when my friend- Tony- approached me about getting some aerial photos for experimentation. - He knew I already had a GoPro camera , and that I was completely obsessed with it GoPros take incredible pictures , and are very durable . The only thing was that we needed a reliable way to get it off the ground . <p> Enter part 2 of the equation , and the greatest part of my obsession : the quadcopter. - Basically , it is a multi-rotor remote controlled helicopter. - Multi-rotor copters are mechanically very simple , very stable , and versatile . Just check out this TED @ @ @ @ @ @ @ @ @ @ . <p> I had been wanting one of these for a long time . Researching here and there , looking at the DIY and pre-built kits . I wanted something that would be easy to fly , but also capable of outdoor flight and a camera payload . Copters can range from inexpensive childrens toys- to high-end hex/octo-rotor professional rigs for many thousands of dollars , and I looked at nearly every one of them . I finally settled on the DJI Phantom . <p> I chose this copter mainly for two reasons : <p> It has a GPS guidance system that makes it easy to fly , corrects for wind drift , and even has a " return to home " feature if the battery gets low or if it loses contact with the controller . Yet , it can be very maneuverable and agile. - <p> DJIs promise of " ease of use " definitely holds true . It was setup and running very quickly , with the longest part of the setup process waiting for the battery to charge . Once we finally got it off the @ @ @ @ @ @ @ @ @ @ everything that I look at , I wonder " Wouldnt that be cool to take a picture of from the air ? " <p> A word of warning , once you start down this path , it is very easy to become obsessed I will elaborate more on this subject in a bit . For now , check out some of my recent photos with this rig . You can see the full collection ( and any future photos ) on my Flickr photostream . <p> In fact , we were so excited to fly it , that we did n't  even wait for daylight ! The Phantom has a series of LEDs for navigation , so there was no problem flying at night , at all . here 's a short video from my first quadcopter flight : <p> While this setup takes amazing still images out-of-the-box , you may have noticed that the video is n't all that smooth . Some of the shaking can be reduced by refining your piloting techniques ( do n't  ascend/descend too fast , etc ) . However , the default configuration is n't ideal for @ @ @ @ @ @ @ @ @ @ corrected with some ad-ons ( which I will be investing in ) , and software-based video stabilization in After Effects . Again , I 'll cover this more in a bit <h> Lessons Learned <p> My explorations into aerial photography have provided me with some valuable lessons <p> Accept the fact that you will crash. - Even though the Phantom is very easy to fly , at some point you will , without a doubt , crash the copter . Luckily every piece of the copter can be replaced . If your battery is almost dead and you try to take off , the copter will just flip over onto the ground . If you are trying to land and there is a strong gust of wind , you will likely crash . If you descend too quickly , you will crash . I mention each of these , because I have done all of them . <p> Immediately replace the nuts that hold the propellers onto the copter. - Within the first week of owning the Phantom , I lost a propeller and the copter came- plummeting- to the ground @ @ @ @ @ @ @ @ @ @ of the ground when this happened . The " dome " nuts that ship with the copter look pretty , but are not reliable . Get some new nuts with lock washers , Nylock nuts , and/or some thread lock to prevent the nuts from loosening due to vibration . If you use thread lock , just be sure NOT to get any on the plastic propellers . I have heard it will do bad things to them . <p> Be prepared to obsess over the ways that you can use and modify the copter . This leads me to my next section <p> Add-Ons <p> If you are like me , you will immediately want to explore all of the ways that you can use the Phantom once you get it . This will lead you down the road of watching countless YouTube videos that others have created , reading DIY instructions for modifications , and scouring the web for upgrade options . <p> Must haves : <p> Extra batteries . The Phantom comes with one battery that lasts 10-12 minutes . You will want more batteries for additional @ @ @ @ @ @ @ @ @ @ . ( See my lessons learned above ) <p> Of course , not all modifications are " must haves " ; here are the next few options that are on my list . If anyone is looking for a gift idea for me , look at this list ! <p> Travel Case. - Without- a doubt , you will want to protect your copter . I have mine in a cardboard box lined with bubble-wrap to prevent damage from travel and toddlers . If you plan on travelling with your copter ( which I do ) , you will want something like one of these . <p> Camera Gimbal System. - A gimbal system will keep your camera stable , regardless of the pitch or yaw of the copter . This will make a huge difference if you are producing video . There are lots of options , from DIY kits , to the official DJI/Zenmuse Gimbal system that was just released today . I am drooling over these options . <p> Vibration Isolation Mount. - If you have a gimbal , you probably already have this , but if @ @ @ @ @ @ @ @ @ @ vibration isolation mount will reduce the copter vibrations that are transferred to the camera . This will result in better quality images and videos , and less " jello effect " . <p> ND/Polarized filters for the GoPro camera. - ND filters and polarized filters can help improve the captured image or video , and can minimize the " jello effect " due to vibrations . <p> FPV Video Navigation System . An FPV ( First Person View ) video system gives you the ability to see exactly what the copter sees when you are flying . This can be really useful for framing of photos , or flying beyond line-of-sight . Most of these will require you to modify your copter . You can use the GoPro app as a limited FPV- system ; although keep in mind that it operates over an ad hoc wifi connection . If you lose the connection , you lose the video feed . <p> I mentioned that people are creating amazing images and videos with this rig ; here are just a few that blew me away <h> Creative Cloud <p> So @ @ @ @ @ @ @ @ @ @ but what about Creative Cloud ? - How does it tie into quadcopters ? Creative Cloud enables you to do more with the aerial imagery that you capture from the quadcopter . <p> When taking still images , I setup the camera so that it will- automatically- take two pictures every second . When you 're flying for 10 minutes , this ends up being a lot of images . Lightroom enables you to quickly import , tag , develop , and publish your images . <p> Many of the images need additional editing before they are suitable for your final product . Whether you need to remove objects ( such as landing gear , shadows , people , etc ) , or you want to tweak clouds or make other dramatic changes , Photoshop enables you to do these things. - Photoshop enables rich editing and retouching of images . <p> Of course , its not all about still images . Premiere and After Effects enable editing and production of your captured videos . You can use Premiere to arrange multiple clips into a broader sequence , and you can @ @ @ @ @ @ @ @ @ @ <p> and what is a video without audio ? Audition provides you with a rich environment for audio production . Whether it is editing the captured audio , or producing new music and soundtracks , Audition has you covered . <p> here 's another video that I produced using Premiere for edits , stabilization with After Effects , and Audition for audio tweaks . Granted , I do not have a gimbal or other stabilization system , so bear with the quality ! <p> This is just the proverbial " tip of the iceberg " . Creative Cloud gives you the tools you need to create amazing multimedia content , and the means to make that content engaging and interactive from hobbyist to die-hard professional . <p> Once you 're happy with what you 've got , do n't  forget you can share it on Behance via your Creative Cloud membership ! <p> The latest version of Premiere Pro CC is seriously awesome . There are tons of new features and performance improvements everything from cloud synchronization of settings across computers , to the new Mercury playback engine , to asset browsing , @ @ @ @ @ @ @ @ @ @ , I think my absolute favorite feature in Premiere Pro CC is the ability to synchronize multiple video clips based on their audio tracks . <p> You can simply select the clips that you want to have synchronized , right-click , select " Synchronize " , then select the " Audio " option . <p> This feature will temporally align the video clips by analyzing their audio tracks . You no longer have to compare waveforms manually to try and align everything to the precise frame . For me , this has been a tremendous time saver . Check it out , in action , in the video below ! <p> This is one of the many features and product updates that were launched in Adobe Creative Cloud today . If you are n't  a member , you 're missing out on all of the latest and greatest features . Head on over to creative.adobe.com- to become a Creative Cloud member and get started today ! <p> The latest release of Adobe Creative Cloud tools are now available ! - This includes updates for over - 15 tools , including Photoshop @ @ @ @ @ @ @ @ @ @ CC , Dreamweaver CC , Edge Animate CC , Edge Code CC , Edge Inspect CC , After Effects CC , and more ! - This release is Adobes best yet , and not only includes our best tools , but also greater integration with Adobes Cloud services . - You can learn more about everything in todays release from the Adobe Creative Cloud Team blog , or go directly to creative.adobe.com to get all of these applications today ! <p> The update to Creative Cloud that is coming in June is loaded with awesome tools and incredible new features . I recently demonstrated shake reduction in Photoshop , which can greatly enhance photos that are blurred from a shaky camera , but that 's not the only great update coming in June . Another feature that I wanted to highlight is the Sound Remover- process in Adobe Audition CC . <p> The Sound Remover enables you to select specific sound frequencies and patterns , and remove them from a sound file/composition . Imagine that you have a great recording which was ruined by a cell phone ringing , birds chirping @ @ @ @ @ @ @ @ @ @ Now it is possible to easily remove those specific frequencies and patterns without losing or damaging the entire audio file . Check out the video below for an example . <p> In a nutshell , the process is this : <p> In the Spectral Frequency Display , use the paintbrush selection tool to select the frequencies and patterns you want to remove . 
@@106848763 @2248763/ <h> Interview : Gathering &amp; analyzing data with drones &amp; IBM Bluemix <p> here 's an interview that I recently did with IBM DeveloperWorks TV at the recent World of Watson conference . In it I discuss a project Ive been working on that analyzes drone imagery to perform automatic damage detection using the Watson Visual Recognition , and generates 3D models from the drone images using photogrammetry processes . The best part the entire thing runs in the cloud on IBM Bluemix. 
@@106848764 @2248764/ <h> Monthly Archives : March 2014 <p> My second article on aerial imaging with a remote controlled helicopter is now live in the March 2014 issue of Adobe Inspire ! - The first article focused on aerial videography and Adobe video tools . This time its all about aerial photography with a GoPro camera and DJI Phantom ( and how to bring these images to life with Photoshop and Lightroom ) . <p> First , you can make so-so video look great with a few simple color correction techniques . Second , a video is only as good as its audio , so you need solid audio to keep viewers engaged . - Hopefully this post helps you improve your videos with simple steps on both of these topics . <p> To give you an idea what I 'm talking about , check out this before and after video . Its the exact same clip played twice . - The first run through is just the raw video straight from the camera and mic. - Colors do n't  " pop " , its a little grainy , and the audio @ @ @ @ @ @ @ @ @ @ color correction applied to enhance the visuals , and also has processed audio to enhance tone , increase volume , and clean up artifacts . <p> Let 's first look at color correction . - Below you can see a " before " and " after " still showing the effects of color correction . - The background is darker and has less grain , there is more contrast , and the colors are warmer . <p> Before and After Color Correction <p> The visual treatment was achieved using two simple effects in Adobe Premiere Pro . - First I used the Fast Color Corrector to adjust the input levels . - By bringing up the black and gray input levels , the background became darker , and it reduced grain in the darker areas . - Then , I applied the " Warm Overall " Lumetri- effect to make the video feel warmer this enhances the reds to add warmth to the image . <p> You can get by with a mediocre video with good audio , but nobody wants to sit through a nice looking video with terrible audio @ @ @ @ @ @ @ @ @ @ help improve your audio , and hopefully keep viewers engaged . <p> In this case , I thought the audio was too quiet and could be difficult to understand . - My goal was to enhance audio volume and dynamics to make this easier to hear . <p> I first used Dynamics Processing to create a noise gate . This process removes quiet sounds from the audio , leaving us with the louder sounds , and generally cleaner audio . - You could also use Noise Reduction or the Sound Remover effects the effect that works best will depend on your audio source . <p> Dynamics Processing ( Noise Gate ) in Adobe Audition <p> Next I used the 10-band graphic equalizer to enhance sounds in specific frequency ranges . - I brought up mid-range sounds to give more depth to the audio track . <p> 10 Band EQ in Adobe Audition <p> Finally , I used the Multiband Compressor to enhance the dynamic range of the audio . - Quieter sounds were brought up and louder sounds were brought down to create more level audio that is easier to @ @ @ @ @ @ @ @ @ @ to make your audio too loud when using the compressor ! - If you 've ever been watching TV and the advertisements practically blow out your eardrums , this is because of overly compressed audio . <p> Multi-band Compressor in Adobe Audition <p> Want to learn more ? - Do n't  miss the Creative Cloud Learn resources to learn more about all of the Creative Cloud tools the learning resources are free for everyone ! - If you are n't  already a member , join Creative Cloud today to access all Adobe media production tools . 
@@106848765 @2248765/ <h> Walkable Restaurants <p> I 'd like to take a moment and introduce " Walkable Restaurants " ( or just " Walkable " as I like to call it ) , a new application that I built to demonstrate how to create PhoneGap applications using Backbone.js . <p> Walkable Restaurants provides you with an easy way to find a bite to eat or refreshing drink anywhere in the US . Either select a type of cuisine , or enter a search phrase , and the Walkable Restaurants app will find destinations that are nearby . - Only destinations near your current location will be returned in your search result . If we are able to calculate travel times based upon walking and train schedules , then we will only return destinations within a 20-minute walk . If we are not able to calculate walk time , only destinations within 2.5 miles of your current location will be displayed . <h> Data Services <p> All walking/travel time distance calculations are obtained through Travel Time , by Igeolise . This service calcualtes travel time between geographic locations based upon walking , driving @ @ @ @ @ @ @ @ @ @ POI ) information is obtained through services provided by factual.com . This application uses the U.S. Restaurants data set , which provides information for over 800,000 restaurants across the United States , including location , price ratings , cuisine , and more . <p> The server-side portion of this application simply aggregates data from Factual and Travel Time , and is developed with node.js , using the expressjs framework . 
@@106848766 @2248766/ <h> Heard of the Creative Cloud Packager ? <p> The Creative Cloud Packager is a tool for CC Enterprise and CC Team- customers that enables them to easily package Creative Cloud products and updates for deployment within their organizations . - It let 's you select specific Creative Cloud products and/or updates and package them into . pkg or . msi installers ( optionally with a serial number for Enterprise customers ) . - These packages can then be deployed on their own or integrated with third-party deployment tools like JAMF Casper or Microsoft SCCM. - The Creative Cloud Packager even let 's you control Creative Cloud update behaviors and more . <p> With the recent releases of Creative Cloud Packager , you can now edit existing deployment packages , create deployment packages from local media ( DVDs ) , and even create deployment packages for older ( CS6 ) creative applications , if you have the proper license . 
@@106848767 @2248767/ <h> Tag Archives : android <p> Next week I 'll be representing Adobe at GDC 2013 , and demonstrating how Adobe Creative Cloud , - PhoneGap , and PhoneGap Build- can be great tools for building casual gaming experiences. - In preparation , I 've been working on a gaming sample application that shows off the potential of HTML games packaged with PhoneGap . <p> and now I 'd like to introduce you to PhoneGap Legends . PhoneGap Legends is a fantasy/RPG themed demo that leverages HTML DOM animation techniques and targets webkit browsers . I was able to get some really outstanding performance out of this example , so be sure to check out the video and read the details below . The name " PhoneGap Legends " does n't  mean anything ; I just thought it sounded videogame-ish and appropriately fitting . <p> PhoneGap Legends <p> This game demo is an infinitely-scrolling top-view RPG themed game that is implemented entirely in HTML , CSS , and JavaScript . There is a scrolling background , enemy characters , HUD overlays , and of course , our " protagonist " hero all @ @ @ @ @ @ @ @ @ @ like interacting with sprites . <p> Again , I was able to get some *really outstanding performance* out of this sample , so I wanted to share , complete with source code , which you 'll find further in this post ( and I encourage you to share it too ) . Take a look at the video below to see the game in action on a variety of devices . Every single bit of this is rendered completely with HTML , CSS , and JavaScript there are no native portions of the application . <p> Update 3/24 : - If you 'd like to test this out on your own devices , you can now access it online at- LONG ... - However , it will still only work on webkit browsers ( Chrome , Safari , Android , iOS , etc ) , and is optimized for small-device screens . - If you attempt to use this on a very large screen , you 'll probably see some sprite clipping . <p> Disclaimer : This sample app is by no means a complete game or complete game engine . Ive implemented @ @ @ @ @ @ @ @ @ @ with game-themed content , but it still needs additional game mechanics . I also wrote this code in about 2 days it needs some additional cleanup/optimization before use in a real-world game . <h> Source <p> Full source code for this demo application is available on GitHub . This code is provided as-is : - https : **37;2061;TOOLONG . I will be making a few updates over the next few days in preparation for GDC next week , but for the most part , it is stable . <p> In general , the DOM is as shallow as possible for achieving the desired experience . All " sprites " , or UI elements are basic DOM nodes with a fixed position and size . - All DOM elements have an absolute position at 0,0 and leverage translate3d for their x/y placement . - This is beneficial for 2 reasons : 1 ) It is hardware accelerated , and 2 ) there are very , very few reflow operations . - Since all the elements are statically positioned and of a fixed size , browser reflow operations are at an extreme @ @ @ @ @ @ @ @ @ @ of tiles that are repeated during the walk/movement sequence : <p> Sprite Sheet for Background Tiles <p> In the CSS styles , each tile is 256+256 square , with a background style that is defined for each " type " of tile : <p> The content displayed within each of the " sprite " DOM elements is applied using sprite sheets and regular CSS background styles . Each sprite sheet contains multiple images , the background for a node is set in CSS , and the position for each image is set using the " background-position " css style . - For example , the walking animation for the hero character is applied just by changing the CSS style that is applied to the " hero " &lt;div&gt; element . <p> Sprite Sheet for Hero <p> There is a sequence of CSS styles that are used to define each state within the walking sequence : <p> This game demo extensively uses translate3d for hardware accelerated composition . - However , note that the 3d transforms are all applied to relatively small elements , and are not nested . All of @ @ @ @ @ @ @ @ @ @ size across all platforms ( 1024+1024 ) , and since it uses sprite sheets and reusable CSS styles , there are relatively few images to load into memory or upload to the GPU . <h> Attribution <p> The following Creative Commons assets were used in the creation of this sample app and the accompanying video : <p> I think some of the most common questions people ask when developing PhoneGap apps are about performance . How do I make my app faster ? How do I make it feel native ? Any tips of making an app feel at-home on a particular platform ? How do you match native OS " look and feel " ? In this post , I 'll attempt to provide some insight on techniques for creating great PhoneGap apps , and also try to debunk a few myths floating around the " interwebs " <p> When we are talking about performance , we are really talking about perceived performance and responsiveness from an end-user perspective . Since PhoneGap applications are based upon HTML &amp; web views , you are limited by the performance of a web @ @ @ @ @ @ @ @ @ @ that does n't  mean all HTML-based apps are- inherently- slow . There are very many successful apps with an HTML-based UI . Some even get featured in the app stores . We cant make the web views faster , but we can help you make your HTML experience faster inside of that web view . <p> However if the web view is n't fast enough for your taste , do n't  forget that you can use PhoneGap as a subview of a native app . In this case , you can leverage native user interface elements when you need them , and leverage PhoneGaps HTML/JS user interface when you need it. - Then use PhoneGap/Cordovas native-to-JS bridge to handle bidirectional communication between the native code and the JavaScript inside of the web view- component . More on this later <h> HTML/Web View Performance <p> First , let 's talk specifics about mobile HTML &amp; web performance . There are a lot of common " tips " floating around , some of these are extremely valid , although others are misleading . Please allow me to clear some things up <h> Hardware Acceleration @ @ @ @ @ @ @ @ @ @ generic statements telling you to force hardware acceleration whenever possible , in every scenario , just by adding a css translate3d transform style : <p> transform : translate3d(0,0,0) ; <p> While it is true that you can force GPU rendering of HTML DOM content , you really need to understand how it can impact the performance of your application . In some cases , this can greatly improve the performance of your app , while in other cases , it can create performance issues that are extremely difficult to track down . In other words : Do not use this for every case , and on every HTML DOM element . <p> You should be very selective when you use transtale3d to force hardware acceleration for a number of reasons . <p> First , whenever you are using translate3d , the content being accelerated takes up memory on the GPU . Over-use of GPU rendering on HTML DOM elements can cause your application to use up all of the available memory on the GPU . When this happens , your app could either crash completely without any error messages or @ @ @ @ @ @ @ @ @ @ device RAM or permanent storage . In either of these cases , you will have worse performance than you did without forced GPU rendering . <p> Second , whenever you apply a translate3d transform , this causes the content of the HTML DOM element to be uploaded on the GPU for rendering . In many cases , the GPU upload time is imperceptible , and the results are positive . However , if you apply the translate3d style to a very large and very complex HTML DOM element , a- noticeable- delay can be encountered , even on the latest and greatest devices and operating systems . The more complex the HTML DOM , the larger the delay . When this happens , the web view ( or browser ) UI thread can be completely locked up for that period . While the worst Ive seen of this was less than a second , it still can have a profoundly negative impact when your entire UI locks up for 500ms . In addition , any time you make changes to a DOM element ( content or style-wise ) , that @ @ @ @ @ @ @ @ @ @ Third , avoid nesting DOM elements that use translate3d . Let 's consider a case where you have lots of DOM elements inside of a &lt;div&gt; elements , multiple levels deep , and all of those elements , including the outtermost &lt;div&gt; use translate3d for GPU rendering . For the outter-most &lt;div&gt; to be rendered , all of its child elements have to be rendered . In order for all of those child elements to be rendered , all of their child elements have to be rendered , etc This means that all of the child elements have to be uploaded to the GPU before the parent element can be rendered . This adds up very quickly . Nested elements that use the css translate3d GPU hack will take longer to load , and can take up significantly more memory , causing very poor performance , slow GPU upload times , or complete app crashes- ( see paragraphs above ) . <p> Fourth , consider the maximum texture size for the GPU . If the DOM element is larger than the maximum texture size supported by the GPU ( in either @ @ @ @ @ @ @ @ @ @ poor performance and visual artifacts . Have you ever seen HTML DOM elements flicker during an animation or when scrolling , when using translate3d ? If so , then there 's a good chance that the DOM elements dimensions are larger than the texture size supported by the devices GPU . The maximum texture size on many mobile devices is 1024+1024 , but varies by platform . The maximum texture size on an iPad 2 is 2048+2048 , where the maximum texture on the iPad 3/4 is 4096+4096 . If the device maximum texture size is 1024 , and you DOM elements height is 1025 pixels , you will have those pesky flickering and performance issues . <p> Note : In some platforms , you can also have flickering due to the browser implementation , which can often be corrected by setting backface-visibility css . See details here . <p> Fifth , remember that you are on a mobile device . Mobile devices tend to have slower CPUs , slower bus , and less memory than desktop devices . While you may get great results leveraging translate3d on the desktop , @ @ @ @ @ @ @ @ @ @ , different platforms have different levels of support for hardware acceleration , and performance can vary . <p> Translate3d can definitely- yield- positive results , but can also- yield- negative results too , depending upon how it is used . - Use it wisely , and always , always , always test on a device . <h> The Impact of Content Reflow <p> If you 've never heard of " reflow " before , you should pay attention . " Reflow " is the browser process for calculating positions and geometries for HTML DOM elements . This includes measurement of width &amp; height , the wrapping of text , the flow/relative position of DOM elements , and basically everything related . <p> The calculations used by the reflow processes can be computationally expensive , and you want to minimize this cost and the amount of content reflow to achieve optimal performance . If you 've ever animated the width of a HTML DOM element , and seen the framerate drop to to 5 FPS or lower , then you 've seen the impact of reflow processes in action . <p> Reflow processes are @ @ @ @ @ @ @ @ @ @ resized , CSS **27;2100;TOOLONG are changed , etc While you may not notice any impact of reflow processes on desktop browsers , reflow can have a significant performance impact on mobile devices . You can never get rid of reflow processes altogether ( well , unless your content absolutely never changes ) , but you can mitigate the impact of those relflow processes . <p> If you 'd like to learn more about reflow processes , I strongly suggest reading these : <p> If you 've seen lists of tips for PhoneGap &amp; Mobile web apps , you may have seen statements like " minimize DOM nodes " , " avoid deeply nested html " , use css animations , preload images , etc The reason that these are suggested is that they are tips for minimizing the impact and occurrence of reflow operations . <p> Minimize DOM node instances The fewer DOM nodes , the less amount of nodes that have to be measured and calculated in reflow operations . <p> Avoid deeply nested HTML DOM structures The deeper the HTML structure , the more complex and more expensive reflow operations @ @ @ @ @ @ @ @ @ @ will cause reflow operations all the way up the tree , ending up more computationally expensive . <p> Use CSS transforms In addition to forcing hardware acceleration as discussed above , you can alter HTML DOM elements *without* invoking reflow processes if you change them using CSS3 transform styles . This could be translation on the X , Y , or Z axis , scaling , or rotation . However , be careful not to overuse css translate3d for the reasons mentioned above . <p> Use CSS animations &amp; transitions- - CSS animations and transitions can definitely make things faster . However this is not a " catch-all " that covers every case . If you use CSS transitions with properties that introduce reflow operations ( such as width or height changes ) , then your performance could stutter . You can achieve fluid animations by leveraging CSS transitions , in conjunction with css transforms ( as mentioned above ) b/c they alter the visual appearance , but do n't  invoke reflow operations . <p> Use fixed widths and heights for DOM elements -- If you do n't  change the @ @ @ @ @ @ @ @ @ @ . This could be for &lt;div&gt; ( or other ) elements , or it could apply to the loading of images . If you do n't  predefine an image size , and wait until the image is loaded , then there is an expensive reflow operation when the image loads . If you have multiple images , this adds up . <p> Preload images or assets used in CSS styles This is a benefit for two reasons . First , your images will already be ready when you need them . This prevents a delay or flicker when waiting for content . Second , if you already have images or other assets preloaded into memory before they are needed , then you do n't  have a secondary reflow operation that could occur after the image/assets have been loaded ( 1st reflow is when DOM is initially calculated , 2nd when its recalculated on load ) . <p> Be smart with your HTML DOM elements- Let 's say you want to create and populate a &lt;table&gt; element based on data you have in a JavaScript array . It is very expensive to @ @ @ @ @ @ @ @ @ @ array and individually append rows to the existing DOM on every loop iteration . Rather , you could loop over the array , and create the HTML DOM element for the table only . Then , after the loop has been completed , append the &lt;table&gt; to the existing HTML DOM . Every time that you append DOM elements , you invoke a reflow operation . You want to minimize these operations . <p> In general , you want to minimize the amount of work required by the browser to calculate layout and positioning of DOM elements . <h> Keep Graphics Simple <p> Designers love to make great looking mockups , but sometimes the implementation of such designs do n't  always yield great performance . Overuse of CSS shadows and CSS gradients can impair runtime performance across different platforms &amp; browser implementations . I 'm not saying do n't  use them at all , just use them wisely . For example , CSS gradients and shadows induce slower performance on Android devices than a compariable iOS device , based on the OS/system web view implementation . <h> Touch Interactivity <p> Youve @ @ @ @ @ @ @ @ @ @ touch events are fast " . This is absolutely a true statement . Instead of using " mousedown " , " mousemove " , " mouseup " , or " click " events , you should use touch events : " touchstart " , " touchmove " and " touchend " . <p> On mobile devices , mouse events have latency that is introduced by the operating system layer . The OS tries to detect if a gesture has occurred . If no gesture has occurred , it passes the event on to the web view as a mouse event . When you use touch events instead of mouse events , you recieve the event immediately , without the operating system-introduced latency . <p> You may have noticed that there is no equivalent of " click " for touch events That 's right , there is n't one . However , you can manually manage the touch events yourself to infer a click action , or you can leverage an existing library to infer " taps " or gestures . Here are a few for consideration : Zepto , FastClick , Hammer.js @ @ @ @ @ @ @ @ @ @ for you inside the scrollable area ) <h> JavaScript Optimizations <p> In essence , write efficient code that does n't  block the UI thread . I could go on for a while on this , but instead , just follow language optimizations and best practices . Here are a few articles on the subject : <p> There are lots of facets and permutations to consider when building your apps , so just try to write decent code that you would n't be- embarrassed- to show to your coder friends . <h> Native Performance <p> So , let 's say that you want a native user experience , native navigation between containers , but you also want the ease of content creation with HTML ? Nobody said you ca n't use PhoneGap as a subview of a native application . In fact , there are some VERY widely used apps in existence that employ this technique If only I could tell you ( NDA ) . <p> This approach requires native coding expertise , but you can use CordovaView as a subview inside of a native application on both iOS and Android platforms . @ @ @ @ @ @ @ @ @ @ you want to , and leverage a PhoneGap/HTML UI when you want to . PhoneGap provides you with the native-to-JavaScript bridge for full bidirectional communication between the native and JavaScript layers . <p> When using HTML , it is really easy to create custom UIs , whether they are complex graphics , tabular data , or anything really . With the CordovaView approach , you let each layer ( native/HTML ) do what it does best . <p> You can learn more about embedding the CordovaView inside of a native application from the PhoneGap Docs . <h> UI &amp; UX Considerations <p> At risk of sounding overly-generalized : You want to pay attention to building a quality user experience . If you do n't  focus on quality , your application will suffer. - I 've already covered- user interface and user experience considerations in detail , but I 'll highlight one common question that I encounter : " How do I make an app that exactly matches native look and feel for all platforms ? " <p> My response : Do n't  . <p> Let me clarify this point : I 'm not @ @ @ @ @ @ @ @ @ @ " on a particular platform . I 'm suggesting that you do n't  try to match the operating system in every single detail . The main reasons being that 1 ) this is very difficult to do , and 2 ) if anything changes in the OS , it will be very obvious in your application . <p> You may or may not have heard of the " uncanny valley " effect . When applied to PhoneGap apps , if you get really , really close to behaving exactly like a native app , but there are minor differences , then it can immediately signal that " something is- different " or " something is wrong " with your app , and can cause an undesired- negative experience- for the user . <p> Instead , I- recommend- creating a unique identity for your app ( that still meets App Store guidelines ) . This includes look and feel , button styles , navigation , content , etc Rather than- mimicking every aspect of the native UX , build a cohesive experience around your identity or brand , and carry that identity @ @ @ @ @ @ @ @ @ @ this identity with your app , rather than comparing it to the native OS . <p> However , if you do want to mimic native look and feel , it can be done completely with CSS styles . <h> Test On Devices <p> I cant emphasize this point enough . - Always test on a device . Always . I like to target older devices , because if you can make it fast on an older device , it will be even faster on a newer one . Be sure to test on-device on all platforms that you are trying to target . For iOS , I test on an iPhone 4 ( not 4s ) and and iPad 2 . On Android , I use a Motorola Atrix , Nexus 7 tablet , Kindle Fire ( first generation ) , and a Samsung Galaxy 10.1 ( first generation ) . I test other platforms whenever I borrow a device , and pretty much use whatever I can get my hands on . - Heck , I 've even gone into retail stores just to install an app on one of @ @ @ @ @ @ @ @ @ @ Thanks to everyone who came out the the MobileDev@TU meetup last night at Towson University . - I 've posted the video online for those that want to review it , or werent able to attend . - You can check it out below . - Unfortuantely I lost the last 15 minutes of video because I was n't patient enough to let the camera finish what it was doing , and I just forced everything to power off , resulting in file corruption . - Patience would have prevailed . I 'm going to record another video discussing emulator and on-device debugging strategies for PhoneGap apps , which I 'll post here on the blog later . <p> I 'm still learning the camera , and I need to tweak a few audio setting for next time ( sorry about the clipping ) ! <p> Its always exciting to see what people in the PhoneGap community area up to . - Seeing featured apps in both iTunes and Google Play that are built with PhoneGap is fantastic . - Keep up the great work everyone ! <p> I am asked all the time " @ @ @ @ @ @ @ @ @ @ . My normal answer is to advise people to check out the PhoneGap Getting Started Guides , which provide a great starting point for every platform . However after further thought , I 'm not sure this is always what people are asking . Rather than " how do I get started ? " , I think people are often looking for insight into the workflow for developing PhoneGap applications . Everything from tools to developer flow , to getting the app on devices . The Getting Started Guides are essential for setting up the initial project structure , but once you get that setup , you might be wondering " what do I do next ? " . In this post , I 'll try to she 'd some light on the workflow and tools that I use when developing PhoneGap applications . <h> Know What You 're Going To Build Before You Build It <p> First and foremost it is essential to have at least some kind of idea what you are going to build before you build it . If you just start hacking things together without a plan , the @ @ @ @ @ @ @ @ @ @ ) UI/UX mockups are fantastic , but you do n't  have to have a fully polished design and screen flow . Just having wireframes/sketches are a great start . Heck , even a sketch on a napkin is better than starting with nothing . <p> The UX design/wireframes help you understand what you application should be doing from the users perspective , which in turn helps you make decisions on how you tackle a project . This can be purely from a HTML level , helping you figure out how you should position DOM elements and/or content . Or , it can help you gauge your projects technical complexity How many " moving parts " do you have , how much of the app is dynamic or asynchronus , or how do different visual elements need to work together ? You can leverage this design/mockup to analyze the needs of your application and determine if a particular framework/development methodology is a best fit ( Bootstrap , Backbone.js , Knockout.js , Sencha , jQuery Mobile , Angular.js , etc ) . <p> When working with a designer , I use Adobes @ @ @ @ @ @ @ @ @ @ designs , chopping up assets , etc I 'm currently working on a project that was designed by the talented- Joni from Adobe XD . Joni designed everything in Creative Suite , and I 'm using Photoshop to view screen flows and extract UI assets for the actual implementation . <p> UI Mockups in PhotoshopScreen Flow in Photoshop <p> Note : This app will also be free and open source as a sample/learning resource for PhoneGap , including all of the design assets I 'll follow up with another post on this later , once the app is- available- in the app stores . <p> If you are n't  a " graphics person " , or do n't  have creative suite , there are a bunch of other tools that you can use for wireframing and/or sketching ( but cmon , Creative Cloud is only $50 a month ) . Here are several Ive used with great success , but this is not a comprehensive list at all : <p> OmniGraffle- A drag &amp; drop wireframing tool for OS X. This is fantastic for wireframing or documenting screen flows . In fact , @ @ @ @ @ @ @ @ @ @ composed in Omnigraffle , using the mockups created in Photoshop . <p> Visio- A powerful drag &amp; drop wireframing/design tool for Windows much like OmniGraffle , but for windows . <p> PowerPoint- or Keynote- - These are n't  just for presentations . They can be really useful for putting together screen flow diagrams , or annotating images/content . <p> Often people like to sketch out ideas &amp; wireframes on their tablets , here are a few tools that I use for that : <p> iBrainstorm A great app for collaboratively taking notes and sketching . I 'm partial to this one b/c used to be on the dev team , and I wrote a good chunk of the graphics sketching logic . <p> There are a bunch of other tablet sketching apps out there , but I have n't used most of them . <h> Coding Environment <p> Coding environments are a tricky subject . There is no single solution that meets the exact wants and needs for everyone . Some people chose lightweight text editors , some people chose large-scale IDEs , some people use designer-centric tools , and many of @ @ @ @ @ @ @ @ @ @ or your background as a designer or developer . Since PhoneGap applications are really just editing HTML , CSS &amp; JavaScript , you can use whatever editor you want . In fact , I know a several people that use vim as their primary editor . <h> Large-Scale IDEs <p> I 'm a bigger fan of of using a complete IDE ( integrated development environment ) than I am of a lightweight editor , simply b/c IDEs tend to have hooks into more features/languages , etc I know people complain about startup time , but there is no startup time if you leave it open all the time . <p> There are a few catches when talking about IDEs with PhoneGap . The first is that if you want to deploy anything locally to devices ( without using PhoneGap Build ) , you have to delpoy using the IDE for the particular platform that you are- targeting . That means Xcode for iOS , Eclipse for Android , or Visual Studio for Windows Phone , etc However if you wish , you can use your editor of choice , and just @ @ @ @ @ @ @ @ @ @ can even share source code across several IDE installations using symlinks ( which I describe here ) . I very often use this type of a configuration to share code between Xcode , Eclipse , and WebStorm . <p> My preference for coding PhoneGap applications is to use- WebStorm by JetBrains . WebStorm has great code-hinting ( even for your own custom JS and 3rd party libraries ) , great refactoring , hooks into Git , CVS , or SVN repositories , and is a very mature IDE . <p> WebStorm IDE <p> I tend to use this as my primary coding tool , then switch to Eclipse or Xcode when I want to locally deploy to a device for testing . When using PhoneGap Build to simplify cross-platform compilation , I just push the code to git , then recompile via PhoneGap Build . <p> I 'm not a fan of Xcodes HTML/JS editing , and havent found an HTML/JS plugin for Eclipse that I really like . To target Windows devices , I use Visual Studio . <h> Lightweight Editors <p> I 'm a bigger fan of larger IDEs than @ @ @ @ @ @ @ @ @ @ as Brackets ) is a great lightweight editor for quick edits. - Edge Code/Brackets is an open source HTML/JS editor that supports live editing in the browser and inline editors for CSS styles , without leaving your HTML files . If you tried Edge Code Preview 1 , but werent sold on it , you should try Edge Code Preview 2 . The team has come a long way very quickly . Its fast , easy to use , and there is a- plugin to tie it into PhoneGap Build . I sometimes use this for quick edits . <p> There are tons of other lightweight editors out there , and everyone has their favorite . As long as you 're happy with the tool , and it can edit text ( HTML , CSS , JS ) files , you can use it to build PhoneGap applications . <h> Designer-Friendly Editors <p> I 'm not necessarily the primary target for Dreamweaver , but it has some nice features . Dreamweaver gives you a great programming environment plus a WYSIWYG editor for HTML experiences . It also features PhoneGap Build integration directly @ @ @ @ @ @ @ @ @ @ for creating web experiences , you can continue to use it and target mobile apps as well . <p> Adobe Dreamweaver <h> Debugging Environments <p> Yes , that is plural Debugging Environments . Due to the cross-platform nature and PhoneGaps leveraging of native web views for each platform , debugging PhoneGap applications can sometimes be tricky . Here are some tips that will make this significantly easier . <h> The PhoneGap Emulator <p> The PhoneGap Emulator is my primary development/debugging tool for all PhoneGap apps . It is a browser-based emulator leveraging the Google Chrome browser and the Ripple Emulation Environment . The PhoneGap Emulator runs inside of Google Chrome , and provides emulation of PhoneGaps core APIs . Since it is built on top of Chrome , it enables you to leverage Chromes Developer Tools , which in my opinion are second to none for web/application development . This is a highly-productive developer environment . <p> PhoneGap Emulator in Google Chrome <p> here 's why I like the PhoneGap/Ripple/Google Chrome development environment : <p> First , - this combination enables you to emulate most core PhoneGap APIs without leaving the @ @ @ @ @ @ @ @ @ @ including geolocation ( with simulated locations ) , device events ( deviceready , back , etc ) , sensor events ( accelerometer , compass ) , and even let 's you test with different device aspect ratios all without having to push anything to an actual device . This saves a lot of time in development iterations . You can read about the supported Ripple emulator features here . <p> Second , Chromes Developer Tools are awesome . Here are just a few things that you can do while developing/debugging your app , live within the emulation environment : <p> Analyze all resources consumed by your app , via the resources panel . This includes all scripts , images , html files , cookies , etc it even includes insight into any local data stored via PhoneGaps local storage database ( WebSQL implementation ) . <p> View/query all local databases within your app . You can write your own queries to view/alter data in the WebSQL database . Thanks to Ray for sharing this , its not immediately intuitive . <p> Debug JavaScript with the Scripts/Sources Panel . You can set @ @ @ @ @ @ @ @ @ @ JS objects in-memory , and view details and line numbers for any exceptions that occur . <p> Use the console to monitor console.log() statements , inspect properties of objects in memory , or execute arbitrary JavaScript whenever you want . <p> The PhoneGap Emulator enables developers to be extremely productive with development , however I can not emphasize enough that on-device testing is critical for having a successful app . On-device testing can expose performance problems or browser rendering variances that you may not notice in the emulator environment . <h> On-Device Remote Debugging <p> As I mentioned above , on-device testing is critical for successful applications . iOS and BlackBerry have an advantage over other platforms b/c the latest developer tools allow you to remotely debug content live on a device . <p> Since the release of iOS 6 , you can debug content in the iOS simulator using Safaris Developer Tools . Safaris developer tools give you many of the same debugging capabilities that I mentioned above for Chrome . <h> Remote Debugging with Weinre <p> Not every platform supports live remote debugging , especially older versions . @ @ @ @ @ @ @ @ @ @ that allows you to inspect/edit DOM and CSS elements on remote devices . Basically , you include some JavaScript in your app , and it communicates back to a server that will tell you what 's happening inside of the app running on the mobile device . It wo n't give you full debugging capabilities like JS breakpoints and memory inspection , but its better than nothing . You can use Weinre by setting up your own instance , or by leveraging debug.phonegap.com . <p> Weinre for On-Device Debugging <h> When All Else Fails <p> If you 're still debugging your apps , and the solutions mentioned above do n't  work , you can always resort to plain-old " alert() " statements to pop up debug messages , or use " console.log() " statements to write to system logs . <p> On Android , all- console.log ( ' ... ' ) ; - messages will appear as printouts in the command-line tool- logcat , which is bundled with the Android SDK and integrated into the Android Eclipse plugin . <p> On BlackBerry , all- console.log ( ' ... ' ) ; - are @ @ @ @ @ @ @ @ @ @ can be accessed by pressing- ALT + LGLG . <p> On iOS , all- console.log ( ' ... ' ) ; - are output to the Xcode Debug Area console . <h> Building PhoneGap Apps <p> The PhoneGap getting started guides will point you to the right direction for getting started with a particular platform . If you are just targeting iOS , you can use Xcode for building . If you are just targeting Android , you can use Eclipse , etc It is all very easy to get up and running . <p> However , this process gets much more complicated when targeting multiple platforms at once . When I have to do this , PhoneGap Build becomes really , really handy . <p> PhoneGap Build allows you to either upload your code , or point to a Git repository . PhoneGap Build will then pull your code and build for 7 different platforms , without you having to do anything special or setup multiple environments . All that you 'll have to do is install the cloud-compiled binaries on your device . You can do this by copying/pasting @ @ @ @ @ @ @ @ @ @ QR code that will directly link to the compiled- application- binary . <p> One other advantage of PhoneGap build is that it let 's designers/developers build mobile applications without having to install any developer tools . If you want to compile a PhoneGap app for iOS , but are on Windows just use PhoneGap build and you wo n't need Xcode or a Mac . <h> PhoneGap UI/Development Frameworks <p> Probably the most common PhoneGap question that I get asked is " what MVC/development framework should I use ? " . If you 've been waiting for me to answer this , do n't  hold your breath . It is impossible to be prescriptive and say that one solution fits all use cases for every- developer . <p> When people ask me this , I like to paraphrase- Brian Leroux from the PhoneGap team : " Use HTML , it works really well . " <p> I think people often overlook the fact that PhoneGaps UI renderer is a system web view . Anything that is valid HTML/CSS content can be rendered as your applications user interface . This could be something incredibly @ @ @ @ @ @ @ @ @ @ could be incredibly creative or complex . The important factor is that you need to focus on a quality user experience . If you 're worried about your UX , and are worried that Apple may reject your app , then read this article where I explain Apple rejections in detail . <p> HTML/JS developers come from many different backgrounds , with varying degrees of programming expertise . Some frameworks appeal to some people , other frameworks appeal to other people . There also seem to be new UI &amp; architectural frameworks popping up every week . It would be a disservice to all people who use PhoneGap for us to proclaim that we should only use one singe framework . <p> There are lots , and lots , and lots more options out in the HTML/JS development world I 'm not even taking into account JavaScript generating tools and languages like CoffeeScript , TypeScript , or others 
@@106848768 @2248768/ <h> Using bootstrap-modal <h> Options <p> Includes a modal-backdrop element . Alternatively , specify static for a backdrop which does n't close the modal on click . <p> keyboard <p> boolean <p> true <p> Closes the modal when escape key is pressed <p> show <p> boolean <p> true <p> Shows the modal when initialized . <h> Markup <p> You can activate modals on your page easily without having to write a single line of javascript . Just set data-toggle= " modal " on a controller element with a data-target= " #foo " or href= " #foo " which corresponds to a modal element i 'd , and when clicked , it will launch your modal . <p> Also , to add options to your modal instance , just include them as additional data attributes on either the control element or the modal markup itself . <h> Markup <p> You can activate a tab or pill navigation without writing any javascript by simply specifying data-toggle= " tab " or data-toggle= " pill " on an element . Adding the nav and nav-tabs classes to the tab ul will apply the bootstrap @ @ @ @ @ @ @ @ @ @ selector then all collapsible elements under the specified parent will be closed when this collapsible item is shown . ( similar to traditional accordion behavior ) <p> toggle <p> boolean <p> true <p> Toggles the collapsible element on invocation <h> Markup <p> Just add data-toggle= " collapse " and a data-target to element to automatically assign control of a collapsible element . The data-target attribute accepts a css selector to apply the collapse to . Be sure to add the class collapse to the collapsible element . If you 'd like it to default open , add the additional class in . <h> Example <h> Using bootstrap-typeahead.js <h> Options <p> Name <p> type <p> default <p> description <p> source <p> array <p> <p> The data source to query against . <p> items <p> number <p> 8 <p> The max number of items to display in the dropdown . <p> matcher <p> function <p> case insensitive <p> The method used to determine if a query matches an item . Accepts a single argument , the item against which to test the query . Access the current query with this.query . Return @ @ @ @ @ @ @ @ @ @ sorter <p> function <p> exact match , case sensitive , case insensitive <p> Method used to sort autocomplete results . Accepts a single argument items and has the scope of the typeahead instance . Reference the current query with this.query . <p> highlighter <p> function <p> highlights all default matches <p> Method used to highlight autocomplete results . Accepts a single argument item and has the scope of the typeahead instance . Should return html . <h> Markup <p> Add data attributes to register an element with typeahead functionality . <p> &lt;input type= " text " data-provide= " typeahead " &gt; <h> Methods <h> . typeahead(options) <p> Initializes an input with a typeahead . <h> Lists bootstrap-list.js <h> About <h> Example <p> Scroll through the list using your scroll wheel , a touch gesture ( works with mouse input too ) , or using the scrollbar ( desktop only ) . Tap an item to receive a " change " event . Gesture-based scrolling animates with inertia for both desktop and mobile browsers . <p> For performance optimizations , the list component uses data virtualization techniques , so there are @ @ @ @ @ @ @ @ @ @ is currently visible on the screen . As the user scrolls through content , the list updates the DOM elements accordingly . This makes scrolling lists of thousands of items extremely fluid . <h> Using bootstrap-list.js <p> There are two methods that the list component can be used . One option is to declare the list structure in HTML markup , another option is to specify a dataProvider array , from which the list will create DOM elements . <h> Declarative Markup <p> This list has 50 items as &lt;LI&gt; elements : <p> item 0 <p> item 1 <p> item 2 <p> item 3 <p> item 4 <p> item 5 <p> item 6 <p> item 7 <p> item 8 <p> item 9 <p> item 10 <p> item 11 <p> item 12 <p> item 13 <p> item 14 <p> item 15 <p> item 16 <p> item 17 <p> item 18 <p> item 19 <p> item 20 <p> item 21 <p> item 22 <p> item 23 <p> item 24 <p> item 25 <p> item 26 <p> item 27 <p> item 28 <p> item 29 <p> item 30 <p> item 31 <p> item @ @ @ @ @ @ @ @ @ @ <p> item 36 <p> item 37 <p> item 38 <p> item 39 <p> item 40 <p> item 41 <p> item 42 <p> item 43 <p> item 44 <p> item 45 <p> item 46 <p> item 47 <p> item 48 <p> item 49 <p> First create the HTML Structure , then instantiate the list via javascript , and add an event handler : <h> Methods <h> . list() <h> . list ( 'setDataProvider ' , dataProviderArray ) <p> Sets the data provider array for the list instance . There is no concrete limit on the length of the data provider array . Elements within the array can be of primitive or complex types . <h> . list ( 'setLabelFunction ' , labelFunction ) <p> Sets label function that can be used to format items in the data provider array into string values that can be displayed within the list . <h> . list ( 'setSelectedIndex ' , index ) <p> Set the item at selected index as selected . <h> . list ( 'getSelectedIndex ' ) <p> Returns the currently selected index . If no list item is selected , returns @ @ @ @ @ @ @ @ @ @ Clears the currently selected item in the list . <h> Events <p> Bootstrap 's list class exposes a change event for handling when the selected item in the list changes . <p> Event <p> Description <p> change <p> This event is fired when the selected item in the list is changed . You can access details of the selected item in the list by accessing attributes of the event . <h> event.index <p> The numeric index for the item in the list that was clicked/touched. <h> event.srcElement <p> A jQuery reference to the list item that was clicked/touched. <h> event.item <p> A reference to the data item for the list item . If using inline &lt;li&gt; in markup , this will be the same DOM element as event.srcElement . If using a dataProvider , it will be the object in the dataProvider array corresponding to the selected list item . 
@@106848769 @2248769/ <p> Photosynth is an impressive service from Microsoft . - It enables you to upload photos and turn them into interactive 360 panoramas , photo walls , spins , or photo walks . The Photosynth team recently announced a new version of Photosynth , and its a really cool web experience . It leverages- WebGL- to visualize the content , and runs great on both desktop and mobile devices ( as long as the devices support WebGL ) . <p> Those who know me well or regularly read the blog probably already know I have an obsession with aerial photography using remote controlled multirotor helicopters . Once I discovered Photosynth , my first thought was " Wow , these Photo Walks will be incredible to visualize flights " . - I capture most of my flights in time-lapse photography mode with a GoPro camera attached to a DJI Phantom copter . The time-lapse images are perfect for Photosynth I normally capture on a two second interval , though the Photosynth team suggested trying an even shorter interval for better results . <p> To generate the best Photosynths , you @ @ @ @ @ @ @ @ @ @ where Lightroom comes into the picture . Lightroom is an incredible tool for editing photos and bringing out their details . You can enhance exposure , colors , clarity , saturation , reduce noise , and more . Even better , it excels at bulk image editing . Thus its perfect for processing your photos for preparation to create a Photosynth . <p> Check out the video below to get an overview of Photosynth , and preparing your photos with Lightroom . <p> Now , you 're ready to learn more about both Lightroom and Photosynth , right ? <p> Below are Photosynths from a few of my flights . - If your browser supports WebGL , you 'll be able to see the fully interactive experience you 'll be able to scrub through the photos , zoom in , and pan the images at full resolution . Its best viewed in full-screen mode . 
@@106848771 @2248771/ <h> Linked Source Files Across PhoneGap Projects on OSX <p> If you are manually building PhoneGap projects across multiple platforms , managing source files can sometimes become a little bit tricky . If you are building for Android , you need a project within Eclipse . If you are building for iOS , you need a project within Xcode . If you are building for both , you need to make sure that the code in the Eclipse project is in synch with the code in the Xcode project so that the platform-specific apps are in parity with each other . <p> Keeping the project source code in synch can be achieved using manual copy/paste between projects , but this is messy and error prone . The synchronization can also be scripted using ANT or other scripting language , but this requires an additional script and/or step in your build process . Although scripting is a reliable process , sometimes you just do n't  need the script . <p> If you do n't  want to manually synch things , and you do n't  want a script , you can @ @ @ @ @ @ @ @ @ @ create a " src " folder outside of each project , and create a symlink reference to the src folder inside of the " www " folder for each project . - Symlinks allow a logical directory mapping , which actually points to another location on the hard disk . <p> From the command line , you just use the following command : <p> ln -s source target <p> To setup your project , first create your directory structure . - I created a parent folder for the project . Inside of that folder , I created a " project-ios " folder , " project-android " folder , and " src " folder . - The " src " folder will contain the shared HTML/JavaScript for the application . - The " project-ios " folder will contain the Xcode project , and the " project-android " folder will contain the Eclipse project . <p> Project Structure <p> Next , create the actual iOS and Android projects inside of these folders , following the normal setup instructions : <p> Once you have set up both projects , you 'll need to configure @ @ @ @ @ @ @ @ @ @ " file into your " src " directory . - Next , go to the " www " directory for each project and delete the " index.html " file to remove any ambiguity or chance for error . <p> However , DO NOT DELETE THE PHONEGAP.js files ! <p> The phonegap.1.4.1.js files are platform specific . - The Android version will not work with iOS , and the iOS version will not work with Android . <p> Next , navigate to your root folder that contains the " src " , " project-iOS " , and " project-Android " folders . Here you will- create the actual symlink references . - When doing so , be sure to use the full path to the source and target destinations . - You will need to create a symlink reference from the " src " directory to " project-ios/www/src " , and a- symlink reference from the " src " directory to " **30;2129;TOOLONG " . <p> If you try to use a relative path from your current location , it will give you errors and a massive headache . - - @ @ @ @ @ @ @ @ @ @ to the full path of your current directory . <p> Here are the commands that I used on my system , where the root directory is LONG ... <p> In the iOS project , open AppDelegate.m . You 'll also need to update it to reference the index.html file inside of " src " . You 'll just need to edit the start page to " src/index.html " inside of the function : ( BOOL ) application : ( UIApplication* ) application LONG ... <p> Also , be sure to update the link inside of " src/index.html " to point to the project-specific PhoneGap JavaScript files in the parent directory " .. /phonegap-1.4.1.js " ( they are not inside of the linked folder ) : 123433 @qwx983433 <p> XCode does not appear to follow the sym links . If I put a real index.html in the folder it works . If I setup the symlinks it fails to resolve the symlinks when bundling the project . 123436 @qwx983436 <p> How do you have the links setup ? It works for me . In mine , I have the simlinks inside of the @ @ @ @ @ @ @ @ @ @ party , but <p> I get a The argument is invalid. alert when I try to run to a device in xcode 4.4.1 . <p> Anyone else running into this problem , and/or have a solution ? <p> Rachel <p> I thought symlinks werent working for me , because Xcode would follow them , but after build , I would get the " ERROR : Start Page at www/index.html was not found . " message . <p> Only now I realized that the link target must be absolute . I 've always used relative targets ( ln -s .. / .. /www/index.html index.html ) and it was working fine on my Ubuntu box , compiling this phonegap project to Android . <p> Apparently the iOS compiler is n't compatible with relative symlinks , so I had to use absolute ones , which is actually the example in this post . <p> This is a shame because absolute paths will usually only work for one dev on one machine . My current symlink is ln -s LONG ... index.html . <p> I thought I 'd post this here seeing I spent a lot of @ @ @ @ @ @ @ @ @ @ Thanks ! Yeah , they have to be absolute , or else you run into a bunch of problems . <p> http : //skeater.co.uk Pete <p> @jayseeg I 'm getting that same error " The argument is invalid . " . <p> Ive used absolute paths in my symlinks , and I 've tried using a bash run script to copy the files but its not working . <p> Did anyone else get or solve this problem ? 123436 @qwx983436 <p> I 'm using Xcode 4.5.1 , and not seeing this issue the SymLinks still work for me . Sorry , I 'm no help . <p> Pete <p> @Andrew " the argument is invalid " only gets thrown when I try to run the app on a device . <p> It works fine on the simulator <p> http : //www.ten24web.com Sumit Verma <p> Adding a run script for build phase fixes the error . Here is the script : <p> One thing though : in more recent versions of Phonegap ( e.g. 2.5.0 ) there 's no longer the need to modify the AppDelegate.m and the Android Main activity to change the " index.html @ @ @ @ @ @ @ @ @ @ config.xml " ( both iOS &amp; Android ) : <p> Andrew , your blog posts are really helpful . I consider them to be the missing manual for phonegap ! <p> Regarding linking of source files , can this process tie into phonegap build and how ? Can you make your src directory into a git repo and push it to phonegap build , or are there other considerations ? <p> Thank you ! 123436 @qwx983436 <p> Hi Christopher , I do n't  think this exact process can tie into a PhoneGap build compilation . There are ways to create shared Git modules/submodules , but I honestly do not know the details about them , and have not personally tested them with PhoneGap Build . <p> Adrian <p> Great post been looking for a way to share www folder for a while ! ( worked fine for me in Xcode 4.6(iOS) , Eclipse ( Android ) with Cordova 2.7.0 , JQuery Mobile 1.3.1 both in sims and on live devices ) <p> way2tour <p> Great post If we have to get the content from web service to mange the html @ @ @ @ @ @ @ @ @ @ I followed these directions and the build portion ( when I tried to install it to a device ) did not work got two " The argument is invalid " errors . I am using XCode 4.5.1 . Here is the file in question . Note the full absolute path . 
@@106848772 @2248772/ <h> Category Archives : Mobile <p> The keynotes and many of the session videos form Adobe MAX are now available online at **27;2161;TOOLONG , and are free for everyone to view . Keep in mind , not all videos are up yet , but will be added soon . <p> The first-day keynote focuses on Adobes evolution of Creative Cloud and various product updates . The second day keynote focuses upon inspiration do not miss this one . - I especially liked Erik Johansson and Rob Legattos segments . <h> Day 1 Keynote : A Creative Evolution <p> About This Episode:The process of where and how we create is dramatically changing thanks to major advancements in technology , and there has never been a more exciting time . Join Adobe CEO Shantanu Narayen , Adobes SVP and GM of Digital Media David Wadhwani , and a collection of Adobe visionaries across digital photography , web design , illustration , video and more as we unveil brand new creative workflows and capabilities . Well take a look at the present and set our sights on the endless possibilities in our @ @ @ @ @ @ @ @ @ @ Creativity <p> About This Episode : - Join David Wadhwani , Adobe 's SVP and GM of Digital Media , as he welcomes four incredibly creative minds to explore how they foster creativity and approach their work . You will hear from Rob Legato , an Oscar winning visual effects supervisor ; Paula Scher , an iconic graphic designer and illustrator ; Erik Johansson , an up and coming photographer and retouch artist ; and Phil Hansen , a constraint-based artist that believes limitations drive creativity . We think you 'll leave with more than a few new ideas to incorporate in your next creative project . <p> Another request that I have gotten from some of our DPS customers is that theyd like to be able to implement gestures inside of the Edge Animate compositions that they are building for DPS publications . This includes double-tap gestures , swipe gestures , etc Out of the box , these gestures are n't  supported , but you can add them to any Edge Animate composition without too great of an effort . <p> Below is a quick video showing Edge Animate @ @ @ @ @ @ @ @ @ @ gestures . - Note : I intended these to be used inside of DPS , but I show them in Safari on iOS. - These gestures override the default mobile browser behaviors . <p> As I mentioned above , this is n't something that is supported out of the box , but it is possible to add gesture features manually . <p> The links below are for the basic examples that I put in the video . - Both should work in desktop and mobile browsers : <p> For the double tap example , just perform a double tap/click gesture anywhere on the stage ( the image area ) , and the animation will start again from the beginning . - For the swipe gesture , just perform a horizontal swipe in either direction with either your finger , or the mouse . <h> Gestures With Hammer.js <p> I leveraged the hammer.js JavaScript library to handle gesture detection since these gestures are n't  supported by default . - Hammer.js also enables other gestures , like long taps , pinch , rotate , etc - However , I 'm only showing double tap @ @ @ @ @ @ @ @ @ @ using the following links : <p> I used this exact setup procedure in both the double-tap and swipe examples . <p> To include this library , I first downloaded the hammer.js file , and saved it inside of the " edgeincludes " folder . <p> Next , you have to disable the web view/browser default double tap behavior , which is to zoom in when double tapped . - You can disable the zoom on double tap by adding a viewport metadata tag inside of the Edge Animate projects html file . - Open your projects . html file ( in this case " DoubleTap.html " , and add the following line to the &lt;head&gt; <p> Next , we have to add the code inside of the Edge Animate composition to enable the gesture behavior. - The first thing you have to do is include the hammer.js library . - In this case , I wanted to add the gestures to the compositions stage , instead of a particular element . - So , right-click on the stage in the Edge Animate Editor , then select - the " Open @ @ @ @ @ @ @ @ @ @ open the actions for the Stage instance . - Next , click on the " + " icon and select " creationcomplete " . - This will create a function that gets invoked once the Stage instance has been created at runtime . <p> In that function , first we need to import the hammer.js library . - Edge Animate compositions include the- yepnope.js library , which is originally intended to detect if a browser includes a specific piece of functionality . - If not , then include a JS library so substitute that missing feature . - In this case , I am passing it a blank test to force it to include the hammer.js library . - The following function forces loading of the hammer.js library . - Once the library has been loaded into memory , it triggers the " init " function : <p> In the init function , we grab a reference to the stages element ( div ) , then use hammer.js to add our gesture event handlers : <p> Now , we need to start looking at the individual examples <h> Double Tap @ @ @ @ @ @ @ @ @ @ a simple timeline animation that plays sequentially . - At the end of the sequence the animation is stopped by a simple sym.stop() function call . - Heres a quick preview of the setup in Edge Animate : <p> To add the double tap gesture , all you have to do is add a hammer.js event for " doubletap " . - In that event handler , were just calling sym.play(0) , which restarts playback from the beginning of the composition . The full code for the creationcomplete event is shown below . - This is all that is needed to add the double-tap gesture to the composition stage instance : <h> Swipe Gestures <p> In the swipe gestures example , we have a simple timeline animation that plays sequentially . - However , at the end of each slide transition , playback is stopped by a simple sym.stop() function call . - Whenever we perform a swipe action , were either just playing forward , or playing in reverse until the next slide animation stops . - Heres a quick preview of the setup in Edge Animate , note @ @ @ @ @ @ @ @ @ @ add the swipe gestures , all you have to do is add a hammer.js event for " swipeleft " or " swiperight " . - In those event handlers , were just calling sym.play() or sym.playReverse() , depending whether it was a left or right swipe . - These play actions progress to the next animation sequence . The full code for the creationcomplete event is shown below . - This is all that is needed to add the swipe gesture to the composition stage instance : <p> With the swipe gestures , you can get some drag event conflicts on mobile devices . - If you run into this and you do not want the page to scroll , the scroll action can be prevented by capturing the touchstart event , and canceling the default behavior. - I did n't  add this , just because I wanted to keep this example very simple . <p> Next week I 'll be representing Adobe at GDC 2013 , and demonstrating how Adobe Creative Cloud , - PhoneGap , and PhoneGap Build- can be great tools for building casual gaming experiences. - In @ @ @ @ @ @ @ @ @ @ that shows off the potential of HTML games packaged with PhoneGap . <p> and now I 'd like to introduce you to PhoneGap Legends . PhoneGap Legends is a fantasy/RPG themed demo that leverages HTML DOM animation techniques and targets webkit browsers . I was able to get some really outstanding performance out of this example , so be sure to check out the video and read the details below . The name " PhoneGap Legends " does n't  mean anything ; I just thought it sounded videogame-ish and appropriately fitting . <p> PhoneGap Legends <p> This game demo is an infinitely-scrolling top-view RPG themed game that is implemented entirely in HTML , CSS , and JavaScript . There is a scrolling background , enemy characters , HUD overlays , and of course , our " protagonist " hero all that 's missing is a story , and general game mechanics like interacting with sprites . <p> Again , I was able to get some *really outstanding performance* out of this sample , so I wanted to share , complete with source code , which you 'll find further in this post ( @ @ @ @ @ @ @ @ @ @ Take a look at the video below to see the game in action on a variety of devices . Every single bit of this is rendered completely with HTML , CSS , and JavaScript there are no native portions of the application . <p> Update 3/24 : - If you 'd like to test this out on your own devices , you can now access it online at- LONG ... - However , it will still only work on webkit browsers ( Chrome , Safari , Android , iOS , etc ) , and is optimized for small-device screens . - If you attempt to use this on a very large screen , you 'll probably see some sprite clipping . <p> Disclaimer : This sample app is by no means a complete game or complete game engine . Ive implemented some techniques for achieving great performance within a PhoneGap application with game-themed content , but it still needs additional game mechanics . I also wrote this code in about 2 days it needs some additional cleanup/optimization before use in a real-world game . <h> Source <p> Full source code for this @ @ @ @ @ @ @ @ @ @ provided as-is : - https : **37;2190;TOOLONG . I will be making a few updates over the next few days in preparation for GDC next week , but for the most part , it is stable . <p> In general , the DOM is as shallow as possible for achieving the desired experience . All " sprites " , or UI elements are basic DOM nodes with a fixed position and size . - All DOM elements have an absolute position at 0,0 and leverage translate3d for their x/y placement . - This is beneficial for 2 reasons : 1 ) It is hardware accelerated , and 2 ) there are very , very few reflow operations . - Since all the elements are statically positioned and of a fixed size , browser reflow operations are at an extreme minimum . <p> The background is made up a series of tiles that are repeated during the walk/movement sequence : <p> Sprite Sheet for Background Tiles <p> In the CSS styles , each tile is 256+256 square , with a background style that is defined for each " type " of @ @ @ @ @ @ @ @ @ @ " sprite " DOM elements is applied using sprite sheets and regular CSS background styles . Each sprite sheet contains multiple images , the background for a node is set in CSS , and the position for each image is set using the " background-position " css style . - For example , the walking animation for the hero character is applied just by changing the CSS style that is applied to the " hero " &lt;div&gt; element . <p> Sprite Sheet for Hero <p> There is a sequence of CSS styles that are used to define each state within the walking sequence : <p> This game demo extensively uses translate3d for hardware accelerated composition . - However , note that the 3d transforms are all applied to relatively small elements , and are not nested . All of the " textures " are well below the max texture size across all platforms ( 1024+1024 ) , and since it uses sprite sheets and reusable CSS styles , there are relatively few images to load into memory or upload to the GPU . <h> Attribution <p> The following Creative Commons @ @ @ @ @ @ @ @ @ @ and the accompanying video : <p> Want to debug your PhoneGap apps , complete with breakpoints , DOM &amp; CSS inspection , profiling , and more ? - This is all possible with the- PhoneGap Emulator , - which allows you to leverage Chromes Developer Tools inside of the desktop Chrome browser ( covered in detail here ) . - However , did you also know that you can have a rich debugging/development experience in an app that is actually running on a device ? <p> Since the release of iOS 6 last Summer , weve all had the ability to debug PhoneGap apps while they are running on external iOS devices , or inside of the iOS simulator . Im surprised how often I hear that people are not aware of this feature . With iOS 6 you can use Safaris developer Tools to connect to any HTML content on the device , either in the mobile Safari browser , or inside of a web view . PhoneGap apps fall into that second category , they are based upon iOS system web view . <p> You can check out @ @ @ @ @ @ @ @ @ @ below : <p> In order to take advantage of this , you 'll first have to enable the remote web inspector for Safari on iOS . Just follow the instructions for " Debugging Web Content on iOS " from Apple be sure not to skip the " Enable Web Inspector on iOS " - section , which is hidden by default . - You have to enable this in iOS Settings in order for the desktop Safari Browser to be able to connect to any web content on the mobile device . <p> Unfortunately , this is only available for PhoneGap on iOS devices at this time . Android enables remote debugging inside of the Chrome browser , however that is n't enabled for PhoneGap apps *yet*. - Whenever Google enables Chrome for web views inside of apps , its on ! <p> I put this post together to follow up a few points from my last post about Adobe Ideas &amp; Adobe Creative Cloud . I promise , I 'll get back to some technical &amp; development topics in my next post . I 'm just having too much fun with this right @ @ @ @ @ @ @ @ @ @ Adobe Ideas , following the same workflow in my previous post : I sketched this composition in Adobe Ideas , then leveraged Adobe Creative Cloud to continue the editing process on the desktop . <h> Video : Adobe Ideas Composition <p> Disclaimer : I am not an artist or illustrator . - I primarily focus on writing software the art stuff is just for fun . <p> Now , I want to elaborate a bit more about the Creative Cloud workflow . Creative Cloud is not just a subscription to all of Adobes creative and Edge tools . Its also not just a " hard drive in the sky " that synchronizes your creative assets across your devices . It is both of these , and much more . <p> We 've already covered that Creative Cloud empowers the seamless creation of content across mobile and desktop devices . You can create content using the Adobe Touch apps , and pull that into the desktop tools for refinement . For example , let 's take the sketch that I created in the video above If I want to isolate and extract individual @ @ @ @ @ @ @ @ @ @ desktop tools : <p> Isolating Layers in Adobe Illustrator <p> I can also pull these layers into Photoshop for further enhancement . In this case , color correction , and rearranging some content ( oh look , more elephants ! ) . <p> Editing in Adobe Photoshop <p> Now , let 's consider workflow beyond just creation of content . What about collaboration and feedback ? Adobe Creative Cloud empowers collaborative experiences too . Everything from simple sharing , to comments/feedback , even having the ability to break down and view individual layers of PSD files . Check out the video below for some more detail : <h> Video : Adobe Creative Cloud Workflow <p> You can share any of your Creative Cloud assets by email or simple URLs , where that content and workflow can be consumed on any device , anywhere . Need feedback from your customers ? Just send them a link to the composition work in progress , and they can leave comments whenever they want . <p> Sharing and Collaboration with Adobe Creative Cloud <p> In fact , If you 're interested , you can check this @ @ @ @ @ @ @ @ @ @ I used to create this composition , which Ive shared publicly . Although , I disabled the ability to download the original files . You can toggle PSD layers , or leave comments ( just be nice ) : 
@@106848773 @2248773/ <h> Monthly Archives : January 2012 <p> After spending some time playing around sketching with the HTML5 canvas element earlier this week , I figured " why not add some enterprise concepts to this example ? " - Next thing you know we 've got a multi-device shared sketching/collaboration experience . <p> To keep things straightforward , I chose to demonstrate the near-realtime collaboration using a short-interval HTTP poll . - HTTP polling is probably the simplest form of near-realtime data in web applications , however you may experience lag when compared to a socket connection of equivalent functionality . - I 'll discuss the various realtime data options you have in Flex/Flash and HTML/JS and their pros &amp; cons further in this post . <p> What you 'll see in the video below is the sketching example- with realtime collaboration added using short-interval data polling of a ColdFusion application server . - The realtime collaboration is shown between an iPad 2 , a Kindle Fire , and a Macbook Pro . <p> Before we get into the code for this example , let 's first review some realtime data basics <p> First @ @ @ @ @ @ @ @ @ @ ? - Here are just a few : <p> Time sensitive information , where any delay could have major- repercussions <p> Realtime financial information <p> Emergency services ( medical , fire , police ) <p> Military/Intelligence scenarios <p> Business critical efficiency/performance- metrics <p> Collaboration <p> Realtime audio/video collaboration <p> Shared experience ( presentations/screen sharing ) <p> Entertainment <p> Streaming media ( audio/video ) <p> Gaming <p> Regardless of whether you are building applications for mobile , the web , or desktop , using any technology ( Flex/Flash , HTML/JS , Java , . NET , Objective C , or C/C++ ( among others ) ) , there are basically 3 methods for streaming/realtime data : <p> Socket Connection <p> HTTP Polling <p> HTTP Push <h> Socket Connections <p> Socket connectionss are basically end-to-end communications channels between two computer processes . - Your computer ( a client ) connects to a server socket and establishes a persistent connection that is used to pass data between the client and server in near-realtime. - Persistent socket connections are generally based upon TCP or UDP- and enable asynchronus bidirectional communication . - Binary or @ @ @ @ @ @ @ @ @ @ point in time , in any sequence , as data is available . - In HTML/JS applications you can use web sockets , which I recently discussed , or use a plugin that handles realtime socket communication . Did you also know that the next version of- ColdFusion- will even have web socket support built in ? - In Flash/Flex/AIR , this can be achieved using the RTMP protocol ( LCDS , Flash Media Server , etc ) or raw sockets ( TCP or UDP ) . <p> Direct Socket Communications <p> In general , direct socket based communication is the most efficient means of data transfer for realtime application scenarios . - There is less back and forth handshaking and less packet encapsulation required by various protocols ( HTTP , etc ) , and you are restricted by fewer network protocol rules . - However , socket based communications often run on non-standard or restricted ports , so they are more likely to be blocked by IT departments or stopped by network firewalls. - - If you are using socket based communication within your applications , which are running @ @ @ @ @ @ @ @ @ @ network , you may want a fallback to another realtime data implementation for failover cases . <h> HTTP Polling <p> HTTP Polling is the process of using standard HTTP requests to periodically check for data updates on the server . - The client application requests information from the server . - Generally , the client will send a timestamp indicating the last data update time . - If there is information available on the server that is newer than the timestamp , that data will be immediately sent back to the client ( and the clients timestamp will be updated ) . - After a period of time , another request will be made , and so forth until the polling is stopped within the application . - Using this approach , the application is more-or-less " phoning home " periodically to the server to see if there are any updates . - You can achieve near-realtime performance by setting a very short polling interval ( less than one second ) . <p> Basic Data Poll Sequence <p> HTTP polling uses standard web protocols and ports , and generally will @ @ @ @ @ @ @ @ @ @ top of standard HTTP ( port 80 ) or HTTPS ( port 443 ) without any issue . - This can be achieved by polling JSON services , XML Services , AMF , or any other data format on top of a HTTP request . - HTTP polling will generally be slower than a direct socket method , and will also utilize more network bandwidth b/c of request/response encapsulation and the periodic requests to the server . - It is also important to keep in mind that the HTTP spec only allows for 2 concurrent connections to a server at any point in time . - Polling requests can consume HTTP connections , thus slowing load time for other portions of your application . - HTTP polling can be employed in HTML/JS , Flex/Flash/AIR , desktop , server , or basically any other type of application using common libraries &amp; APIs . <h> HTTP Push <p> HTTP Push technologies fall into 2 general categories depending upon the server-side **25;2229;TOOLONG - This can refer to- HTTP Streaming , where a connection is opened between the client and server and kept open @ @ @ @ @ @ @ @ @ @ the client , it will be pushed across the existing open HTTP connection . - HTTP Push can also refer to HTTP Long Polling , where the client will periodically make a HTTP request to the server , and the server will " hold " the connection open until data is available to send to the client ( or a timeout occurs ) . - Once that request has a complete response , another request is made to open another connection to wait for more data . - Once Again , with HTTP Long Poll there should be a very short polling interval to maintain near-realtime performance , however you can expect some lag . <p> HTTP Long Poll Sequence <p> HTTP Streaming &amp; HTTP Long polling can be employed in HTML/JS applications using the Comet approach ( supported by numerous backend server technologies ) and can be employed in Flex/Flash/AIR using BlazeDS or LCDS . <h> Collaborative Applications <p> Now back to the collaborative sketching application shown in the video above the application builds off of the sketching example from previous blog posts . - I added logic to @ @ @ @ @ @ @ @ @ @ service to share content between sessions that share a common I 'd . <p> Realtime Collaborative Sketches <p> In the JavaScript code , I created an ApplicationController class that acts as an observer to the input from the Sketcher class . - The ApplicationController encapsulates all logic handling data polling and information sharing between sessions. - When the application loads , it sets up the polling sequence . <p> The polling sequence is setup so that a new request will be made to the server 250MS- after receiving a response from the previous request . - Note : this is very different from using a 250MS interval using setInterval. - This approach guarantees 250MS from response to the next request . - If you use a 250MS interval using setInterval , then you are only waiting 250MS between each request , without waiting for a response . - If your request takes more than 250 MS , you will can end up have stacked , or " concurrent " requests , which can cause serious performance issues . <p> When observing the sketch input , the start and end positions and @ @ @ @ @ @ @ @ @ @ of captured transactions that will be pushed to the server . - ( The code supports multiple colors , even though there is no method to support changing colors in the UI . ) <p> The server then stores the pending transactions in memory ( I am not persisting these , they are in-ram on the server only ) . - The server checks the transactions that are already in memory against the last timestamp from the client , and it will return all transactions that have taken place since that timestamp . <p> In a previous post on capturing user signatures in mobile applications , I explored how you capture user input from mouse or touch events and visualize that in a HTML5 Canvas . - Inspired- by activities with my daughter , I decided to take this signature capture component and make it a bit more fun &amp; exciting . - My daughter and I often draw and sketch together whether its a magnetic sketching toy , doodling on the iPad , or using a crayon and a placemat at a local pizza joint , there is always @ @ @ @ @ @ @ @ @ @ I was actually good at drawing . ) <p> Olivia &amp; the iPad <p> You can take that exact same signature capture example , make the canvas bigger , and then combine it with a tablet and a stylus , and you 've got a decent sketching application . - However , after doodling a bit you will quickly notice that your sketches leave something to be desired . - When you are drawing on a canvas using moveTo ( x , y ) and lineTo ( x , y ) , you are somewhat limited in what you can do . You have lines which can have consisten thickness , color , and opacity . You can adjust these , however in the end , they are only lines . <p> If you switch your approach away from moveTo and lineTo , then things can get interesting with a minimal amount of changes . You can use images to create " brushes " for drawing strokes in a HTML5 canvas element and add a lot of style and depth to your sketched content . - This is an approach that @ @ @ @ @ @ @ @ @ @ I 've worked on in the past . - Take a look at the video below to get an idea what I mean . <p> Examining the sketches side by side , it is easy to see the difference that this makes . - The variances in stroke thickness , opacity &amp; angle add depth and style , and provide the appearance of drawing with a magic marker . <p> Sketches Side By Side <p> Its hard to see the subtleties in this image , so feel free to try out the apps on your own using an iPad or in a HTML5 Canvas-capable browser : <p> Just click/touch and drag in the gray rectangle area to start drawing . <p> Now , let 's examine how it all works . - Both approaches use basic drawing techniques within the HTML5 Canvas element . - If you are n't  familiar with the HTML5 Canvas , you can quickly get up to speed from the tutorials from Mozilla. <h> moveTo , lineTo <p> The first technique uses the canvass drawing context moveTo ( x , y ) and lineTo ( x , y @ @ @ @ @ @ @ @ @ @ coordinates . - Think of this as playing " connect the dots " and drawing a solid line between two points . <p> The sample output will be a line from point A , to point B , to point C : <p> lineTo ( x , y ) Stroke Sample <h> Brush Images <p> The technique for using brush images is identical in concept to the previous example you are drawing a line from point A to point B. - However , rather than using the built-in drawing APIs , you are programmatically repeating an image ( the brush ) from point A to point B. <p> First , take a look at the brush image shown below at 400% of the actual scale . - It is a simple image that is a diagonal shape that is thicker and more opaque on the left side . - By itself , this will just be a mark on the canvas . <p> Brush Image ( 400% scale ) <p> When you repeat this image from point A to point B , you will get a " solid " line @ @ @ @ @ @ @ @ @ @ upon the angle of the stroke . - Take a look at the sample below ( approximated , and zoomed ) . <p> Brush Stroke Sample ( simulated ) <p> The question is how do you actually do this in JavaScript code ? <p> First , create an Image instance to be used as the brush source . <p> brush = new Image() ; brush.src = ' assets/brush2.png ' ; <p> Once the image is loaded , the image can be drawn into the canvas context using the drawImage() function . The trick here is that you will need to use some trigonometry to determine how to repeat the image . In this case , you can calculate the angle and distance from the start point to the end point . Then , repeat the image based on that distance and angle . <p> This example uses the twitter bootstrap UI framework , jQuery , and Modernizr. - Both the lineTo.html and brush.html apps use the exact same code , which just uses a separate rendering function based upon the use case . - - Feel free to try out @ @ @ @ @ @ @ @ @ @ a HTML5 Canvas-capable browser : <p> Here are some interesting and quite surprising statistics for the US Census Browser HTML/PhoneGap showcase application that I released in December , which I wanted to share . The app is a browser for US Census data , full detail available here : http : **35;2256;TOOLONG . The Census Browser application was intended as a showcase app for enterprise-class data visualization in HTML-based applications , and all source code is freely available to the public . <p> What is really surprising is the " health " of my app within the given ecosystems . I offered the app as a free download in each market . The app is focused on Census data , so there is obviously not a ton of consumer demand , however the data is still interesting to play around with . I would not expect the same results for all types of apps in all markets . <p> BlackBerry Playbook downloads were in 3rd , just behind iOS ( BB is 11% of all downloads ) <p> Android traffic was minimal ( 2% of all downloads ) <p> The @ @ @ @ @ @ @ @ @ @ market is strongest , followed by Android , and that BB is dead . These numbers show a conflicting reality . Barnes &amp; Noble was the strongest , with iOS in second place , and BlackBerry just behind iOS. 
@@106848774 @2248774/ <h> Category Archives : Photography <p> I recently purchased a Beholder Lite camera gimbal for my DJI Phantom quadcopter , and I am very pleased with it . Bottom line for those that do n't  want to read this entire post The Beholder Lite is hard to beat for the price , as long as you have some time to tune the gimbal. - Output video is very steady when flying reasonably , and still images are far more crisp than they are without a gimbal . The final output is not quite as good as the- H3-2D Zenmuse , but it is still much more than acceptable. - Plus , you can buy almost 4 Beholder Lites for the cost of one Zenmuse . However , when flying aggressively , there is a lot of vibration . Read more for an explanation , pros &amp; cons , plus tips for set up and some sample videos . <p> DJI Phantom with Beholder Lite Gimbal <h> Background <p> Um What is a camera gimbal ? <p> First , a gimbal is : " a- pivoted support that allows the @ @ @ @ @ @ @ @ @ @ set of three gimbals , one mounted on the other with- orthogonal- pivot axes , may be used to allow an object mounted on the innermost gimbal to remain independent of the rotation of its support . " <p> Basic Gimbal ( photo : Wikipedia ) <p> A camera gimbal is a set of these gimbal supports that enables the cameras movement to be independent from the support structure . In this case , the camera gimbal allows the cameras orientation to be independent from the orientation of the helicopter . Check out this video for more detail : <h> The Beholder Lite Gimbal <p> The Beholder Lite gimbal is a direct-drive brushless gimbal . This means : 1 ) the motors are brushless motors , and 2 ) that the motors directly drive the support arms for the gimbal ; there are no servo arms or other moving parts in the gimbal assembly . Brushless motors are faster than traditional/brushed servo motors , so they offer a smoother response and better stabilization . I have used the Phantom with no gimbal , as well as with a brushed/servo gimbal @ @ @ @ @ @ @ @ @ @ best quality . <p> I mentioned that I am very happy with this gimbal , though it has not been without its own hiccups . <h> Pros : <p> Very stable footage ****once properly installed and tuned**** <p> Still images are more crisp <p> Videos are far more stable , though aggressive flying will cause significant vibration ( this happens with all gimbals , though some more than others ) <p> Very affordable compared to similar gimbals on the market . <h> Cons : <p> Installation instructions are not very good . They are based on pictures only , and do not clearly identify motor orientation or wiring . I originally had the motors in the wrong direction , and the motor polarity reversed . This third-party post ( with video ) was very helpful for installation . Ive provided additional installation details at the bottom of this post . <p> The gimbal is marketed as " ready to go " you just assemble it and start flying . Once assembled , I had lots of vibration to the point of being unusable . I seriously considered returning it . @ @ @ @ @ @ @ @ @ @ could get some decent footage . This was both physical ( in the mounting ) , and in software configuration . I had to adjust the output gains before the gimbal would provide stable footage ( details below ) . <p> The vibration absorbing mounts are too soft . You will get vibration from the camera oscillating below the copter . Luckily this can be corrected but sticking some foam ear plugs inside the vibration mounts ( see details below ) . <p> Both aggressive flying and high-wind environments will cause additional vibration in both the copter and gimbal , so keep those in mind when you are filming. - <h> Sample Video Footage <p> I still do n't  feel like I have the gimbal settings ( gains ) 100% dialed-in , but I have it close enough to get some high quality footage . I have found that the 1080p@60fps video mode on the GoPro camera has the best results . 2.7K@30fps has too much rolling shutter effect for the footage to be really usable . I also have not balanced the propellors on my copter , which will @ @ @ @ @ @ @ @ @ @ samples from the camera/gimbal combination . All of this footage was captured with a GoPro Hero 3 Black Edition . <p> 1080p@60fps with Post Processing via Adobe Premiere : I still think I can get it more stable by changing a few editing parameters and changing the sequence of my scaling and effects , but I am very happy with this output . <p> 2.7K@30fps with 10% Warp Stabilization The footage is far better than it is without a gimbal , but there 's still some rolling shutter effect evident ( capturing Tonys hex liftoff ) . <p> Aggressive Flight : This footage is completely raw , without any post-processing or stabilization . This shows output when flying aggressively . This was *very* aggressive flight full speed in ATTI mode . I recommend that you *never* attempt to shoot professional footage flying like this : <p> Pay particular attention to motor orientation and wiring . The motors look symmetrical , but their weight and operation is not . The pitch motor should have the wire coming out the side that is closest to the copter . The roll motor should have @ @ @ @ @ @ @ @ @ @ . If you have your motor polarity reversed , you 'll know b/c the gimbal will vibrate back and forth . This wont hurt it , and you can just reverse the wires easily if this happens . In the picture below you can see my motor orientation : <p> Motor Orientation and Wiring <p> Very important : make sure your gimbal is balanced . If the power is off , the GoPro should not fall in any direction . It should just stay where it is . Having it perfectly balanced is key to having smooth footage . <p> The rubber vibration dampeners are too soft for the gimbal . If you fly it " stock " , you will get a TON of vibration from the gimbal oscillating under the copter . Use some soft foam earplugs , roll them up , and put them inside of the dampeners . I used the cheap foam orange ones you get from any drug store , and they work great . I put one in each of the 6 vibration dampeners . Another picture below , so you can see the @ @ @ @ @ @ @ @ @ @ necessary , but I also put Moongel in between the gimbal and the copter body to absorb vibrations , and I also put it on top of the gimbals gyro board to absorb vibrations that may affect gimbal orientation ( see picture below ) . <p> Beholder Lite on DJI Phantom <p> Do not get the white wires too close to the power wires or to the motors . They are very light weight , and can get interference from power wires , motors , and motor wires . You will get inexplicable vibrations if this occurs . <p> One last thing If you use zip ties on the gimbal for extra safety , keep them very loose . If you compress the dampeners , the bottom plate will hit the controller board , and cause significant interference with gimbal operation . Everything will be fine one second , then completely out of control the next . <h> Battery Life <p> Before having a gimbal , I could easily get 12+ minutes of flight time per battery . With a servo-based gimbal , I could get about 10+ minutes per @ @ @ @ @ @ @ @ @ @ max of 8 minutes per battery . I set a timer for 7 minutes , and be sure to bring it down as soon as the time goes off . However , this seems comparable to battery life with other brushless gimbals that friends/coworkers use . The decreased battery life is due to additional weight of the gimbal , plus the additional power consumption from the gimbal motors . <h> What Next ? <p> Use Creative Cloud to process all of your video and images to make them the best they can be ! - Here are some very useful posts for processing your content with this configuration ( or similar configurations ) <p> Adobe has introduced a bunch of new features in the Photoshop CC releases over the course of the year . One feature that I knew about , but had n't  used much myself was the new " preserve details " upsampling algorithm when resizing images , which was introduced in the spring . - Well , I was just looking into resizing one of my aerial photos of San Francisco for printing , and I was @ @ @ @ @ @ @ @ @ @ . <p> Aerial Panorama of San Francisco <p> I stitched together the panorama from 6 or 7 images captured with a GoPro and DJI Phantom quadcopter ( details here ) . The original panorama image size was- 4746+1706 pixels I wanted to see how big I could make it for large-scale printing , and I upscaled the image by 469% to 22256+8000 using Photoshops image resizing features . The results are very impressive . <p> Here are a few side by side comparisons of different resampling/upscaling algorithms , the new " Preserve Details " algorithm is the last one . While none of them is perfect , the output of the new " preserve details " algorithm produced very good/high quality results . You can adjust the " reduce noise " slider to tweak the resizing algorithms output to minimize visual artifacts that may come from the up-sampling process . <p> You can check out the final output by interacting with the image below . - You can zoom in and pan around just by interacting with the image directly . When you zoom in really far , you can @ @ @ @ @ @ @ @ @ @ in the original picture , but you 'll quickly see this can be immensely useful for upscaling smaller images for printing or other uses . - As a simple game , see if you can spot the Adobe San Francisco office ! <p> Check out more details on the Preserve Details upscaling feature in the video below , from Adobe TV : <p> I 'm testing out something new Im calling it " Photoshop Friday " , where every Friday , optimistically assuming that I do n't  have deadlines looming , I 'm going to try and do something fun with Photoshop . Whatever I can think of Two weeks ago I did some " crystal ball " compositions , and today Ive put together what I 'm calling Sky Aquariums. - Check out the timelapse video below <p> If you 're wondering how I created the timelapse , this technique for interval-based screen grabs works beautifully . - Just run this as a shell script from the directory where you want the screen capture images to be stored ( OSX only ) : <p> Last week I had the opportunity to feed my obsession @ @ @ @ @ @ @ @ @ @ Training- workshop at Photoshop World , and it was an awesome experience ! Many thanks to Russell for putting together this incredible workshop . There 's nothing like a giant room full of RC photography enthusiasts , with multiple copters flying around all over the place , coupled with a wealth of Photoshop and video processing knowledge ( links below for all of this content ) ! <p> There are few places that have as dramatic architecture and landscape as Vegas , and this also made for some great- photos . <p> Aerial shot over the Luxor and Mandalay Bay in Las Vegas <p> The day started with an intro from DJI , who sponsored the event and donated copters for workshop participants to use . - If you have n't seen their copters/footage yet , you should check out the short video below to see what 's possible with todays radio controlled aircraft . <p> Next , we broke off into teams to cover basic flight training principles everything from safety ( the most important thing of all ) , basic takeoff and landing procedures , flight modes and capabilities , to @ @ @ @ @ @ @ @ @ @ had a chance to fly the copters and get acclimated , we regrouped to learn how to work with your aerial footage. - Do not miss the videos from Russell Brown and Colin Smith in the links below . These are must-watch content for any aerial photographer , cinematographer , or GoPro enthusiast and contain a wealth of information ! <p> Finally , the day ended with a flying competition The person who could fly through 4 hula hoops without knocking anything over , without crashing , and land in a controlled fashion walked home with a brand new DJI Phantom , and the winner did it in 26 seconds ! This was not an easy task . <p> I consider this to be a fairly minimalist rig , and everything is very portable , which is great for travel . <h> Video Capture <p> For all of my front-facing videos I am using a Panasonic Lumix LX7 , and for some of my on-device and secondary camera angles , I use a GoPro Hero 3 Black Edition . The LX7 is my go-to camera for both photography and video @ @ @ @ @ @ @ @ @ @ fully automatic or manual control of the image capture settings . <p> Panasonic Lumix LX7 and GoPro Hero 3 Black <p> When recording videos , I set the LX7 to manual mode so it wont auto-focus or automatically adjust light balance , and I record everything in full HD ( 1080p , 30 FPS ) . On the GoPro , I normally select 1080p at either 30 or 60 FPS ( depending on the situation and lighting ) . If I am outdoors , I 'll have Protune on , if I 'm indoors I usually have Protune off . <p> Camera settings are only part of the whole process . To have a decent output , you really need proper lighting , and a backdrop that is n't distracting . I try to keep this setup very simple : I 'll place the camera on a tripod on the opposite side of my desk so that it faces me directly . Behind me , I 'll have a black muslin backdrop this helps everything else stand out from the background , and it does n't  reflect any light . Simple tip : Use a @ @ @ @ @ @ @ @ @ @ will fall out pretty quickly . <p> Office/Studio Lighting and Backdrop <p> For lighting , I have darkening blinds that block out nearly all outside light this way you can control the lighting for your video shoots . Often shooting a video may be done over several days , and I cant rely on the weather and natural light to be consistent . With the room darkened , I normally use a single light source above and slightly off to the side of the camera . I try to find an angle that lights me up from the front , but does not reflect in my glasses . I also dim the display on my computer , so its not reflecting in my glasses , or dramatically altering the the lighting . <p> Once the lighting has been set , I set up the camera and adjust zoom , focus , and aperture ( exposure ) where I want it for the current video . <p> Framing The Shot <h> Audio Capture <p> Great video is only half of the equation Without clear audio , the videos are n't  nearly @ @ @ @ @ @ @ @ @ @ audio . I started off using the built-in mics on my cameras , but quickly learned that the internal microphones werent going to cut it . For all of my recent videos Iver started using a Zoom H4N digital audio recorder . The Zoom enables high quality stereo recording . Its very easy to use , and the recording quality is fantastic now if only I could get those birds to stop chirping outside of my window . <p> Zoom H4N Digital Audio Recorder <h> Editing &amp; Post-Production <p> Capturing content is the first part of the process . The second part is editing everything together . In the editing process , I take advantage of all of the creative tools Adobe has to offer . Most of my video editing is done with- Premiere Pro . This includes clipping &amp; sequencing , color correction , effects , etc All of my audio production is done with Adobe Audition- this includes sound cleanup , and mastering . For graphics , I use Photoshop and Illustrator , depending on the format and content . If you want to insert animations , @ @ @ @ @ @ @ @ @ @ , export as video , and pull it into your Premiere project . <p> Video Editing in Adobe Premiere Pro CC <p> Once everything is how I want it , I 'll export to H.264 format ( for the web ) , upload to YouTube , and then start syndicating it however possible/necessary . Normally , its just pulling in a YoutTube- video into a blog post . 
@@106848776 @2248776/ <h> AMF vs . JSON in AIR Mobile Applications <p> Recently , I 've been asked more than once which is better : AMF or JSON for AIR mobile applications . This post is to highlight some performance comparisons , and a sample testing application that I put together . First , it is important to know what both AMF and JSON are . <h> AMF <p> Action Message Format ( AMF ) is a compact binary format that is used to serialize ActionScript object graphs . Once serialized an AMF encoded object graph may be used to persist and retrieve the public state of an application across sessions or allow two endpoints to communicate through the exchange of strongly typed data . <h> JSON <p> JSON ( JavaScript Object Notation ) is a lightweight data-interchange format . It is easy for humans to read and write . It is easy for machines to parse and generate . It is based on a subset of the JavaScript Programming Language , Standard ECMA-262 3rd Edition December 1999 . JSON is a text format that is completely language independent but uses conventions @ @ @ @ @ @ @ @ @ @ , including C , C++ , C# , Java , JavaScript , Perl , Python , and many others . <p> I put together a very basic test case where a mobile application makes requests of simple data objects from a ColdFusion CFC. - In each test iteration , a request is made for 1 , 10 , 100 , 1000 , and 10000 value objects , in both AMF and JSON formats . The total round trip time from request to deserialization is measured and compared for each case , for a total of 5 iterations through each cycle . - My findings are that AMF and JSON have- comparable- performance in smaller record sets . - However , AMF seems to have better performance as data sets grow . - In my test cases , the 1000+ record results were consistently faster using AMF. - However , in smaller data sets , JSON was often faster ( however not consistently , or by much of a margin ) . I tested these times on both an iPhone 4 and Motorolla Atrix , both running on the carrier networks @ @ @ @ @ @ @ @ @ @ video of the serialization testing application at work . <p> Here are a few screenshots of the application . <h> The Tests <p> For these tests I created two basic CFCs ( ColdFusion Components ) . One is a simple data value object . The other CFC is a gateway to expose a remote service that returns the value objects to the client . I chose a ColdFusion CFC for this case b/c it can easily be serialized as AMF or JSON just by changing the endpoint used to consume the service . <p> Obviously , this is a fictional data object with randomly generated values . However , it still represents a reasonable service payload for data serialization . By accessing the data via the ColdFusion Flex/Remoting gateway , you access the remote services via AMF3 . <p> In the mobile client application , I have a **27;2293;TOOLONG class that handles all of the test logic and communications back and forth with the server . The time for each test is measured from immediately before the the request is made to the server , until after the data has @ @ @ @ @ @ @ @ @ @ **27;2322;TOOLONG class below : <p> The answer to the question of " should I use AMF or JSON " is subjective What kind of data are you returning , and how much data is it ? Do you already have AMF services built ? - Do you already have JSON services built ? - Are the services consumed by multiple endpoints , with multiple technologies ? - Do you rely upon strongly typed objects in you development and maintenance processes ? - Both AMF and JSON are viable solutions for mobile applications . 123433 @qwx983433 <p> Is it correct to assume that , given your description of the technologies , AMF is more secure than JSON ? Or would that depend on the protocol through which you access the data ? Does AMF eliminate the need to use SSL ? 123436 @qwx983436 <p> No , I would n't say that it is more secure . Both are serialization formats used to structure data , not encryption schemes . Both AMF and JSON are normally delivered over HTTP . HTTP is not considered a secure mechanism . You can add security to @ @ @ @ @ @ @ @ @ @ HTTPS ) . <p> http : //esdot.ca shawn <p> I think the test is flawed , because 80% of the time must be downloading the file So , when AMF appears 20% faster , if you remove the download time , its actually probably 500% faster . <p> I think the real thing we want to measure is code execution time when reading or writing the data . <p> 10,000 AMF objects should n't take more than 500ms on a mobile device 123436 @qwx983436 <p> In these tests I am specifically measuring end-to-end performance , including download time . Not just the client-side deserialization . The user is going to experience the full end-to-end scenario from the point that they request data , to the time that the data is delivered to their device . Using AMF for large data collections can shave off several seconds of download time opposed to JSON . <p> http : //esdot.ca shawn <p> In my mind that obscures the comparison between the two . <p> Download time is not dependant on the format used , except as it relates to fileSize . Its also highly @ @ @ @ @ @ @ @ @ @ test that measures : 1 . File size between the the two formats ( I can calculate theoretical download speed if I want ) 2 . Time required to write , and time requires to read , the files into AS3 . <p> Reading this comparison , I still do n't  come away with any idea which produces larger file size , or which is faster to decompress <p> Keep in mind , these formats are not always used to fetch data remotely . Many applications will use AMF or JSON to cache data internally for their applications . <p> Knowing which is faster for reading and writing , and therefore more scalable , and less likely to lock up the device , is the key piece of info <p> At the very least , its certainly valuable to add another comparison that removes download time from the equation . 123436 @qwx983436 <p> I 'll see what else I can dig up to get more file size and speed #s out of this . The native JSON parser is significantly faster than the other open source ActionScript based JSON parsers that @ @ @ @ @ @ @ @ @ @ I 'd say AMF is slightly more secure , as in its not human readable . <p> In order to read AMF , someone would need to capture the file , load it into a flash project , and decompress the data . <p> Not secure by any means , but at least they cant literally open it , and start browsing through the values 123436 @qwx983436 <p> I would emphasize the *slightly* , b/c there are cases when you can still read some text out of AMF encoded data when viewed in a text editor . There are also tools to decode AMF . For enterprise class security , I strongly recommend an encryption layer . <p> James Wards application compares comressed vs uncompressed . I have not played with toggling compression on/off . I do not have hard metrics on native json vs as3core , however my experience has been that it is noticably faster using the native JSON classes . <p> http : //www.mikutech.com Mark <p> Just to confirm these tests were run on Flash Player 11 , with the native JSON support ? You mention it in @ @ @ @ @ @ @ @ @ @ you test against , so I thought I 'd make sure . 123436 @qwx983436 <p> Yes , it uses FP11 , using native JSON . I do n't  recall the exact version number though . 
@@106848777 @2248777/ <p> This post is inspired by all the comments Ive seen this week about JS in the enterprise . I would have never imagined- this 10- years ago , but JavaScript is now pretty much ubiquitous . Here are a few reasons why you need to paying attention to JavaScript if you are n't  already , and why you should definitely not- write it off . <p> First , I think one of the major reasons for JavaScripts ubiquity is that- JavaScript is approachable . It is relatively easy for beginners to learn JavaScript , and powerful enough for advanced users to build complex and reliable systems . <p> Second , why you need to pay attention , JavaScript is everywhere . <p> You can now use JavaScript to develop on virtually any platform : client side applications , server side logic , embedded chips/IoT devices , manage build scripts and dependencies , and more . <p> This does n't  mean you 'll use the exact same code in every case , rather that you can use the same skill set JavaScript Development to deliver solutions across multiple paradigms . @ @ @ @ @ @ @ @ @ @ power client side apps/user interfaces , and user interactions on numerous platforms and devices . <h> Web <p> Of course JavaScript powers the web , this is a given . JavaScript is the primary scripting language for all web browsers . I wo n't focus on this much b/c its already well known . <h> Mobile <p> JavaScript can also be used to power mobile applications that are natively installed on a device . <p> Apache Cordova/PhoneGap- You can build natively installed apps with web technology using PhoneGap or Cordova . PhoneGap is Adobes branded distribution of Cordova , but from the developers perspective , they are basically the same thing . Your app runs within a webview on the mobile device , and you build your user interface the same way you you build a dynamic web application . Your user interface- is implemented in HTML , styled with CSS , and all interactivity is created with JavaScript . <p> React Native- JavaScript powered web apps do n't  just have to be inside of a a web view . The React Native framework gives developers the ability to write their @ @ @ @ @ @ @ @ @ @ in a native application running on the mobile device . The logic is interpreted JavaScript at runtime , but everything that the user- interacts with ( all UI elements ) is 100% native , providing a very high quality user experience , and it is now available for both iOS and Android applications . <p> Unity 3D You can even develop rich &amp; immersive mobile 3D simulation or gaming experience , entirely powered by JavaScript using the Unity 3D engine . **These can be web , desktop , or mobile , but is often used in mobile gaming . <h> Desktop <p> Yup , desktop apps are not left out of the mix . Most desktop solutions fall into a category similar to Apache Cordova , where the end results is a web view that has access to lower level APIs , whose content is developed with web based technology . <h> The Server Side <p> Most obviously- Node.js a JavaScript runtime buit on Chromes V8 JavaScript Engine has made huge inroads into server side development and the enterprise . Node.js , powered by frameworks like express.js or loopback.io makes @ @ @ @ @ @ @ @ @ @ . <p> Here are some stats that- show the magnitude of growth and adoption for Node.js/npm.js alone . NPM- stats currently shows a total of- 186,946 packages available for download , 94,978,032 package downloads in the last day , and 2,451,734,737 package downloads in the last month . <p> This does n't  mean that JavaScript is the best language at everything . It- also does n't  meant that you can take a single piece of source- code and run it in every device/context imaginable . <p> It means that you can use your skills in JavaScript to develop for just about any kind of device/context out there . Its not going to be write once , run everywhere , rather in the words of the React.js team : learn once , write everywhere . <p> Node.js is an incredible tool for rapidly building highly performant and scalable back end systems , and you develop it using a familiar core language that most front-end developers are already accustomed to , JavaScript. - This acquisition is positioned to greatly enhance Node.js in the enterprise , and StrongLoops offerings will be integrated into @ @ @ @ @ @ @ @ @ @ Even though the acquisition is still " hot off of the presses " , - you can start using these tools together- today : <p> If you have n't heard about StrongLoop 's LoopBack framework , it enables you to easily connect and expose your data as REST services . It provides the ability to visually create data models in a graphical ( or command line ) interface , which are used to automatically generate REST APIs " thus generating CRUD operations for your REST services tier , without having to write any code . <p> Why is this important ? <p> It makes API development easier and drastically reduces time from concept to implementation . - If you have n't yet looked at the LoopBack framework , you should definitely check it out . - You can build API layers for your apps literally in minutes . - Check out the video below for a quick introduction : <p> Again , be sure to check out these posts that detail the integration steps so you can start using these tools together today : <p> That title get your attention ? - @ @ @ @ @ @ @ @ @ @ change based on personal context " with near real-time rules application , without much extra development effort . - If that sounds interesting to you , or like a product you might want to use within your own apps , then you might want to check out this site where you can get involved in the products development : - http : **35;2351;TOOLONG <p> IBM is looking for your input on creating these types of mobile app experiences. - User experiences within a single app that can be- dramatically different per user based on location , past behavior , profile information , social media activity , - and so much more . - With this behavior being driven by configurable rules that can be changed without redeploying an app to the app store . <h> How it works for your customer <p> Consider this scenario : <p> Jon and Andrea download the mobile app for S&amp;W , a retailer known for its attention to providing great customer service . Over the next month , Jon and Andrea use the app to browse and discover content and merchandise differently . <p> @ @ @ @ @ @ @ @ @ @ teams to find gear and clothes for travel to his favorite team 's games . Andrea scours the app for sales and fashion trends and usually ends up following her favorite designers . <p> Andrea and Jon go to a baseball game together . She 's never enjoyed watching it , so she opens up the S&amp;W app to entertain herself , and her app 's navigation quickly steers her through Spring fashion articles . <p> Jon however , wants to replace the hat he 's worn the last three times the team lost , and since he 's in the stadium , his S&amp;W app opens right up to the team 's gear page . The app knows he 's out of town and tells him how to get to an S&amp;W store . <h> How it works for the dev team <p> Consider another scenario : <p> One of the developers on the team , George , sets up the system and application . He then gives access to Janet who is responsible for the customer experience . <p> Janet writes rules defining how the application could adapt @ @ @ @ @ @ @ @ @ @ media , geolocation , app usage , or customer information data . <p> Once Janet has built out her rules , she simply hits Submit ' and can immediately see her clever interactions reflected in the mobile application without having to involve the development team . <p> Analytics let Janet know which adaptations are working best , and helps her find new opportunities to optimize the app 's user experience . <p> Were not talking about a content management system , or translation based on locale , instead a rules-driven product that can adapt literally every aspect of your app : - customize the user interface , enable or disable different features , customized messaging and notifications , and much more , all variable based upon the user context . - This can be used to present- contextually relevant information , drive adoption , provide more/less data depending on your physical context , and so much more . <p> It wont be tied to a specific UI framework , wont be tied to a specific content management system , is n't attempting to re-create Google Now or Apple Proactive Assistance . @ @ @ @ @ @ @ @ @ @ engine that enable you to customize and tailor the app experience to the individual user . <p> In this post I 'd like to show a fairly simple application that I put together which shows off some of the rich capabilities for IBM MobileFirst for Bluemix that you get out of the box All with an absolute minimal amount of your own developer effort . - Bluemix , of course , being IBMs platform as a service offering . <p> GeoPix is a sample application leveraging IBM MobileFirst for Bluemix to capture data and images on a mobile device , persist that data locally ( offline ) , and replicate that data to the cloud . Since its built with IBM MobileFirst , we get lots of things out of the box , including operational analytics , user authentication , and much more . <p> ( full source code at the bottom of this post ) <p> Heres what the application currently- does : <p> User can take a picture or select an image from the device <p> App captures geographic location when the image is captured <p> App saves both @ @ @ @ @ @ @ @ @ @ the device . <p> App uses asynchronous replication to automatically save any data in local store up to the remote store whenever the network is available <p> Oh yeah , cant forget , the user auth- is via Facebook <p> MobileFirst provides all the analytics we need . - Bluemix provides the cloud based server and Cloudant- NoSQL data store . <p> All captured data is available on a web based front-end powered by Node.js <p> In this sample I 'm using everything but the Push Notifications service . - Im- using user authentication , the Cloudant DB ( offline/local store and remote/cloud store ) , and the node.js backend. - You get the operational analytics automatically . <h> Capturing Images <p> Capturing images from the device is also very straightforward . - In the app I leverage Apples- UIImagePickerController- to allow the user to either upload an existing image or capture a new image . - See the presentImagePicker and- **29;2388;TOOLONG below . All of this standard practice using Apples developer SDK : <h> Persisting Data <p> If you notice in the- **29;2419;TOOLONG method above , there is a call @ @ @ @ @ @ @ @ @ @ we save data locally and rely on Cloudants replication to automatically save data from the local data store up to the Cloudant NoSQL database . - This is powered by the iOS 8 Data service from Bluemix . <p> The first thing that we will need to do is initialize the local and remote data stores . Below you can see my init method from my DataManager class . In this , you can see the local data store is initialized , then the remote data store is initialized . If either data store already exists , the existing store will be used , otherwise it is created . <p> Once the data stores are created , you can see that the replicate method is invoked . - This starts up the replication process to automatically push changesfrom the local data store to the remote data store " in the cloud " . <p> Therefore , if you 're collecting data when the app is offline , then you have nothing to worry about . - All of the data will be stored locally and pushed up to the cloud whenever @ @ @ @ @ @ @ @ @ @ part . - When using replication with the Cloudant SDK , you just have to start the replication process and let it do its thing fire and forget . <p> In my replicate function , I setup- CDTPushReplication for pushing changes to the remote data store . - You could also setup two-way replication to automatically pull new changes from the remote store . <p> Once weve setup the remote and local data stores and setup replication , we now are ready to save the data the were capturing within our app . <p> Next is my saveImage withLocation method . - Here you can see that it creates a new- **26;2450;TOOLONG object ( this is a generic object for the Cloudant NoSQL database ) , and populates it with the location data and timestamp. - It then creates a jpg image from the UIImage ( passed in from the UIImagePicker above ) and adds the jpg as an attachment to the document revision . - Once the document is created , it is saved to the local data store . - We then let replication take care of persisting @ @ @ @ @ @ @ @ @ @ want to query data from either the remote or local data stores , we can just use the performQuery method on the data store . Below you can see a method for retrieving data for all of the images in the local data store . <p> At this point we 've now captured an image , captured the geographic location , saved that data in our local offline store , and then use- replication to save that data up to the cloud whenever it is available . <p> AND <p> We did all of this without writing a single line of server-side logic . - Since this is built on top of MobileFirst for Bluemix , all the backend infrastructure is setup for us , and we get operational analytics to monitor everything that is happening . <p> With the operational analytics we get : <p> App usage <p> Active Devices <p> Network Usage <p> Authentications <p> Data Storage <p> Device Logs ( yes , complete debug/crash logs from devices out in the field ) <p> Push Notification Usage <h> Sharing on the web <p> Up until this point we have n't @ @ @ @ @ @ @ @ @ @ app boilerplate on Bluemix comes with a Node.js server . - We might as well take advantage of it . <p> The Node.js back end comes preconfigured to leverage the- express.js- framework for building web applications . - I added the- jade template engine and Leaflet for web-mapping , and was able to crank this out ridiculously quickly . <p> The first thing we need to do is make sure - we have our configuration variables for accessing the Cloudant service from our node app. - These are environment vars that you get automatcilly if you 're running on Bluemix , but you need to set these for your local dev environment : <p> Next you 'll se the logic for querying the Cloudant data store and preparing the data for our UI templates . You can customize this however you want caching for performance , refactoring for abstraction , or whatever you want . All interactions with Cloudant are powered by the Cloudant Node.js Client <p> Push notifications , love them or hate them , are everywhere and there 's no getting around it . Push notifications are short messages that can @ @ @ @ @ @ @ @ @ @ are actually running . They can be used to send reminders , drive engagement with the mobile app , notify completion of long running processes , and more . Push notifications- send information to you in real time , rather than you having to request that information . <p> If you are building a back-end infrastructure to manage your applications data , and you want to leverage push notifications , then guess what ? You also have to build the hooks to manage subscription and distribution of push notifications for each platform . <p> The- unified- push notification API allows you to develop your app against a single API , yet deliver push notifications to multiple platforms , and it works with both hybrid ( HTML/CSS/JS ) apps , as well as native apps . <p> You will still have to build the logic to subscribe devices for messaging , and dispatch push notification messages , but you 'll only have to do it once against the unified API not once for each platform . <p> The apps that I showed in the video above are sample apps taken straight from @ @ @ @ @ @ @ @ @ @ , and can be accessed in their entirety ( with both client and server code ) using the links below : <p> On the client app , you 'll need to subscribe for messages from the event source . See the hybrid or native code- linked to above for syntax and examples . <p> Once your clients are subscribed , you can use a single server-side implementation to distribute messages to client apps . Below is an- excerpt from the sample application which demonstrates sending a push notification to all devices for a particular user ( on any platform ) : <p> From- the MobileFirst console , you will be able to monitor and manage event sources , platforms , and the devices that are consuming push notifications . <p> Push Notifications on the MobileFirst Console <p> If you were wondering , yes , these can be cloud-hosted on IBM BlueMix and yes , it can also be installed on-premise on your own server in your data center . - You have the option to configure- your physical or cloud servers however you want . 
@@106848778 @2248778/ <p> I consider this to be a fairly minimalist rig , and everything is very portable , which is great for travel . <h> Video Capture <p> For all of my front-facing videos I am using a Panasonic Lumix LX7 , and for some of my on-device and secondary camera angles , I use a GoPro Hero 3 Black Edition . The LX7 is my go-to camera for both photography and video work . It captures great images , and allows for fully automatic or manual control of the image capture settings . <p> Panasonic Lumix LX7 and GoPro Hero 3 Black <p> When recording videos , I set the LX7 to manual mode so it wont auto-focus or automatically adjust light balance , and I record everything in full HD ( 1080p , 30 FPS ) . On the GoPro , I normally select 1080p at either 30 or 60 FPS ( depending on the situation and lighting ) . If I am outdoors , I 'll have Protune on , if I 'm indoors I usually have Protune off . <p> Camera settings are only part of the whole @ @ @ @ @ @ @ @ @ @ need proper lighting , and a backdrop that is n't distracting . I try to keep this setup very simple : I 'll place the camera on a tripod on the opposite side of my desk so that it faces me directly . Behind me , I 'll have a black muslin backdrop this helps everything else stand out from the background , and it does n't  reflect any light . Simple tip : Use a clothes steamer on the muslin backdrop , and the wrinkles will fall out pretty quickly . <p> Office/Studio Lighting and Backdrop <p> For lighting , I have darkening blinds that block out nearly all outside light this way you can control the lighting for your video shoots . Often shooting a video may be done over several days , and I cant rely on the weather and natural light to be consistent . With the room darkened , I normally use a single light source above and slightly off to the side of the camera . I try to find an angle that lights me up from the front , but does not reflect in my glasses @ @ @ @ @ @ @ @ @ @ so its not reflecting in my glasses , or dramatically altering the the lighting . <p> Once the lighting has been set , I set up the camera and adjust zoom , focus , and aperture ( exposure ) where I want it for the current video . <p> Framing The Shot <h> Audio Capture <p> Great video is only half of the equation Without clear audio , the videos are n't  nearly as good , and nobody wants to listen to bad audio . I started off using the built-in mics on my cameras , but quickly learned that the internal microphones werent going to cut it . For all of my recent videos Iver started using a Zoom H4N digital audio recorder . The Zoom enables high quality stereo recording . Its very easy to use , and the recording quality is fantastic now if only I could get those birds to stop chirping outside of my window . <p> Zoom H4N Digital Audio Recorder <h> Editing &amp; Post-Production <p> Capturing content is the first part of the process . The second part is editing everything together . @ @ @ @ @ @ @ @ @ @ of the creative tools Adobe has to offer . Most of my video editing is done with- Premiere Pro . This includes clipping &amp; sequencing , color correction , effects , etc All of my audio production is done with Adobe Audition- this includes sound cleanup , and mastering . For graphics , I use Photoshop and Illustrator , depending on the format and content . If you want to insert animations , you can even use Flash Pro as an animation platform , export as video , and pull it into your Premiere project . <p> Video Editing in Adobe Premiere Pro CC <p> Once everything is how I want it , I 'll export to H.264 format ( for the web ) , upload to YouTube , and then start syndicating it however possible/necessary . Normally , its just pulling in a YoutTube- video into a blog post . <p> I think this camera is awesome ! Its really pretty amazing While it might seem " bare bones " without a view finder or zoom capabilities , it is very rugged , it takes great still photos ( 12MP @ @ @ @ @ @ @ @ @ @ FPS all the way to 4K at 15 FPS ) . It excels in bright light and action shots , and is a very versatile and impressive camera . You can also use the GoPro app- on your iOS , Android , or Windows Phone device as a remote viewfinder and controller , which is very cool. - Though the GoPro is not without its quirks/annoyances . <h> Bottom Line : <p> Awesome camera. - I want multiple of them . For video , either 1080p or 720p video is more than adequate for most circumstances . Audio capture is okay , but its better if you have an external mic and can sync audio in post-production . While 4K video is seriously awesome , it kills the battery very quickly , has HUGE file sizes , and is n't my primary need . Still images look great , although there is significant fisheye distortion from the 170 degree lens . While this looks fantastic in a lot of cases , sometimes we do n't  want it Luckily we can get rid of the fisheye distortion with Photoshop ! <p> The @ @ @ @ @ @ @ @ @ @ very good , and it is very portable and compact ( though , you may end up with more accessories that take up more space ) . <h> Notes : <p> Here are some notes about the camera , mostly focused on using it as a lightweight travel camera : <h> Images &amp; Videos Produced : <p> Very good quality <p> Images and videos look great in decent light , but the camera does n't  function well in very low-light situations . <h> Pros : <p> Wide angle lens great for capturing landscapes or presentation stage/projector ( You only need the camera to be about 6-10 feet away for a decent stage coverage area ) <p> Great in both indoor and outdoor environments <p> Small , very portable <h> Cons : <p> No view finder/lcd screen , though you can use a mobile device as a viewfinder <p> No wall charger included , however you can charge it by plugging the USB adapter into an iPad ( or other USB ) power supply <p> Relatively short battery life when capturing video <p> Video files are huge ( though it is @ @ @ @ @ @ @ @ @ @ &amp; Annoyances : <p> If you unplug it from your computer without ejecting , it will lock up the camera . You have to remove the memory card , then remove the battery and reinstall both before the camera will reboot . <p> You can record video while plugged into a power source However , you MUST start recording before you plug it in , otherwise it wo n't let you record while plugged into computer or wall . <p> If you improperly shut down the camera , the most recently recorded video may be corrupt . This can happen if the camera is dropped or jarred from a crash . If this happens , just re-insert the memory card , then reboot the camera . It will go into recover mode , and fix the corrupted file . If auto-recover does n't  restore your video , you *might* be able to recover the video from the memory card , but there is no guarantee this has happened to me . <p> The camera does not automatically turn off . You must manually power off the camera when not in use @ @ @ @ @ @ @ @ @ @ n't  power off , however you can also change this behavior in the GoPro settings . <p> With the battery pack connected and the " frame " mount connected , you might get a " hum " in the audio you can reduce this in Adobe Audition , but if you have an external mic , then you do n't  have to worry . I 'm still not 100% sure if it was the camera/backpack , or something else in the room that caused the audio interference . <p> Let the camera finish whatever it is doing when you are done recording . Do n't  force a power-off , otherwise you might lose whatever is in memory buffer on the camera I know this from experience &amp; recently lost about 20 minutes of video . <p> Opening the waterproof case for the first time is trickier than you might imagine . Once you know how to do it , its really easy . <p> Although the camera is supposed to support 64 Gig memory cards , I strongly advise not using anything greater than 32 Gigs . I ran into some @ @ @ @ @ @ @ @ @ @ card to the point that it was unrecognizable by a computer or by the camera . After scouring the web and contacting technical support , I had to replace the card . It turns out the particular camera firmware version was not compatible with the file format for my memory card . Since switching to 32 Gig cards , I have n't had any problems . <h> Charge Time : <p> Approximately 2 hours for a full charge from completely dead . This will take longer with battery BacPac yes , this was very unscientific , as were my battery tests <h> Battery Tests : <p> 720p , 60FPS , WiFi Disabled : Approx 1 hour ( 2 iterations ) <p> 720p , 60FPS , WiFi Disabled , with battery BacPac : Approx 90 mins ( though the BacPac was not fully charged ) it is advertised to double battery life . <p> These are more than sufficient battery times for my normal circumstances . <h> Essential Accessories : <p> Micro SD Memory Card Required , it does not come with one . I reccommend 32 Gig cards , and get @ @ @ @ @ @ @ @ @ @ out . <p> Tripod mount- ( if you want it on a tripod . ) There is only the custom GoPro mount in the package , however you can use part of the package display as an inexpensive stand ) <p> " The Frame " mount- The only mounting option that comes with it is the waterproof enclosure . This is n't great for presentations b/c you get better audio quality by just having the camera exposed . This also comes with a much needed lens cap . <p> Battery BacPac- -- Extend the life of the GoPro with a secondary battery , although if you use this , you cant attach the LCD back . <p> Some sort of bag to put everying in . There are a lot of parts and accessories , although not all of them are essential to carry around with you . <h> Non-essential accessories : <p> These would be nice to have , but you can get by without them : <p> Wall Charger- I use an iPad power supply just plug in the USB adapter to the iPad " wall wart " . @ @ @ @ @ @ @ @ @ @ these , I was fine just pointing it and shooting . However , you can not use this with the battery BacPac . <h> Side by Side Comparison : <p> Here is a sample where I was recording one of Raymond Camdens presentations . Compared with a Sony Bloggie ( at exact same distance ) . Bloggie feels " closer " , but has worse video quality , worse in poor lighting , and does n't  capture the full experience of the presentation . This is without any color correction or post processing other than resizing the image : <h> Other Sample Images : <h> Conclusion : <p> My wishlist for the GoPro is that I wish it took higher than 12 MP photos . - 12 MP is actually great , but *I NEEDZ MOAR P1XELS* . I also wish there was a way to interchange the lens with a non-fisheye lens , but otherwise I love this camera . Yes , I would buy another one in a hearbeat ! <p> Some days you are just meant to be creative I think yesterday was that day for me @ @ @ @ @ @ @ @ @ @ , and it sparked a wave of creativity within me . I discovered Eriks work via Adobe Max this year his presentation was awesome and very inspiring . Seriously , do yourself a favor and go watch it . <p> I digress I 've been doing a lot of aerialphotography lately , and after seeing Eriks latest composition I thought to myself : why not take some of my aerial photos and start altering reality ? My immediate idea was to create a surrealistic composition where you are looking down through a crystal ball . Next thing you know , this happened <p> Here are a few iterations from the creative spark . I think v.2 ( without the flames is my favorite ) : <p> Crystal Ball v.1Crystal Ball v.2Crystal Ball v.2 with Flames <p> Attribution : <p> The aerial image was mine , captured with a DJI Phantom and GoPro- camera . Check out the original image on Flickr . <p> Earlier this week I did a post on the new lens profiles for GoPro cameras in Camera Raw 8.2 and Lightroom 5.2 . I mentioned in that post @ @ @ @ @ @ @ @ @ @ ) that you can also reduce the fisheye effect using Photoshop CCs Adaptive Wide Angle filter . So I decided to put together a quick post showing the output of the Lens Correction and Adaptive Wide Angle filters side-by-side on the same image . <p> Check out the video see how to apply both techniques , and compare the output . <p> You can check out the final images here ( shrunk to 600+450 for the blog ) : <p> Do you like keeping up with the latest news and updates from Adobe ? Do you like reading the latest posts and articles from all of the Adobe Evangelists ? Do you like seeing featured/highlighted content from Adobe ? Do you like reading your content onFlipboard ? <p> The- Adobe Evangelists Magazine- is a manually curated aggregation of all of our blogs , plus articles and posts that we find interesting and applicable , all combined into one great resource . All links point to the original source content , not a copy . The intent is to provide a one-stop-shop for Adobe Evangelists goodness . 
@@106848781 @2248781/ <h> Monthly Archives : February 2015 <p> Last week I gave a presentation to the NYC Bluemix Meetup Group on IBM MobileFirst for Bluemix . Not familiar with the branding and have no idea what that means ? - It is a mobile backend as a service , which gives you analytics , remote logging , user auth , data persistence &amp; offline synch , push notification management , and more for your mobile applications . - Yes , as a service you can create a Bluemix account today for free and start building your apps very quickly and very efficiently. - - No problem if you werent able to make it to the meetup. - I recorded my session which you can check out in the embedded video below . <p> I know the video quality is n't fantastic , but its the best I had at the time . - ( I almost always have a GoPro with me. ) - If you want to see the code that makes all of this work in much , much more detail , check out my post on Getting Started @ @ @ @ @ @ @ @ @ @ and more . - Enjoy ! <p> This post specifically covers- native iOS , though there are also Android and hybrid options available . This should have everything you need to get started . It covers all aspects- from creating the app , to updating the back end , to leveraging Cloudant storage , push notifications , and monitoring &amp; logging . <p> So , without further ado , let 's get started <h> Part 1 : Getting Started with Bluemix Mobile Services <p> In this first video I show how to create a new mobile app on Bluemix , connect to the cloud app instance , and implement- remote logging from the client application . This process is covered in more detail in the Getting Started docs , but below- are the basics from my experience . <p> Youll- first need to- sign into your Bluemix account . If you do n't  already have one , you can create a trial account- for free . Once you 're signed in , you just need to create a new mobile app instance . <p> The process is very simple , and there @ @ @ @ @ @ @ @ @ @ first thing that you need to do is create a new app by clicking the big " Create an App " button on your bluemix dashboard . <p> Create a new app from IBM Bluemix Dashboard <p> Next , select which kind of app you 're going to create . For MBaaS , you 'll want to select the " Mobile " option . <p> Select the type of app <p> Next you 'll need to select your platform target . You can choose either " iOS , Android , Hybrid " , or the " iOS 8 beta " target . In this case I chose the iOS 8 beta , but the process is similar for both targets. - Hybrid apps are built leveraging the Apache Cordova- container . <p> Select your platform target <p> Next , just specify an app name and click " Finish " . <p> Give your app a name <p> Once your app is created , you will be presented with instructions how to connect the app in Xcode . I 'll get to that in a moment <p> Now that- your app has been created , @ @ @ @ @ @ @ @ @ @ . This app will consist of several components : a Node.js back-end instance , a Cloudant NoSQL database instance , an Advanced Mobile Access instance , and a Push instance . The Advanced Mobile Access component provides you with app analytics , user auth management , remote logging , and more . The Push component gives you the ability to manage and send push notifications ( either manually , or with a rest-based API ) . <p> You app has been created here are the components and the activity <p> Once your app has been created , you will need to setup the mobile app to connect to Bluemix to consume the services . Again , this is a very straightforward process . <p> The next step is to register your client application . Once your app is created , you will be presented with a screen to do this . If you do n't  complete it right away , you can always come back later and register an application . You 'll need to specify the Bundle I 'd and version of your app , then you can setup any authentication @ @ @ @ @ @ @ @ @ @ bundle I 'd and version <p> Once your app has been registered , you need to configure Xcode . Youll first need to create a new project in Xcode . There are two options for configuring your Xcode project : 1 ) automated installation using CocoaPods , or 2 ) manual installation . I used the CocoaPods installation simply because it is easier and manages dependencies for you . <p> If you are n't  familiar with CocoaPods , it is much like NPM CocoaPods is a dependency manager for Cocoa projects . It helps you configure- the Bluemix libraries and manages dependencies for you . <p> If you 've got Xcode up and running , then close it and install CocoaPods , if you do n't  already have it . Next open up a terminal/command prompt , go to the directory that contains your Xcode project and initialize CocoaPods- using the " setup " - command : <p> pod setup <p> This will create a new file called " podfile " . Open this file in any text editor and paste the following ( note : you can remove any lines that @ @ @ @ @ @ @ @ @ @ source ' https : **32;2478;TOOLONG ' # Copy the following list as is and # remove the dependencies you do not need pod ' IMFCore ' pod ' IMFGoogleAuthentication ' pod ' **25;2512;TOOLONG ' pod ' IMFURLProtocol ' pod ' IMFPush ' pod ' CloudantToolkit ' <p> Save the changes to the " podfile " file , and close the text editor . Then go back to your command promprt/terminal- - and run the installation process : <p> pod install <p> Your project will be configured , and all dependencies will be downloaded automatically . Once this is complete , open up the newly created . xcworkspace file ( Xcode Workspace ) . <p> You have to initialize the Bluemix inside of your application to connect to the cloud service to be able to take advantage of any Bluemix features ( logging , data access , auth , etc ) . The best place to put this is inside of your AppDelegate.m- class- application **29;2539;TOOLONG method because it is the first code that will be run within your application : <p> One of the first features I wanted to take @ @ @ @ @ @ @ @ @ @ can do this using the IMFLogger class , in much the same fashion as you do with OCLogger in MobileFirst Foundation server . Once great feature that requires almost no- additional configuration is the- **25;2570;TOOLONG method , which automatically configures the Advanced Mobile Access component to collect information for all app crashes . <p> Next , launch your app in the iOS simulator , or on a device , and you 'll see everything come together . Log into your Bluemix dashboard , and you 'll be able to monitor app analytics and remote logs . <p> Note : If you experience any issues connecting to the Bluemix mobile app from within the iOS simulator , just clear the iOS Simulator by going to the menu command " iOS Simulator -&gt; Reset Content and Settings " , and everything should connect properly the next time you launch the app . <h> Part 2 : Configuring the Node.js Backend <p> In the next video , I demonstrate how to- grab the code for the backend Node.js application , create a git repository on IBM JazzHub , then pull the code for local development @ @ @ @ @ @ @ @ @ @ an " add git " link under the app name . Using this link , you can create a git repository for the backend code . <p> Add a git repository <p> Once your git repo has been created , you can check out the code using any Git client ( I used the CLI ) . You 'll need to use the " npm install " command to pull down all the app dependencies . The biggest thing you need to know is that it uses express.js for the web application framework. - - You can make any changes that you want , and they will be automatically deployed to your Bluemix server instance upon commit . Yes , this workflow is also configurable b/c this process may not work for everyone . <p> One other thing that you will need to watch out for if you are doing local development : You will want to wrap the following code on line 6 in a try/catch block , otherwise you will hit errors in the local environment which will prevent your app from launching locally : <h> Part 3 : @ @ @ @ @ @ @ @ @ @ applications is the Cloudant NoSQL database . The Cloudant NoSQL database is a powerful solution that gives you remote storage , querrying , and client-side- data storage mechanisms with automatic online/offline synchronization , all with monitoring/analytics capabilities . <p> By default , objects within the Cloudant data store are treated as generic objects ( over-simplification : think of it is an extremely powerful JSON store in the cloud ) . However you can also serialize your objects to strong data types within the client code configuration . <p> In your AppDelegate class- application **29;2597;TOOLONG method , you 'll also want to initialize the IMFDataManager class , which is the class used for interacting with all Cloudant data operations . <p> IMFDataManager *manager = IMFDataManager sharedInstance ; <p> In my sample , I setup the database manually with open permissions , but you 'll probably want something more secure . Once your database is created , you can create indexes , search for data , create data , etc <p> In the following code , I create a search index and query for data from the remote Cloudant database . You really only @ @ @ @ @ @ @ @ @ @ exist . You can do this either through the mobile app code , or manually through the Cloudant databases web interface . I did this inline in the following code , just for the sake of simplicity : <h> Part 4 : Push Notifications <p> The IBM Bluemix mobile services app also contains a component for managing push notifications within your mobile applications . With this service , you can send push notifications to a specific device , a group of devices using tags , or all devices , and you can send push notifications either manually via the web interface , or as part of an automated workflow using the REST API . <h> Part 5 : Monitoring and Logging <p> Did I- mention that every action that you perform through Bluemix Mobile Services can be monitored ? Analytics are available for the Advanced Mobile Access component , the Cloudant NoSQL data store , and the Push Notifications service . In addition , you also have remote collection of client logs and crash reports . This provides- - unparalleled insight into- the health of your applications . 
@@106848783 @2248783/ <p> Node.js is an incredible tool for rapidly building highly performant and scalable back end systems , and you develop it using a familiar core language that most front-end developers are already accustomed to , JavaScript. - This acquisition is positioned to greatly enhance Node.js in the enterprise , and StrongLoops offerings will be integrated into IBM Bluemix , IBM MobileFirst , and WebSphere . <p> Even though the acquisition is still " hot off of the presses " , - you can start using these tools together- today : <p> If you have n't heard about StrongLoop 's LoopBack framework , it enables you to easily connect and expose your data as REST services . It provides the ability to visually create data models in a graphical ( or command line ) interface , which are used to automatically generate REST APIs " thus generating CRUD operations for your REST services tier , without having to write any code . <p> Why is this important ? <p> It makes API development easier and drastically reduces time from concept to implementation . - If you have n't yet looked at the @ @ @ @ @ @ @ @ @ @ - You can build API layers for your apps literally in minutes . - Check out the video below for a quick introduction : <p> Again , be sure to check out these posts that detail the integration steps so you can start using these tools together today : <p> That title get your attention ? - Yes , it really read " Adaptive- mobile- apps that- change based on personal context " with near real-time rules application , without much extra development effort . - If that sounds interesting to you , or like a product you might want to use within your own apps , then you might want to check out this site where you can get involved in the products development : - http : **35;2628;TOOLONG <p> IBM is looking for your input on creating these types of mobile app experiences. - User experiences within a single app that can be- dramatically different per user based on location , past behavior , profile information , social media activity , - and so much more . - With this behavior being driven by configurable rules that can @ @ @ @ @ @ @ @ @ @ . <h> How it works for your customer <p> Consider this scenario : <p> Jon and Andrea download the mobile app for S&amp;W , a retailer known for its attention to providing great customer service . Over the next month , Jon and Andrea use the app to browse and discover content and merchandise differently . <p> Jon primarily navigates to sports related content for his favorite teams to find gear and clothes for travel to his favorite team 's games . Andrea scours the app for sales and fashion trends and usually ends up following her favorite designers . <p> Andrea and Jon go to a baseball game together . She 's never enjoyed watching it , so she opens up the S&amp;W app to entertain herself , and her app 's navigation quickly steers her through Spring fashion articles . <p> Jon however , wants to replace the hat he 's worn the last three times the team lost , and since he 's in the stadium , his S&amp;W app opens right up to the team 's gear page . The app knows he 's out of @ @ @ @ @ @ @ @ @ @ store . <h> How it works for the dev team <p> Consider another scenario : <p> One of the developers on the team , George , sets up the system and application . He then gives access to Janet who is responsible for the customer experience . <p> Janet writes rules defining how the application could adapt and become more personalized based on inputs like , social media , geolocation , app usage , or customer information data . <p> Once Janet has built out her rules , she simply hits Submit ' and can immediately see her clever interactions reflected in the mobile application without having to involve the development team . <p> Analytics let Janet know which adaptations are working best , and helps her find new opportunities to optimize the app 's user experience . <p> Were not talking about a content management system , or translation based on locale , instead a rules-driven product that can adapt literally every aspect of your app : - customize the user interface , enable or disable different features , customized messaging and notifications , and much more , all @ @ @ @ @ @ @ @ @ @ be used to present- contextually relevant information , drive adoption , provide more/less data depending on your physical context , and so much more . <p> It wont be tied to a specific UI framework , wont be tied to a specific content management system , is n't attempting to re-create Google Now or Apple Proactive Assistance . - Rather , a set of tools and a rules engine that enable you to customize and tailor the app experience to the individual user . <p> Last week I had the opportunity to present to a great audience at- the- MoDev DC meetup group on " Smarter Apps with Cognitive Computing " . - In this session I focused on how you can create a voice-driven experience in your mobile apps . I gave an introduction to IBM Bluemix and IBM Watson services ( particularly the Watson language services ) , and demonstrated how you can integrate them into your native iOS apps . I also covered IBM MobileFirst for operational analytics and- remote logging to provide insight into your apps performance once it goes live . - Check out a recording @ @ @ @ @ @ @ @ @ @ Back in February I had the opportunity to present " Enabling the Next Generation of Apps with IBM MobileFirst " at the DevNexus developer conference in Atlanta . - It was a great event , packed with lots of useful content . - Luckily for everyone who was n't able to attend , the organizers recorded most of the sessions which have just- been made available on Youtube . <p> here 's the session Description : Once your app goes live in the app store you will have just entered into an iterative cycle of updates , improvements , and releases . Each successively building on features ( and defects ) from previous versions . IBM MobileFirst Foundation gives you the tools you need to manage every aspect of this cycle , so you can deliver the best possible product to your end user . In this session , we 'll cover the process of integrating a native iOS application with IBM MobileFirst Foundation to leverage all of the capabilities the platform has to offer . <p> If you think that voice-driven apps are too complicated , or out of your reach @ @ @ @ @ @ @ @ @ @ are not ! Last week , IBM elevated- several IBM Watson voice services from Beta to General Availability that means you can use them reliably in your own systems too ! <p> Let 's examine the two parts of the system , and see what solutions IBM has available right now for you to take advantage of <p> Transcribe audible signal to text transcript <p> Part one of this equation is converting the audible signal into text that can be parsed and acted upon . The IBM Speech to Text service fits this bill perfectly , and can be called from any app platform that supports REST services which means just about anything . It could be from the browser , it could be from the desktop , and it could be from a native mobile app . The Watson STT service is very easy to use , you simply post a request to the REST API containing an audio file , and the service will return to you a text transcript based upon what it is able to analyze from the audio file . With this API you do n't  @ @ @ @ @ @ @ @ @ @ your own no concern for accents , etc Let Watson do the heavy lifting for you . <p> Perform a system action by parsing text transcript <p> This one is perhaps not quite as simple because it is entirely subjective , and depends upon what you/your app is trying to do . You can parse the text transcript on your own , searching for actionable keywords , or you can leverage something like the IBM Watson Q&amp;A service , which enables natural language search queries to Watson data corpora . <p> Riding on the heels of the Watson language services promotion , I put together a sample application that enables a voice-driven app- experience on the iPhone , powered by both the Speech To Text and Watson Question &amp; Answer services , and have made the mobile app and Node.js middleware source code available on github . <p> The app communicates to the Speech to Text and Question &amp; Answer services through the Node.js middelware tier , and connects directly to the Advanced Mobile Access service to provide operational analytics ( usage , devices , network utilization ) and remote @ @ @ @ @ @ @ @ @ @ . <p> For the Speech To Text service , the app records audio from the local device , and sends a WAV file to the Node.js in a HTTP post request . The Node.js tier then delegates to the Speech To Text service to provide transcription capabilities . The Node.js tier then formats the respons JSON object and returns the query to the mobile app . <p> For the QA service , the app makes an HTTP GET request ( containing the query string ) to the Node.js server , which delegates to the Watson QA natural language processing service to return search results . The Node.js tier then formats the respons JSON object and returns the query to the mobile app . <p> The general flow between these systems is shown in the graphic below : <p> IBM Watson Speech QA for iOS Logic Flow <h> Code Explained <p> The code for this example is really in 2- main areas : The client side integration in the mobile app ( Objective-C , but could also be done in Swift ) , and the application server/middleware implemented in Node.js . @ @ @ @ @ @ @ @ @ @ the- Watson Node.js Wrapper , which enables you- to- easily instantiate Watson services in- just a few short lines of code <p> The credentials come from your Bluemix environment configuration , then you just create instances of whichever services that you want to consume . <p> I implemented two methods in the Node.js application tier . The first accepts the audio input from the mobile client as an attachment to a HTTP POST request and returns a transcript from the Speech To Text- service : <p> Note : I am using the free/open Watson Healthcare data set . However- the Watson QA service can handle other data sets- these- require an engagement with IBM to train the Watson service to understand the desired data sets . <h> Native iOS Objective C <p> On the mobile side- were working with a native iOS application . My code is written in Objective C , however you could also implement this using Swift . I wont go into complete line-by-line code here for the sake of brevity , but you can access the client side code in the ViewController.m file . In particular @ @ @ @ @ @ @ @ @ @ <p> You can see the flow of the application within the image below : <p> App Flow : User speaks , transcript displayed , results displayed <p> The native mobile app first captures audio input from devices microphone . This is then sent to the Node.js servers /transcribe method- as an attachment to- a HTTP POST request- ( postToServer method on line 191 ) . On the server side this delegates to the Speech To Test service as described above . Once the result is received on the client , the transcribed text is displayed in the UI and then a request is made to the QA service . <p> In the requestQA method , the mobile app makes a HTTP GET request to the Node.js apps /ask method ( as shown on line 257 ) . The Node.js app delegates to the Watson QA service as shown above . Once the results are returned to the client they are displayed within a standard UITableView in the native app . <h> MobileFirst Advanced Mobile Access <p> A few other things you may notice if you decide to peruse the native @ @ @ @ @ @ @ @ @ @ to IMFClient , IMFAnalytics , and OCLogger classes . These enable operational analytics and log collection within the Advanced MobileAccess service . 
@@106848785 @2248785/ <h> URL Monitor <p> customcontact form=4 <h> URL Monitor <p> The URL Monitor application is a simple simple diagnostic URL endpoing monitor application for iOS , Android , and BlackBerry Playbook , built using Adobe Flex &amp; AIR , and has an adaptive user interface that changes view layout depending whether the application is being consumed on a phone or tablet device . <p> Simply enter a URL into the text box and add it to the list . A polling HTTP request will be made every 10 seconds to determine the availability of a given endpoint . HTTP codes 200 , 202 , 204 , 205 and 206 will be identified as a success with a green check . All other HTTP codes will indicate a problem as a red X. To remove a listing , simply perform a horizontal swipe on a given row to reveal a delete button . 
@@106848786 @2248786/ <h> Category Archives : Digital Publishing <p> I wanted to follow up my last post on 3D Parallax effects in HTML or Adobe DPS , I 've decided to release some of the other experiments that I 've been exploring with device motion in DPS publications . Check out the video below to see two new samples , and a corrected version of the strawberries example from my last post ( the plants were going the wrong way in the last post ) . <p> All three of these samples leverage the same basic technique for responding to device motion inside of a DPS publication . The motion-interactive components are implemented using HTML and JavaScript , and are included in publications as web content overlays . In JavaScript , it takes advantage of the ondevicemotion event handler to respond to the physical orientation of the device . <p> In all three of samples , the web content overlay is set to autoplay , with user interaction disabled . This way the HTML &amp; JavaScript automatically loads and the scripting is active , but it does n't  block interaction or gestures for @ @ @ @ @ @ @ @ @ @ Fit " so that HTML content scales appropriately between retina and non-retina devices . <h> Adobe San Francisco <p> The Adobe/inline content example is implemented in the same manner as the strawberries example . The large city image It is a two-layer composition created with Adobe Edge Animate . The foreground building and flag move independently from the background image . I used Photoshop to separate the content into layers and made them animate based on device orientation in the exact same fashion as the strawberries sample . All of the text and image content surrounding the cityscape panorama is laid out with InDesign . <h> AT&amp;T Park/San Francisco Giants <p> The AT&amp;T Park/San Francisco Giants example is implemented with basic HTML and JavaScript , no additional tools were used to create this interactive scenario . - The content on the left hand side was all laid out with InDesign . The content on the right side is the interactive HTML . <p> The image used in this example is a vertical panorama captured from a remote control helicopter . This image contains various perspectives that have been composited in @ @ @ @ @ @ @ @ @ @ match the perspectives in the image/viewport ; When the device is facing down , the image is looking down and when the device is vertical , the image faces forward . You can check out the vertical panorama image below . If you 're interested in creating a vertical panorama , be sure to- check out this tutorial from Russell Brown . <p> Vertical Panorama over AT&amp;T Park <p> The HTML and JavaScript used in this example is fairly minimal . The image is applied as the background of the root HTML &lt;body&gt; element , and the position of the background is shifted based upon the device motion event . This approach keeps the HTML DOM as flat and simple as possible . <p> A few weeks ago , a fellow Adobe colleague showed me a DPS publication that had an amazing design . All of the content looked great by itself , but what really made parts of it " pop " was that in certain areas there was a 3D parallax effect , which made it feel like you were looking into an image that had depth . You @ @ @ @ @ @ @ @ @ @ person , or around the corner of a building . <p> Heres what I mean on the surface the image looked static , but as I rotated it , elements shifted to give the illusion of depth . The background and foreground elements all moved at different rates : <p> 3D Parallax Effects on a Device <p> I thought this was an incredible example of added interactivity and immersive- experiences , and its not really that difficult to implement. - In fact , I put together this tutorial to show exactly how you can create these types of effects in your own compositions . <p> To create this kind of an effect , the first thing you need to do is break apart an image into layers note : - you may need to synthesize edges so that there is an overlap in all transparent areas . Then you need to add interactivity in HTML . Align those images so that their default state looks just like the still image , then move the images based upon the device orientation . I move the foreground one way , keep the @ @ @ @ @ @ @ @ @ @ background content the opposite direction ( all based upon which way you are rotating the mobile device ) . Since this is all HTML , you can take this content and use it on the web , or import it into Adobe InDesign to export a DPS digital publication . <h> Step 1 : Create Layered Images <p> You can either create your own layers , or break apart an existing image into layers so that each individual layer can be placed over top each other to form a seamless composition . In this case , I separated the strawberries , the rows of plants , my daughter , and the sky out to separate layers . <p> Update 1/7/2014 : I added logic to support both landscape and portrait orientation . <p> Be sure to add both of those JavaScript snippets inside of the creationComplete event for the Stage . - I also over-exaggerated the movement in the timeline. - I think it would look better with slightly less ( more subtle ) movement . <p> At this point , you could publish the composition and use it on @ @ @ @ @ @ @ @ @ @ fact , you can check it out here , just load it on an iPad and rotate the device to see the effect . However , please keep in mind that 1 ) I have n't added a preloader , 2 ) the assets are non-optimized and are all retina size , 3 ) I do n't  have it auto scaling for the viewport size , so it will only look right on a retina iPad , and 4 ) I have only tested this on an iPad no other devices . <p> Note : You could also do this without using Edge Animate , but you 'd have to hand code the HTML/JS for it . <h> Step 3 : Include in InDesign/DPS Composition <p> To include this in a DPS publication , all that you need to do is export an Animate Deployment Package ( . oam file ) from Adobe Edge Animate . You can then just drag and drop this into InDesign for inclusion in a DPS publication . <p> If you are n't  already a member of Creative Cloud , join today to take advantage of all @ @ @ @ @ @ @ @ @ @ this I realized that the movement of the plants should actually be reversed . - If you view this link , you 'll see the updated motion- ( which looks more realistic ) , but I cant update the video that 's already been published . <p> Another request that I have gotten from some of our DPS customers is that theyd like to be able to implement gestures inside of the Edge Animate compositions that they are building for DPS publications . This includes double-tap gestures , swipe gestures , etc Out of the box , these gestures are n't  supported , but you can add them to any Edge Animate composition without too great of an effort . <p> Below is a quick video showing Edge Animate Compositions that are taking advantage of both double-tap and swipe gestures . - Note : I intended these to be used inside of DPS , but I show them in Safari on iOS. - These gestures override the default mobile browser behaviors . <p> As I mentioned above , this is n't something that is supported out of the box , but it is @ @ @ @ @ @ @ @ @ @ below are for the basic examples that I put in the video . - Both should work in desktop and mobile browsers : <p> For the double tap example , just perform a double tap/click gesture anywhere on the stage ( the image area ) , and the animation will start again from the beginning . - For the swipe gesture , just perform a horizontal swipe in either direction with either your finger , or the mouse . <h> Gestures With Hammer.js <p> I leveraged the hammer.js JavaScript library to handle gesture detection since these gestures are n't  supported by default . - Hammer.js also enables other gestures , like long taps , pinch , rotate , etc - However , I 'm only showing double tap and swipe . - You can read more about hammer.js using the following links : <p> I used this exact setup procedure in both the double-tap and swipe examples . <p> To include this library , I first downloaded the hammer.js file , and saved it inside of the " edgeincludes " folder . <p> Next , you have to disable the web @ @ @ @ @ @ @ @ @ @ in when double tapped . - You can disable the zoom on double tap by adding a viewport metadata tag inside of the Edge Animate projects html file . - Open your projects . html file ( in this case " DoubleTap.html " , and add the following line to the &lt;head&gt; <p> Next , we have to add the code inside of the Edge Animate composition to enable the gesture behavior. - The first thing you have to do is include the hammer.js library . - In this case , I wanted to add the gestures to the compositions stage , instead of a particular element . - So , right-click on the stage in the Edge Animate Editor , then select - the " Open Actions for Stage " menu option . <p> This will open the actions for the Stage instance . - Next , click on the " + " icon and select " creationcomplete " . - This will create a function that gets invoked once the Stage instance has been created at runtime . <p> In that function , first we need to import @ @ @ @ @ @ @ @ @ @ yepnope.js library , which is originally intended to detect if a browser includes a specific piece of functionality . - If not , then include a JS library so substitute that missing feature . - In this case , I am passing it a blank test to force it to include the hammer.js library . - The following function forces loading of the hammer.js library . - Once the library has been loaded into memory , it triggers the " init " function : <p> In the init function , we grab a reference to the stages element ( div ) , then use hammer.js to add our gesture event handlers : <p> Now , we need to start looking at the individual examples <h> Double Tap Gestures <p> In the double tap example , we have a simple timeline animation that plays sequentially . - At the end of the sequence the animation is stopped by a simple sym.stop() function call . - Heres a quick preview of the setup in Edge Animate : <p> To add the double tap gesture , all you have to do is add @ @ @ @ @ @ @ @ @ @ that event handler , were just calling sym.play(0) , which restarts playback from the beginning of the composition . The full code for the creationcomplete event is shown below . - This is all that is needed to add the double-tap gesture to the composition stage instance : <h> Swipe Gestures <p> In the swipe gestures example , we have a simple timeline animation that plays sequentially . - However , at the end of each slide transition , playback is stopped by a simple sym.stop() function call . - Whenever we perform a swipe action , were either just playing forward , or playing in reverse until the next slide animation stops . - Heres a quick preview of the setup in Edge Animate , note the stop points highlighted by the arrows : <p> To add the swipe gestures , all you have to do is add a hammer.js event for " swipeleft " or " swiperight " . - In those event handlers , were just calling sym.play() or sym.playReverse() , depending whether it was a left or right swipe . - These play actions progress to @ @ @ @ @ @ @ @ @ @ creationcomplete event is shown below . - This is all that is needed to add the swipe gesture to the composition stage instance : <p> With the swipe gestures , you can get some drag event conflicts on mobile devices . - If you run into this and you do not want the page to scroll , the scroll action can be prevented by capturing the touchstart event , and canceling the default behavior. - I did n't  add this , just because I wanted to keep this example very simple . <p> Lately Ive been spending a lot more time working with- Adobe Edge Animate , - Adobe InDesign , and Adobe DPS . If you are n't  familiar with these tools , Adobe Edge Animate is a tool that enables the creation of animated or interactive HTML content , Adobe InDesign is a desktop publishing design tool , and Adobe DPS is Adobes Digital Publishing Suite , which is used for creating digital publications from InDesign everything from digital magazines , catalogs , corporate publications , education , and more . <p> So , you might be wondering @ @ @ @ @ @ @ @ @ @ Well From Edge Animate you can export compositions into a . oam package , which can be imported directly into InDesign for use with a web content overlay . You can read more about this process on Adobe Developer Connection . <p> I was recently asked by a customer " does Edge Animate support 3D transforms ? " . Unfortunately , at this time 3D transforms are not supported in the timeline editor . However , you can add 3D transformations programmatically with JavaScript . Here are some examples showing how to integrate CSS3 3D transforms with Edge Animate compositions : <p> These can be great additions to the interactive experience , but I also wanted to share that you do n't  always need 3D transforms to add dimensionality to an interactive experience . By leveraging 2D translation , scaling , and opacity you can easily create interactive experiences that have a feeling of depth . <p> Let 's take a look at a quick example . The image below is from screenshots of an Edge Animate Composition that I put together . On the left-hand side there is an anatomical @ @ @ @ @ @ @ @ @ @ been broken out into separate layers , with emphasis placed on the topmost layer . <p> Just click or tap on the image to see an animation that transforms the illustration on the left to multi-layered cutaway on the right hand side . <p> So , while this animation does n't  leverage any actual three dimensional elements , it leverages those 2D transforms to visually create a sense of depth . here 's how it works : <p> First , there are 3 images . The bottom-most image shows the skeletal structure and body outline . The middle image shows parts of the digestive system , and the top-most image shows another layer of major organs . The top 2 images have transparency so that they do not completely hide content from the underlying layers . <p> The default state is that all of these images are aligned so that they appear as a single image . <p> Once you click/tap the image , a set of two-dimensional- animations take place providing a sense of depth and emphasizing the top layer . The underlying layers have both a scale and opacity @ @ @ @ @ @ @ @ @ @ opaque . The underlying layers also have a two dimensional ( top/left ) transform . In this example , I 've tried to align both the scale and top/left transforms to correspond with a 3D point of origin to the left side . <p> Edge Animate Anatomy Composition <p> This technique provides the illusion of three-dimensional depth , even though we are n't  actually performing any kind of translation , rotation , or deformation on a three dimensional coordinate system . AND this can be implemented- completely- with the timeline . So , you do n't  have to be a programmer to add a dimensional feeling to you Edge Animate compositions . This effect was achieved simply by using the timeline editor and visual workspace . <p> You can preview this animation in a new window , or download the full source using the links below : 
@@106848787 @2248787/ <p> First , navigate to- https : //developer.amazon.com/- and click on the " Amazon Appstore for Android " link . Once there , you will need to walk through the full registration process to create your account . <p> Amazon Appstore Landing Page <p> Once you click on the " Get Started " button , you will be guided through the Appstore registration process . Once that is complete , you will be directed to the Appstore developer portal home page . - From here , just click on " Add a New App " . <p> Amazon Appstore Home <p> Next , you will being the App upload wizard . - First , you will need to enter primary metadata for the application , including a title , form factor , supported languages , and contact information . - Once you have entered this information , click on the " Save " button . <p> Amazon Appstore App Metadata <p> Next , you need to enter merchandising information . - This includes the app category , keywords , a description , price , and release/availability dates . - @ @ @ @ @ @ @ @ @ @ click on the " Save " button to proceed . <p> Amazon Appstore App Merchandising Info <p> Next , you will have to specify content rating information . - Just fill out the information about your content , and click on the " Save " button to proceed . - I did n't  run into any content rating issues in the Amazon Appstore , like I did with the iOS App Store . <p> Amazon Appstore App Content Ratings <p> Next , upload multimedia that will be associated with the application . - This includes application icons ( note : they must match , event though this screenshot does n't  show it I was rejected b/c of this ) , and actual screenshots of the application . <p> Amazon Appstore App Multimedia <p> Scroll down to see more of the " Multimedia Content " form . - You will also be able to enter promotional images and promotional video assets for your application . - Once you have uploaded all necessary multimedia , click the " Done " button . <p> Amazon Appstore App Promotional Media <p> Now you are @ @ @ @ @ @ @ @ @ @ instructions for uploading an APK file . - Once it is uploaded you will see information about the file that was uploaded . <p> Amazon Appstore App Binary <p> Finally , click on the " Submit App " button to submit your application for approval . - There is an approval process for the Amazon Appstore similar to Apples , and my application was live in less than a week from submission . <p> Once you have successfully uploaded the application APK , you will be prompted with the details about your application . - Click " Save " to being entering application metadata information . <p> Android Market APK Uploaded <p> You will need to upload at least 2 screenshots of your application , as well as an application icon for use within the Android Market . <p> Android Market APK Metadata <p> Next , scroll down to the " Listing Details " area and begin entering metadata for your application . - This includes an application title and description , as well as release notes , classifications , and promotional text . <p> Android Market APK Metadata - @ @ @ @ @ @ @ @ @ @ Options " screen , where you can configure copy protection settings , content ratings , pricing , and available markets . <p> Once you have completed the form in its entirety , scroll back up to the top of the page and click on the " Save " button to save content , or " Publish " to save and publish the application to the Android Market . - Once you have successfully published it , your application will show up in your home dashboard , as shown below . <p> Android Market APK App Ready <p> That 's it ! - The Android Market makes it extremely easy to publish your application . - My application started showing up in the market after about 30 minutes from pressing the " Submit " button . <p> In this post , we will cover the workflow for delivering a solution from development in Flash Builder through packaging for various Android ecosystems . - In subsequent posts , this will be followed up with detail for delivering to each individual ecosystem . <p> You can start developing apps for Android using the debugging @ @ @ @ @ @ @ @ @ @ step in your development process is that you must test and evaluate your applications on a physical device ( preferably before you try to ship it ) . - Luckily for all of us , that is really easy with Flash Builder . <p> First , develop your mobile application . - When you are ready to test it on a device , simply go to the " Debug " menu and select " Debug Configurations " . <p> Make sure that your test device(s) are plugged in via USB , and then hit the " Debug " button . - Using this approach , you can deploy to any Android device that has USB Debugging enabled . - This applies to phones and tablets to enable USB debugging , you just need to go to Settings -&gt; Applications -&gt; Development , and then select the checkbox to enable USB debugging . - - You can also use this method to debug on a Nook Color device , however you must have it unlocked already via your Nook developer account . <p> Once you have developed and debugged your application @ @ @ @ @ @ @ @ @ @ consumption . - - The first thing you need to do is specify you applicaiton name , version and i 'd in your app.xml file . - Be sure that these are the final values , as the application i 'd will be the unique identifier for your application in all marketplaces and if you distribute the APK manually . <p> Application XML Configuration <p> Once your application configuration is finalized , you will be ready to actually export the project . - Right-click on your project in the package explorer and go to the " Export " option . - You can also perform the same action by going to the File -&gt; Export menu . <p> Export Menu Options <p> Next , select the " Release Build " export option , and click " Next " . <p> Export Release Build <p> Set your export folder , and a base filename for the output file(s) , be sure to select the target platform " Google Android " , and export as " Signed packages for each target platform " . - Click " Next " to setup your platform- signing- @ @ @ @ @ @ @ @ @ @ , you can use a purchased signing certificate , or use a self-signed certificate . - Just use the browser button to locate your signing certificate , or click " Create " to crate a certificate . <p> Export Release Build - Platform Signing Options <p> If you create your own certificate , the following window will be displayed . - Just enter a publisher name ( your name ) , a password , and a file location . - Next click " OK " and the certificate will be created and you will return to the Platform Signing Options screen . <p> Self Signed Certificate <p> Next , click " Finish " and your APK file will be generated . <p> Export Release Build - Platform Options <p> Now that you 've generated your APK binary file , you are ready to push to the various app stores Let 's take a look at this process for each ecosystem : <p> In this presentation , Adobe Evangelist Andrew Trice will walk through several applications utilizing Flex and AIR for mobile and will highlight application ecosystems , development processes , device considerations @ @ @ @ @ @ @ @ @ @ of multi-platform development using Adobes toolsets . <p> In this session , we will explore mobile application development using Flex and AIR for mobile , powered by ColdFusion backend systems . By the end of this session , you will know how to build natively installed iOS , Android , and BlackBerry Playbook applications , and you will be able to utilize your existing CF skills to power them . <p> here 's a little demo I put together that I 'm just too excited not to share Fellow Adobe Evangelist Christophe Coenraets has an awesome- Mobile Trader- application that highlights the power of LCCS collaboration and realtime information , using Flex for mobile . - I decided to see if that same collaboration example would work with the Stage3D data visualization component Ive been playing around with , and guess what : - it works fantastic . - Ive given it the nickname " project awesome " b/c it combines realtime collaboration ( audio , video &amp; data ) , Stage3D hardware accelerated graphics , and desktop to mobile paradigms. - Check it out below : <p> If you have n't already @ @ @ @ @ @ @ @ @ @ here : <p> Christophe has made the source of the Mobile Trader application freely available on github , so go download it and creating your own collaborative applications ! I know , you *really* want to see Stage3D out in the public I promise you , it is coming , and it will be enable you to build great apps &amp; games . <p> I almost forgot to mention : I wrote less than 200 lines of code to create project awesome ! - It uses the Away3D library , and LCCS the hard stuff is easy with Flex . 
@@106848788 @2248788/ <p> I recently gave a presentation at IBM Insight on Cognitive Computing in mobile apps . - I showed two apps : one that uses Watson natural language processing to perform search queries , and another that uses Watson translation and speech to text services to take text in one language , translate it to another language , then even- have the app play back the spoken audio in the translated language . - Its this second app that I want to highlight today . <p> In fact , it gets much cooler than that . - I had an idea : " What if we hook up an OCR ( optical character recognition ) engine to the translation services ? " That way , you can take a picture of something , extract the text , and translate it . - It turns out , its not that hard , and I was able to put together this sample app in just under two days . - Check out the video below to see it in action . <p> To be clear , I ended up using a @ @ @ @ @ @ @ @ @ @ . This is not based on any of the work IBM research is doing with OCR or natural scene OCR , and should not be confused with any IBM OCR work . - This is basic OCR and works best with dark text on a light background . <p> The Tesseract engine let 's you pass in an image , then handles the OCR operations , returning you a collection of words that it is able to extract from that image . - Once you have the text , you can do whatever you want from it . <p> So , here 's where Watson Developer Cloud Services come into play . First , I used the Watson Language Translation Service to perform the translation . - When using this service , I make a request to my- Node.js app running on IBM Bluemix ( IBMs cloud platform ) . - The Node.js app acts as a facade and delegates to- the Watson service for the actual translation . <p> You can check out a sample on the web here : <p> Translate english to : <p> On the mobile client , @ @ @ @ @ @ @ @ @ @ something with the response . The example below uses the IMFResourceRequest API to make a request to the server ( this can be done in either Objective C or Swift ) . IMFResourceRequest is the MobileFirst wrapper for networking requests that enables the MobileFirst/Mobile Client Access service to capture operational analytics for every request made by the app . <p> Once you receive the result from the server , then you can update the UI , make a request to the speech to text service , or pretty much anything else . <p> To generate audio using the Watson Text To Speech service , you can either use the Watson Speech SDK , or you can use the Node.js facade again to broker requests to the Watson Speech To Text Service . In this sample I used the Node.js facade to generate Flac audio , which I played in the native iOS app using the open source Origami Engine library that supports Flac audio formats . <p> You can preview audio generated using the Watson Text To Speech service using the embedded audio below . Note : In this sample @ @ @ @ @ @ @ @ @ @ work in browsers that support OGG . <p> On the native iOS client , I download the audio file and play it using the Origami Engine player . This could also be done with the Watson iOS SDK ( much easier ) , but I wrote this sample before the SDK was available . <p> Cognitive computing is all about augmenting the experience of the user , and enabling the users to perform their duties more efficiently and more effectively . The Watson language services enable any app to greater facilitate communication and broaden the reach of content across diverse user bases . You should definitely check them out to see how Watson services can benefit you . <h> MobileFirst <p> So , I mentioned that this app uses IBM MobileFirst offerings on Bluemix . In particular I am using the Mobile Client Access service to collect logs and operational analytics from the app . This let 's you capture logs and usage metrics for apps that are live " out in the wild " , providing insight into what people are using , how they 're using it , and the @ @ @ @ @ @ @ @ @ @ <h> Source <p> You can access the sample iOS client and Node.js code at https : **38;2665;TOOLONG . Setup instructions are available in the readme document . I intend on updating this app with some more translation use cases in the future , so be sure to check back ! <p> Last week I attended IBM Insight in Las Vegas . It was a great event , with tons of great information for attendees . I had- a few sessions on mobile applications . In particular , my dev@Insight session on Wearables powered by IBM MobileFirst was recorded . You can check it out here : <h> Key takeaways from the session : <p> Wearables are the most personal computing devices ever . Your users can use them to be notified of information , search/consume data , or even collect environmental data for reporting or actionable analysis . <p> Regardless of whether developing for a peripheral device like the Apple Watch or Microsoft Band , or a standalone device like Android Wear , you are developing an app that runs in an environment that mirrors that of a a native @ @ @ @ @ @ @ @ @ @ the same . You write native code , that uses standard protocols and common conventions to interact with the back-end . <p> Caveat to #1 : You user interface is much smaller . You should design the user interface and services to acomodate for the reduced amount of information that can be displayed . <p> You can share code across both the phone/tablet and watch/wearable experience ( depending on the target device ) . <p> Using IBM MobileFirst you can easily expose data , add authentication , and capture analytics for both the mobile and wearable solutions . <p> This post is inspired by all the comments Ive seen this week about JS in the enterprise . I would have never imagined- this 10- years ago , but JavaScript is now pretty much ubiquitous . Here are a few reasons why you need to paying attention to JavaScript if you are n't  already , and why you should definitely not- write it off . <p> First , I think one of the major reasons for JavaScripts ubiquity is that- JavaScript is approachable . It is relatively easy for beginners to @ @ @ @ @ @ @ @ @ @ build complex and reliable systems . <p> Second , why you need to pay attention , JavaScript is everywhere . <p> You can now use JavaScript to develop on virtually any platform : client side applications , server side logic , embedded chips/IoT devices , manage build scripts and dependencies , and more . <p> This does n't  mean you 'll use the exact same code in every case , rather that you can use the same skill set JavaScript Development to deliver solutions across multiple paradigms . <h> The Client Side <p> JavaScript can be used to power client side apps/user interfaces , and user interactions on numerous platforms and devices . <h> Web <p> Of course JavaScript powers the web , this is a given . JavaScript is the primary scripting language for all web browsers . I wo n't focus on this much b/c its already well known . <h> Mobile <p> JavaScript can also be used to power mobile applications that are natively installed on a device . <p> Apache Cordova/PhoneGap- You can build natively installed apps with web technology using PhoneGap or Cordova . PhoneGap is Adobes @ @ @ @ @ @ @ @ @ @ , they are basically the same thing . Your app runs within a webview on the mobile device , and you build your user interface the same way you you build a dynamic web application . Your user interface- is implemented in HTML , styled with CSS , and all interactivity is created with JavaScript . <p> React Native- JavaScript powered web apps do n't  just have to be inside of a a web view . The React Native framework gives developers the ability to write their application using JavaScript and declarative UI elements , and results in a native application running on the mobile device . The logic is interpreted JavaScript at runtime , but everything that the user- interacts with ( all UI elements ) is 100% native , providing a very high quality user experience , and it is now available for both iOS and Android applications . <p> Unity 3D You can even develop rich &amp; immersive mobile 3D simulation or gaming experience , entirely powered by JavaScript using the Unity 3D engine . **These can be web , desktop , or mobile , but is @ @ @ @ @ @ @ @ @ @ , desktop apps are not left out of the mix . Most desktop solutions fall into a category similar to Apache Cordova , where the end results is a web view that has access to lower level APIs , whose content is developed with web based technology . <h> The Server Side <p> Most obviously- Node.js a JavaScript runtime buit on Chromes V8 JavaScript Engine has made huge inroads into server side development and the enterprise . Node.js , powered by frameworks like express.js or loopback.io makes server side development and complex enterprise apps with JavaScript possible . <p> Here are some stats that- show the magnitude of growth and adoption for Node.js/npm.js alone . NPM- stats currently shows a total of- 186,946 packages available for download , 94,978,032 package downloads in the last day , and 2,451,734,737 package downloads in the last month . <p> This does n't  mean that JavaScript is the best language at everything . It- also does n't  meant that you can take a single piece of source- code and run it in every device/context imaginable . <p> It means that you can use your @ @ @ @ @ @ @ @ @ @ of device/context out there . Its not going to be write once , run everywhere , rather in the words of the React.js team : learn once , write everywhere . <p> That title get your attention ? - Yes , it really read " Adaptive- mobile- apps that- change based on personal context " with near real-time rules application , without much extra development effort . - If that sounds interesting to you , or like a product you might want to use within your own apps , then you might want to check out this site where you can get involved in the products development : - http : **35;2705;TOOLONG <p> IBM is looking for your input on creating these types of mobile app experiences. - User experiences within a single app that can be- dramatically different per user based on location , past behavior , profile information , social media activity , - and so much more . - With this behavior being driven by configurable rules that can be changed without redeploying an app to the app store . <h> How it works for your customer @ @ @ @ @ @ @ @ @ @ the mobile app for S&amp;W , a retailer known for its attention to providing great customer service . Over the next month , Jon and Andrea use the app to browse and discover content and merchandise differently . <p> Jon primarily navigates to sports related content for his favorite teams to find gear and clothes for travel to his favorite team 's games . Andrea scours the app for sales and fashion trends and usually ends up following her favorite designers . <p> Andrea and Jon go to a baseball game together . She 's never enjoyed watching it , so she opens up the S&amp;W app to entertain herself , and her app 's navigation quickly steers her through Spring fashion articles . <p> Jon however , wants to replace the hat he 's worn the last three times the team lost , and since he 's in the stadium , his S&amp;W app opens right up to the team 's gear page . The app knows he 's out of town and tells him how to get to an S&amp;W store . <h> How it works for the @ @ @ @ @ @ @ @ @ @ the developers on the team , George , sets up the system and application . He then gives access to Janet who is responsible for the customer experience . <p> Janet writes rules defining how the application could adapt and become more personalized based on inputs like , social media , geolocation , app usage , or customer information data . <p> Once Janet has built out her rules , she simply hits Submit ' and can immediately see her clever interactions reflected in the mobile application without having to involve the development team . <p> Analytics let Janet know which adaptations are working best , and helps her find new opportunities to optimize the app 's user experience . <p> Were not talking about a content management system , or translation based on locale , instead a rules-driven product that can adapt literally every aspect of your app : - customize the user interface , enable or disable different features , customized messaging and notifications , and much more , all variable based upon the user context . - This can be used to present- contextually relevant information , @ @ @ @ @ @ @ @ @ @ context , and so much more . <p> It wont be tied to a specific UI framework , wont be tied to a specific content management system , is n't attempting to re-create Google Now or Apple Proactive Assistance . - Rather , a set of tools and a rules engine that enable you to customize and tailor the app experience to the individual user . <p> Last week I had the opportunity to present to a great audience at- the- MoDev DC meetup group on " Smarter Apps with Cognitive Computing " . - In this session I focused on how you can create a voice-driven experience in your mobile apps . I gave an introduction to IBM Bluemix and IBM Watson services ( particularly the Watson language services ) , and demonstrated how you can integrate them into your native iOS apps . I also covered IBM MobileFirst for operational analytics and- remote logging to provide insight into your apps performance once it goes live . - Check out a recording of the complete presentation in the video below : 
@@106848789 @2248789/ <h> Stage3D &amp; Flex Demo w/ Source <p> Back in the summer , I was lucky enough to get my hands on some early builds of Stage3D for mobile . I built some simple examples , including basic geometric shapes and simple 3D bubble charts inside of mobile Flex/AIR applications . I have been asked numerous times for the source code , and Ive finally given in , and am sharing some source code . <p> I am not posting the full mobile application source code , since Stage3D for mobile is not yet available . However , I have ported the 3D bubble chart example to run in a Flex application targeting the desktop ( Flash Player 11 ) . The bubble chart example extends the concepts explored in the basic geometric shapes example . <p> Before you say " shoot , he did n't  give us the mobile code " , let me explain When I ported the code from the mobile project to the desktop Flex project , all I changed was code specific to the mobile Flex framework . I changed **34;2742;TOOLONG to &lt;s:Application&gt; and @ @ @ @ @ @ @ @ @ @ changed the list item renderers to Spark item renderers based on &lt;s:Group&gt; instead of mobile item renderers. - In the mobile item renderers , all my drawing logic was done using the ActionScript drawing API. - For simplicity in the port , I just used &lt;s:Rect&gt; to add the colored regions in the desktop variant . <p> That is all I changed ! - <p> The stage3D code between the desktop and mobile implementations is identical . - - You can see the desktop port in action in the video below : <p> The source code was intended to be exploratory at best I was simply experimenting with hardware accelerated content , and how it can be used within your applications . - There is one big " gotcha " that you will have to watch out for if you want Stage3D content within a Flex application Stage3D content shows up behind Flex content on the display list . - By default , Flex apps have a background color , and they will hide the Stage3D content . - If you want to display any Stage3D content within a Flex @ @ @ @ @ @ @ @ @ @ mobile ) , you must set the background alpha of the Flex application to zero ( 0 ) . - Otherwise you will pull out some hair trying to figure out why it does n't  show up . <p> The source code for the web/Flex port of this example is available at : <p> Stage3D is not available on mobile devices yet . Once it is available , yes , it will run on both iOS and Android . 123436 @qwx983436 <p> Correction : Stage3D is already available for the BlackBerry Playbook , and iOS and Android will be supported in the future . <p> Nauman <p> Stage3D is available but the read only vector array ( stage.stage3Ds ) is always null . This is the issue in the 3D development yet , but i am hopeful for some good solution by Adobe 123436 @qwx983436 <p> Stage3D is not yet enabled for mobile devices . This is why it is always null if you attempt to access it . <p> fox <p> Hi Andrew , you said Stage3d is not available in Android and IOS yet.but i saw your application @ @ @ @ @ @ @ @ @ @ BleckBerry ? 123436 @qwx983436 <p> That video shows a prerelease build of the Stage3D capability on Android . The device shown is a standard " off the shelf " Samsung Galaxy 10.1 . Stage3D is is not yet publicly available . <p> Tomdog <p> Why you can use Stage3D in mobile devices in this vedio ? 123436 @qwx983436 <p> That video shows a prerelease build of the Stage3D capability on Android . <p> Richard <p> I 'm using Flash Builder ver 4.6 and am trying to create a simple , bare-bones project using a Flex Mobile Project + away3D and am having the worst luck . I keep getting the following error ( even with direct ) : <p> Can not access a property or method of a null object reference . at LONG ... Files LONG ... at away3d.containers : : View3D/render()C : Program Files LONG ... <p> Is there any way you could post/send an example project that works with Flex + Mobile + Away3d ? <p> Thanks 123436 @qwx983436 <p> Stage3D is not enabled for mobile projects . It has not been publicly released yet . <p> Richard @ @ @ @ @ @ @ @ @ @ haven any ideas as to when it will be released ? <p> Thanks , Richard 123436 @qwx983436 <p> Sorry , the date is n't public yet All I can say is " soon " . <p> Patil <p> Hi , <p> Can you please tell me which version of Flex SDK and Away3D you are using to compile your demo app . <p> Thanks 123436 @qwx983436 <p> Patil , That uses the Flex 4.6 SDK with the latest version of Away3D . I did not keep track of the exact build number , but I am pretty sure it will work with the 4.0 beta version available at : http : //away3d.com/download/ <p> jack <p> May be you should put that in the post above . <p> " Stage3D is not enabled for mobile projects " 123436 @qwx983436 <p> Jack , that is mentioned in the post above " Stage3D for mobile is not yet available " <p> Basically I 'd like to render ANY Away3d primitive inside of a View/ViewNavigator control in a Mobile Flex app . <p> Thanks in advance for any help that you can provide ! ! ! @ @ @ @ @ @ @ @ @ @ works with Stage 3D . Make sure to set background alpha to 0 on ALL containers that may overlap the Stage3D area . This includes the root Flex application . Stage3D content shows up behind the display list , and any flex content has the potential to hide it . <p> Richard <p> I am currently having issues getting a flex mobile + away3d + air 3.2 project working on the iPhone . Since air 3.2 has been released , would it be possible to release/publish at least a barebones project with away3d in a tab/view or something similar ? Thanks for any help you can provide . <p> http : **28;2778;TOOLONG Pierre Chamberlain <p> Hi Andrew , <p> I was curious to know if you made ( or know of ) any other charting components to draw 2D Graphs in Stage3D ? I would be very interested to know how to pull off bezier curves in Stage3D and how they are drawn . <p> Thanks ! 123436 @qwx983436 <p> Sorry , I never pushed it much further than what I originally posted . I would think you could render @ @ @ @ @ @ @ @ @ @ from the Graphics class , and then use that bitmap as a texture in Stage3D . Otherwise , my guess is that you 'd have to create the geometry for a curve yourself , and have it rendered using AGAL or one of the open source frameworks . 
@@106848790 @2248790/ <p> I recently attended the IBM InterConnect conference , and it was great to be there presenting IBM MobileFirst and engaging with our clients and partners . - While there , I also took part in the " Ignite " presentation series . If you are n't  familar with Ignite , each presenter has 20 slides , with the slides auto-advancing every 15 seconds . That gives you 5 minutes to make your point , and in the session you get to see 5 or 6 presentations on different topics within an hour . - I really liked this format , so I re-recorded- it to share here <p> Here are all my sources for the charts/data. - I chose to use publicly accessible- data for everything . - If you have n't been paying attention to mobile usage numbers and adoption , its quite staggering , and definitely worth paying attention to . <p> Last week I gave a presentation to the NYC Bluemix Meetup Group on IBM MobileFirst for Bluemix . Not familiar with the branding and have no idea what that means ? - It is a @ @ @ @ @ @ @ @ @ @ , remote logging , user auth , data persistence &amp; offline synch , push notification management , and more for your mobile applications . - Yes , as a service you can create a Bluemix account today for free and start building your apps very quickly and very efficiently. - - No problem if you werent able to make it to the meetup. - I recorded my session which you can check out in the embedded video below . <p> I know the video quality is n't fantastic , but its the best I had at the time . - ( I almost always have a GoPro with me. ) - If you want to see the code that makes all of this work in much , much more detail , check out my post on Getting Started with Bluemix Mobile Services it has code , video tutorials and more . - Enjoy ! <p> This post specifically covers- native iOS , though there are also Android and hybrid options available . This should have everything you need to get started . It covers all aspects- from creating the app , @ @ @ @ @ @ @ @ @ @ , push notifications , and monitoring &amp; logging . <p> So , without further ado , let 's get started <h> Part 1 : Getting Started with Bluemix Mobile Services <p> In this first video I show how to create a new mobile app on Bluemix , connect to the cloud app instance , and implement- remote logging from the client application . This process is covered in more detail in the Getting Started docs , but below- are the basics from my experience . <p> Youll- first need to- sign into your Bluemix account . If you do n't  already have one , you can create a trial account- for free . Once you 're signed in , you just need to create a new mobile app instance . <p> The process is very simple , and there is a " wizard " to guide you . The first thing that you need to do is create a new app by clicking the big " Create an App " button on your bluemix dashboard . <p> Create a new app from IBM Bluemix Dashboard <p> Next , select which kind of @ @ @ @ @ @ @ @ @ @ want to select the " Mobile " option . <p> Select the type of app <p> Next you 'll need to select your platform target . You can choose either " iOS , Android , Hybrid " , or the " iOS 8 beta " target . In this case I chose the iOS 8 beta , but the process is similar for both targets. - Hybrid apps are built leveraging the Apache Cordova- container . <p> Select your platform target <p> Next , just specify an app name and click " Finish " . <p> Give your app a name <p> Once your app is created , you will be presented with instructions how to connect the app in Xcode . I 'll get to that in a moment <p> Now that- your app has been created , you 'll be able to see it on your Bluemix dashboard . This app will consist of several components : a Node.js back-end instance , a Cloudant NoSQL database instance , an Advanced Mobile Access instance , and a Push instance . The Advanced Mobile Access component provides you with app analytics , user @ @ @ @ @ @ @ @ @ @ Push component gives you the ability to manage and send push notifications ( either manually , or with a rest-based API ) . <p> You app has been created here are the components and the activity <p> Once your app has been created , you will need to setup the mobile app to connect to Bluemix to consume the services . Again , this is a very straightforward process . <p> The next step is to register your client application . Once your app is created , you will be presented with a screen to do this . If you do n't  complete it right away , you can always come back later and register an application . You 'll need to specify the Bundle I 'd and version of your app , then you can setup any authentication ( if you choose ) . <p> Register your apps bundle I 'd and version <p> Once your app has been registered , you need to configure Xcode . Youll first need to create a new project in Xcode . There are two options for configuring your Xcode project : 1 ) automated installation @ @ @ @ @ @ @ @ @ @ used the CocoaPods installation simply because it is easier and manages dependencies for you . <p> If you are n't  familiar with CocoaPods , it is much like NPM CocoaPods is a dependency manager for Cocoa projects . It helps you configure- the Bluemix libraries and manages dependencies for you . <p> If you 've got Xcode up and running , then close it and install CocoaPods , if you do n't  already have it . Next open up a terminal/command prompt , go to the directory that contains your Xcode project and initialize CocoaPods- using the " setup " - command : <p> pod setup <p> This will create a new file called " podfile " . Open this file in any text editor and paste the following ( note : you can remove any lines that you do n't  want to actually use ) : <p> source ' https : **32;2808;TOOLONG ' # Copy the following list as is and # remove the dependencies you do not need pod ' IMFCore ' pod ' IMFGoogleAuthentication ' pod ' **25;2842;TOOLONG ' pod ' IMFURLProtocol ' pod ' IMFPush ' pod @ @ @ @ @ @ @ @ @ @ podfile " file , and close the text editor . Then go back to your command promprt/terminal- - and run the installation process : <p> pod install <p> Your project will be configured , and all dependencies will be downloaded automatically . Once this is complete , open up the newly created . xcworkspace file ( Xcode Workspace ) . <p> You have to initialize the Bluemix inside of your application to connect to the cloud service to be able to take advantage of any Bluemix features ( logging , data access , auth , etc ) . The best place to put this is inside of your AppDelegate.m- class- application **29;2869;TOOLONG method because it is the first code that will be run within your application : <p> One of the first features I wanted to take advantage of was remote collection of client-side logs . You can do this using the IMFLogger class , in much the same fashion as you do with OCLogger in MobileFirst Foundation server . Once great feature that requires almost no- additional configuration is the- **25;2900;TOOLONG method , which automatically configures the Advanced Mobile @ @ @ @ @ @ @ @ @ @ <p> Next , launch your app in the iOS simulator , or on a device , and you 'll see everything come together . Log into your Bluemix dashboard , and you 'll be able to monitor app analytics and remote logs . <p> Note : If you experience any issues connecting to the Bluemix mobile app from within the iOS simulator , just clear the iOS Simulator by going to the menu command " iOS Simulator -&gt; Reset Content and Settings " , and everything should connect properly the next time you launch the app . <h> Part 2 : Configuring the Node.js Backend <p> In the next video , I demonstrate how to- grab the code for the backend Node.js application , create a git repository on IBM JazzHub , then pull the code for local development . <p> When the app is created , you 'll see an " add git " link under the app name . Using this link , you can create a git repository for the backend code . <p> Add a git repository <p> Once your git repo has been created , you can check @ @ @ @ @ @ @ @ @ @ the CLI ) . You 'll need to use the " npm install " command to pull down all the app dependencies . The biggest thing you need to know is that it uses express.js for the web application framework. - - You can make any changes that you want , and they will be automatically deployed to your Bluemix server instance upon commit . Yes , this workflow is also configurable b/c this process may not work for everyone . <p> One other thing that you will need to watch out for if you are doing local development : You will want to wrap the following code on line 6 in a try/catch block , otherwise you will hit errors in the local environment which will prevent your app from launching locally : <h> Part 3 : Consuming Data from Cloudant <p> Another part of Bluemix mobile applications is the Cloudant NoSQL database . The Cloudant NoSQL database is a powerful solution that gives you remote storage , querrying , and client-side- data storage mechanisms with automatic online/offline synchronization , all with monitoring/analytics capabilities . <p> By default , objects @ @ @ @ @ @ @ @ @ @ ( over-simplification : think of it is an extremely powerful JSON store in the cloud ) . However you can also serialize your objects to strong data types within the client code configuration . <p> In your AppDelegate class- application **29;2927;TOOLONG method , you 'll also want to initialize the IMFDataManager class , which is the class used for interacting with all Cloudant data operations . <p> IMFDataManager *manager = IMFDataManager sharedInstance ; <p> In my sample , I setup the database manually with open permissions , but you 'll probably want something more secure . Once your database is created , you can create indexes , search for data , create data , etc <p> In the following code , I create a search index and query for data from the remote Cloudant database . You really only need to create the index if it does n't  already exist . You can do this either through the mobile app code , or manually through the Cloudant databases web interface . I did this inline in the following code , just for the sake of simplicity : <h> Part 4 : Push @ @ @ @ @ @ @ @ @ @ a component for managing push notifications within your mobile applications . With this service , you can send push notifications to a specific device , a group of devices using tags , or all devices , and you can send push notifications either manually via the web interface , or as part of an automated workflow using the REST API . <h> Part 5 : Monitoring and Logging <p> Did I- mention that every action that you perform through Bluemix Mobile Services can be monitored ? Analytics are available for the Advanced Mobile Access component , the Cloudant NoSQL data store , and the Push Notifications service . In addition , you also have remote collection of client logs and crash reports . This provides- - unparalleled insight into- the health of your applications . <p> This is more than just " Cloud Services " which more generally refer to a scalable virtual cluster- of computing or storage resources . - Bluemix is IBMs suite of cloud service offerings , and covers lots of use cases : <p> Bluemix is an open-standards , cloud-based platform for building , managing , @ @ @ @ @ @ @ @ @ @ , mobile , big data , and smart devices . Capabilities include Java , mobile back-end development , and application monitoring , as well as features from ecosystem partners and open source " all provided as-a-service in the cloud . <p> Why is it a hot topic ? - MBaaS- enables growth of mobile applications- with seamless ( and virtually endless ) scalability , all without having to manage individual systems for the application server , database , identify management , push notifications , or platform-specific services . <p> Ive been writing a lot about IBM MobileFirst lately for a seamless API to deliver mobile apps to multiple platforms ; though it has been- in the context of an on-premise installation . - However , did you know that many of the exact same MobileFirst features are available as- MBaaS services on IBM Bluemix ? <p> Mobile Data The mobile data service includes- a- NOSQL database ( powered by IBM Cloudant ) , file storage- capabilities , and appropriate management and analytics features to measure the number of calls , storage usage , time/activity , and OS distribution . <p> @ @ @ @ @ @ @ @ @ @ push data to the right people at the right time on- either Apple APNS or Google GCM platforms all with a single API. - Notifications can be sent by either an app or backend system , and can be sent to a single device , or a group of devices based on their tags/subscriptions. - Of course , with appropriate analytics for monitoring activity , distribution , and engagement . <p> Many of these are the exact same features that you can host in your own on-premise IBM MobileFirst Platform Foundation server the difference is that you do n't  have to maintain the infrastructure . - You can scale as needed through the Bluemix cloud offering . 
@@106848791 @2248791/ <h> Monthly Archives : January 2017 <p> Its not every day that you get the opportunity to have your work showcased front and center on the main landing page for one of the largest companies in the world . Well , today is definitely my lucky day . I was interviewed last month about a drone-related project that I 've been working on that focuses on insurance use cases and safety/productivity improvement by using cognitive/artifical intelligence via IBM Watson . I knew it was going to be used for some marketing materials , but the last thing that I expected was to have my image right there on ibm.com . I see this as a tremendous honor , and am humbled by the opportunity and exposure . 
@@106848792 @2248792/ <p> In this post I 'd like to show a fairly simple application that I put together which shows off some of the rich capabilities for IBM MobileFirst for Bluemix that you get out of the box All with an absolute minimal amount of your own developer effort . - Bluemix , of course , being IBMs platform as a service offering . <p> GeoPix is a sample application leveraging IBM MobileFirst for Bluemix to capture data and images on a mobile device , persist that data locally ( offline ) , and replicate that data to the cloud . Since its built with IBM MobileFirst , we get lots of things out of the box , including operational analytics , user authentication , and much more . <p> ( full source code at the bottom of this post ) <p> Heres what the application currently- does : <p> User can take a picture or select an image from the device <p> App captures geographic location when the image is captured <p> App saves both the image and metadata to a local data store- on the device . <p> App @ @ @ @ @ @ @ @ @ @ store up to the remote store whenever the network is available <p> Oh yeah , cant forget , the user auth- is via Facebook <p> MobileFirst provides all the analytics we need . - Bluemix provides the cloud based server and Cloudant- NoSQL data store . <p> All captured data is available on a web based front-end powered by Node.js <p> In this sample I 'm using everything but the Push Notifications service . - Im- using user authentication , the Cloudant DB ( offline/local store and remote/cloud store ) , and the node.js backend. - You get the operational analytics automatically . <h> Capturing Images <p> Capturing images from the device is also very straightforward . - In the app I leverage Apples- UIImagePickerController- to allow the user to either upload an existing image or capture a new image . - See the presentImagePicker and- **29;2958;TOOLONG below . All of this standard practice using Apples developer SDK : <h> Persisting Data <p> If you notice in the- **29;2989;TOOLONG method above , there is a call to- the DataManagers saveImage withLocation method . This is where we save data locally and @ @ @ @ @ @ @ @ @ @ local data store up to the Cloudant NoSQL database . - This is powered by the iOS 8 Data service from Bluemix . <p> The first thing that we will need to do is initialize the local and remote data stores . Below you can see my init method from my DataManager class . In this , you can see the local data store is initialized , then the remote data store is initialized . If either data store already exists , the existing store will be used , otherwise it is created . <p> Once the data stores are created , you can see that the replicate method is invoked . - This starts up the replication process to automatically push changesfrom the local data store to the remote data store " in the cloud " . <p> Therefore , if you 're collecting data when the app is offline , then you have nothing to worry about . - All of the data will be stored locally and pushed up to the cloud whenever you 're back online all with no additional effort on your part . - When using @ @ @ @ @ @ @ @ @ @ start the replication process and let it do its thing fire and forget . <p> In my replicate function , I setup- CDTPushReplication for pushing changes to the remote data store . - You could also setup two-way replication to automatically pull new changes from the remote store . <p> Once weve setup the remote and local data stores and setup replication , we now are ready to save the data the were capturing within our app . <p> Next is my saveImage withLocation method . - Here you can see that it creates a new- **26;3020;TOOLONG object ( this is a generic object for the Cloudant NoSQL database ) , and populates it with the location data and timestamp. - It then creates a jpg image from the UIImage ( passed in from the UIImagePicker above ) and adds the jpg as an attachment to the document revision . - Once the document is created , it is saved to the local data store . - We then let replication take care of persisting this data to the back end . <p> If we want to query data from @ @ @ @ @ @ @ @ @ @ just use the performQuery method on the data store . Below you can see a method for retrieving data for all of the images in the local data store . <p> At this point we 've now captured an image , captured the geographic location , saved that data in our local offline store , and then use- replication to save that data up to the cloud whenever it is available . <p> AND <p> We did all of this without writing a single line of server-side logic . - Since this is built on top of MobileFirst for Bluemix , all the backend infrastructure is setup for us , and we get operational analytics to monitor everything that is happening . <p> With the operational analytics we get : <p> App usage <p> Active Devices <p> Network Usage <p> Authentications <p> Data Storage <p> Device Logs ( yes , complete debug/crash logs from devices out in the field ) <p> Push Notification Usage <h> Sharing on the web <p> Up until this point we have n't had to write any back-end code . However the mobile app boilerplate on Bluemix comes @ @ @ @ @ @ @ @ @ @ take advantage of it . <p> The Node.js back end comes preconfigured to leverage the- express.js- framework for building web applications . - I added the- jade template engine and Leaflet for web-mapping , and was able to crank this out ridiculously quickly . <p> The first thing we need to do is make sure - we have our configuration variables for accessing the Cloudant service from our node app. - These are environment vars that you get automatcilly if you 're running on Bluemix , but you need to set these for your local dev environment : <p> Next you 'll se the logic for querying the Cloudant data store and preparing the data for our UI templates . You can customize this however you want caching for performance , refactoring for abstraction , or whatever you want . All interactions with Cloudant are powered by the Cloudant Node.js Client <p> A while back I wrote about adding parallax effects to your HTML/JS experiences to make them feel a bit richer and closer to a native experience . - I 've just added this subtle ( key word *subtle* ) effect to @ @ @ @ @ @ @ @ @ @ to share here . <p> If you are wondering what I am talking about with " parallax effects " Parallax movement is where objects in the background move at a different rate than objects in the foreground , thus causing the perception- of depth . - Read more about it if you 're interested . <p> First , here 's a quick video of this latest app in action . - Its a hybrid MobileFirst app , but this technique could be used in any **35;3048;TOOLONG web app experience . - The key is to keep it subtle and not too much " in your face " , and yes , it is very subtle in this video . - You have to watch closely . <p> The techniques that I wrote about in the previous post still apply Ive just added a bit more to cover more use cases . <p> This sets the background image and default position . - The distinct change here is that I set the background size to " auto " width and 120% height . - In this case , you can have a huge @ @ @ @ @ @ @ @ @ @ window size , or a small image that scales up to a larger window size . - This way you do n't  end up with seams in a repeated background or a background that is too big to highlight the parallax effect effectively . <p> In the requestAnimationFrame loop , it only applies changes *if* there are changes to apply . - This prevents needless calls to apply CSS even if the CSS styles had n't  changed . - In this , I also truncate- the numeric CSS string so that it is n't reapplying CSS if the position should shift by 0.01 pixels . Side note : If you are n't  using requestAnimationFrame for HTML animations , you should learn about- it . <p> If you used my old code and were holding the device upside down , it would n't work . - Not even a little bit . - This has that fixed ( see comments inline above ) . <p> This moves the background in CSS , which does n't  cause browser reflow operations , and moves the foreground content ( inside of a div ) using translate3d @ @ @ @ @ @ @ @ @ @ - This helps keep animations smooth and the UX performing optimally . <p> I also added a global variable to turn parallax on and off very quickly , if you need it . <p> The result is a faster experience that is more efficient and less of a strain on CPU and battery . - Feel free to test this technique out on your own . <p> If you use the code above , you can modify the xMovement and yMovement variables to exaggerate the parallax effect . <p> I recently attended the IBM InterConnect conference , and it was great to be there presenting IBM MobileFirst and engaging with our clients and partners . - While there , I also took part in the " Ignite " presentation series . If you are n't  familar with Ignite , each presenter has 20 slides , with the slides auto-advancing every 15 seconds . That gives you 5 minutes to make your point , and in the session you get to see 5 or 6 presentations on different topics within an hour . - I really liked this format , so @ @ @ @ @ @ @ @ @ @ my sources for the charts/data. - I chose to use publicly accessible- data for everything . - If you have n't been paying attention to mobile usage numbers and adoption , its quite staggering , and definitely worth paying attention to . 
@@106848793 @2248793/ <h> Monthly Archives : July 2015 <p> Back in February I had the opportunity to present " Enabling the Next Generation of Apps with IBM MobileFirst " at the DevNexus developer conference in Atlanta . - It was a great event , packed with lots of useful content . - Luckily for everyone who was n't able to attend , the organizers recorded most of the sessions which have just- been made available on Youtube . <p> here 's the session Description : Once your app goes live in the app store you will have just entered into an iterative cycle of updates , improvements , and releases . Each successively building on features ( and defects ) from previous versions . IBM MobileFirst Foundation gives you the tools you need to manage every aspect of this cycle , so you can deliver the best possible product to your end user . In this session , we 'll cover the process of integrating a native iOS application with IBM MobileFirst Foundation to leverage all of the capabilities the platform has to offer . <p> If you think that voice-driven apps are @ @ @ @ @ @ @ @ @ @ I have great news for you : They are not ! Last week , IBM elevated- several IBM Watson voice services from Beta to General Availability that means you can use them reliably in your own systems too ! <p> Let 's examine the two parts of the system , and see what solutions IBM has available right now for you to take advantage of <p> Transcribe audible signal to text transcript <p> Part one of this equation is converting the audible signal into text that can be parsed and acted upon . The IBM Speech to Text service fits this bill perfectly , and can be called from any app platform that supports REST services which means just about anything . It could be from the browser , it could be from the desktop , and it could be from a native mobile app . The Watson STT service is very easy to use , you simply post a request to the REST API containing an audio file , and the service will return to you a text transcript based upon what it is able to analyze from the audio @ @ @ @ @ @ @ @ @ @ worry about any of the transcription actions on your own no concern for accents , etc Let Watson do the heavy lifting for you . <p> Perform a system action by parsing text transcript <p> This one is perhaps not quite as simple because it is entirely subjective , and depends upon what you/your app is trying to do . You can parse the text transcript on your own , searching for actionable keywords , or you can leverage something like the IBM Watson Q&amp;A service , which enables natural language search queries to Watson data corpora . <p> Riding on the heels of the Watson language services promotion , I put together a sample application that enables a voice-driven app- experience on the iPhone , powered by both the Speech To Text and Watson Question &amp; Answer services , and have made the mobile app and Node.js middleware source code available on github . <p> The app communicates to the Speech to Text and Question &amp; Answer services through the Node.js middelware tier , and connects directly to the Advanced Mobile Access service to provide operational analytics ( usage @ @ @ @ @ @ @ @ @ @ from the client app on the mobile devices . <p> For the Speech To Text service , the app records audio from the local device , and sends a WAV file to the Node.js in a HTTP post request . The Node.js tier then delegates to the Speech To Text service to provide transcription capabilities . The Node.js tier then formats the respons JSON object and returns the query to the mobile app . <p> For the QA service , the app makes an HTTP GET request ( containing the query string ) to the Node.js server , which delegates to the Watson QA natural language processing service to return search results . The Node.js tier then formats the respons JSON object and returns the query to the mobile app . <p> The general flow between these systems is shown in the graphic below : <p> IBM Watson Speech QA for iOS Logic Flow <h> Code Explained <p> The code for this example is really in 2- main areas : The client side integration in the mobile app ( Objective-C , but could also be done in Swift ) , @ @ @ @ @ @ @ @ @ @ Middleware <p> The server side JavaScript- code uses the- Watson Node.js Wrapper , which enables you- to- easily instantiate Watson services in- just a few short lines of code <p> The credentials come from your Bluemix environment configuration , then you just create instances of whichever services that you want to consume . <p> I implemented two methods in the Node.js application tier . The first accepts the audio input from the mobile client as an attachment to a HTTP POST request and returns a transcript from the Speech To Text- service : <p> Note : I am using the free/open Watson Healthcare data set . However- the Watson QA service can handle other data sets- these- require an engagement with IBM to train the Watson service to understand the desired data sets . <h> Native iOS Objective C <p> On the mobile side- were working with a native iOS application . My code is written in Objective C , however you could also implement this using Swift . I wont go into complete line-by-line code here for the sake of brevity , but you can access the client side @ @ @ @ @ @ @ @ @ @ is within the postToServer and requestQA methods . <p> You can see the flow of the application within the image below : <p> App Flow : User speaks , transcript displayed , results displayed <p> The native mobile app first captures audio input from devices microphone . This is then sent to the Node.js servers /transcribe method- as an attachment to- a HTTP POST request- ( postToServer method on line 191 ) . On the server side this delegates to the Speech To Test service as described above . Once the result is received on the client , the transcribed text is displayed in the UI and then a request is made to the QA service . <p> In the requestQA method , the mobile app makes a HTTP GET request to the Node.js apps /ask method ( as shown on line 257 ) . The Node.js app delegates to the Watson QA service as shown above . Once the results are returned to the client they are displayed within a standard UITableView in the native app . <h> MobileFirst Advanced Mobile Access <p> A few other things you may @ @ @ @ @ @ @ @ @ @ : <p> Within AppDelegate.m you will see calls to IMFClient , IMFAnalytics , and OCLogger classes . These enable operational analytics and log collection within the Advanced MobileAccess service . 
@@106848795 @2248795/ <h> Category Archives : Wearables <p> Last week I was in good ole Las Vegas for IBM InterConnect IBMs largest conference of the year . With over 20,000 attendees , it was a fantastic event that covered everything from technical details for developers to forward-looking strategy and trends for C-level executives . IBM also made some big announcements for developers OpenWhisk serverless computing and bringing the Swift language to the server just to name a few . Both of these are exciting new initiatives- that offer radical changes &amp; simplification to developer workflows . <p> It was a busy week to say the least lots of presentations , a few labs , and even a role in the main stage Swift keynote . You can expect to find more detail on each of these here on the blog in the days/weeks to come . <p> For starters , here are two " lightning talks " I presented in the InterConnect Dev@ developer zone : <h> Smarter apps with Cognitive Computing <p> This session introduces the concept of cognitive computing , and demonstrates how you can use cognitive services in @ @ @ @ @ @ @ @ @ @ familiar with cognitive computing , then I strongly recommend that you check out this post : The Future of Cognitive Computing . <p> In the presentation below , I show two apps leveraging services on Bluemix , IBMs Cloud computing platform , and the iOS SDK for Watson . <p> Actually , I 'm using two Watson SDKs The older Speech SDK for iOS , and the new iOS SDK. - I 'm using the older speech SDK in one example because it supports continuous listening for Watson Speech To Text , which is currently still in development for the new SDK . <h> Redefining your personal mobile expression with on-body computing <p> My second presentation highlighted how we can use on-body computing devices to change how we interact with systems and data . - For example , we can use a luxury smart watch ( ex : Apple Watch ) to consume and engage with data in more efficient and more personal ways . - Likewise , we can also use smart/wearable peripherals devices to access and act on data in ways that were- never possible- before . <p> For example @ @ @ @ @ @ @ @ @ @ raw data transmitted by the on-body devices . - For this , I leveraged the new IBM Wearables SDK. - The IBM Wearables SDK provides a consistent interface/abstraction layer for interacting with wearable sensors . - This allows you to focus on building your apps that interact with the data , rather thank learning the ins &amp; outs of a new device-specific SDK . <p> The wearables SDK also users data interpretation algorithms to enable you to define gestures or patterns in the data , and use those patterns to act upon events when they happen without additional user interaction . - For example : you can determine if someone falls down , you can determine when someone is raising their hand , you can determine anomalies in heart rate or skin temperature , and much more . - The system is capable of learning patterns for any type of action or virtually any data being submitted to the system . - Sound interesting ? - Then check it out here . <p> I also had some other- sessions- on integrating drones with cloud services , integrating weather services in @ @ @ @ @ @ @ @ @ @ sure to post updates for this- content- I make them publicly available . - I think you 'll find the session on drones + cloud especially interesting I know I did . <p> Last week I attended IBM Insight in Las Vegas . It was a great event , with tons of great information for attendees . I had- a few sessions on mobile applications . In particular , my dev@Insight session on Wearables powered by IBM MobileFirst was recorded . You can check it out here : <h> Key takeaways from the session : <p> Wearables are the most personal computing devices ever . Your users can use them to be notified of information , search/consume data , or even collect environmental data for reporting or actionable analysis . <p> Regardless of whether developing for a peripheral device like the Apple Watch or Microsoft Band , or a standalone device like Android Wear , you are developing an app that runs in an environment that mirrors that of a a native app . So , the fundamental development principles are exactly the same . You write native code , that @ @ @ @ @ @ @ @ @ @ back-end . <p> Caveat to #1 : You user interface is much smaller . You should design the user interface and services to acomodate for the reduced amount of information that can be displayed . <p> You can share code across both the phone/tablet and watch/wearable experience ( depending on the target device ) . <p> Using IBM MobileFirst you can easily expose data , add authentication , and capture analytics for both the mobile and wearable solutions . 
@@106848796 @2248796/ <h> Monthly Archives : September 2013 <p> Last week I had the opportunity to feed my obsession with aerial photography and participate in Russell BrownsTop Gun Flight Training- workshop at Photoshop World , and it was an awesome experience ! Many thanks to Russell for putting together this incredible workshop . There 's nothing like a giant room full of RC photography enthusiasts , with multiple copters flying around all over the place , coupled with a wealth of Photoshop and video processing knowledge ( links below for all of this content ) ! <p> There are few places that have as dramatic architecture and landscape as Vegas , and this also made for some great- photos . <p> Aerial shot over the Luxor and Mandalay Bay in Las Vegas <p> The day started with an intro from DJI , who sponsored the event and donated copters for workshop participants to use . - If you have n't seen their copters/footage yet , you should check out the short video below to see what 's possible with todays radio controlled aircraft . <p> Next , we broke off into teams to cover @ @ @ @ @ @ @ @ @ @ important thing of all ) , basic takeoff and landing procedures , flight modes and capabilities , to techniques for panoramas and dramatic shots . <p> Once everyone had a chance to fly the copters and get acclimated , we regrouped to learn how to work with your aerial footage. - Do not miss the videos from Russell Brown and Colin Smith in the links below . These are must-watch content for any aerial photographer , cinematographer , or GoPro enthusiast and contain a wealth of information ! <p> Finally , the day ended with a flying competition The person who could fly through 4 hula hoops without knocking anything over , without crashing , and land in a controlled fashion walked home with a brand new DJI Phantom , and the winner did it in 26 seconds ! This was not an easy task . <p> Its an exciting day for Creative Cloud members ! Two significant updates have just been released ! <p> The first major update is for- Adobe Photoshop CC . This release introduces Adobe Generator for Photoshop CC . Generator enables real-time generation of image @ @ @ @ @ @ @ @ @ @ JavaScript-based architecture . Changes include : <p> Real-time image asset generation gives Photoshop CC users the power to eliminate time-consuming production steps while ensuring their designs are properly implemented both on the desktop and mobile screens <p> Tagged layers and groups in Photoshop CC are automatically saved and updated in real-time as individual files , in the format selected <p> Layers can now be exported as JPEG , GIF , or PNG with a variety of options , including scaling for Retina displays and varying levels of compression <p> In one click , Edge Reflow CC can import Photoshop CC assets , including images and text directly into Edge Reflow , allowing designers to immediately begin their responsive design process and reduce manual production steps <p> You can learn more about this version of Adobe Photoshop CC in the video below : <p> Generator enables new workflows by simplifying the process of exporting content from Photoshop . This could just be for image extraction , could be used to simplify design processes , or even integrate with a larger workflow . Some example uses are these iOS icon and PhoneGap @ @ @ @ @ @ @ @ @ @ . You can see these in action in this video from Raymond Camden . <p> How else might this affect your workflow ? Well , check out the new Photoshop to Adobe Edge Reflow workflow powered by Generator in the video below . With Generator , Edge Reflow can now seamlessly connect to Photoshop and extract assets for inclusion in Edge Reflow designs . <p> Read more more about the Edge Reflow and Photoshop CC live integration on html.adobe.com . <p> Seriously , though this is just the- proverbial " tip of the iceberg . " Check out the next video to see how the Loom game engine can leverage Adobe Photoshop CC and Generator for live authoring of content inside of a game running on separate mobile devices in real time . <p> A new Direct Link Color Pipeline between Adobe Premiere Pro CC and SpeedGrade CC- provides an integrated workflow that allows users to move multi-track timelines seamlessly back and forth ; open Adobe Premiere Pro CC sequences in SpeedGrade quickly ; and see the results as effects in Adobe Premiere Pro CC that are managed by the @ @ @ @ @ @ @ @ @ @ UltraHD , 4K and higher resolutions , - high frame rates and RAW formats , enables editors to work with footage from the hottest new high-res cameras natively " without having to wait to transcode and re-wrap files . <p> The Mask Tracker in After Effects- enables video professionals to create masks and apply effects that track automatically frame-by-frame throughout a composition to save countless hours of tedious work . <p> Performance enhancements- punctuate this release with support in Premiere Pro CC for OpenCL , providing editors with the speed and power they need for the most demanding projects ; and new GPU debayering of the Cinema DNG file format for real time playback . <p> A preview of the upcoming Prelude CC Live Logger iPad app , which enables users to log notes , events , and other data on their iPad while shooting , including the ability to sync with timecode on set via supported wireless timecode generators , and then sync metadata to footage via Creative Cloud for faster editing . <p> Advanced color grading- with the new SpeedLooks in SpeedGrade CC offers dedicated camera patches . @ @ @ @ @ @ @ @ @ @ different camera formats . New multiple masks and linked mask layers capabilities also enable SpeedGrade users more control over complex looks . <p> New production planning features in Adobe Story Plus- provide powerful scheduling and reporting tools for managing productions efficiently , making it easy to modify and share lists between productions and users . <p> Read more about the new updates for video editing tools in the release notes below : <p> Want to get inspired , learn the latest and greatest about Adobe and Creative Cloud , or get tips on all of Adobes creative tools ? - If you answered yes to any of those , then be sure to check out the Create Now World Tour coming to a city near you . - It is a free event , so you have no excuse to miss it ! <p> CREATE NOW WORLD TOUR- brings Adobe experts and evangelists to your city , so you can learn new ways to create right from the pros . Join these free events to discover new tips and tricks that will help you work more efficiently and give you more control over all your projects . 
@@106848797 @2248797/ <h> Powering Apple Watch Apps with IBM MobileFirst Part 1 <p> This is the first entry in a multipart series on powering native iPhone and Apple Watch apps using the IBM MobileFirst Platform . - In this entry we will cover how to setup the MobileFirst Platform for use within Apple WatchKit apps and leverage the operational analytics and remote logging features . <p> So , let 's first take a look at the app were going to build in this video : <p> The app is a simple location tracker. - Think of something like a much simpler version of Run Keeper that will allow you to track your location path over a period of time , and show your location on a map . - Were also building a WatchKit app that enables you to quickly start or stop tracking your location without ever having to pull your iPhone out of your pocket . - All of this powered by IBM MobileFirst . <p> When you 're setting up your WatchKit app , you need to follow the exact same steps that you did for the native app target @ @ @ @ @ @ @ @ @ @ <p> First you need to add the required frameworks and dependencies ( full list here , also be sure to include **36;3085;TOOLONG that is inside the iOS API ) : <p> Add MobileFirst Frameworks and Dependencies <p> Next , add the " -ObjC " linker flag : <p> Add Linker Flag <p> Then make sure that worklight.plist ( which is inside of the MobileFirst API you generated from either the CLI or Eclipse Studio ) so that it is included in both the native app and WatchKit extension . <p> Worklight.plist Target Membership <p> This allows you to take advantage of MobileFirst APIs within your WatchKit extension , complete with operational analytics. - You cansave remote logs , you can access data adapters , and more. - The server-side security mechanisms also work , so if you want to shut down your API for specific versions , you have that ability . <p> I mentioned earlier , its just like a native iOS app , but with a few exceptions . - The most important and notable exception is that the UI elements ( modal dialogs , alerts , etc @ @ @ @ @ @ @ @ @ @ interface do not appear in the WatchKit interface . - You do n't  get errors you just do n't  see the notification . - So , you need to work around any scenarios that rely on this , and make sure you handle errors accordingly . <p> To invoke MobileFirst APIs , you call them as you wold normally in either Objective-C or Swift . - For example : <p> and the remote log search capability , including- logs from the WatchKit extension : <p> MobileFirst Remote Logging with the WatchKit App <p> That 's all that you need to get started ! <p> Stay tuned ! - Full source code will be released on my github account in a subsequent post. - Also be sure to stay tuned for future entries that cover the MobileFirst platform with offline data , persisting data to the server , push notifications , geo notifications , bidirectional communication between the watch and host app , background processing , and more ! I will update this post to links to each subsequent post as it is made available . <p> Wondering what IBM MobileFirst is @ @ @ @ @ @ @ @ @ @ and maintain mobile applications throughout their entire lifecycle. - This includes tools to easily manage data , offline storage , push notifications , user authentication , and more , plus you get operational analytics and remote logging to keep an eye on things once you 've deployed it to the real world , and its available as- either cloud or on-premise solutions . 
@@106848801 @2248801/ <h> Tag Archives : android <p> The recording for my session " PhoneGap and Hardware " from PhoneGap Day back in July is now available ! Be sure to check it out . There were apparently some issues with the audio , but you can still hear everything . <p> I 'd like to express a huge Thank You- to everyone who attended , and to everyone who watches this video ! <p> Below are the sample projects I showed in the presentation , including source code . However , keep in mind that all of these examples were written before PhoneGap 3.0 . The native plugin syntax , and inclusion methods have changed . <h> Pressure Sensitive Sketching in PhoneGap <p> In this example , - the pressure-sensitive Pogo Connect Stylus uses a low energy Bluetooth 4 connection to relay touch/pressure information back to the PhoneGap application . This makes for a unique drawing and sketching experience powered with the HTML5 Canvas element . I 've written about this example previously Check out the video below to see it in action , and read the blog post for technical details @ @ @ @ @ @ @ @ @ @ example that I explored is a PhoneGap native plugin that is used to handle input from a Moga game controller inside of a PhoneGap application on Android . <p> This implementation is intended to be a proof of concept demonstrating how you could integrate the gamepad within your application . It currently only supports input from the joysticks ( axisX and axisY ) and the A and B buttons , and it does not handle all possible input from the controller . <p> This implementation is adapted directly from the **38;3123;TOOLONG example from the Moga developers SDK samples available for download at : - http : **34;3163;TOOLONG 123434 @qwx983434 <p> The game is based on the Universe- prototype that was used as a sub-game inside of the MaxMe app for the recent Adobe MAX conference . I make no guarantees about the code for this game , it was in a huge rush ! <p> With Halloween less than a month away , I figured its about time to update my " Instant Halloween " PhoneGap sound effects app . I 'm happy to say that the latest version is now @ @ @ @ @ @ @ @ @ @ few new sounds , a new UI style , and has been updated for iOS 7 . I also updated the low latency audio plugin- so it now supports PhoneGap 3.0 method signatures and supports the command line tooling for installation . <p> This app is fantastic for scaring people Just hook it up to really loud speakers , and start playing the sounds to your hearts content . Its got everything from background/ambient loops to maniacal laughter , screams , ghosts , zombies , and other spooky sound effects . <p> It is available now , for FREE , for both iOS ( 5.0+ ) and Android ( 4.0+ ) . <p> - <p> So what has changed in this version ? <p> First , I updated the app to support iOS 7 . For the most part , this is a non-issue . PhoneGap apps are based on web standards , and HTML/JS/CSS work pretty much everywhere . However , you do have to account for a few minor changes . One is that the OS status bar now sits over top of the application . You 'll need @ @ @ @ @ @ @ @ @ @ are no UI issues . Check out this post from Christophe Coenraets for details regarding creating PhoneGap apps for iOS 7 . <p> iOS 7 also introduces some new UI design paradigms and guidelines . I simplified the user interface , got rid of all textures , and tried to make things as simple and minimalistic and native-feeling as possible . I also got rid of iScroll for touch-based scrolling both the iOS and Android versions now use native inertial based scrolling from the operating system . This is the reason that the new Android version is only Android 4.0 and later , but it is also the reason that the app feels much closer to a fully native experience . <p> I updated the low latency audio native plugin to support PhoneGap PhoneGap 3 . There were two parts to this : First , there is the updated method signature on the native interfaces . I just took the old plugin , and updated it for the new method signature . The new method signature- was actually introduced a while back , but I never updated the plugin for @ @ @ @ @ @ @ @ @ @ to enable CLI-based installation of the plugin for both iOS and Android . Take a look at the PhoneGap documentation for details on creating PhoneGap native plugins- and plugin.xml. - Also check out this tutorial from Holly Schinsky for help creating native plugins for Android . <p> In the process , I also ran into an unexpected issue with Android deployment Back in the spring I had a corrupted hard drive . I was able to recover *most* of my data , and I thought I had all of my app signing keys . It turns out that for Android , your apps must not only have the same signing keys , but also the same key store when you sign the APK files , or else Google wont let you distribute the Android APK file as an update ; it must be a new app . It turns out that I had recovered the key , but not the keystore . So , I had no choice but to distribute it as a new app . <p> Go download the free apps , get the source code , and @ @ @ @ @ @ @ @ @ @ like to express a huge Thank You- to everyone who attended PhoneGap Day in Portland last week , and to Colene and the team that put everything together ! The day was loaded with fantastic presentations , and great community interaction tons of information , tons of great questions , and tons of great people ( oh , and beer it would n't be PhoneGap Day without beer ) . <p> My session was about the integration of PhoneGap with hardware . Basically , it was an overview and exploration how you can use native plugins to extend the capabilities of PhoneGap applications and interact with device peripherals . This enables new interaction paradigms , and helps evaluate and evolve what is attainable with web-related technologies . <p> In this session , I covered two use cases The first use case is the use of a pressure-sensitive stylus for interacting with a PhoneGap application on iOS . The second use case is integration of a Moga gamepad with a PhoneGap application on Android . In both cases , the application experience is augmented by the peripheral device , which changes how @ @ @ @ @ @ @ @ @ @ with the application content and context . <h> Pressure Sensitive Sketching in PhoneGap <p> In this example , - the pressure-sensitive Pogo Connect Stylus uses a low energy Bluetooth 4 connection to relay touch/pressure information back to the PhoneGap application . This makes for a unique drawing and sketching experience powered with the HTML5 Canvas element . I 've written about this example previously Check out the video below to see it in action , and read the blog post for technical details and source code . <h> Moga Gamepad <p> The second example that I explored is a PhoneGap native plugin that is used to handle input from a Moga game controller inside of a PhoneGap application on Android . <p> This implementation is intended to be a proof of concept demonstrating how you could integrate the gamepad within your application . It currently only supports input from the joysticks ( axisX and axisY ) and the A and B buttons , and it does not handle all possible input from the controller . <p> This implementation is adapted directly from the **38;3199;TOOLONG example from the Moga developers SDK samples @ @ @ @ @ @ @ @ @ @ 123434 @qwx983434 <p> The game is based on the Universe- prototype that was used as a sub-game inside of the MaxMe app for the recent Adobe MAX conference . I make no guarantees about the code for this game , it was in a huge rush ! <p> I am very excited to share that the Fresh Food FinderPhoneGap application which I built last summer was recently featured in the June 2013 issue of SELF Magazine . This app was built as a demonstration/sample application for PhoneGap , but has gained popularity for being an easy way to find farmers markets by leveraging the USDA farmers markets directory- and has appeal to the general public . <p> In fact , Ive seen so many downloads that I am working on an updated version of it based on user feedback , with more recent data ( live services instead of embedded ) , and will be leveraging the awesome topcoat.io CSS framework for styling . <p> Fresh Food Finder in SELF Magazine <p> SELF Magazine has huge readership numbers ( almost 1.5 million monthly subscribers according to Wikipedia ) . - @ @ @ @ @ @ @ @ @ @ magazines calendar , this has driven both app downloads and visitors to my blog ( peaked at 348 downloads per day ) ! <p> App downloads May 1- July 8 <p> This also brought the app higher up in the iOS market ranks ( peak #57 in the " Food &amp; Drink " category ) : <p> App store ranking May 1- July 8 <p> You can download the Fresh Food Finder app- today in the following markets : <p> One of my first tasks upon downloading Android Studio was to get a PhoneGap app up and running in it . here 's how to get started . Note : I used PhoneGap 2.7- to create a new project with- the latest stable release , however you could use the same steps ( minus the CLI create ) to import an already-existing PhoneGap application . Be sure to backup your existing project before doing so , just in case you have issues ( Android Studio is still in beta/preview ) . <p> Next , you 'll have to select the directory to import . Choose the directory for the PhoneGap project you @ @ @ @ @ @ @ @ @ @ you click " OK " , you will proceed through several steps of the import wizard . On the next screen , make sure that " Create project from existing sources " is selected , and click the " Next " button . <p> You will next specify a project name and project location . Make sure that the project location is the same as the location you selected above ( and used in the PhoneGap command line tools ) . I noticed that the default setting was to create a new directory , which you do not want . Once you 've verified the name and location , click " Next " . <p> On the next step , leave the default settings ( everything checked ) , and click " Next " . <p> Android Studio should now open the full IDE/editor . You can just double click on a file in the " Project " tree to open it . <p> To run the project , you can either go to the " Run " menu and select " Run project name " , or click on the @ @ @ @ @ @ @ @ @ @ launch the application in your configured environment ( either emulator or on a device ) . You can see the new PhoneGap application running in the Android emulator in the screenshot below . If you 'd like to change your " Run " configuration profile , go to the " Run " menu and select " Edit Configurations " , and you can create multiple launch configurations , or modify existing launch configurations. 
@@106848802 @2248802/ <h> Category Archives : Creative Cloud <p> Last week I had the opportunity to present an incredibly fun topic to the DC/MD/VA Creative Professionals user group GoPro Cameras , Quadcopters , and Adobe Creative Cloud . - Thanks to everyone who attended . - This topic is a personal interest of mine , and I had a great time . - There were great questions and great conversations all around . <p> For those who werent able to attend , here 's a video of the full 2-hour presentation . - The audio quality is n't perfect , but you can still catch most of it : <p> FPV : KumbaCam- Great for a remote viewfinder , though the GoPro feed flickers when in time lapse photography mode . - I put it on a tripod at eye level so I can quickly glance between LOS and FPV viewing ( FPV = First Person View ) . I use this as a remote viewfinder , not a primary flight mechanism , and never go beyond line-of-sight . <p> Without the gimbal and FPV , you 'll get about 12-15 mins of battery @ @ @ @ @ @ @ @ @ @ I get about 7-8 minutes per flight I 'm currently researching options to extend battery life &amp; flight time . <p> You can definitely get bigger copters with a heavier lift capacity , but this configuration is great for getting started , and is designed specifically for the GoPro . Then , use Creative Cloud to polish your images and video . <p> here 's a quick video I put together showing how to create incredible panorama images in ten easy steps , using Adobe Photoshop CC. - I captured this one on Monday in a sunrise session- in Richmond , VA with a DJI Phantom and GoPro . <p> and the ten steps are : <p> Go get some awesome shots ! Just be sure that you 're in the same location and there is overlap between each shot <p> In my last post , I proclaimed my love for Adobe Creative Cloud . This post will show you the reason why . I was playing around with some of the aerial footage I captured last week in San Francisco . Just for fun , I wanted to create a HUD ( @ @ @ @ @ @ @ @ @ @ of the video . My inspiration was the HUD created for The Avengers &amp; Iron Man , which was created using Adobe After Effects . This turned out far better and far more interesting than I could have possibly hoped , and it is all thanks to the power of Adobe Creative Cloud . here 's the final video complete with special effects , and below I will discuss how I used Creative Cloud to get to this . ( Best experienced at 1080p , with audio preferably loud , with lots of bass . ) <p> First things first , I had to design the HUD . I used Photoshop to pull in a still from the video footage , and started layering elements on top of it . I Googled images of real fighter jet HUD displays , and used those as inspiration . I obviously did n't  have all of the same information , so I could n't make my HUD absolutely real , but I could make it look " good enough " . <p> HUD Design in Adobe Photoshop <p> I got the mockup to a point @ @ @ @ @ @ @ @ @ @ time to implement it for real in the video . It turns out my initial design did n't  work great in the final implementation , so I came back to Photoshop played with colors , and sizes , and chopped pieces up into separate image assets that I could pull into the final composition . <p> Next , I pulled the video into After Effects , and started overlaying the HUD graphic elements . - I chose After Effects for this instead of Premiere because After Effects has better control over the visual output and effects Premiere is my primary tool for sequencing multiple clips into a larger composition . <p> Editing in Adobe After Effects <p> I added all of the HUD elements and manually animated rotation and position so that it fit well with the actual flight path . Everything seemed in place , but I felt like it needed more . <p> Why not have targeting indicators that follow the cars ? With After Effects Track Motion feature , this was easily done . I created a " target " graphic , inserted it into the composition @ @ @ @ @ @ @ @ @ @ path for the graphic . To do this , select the video layer that you want to use for motion tracking , and go to a frame that has the object that you want to track . Then click on the " Track Motion " button in the " Tracker " panel . Youll have to select an area that will be tracked . When you analyze frames , it will detect the movement of your selection over time , and translate that to x/y coordinates , which are applied to the motion target that you choose ( the " target " symbol ) . <p> I repeated this step for a bunch of vehicles , and it started looking much better . Once I had the red target indicators in the HUD , I thought " that looks cool , but its still not enough , and its not believable . " <p> I added some color correction using After Effects Tritone color correction . - This made the HUD really stand out from the video , and gave it a nice cinematic look and feel , but I @ @ @ @ @ @ @ @ @ @ you 're going to go " over the top " , you might as well go " way over the top " , so I started getting creative/ridiculous . I had this robotic fighter jet feel in the video , so I figured that something needed to blow up . I found this explosion and that 's when things started getting really interesting . I added one , then two , then three explosions to the scene by leveraging After Effects Linear Color Key effect so that the explosion was overlaid without the background . Add some color correction on the explosion , and voila you have an explosion on top of the video with minimal artifacts . <p> Note : Keying is the process of removing pixels from the background based on pixel colors . You can also remove pieces of a video clip using the rotoscoping tool- its like a Photoshop selection over time . <p> This was really starting to come together . Since I had explosions , I needed more smoke . I first tried the- After Effects particle system- for producing smoke , but it did n't  @ @ @ @ @ @ @ @ @ @ found some stock footage of smoke plumes and ambient smoke , and started going to town . Pairing the stock footage with Track Motion , I was able to add smoke to the landscape that followed buildings as the camera rotated to focus back on the building . <p> Like I said earlier , I wanted to go " way over the top " , so of course , why not add a flyby from some jets . So I added some stock footage of computer generated jets flying overhead , again with Keying to remove the background . <p> At this point , things were really coming together for this scene , so I wanted to add an intro title and some music enter Adobe Premiere . <p> Composing/Sequencing in Adobe Premiere <p> Here I added the title , and started adding the static effect overlaid in the beginning and the ending of the composition . - Next , I needed background music and sound effects . - Sound is critical to the experience of video . - It can help convey emotion , and tie everything together . @ @ @ @ @ @ @ @ @ @ . Things were starting to come together really well , but I needed more sound effects- A while back I stumbled across freesound.org , which has a bunch of Creative Commons sound effects . - This has been a goldmine for me . I pulled in sound effects for the explosions , the ambient aircraft noise , ambient machine guns , and radar beeps . <p> Then I pulled some of the sounds into Adobe Audition for some fine-tuning <p> Audio Processing with Adobe Audition <p> Once I had everything sequenced where I wanted it , I just exported the video from Premiere , uploaded it to Youtube , and started sharing it . <p> The best parts of this entire process : <p> I did this whole thing start to finish in a little over one day . - I started working with the video on Monday night , and uploaded it to YouTube this morning . Creative Cloud has an insanely productive workflow . <p> My background is in software development , not in video production I do that for fun . By using Creative Cloud , I @ @ @ @ @ @ @ @ @ @ together . <p> Creative Cloud is absolutely incredible . Yes , I work for Adobe , so this may sound like a biased opinion , but even if I did n't  work for Adobe , I would say the same thing . " What 's so incredible ? " you are wondering ? Well , for starters , it has everything I need to bring a creative vision to life . <p> I can hear it now you are thinking " Thats awfully vague . " - Let me try to provide some context <p> My background and all of my formal education is in computer science and software development . My career has been focused on building great applications and immersive experiences for the end user . This has taken me on a perilous journey between the client and the server ; from the depths of C/C++/Objective C , to the outer regions of JavaScript , from the early interactive web with Flash/ActionScript , to the innermost workings of Java , from countless acronyms , realtime data streams , and REST/web services on towards the future with the evolution web @ @ @ @ @ @ @ @ @ @ of these roles and languages is that you have to create assets to build your user interface . - Throughout my career , Ive used Photoshop and Illustrator to build the assets . - While my primary use case has been creating and manipulating assets for software , I occasionally would use other tools for other purposes . <p> Creative Cloud has everything that I need to execute on this workflow , plus more . A whole lot more . <p> Creative Cloud has everything I need to bring a creative vision to life . <p> For those cases where I need to switch roles and edit video , cleanup some audio , or retouch a series of images , I already have everything I could possibly need with Creative Cloud . Video tools , audio tools , print design tools , digital publishing tools , development tools Theres something for everyone . <p> On occasion , I need to edit video . - Did you know that with Creative Cloud , you get THE BEST VIDEO EDITING WORKFLOW ON THE PLANET ? No , that 's not a joke or @ @ @ @ @ @ @ @ @ @ , Life of Pi , and many , many more movies and television shows leverage the Adobe toolchain to execute their creative vision . <p> Trying to recreate some of these movie visual effects is actually what inspired me to write this post . I was metaphorically " blown away " by the capability of everything I already had installed on my computer . The end result and video effects I 'll cover in my next post , but here 's a quick screenshot : <p> I 've had an interest in art , photography , computer graphics , music , and audio production my entire life . I took my first Photoshop class back in the mid 90s , and never looked back . I played in a band , and thought music and digital arts would be my entire life . Creative Cloud makes perfect sense to me . I get the tools I need on a daily basis , and so much more . Whatever I need , its there . <p> Ive been spending a lot of time with Photoshop recently Whether it has been retouching video or images @ @ @ @ @ @ @ @ @ @ , it has been a lot of fun . One thing that I 've been doing is exporting really large images to the web . So far this has been a very manual process Export from Photoshop using Zoomify . Then , since the default Zoomify renderer uses Flash ( and I want this consumable on mobile devices ) , take the Zoomify image tiles , and put them into a custom-coded HTML experience using the Leaflet tile- engine with a custom tile layer . <p> Leaflet is normally used for web-based mapping , but it is a perfect solution for rendering image tiles on the web . It already has touch and mouse interactions , inertial scrolling , progressive viewing , and a comprehensive API that can be extended if you so choose . <p> I 've done this enough times that I figured " There has to be an easier way " and there definitely is . I 've created a new Zoomify template that allows you to export from Photoshops Zoomify feature directly to HTML , leveraging the Leaflet engine . All of the code and installation instructions are below @ @ @ @ @ @ @ @ @ @ see it in action : <h> Samples <p> Here are few samples from the Zoomify output ; both are the compositions that I showed in the video above . Use the mouse or touch interactions to pan and zoom on each of them . <p> The first is an export from a 10MP aerial panorama ( 4340+2325 pixels ) , which was created by stitching together multiple images captured with a GoPro camera and remote controlled helicopter . <p> The second example is a massive 139MP composite image ( 14561+9570 pixels ) . I created this by stitching together 48 10mp images in Photoshop . Its not perfect , but shows how far you can zoom into an image some images had different exposures , some were out of focus , there is still some perspective warp , and I definitely have some bad stitching seams . This image is so huge that I actually maxed out the system RAM , and filled up all hard disk space with the memory swap file when creating it ( I had over 100 Gigs of free space ) ! <p> Extract the @ @ @ @ @ @ @ @ @ @ directory . On OS X , with the default configuration , these files should be located in /Applications/Adobe Photoshop CC/Presets/Zoomify/ <p> L.TileLayer.Zoomify.js <p> Zoomify Leaflet HTML.zvt <p> leaflet.css <p> leaflet.js <p> Restart Photoshop . <p> When you have a file open that you want to export , choose File -&gt; Export -&gt; Zoomify- <p> Then select the " Zoomify Leaflet HTML " template that should now be in the list . Select an output location , base name , and image options , and hit " OK " . Ignore the browser width and height , since the template ignores these . Instead , it takes 100% of the width and height of the browser window . <p> - This will generate all of the image tiles and the HTML structure . From here , do whatever you want with it You can modify it , put it on a server , or anything else . the file output will look something like the image below . You will have a folder that contains the generated HTML file , the Leaflet JS and CSS files , and a directory that contains the generated tiles and appropriate XML metadata. 
@@106848803 @2248803/ <h> Category Archives : Cordova <p> One criticism of PhoneGap apps that I sometimes hear is that they often do n't  have " standard " features from the native operating system . - Little things , like iOSs ability to scroll a container back to the top , just by tapping on the operating systems status bar . These types of features are not hard to add to a PhoneGap application , at all . This is more of an " attention to detail " issue , not something that the platform ca n't do . <p> Since this is n't a feature that is applicable on all platforms , and it can vary per PhoneGap app implementation , it is not part of the core PhoneGap/Cordova download . - However , this can be very easily added via a native plugin. - Native plugins enable you to access native code , or augment the capabilities of PhoneGap for a particular platform . <p> If you download the app from the app store today , you wo n't see this yet because I literally *just* submitted it to Apple . <p> So @ @ @ @ @ @ @ @ @ @ . The first thing to do is check and see if there was an existing native plugin that has already been created by someone in the PhoneGap/Cordova community . - It turns out , Greg- pointed out- one that already existed . - Since this plugin was built targeting an older version of PhoneGap , and my project was built using PhoneGap 3.0 , I had a few minor updates . - Though , I was able to get everything all set up in a very short period of time . <p> Then , in your PhoneGap application , you just have to add an event listener for the " statusTap " event , which is triggered when the user taps on the operating systems status bar . - It is literally this simple : <p> This just shows an alert that the status bar was tapped . If you want to animate specific containers , you have to do this manually yourself via JavaScript . Again , that is n't hard to do . here 's an excerpt that I used from the Halloween app , using jQuery syntax : <p> @ @ @ @ @ @ @ @ @ @ its about time to update my " Instant Halloween " PhoneGap sound effects app . I 'm happy to say that the latest version is now out for both iOS and Android . It has a few new sounds , a new UI style , and has been updated for iOS 7 . I also updated the low latency audio plugin- so it now supports PhoneGap 3.0 method signatures and supports the command line tooling for installation . <p> This app is fantastic for scaring people Just hook it up to really loud speakers , and start playing the sounds to your hearts content . Its got everything from background/ambient loops to maniacal laughter , screams , ghosts , zombies , and other spooky sound effects . <p> It is available now , for FREE , for both iOS ( 5.0+ ) and Android ( 4.0+ ) . <p> - <p> So what has changed in this version ? <p> First , I updated the app to support iOS 7 . For the most part , this is a non-issue . PhoneGap apps are based on web standards , and HTML/JS/CSS @ @ @ @ @ @ @ @ @ @ to account for a few minor changes . One is that the OS status bar now sits over top of the application . You 'll need to update your UI on iOS 7 , so there are no UI issues . Check out this post from Christophe Coenraets for details regarding creating PhoneGap apps for iOS 7 . <p> iOS 7 also introduces some new UI design paradigms and guidelines . I simplified the user interface , got rid of all textures , and tried to make things as simple and minimalistic and native-feeling as possible . I also got rid of iScroll for touch-based scrolling both the iOS and Android versions now use native inertial based scrolling from the operating system . This is the reason that the new Android version is only Android 4.0 and later , but it is also the reason that the app feels much closer to a fully native experience . <p> I updated the low latency audio native plugin to support PhoneGap PhoneGap 3 . There were two parts to this : First , there is the updated method signature on the native interfaces @ @ @ @ @ @ @ @ @ @ it for the new method signature . The new method signature- was actually introduced a while back , but I never updated the plugin for it. - Second , I added the appropriate XML metadata to enable CLI-based installation of the plugin for both iOS and Android . Take a look at the PhoneGap documentation for details on creating PhoneGap native plugins- and plugin.xml. - Also check out this tutorial from Holly Schinsky for help creating native plugins for Android . <p> In the process , I also ran into an unexpected issue with Android deployment Back in the spring I had a corrupted hard drive . I was able to recover *most* of my data , and I thought I had all of my app signing keys . It turns out that for Android , your apps must not only have the same signing keys , but also the same key store when you sign the APK files , or else Google wont let you distribute the Android APK file as an update ; it must be a new app . It turns out that I had recovered the @ @ @ @ @ @ @ @ @ @ had no choice but to distribute it as a new app . <p> Go download the free apps , get the source code , and start building your own PhoneGap apps today ! <p> About a year ago I released the Fresh Food Finder , a multi-platform mobile application built with PhoneGap . The Fresh Food Finder provides an easy way to search for farmers markets near your current location , based on the farmers markets listings from the USDA . This app has seen a lot of popularity lately , so I 'm working on a new version for all platforms with a better data feed , better UI , and overall better UX unfortunately , that version is n't ready yet . However , I have been able to bring it to an additional platform this week : Firefox OS ! <p> Fresh Food Finder on iOS , Firefox OS , &amp; Android <p> PhoneGap support is coming for Firefox OS , and in preparation I wanted to become familiar with the Firefox OS development environment and platform ecosystem . So I ported the Fresh Food Finder , minus the @ @ @ @ @ @ @ @ @ @ this really shows the power of web-standards based development ) is that I was able to take the existing PhoneGap codebase , and turn it into a Firefox OS app AND submit it to the Firefox Marketplace in under 24 hours ! If you 're interested , you can check out progress on Firefox OS support in the Cordova project , and it will be available on PhoneGap.com once its actually released . <p> Basically , I commented out the PhoneGap-specific API calls , added a few minor bug fixes , and added a few Firefox-OS specific layout/styling changes ( just a few minor things so that my app looked right on the device ) . Then you put in a mainfest.webapp configuration file , package it up , then submit it to the app store . Check it out in the video below to see it in action , running on a Firefox OS device <p> The phone I am using is a Geeksphone Firefox OS developer device . Its not a production/consumer model , so there were a few hiccups using it , but overall it was a good @ @ @ @ @ @ @ @ @ @ Mozilla for helping me get the latest device image running on my phone . <p> You can learn more about getting started with Firefox OS development here : <p> I 'd like to express a huge Thank You- to everyone who attended PhoneGap Day in Portland last week , and to Colene and the team that put everything together ! The day was loaded with fantastic presentations , and great community interaction tons of information , tons of great questions , and tons of great people ( oh , and beer it would n't be PhoneGap Day without beer ) . <p> My session was about the integration of PhoneGap with hardware . Basically , it was an overview and exploration how you can use native plugins to extend the capabilities of PhoneGap applications and interact with device peripherals . This enables new interaction paradigms , and helps evaluate and evolve what is attainable with web-related technologies . <p> In this session , I covered two use cases The first use case is the use of a pressure-sensitive stylus for interacting with a PhoneGap application on iOS . The second use case @ @ @ @ @ @ @ @ @ @ on Android . In both cases , the application experience is augmented by the peripheral device , which changes how it is possible for the user to interact and engage with the application content and context . <h> Pressure Sensitive Sketching in PhoneGap <p> In this example , - the pressure-sensitive Pogo Connect Stylus uses a low energy Bluetooth 4 connection to relay touch/pressure information back to the PhoneGap application . This makes for a unique drawing and sketching experience powered with the HTML5 Canvas element . I 've written about this example previously Check out the video below to see it in action , and read the blog post for technical details and source code . <h> Moga Gamepad <p> The second example that I explored is a PhoneGap native plugin that is used to handle input from a Moga game controller inside of a PhoneGap application on Android . <p> This implementation is intended to be a proof of concept demonstrating how you could integrate the gamepad within your application . It currently only supports input from the joysticks ( axisX and axisY ) and the A and B @ @ @ @ @ @ @ @ @ @ from the controller . <p> This implementation is adapted directly from the **38;3275;TOOLONG example from the Moga developers SDK samples available for download at : : - http : **34;3315;TOOLONG 123434 @qwx983434 <p> The game is based on the Universe- prototype that was used as a sub-game inside of the MaxMe app for the recent Adobe MAX conference . I make no guarantees about the code for this game , it was in a huge rush ! <p> I am very excited to share that the Fresh Food FinderPhoneGap application which I built last summer was recently featured in the June 2013 issue of SELF Magazine . This app was built as a demonstration/sample application for PhoneGap , but has gained popularity for being an easy way to find farmers markets by leveraging the USDA farmers markets directory- and has appeal to the general public . <p> In fact , Ive seen so many downloads that I am working on an updated version of it based on user feedback , with more recent data ( live services instead of embedded ) , and will be leveraging the awesome topcoat.io CSS @ @ @ @ @ @ @ @ @ @ Magazine <p> SELF Magazine has huge readership numbers ( almost 1.5 million monthly subscribers according to Wikipedia ) . - While it was just a very small callout in the magazines calendar , this has driven both app downloads and visitors to my blog ( peaked at 348 downloads per day ) ! <p> App downloads May 1- July 8 <p> This also brought the app higher up in the iOS market ranks ( peak #57 in the " Food &amp; Drink " category ) : <p> App store ranking May 1- July 8 <p> You can download the Fresh Food Finder app- today in the following markets : 
@@106848804 @2248804/ <h> Introducing the Fresh Food Finder , an open source PhoneGap application <p> The Fresh Food Finder is an open source mobile application built using PhoneGap ( Apache Cordova ) that helps users locate local farmers markets that are registered with the FDA. - You can search for specific markets , or find the closest markets to your current location . - Check out the video below to see it in action : <p> It was originally intended to just be a sample application for- app-UI- with full source available here , - but happens to be quite popular and useful in the " real world " as well . <p> In fact , the Fresh Food Finder made it all the way up to #18 in the iPad " Lifestyle " category on iTunes in the first week of its release , and even made one of the featured apps in the Lifestyle category : <p> All of the information displayed within the Fresh Food Finder is freely available from the US Department of Agriculture through data.gov . This data set was last updated on April 25 , @ @ @ @ @ @ @ @ @ @ the Fresh Food Finder is improved data , with market schedules . I 've heard everyone loud and clear , and am happy to say that I have some improved data on the way ( including schedules and times ) , so keep an eye out for it in the not-to-distant future . <p> The Fresh Food Finder is written entirely using HTML , CSS , and JavaScript , and runs on numerous platforms . - It is currently available for iOS and Android . I 've submitted it for approval in the Windows Phone Marketplace , but its still awaiting approval . - I 've also tested it on the BlackBerry Playbook , and it works great there too , but I just have n't gotten around to submitting it to BlackBerry App World yet . <p> The Fresh Food Finder can be downloaded today in the following markets : <p> PhoneGap : http : //www.phonegap.com- PhoneGap is an HTML5 app platform that allows you to author native applications with web technologies and get access to APIs and app stores . <p> App-UI : http : **29;3351;TOOLONG App-UI is a free &amp; open @ @ @ @ @ @ @ @ @ @ components that may be helpful to web and mobile developers for creating interactive applications using HTML and JavaScript , especially those targeting mobile devices . <p> Mustache : https : **30;3382;TOOLONG - Mustache is a logic-less template syntax . It can be used for HTML , config files , source code anything . It works by expanding tags in a template using values provided in a hash or object . <p> The entire user interface of the application is created dynamically at runtime based on JavaScript and the Mustache templates. - You can download the full application source code at- https : **38;3414;TOOLONG - Feel free to fork it , or use it as a learning tool to build UI experiences for PhoneGap applications . <p> The code is organized into the following structure : <p> assets This folder contains fonts , images , and CSS styles used within the application . <p> js - This folder contains JavaScript resources and libraries used within the application . <p> When the application loads , all templates are loaded into memory as part of the bootstrapping/startup process . - Once all the @ @ @ @ @ @ @ @ @ @ is presented to the user . The majority of the application logic is inside application.js , all views are rendered from the Mustache templates inside of viewAssembler.js , and all UI styling is applied via CSS within styles.css . <p> Mustache is a templating framework that enables you to easily separate presentation layer ( HTML structure ) from application logic and the data model . - Basically , you create templates that Mustache will parse and convert into HTML strings based upon the data that gets passed in. - I 'll write another post later about Mustache , but it can be extremely useful for larger applications . <p> I cant run it on windows phone . I created a cordova 1.9 project , copied the files , edited the cordova link ( 1.7 -&gt; 1.9 ) , uncommented the css for IE. I got only a grey screen . How did you compile for WP ? 123436 @qwx983436 <p> I have a separate code fork for Windows Phone , which I have not release yet b/c it has not been approved my Microsoft yet . Once that is live , @ @ @ @ @ @ @ @ @ @ changes I had to make to get it to work in Windows Phone . <p> pamarcan <p> can you share your work with windows phone ? I cant understand , how do you make leaflet pan working ? <p> Thank you Andrew , I will give my contribution when you release the source <p> Sirko <p> Hi Andrew , really cool Application . I am new at PhoneGap and want to run the application on my phone ( HTC Desire HD ) . I have placed the folders in the structure in my Eclipse project , but there 's nothing else like a " Hello World " . What should I do and where I have to place all the files ? ! Thanks ! , Sirko <p> or do you have a good step-by-step guide for the installation of PhoneGap ? On the PG webiste I visit the " Getting started " - Guide , but it has a other structure like your folders ? ! Thanks again , Sirko <p> Sirko <p> Hi Andrew , thank you for your tip . It was very helpful and the app is @ @ @ @ @ @ @ @ @ @ Bailey <p> Hi , Andrew . In the description for this app ( both in this write-up and in iTunes ) you say that this only shows markets that are registered with the FDA . I think you mean , USDA , as were the group that maintains the National Farmers Market Directory database your app is based on . <p> Try doing a project clean . In eclipse , go to the " Project " menu , then select " Clean " . If you 've set it up correctly , it probably just needs to repackage everything now that you 've added the files . This sounds like it just does n't  have the contents of the www directory embedded . <p> Nice app , which version of cordova are you using ? I noticed there is not orientation feature . <p> I am working on a project and my orientation landscape is not showing full width .. maybe you know why . <p> Anyways .. nice work . <p> Regards , 123436 @qwx983436 <p> I do n't  recall exactly which version , but I think this is PhoneGap 2.0 @ @ @ @ @ @ @ @ @ @ 6 , you should update to PhoneGap 2.2 , and it fixes the iOS-6 related issues . If its happening on other OS versions/platforms , then its probably a HTML/CSS layout issue . <p> Hello there great app , ive been playing around with source code its pretty interesting but i just ca n't find out in which way are you using phonegap , i cant find the cordova file on the project or phonegap but the project runs fine , without the geolocalization service but i guess its because i 'm testing it on a desktop browser ( safari ) . <p> Well thanks in advance for your time and thanks for this nice WORK ! ! 123436 @qwx983436 <p> The cordova.js file is not included in the project . I 'm using Geolocation and " back " events ( for android ) . Its not using any other phonegap APIs . The example is primarily focused on an app-like user experiences , as opposed to specific API requests . If you look at index.html , it does include the Cordova.js file . <p> I noticed that after installing your android app @ @ @ @ @ @ @ @ @ @ icon on my homescreen . I am using phonegap 2.3.0 with your source code , and everything seem fine , but I do not get a shortcut icon on my homescreen ( android ) after installation . How can I achieve this ? Is there any plugin or permission i need to set in the config.xml file ? <p> Regards . 123436 @qwx983436 <p> I do n't  recall doing anything in particular . Are you installing from the marketplace , or a local install ? If you 're doing a local install , this might only happen with apps downloaded from the android market . <p> Lorant <p> Hello , <p> Any news regarding the Windows Phone version ? I need to create an app for iOS , Android and WP8 . It would be very helpful to share your experiences with WP8 . IE10 should now be capable of everything iOS and Android is. 123436 @qwx983436 <p> The app was approved for content , but rejected b/c I do n't  have any privacy/legal policy ( Windows Store is VERY strict about apps that use GPS ) . I have n't made @ @ @ @ @ @ @ @ @ @ new build of the Fresh Food Finder Application soon , and will be re-attempting Windows Phone submission . <p> Sven <p> Hi Andrew , first of all , thank you for sharing the source code of the fresh food finder . Its a great help for me building my first app using phone gap . <p> Since I just started learning your code I still try to get the overview of all files and scripts and how they work together . I was wondering where the design of the maps comes from . Where is it defined ? Somewhere in the leaflet files ? <p> Hi , Thanks for app example . I have download app from play store . App work fine . I have build app with phonegap build service linking git repository with source code . The resouult is not same . Second app is very slow in navigation . <p> Why ? How is build app in play store ? 123435 @qwx983435 <p> I built it locally in Eclipse , but the code is identical . What are you seeing for " very slow in navigation @ @ @ @ @ @ @ @ @ @ a project-specific setting that you are hitting ? <p> I do n't  understand , do n't  understand ! How is possible ? Build whit eclips or whit phonegap build service is not the same ? <p> Can add you how collaborator in phonegap buil so can view the problem ? <p> Thanks 123435 @qwx983435 <p> They are exactly the same for *my* source code . The difference would likely be which version of the Android SDK that was used to compile the app . I did that app a while ago , so its probably using an older version of the Android SDK . If you set the SDK version to " 8 " support older devices ( Android 2.3 ) , it will use an older SDK . This is in the " android-minSdkVersion " tag in config.xml . See https : **36;3481;TOOLONG for details <p> Andrea <p> I have try , but equals problem <p> My device is a new galaxy tab 2 . Do n't  is a problem of device performance . <p> Is *your* sorce code equals at git code repository ? <p> Thanks <p> Andrea @ @ @ @ @ @ @ @ @ @ phonegap plugin for this app ? <p> I 'm doing an article on privacy , cell phones and location . Thanks for uploading this source code , it saves me some time . I 'm a law student and former programmer . I 've had a chance to play around with Windows Phone and Android , but I 'd like some information on iPhone and Blackberry . It appears there are similarities between them all . Correct me if I 'm wrong , but you put notice in the header ( or manifest ) that you want to use location services , and that is all you need ? Is there a requirement to ask the user before using location services ? Thanks , David <p> Emma <p> Hi , first , Thank youu andrew for this application the app is work but when i shearch for location , the location is not displayed ? ? ? thanks <p> Anibal Centurion <p> awesome ! ** <p> Trish <p> Do you have a script or anything that can do that ? Where you may have bunched up all he regular expressions ? 
@@106848806 @2248806/ <p> Weve been able to write native iOS apps leveraging the scaffolding and analytics of the IBM MobileFirst Platform Foundation Server for a while now . This was first introduced way back when MobileFirst still went by the Worklight name , serveral versions ago . <p> As I would write apps , one thing I really wanted was to use code blocks instead of having to implement delegate classes every time I need to call a procedure on the MobileFirst server . - In MobileFirst 7.0 , the new WLResourceRequest- API allows you to invoke requests using either the completionHandler ( code block ) or delegate implementations . <p> But what if you 're still using an earlier version of the MobileFirst platform , or what if you still want to leverage your existing code that uses- **25;3519;TOOLONG parameters , but do n't  want to have to create a new delegate for every request ? - Well , look no further . - I put together a very simple utility class that helps with this task by allowing you to pass code blocks as parameters for the requests to the @ @ @ @ @ @ @ @ @ @ prefer code blocks b/c they allow you to encapsulate functionality inside of a single class , instead of having logic spread between a controller and delegate class ( and having to worry about communication between the two ) . <p> The other getLoggerForInstance utility function is just a shortcut to get an OClogger instance with the package string matching the class name of the instance passed , with just a single line of code : 
@@106848807 @2248807/ <h> Significant Advances in the Consumer Drone Market <p> Lately Ive been so focused on mobile , apps , development , conferences , and more that I have n't posted much besides IBM work news and projects . - Well , I 'm taking a break for just a moment <p> If you 've followed my blog for a while , then you already know that I 'm pretty much obsessed with " drones " . - It is- by far the most fun and exciting recreation- that Ive taken up in a very long time . Not only are they fun to fly , but they get you into some amazing views that were previously inacessible , and have applications far beyond just taking pictures . - I 've written how-tos for aerial photography and videography , taken tons of pictures for fun , and even shot some indoor footage for TV commercials . <p> I 'm always following the news feeds , watching the advances in technology , watching prices drop , and- am continually blown away- by what the industry is offering . - The last week to ten days have been @ @ @ @ @ @ @ @ @ @ the latest from- DJI , who announced the Phantom 3- - a consumer- drone with some very impressive specs and performance . <p> The Phantom- 3 - is an easy to fly copter that sports a 3-axis gimbal ( camera stabilizer ) , up to 4K video footage , an integrated rectilinear ( flat ) lens camera , live HD first-person view , integrated iOS and Android apps , a vision positioning system- ( for stabilized indoor flights ) and up to a 1.2 mile flight range . - All for a cost of under $1300 USD. - Thats one heck of a package , and officially makes my old- Phantom look like a dinosaur . <p> 3 Days later , 3D Robotics announced the Solo , a direct competitor to the Phantom . The- Solo- is also very impressive , and has already- won an- award- for Best Drone- at NAB in Las Vegas . <p> The Solo also has a 3-axis gimbal for stabilized footage , and is designed to work with GoPro cameras . - In fact , it is the only copter that integrates with the @ @ @ @ @ @ @ @ @ @ Solo also has dual processors ( one in the controller , one in the copter ) , HD first person view , and has- an upgradeable system that can have- new camera systems or payloads configured . - It does n't  have an optical stabilization system built in , but that can be added to the expansion bay . - What really sets the Solo apart is the intelligent auto-pilot sytem that appears to make complex shots very easy. - All of this with a price tag starting at $1000 USD . <p> I currently own DJI products , but this has gotten me- seriously considering a purchase . <p> Both of these are small aircraft- targeting consumers , but from the look of it they are definitely capable of high end applications . - Their small size make them extremely portable , and a potential add in many industries and use cases . - Larger copters are always available for larger scale applications . <p> Let 's not forget drones for the enterprise - Last week Airware- launched their drone operating system . - Business can now license their operating @ @ @ @ @ @ @ @ @ @ , people everywhere still freak out over drones as a political debate , ignoring their utility and positive value . The rules for commercial use continue to shake out , but oh man , its an exciting time . 
@@106848808 @2248808/ <h> All posts by Andrew <p> HTML5 Developer Conference just wrapped up in San Francisco , and it was a great event . There was a lot to see and hear from the entire HTML/JS community . Adobe showed off some amazing showcases of the work that is being done with rich layout on the web and released an awesome new SVG library , Snap.svg. - I also had two sessions on PhoneGap . <p> First , news and announcements from Adobe <h> Snap.svg <p> Adobe has released Snap.svg , the JavaScript SVG library for the modern web . - Snap.svg is focused on making the most out of everything that SVG can offer , including masking , clipping , patterns , gradients , groups , and much more . - It is definitely worth checking out . <h> My Sessions on PhoneGap <p> Last , but certainly not least , I had two sessions on PhoneGap : one an intro , and the other a more advanced architecture topic . - Thanks to everyone who came out for my sessions . You were a great audience , and @ @ @ @ @ @ @ @ @ @ recorded by the conference organizers , and will be available at a later date . <p> You can access my presentation slides in the links below ; just use the space bar to advance each slide : <p> However , I must also apologize that a few of my samples in the " Getting Started with PhoneGap and Cross-Platform Mobile Development " did not work . I was connected to the network , but was n't able to receive any data , so I could n't access PhoneGap Build , or even add device features to a PhoneGap project from the command line tools . - I promise , these features do work when you 're on a reliable network connection . Go check out phonegap.com to learn more and get started today . <p> This week I was in the video studio recording some content for Adobe Inspire magazine on creative uses of quadcopters , GoPros and Creative Cloud for aerial photography and videography . Adobe Inspire is a great place to get new ideas or learn tips and tricks with Adobe Creative Cloud tools . Subscribe to the free Adobe Inspire @ @ @ @ @ @ @ @ @ @ . <p> My series will be released in early 2014 , but here are a few teasers to whet the appetite . I had a blast with this shoot , and cant wait for the articles and videos to be released radio-controlled aerial photography is my latest hobby/obsession . <p> In the studio : <p> Video from the shoot : Skies above the Adobe office in San Francisco . Captured with a DJI Phantom and GoPro : <p> If you look really closely , you can see us standing on top of the parking garage to the back left of the Baker &amp; Hamilton sign . <p> Subscribe- today or download the- iOS app- to be notified once this is live in Inspire . Now , go get creative and do amazing things ! <p> The recording for my session " PhoneGap and Hardware " from PhoneGap Day back in July is now available ! Be sure to check it out . There were apparently some issues with the audio , but you can still hear everything . <p> I 'd like to express a huge Thank You- to everyone who @ @ @ @ @ @ @ @ @ @ <p> Below are the sample projects I showed in the presentation , including source code . However , keep in mind that all of these examples were written before PhoneGap 3.0 . The native plugin syntax , and inclusion methods have changed . <h> Pressure Sensitive Sketching in PhoneGap <p> In this example , - the pressure-sensitive Pogo Connect Stylus uses a low energy Bluetooth 4 connection to relay touch/pressure information back to the PhoneGap application . This makes for a unique drawing and sketching experience powered with the HTML5 Canvas element . I 've written about this example previously Check out the video below to see it in action , and read the blog post for technical details and source code . <h> Moga Gamepad <p> The second example that I explored is a PhoneGap native plugin that is used to handle input from a Moga game controller inside of a PhoneGap application on Android . <p> This implementation is intended to be a proof of concept demonstrating how you could integrate the gamepad within your application . It currently only supports input from the joysticks ( axisX and axisY @ @ @ @ @ @ @ @ @ @ does not handle all possible input from the controller . <p> This implementation is adapted directly from the **38;3546;TOOLONG example from the Moga developers SDK samples available for download at : - http : **34;3586;TOOLONG 123434 @qwx983434 <p> The game is based on the Universe- prototype that was used as a sub-game inside of the MaxMe app for the recent Adobe MAX conference . I make no guarantees about the code for this game , it was in a huge rush ! <p> Ive been spending a lot of time with Photoshop recently Whether it has been retouching video or images , creating panoramas , or working with my aerial photos , it has been a lot of fun . One thing that I 've been doing is exporting really large images to the web . So far this has been a very manual process Export from Photoshop using Zoomify . Then , since the default Zoomify renderer uses Flash ( and I want this consumable on mobile devices ) , take the Zoomify image tiles , and put them into a custom-coded HTML experience using the Leaflet tile- engine with @ @ @ @ @ @ @ @ @ @ for web-based mapping , but it is a perfect solution for rendering image tiles on the web . It already has touch and mouse interactions , inertial scrolling , progressive viewing , and a comprehensive API that can be extended if you so choose . <p> I 've done this enough times that I figured " There has to be an easier way " and there definitely is . I 've created a new Zoomify template that allows you to export from Photoshops Zoomify feature directly to HTML , leveraging the Leaflet engine . All of the code and installation instructions are below in this post . Check out the video below to see it in action : <h> Samples <p> Here are few samples from the Zoomify output ; both are the compositions that I showed in the video above . Use the mouse or touch interactions to pan and zoom on each of them . <p> The first is an export from a 10MP aerial panorama ( 4340+2325 pixels ) , which was created by stitching together multiple images captured with a GoPro camera and remote controlled helicopter . <p> @ @ @ @ @ @ @ @ @ @ 14561+9570 pixels ) . I created this by stitching together 48 10mp images in Photoshop . Its not perfect , but shows how far you can zoom into an image some images had different exposures , some were out of focus , there is still some perspective warp , and I definitely have some bad stitching seams . This image is so huge that I actually maxed out the system RAM , and filled up all hard disk space with the memory swap file when creating it ( I had over 100 Gigs of free space ) ! <p> Extract the zip file and copy the following files to Photoshops Presets/Zoomify directory . On OS X , with the default configuration , these files should be located in /Applications/Adobe Photoshop CC/Presets/Zoomify/ <p> L.TileLayer.Zoomify.js <p> Zoomify Leaflet HTML.zvt <p> leaflet.css <p> leaflet.js <p> Restart Photoshop . <p> When you have a file open that you want to export , choose File -&gt; Export -&gt; Zoomify- <p> Then select the " Zoomify Leaflet HTML " template that should now be in the list . Select an output location , base name , @ @ @ @ @ @ @ @ @ @ Ignore the browser width and height , since the template ignores these . Instead , it takes 100% of the width and height of the browser window . <p> - This will generate all of the image tiles and the HTML structure . From here , do whatever you want with it You can modify it , put it on a server , or anything else . the file output will look something like the image below . You will have a folder that contains the generated HTML file , the Leaflet JS and CSS files , and a directory that contains the generated tiles and appropriate XML metadata . <p> I 've posted about GoPro cameras here before I love mine b/c it is so versatile . You can mount it on just about anything underwater , on the ground , or in the air . There are tons of accessories for mounting it all kinds of different ways . However , the cost of these mounts can quickly add up . The adhesive mounts are essentially 1-use mounts . Once you stick them to something , they are firmly @ @ @ @ @ @ @ @ @ @ times . If you want a reusable mount , GoPro has their own clamp mount , but is also a bit pricey if you have other accessories . here 's how you can create an inexpensive and extremely versatile clamp mount . <p> I already had a frame mount , and a tripod mount for the GoPro , so I already had half of the equation . ( Yes , if you have to go out and purchase both of these , its just as much as GoPros clamp mount , but this approach is far more versatile when you have all of them . ) <p> To turn this into a clamp mount , you just need to add the clamp and a mounting point , which can be purchased at a hardware store for just a few dollars . You 'll need a clamp ; any size will do , you just need one big enough to put a 1/4 inch bolt through the handle . Generally , the bigger the clamp , the stronger the spring will be to hold it closed . The one I used is pretty @ @ @ @ @ @ @ @ @ @ You 'll also need a 1/4 inch bolt , some washers , and some 1/4 inch nuts . I used a thumb screw with two wing nuts so that it is easy to adjust in any situation without any tools . <p> Clamp Mount Parts <p> Just drill a hole through the clamp handle big enough to fit the 1/4 inch thumb screw . Put a washer on either side , and tighten it with one of the wing nuts . Then put the other wing nut in the opposite direction , followed by the tripod mount . Use this second wing nut to tighten the tripod mount in the correct position . Next , just attach the " arms " that come with the GoPro , and attach the camera inside of the frame mount . Voila you know have an extremely versatile clamp mount for the GoPro . This will also work with the waterproof enclosure . <p> DIY Clamp Mount <p> Now , get out there , capture some great images and videos , and then use Creative Cloud to bring out the best in them . You @ @ @ @ @ @ @ @ @ @ , color correction , and much , much , more on both images and videos with Photoshop , Lightroom , and all of the tools that Creative Cloud offers . 
@@106848809 @2248809/ <h> Tag Archives : development <p> That title get your attention ? - Yes , it really read " Adaptive- mobile- apps that- change based on personal context " with near real-time rules application , without much extra development effort . - If that sounds interesting to you , or like a product you might want to use within your own apps , then you might want to check out this site where you can get involved in the products development : - http : **35;3622;TOOLONG <p> IBM is looking for your input on creating these types of mobile app experiences. - User experiences within a single app that can be- dramatically different per user based on location , past behavior , profile information , social media activity , - and so much more . - With this behavior being driven by configurable rules that can be changed without redeploying an app to the app store . <h> How it works for your customer <p> Consider this scenario : <p> Jon and Andrea download the mobile app for S&amp;W , a retailer known for its attention to providing great @ @ @ @ @ @ @ @ @ @ Andrea use the app to browse and discover content and merchandise differently . <p> Jon primarily navigates to sports related content for his favorite teams to find gear and clothes for travel to his favorite team 's games . Andrea scours the app for sales and fashion trends and usually ends up following her favorite designers . <p> Andrea and Jon go to a baseball game together . She 's never enjoyed watching it , so she opens up the S&amp;W app to entertain herself , and her app 's navigation quickly steers her through Spring fashion articles . <p> Jon however , wants to replace the hat he 's worn the last three times the team lost , and since he 's in the stadium , his S&amp;W app opens right up to the team 's gear page . The app knows he 's out of town and tells him how to get to an S&amp;W store . <h> How it works for the dev team <p> Consider another scenario : <p> One of the developers on the team , George , sets up the system and application . @ @ @ @ @ @ @ @ @ @ the customer experience . <p> Janet writes rules defining how the application could adapt and become more personalized based on inputs like , social media , geolocation , app usage , or customer information data . <p> Once Janet has built out her rules , she simply hits Submit ' and can immediately see her clever interactions reflected in the mobile application without having to involve the development team . <p> Analytics let Janet know which adaptations are working best , and helps her find new opportunities to optimize the app 's user experience . <p> Were not talking about a content management system , or translation based on locale , instead a rules-driven product that can adapt literally every aspect of your app : - customize the user interface , enable or disable different features , customized messaging and notifications , and much more , all variable based upon the user context . - This can be used to present- contextually relevant information , drive adoption , provide more/less data depending on your physical context , and so much more . <p> It wont be tied to a specific @ @ @ @ @ @ @ @ @ @ management system , is n't attempting to re-create Google Now or Apple Proactive Assistance . - Rather , a set of tools and a rules engine that enable you to customize and tailor the app experience to the individual user . <p> Last week I had the opportunity to present to a great audience at- the- MoDev DC meetup group on " Smarter Apps with Cognitive Computing " . - In this session I focused on how you can create a voice-driven experience in your mobile apps . I gave an introduction to IBM Bluemix and IBM Watson services ( particularly the Watson language services ) , and demonstrated how you can integrate them into your native iOS apps . I also covered IBM MobileFirst for operational analytics and- remote logging to provide insight into your apps performance once it goes live . - Check out a recording of the complete presentation in the video below : <p> Back in February I had the opportunity to present " Enabling the Next Generation of Apps with IBM MobileFirst " at the DevNexus developer conference in Atlanta . - It was a great @ @ @ @ @ @ @ @ @ @ Luckily for everyone who was n't able to attend , the organizers recorded most of the sessions which have just- been made available on Youtube . <p> here 's the session Description : Once your app goes live in the app store you will have just entered into an iterative cycle of updates , improvements , and releases . Each successively building on features ( and defects ) from previous versions . IBM MobileFirst Foundation gives you the tools you need to manage every aspect of this cycle , so you can deliver the best possible product to your end user . In this session , we 'll cover the process of integrating a native iOS application with IBM MobileFirst Foundation to leverage all of the capabilities the platform has to offer . <p> If you think that voice-driven apps are too complicated , or out of your reach , then I have great news for you : They are not ! Last week , IBM elevated- several IBM Watson voice services from Beta to General Availability that means you can use them reliably in your own systems too ! <p> Let 's @ @ @ @ @ @ @ @ @ @ what solutions IBM has available right now for you to take advantage of <p> Transcribe audible signal to text transcript <p> Part one of this equation is converting the audible signal into text that can be parsed and acted upon . The IBM Speech to Text service fits this bill perfectly , and can be called from any app platform that supports REST services which means just about anything . It could be from the browser , it could be from the desktop , and it could be from a native mobile app . The Watson STT service is very easy to use , you simply post a request to the REST API containing an audio file , and the service will return to you a text transcript based upon what it is able to analyze from the audio file . With this API you do n't  have to worry about any of the transcription actions on your own no concern for accents , etc Let Watson do the heavy lifting for you . <p> Perform a system action by parsing text transcript <p> This one is perhaps not quite @ @ @ @ @ @ @ @ @ @ upon what you/your app is trying to do . You can parse the text transcript on your own , searching for actionable keywords , or you can leverage something like the IBM Watson Q&amp;A service , which enables natural language search queries to Watson data corpora . <p> Riding on the heels of the Watson language services promotion , I put together a sample application that enables a voice-driven app- experience on the iPhone , powered by both the Speech To Text and Watson Question &amp; Answer services , and have made the mobile app and Node.js middleware source code available on github . <p> The app communicates to the Speech to Text and Question &amp; Answer services through the Node.js middelware tier , and connects directly to the Advanced Mobile Access service to provide operational analytics ( usage , devices , network utilization ) and remote log collection from the client app on the mobile devices . <p> For the Speech To Text service , the app records audio from the local device , and sends a WAV file to the Node.js in a HTTP post request . The @ @ @ @ @ @ @ @ @ @ to provide transcription capabilities . The Node.js tier then formats the respons JSON object and returns the query to the mobile app . <p> For the QA service , the app makes an HTTP GET request ( containing the query string ) to the Node.js server , which delegates to the Watson QA natural language processing service to return search results . The Node.js tier then formats the respons JSON object and returns the query to the mobile app . <p> The general flow between these systems is shown in the graphic below : <p> IBM Watson Speech QA for iOS Logic Flow <h> Code Explained <p> The code for this example is really in 2- main areas : The client side integration in the mobile app ( Objective-C , but could also be done in Swift ) , and the application server/middleware implemented in Node.js . <h> Node.js Middleware <p> The server side JavaScript- code uses the- Watson Node.js Wrapper , which enables you- to- easily instantiate Watson services in- just a few short lines of code <p> The credentials come from your Bluemix environment configuration , then you @ @ @ @ @ @ @ @ @ @ consume . <p> I implemented two methods in the Node.js application tier . The first accepts the audio input from the mobile client as an attachment to a HTTP POST request and returns a transcript from the Speech To Text- service : <p> Note : I am using the free/open Watson Healthcare data set . However- the Watson QA service can handle other data sets- these- require an engagement with IBM to train the Watson service to understand the desired data sets . <h> Native iOS Objective C <p> On the mobile side- were working with a native iOS application . My code is written in Objective C , however you could also implement this using Swift . I wont go into complete line-by-line code here for the sake of brevity , but you can access the client side code in the ViewController.m file . In particular , this is within the postToServer and requestQA methods . <p> You can see the flow of the application within the image below : <p> App Flow : User speaks , transcript displayed , results displayed <p> The native mobile app first captures @ @ @ @ @ @ @ @ @ @ to the Node.js servers /transcribe method- as an attachment to- a HTTP POST request- ( postToServer method on line 191 ) . On the server side this delegates to the Speech To Test service as described above . Once the result is received on the client , the transcribed text is displayed in the UI and then a request is made to the QA service . <p> In the requestQA method , the mobile app makes a HTTP GET request to the Node.js apps /ask method ( as shown on line 257 ) . The Node.js app delegates to the Watson QA service as shown above . Once the results are returned to the client they are displayed within a standard UITableView in the native app . <h> MobileFirst Advanced Mobile Access <p> A few other things you may notice if you decide to peruse the native Objective-C code : <p> Within AppDelegate.m you will see calls to IMFClient , IMFAnalytics , and OCLogger classes . These enable operational analytics and log collection within the Advanced MobileAccess service . <p> I 'm sure you 've already heard Apples big announcements from the- @ @ @ @ @ @ @ @ @ @ lucky enough to snag a ticket in Apples lottery and got to check it all out in person . There were lots of great sessions , with tons of content . - Here are the highlights as I saw them from a mobile developers perspective *not* from the general consumer point of view . - For the most part , I think this years announcements highlighted the evolution and maturity of existing products and projects no new amazing breakthoughs , but definitely steps in the right direction . <p> If you have n't seen them already , the Keynote and the Platforms State of the Union videos cover most of the- announcements , but not in complete detail . Just be warned , the Keynote is loaded with product marketing fluff , not just developer topics . - Once you- get to " weve got one more thing " you can turn off the Keynote the Apple Music announcement has pretty much zero- significance for- developers . <p> So let 's get started <h> Swift 2.0 <p> There was a tremendous emphasis on the Swift language at this years WWDC event . @ @ @ @ @ @ @ @ @ @ be open sourced , plus many language enhancements , and nearly every piece of sample code that was shown was written in Swift . - It is very clear that Swift is Apples direction moving forward . <p> I think the open souring of Swift is a big deal b/c it opens up the language for use beyond just iOS and OSX applications . - Think about it Perhaps another platform might adopt Switft to develops apps ( Windows ? ) , or let 's hypothetically say you really like Node.js on the backend b/c its the same language as your web front end ( JavaScript , that is ) . What if you are developing native apps , and youd like to write your back end in the same language as the front end mobile client , or what if you want an ECMAScript inspired language that is more structured than Node , with real Object Oriented or functional programming constructs ( and what if you want something that is really multi-threaded ) ? - Swift is your answer . I 'm willing to bet that we will see server-side Swift @ @ @ @ @ @ @ @ @ @ just hope that Swift is opened in the truest sense you know , actually accepting input and contributions from external parties . <p> The Swift language itself has also evolved quite significantly . - Better error handling , protocol extensions , and improved performance are a great start . - Heck , if I understood one of the speakers correctly , its now even faster than Objective C at runtime in some cases . <p> Want to learn more about Swift ? - Check out these session videos from WWDC ( requires Safari ) : <h> OS- Improvements <p> New versions of both OS X and iOS were announced and released to - developers OS X El Capitan and iOS 9 respectively . - Both seem to be incremental updates of the previous OSes . New apps , new features , etc for the end users . - Not necessarily significant changes for developers . - If you 're a graphics programmer , Metal- will be a big deal for you ( low level graphics/gpu API ) , but if you 're not a graphics guru , you probably wont even know @ @ @ @ @ @ @ @ @ @ 9 **25;3659;TOOLONG mode for iPad is going to be a great addition which brings the iPad even closer to being a full laptop replacement . - Having the ability to have multiple apps open next to each other will improve the- iPads- " get $h1t done " ability . - Youll have to ensure that youve- authored your apps to leverage adaptive layouts , but that 's pretty much all that you need to do to take advantage of iPad Multitasking . <p> These videos will get you going in the right direction for iOS multitasking and adaptive layouts : <h> App Thinning <p> The new " App Thinning " features in Xcode 7/iOS 9 are also a great addition . - Currently if you build an iOS app it gets bundled with lots of resources that may never be used depending on the type of device . - App thinning introduces three concepts that help minimize the footprint and increase the quality of your installed apps : App Slicing , On Demand Resources , and Bitcode. - According to the presenters , these can decrease the download/installed size of your @ @ @ @ @ @ @ @ @ @ feature that creates variants of your app executable depending on the device that you are downloading the app to . So , if your app does n't  use @3x graphics , or does n't  use the arm7s architecture on a particular device , then they wont be downloaded. - Likewise , if your device- does- leverage- those assets , then the other smaller scale assets and non-used binaries wont be downloaded . <p> App Slicing from iOS Docs <p> On Demand Resources- give you the ability to download specific sets of resources from the app store as they are needed . - They are still hosted by the app store , but not part of the initial download. - Let 's say you are building a platform game . - Initially the shell/navigation assets will be downloaded. - While the app is running you 'll be able to download assets for level 1 , level 2 , level 3 , etc incrementally as they are needed . - The system can also clean up ODR resources to conserve space using a least-recently-used cleanup routine . <p> Bitcode is an intermediate representation of @ @ @ @ @ @ @ @ @ @ that contain bitcode will be compiled and linked on the App Store . Including bitcode will allow Apple to re-optimize your app binary in the future without the need to submit a new version of your app to the store . <p> Bitcode enables the app store to re-compile your code to take advantage of new LLVM optimizations without you even having to recompile and upload a new application binary . <h> UI Testing <p> The new UI testing features in Xcode 7 look pretty awesome as far as automated UI testing goes . - It enables you to record/playback steps and generated UI unit tests all from within Xcode. - What 's even better , it enables you to set breakpoints within your tests , so you can debug why your tests might be failing , or you can set breakpoints inside of your app , and the automated testing stops at the breakpoints and allows you to step through code while inside the automated unit test . - Definitely do not miss the session on UI Testing in Xcode 7 if you have any ( even remote ) interest @ @ @ @ @ @ @ @ @ @ . <h> Improved Search and Deep Linking <p> Improved search functionality was also announced for both iOS and OS X. - This improves the search functionality , and also enables your apps to index their content , so using the device search enables you to search for information hosted *inside* of the app. - To complement the enhanced search , there are also features that better facilitate deep linking into your app. - This enables apps to be launched directly into the appropriate content/context with greater ease . - I need to look into this more , but it sounded interesting <h> watchOS 2 <p> Last , but by certainly no means least , the announcement of watchOS 2- looks like a massive leap forward for developing for the Apple Watch . <p> WatchOS 2 brings us the ability to execute code natively on the- Apple Watch , not just in the WatchKit extension running on your iPhone , brings us the ability to implement custom watch complications , - access to network connectivity if your phone is not connected , - support for multimedia , - and direct access @ @ @ @ @ @ @ @ @ @ watch complications " are , they are the widgets on the watch face that enable you to display customized information . <p> WatchOS Complications <p> You should definitely check out the videos on developing for the Apple Watch if you have any interest in- watchOS : <p> There are also new APIs , enhanced features in CloudKit , MapKit , HomeKit , Core Motion , Core Location , updates to Apple Pay , security updates , networking updates , and lots more . - Be sure to check out the complete list of WWDC videos- for more . <p> There was so much to absorb , I 'm sure I missed something , so feel free to point anything out that I 've overlooked ! 
@@106848810 @2248810/ <p> I recently gave a presentation at IBM Insight on Cognitive Computing in mobile apps . - I showed two apps : one that uses Watson natural language processing to perform search queries , and another that uses Watson translation and speech to text services to take text in one language , translate it to another language , then even- have the app play back the spoken audio in the translated language . - Its this second app that I want to highlight today . <p> In fact , it gets much cooler than that . - I had an idea : " What if we hook up an OCR ( optical character recognition ) engine to the translation services ? " That way , you can take a picture of something , extract the text , and translate it . - It turns out , its not that hard , and I was able to put together this sample app in just under two days . - Check out the video below to see it in action . <p> To be clear , I ended up using a @ @ @ @ @ @ @ @ @ @ . This is not based on any of the work IBM research is doing with OCR or natural scene OCR , and should not be confused with any IBM OCR work . - This is basic OCR and works best with dark text on a light background . <p> The Tesseract engine let 's you pass in an image , then handles the OCR operations , returning you a collection of words that it is able to extract from that image . - Once you have the text , you can do whatever you want from it . <p> So , here 's where Watson Developer Cloud Services come into play . First , I used the Watson Language Translation Service to perform the translation . - When using this service , I make a request to my- Node.js app running on IBM Bluemix ( IBMs cloud platform ) . - The Node.js app acts as a facade and delegates to- the Watson service for the actual translation . <p> You can check out a sample on the web here : <p> Translate english to : <p> On the mobile client , @ @ @ @ @ @ @ @ @ @ something with the response . The example below uses the IMFResourceRequest API to make a request to the server ( this can be done in either Objective C or Swift ) . IMFResourceRequest is the MobileFirst wrapper for networking requests that enables the MobileFirst/Mobile Client Access service to capture operational analytics for every request made by the app . <p> Once you receive the result from the server , then you can update the UI , make a request to the speech to text service , or pretty much anything else . <p> To generate audio using the Watson Text To Speech service , you can either use the Watson Speech SDK , or you can use the Node.js facade again to broker requests to the Watson Speech To Text Service . In this sample I used the Node.js facade to generate Flac audio , which I played in the native iOS app using the open source Origami Engine library that supports Flac audio formats . <p> You can preview audio generated using the Watson Text To Speech service using the embedded audio below . Note : In this sample @ @ @ @ @ @ @ @ @ @ work in browsers that support OGG . <p> On the native iOS client , I download the audio file and play it using the Origami Engine player . This could also be done with the Watson iOS SDK ( much easier ) , but I wrote this sample before the SDK was available . <p> Cognitive computing is all about augmenting the experience of the user , and enabling the users to perform their duties more efficiently and more effectively . The Watson language services enable any app to greater facilitate communication and broaden the reach of content across diverse user bases . You should definitely check them out to see how Watson services can benefit you . <h> MobileFirst <p> So , I mentioned that this app uses IBM MobileFirst offerings on Bluemix . In particular I am using the Mobile Client Access service to collect logs and operational analytics from the app . This let 's you capture logs and usage metrics for apps that are live " out in the wild " , providing insight into what people are using , how they 're using it , and the @ @ @ @ @ @ @ @ @ @ <h> Source <p> You can access the sample iOS client and Node.js code at https : **38;3686;TOOLONG . Setup instructions are available in the readme document . I intend on updating this app with some more translation use cases in the future , so be sure to check back ! <p> Last week I attended IBM Insight in Las Vegas . It was a great event , with tons of great information for attendees . I had- a few sessions on mobile applications . In particular , my dev@Insight session on Wearables powered by IBM MobileFirst was recorded . You can check it out here : <h> Key takeaways from the session : <p> Wearables are the most personal computing devices ever . Your users can use them to be notified of information , search/consume data , or even collect environmental data for reporting or actionable analysis . <p> Regardless of whether developing for a peripheral device like the Apple Watch or Microsoft Band , or a standalone device like Android Wear , you are developing an app that runs in an environment that mirrors that of a a native @ @ @ @ @ @ @ @ @ @ the same . You write native code , that uses standard protocols and common conventions to interact with the back-end . <p> Caveat to #1 : You user interface is much smaller . You should design the user interface and services to acomodate for the reduced amount of information that can be displayed . <p> You can share code across both the phone/tablet and watch/wearable experience ( depending on the target device ) . <p> Using IBM MobileFirst you can easily expose data , add authentication , and capture analytics for both the mobile and wearable solutions . <p> You may have heard a lot of buzz coming out of IBM lately about Cognitive Computing , and you might have also wondered " what the heck are they talking about ? " - You may have heard of- services for data and predictive analytics , services for natural language text processing , services for sentiment analysis , services understand speech and translate languages , but its sometimes hard to see the forest through the trees . <p> I highly recommend taking a moment to watch this video that introduces Cognitive @ @ @ @ @ @ @ @ @ @ interact naturally with people to extend what either humans or machine could do on their own . <p> They help human experts make better decisions by penetrating the complexity of Big Data . <p> Cognitive systems are often based upon massive sets of data and powerful analytics algorithms that detect- patterns and concepts that can be turned into actionable information for the end users . - Its not " artificial intelligence " in the sense that the services/machines act upon their own ; rather a system that provides the user tools or information that enables them to make better decisions . <p> The benefits of cognitive systems in a nutshell : <p> They augment the users experience <p> They provide the ability to process information faster <p> They- make complex information easier to understand <p> They enable you to do things you might not otherwise be able to do <p> Curious where this will lead ? - Now take a moment and watch this video talking about the industry-transforming opportunities that Cognitive Computing is already beginning to bring to life " <p> So , why is the " mobile guy @ @ @ @ @ @ @ @ @ @ its because Cognitive Computing is big I mean , really , really big . - Cognitive systems are literally transforming industries and providing powerful analytics and insight into the hands of both experts and " normal people " . - When I say " into the hands " , I again- mean this literally ; much of this cognitive ability is being delivered to those end users through their mobile devices . <p> Last , and this is purely just- personal opinion , I see the mobile MobileFirst offerings themselves as providing somewhat of cognitive service for developing mobile apps . - If you look at it from the operational analytics perspective , you have an immediate insight and a snapshot into the health of your system that you would never have seen otherwise . - You can know what types of devices are hitting your system , what services- are being used , how long things are taking , and detect issues , all without any additional development efforts on your end . Its not predictive analytics , but sure is helpful and gets us moving in the right direction . 
@@106848811 @2248811/ <h> Category Archives : AIR <p> I was recently asked by a friend and former colleague about the best way to get text within a s:Label to behave and scroll properly , especially in the Flex mobile SDK . In particular , having a large block of text wrap correctly and scroll only in the vertical direction . By default if you do n't  set a size on the label , the behavior of the Flex framework is that the views containing the label will resize , and the text will be displayed as entered ( without word wrap or truncation ) . This may cause some layout issues and confusion as to " what the heck is going on with my text " . <p> I 've found that the best way to achieve the desired behavior is to set a maxWidth on the label to force proper word wrapping , and then wrap the label in a s:Scroller to have it scroll properly . I chose to set a maxWidth to allow the label to determine its own size , and only to wrap if it needs to @ @ @ @ @ @ @ @ @ @ the maxWidth of the label to the width of the scroller component . Also , DO NOT set a static height or a max height . This will cause the text within the label to be truncated , and it will not scroll at all if the static height is less than the height of the scroller . I 've also noticed that setting cacheAsBitmap=true on the label also helps scroll performance in some circumstances , but this is not required . <p> Check out a video showing the scroll behavior of a large text block using this approach : <p> Below is the code that makes it work , which follows the method described above : <p> The recordings of my presentations do n't  seem to be available yet on Adobe TV , but here is the content , as promised . I spoke at MAX this year on " Multi Device Best Practices using Flex &amp; AIR for Mobile " , and " Create beautiful , immersive content and applications with HTML5 and CSS3 ? , and the content from these presentations is below . <h> Multi Device Best @ @ @ @ @ @ @ @ @ @ multi-device best practices session I covered the basics for building a **30;3726;TOOLONG application that conforms to device constraints ( phone and tablet ) , using a single codebase that is able to detect device dimensions and orientation . - This was followed by online/offline detection for occasionally-connected applications , and then followed by device-specific layout using CSS media queries and MultiDPIBitmapSource images . - The presentation slides are below . <h> Create beautiful , immersive content and applications with HTML5 and CSS3 <p> In this session , I gave a " crash course " in developing rich content experiences with HTML5 and CSS3. - I started with a general overview presentation , followed by diving directly into code . - I covered &lt;video&gt; , &lt;audio&gt; , dynamic graphics with &lt;canvas&gt; , &lt;svg&gt; , HTML5 Form elements , CSS3 Web Fonts , Visual Styles ( shadows , corners ) , CSS3 Color spaces ( RGBA , HSLA ) , graidents , transforms , animations , and media queries . - In the presentation , I also discussed the necessity of client-side solution- accelerator- frameowrks ( jQuery or other JS framework ) @ @ @ @ @ @ @ @ @ @ using Modernizr. - The presentation slides are below : <p> Although there were no official announcements around Flex , Flash &amp; AIR ( other than the release of FP11 &amp; AIR3 ) , do n't  think that the platform is going away or becoming stale In fact , it is quite the opposite . The Flash Platform will continue to thrive and innovate , providing outstanding solutions that set the pace for other technologies to follow . In case you missed the session , here is the " Flash Platform Roadmap " , provided by Scott Castle , Adam Lehman , and Raghu Thricovil , Product Managers for Flash Platform tooling : <p> If that wasnt enough , did you see the new " Monocle " tool , shown by Deepa Subramaniam ? Monocle is the new realtime profiling tool for Flash-based content which will provide additional insight into what 's happening at runtime , and how you can optimize your applications . <p> Did you also see the latest demos showing the Epic Games &amp; the Unreal engine running INSIDE of the Flash Player ? <p> Wednesday concluded another great @ @ @ @ @ @ @ @ @ @ interact with the design &amp; development communities , as well as check out Adobes latest and greatest . Ill try to summarize as much of the content and announcements as possible here . First , let 's start with the keynotes : <h> MAX Day 1 Keynote <p> Day one focused on new creative tooling , including the Creative Cloud and Adobe Touch suite of applications . You can view the full day one keynote below : <h> MAX Day 2 Keynote <p> Day two focused on new HTML5 tooling and Flash Platform developments . You can view the full day two keynote here : <p> Creative Cloud is a new cloud-based service that allows you to seamlessly share content between creative suite applications , and includes software subscriptions . You can find details on Adobe TV , on the Creative Cloud home page , in the press release , or in the FAQ . <p> Adobe has introduced new tablet-based creative applications for both iOS and Android ( some currently available with iOS coming soon , and others vice versa ) . You can learn more about the Adobe Touch @ @ @ @ @ @ @ @ @ @ I worked for a company that sells tile flooring , it would be tough for me to convince someone that tile is the best choice for a particular room because they will expect me to say that tile is best for everything . Conversely , if I worked for a company that sells wood flooring , they expect me to say wood is best . However , if I work for a company that sells both types of flooring , I can have a real discussion about which is best on a room by room basis because I will have credibility in both types of flooring . <p> Adobe announced that it has acquired privately held Typekit Inc. , a leader in the delivery of hosted , high-quality fonts for use on websites . Available as a subscription-based cloud service , You can read details around the Typekit acquisition from the press release . <h> Adobe Announces Digital Publishing Suite , Single Edition <p> Adobes Digital Publishing Suite now offers " Single Edition " , an addition to the DPS tooling that enables small business and single-publishers to create digital @ @ @ @ @ @ @ @ @ @ etc ) for tablet devices . You can read more about DPS Single Edition from the DPS blog , press release , or Adobe TV . <p> Here 's another post that I originally wrote way back in 2006 , when object oriented development was a newer concept to client-side web applications . Again , this post is still very relevant with Flex , AIR &amp; ActionScript for mobile/web/desktop , so I decided to resurrect it from the old blog archive as well . Enjoy <p> Understanding of OOP ( Object Oriented Programming ) is fundamental in being successful with the Flex framework and being able to get the most out of it . Developers who do not possess a computer science-related background may not be aware of the fundamental concepts that comprise OOP and how to apply them correctly , so here is a quick piece to help you out . <p> First , object oriented programming is a programming paradigm where your code is organized into logical objects , and each object has properties and methods . Each object contains similar and/or related functionality , and is organized @ @ @ @ @ @ @ @ @ @ . <p> For Example : Let 's say that we have a class " Automobile " . This class would contain the information and functions necessary for our application to use the Automobile class . We could have a numeric property for the number of wheels , the speed , and the direction ( degrees on a compass ) . This class would also contain methods that control the actions of the Automobile object : accelerate , decelerate(break) , turn , start engine , stop engine , etc Our class would look something like this <p> Ok , now that we have a brief explanation of what object oriented programming is , we can get into some more aspects of OOP : inheritance and interfaces . <p> Inheritance is a way to form new objects based on existing objects . When a class inherits from a base class , the new class extends the functionality of the base class , and can utilize public and protected properties and methods from that base class . Inheritance can be used to create different objects that utilize functions within the base class , so @ @ @ @ @ @ @ @ @ @ . Inheritance can be used to extend the functionality of existing objects , and inheritance can also be used to override and/ or change functionality from the base class . <p> In Actionscript 3 , you can access the parent class of your class by using the " super " keyword . For instance , calling the constructor of the parent class would use " super() " , where accessing a method of the parent class would use something like : " super.myMethodName() " . If a property of the parent class is created with public or protected access , you can access that property in the child class directly by the property name ( you would use this.propertyName , not super.propertyName ) . <p> Now , Let 's take our Automobile example and apply object-oriented inheritance . We already have a base Automobile class that covers the basic functionality . We can create child classes that extend the functionality of the Automobile . <p> public class SportsCar extends Automobile public function SportsCar() super() ; override public function accelerate():void /* we can override the accelerate function so that it accelerates faster @ @ @ @ @ @ @ @ @ @ base functionality of the Automobile class , and therefore are instances of the Automobile class . If we have a function outside of the Automobile class , which takes an automobile as the parameter , both a SportsCar and Truck will work since they are both Automobiles . We could have a function such as the following : If we pass in a Truck Instance , and a SportsCar instance , both will work , and each will use the functionality of their specific class instead of the base Automobile class . <p> I 'll get into some more fine-grain details about inheritance later in this post now , let 's move on to interfaces <p> Interfaces are slightly different than inheritance . An interface is a set of " rules " which an object must adhere to . The " rules " are actually method signatures that your class must implement . When we define an interface , we define method signatures that are required for classes that implement that interface . There is no actual code in an interface ; it simply defines methods that must exist within your class @ @ @ @ @ @ @ @ @ @ code for the actual function . If you have multiple classes that implement an interface , those classes must have the same functions ( only the ones required by the interface ) , but that is where the similarities of the two classes may stop . They could have completely different logic and properties within them this is where inheritance and interfaces differ . Two objects that inherit from the same base class have a lot in common ( properties and methods ) : two objects that implement the same interface only have those interface method signatures in common . <p> Let 's now make an Automobile Interface that defines the functions required to create an IAutomobile object ( note the " I " stands for " interface " ) : <p> We can use the IAutomobile interface to create objects ( classes ) that behave as Automobile objects . These classes do not necessarily inherit from each other and do not necessarily share any common properties . <p> The previous two components both implement the IAutomobile interface , but have nothing else in common . One is simply a class @ @ @ @ @ @ @ @ @ @ component that implements the interface . The mxml component example extends the mx:Canvas component ( the same thing could be done by creating an AS class that extends mx.containers.Canvas ) . Now , let 's look at a function similar to the " race " function from earlier <p> This example will work with either object that I have created because both objects implement the IAutomobile interface . They do not rely upon functions in the class hierarchy , just those that were implemented for this interface . You can also use multiple interfaces on classes that you create . Implementing multiple interfaces basically means that you are adding more required method signatures to your class , and you will have to implement these methods to satisfy each interface . <p> On the other hand , you can not inherit from multiple classes . Some programming languages allow for multiple inheritance ActionScript 3 does not support multiple inheritance ( so i 'll stop there ) . <p> OK enough of this rambling What does this have to do with Flex ? Inheritance and interfaces are used extensively in AS3 to create the @ @ @ @ @ @ @ @ @ @ to the screen extend from the UIComponent class . AbstractService , DataSerice or EventDispatcher object implements the IEventDispatcher Interface . You may be using these concepts every day , but werent aware of them . Inheritance seems easier to take advanatage of at first Let 's say that you want to create several objects , all of which will have identical functions and variables . It is easy to see that you can create a base class that encapsulates all of the common functionality . You can then create a sub-classes that implement the differing functionality for each class . <p> When putting these concepts into real-world Flex applications you 'll need to get familiar with the following keywords : <p> extends This is used when defining a child class from a parent class . <p> public class MyImage extends Image <p> implements This is used when implementing an interface . <p> public class MyClass implements MyInterface <p> final Classes and methods implented with " final " can not be overridden . <p> final function myFunction() : void <p> static The static keyword is used when creating variables or functions in a @ @ @ @ @ @ @ @ @ @ instance . Static properties and methods do not require variable instantiation to be executed . <p> public static function myStaticFunction() : void //to use it call it directly from **31;3758;TOOLONG <p> internal This is used when creating a method or property that can be accessed by any object within the same package ( namespace ) <p> internal var foo : String ; <p> override This is used when creating a function that overrides another function from a parent class . <p> override public function myFunction() : void <p> private This is used when creating methods or properties that are only available to the class where it is defined . A private variable can not be accessed by outside classes or from descendant classes . <p> private var myPrivateValue : String ; <p> protected This is used when creating methods or properties that are only available to the class where it is defined and descendant classes . A protected variable can not be accessed by outside classes . <p> protected var myProtectedValue : String ; <p> public This is used when creating properties and methods that are available to any class . 
@@106848812 @2248812/ <h> Video : MobileFirst for Bluemix ( MBaaS ) <p> Last week I gave a presentation to the NYC Bluemix Meetup Group on IBM MobileFirst for Bluemix . Not familiar with the branding and have no idea what that means ? - It is a mobile backend as a service , which gives you analytics , remote logging , user auth , data persistence &amp; offline synch , push notification management , and more for your mobile applications . - Yes , as a service you can create a Bluemix account today for free and start building your apps very quickly and very efficiently. - - No problem if you werent able to make it to the meetup. - I recorded my session which you can check out in the embedded video below . <p> I know the video quality is n't fantastic , but its the best I had at the time . - ( I almost always have a GoPro with me. ) - If you want to see the code that makes all of this work in much , much more detail , check out my post @ @ @ @ @ @ @ @ @ @ , video tutorials and more . - Enjoy ! 
@@106848814 @2248814/ <p> This post is inspired by all the comments Ive seen this week about JS in the enterprise . I would have never imagined- this 10- years ago , but JavaScript is now pretty much ubiquitous . Here are a few reasons why you need to paying attention to JavaScript if you are n't  already , and why you should definitely not- write it off . <p> First , I think one of the major reasons for JavaScripts ubiquity is that- JavaScript is approachable . It is relatively easy for beginners to learn JavaScript , and powerful enough for advanced users to build complex and reliable systems . <p> Second , why you need to pay attention , JavaScript is everywhere . <p> You can now use JavaScript to develop on virtually any platform : client side applications , server side logic , embedded chips/IoT devices , manage build scripts and dependencies , and more . <p> This does n't  mean you 'll use the exact same code in every case , rather that you can use the same skill set JavaScript Development to deliver solutions across multiple paradigms . @ @ @ @ @ @ @ @ @ @ power client side apps/user interfaces , and user interactions on numerous platforms and devices . <h> Web <p> Of course JavaScript powers the web , this is a given . JavaScript is the primary scripting language for all web browsers . I wo n't focus on this much b/c its already well known . <h> Mobile <p> JavaScript can also be used to power mobile applications that are natively installed on a device . <p> Apache Cordova/PhoneGap- You can build natively installed apps with web technology using PhoneGap or Cordova . PhoneGap is Adobes branded distribution of Cordova , but from the developers perspective , they are basically the same thing . Your app runs within a webview on the mobile device , and you build your user interface the same way you you build a dynamic web application . Your user interface- is implemented in HTML , styled with CSS , and all interactivity is created with JavaScript . <p> React Native- JavaScript powered web apps do n't  just have to be inside of a a web view . The React Native framework gives developers the ability to write their @ @ @ @ @ @ @ @ @ @ in a native application running on the mobile device . The logic is interpreted JavaScript at runtime , but everything that the user- interacts with ( all UI elements ) is 100% native , providing a very high quality user experience , and it is now available for both iOS and Android applications . <p> Unity 3D You can even develop rich &amp; immersive mobile 3D simulation or gaming experience , entirely powered by JavaScript using the Unity 3D engine . **These can be web , desktop , or mobile , but is often used in mobile gaming . <h> Desktop <p> Yup , desktop apps are not left out of the mix . Most desktop solutions fall into a category similar to Apache Cordova , where the end results is a web view that has access to lower level APIs , whose content is developed with web based technology . <h> The Server Side <p> Most obviously- Node.js a JavaScript runtime buit on Chromes V8 JavaScript Engine has made huge inroads into server side development and the enterprise . Node.js , powered by frameworks like express.js or loopback.io makes @ @ @ @ @ @ @ @ @ @ . <p> Here are some stats that- show the magnitude of growth and adoption for Node.js/npm.js alone . NPM- stats currently shows a total of- 186,946 packages available for download , 94,978,032 package downloads in the last day , and 2,451,734,737 package downloads in the last month . <p> This does n't  mean that JavaScript is the best language at everything . It- also does n't  meant that you can take a single piece of source- code and run it in every device/context imaginable . <p> It means that you can use your skills in JavaScript to develop for just about any kind of device/context out there . Its not going to be write once , run everywhere , rather in the words of the React.js team : learn once , write everywhere . <p> Node.js is an incredible tool for rapidly building highly performant and scalable back end systems , and you develop it using a familiar core language that most front-end developers are already accustomed to , JavaScript. - This acquisition is positioned to greatly enhance Node.js in the enterprise , and StrongLoops offerings will be integrated into @ @ @ @ @ @ @ @ @ @ Even though the acquisition is still " hot off of the presses " , - you can start using these tools together- today : <p> If you have n't heard about StrongLoop 's LoopBack framework , it enables you to easily connect and expose your data as REST services . It provides the ability to visually create data models in a graphical ( or command line ) interface , which are used to automatically generate REST APIs " thus generating CRUD operations for your REST services tier , without having to write any code . <p> Why is this important ? <p> It makes API development easier and drastically reduces time from concept to implementation . - If you have n't yet looked at the LoopBack framework , you should definitely check it out . - You can build API layers for your apps literally in minutes . - Check out the video below for a quick introduction : <p> Again , be sure to check out these posts that detail the integration steps so you can start using these tools together today : <p> What I 'm about to show you might @ @ @ @ @ @ @ @ @ @ can assure you it is not . Actually , every piece of this is available for you to use as a service . - Today . <p> Yesterday- Twilio , an IBM partner whose services are available- via- IBM- Bluemix , announced several new SDKs , including live video chat as a service . - This makes live video very easy to integrate into your native mobile or web based applications , and gives you the power to do some very cool things . For example , what if you could add video chat capabilities between your mobile and web clients ? Now , what if you could take things a step further , and add IBM Watson cognitive computing capabilities to add real-time transcription and analysis ? <p> Jeff and Damion did an awesome job showing of both the new video service and the power of IBM Watson . I can also say first-hand that the new Twilio video services are- pretty easy to integrate into your own projects ( I helped them integrate these services into the native iOS client ( physicians app ) - shown in the @ @ @ @ @ @ @ @ @ @ , add your app tokens , and instantiate a video chat . - Jeff is pulling the audio stream from the WebRTC client and pushing it up to Watson in real time for the transcription and sentiment analysis services . <p> Earlier this week I had the privilege of speaking at ApacheCon- in Austin , TX on the topic of data management for apps that work as well offline as they do online . - This is an important topic for mobile apps , since , as we all painfully know already , there is never a case when you are always online on your mobile devices . - There always ends up being a time when you need your device/app , but you cant get online to get the information you need . - Well , this does n't  always have to be the case . There are strategies you can employ to build apps that work just as well offline as they do online , and the strategy- I 'd like to highlight today is based upon data management using the IBM Cloudant NoSQL database as a service , @ @ @ @ @ @ @ @ @ @ link to the presentation slides ( built using reveal.js ) just use the space bar to advance the presentation slides : <p> The " couch " in CouchDB is actually an acronym for Cluster of Unreliable Commodity Hardware . At the core of this cluster is the concept of replication , which in the most basic of terms means that - data is shared between multiple sources . - Replication is used to share information between nodes of the cluster , which provides for cluster reliability and fault tolerance . <p> Cloudant is a- clustered NoSQL- database services that provides an extremely powerful and searchable data store . - It is- designed to power- the web and mobile apps , and all information is exposed via REST services . Since the IBM Cloudant service is based on CouchDB ( and not so coincidentally , IBM is a major contributor to the CouchDB project ) , replication is also core the the Cloudant service . <p> With replication , you only have to write your data/changes to a single node in the cluster , and replication takes care of propagating- these @ @ @ @ @ @ @ @ @ @ apps for the web or mobile , there are options to extend the data replication locally either on the device or in the browser. - This means that you can have a local data store that automatically pushes and/or pulls data from the remote store using replication , and it can be done either via native languages , or using JavaScript . <p> If you want to have local replication in either a web or hybrid ( Cordova/PhoneGap ) app , you can use PouchDB. - PouchDB- is a local JavaScript database modeled- after CouchDB and implements that CouchDB replication API. - So , you can store your data in the browsers local storage , and those changes will automatically be replicated to the remote Cloudant store . - This works in the browser , in a hybrid ( web view ) app , or even inside of a Node.js instance . Granted , if you 're in-browser you 'll need to leverage the HTML5 cache to have your app cached locally . <p> If you are building a native app , do n't  worry , you can take advantage of the @ @ @ @ @ @ @ @ @ @ replication . - This is available for iOS and- Android , and implements the CouchDB replication API . <p> The sample app that I showed in the presentation is a native iOS application based on the GeoPix MobileFirst sample app- that I detailed in a previous post . - The difference is that in this case I showed it using the Cloudant Sync API , instead of the MobileFirst data wrapper classes , even though it was pointing at the exact same Cloudant database instance . - You can see a video of the app in action below . <p> All that you have to do is create a local data store instance , and then use replication to synchronize data between the local store and a remote store . <p> Replication be either one-way ( push or pull ) , or two-way . - So , any changes between the local and remote stores are replicated across the cluster . - Essentially , the local data store just becomes a node in the cluster . - This provides complete access to the local data , even if there is @ @ @ @ @ @ @ @ @ @ the local store , and replication takes care of the rest . <p> In the native Objective-C code , you just need to setup the CDTDatastore manager , and initialize your datastore instance . <p> Once your datastore is created , you can read/write/modify any data in the local store . - In this case I am creating a generic data object ( basically - like a JSON object ) , and creating a document containing this data . - A document is a record within the data store . <p> You can add attachments to the document or modify the document as your app needs . - In the code below , I add a JPG atttachment to the document . <p> Replication is a fire-and-forget process . - You simply need to initialize the replication process , and any changes to the local data store will be replicated to the remote store automatically when the device is online . <p> By assigning a replicator delegate class ( as shown above ) , your app can monitor and respond to changes in replication state . - For example , @ @ @ @ @ @ @ @ @ @ complete , or if an error condition was encountered . <p> If you want to access data from the local store , it is always available within the app , regardless of whether or not the device has an active internet connection . - For example , this method will return all documents within the local data store . <p> Last month I had the opportunity to speak at the DevNexus- developer conference in Atlanta on building native iOS apps- IBM MobileFirst . DevNexus is a great event , and it is always a privilege to attend- I highly recommend it for next year . - If you werent able to make it , no worries ! - Most of the sessions were recorded and are available for viewing online via dzone . <p> The recording of my- session is embedded below . - It covers everything you need to know to get started building apps with the MobielFirst platform . <p> Once your app goes live in the app store you will have just entered into an iterative cycle of updates , improvements , and releases . Each successively @ @ @ @ @ @ @ @ @ @ . IBM MobileFirst Foundation gives you the tools you need to manage every aspect of this cycle , so you can deliver the best possible product to your end user . In this session , we 'll cover the process of integrating a native iOS application with IBM MobileFirst Foundation to leverage all of the capabilities the platform has to offer . 
@@106848815 @2248815/ <p> Another request that I have gotten from some of our DPS customers is that theyd like to be able to implement gestures inside of the Edge Animate compositions that they are building for DPS publications . This includes double-tap gestures , swipe gestures , etc Out of the box , these gestures are n't  supported , but you can add them to any Edge Animate composition without too great of an effort . <p> Below is a quick video showing Edge Animate Compositions that are taking advantage of both double-tap and swipe gestures . - Note : I intended these to be used inside of DPS , but I show them in Safari on iOS. - These gestures override the default mobile browser behaviors . <p> As I mentioned above , this is n't something that is supported out of the box , but it is possible to add gesture features manually . <p> The links below are for the basic examples that I put in the video . - Both should work in desktop and mobile browsers : <p> For the double tap example , just perform a @ @ @ @ @ @ @ @ @ @ area ) , and the animation will start again from the beginning . - For the swipe gesture , just perform a horizontal swipe in either direction with either your finger , or the mouse . <h> Gestures With Hammer.js <p> I leveraged the hammer.js JavaScript library to handle gesture detection since these gestures are n't  supported by default . - Hammer.js also enables other gestures , like long taps , pinch , rotate , etc - However , I 'm only showing double tap and swipe . - You can read more about hammer.js using the following links : <p> I used this exact setup procedure in both the double-tap and swipe examples . <p> To include this library , I first downloaded the hammer.js file , and saved it inside of the " edgeincludes " folder . <p> Next , you have to disable the web view/browser default double tap behavior , which is to zoom in when double tapped . - You can disable the zoom on double tap by adding a viewport metadata tag inside of the Edge Animate projects html file . - Open your projects @ @ @ @ @ @ @ @ @ @ , and add the following line to the &lt;head&gt; <p> Next , we have to add the code inside of the Edge Animate composition to enable the gesture behavior. - The first thing you have to do is include the hammer.js library . - In this case , I wanted to add the gestures to the compositions stage , instead of a particular element . - So , right-click on the stage in the Edge Animate Editor , then select - the " Open Actions for Stage " menu option . <p> This will open the actions for the Stage instance . - Next , click on the " + " icon and select " creationcomplete " . - This will create a function that gets invoked once the Stage instance has been created at runtime . <p> In that function , first we need to import the hammer.js library . - Edge Animate compositions include the- yepnope.js library , which is originally intended to detect if a browser includes a specific piece of functionality . - If not , then include a JS library so substitute that missing feature @ @ @ @ @ @ @ @ @ @ a blank test to force it to include the hammer.js library . - The following function forces loading of the hammer.js library . - Once the library has been loaded into memory , it triggers the " init " function : <p> In the init function , we grab a reference to the stages element ( div ) , then use hammer.js to add our gesture event handlers : <p> Now , we need to start looking at the individual examples <h> Double Tap Gestures <p> In the double tap example , we have a simple timeline animation that plays sequentially . - At the end of the sequence the animation is stopped by a simple sym.stop() function call . - Heres a quick preview of the setup in Edge Animate : <p> To add the double tap gesture , all you have to do is add a hammer.js event for " doubletap " . - In that event handler , were just calling sym.play(0) , which restarts playback from the beginning of the composition . The full code for the creationcomplete event is shown below . - This is @ @ @ @ @ @ @ @ @ @ the composition stage instance : <h> Swipe Gestures <p> In the swipe gestures example , we have a simple timeline animation that plays sequentially . - However , at the end of each slide transition , playback is stopped by a simple sym.stop() function call . - Whenever we perform a swipe action , were either just playing forward , or playing in reverse until the next slide animation stops . - Heres a quick preview of the setup in Edge Animate , note the stop points highlighted by the arrows : <p> To add the swipe gestures , all you have to do is add a hammer.js event for " swipeleft " or " swiperight " . - In those event handlers , were just calling sym.play() or sym.playReverse() , depending whether it was a left or right swipe . - These play actions progress to the next animation sequence . The full code for the creationcomplete event is shown below . - This is all that is needed to add the swipe gesture to the composition stage instance : <p> With the swipe gestures , you can get @ @ @ @ @ @ @ @ @ @ you run into this and you do not want the page to scroll , the scroll action can be prevented by capturing the touchstart event , and canceling the default behavior. - I did n't  add this , just because I wanted to keep this example very simple . <p> Lately Ive been spending a lot more time working with- Adobe Edge Animate , - Adobe InDesign , and Adobe DPS . If you are n't  familiar with these tools , Adobe Edge Animate is a tool that enables the creation of animated or interactive HTML content , Adobe InDesign is a desktop publishing design tool , and Adobe DPS is Adobes Digital Publishing Suite , which is used for creating digital publications from InDesign everything from digital magazines , catalogs , corporate publications , education , and more . <p> So , you might be wondering , how does Edge Animate fall into this grouping ? Well From Edge Animate you can export compositions into a . oam package , which can be imported directly into InDesign for use with a web content overlay . You can read more @ @ @ @ @ @ @ @ @ @ was recently asked by a customer " does Edge Animate support 3D transforms ? " . Unfortunately , at this time 3D transforms are not supported in the timeline editor . However , you can add 3D transformations programmatically with JavaScript . Here are some examples showing how to integrate CSS3 3D transforms with Edge Animate compositions : <p> These can be great additions to the interactive experience , but I also wanted to share that you do n't  always need 3D transforms to add dimensionality to an interactive experience . By leveraging 2D translation , scaling , and opacity you can easily create interactive experiences that have a feeling of depth . <p> Let 's take a look at a quick example . The image below is from screenshots of an Edge Animate Composition that I put together . On the left-hand side there is an anatomical illustration . On the right-hand side , that illustration has been broken out into separate layers , with emphasis placed on the topmost layer . <p> Just click or tap on the image to see an animation that transforms the illustration on the @ @ @ @ @ @ @ @ @ @ <p> So , while this animation does n't  leverage any actual three dimensional elements , it leverages those 2D transforms to visually create a sense of depth . here 's how it works : <p> First , there are 3 images . The bottom-most image shows the skeletal structure and body outline . The middle image shows parts of the digestive system , and the top-most image shows another layer of major organs . The top 2 images have transparency so that they do not completely hide content from the underlying layers . <p> The default state is that all of these images are aligned so that they appear as a single image . <p> Once you click/tap the image , a set of two-dimensional- animations take place providing a sense of depth and emphasizing the top layer . The underlying layers have both a scale and opacity change . The bottom layers are smaller , and less opaque . The underlying layers also have a two dimensional ( top/left ) transform . In this example , I 've tried to align both the scale and top/left transforms to correspond with a @ @ @ @ @ @ @ @ @ @ Edge Animate Anatomy Composition <p> This technique provides the illusion of three-dimensional depth , even though we are n't  actually performing any kind of translation , rotation , or deformation on a three dimensional coordinate system . AND this can be implemented- completely- with the timeline . So , you do n't  have to be a programmer to add a dimensional feeling to you Edge Animate compositions . This effect was achieved simply by using the timeline editor and visual workspace . <p> You can preview this animation in a new window , or download the full source using the links below : <p> Its Valentines day , and here at Adobe were showing our love for creating great HTML experiences Just check out what we released today ! - All of these are available immediately via- Adobe Creative Cloud- subscriptions . <h> Adobe Edge Reflow ( Preview ) <p> Adobe Edge Reflow is a new responsive design tool that helps designers create and communicate responsive intent in their designs to both customers and developers . - Adobe Edge Reflow enables designers to create responsive HTML experiences in a visual @ @ @ @ @ @ @ @ @ @ that are- accurate- to the capabilities of the modern web . <h> Stay Informed ! <p> As I mentioned above , all of these are available immediately via Adobe Creative Cloud subscriptions . Even better , you can get Adobe Edge Animate , Adobe Edge Reflow , and Adobe Edge Code as part of the free Creative Cloud tier . Go download them now ! <p> Youve probably heard of Adobe Edge , a timeline-based tool for creating interactive and animated HTML content . Edge enables you to easily create interactive experiences that rely only on HTML , CSS , and JavaScript . If you 've used other Adobe Creative Suite tools , such as Flash Professional , Premiere , or After Effects , then Edge will probably look quite familiar . You have a timeline and controls to edit your content . <p> Currently , the " normal " use case for Edge is creating interactive experiences that are loaded when the page loads . - You can chain animation compositions in sequence , but they have to be in the same wrapper HTML file . - This works great @ @ @ @ @ @ @ @ @ @ I wanted to do is create an Edge animation and use that as a component that is arbitrarily added to the HTML DOM at any point in time . My findings : It can be done , although with a few gotchas . <p> Using Edge animations as components inside of a larger HTML experience is n't the primary use case which Edge was designed for . However this use case is being evaluated and may end up in Edge at a later date . If that happens , this process will become much easier . <p> If you 're wondering " What was I thinking ? " , Ill try to explain while discussing the process of building HTML-based apps , I had the thought : <p> Wouldnt it be cool to have a really elaborate loading animation while loading data from the server ? We could use Edge to build the animation ! <p> As a proof of concept , I created a very basic application that loads two separate Edge animations on demand . Before I go into too much detail on what I built , let 's take a @ @ @ @ @ @ @ @ @ @ buttons , one shows a car animation , one shows an airplane animation . Its pretty basic and straightforward : <p> The first thing that I did was create two simple Edge animations which you can view here : <p> Once the animations were complete , I started looking at the generated HTML output , and figuring out how I can add it to the HTML DOM of an existing HTML page . I then started putting together the sample application using- Mustache.js as a templating engine to abstract HTML views away from application logic . Note : I also have a simple utility that enables me to include Mustache.js templates in separate HTML files , so that I can keep everything separate . <p> First , I created the basic shell for the application . It is more or less an empty HTML structure , where all content is added at runtime : <p> Inside of the " contentHost " div , all UI is added to the HTML DOM upon request . Basically , when the user clicks a button , the Edge animation is added to the @ @ @ @ @ @ @ @ @ @ order to get this working , I had to change a few things in the generated Edge output : <p> in the *edge.js file , I changed the DOM Ready event handler to use an arbitrary event that I can control . By default , Edge uses the jQuery $ ( window ) . ready() event to start the animation . Since I am adding this to an existing HTML DOM , the $ ( window ) . ready() event is not applicable . - Instead , I changed this to use a custom " animationReady " event : <p> In the *edgePreload.js file , I added a reference to the onDocLoaded function so that I can manually invoke it later , once the Edge animation has been added to the DOM , since again , this wont rely on the " load " event . <p> //added this so it can be invoked later window.onDocLoaded = onDocLoaded ; <p> I also changed the aLoader object to reference the appropriate JavaScript files , since I changed their location in the directory structure : <p> Finally , I created the Mustache.js @ @ @ @ @ @ @ @ @ @ DOM elements that will be appended to the existing DOM . - In this there is a wrapper DIV , some HTML content including a button and some text ( the animation number is dynamic for the templating ) , the styles , a " Stage " div , and Edge preload JavaScript files necessary for the animation . <p> Next , let 's look at how this is actually injected into the DOM . - I created a setupAnimationView() function to inject the animations into the DOM . - This function is used by both animations . The first thing that it does is remove any existing DOM content and dereference the AdobeEdge variables in memory . Since Edge was n't originally designed for- asynchronously- loading animations , I found it to be easiest to just wipe-out Edge and reload it for every animation . - The unfortunate side effect is that you can only have one Edge animation on screen at any given point in time . - Next , the setupAnimationView() function generates the HTML DOM elements and event listeners and adds them to the DOM. - Finally , @ @ @ @ @ @ @ @ @ @ Edge is loaded . - If not , it loads the Edge runtime . The edgeDetectionFunction() then checks if the Edge animation is sufficiently loaded . If the animation definition is not loaded , it just waits and tries again . - If the animation definition is loaded , it dispatches the " animationReady " event ( discussed in step 1 ) to invoke the actual animation . <p> Since I am using Edge in a manner for which it was not initially designed , there are a few " gotchas " that I ran into : <p> You cant have multiple instances of the same Edge animation in a single HTML DOM at least , not easily . - Each Edge animation is assigned a unique I 'd . - This I 'd is referenced in the HTML structure and the *edge.js , *edgeActions.js , and *edgePreload.js files . - You would need to assign a unique I 'd to each instance , and make sure everything is referenced consistently . <p> It will be very tricky- asynchronously- add more than one Edge animation at the same time . - The shortcut @ @ @ @ @ @ @ @ @ @ wipe away the Edge variables in JS and reload them this would cause some issues with more than one animation . <p> If the capability to have Edge animations as components gets built into Edge ( which I hope it does ! ) , then you will not have to go through all of these steps , and it will be much easier . - I 'll be sure to share more if this feature develops . 
@@106848816 @2248816/ <p> IBMs Watson Developer Cloud speech services just got a whole lot easier for mobile developers . - I myself just learned about these two , and cant wait to integrate them into my own mobile applications . <p> The Watson Speech to Text and Text to Speech services are now available in both native iOS and Android SDKs , making it even easier to integrate language services into your apps . <p> This post is inspired by all the comments Ive seen this week about JS in the enterprise . I would have never imagined- this 10- years ago , but JavaScript is now pretty much ubiquitous . Here are a few reasons why you need to paying attention to JavaScript if you are n't  already , and why you should definitely not- write it off . <p> First , I think one of the major reasons for JavaScripts ubiquity is that- JavaScript is approachable . It is relatively easy for beginners to learn JavaScript , and powerful enough for advanced users to build complex and reliable systems . <p> Second , why you need to pay attention @ @ @ @ @ @ @ @ @ @ JavaScript to develop on virtually any platform : client side applications , server side logic , embedded chips/IoT devices , manage build scripts and dependencies , and more . <p> This does n't  mean you 'll use the exact same code in every case , rather that you can use the same skill set JavaScript Development to deliver solutions across multiple paradigms . <h> The Client Side <p> JavaScript can be used to power client side apps/user interfaces , and user interactions on numerous platforms and devices . <h> Web <p> Of course JavaScript powers the web , this is a given . JavaScript is the primary scripting language for all web browsers . I wo n't focus on this much b/c its already well known . <h> Mobile <p> JavaScript can also be used to power mobile applications that are natively installed on a device . <p> Apache Cordova/PhoneGap- You can build natively installed apps with web technology using PhoneGap or Cordova . PhoneGap is Adobes branded distribution of Cordova , but from the developers perspective , they are basically the same thing . Your app runs within a webview on @ @ @ @ @ @ @ @ @ @ the same way you you build a dynamic web application . Your user interface- is implemented in HTML , styled with CSS , and all interactivity is created with JavaScript . <p> React Native- JavaScript powered web apps do n't  just have to be inside of a a web view . The React Native framework gives developers the ability to write their application using JavaScript and declarative UI elements , and results in a native application running on the mobile device . The logic is interpreted JavaScript at runtime , but everything that the user- interacts with ( all UI elements ) is 100% native , providing a very high quality user experience , and it is now available for both iOS and Android applications . <p> Unity 3D You can even develop rich &amp; immersive mobile 3D simulation or gaming experience , entirely powered by JavaScript using the Unity 3D engine . **These can be web , desktop , or mobile , but is often used in mobile gaming . <h> Desktop <p> Yup , desktop apps are not left out of the mix . Most desktop solutions fall @ @ @ @ @ @ @ @ @ @ end results is a web view that has access to lower level APIs , whose content is developed with web based technology . <h> The Server Side <p> Most obviously- Node.js a JavaScript runtime buit on Chromes V8 JavaScript Engine has made huge inroads into server side development and the enterprise . Node.js , powered by frameworks like express.js or loopback.io makes server side development and complex enterprise apps with JavaScript possible . <p> Here are some stats that- show the magnitude of growth and adoption for Node.js/npm.js alone . NPM- stats currently shows a total of- 186,946 packages available for download , 94,978,032 package downloads in the last day , and 2,451,734,737 package downloads in the last month . <p> This does n't  mean that JavaScript is the best language at everything . It- also does n't  meant that you can take a single piece of source- code and run it in every device/context imaginable . <p> It means that you can use your skills in JavaScript to develop for just about any kind of device/context out there . Its not going to be write once , run everywhere @ @ @ @ @ @ @ @ @ @ learn once , write everywhere . <p> Node.js is an incredible tool for rapidly building highly performant and scalable back end systems , and you develop it using a familiar core language that most front-end developers are already accustomed to , JavaScript. - This acquisition is positioned to greatly enhance Node.js in the enterprise , and StrongLoops offerings will be integrated into IBM Bluemix , IBM MobileFirst , and WebSphere . <p> Even though the acquisition is still " hot off of the presses " , - you can start using these tools together- today : <p> If you have n't heard about StrongLoop 's LoopBack framework , it enables you to easily connect and expose your data as REST services . It provides the ability to visually create data models in a graphical ( or command line ) interface , which are used to automatically generate REST APIs " thus generating CRUD operations for your REST services tier , without having to write any code . <p> Why is this important ? <p> It makes API development easier and drastically reduces time from concept to implementation . - If you @ @ @ @ @ @ @ @ @ @ definitely check it out . - You can build API layers for your apps literally in minutes . - Check out the video below for a quick introduction : <p> Again , be sure to check out these posts that detail the integration steps so you can start using these tools together today : 
@@106848818 @2248818/ <h> Monthly Archives : December 2013 <p> here 's a fun tutorial for one of the last Friday afternoons of 2013 the creation of 360 degree panoramas , or " planets " as some like to call them . - If you 're not quite sure what I 'm talking about , check out the image below : <p> Planet Vegas <p> A panorama is a wide-angle view , usually captured with either a special lens , or by stitching together multiple images to create the wide angle view . - A 360 degree panorama is a representation of the wide angle view into a sphere . <p> This image was created in Photoshop by taking multiple images which were captured by GoPro attached to a remote controlled helicopter , stitching them together to create a wide angle panorama , then creating a 360 panorama from the wide angle panorama . <p> Check out the video below to see a timelapse for the creation of this image . Details below <p> Pretty cool , right ? - I had a great time putting this together . - Just ask any of my @ @ @ @ @ @ @ @ @ @ been completely obsessed with this . Its not a new technique This has been around for years , but its one not everyone knows off of the top of their head . - You should know how to do it too ! <p> here 's how you go about creating a 360 degree spherical panorama : <h> Step 1 : Start with the image - <p> You do n't  have to have a massive panorama . You can use any image that you want . Though , I 've had the best results by applying this to panoramas. - You could also crop an image to be much wider than it is tall . - If you want to learn how to create panoramas , check out these tutorials : <p> I did a manual process that is a variation of these The original images I had were from a GoPro Hero 3 camera . - I applied lens profile correction in Lightroom , then loaded the images into Photoshop as separate layers of the same PSD composition . - Then I used auto-align layers and auto-blend layers to merge the images @ @ @ @ @ @ @ @ @ @ and Crop <p> You 'll want to rotate and crop the image so that the horizon is level , and you get rid of missing areas within the image . If you 're missing part of the image , but you do n't  want to crop any more , you can use content aware fill or the clone stamp to synthesize the missing parts . <p> Rotating and cropping in Photoshop <h> Step 4 : Get Rid of Seams <p> You do n't  necessarily need a full view of 360 degrees to create these types of images , but you will get the best results if the image does n't  have any glaring seams . The way that I usually go about doing this is by splitting the image in half , and swapping the two sides so that the seam is in the middle of the image , with some overlap . <p> Work in progress removing seams <p> Then I use Photoshops clone stamp , brushes , or content aware fill to get rid of any glaring visual seams so that it is one complete image . You 'll probably need @ @ @ @ @ @ @ @ @ @ introduced from the overlapping . <h> Step 5 : Make the image a square <p> Just go to Image-&gt;Image Size and make the width and the height of the image the exact same value ( you 'll need to unlink width and height ) . - This will stretch your image out vertically do n't  worry , the stretching is OK and is part of the process . <p> " Squareifying " the image <h> Step 6 : Flip the image <p> Next , you 'll want flip the image so that it is upside down . - You can do this either by going to **29;3791;TOOLONG 180 , or **27;3822;TOOLONG Vertical . <p> Flipped Image <h> Step 7 : Apply Polar Coordinates Distortion <p> Make sure that your image has been flattened or that all layers are within a smart object , so you are applying this filter to only one layer . - Select the target layer , then go to **28;3851;TOOLONG Coordinates . - Make sure that the " Rectangular to Polar " option is selected , and hit " OK " . <p> Polar Coordinates to " spherize @ @ @ @ @ @ @ @ @ @ , the content in the top 25-30% of your image will be stretched out , and the bottom 25-30% will be pinched/squeezed , so just be aware that this will happen . - You will want to play around with this feature with variations of image cropping to get a feel for what it will do to your images . <h> Step 8 : Polish your image <p> At this point , you will now have a 360 spherical panorama , but you might not be 100% happy with the output . - If you do n't  like how the distortion was applied , go back and change cropping and try again . If you want to make the colors " pop " , try applying Camera RAW as a filter . - Youll also notice that the corners of the image are partially transparent and stretched badly you can crop this area , retouch the area , or layer assets within a composition . - This is really up to you as the creator of the composition . Whatever you do , take advantage of Photoshop it has a @ @ @ @ @ @ @ @ @ @ . <p> I wanted to follow up my last post on 3D Parallax effects in HTML or Adobe DPS , I 've decided to release some of the other experiments that I 've been exploring with device motion in DPS publications . Check out the video below to see two new samples , and a corrected version of the strawberries example from my last post ( the plants were going the wrong way in the last post ) . <p> All three of these samples leverage the same basic technique for responding to device motion inside of a DPS publication . The motion-interactive components are implemented using HTML and JavaScript , and are included in publications as web content overlays . In JavaScript , it takes advantage of the ondevicemotion event handler to respond to the physical orientation of the device . <p> In all three of samples , the web content overlay is set to autoplay , with user interaction disabled . This way the HTML &amp; JavaScript automatically loads and the scripting is active , but it does n't  block interaction or gestures for DPS navigation . I also enabled @ @ @ @ @ @ @ @ @ @ scales appropriately between retina and non-retina devices . <h> Adobe San Francisco <p> The Adobe/inline content example is implemented in the same manner as the strawberries example . The large city image It is a two-layer composition created with Adobe Edge Animate . The foreground building and flag move independently from the background image . I used Photoshop to separate the content into layers and made them animate based on device orientation in the exact same fashion as the strawberries sample . All of the text and image content surrounding the cityscape panorama is laid out with InDesign . <h> AT&amp;T Park/San Francisco Giants <p> The AT&amp;T Park/San Francisco Giants example is implemented with basic HTML and JavaScript , no additional tools were used to create this interactive scenario . - The content on the left hand side was all laid out with InDesign . The content on the right side is the interactive HTML . <p> The image used in this example is a vertical panorama captured from a remote control helicopter . This image contains various perspectives that have been composited in Photoshop . The motion of the @ @ @ @ @ @ @ @ @ @ ; When the device is facing down , the image is looking down and when the device is vertical , the image faces forward . You can check out the vertical panorama image below . If you 're interested in creating a vertical panorama , be sure to- check out this tutorial from Russell Brown . <p> Vertical Panorama over AT&amp;T Park <p> The HTML and JavaScript used in this example is fairly minimal . The image is applied as the background of the root HTML &lt;body&gt; element , and the position of the background is shifted based upon the device motion event . This approach keeps the HTML DOM as flat and simple as possible . <p> A few weeks ago , a fellow Adobe colleague showed me a DPS publication that had an amazing design . All of the content looked great by itself , but what really made parts of it " pop " was that in certain areas there was a 3D parallax effect , which made it feel like you were looking into an image that had depth . You could rotate the device and see @ @ @ @ @ @ @ @ @ @ of a building . <p> Heres what I mean on the surface the image looked static , but as I rotated it , elements shifted to give the illusion of depth . The background and foreground elements all moved at different rates : <p> 3D Parallax Effects on a Device <p> I thought this was an incredible example of added interactivity and immersive- experiences , and its not really that difficult to implement. - In fact , I put together this tutorial to show exactly how you can create these types of effects in your own compositions . <p> To create this kind of an effect , the first thing you need to do is break apart an image into layers note : - you may need to synthesize edges so that there is an overlap in all transparent areas . Then you need to add interactivity in HTML . Align those images so that their default state looks just like the still image , then move the images based upon the device orientation . I move the foreground one way , keep the middle content more or less stationary @ @ @ @ @ @ @ @ @ @ all based upon which way you are rotating the mobile device ) . Since this is all HTML , you can take this content and use it on the web , or import it into Adobe InDesign to export a DPS digital publication . <h> Step 1 : Create Layered Images <p> You can either create your own layers , or break apart an existing image into layers so that each individual layer can be placed over top each other to form a seamless composition . In this case , I separated the strawberries , the rows of plants , my daughter , and the sky out to separate layers . <p> Update 1/7/2014 : I added logic to support both landscape and portrait orientation . <p> Be sure to add both of those JavaScript snippets inside of the creationComplete event for the Stage . - I also over-exaggerated the movement in the timeline. - I think it would look better with slightly less ( more subtle ) movement . <p> At this point , you could publish the composition and use it on the web there 's nothing stopping you @ @ @ @ @ @ @ @ @ @ out here , just load it on an iPad and rotate the device to see the effect . However , please keep in mind that 1 ) I have n't added a preloader , 2 ) the assets are non-optimized and are all retina size , 3 ) I do n't  have it auto scaling for the viewport size , so it will only look right on a retina iPad , and 4 ) I have only tested this on an iPad no other devices . <p> Note : You could also do this without using Edge Animate , but you 'd have to hand code the HTML/JS for it . <h> Step 3 : Include in InDesign/DPS Composition <p> To include this in a DPS publication , all that you need to do is export an Animate Deployment Package ( . oam file ) from Adobe Edge Animate . You can then just drag and drop this into InDesign for inclusion in a DPS publication . <p> If you are n't  already a member of Creative Cloud , join today to take advantage of all of our creative tools ! <p> @ @ @ @ @ @ @ @ @ @ of the plants should actually be reversed . - If you view this link , you 'll see the updated motion- ( which looks more realistic ) , but I cant update the video that 's already been published . <p> One of my favorite parts of Creative Cloud is that it gives you everything you need to be creative . Whether you are into photography , video , illustration , print design , web design , or just dabble in creativity , Creative Cloud has everything that you need . Ive been doing a lot of photography lately . My main tools for retouching images are Photoshop and Lightroom Photoshop for the heavy edits and re-composition , and Lightroom for retouching/color correcting and bulk edits . <p> Using either of these tools you can turn images that originally looked " blah " into " Awesome ! " hence my tag line " bringing out the awesome " . <p> One great feature that I use in both of these tools is the ability to retouch colors using Adobe Camera RAW ( In Photoshop CC this is a filter , in Lightroom @ @ @ @ @ @ @ @ @ @ This gives you the ability to enhance colors , enhance clarity , add effects , heal , and much more , and it is all relatively easy once you get the hang of it . Check out the images below for before and after shots of a photo I recently captured from a flight over Charlotte , NC . <p> Before and after retouching in Lightroom <p> Now , check out a time-lapse video showing the retouching process , to get an idea how this was done ( more details below ) <p> That entire composition retouch only took a few minutes . Heres what I used inside of Adobe Photoshop Lightroom to put it all together . <p> The " Basic " panel gives you the ability to quickly adjust color temperature , tint , exposure , contrast , highlight/shadows/white balance , and enhance clarity and color saturation as it applies to the entire image . Just drag the sliders to see the impact in real time . <p> Basic Panel in Lightroom <p> The " Effects " panel gives you the ability to add a vignette effect to @ @ @ @ @ @ @ @ @ @ vignette amount , midpoint size , roudness , feather , and more . <p> Effects panel in Lightroom <p> The " Detail " panel gives you the ability to sharpen or reduce noise in your images . In this case I used it to reduce luminance ( brightness ) noise on the image , but you can also apply noise reduction to colors . <p> Detail panel in Lightroom <p> The Graduated Filter and Adjustment Brush allow you easily apply localized adjustments to areas within your image . In this case , I applied two graduated filters : one to bring out contrast and definition in the sky , and another to darken and unsharpen the ground . Read more in the Lightroom documentation to learn how to use both the graduated filters and adjustment brush . <p> Graduated Filter in Lightroom <p> and that 's all I did . I did n't  even use a big fancy camera to take this photo . I used a Panasonic Lumix LX7 , which is a pretty good point and shoot camera , but its definitely not a DSLR . <p> If you @ @ @ @ @ @ @ @ @ @ a higher resolution variation over on my Flickr page . <p> Skies over Charlotte , NC <p> Now , get out there and create amazing compositions and images . If you 're not already a member of Creative Cloud , join today at creative.adobe.com . <p> Second , a video of my presentation on " Architectural Considerations for PhoneGap and Mobile Web Apps " has been published by the Atlanta HTML5 meetup group . - Check it out in the video below , and if you 're in the Atlanta area , be sure to check out the meetup group ! - Heres the presentation description : <p> Tired of Hello World ? In this session , we explore best practices to build real-world PhoneGap applications . We investigate the Single Page Architecture , HTML templates , effective Touch events , performance techniques , modularization and more . We also compare and contrast the leading JavaScript and Mobile Frameworks . This session is a must If you plan to build a PhoneGap application that has more than a couple of screens . 
@@106848819 @2248819/ <h> Category Archives : Video <p> If you have n't already heard of Adobe Edge Code or Brackets , it is a new HTML/CSS/JS code editor that enables a rapid development cycle by connecting directly to your browser for automatic real-time updates to the HTML content running inside the browser . This promotes rapid development , and allows you to focus on writing your code rather than worrying about all the steps required to switch between your editor and browser and step through development and- debugging- iterations . <p> In most contexts we talk about the live HTML/JS/CSS development perspective on the client side , but I 'd also like to highlight that- Adobe Edge Code- and/or- Brackets- can be used to rapidly develop dynamic server-side web code too . Check out the screencast below to see live editing of dynamically generated HTML content using the latest Brackets release ( Sprint 23 ) . <p> This streamlined workflow can be incredibly powerful , especially for rapid prototyping scenarios . You just focus on writing code and creating an experience . Let the edtior offload the work of updating the live content @ @ @ @ @ @ @ @ @ @ at hand . <p> To enable the Edge Code/Brackets preview on a server , just go into " File-&gt;Project Settings " , and then specify a " Preview Base URL " string . The preview base URL will be prefixed to the name of the file that you are editing , and will be used for the browsers URL in the live connection . So , if you are editing " index.html " with a preview base URL of http : //tricedesigns.com , brackets will launch the live connection with the URL " http : **29;3881;TOOLONG " . <p> However , keep in mind that Edge Code &amp; Brackets primary use case is editing HTML/JS/CSS , not server-side languages . The editor wont have code hinting for your server-side code , unless you are using an extension that enables code hinting ( with the exception of NodeJS because JavaScript is supported ) . The current build of Brackets also changes the live preview URL when you change files . So , if you are editing " index.cfm " , and switch to " content.cfm " in the editor , the @ @ @ @ @ @ @ @ @ @ : //urlprefix/content.cfm " . If the files are intended to be separate , this works great . However , if content.cfm is n't a standalone file , the live preview wont be as valuable . Meaning : if content.cfm is actually an included file inside of index.cfm , the live preview of content.cfm may not work the way you want it to . In either case , this can still be a very valuable tool with rapid prototyping . <p> Speaking of extensions Edge Code/Brackets is built with an extensible plugin architecture . Since the editor itself and all of the extensions are written with HTML/JS , you can easily extend and customize the tool to add any functionality that you want . <p> if you 're wondering about the difference between the names Adobe Edge Code- and- Brackets , - Adobe Edge Code- is Adobes distribution of the Brackets editor . Adobe Edge Code has scheduled releases , and includes useful extensions and tie-ins to other Adobe services ( for example , Adobe Web Fonts and more ) . Brackets- is the open source project for the core editor itself . @ @ @ @ @ @ @ @ @ @ is rapidly evolving . Essentially , Brackets is the " engine " that powers Adobe Edge Code . <p> I chose a simple ColdFusion- demonstration because I already had an existing code sample I could reuse , however this will work with PHP , NodeJS , or other server-side scripting libraries . The latest builds of Brackets even have- NodeJS built directly into it . <p> Another request that I have gotten from some of our DPS customers is that theyd like to be able to implement gestures inside of the Edge Animate compositions that they are building for DPS publications . This includes double-tap gestures , swipe gestures , etc Out of the box , these gestures are n't  supported , but you can add them to any Edge Animate composition without too great of an effort . <p> Below is a quick video showing Edge Animate Compositions that are taking advantage of both double-tap and swipe gestures . - Note : I intended these to be used inside of DPS , but I show them in Safari on iOS. - These gestures override the default mobile browser behaviors @ @ @ @ @ @ @ @ @ @ that is supported out of the box , but it is possible to add gesture features manually . <p> The links below are for the basic examples that I put in the video . - Both should work in desktop and mobile browsers : <p> For the double tap example , just perform a double tap/click gesture anywhere on the stage ( the image area ) , and the animation will start again from the beginning . - For the swipe gesture , just perform a horizontal swipe in either direction with either your finger , or the mouse . <h> Gestures With Hammer.js <p> I leveraged the hammer.js JavaScript library to handle gesture detection since these gestures are n't  supported by default . - Hammer.js also enables other gestures , like long taps , pinch , rotate , etc - However , I 'm only showing double tap and swipe . - You can read more about hammer.js using the following links : <p> I used this exact setup procedure in both the double-tap and swipe examples . <p> To include this library , I first downloaded the hammer.js file @ @ @ @ @ @ @ @ @ @ folder . <p> Next , you have to disable the web view/browser default double tap behavior , which is to zoom in when double tapped . - You can disable the zoom on double tap by adding a viewport metadata tag inside of the Edge Animate projects html file . - Open your projects . html file ( in this case " DoubleTap.html " , and add the following line to the &lt;head&gt; <p> Next , we have to add the code inside of the Edge Animate composition to enable the gesture behavior. - The first thing you have to do is include the hammer.js library . - In this case , I wanted to add the gestures to the compositions stage , instead of a particular element . - So , right-click on the stage in the Edge Animate Editor , then select - the " Open Actions for Stage " menu option . <p> This will open the actions for the Stage instance . - Next , click on the " + " icon and select " creationcomplete " . - This will create a function that gets @ @ @ @ @ @ @ @ @ @ . <p> In that function , first we need to import the hammer.js library . - Edge Animate compositions include the- yepnope.js library , which is originally intended to detect if a browser includes a specific piece of functionality . - If not , then include a JS library so substitute that missing feature . - In this case , I am passing it a blank test to force it to include the hammer.js library . - The following function forces loading of the hammer.js library . - Once the library has been loaded into memory , it triggers the " init " function : <p> In the init function , we grab a reference to the stages element ( div ) , then use hammer.js to add our gesture event handlers : <p> Now , we need to start looking at the individual examples <h> Double Tap Gestures <p> In the double tap example , we have a simple timeline animation that plays sequentially . - At the end of the sequence the animation is stopped by a simple sym.stop() function call . - Heres a quick preview of @ @ @ @ @ @ @ @ @ @ double tap gesture , all you have to do is add a hammer.js event for " doubletap " . - In that event handler , were just calling sym.play(0) , which restarts playback from the beginning of the composition . The full code for the creationcomplete event is shown below . - This is all that is needed to add the double-tap gesture to the composition stage instance : <h> Swipe Gestures <p> In the swipe gestures example , we have a simple timeline animation that plays sequentially . - However , at the end of each slide transition , playback is stopped by a simple sym.stop() function call . - Whenever we perform a swipe action , were either just playing forward , or playing in reverse until the next slide animation stops . - Heres a quick preview of the setup in Edge Animate , note the stop points highlighted by the arrows : <p> To add the swipe gestures , all you have to do is add a hammer.js event for " swipeleft " or " swiperight " . - In those event handlers , were just @ @ @ @ @ @ @ @ @ @ left or right swipe . - These play actions progress to the next animation sequence . The full code for the creationcomplete event is shown below . - This is all that is needed to add the swipe gesture to the composition stage instance : <p> With the swipe gestures , you can get some drag event conflicts on mobile devices . - If you run into this and you do not want the page to scroll , the scroll action can be prevented by capturing the touchstart event , and canceling the default behavior. - I did n't  add this , just because I wanted to keep this example very simple . <p> Next week I 'll be representing Adobe at GDC 2013 , and demonstrating how Adobe Creative Cloud , - PhoneGap , and PhoneGap Build- can be great tools for building casual gaming experiences. - In preparation , I 've been working on a gaming sample application that shows off the potential of HTML games packaged with PhoneGap . <p> and now I 'd like to introduce you to PhoneGap Legends . PhoneGap Legends is a fantasy/RPG themed demo that @ @ @ @ @ @ @ @ @ @ I was able to get some really outstanding performance out of this example , so be sure to check out the video and read the details below . The name " PhoneGap Legends " does n't  mean anything ; I just thought it sounded videogame-ish and appropriately fitting . <p> PhoneGap Legends <p> This game demo is an infinitely-scrolling top-view RPG themed game that is implemented entirely in HTML , CSS , and JavaScript . There is a scrolling background , enemy characters , HUD overlays , and of course , our " protagonist " hero all that 's missing is a story , and general game mechanics like interacting with sprites . <p> Again , I was able to get some *really outstanding performance* out of this sample , so I wanted to share , complete with source code , which you 'll find further in this post ( and I encourage you to share it too ) . Take a look at the video below to see the game in action on a variety of devices . Every single bit of this is rendered completely with HTML , CSS , @ @ @ @ @ @ @ @ @ @ . <p> Update 3/24 : - If you 'd like to test this out on your own devices , you can now access it online at- LONG ... - However , it will still only work on webkit browsers ( Chrome , Safari , Android , iOS , etc ) , and is optimized for small-device screens . - If you attempt to use this on a very large screen , you 'll probably see some sprite clipping . <p> Disclaimer : This sample app is by no means a complete game or complete game engine . Ive implemented some techniques for achieving great performance within a PhoneGap application with game-themed content , but it still needs additional game mechanics . I also wrote this code in about 2 days it needs some additional cleanup/optimization before use in a real-world game . <h> Source <p> Full source code for this demo application is available on GitHub . This code is provided as-is : - https : **37;3912;TOOLONG . I will be making a few updates over the next few days in preparation for GDC next week , but for the most part @ @ @ @ @ @ @ @ @ @ DOM is as shallow as possible for achieving the desired experience . All " sprites " , or UI elements are basic DOM nodes with a fixed position and size . - All DOM elements have an absolute position at 0,0 and leverage translate3d for their x/y placement . - This is beneficial for 2 reasons : 1 ) It is hardware accelerated , and 2 ) there are very , very few reflow operations . - Since all the elements are statically positioned and of a fixed size , browser reflow operations are at an extreme minimum . <p> The background is made up a series of tiles that are repeated during the walk/movement sequence : <p> Sprite Sheet for Background Tiles <p> In the CSS styles , each tile is 256+256 square , with a background style that is defined for each " type " of tile : <p> The content displayed within each of the " sprite " DOM elements is applied using sprite sheets and regular CSS background styles . Each sprite sheet contains multiple images , the background for a node is set in CSS @ @ @ @ @ @ @ @ @ @ the " background-position " css style . - For example , the walking animation for the hero character is applied just by changing the CSS style that is applied to the " hero " &lt;div&gt; element . <p> Sprite Sheet for Hero <p> There is a sequence of CSS styles that are used to define each state within the walking sequence : <p> This game demo extensively uses translate3d for hardware accelerated composition . - However , note that the 3d transforms are all applied to relatively small elements , and are not nested . All of the " textures " are well below the max texture size across all platforms ( 1024+1024 ) , and since it uses sprite sheets and reusable CSS styles , there are relatively few images to load into memory or upload to the GPU . <h> Attribution <p> The following Creative Commons assets were used in the creation of this sample app and the accompanying video : <p> I put this post together to follow up a few points from my last post about Adobe Ideas &amp; Adobe Creative Cloud . I promise , @ @ @ @ @ @ @ @ @ @ my next post . I 'm just having too much fun with this right now <p> here 's another composition that I put together using Adobe Ideas , following the same workflow in my previous post : I sketched this composition in Adobe Ideas , then leveraged Adobe Creative Cloud to continue the editing process on the desktop . <h> Video : Adobe Ideas Composition <p> Disclaimer : I am not an artist or illustrator . - I primarily focus on writing software the art stuff is just for fun . <p> Now , I want to elaborate a bit more about the Creative Cloud workflow . Creative Cloud is not just a subscription to all of Adobes creative and Edge tools . Its also not just a " hard drive in the sky " that synchronizes your creative assets across your devices . It is both of these , and much more . <p> We 've already covered that Creative Cloud empowers the seamless creation of content across mobile and desktop devices . You can create content using the Adobe Touch apps , and pull that into the desktop tools for refinement @ @ @ @ @ @ @ @ @ @ created in the video above If I want to isolate and extract individual assets , I can do that very easily using the desktop tools : <p> Isolating Layers in Adobe Illustrator <p> I can also pull these layers into Photoshop for further enhancement . In this case , color correction , and rearranging some content ( oh look , more elephants ! ) . <p> Editing in Adobe Photoshop <p> Now , let 's consider workflow beyond just creation of content . What about collaboration and feedback ? Adobe Creative Cloud empowers collaborative experiences too . Everything from simple sharing , to comments/feedback , even having the ability to break down and view individual layers of PSD files . Check out the video below for some more detail : <h> Video : Adobe Creative Cloud Workflow <p> You can share any of your Creative Cloud assets by email or simple URLs , where that content and workflow can be consumed on any device , anywhere . Need feedback from your customers ? Just send them a link to the composition work in progress , and they can leave comments whenever @ @ @ @ @ @ @ @ @ @ Cloud <p> In fact , If you 're interested , you can check this composition out for yourself . Here are the assets that I used to create this composition , which Ive shared publicly . Although , I disabled the ability to download the original files . You can toggle PSD layers , or leave comments ( just be nice ) : <p> In this post I 'm changing things up a bit Rather than focusing just on developer-centric topics , I 've decided to show a little more creative workflow using Adobe Ideas . - Adobe Ideas is an awesome sketching app for the iPad . You can sketch whatever you want , whenever you want . It is all saved as vector content that you can then pull into Adobe Illustrator to integrate into your desktop workflow using Adobe Creative Cloud. - Take a look at the video below to see it in action : <p> Once you save your sketch , it automatically gets synched with your Creative Cloud account . You can then pull the composition directly into Illustrator and use it within other creative suite tools . @ @ @ @ @ @ @ @ @ @ that video : <p> You can export as raster ( png , jpg , etc ) or vector ( ai , svg , etc ) formats , so it can be a great tool for enabling you to hand-sketch high- quality- graphical assets for your creative workflow . 
@@106848820 @2248820/ <p> Its been a while since I 've posted here on the blog - In fact , - I just did the math , and- its been over 7 months. - Lots of things have happened since , I 've moved to a new team within IBM , built new developertools , worked directly with clients- on their solutions , worked on a few high profile keynotes , built apps for kinetic motion and activity tracking , built a mobile client for a chat bot , and even completed some new drone projects . - Its been exciting to say the least , but the real reason I 'm writing this post is to share a few- of the public projects Ive been involved with from recent conferences . <p> I recently returned from Gartner Symposium and IBMs annual World of Watson conference , and- its been one of the busiest , yet most exciting span of two weeks Ive experienced- in quite a while . <p> At both events , we showed a project Ive been working on with IBMs Global Business Services team that focuses on the use of small @ @ @ @ @ @ @ @ @ @ . In particular , by leveraging IBM Watson to automatically detect roof damage , in conjunction with photogrammetry to create 3D reconstructions and generate measurements of afflicted areas to expedite and automate claims processing . <p> This application leverages many of the services IBM Bluemix has to offer on-demand CloudFoundry runtimes , a Cloudant NoSQL database , scalable Cloud Object Storage ( S3 compatible storage ) , and BareMetal servers on Softlayer . Bare Metal servers are *awesome* I have a dedicated server in the cloud that has 24 cores ( 48 threads ) , 64 GB RAM , RAID array of SSD drives , and 2 high end multi-core GPUs . Its taken my analysis processes from 2-3 hours on my laptop down to 10 minutes for photogrammetric reconstruction with Watson analysis . <p> Its been an incredibly interesting project , - and you can check it out yourself in the links below . <h> World of Watson <p> World of Watson was a whirlwind of the best kind I had the opportunity to join IBM SVP of Cloud , Robert LeBlanc , on stage as part of the @ @ @ @ @ @ @ @ @ @ that seats over 20,000 people ) to show off the drone/insurance demo , plus 2 more presentations , and an " ask me anything " session on the expo floor . <p> You can also check out my session " Elevate Your apps with IBM Bluemix " on UStream to see an overview in much more detail : <p> .. and that 's not all . I also finally got to see a complete working version of the Olympic Cycling teams training app on the expo floor , including cycling/biometric feedback , video , etc I worked with an IBM JStart team and wrote the video integration layer into for the mobile app using IBM Cloud Object Storage and Aspera for efficient network transmission . <h> Drones <p> On this project we 've been working with a partner DataWing , who provides drone image/data capture as a service . However , I 've also been flying and capturing my own data . The app can process virtually any images with appropriate metadata , but I 've been putting both the DJI Phantom and Inspire 1 to work , and they 're working fantastically. 
@@106848821 @2248821/ <h> Tag Archives : ios <p> Weve been able to write native iOS apps leveraging the scaffolding and analytics of the IBM MobileFirst Platform Foundation Server for a while now . This was first introduced way back when MobileFirst still went by the Worklight name , serveral versions ago . <p> As I would write apps , one thing I really wanted was to use code blocks instead of having to implement delegate classes every time I need to call a procedure on the MobileFirst server . - In MobileFirst 7.0 , the new WLResourceRequest- API allows you to invoke requests using either the completionHandler ( code block ) or delegate implementations . <p> But what if you 're still using an earlier version of the MobileFirst platform , or what if you still want to leverage your existing code that uses- **25;3951;TOOLONG parameters , but do n't  want to have to create a new delegate for every request ? - Well , look no further . - I put together a very simple utility class that helps with this task by allowing you to pass code blocks as parameters @ @ @ @ @ @ @ @ @ @ server . <p> I normally prefer code blocks b/c they allow you to encapsulate functionality inside of a single class , instead of having logic spread between a controller and delegate class ( and having to worry about communication between the two ) . <p> The other getLoggerForInstance utility function is just a shortcut to get an OClogger instance with the package string matching the class name of the instance passed , with just a single line of code : <p> What I 'm about to show you might seem like science fiction from the future , but I can assure you it is not . Actually , every piece of this is available for you to use as a service . - Today . <p> Yesterday- Twilio , an IBM partner whose services are available- via- IBM- Bluemix , announced several new SDKs , including live video chat as a service . - This makes live video very easy to integrate into your native mobile or web based applications , and gives you the power to do some very cool things . For example , what if you could add @ @ @ @ @ @ @ @ @ @ Now , what if you could take things a step further , and add IBM Watson cognitive computing capabilities to add real-time transcription and analysis ? <p> Jeff and Damion did an awesome job showing of both the new video service and the power of IBM Watson . I can also say first-hand that the new Twilio video services are- pretty easy to integrate into your own projects ( I helped them integrate these services into the native iOS client ( physicians app ) - shown in the demo ) ! - You just pull in the SDK , add your app tokens , and instantiate a video chat . - Jeff is pulling the audio stream from the WebRTC client and pushing it up to Watson in real time for the transcription and sentiment analysis services . <p> In this entry were going- to focus- on building Apple Watch apps that can communicate back and forth with- the host application running on the iPhone. - This is extremely important since the Apple Watch provides a second screen/peripheral complimentary experience to the main app running on the iOS device be @ @ @ @ @ @ @ @ @ @ happening within the bigger picture . <p> In my last post I showed how to setup remote logging and **25;3978;TOOLONG in an Apple Watch app using IBM MobileFirst Platform Foundation server . - I used the- methods described below for communicating between the WatchKit and host apps in the- sample app from that- previous post . <p> When were talking about bidirectional communication , were talking about sending data two ways : <p> Sending data from the host app to the WatchKit app <p> Sending data to the WatchKit app from the host- app <p> At first thought , you might think " oh that 's easy , just use- NSNotificationCenter to communicate between the separate classes of the application " , but things are n't  exactly that simple . <p> An Apple Watch app is really made of 3 parts : 1 ) the main iOS application binary , 2 ) the user interface on the Apple Watch , and 3 ) the WatchKit extension binary ( on the iOS device ) . <p> Apple Watch App Architectural Components <p> Yep , you read that correctly , the WatchKit extension @ @ @ @ @ @ @ @ @ @ Watch UI and resides on the iOS device ) is a separate binary from the " main " iOS application binary . - These are separate processes , so- objects in memory in the main app are not the same objects in memory in the extension , and as a result , these processes do not communicate directly . NSNotificationCenter is n't going to work . <p> However there are definitely ways you can make this type of a scenario work . <p> First , WatchKit has methods to invoke actions on the host application from the WatchKit extension . - WatchKits- openParentApplication- or **30;4005;TOOLONG methods both provide the ability to invoke actions and pass data in the containing app , and provide a mechanism to invoke a " reply " code block back in the WatchKit extension after the code in the host application has been completed . <p> For example , in the- WatchKit extension , - this will invoke an action in the host application and handle the reply : <p> Inside the host application we have access to the userInfo NSDictionary that was passed , and we @ @ @ @ @ @ @ @ @ @ the code below I am setting a string value on the userInfo instance , and taking appropriate actions based upon the value of that string . <p> This covers the " pull " scenario , and is great if you want to invoke actions within your host app from your WatchKit extension , and then handle the responses back in- the WatchKit extension to update your Apple Watch UI accordingly . <p> What about the " push " scenario ? - The previous scenario only covers requests that originate inside the WatchKit extension . - What happens if you have a process running inside of your host app , and- have updates that you want to push to the WatchKit extension without an originating request ? <p> There is no shared memory , and it is not a shared process , so neither NSNotificationCenter or direct method invocation will work . However , you *can* use Darwin notifications ( which work across seprate processes by using- CFNotificationCenter ) . - This enables near-realtime interactions across processes , and you can share data as attributes- of a CFdictionary- object based between @ @ @ @ @ @ @ @ @ @ using access- groups , and notify the separate processes using the CFNotificationCenter implementation . <p> Note : CFNotificationCenter is C syntax , not Objective-C syntax . <p> First you 'll need to subscribe for the notifications in the WatchKitExtension . Pay attention to the static i 'd instance " staticSelf " you 'll need this later when invoking Objective-C methods from the C notification callback . <p> We 've now covered scenarios where you you can request data or actions in the host application *from* the WatchKit extension , and also how you can push data from the host application to the WatchKit extension . <p> Now , what if there was a library that encapsulated some of this , and made it even easier for the developer ? - When I wrote the app in my previous post , I used the methods described above . However , I recently stumbled across the open source- MMWormhole , which wraps the Darwin Notifications method ( above ) for ease of use . - I 'm pretty sure I 'll be using this in my next WatchKit app . <h> Helpful Links for inter-process communication between WatchKit @ @ @ @ @ @ @ @ @ @ in a multipart series on powering native iPhone and Apple Watch apps using the IBM MobileFirst Platform . - In this entry we will cover how to setup the MobileFirst Platform for use within Apple WatchKit apps and leverage the operational analytics and remote logging features . <p> So , let 's first take a look at the app were going to build in this video : <p> The app is a simple location tracker. - Think of something like a much simpler version of Run Keeper that will allow you to track your location path over a period of time , and show your location on a map . - Were also building a WatchKit app that enables you to quickly start or stop tracking your location without ever having to pull your iPhone out of your pocket . - All of this powered by IBM MobileFirst . <p> When you 're setting up your WatchKit app , you need to follow the exact same steps that you did for the native app target , just apply them to your WatchKit extension target . <p> First you need to add the @ @ @ @ @ @ @ @ @ @ be sure to include **36;4037;TOOLONG that is inside the iOS API ) : <p> Add MobileFirst Frameworks and Dependencies <p> Next , add the " -ObjC " linker flag : <p> Add Linker Flag <p> Then make sure that worklight.plist ( which is inside of the MobileFirst API you generated from either the CLI or Eclipse Studio ) so that it is included in both the native app and WatchKit extension . <p> Worklight.plist Target Membership <p> This allows you to take advantage of MobileFirst APIs within your WatchKit extension , complete with operational analytics. - You cansave remote logs , you can access data adapters , and more. - The server-side security mechanisms also work , so if you want to shut down your API for specific versions , you have that ability . <p> I mentioned earlier , its just like a native iOS app , but with a few exceptions . - The most important and notable exception is that the UI elements ( modal dialogs , alerts , etc ) that you would normally see in the native phone interface do not appear in the WatchKit @ @ @ @ @ @ @ @ @ @ do n't  see the notification . - So , you need to work around any scenarios that rely on this , and make sure you handle errors accordingly . <p> To invoke MobileFirst APIs , you call them as you wold normally in either Objective-C or Swift . - For example : <p> and the remote log search capability , including- logs from the WatchKit extension : <p> MobileFirst Remote Logging with the WatchKit App <p> That 's all that you need to get started ! <p> Stay tuned ! - Full source code will be released on my github account in a subsequent post. - Also be sure to stay tuned for future entries that cover the MobileFirst platform with offline data , persisting data to the server , push notifications , geo notifications , bidirectional communication between the watch and host app , background processing , and more ! I will update this post to links to each subsequent post as it is made available . <p> Wondering what IBM MobileFirst is ? - Its a platform that enables you to deliver and maintain mobile applications throughout their entire @ @ @ @ @ @ @ @ @ @ offline storage , push notifications , user authentication , and more , plus you get operational analytics and remote logging to keep an eye on things once you 've deployed it to the real world , and its available as- either cloud or on-premise solutions . <p> I recently put together some content on building " Apps that Work as Well Offline as they do Online " using IBM MobileFirst and Bluemix ( cloud services ) . - There was the original- blog post , I used the content in a presentation at ApacheCon , and now I 've opened everything up for anyone use or learn from . <p> The content now lives on the IBM Bluemix github account , and includes code for the native iOS app , code for the web ( Node.js ) endpoint , a comprehensive script that walks through every step of of the process configuring the application , - and also a video walkthrough of the entire process from backend creation to a complete solution . <p> Key concepts demonstrated in these materials : <p> User authentication using the Bluemix Advanced Mobile Access service <p> @ @ @ @ @ @ @ @ @ @ Access service <p> Client-side Objective-C code ( you can do this in either hybrid or other native platforms too , but I just wrote it for iOS ) . - The " iOS-native " folder contains the source code for a complete sample application leveraging this workflow . The " GeoPix-complete " folder contains a completed project ( still needs you to walk through backend configuration ) . The " GeoPix-starter " folder contains a starter application , with all MobileFirst/Bluemix code commented out . You can follow the steps inside of the " Step By Step Instructions.pdf " file to setup the backend infrastructure on Bluemix , and setup all code within the " GeoPix-starter " project . 
@@106848823 @2248823/ <h> Tag Archives : PhotoShop <p> In addition to my addiction to aerial photography , I 'm also fascinated by time-lapse photography . With time lapse photography , you set up your camera to take pictures on an interval . This could be every few seconds , every few minutes , every few hours , or heck , once a day . Its really up to you how you want to set up your shots and what you want to shoot . In any case , you can end up with a lot images each by itself could be great , but it only tells a limited story . - However , you can put all those images together in a sequence to create some truly amazing visuals . Subtle motion becomes pronounced , and you can clearly view the passage of time . Often , this ends up with an amazing visual story that would be hard to otherwise capture . <p> All that you need start diving into time-lapse photography is a camera that is capable of capturing images on an interval normally there is some kind of @ @ @ @ @ @ @ @ @ @ frequency and duration . Then , once you 've got your images , you can process them with Creative Cloud tools to bring out their full potential . <p> Here are two time-lapse sequences I created this weekone a snow storm , one a sunset . <p> Neither sequence required a lot of specialized or expensive equipment . I used a GoPro Hero 3 Black camera , set it on my window sill , and let it do its thing . ( I do want to upgrade to better gear , but this still works fantastically , and I love the GoPro . ) <p> GoPro Hero 3 Black Edition <p> The sunset was a ten second interval captured over about 2 hours and played back in 30 seconds . The snow storm was a 60 second interval captured over roughly 14 hours , played back in 40 seconds . <p> So , you 've captured the images , what next ? - <p> You can check out the video below , or read on for further explanation how I processed and assembled the images into a video sequence , complete with @ @ @ @ @ @ @ @ @ @ everything together as a sequence , I wanted to enhance the photos to bring out as much detail as possible . here 's where Adobe Lightroom comes into the picture . I used Lightroom to import all of my photos , add them to a collection , and then perform bulk/batch processing to enhance all of the images . <p> Editing Photos with Lightroom <p> First , select an image to use as your baseline for adjustments . I would n't start with your darkest image , and I would n't start with your lightest either . I normally start somewhere in the middle . Select the image , and then switch over to the " Develop " module . I use the basic panel to make adjustments to this image . For the GoPro , I like to bring up the shadows and bring down the highlights to pull out details out . If I 'm shooting a landscape , I also like to bring up the clarity and maybe even the vibrance and saturation just do n't  over do it . You could also use one of Lightrooms presets if you want @ @ @ @ @ @ @ @ @ @ careful that it is not too dark or too light b/c were going to apply these settings to all images in the sequence . <p> Lightroom Basic Panel <p> If you want to adjust hue , saturation or luminance of specific colors , you can do that within the HSL/Color/B&amp;W panel . Using this you can make specific colors more or less intense . - I normally try to tone down the yellows in my GoPro images after I 've increased overall saturation . <p> Since I used the GoPro , there is a lot of fisheye distortion from the lens the GoPro has a 2.77mm lens whichgives an ultra-wide 170 degree field of view . This makes for some awesome wide angle shots , but sometimes you do n't  want that extreme distortion . This is where lens correction gets really handy . Next , I opened up the Lens Correction panel . As soon as you check the " Enable Profile Corrections " checkbox , Lightroom should automatically select the GoPro Hero 3 Black Edition lens profile based upon metadata within the image . I did n't  want to @ @ @ @ @ @ @ @ @ @ , so I turned down the distortion correction using the " Distortion " slider . <p> Lightroom Lens Correction <p> Once you have your baseline image the way you want it , you need to apply these settings to all of your images in the sequence . Just select them all , and then either click on the " Sync " button in the bottom right of the Develop module , or use the Settings -&gt; Synch Settings menu . This will apply you changes on this image to all of the images that were selected . This will happen automatically if you are using auto-sync. - You can learn more about synchronizing metadata between photos in the Lightroom documentation . <p> Next , be sure to view several images in your collection , the lightest to the darkest , and make sure they all look decent . If you need to make any changes because they are too light , or too dark , or do n't  have the right contrast , then now is your time to fix it . Once you 're happy with the images in your @ @ @ @ @ @ @ @ @ @ as JPG with 100% quality at full resolution with sequential names . <p> Now we 've got a lot of processed images . What 's next ? We need to make a video ! <p> If you 're wondering how I got the motion in the time lapse sequence , no I did n't  have the camera moving . There are devices which make this possible , but I just used a video editing trick . The images are 12 MP , or 4000 by 3000 pixels . A " standard " HD video sequence is 1920 by 1080 pixels . The image below reflects this scale the red area represents the 4000 by 3000 still image , and the yellow represents the 1920 by 1080 video . <p> Video &amp; Image Size Comparison <p> You 'll notice that leaves us with a lot of room to zoom and pan around the image . I zoom into the image so that it fills the entire horizontal space within the video sequence you can zoom in more if you want . This leaves a fair amount of vertical content outside the clipping rectangle of the @ @ @ @ @ @ @ @ @ @ panning vertically within this area . - I just made the pan very slow and deliberate so it appears that there is constant motion of the camera throughout the entire video . <p> The final result is that the content in the video ( yellow area ) appears to move because the actual image sequence is moving relative to the video viewport . <p> Photosynth is an impressive service from Microsoft . - It enables you to upload photos and turn them into interactive 360 panoramas , photo walls , spins , or photo walks . The Photosynth team recently announced a new version of Photosynth , and its a really cool web experience . It leverages- WebGL- to visualize the content , and runs great on both desktop and mobile devices ( as long as the devices support WebGL ) . <p> Those who know me well or regularly read the blog probably already know I have an obsession with aerial photography using remote controlled multirotor helicopters . Once I discovered Photosynth , my first thought was " Wow , these Photo Walks will be incredible to visualize flights @ @ @ @ @ @ @ @ @ @ time-lapse photography mode with a GoPro camera attached to a DJI Phantom copter . The time-lapse images are perfect for Photosynth I normally capture on a two second interval , though the Photosynth team suggested trying an even shorter interval for better results . <p> To generate the best Photosynths , you need to start with the best photos . This is where Lightroom comes into the picture . Lightroom is an incredible tool for editing photos and bringing out their details . You can enhance exposure , colors , clarity , saturation , reduce noise , and more . Even better , it excels at bulk image editing . Thus its perfect for processing your photos for preparation to create a Photosynth . <p> Check out the video below to get an overview of Photosynth , and preparing your photos with Lightroom . <p> Now , you 're ready to learn more about both Lightroom and Photosynth , right ? <p> Below are Photosynths from a few of my flights . - If your browser supports WebGL , you 'll be able to see the fully interactive experience you 'll be able @ @ @ @ @ @ @ @ @ @ pan the images at full resolution . Its best viewed in full-screen mode . <p> Today new versions of Illustrator , InDesign , and Photoshop were released , and there are some amazing new features added for Creative Cloud members . I 'd like to take this opportunity to show off a really cool new feature in Adobe Photoshop CC Perspective Warp ! Perpsective warp was sneaked at Adobe MAX last year its a new way to manipulate your images by changing their three dimensional perspective . - You can manipulate parts of the image to give the appearance that the camera perspective changes , all without having to create a complex 3D model ! This filter can be used for changing entire images , and is especially useful when aligning perspectives while compositing content from multiple images . <p> here 's a quick video I put together showing how you can use it within your own Photoshop creations . <p> All that you have to do is select the layer that you want to manipulate , then select the Edit -&gt; Perspective Warp menu option. - Using Perspective Warp is a @ @ @ @ @ @ @ @ @ @ " mode . - Create planes within your images to match areas or geometries of content within your image . This might be the sides of a building , or other areas that you do n't  want deformed . <p> Next , Switch " Warp " mode . - In warp mode you can drag the pins/vertices to warp the content within the image . You can also hold shift and click on a line to lock that line to a horizontal or vertical position . <p> Drag the perspective warping where you want it , and then commit the changes by clicking on the " Commit " check button . <p> Perspective Warp Toolbar <p> Now , let 's look at two scenarios where Ive used perspective warp ( see the video for step by step details ) . <p> The first example shows how you can change the perspective/focal area of the image . - In this case I applied perspective warp to the entire image . - I created planes to match the Adobe building in the center of the image , and then used Perspective Warp to @ @ @ @ @ @ @ @ @ @ In the second example I composited a truck onto a street . - In the original images , the trucks perspective is close , but does not match the perspective of the street . - Using Perspective Warp , you can easily re-shape content to match perspectives within your composition . <p> here 's a fun tutorial for one of the last Friday afternoons of 2013 the creation of 360 degree panoramas , or " planets " as some like to call them . - If you 're not quite sure what I 'm talking about , check out the image below : <p> Planet Vegas <p> A panorama is a wide-angle view , usually captured with either a special lens , or by stitching together multiple images to create the wide angle view . - A 360 degree panorama is a representation of the wide angle view into a sphere . <p> This image was created in Photoshop by taking multiple images which were captured by GoPro attached to a remote controlled helicopter , stitching them together to create a wide angle panorama , then creating a 360 panorama from the wide @ @ @ @ @ @ @ @ @ @ see a timelapse for the creation of this image . Details below <p> Pretty cool , right ? - I had a great time putting this together . - Just ask any of my friends , and they will tell you that I have been completely obsessed with this . Its not a new technique This has been around for years , but its one not everyone knows off of the top of their head . - You should know how to do it too ! <p> here 's how you go about creating a 360 degree spherical panorama : <h> Step 1 : Start with the image - <p> You do n't  have to have a massive panorama . You can use any image that you want . Though , I 've had the best results by applying this to panoramas. - You could also crop an image to be much wider than it is tall . - If you want to learn how to create panoramas , check out these tutorials : <p> I did a manual process that is a variation of these The original images I had were @ @ @ @ @ @ @ @ @ @ lens profile correction in Lightroom , then loaded the images into Photoshop as separate layers of the same PSD composition . - Then I used auto-align layers and auto-blend layers to merge the images into a single panorama . <h> Step 3 : Rotate and Crop <p> You 'll want to rotate and crop the image so that the horizon is level , and you get rid of missing areas within the image . If you 're missing part of the image , but you do n't  want to crop any more , you can use content aware fill or the clone stamp to synthesize the missing parts . <p> Rotating and cropping in Photoshop <h> Step 4 : Get Rid of Seams <p> You do n't  necessarily need a full view of 360 degrees to create these types of images , but you will get the best results if the image does n't  have any glaring seams . The way that I usually go about doing this is by splitting the image in half , and swapping the two sides so that the seam is in the middle of the image @ @ @ @ @ @ @ @ @ @ seams <p> Then I use Photoshops clone stamp , brushes , or content aware fill to get rid of any glaring visual seams so that it is one complete image . You 'll probably need to crop this again to get rid of any whitespace introduced from the overlapping . <h> Step 5 : Make the image a square <p> Just go to Image-&gt;Image Size and make the width and the height of the image the exact same value ( you 'll need to unlink width and height ) . - This will stretch your image out vertically do n't  worry , the stretching is OK and is part of the process . <p> " Squareifying " the image <h> Step 6 : Flip the image <p> Next , you 'll want flip the image so that it is upside down . - You can do this either by going to **29;4075;TOOLONG 180 , or **27;4106;TOOLONG Vertical . <p> Flipped Image <h> Step 7 : Apply Polar Coordinates Distortion <p> Make sure that your image has been flattened or that all layers are within a smart object , so you are applying this @ @ @ @ @ @ @ @ @ @ layer , then go to **28;4135;TOOLONG Coordinates . - Make sure that the " Rectangular to Polar " option is selected , and hit " OK " . <p> Polar Coordinates to " spherize " the image <p> When you apply the distortion filter , the content in the top 25-30% of your image will be stretched out , and the bottom 25-30% will be pinched/squeezed , so just be aware that this will happen . - You will want to play around with this feature with variations of image cropping to get a feel for what it will do to your images . <h> Step 8 : Polish your image <p> At this point , you will now have a 360 spherical panorama , but you might not be 100% happy with the output . - If you do n't  like how the distortion was applied , go back and change cropping and try again . If you want to make the colors " pop " , try applying Camera RAW as a filter . - Youll also notice that the corners of the image are partially transparent and @ @ @ @ @ @ @ @ @ @ area , or layer assets within a composition . - This is really up to you as the creator of the composition . Whatever you do , take advantage of Photoshop it has a LOT of features you can use to enhance your creativity . <p> A few weeks ago , a fellow Adobe colleague showed me a DPS publication that had an amazing design . All of the content looked great by itself , but what really made parts of it " pop " was that in certain areas there was a 3D parallax effect , which made it feel like you were looking into an image that had depth . You could rotate the device and see what 's hiding behind a person , or around the corner of a building . <p> Heres what I mean on the surface the image looked static , but as I rotated it , elements shifted to give the illusion of depth . The background and foreground elements all moved at different rates : <p> 3D Parallax Effects on a Device <p> I thought this was an incredible example of added interactivity @ @ @ @ @ @ @ @ @ @ to implement. - In fact , I put together this tutorial to show exactly how you can create these types of effects in your own compositions . <p> To create this kind of an effect , the first thing you need to do is break apart an image into layers note : - you may need to synthesize edges so that there is an overlap in all transparent areas . Then you need to add interactivity in HTML . Align those images so that their default state looks just like the still image , then move the images based upon the device orientation . I move the foreground one way , keep the middle content more or less stationary , and move the background content the opposite direction ( all based upon which way you are rotating the mobile device ) . Since this is all HTML , you can take this content and use it on the web , or import it into Adobe InDesign to export a DPS digital publication . <h> Step 1 : Create Layered Images <p> You can either create your own layers , or @ @ @ @ @ @ @ @ @ @ individual layer can be placed over top each other to form a seamless composition . In this case , I separated the strawberries , the rows of plants , my daughter , and the sky out to separate layers . <p> Update 1/7/2014 : I added logic to support both landscape and portrait orientation . <p> Be sure to add both of those JavaScript snippets inside of the creationComplete event for the Stage . - I also over-exaggerated the movement in the timeline. - I think it would look better with slightly less ( more subtle ) movement . <p> At this point , you could publish the composition and use it on the web there 's nothing stopping you at all . In fact , you can check it out here , just load it on an iPad and rotate the device to see the effect . However , please keep in mind that 1 ) I have n't added a preloader , 2 ) the assets are non-optimized and are all retina size , 3 ) I do n't  have it auto scaling for the viewport size , so it @ @ @ @ @ @ @ @ @ @ 4 ) I have only tested this on an iPad no other devices . <p> Note : You could also do this without using Edge Animate , but you 'd have to hand code the HTML/JS for it . <h> Step 3 : Include in InDesign/DPS Composition <p> To include this in a DPS publication , all that you need to do is export an Animate Deployment Package ( . oam file ) from Adobe Edge Animate . You can then just drag and drop this into InDesign for inclusion in a DPS publication . <p> If you are n't  already a member of Creative Cloud , join today to take advantage of all of our creative tools ! <p> Update : After publishing this I realized that the movement of the plants should actually be reversed . - If you view this link , you 'll see the updated motion- ( which looks more realistic ) , but I cant update the video that 's already been published . 
@@106848824 @2248824/ <p> Mark your calendars ! Next Thursday , August 18th at 9:00AM to 10:30 AM ( PDT GMT-7 ) , the evangelism team will be hosting a live Q&amp;A chat . Come join us to learn more about Flex and AIR for mobile , and bring your questions with you ! <p> This session will be Q&amp;A only . There will be no demos , no slides , no speaking just pure Q&amp;A through Adobe Connect ( all you need is a web browser or the mobile app ) . Whether you are actively involved in mobile development , or about to start , join us and bring your questions ! <p> On Thursday morning , just go to http : //flex.org/ask and join us in the chat . I hope to see you there ! <p> How : We will have at least five evangelists in the Connect session to answer questions . This is a bit of a science experiment so we 'll see how it goes ! We will allow as many people in as we can handle , then we 'll start blocking entry and @ @ @ @ @ @ @ @ @ @ minutes so you can come and go as you want . There are no presentations and no demos . It 's purely a Q&amp;A session . <p> Why : For many of us , building apps for mobile devices is a very dramatic shift . We 've never before had to deal with touch interfaces , varying screen densities , and input from the GPS and accelerometer . At the same time , we have to be much more conscience than ever before about resources , because our apps run on much slower CPUs with far less memory than we are used to and on OSes that will shut down your app if it crosses the line ! It 's not a trivial transition ! <p> Our team has been building real apps with Flex for several months and we have learned a lot . This is an opportunity for us to share that knowledge in an informal setting . We 've never attempted this before , but if it works well , we 'll do it often . If it 's a bust , we 'll figure out a @ @ @ @ @ @ @ @ @ @ mind for the chat : <p> Questions should be as specific as possible and limited to Flex on Mobile . <p> We ca n't promise that every question will be answered , but we 'll do our best . <p> We ca n't debug your code , that 's your job ! However , we can direct you to good online resources . <p> Keep it friendly and G-rated . We do n't want to see you on webcam ! <p> No flaming ! We 're here to answer specific questions , not to debate you about technology . <p> If you had n't  heard yet , Beta 2 of AIR 3.0 and Flash Player 11 are now availabe on Adobe Labs . The AIR 3.0 beta release is sporting some great new features , including hardware accelerated video playback for mobile , iOS background audio , android licensing support , front-facing camera support , encrypted local storage for mobile , H.264 software encoding for desktop applications , and last , but not least , captive runtime support for desktop and Android applications . <p> If you are wondering what @ @ @ @ @ @ @ @ @ @ to explain Currently all AIR applications that are deployed on the desktop and in Android require the 3rd-party Adobe AIR runtime . If you are familiar with the process for developing mobile AIR applications for Apples iOS devices , then you may already know that these applications do n't  require the 3rd-party runtime ; they are completely self-contained applications . These AIR applications for iOS already take advantage of the captive runtime . All necessary components of the AIR framework are bundled into a self-contained , compiled distributable application that has no dependence upon other frameworks . <p> With AIR 3.0 , you will have the option to bundle the AIR framework into your applications to eliminate the 3rd-party dependency . However , one thing to keep in mind is that you can only export mac application packages on Macs and Windows EXEs on Windows . You ca n't target native installers or bundled runtimes for cross-platform development . You can only have a single app that targets both platforms if you export a . AIR file ( which requires the 3rd-party AIR runtime ) . <p> A great question came @ @ @ @ @ @ @ @ @ @ Ca n't I Run My App in the iOS Simulator " asking if it is possible to export the mobile simulator from Flash Builder to allow you to distribute builds for functional testing/validation. - The quick answer is yes ( but you do n't  really export the simulator ) . <p> Let me explain When running or debugging a mobile application directly within Flash Builders environment , you really are running a local desktop application within the ADL executable ( AIR Debug Launcher ) that is part of the AIR SDK. - The size/layout of ADL in this case is locked to- imitate- the experience of a particular device/configuration. - The mobile application is n't running in a device-specific simulator . <p> All Flex/AIR mobile applications can actually be exported as . AIR files that can run within the desktop runtime environment . - This enables you to easily repurpose an application from mobile to desktop . <p> Keep in mind , this does not mean there will be a 100% parity between desktop and mobile features , interaction , and performance . - Interaction paradigms , such mouse vs. touch , @ @ @ @ @ @ @ @ @ @ well as hardware ( CPU/memory ) may all have critical impacts in the overall experience . - Again , if you are targeting mobile devices , then it is imperative that you test your applications on real , physical devices to identify any kind of performance issues , bugs , or UX issues . <p> To export a mobile application for desktop usage as a . AIR file , just select the " Signed AIR package for installation on desktop " export option within the " Export Release Build " dialog . <p> Flash Builder Export AIR Options <p> The exported AIR file will use the standard AIR runtime , inclusive of cross-platform features and the AIR installer. - Note : I used a local dev certificate in this example , with just the default options . <p> AIR Installer <p> At runtime , it will look like any other AIR application . - You can configure the AIR app-xml descriptor files to customize it . <p> The reason that you can run a Flex Mobile app in the Android emulator , but not in the iOS Simulator comes down @ @ @ @ @ @ @ @ @ @ . An emulator emulates a physical device ; The emulator program mimics the hardware , and the device-specific code that will run on the actual device runs within the mimicked environment . A simulator simulates an environment it has a likeness or model of an environment , however it is not identical to the target environment . <p> In this case , the Android emulator mimics the hardware environment and is capable of running a compiled APK for a Flex/AIR mobile application . However , the iOS simulator is not capable of executing the contents of an IPA file . This is n't specific to an IPA file for an AIR mobile app , but any IPA file even those downloaded from Apples own app store . <p> The executable content within an IPA file is compiled targeting the devices A4-ARM processor . Your desktop computer uses an intel-based processor architecture , which is n't compatible and will not work . Even if you rename an IPA file to a ZIP file and extract the contents , it will not work within the iOS Simulator because of the CPU architecture differences . @ @ @ @ @ @ @ @ @ @ important point that I emphasize regarding mobile application development is that nothing is more important than testing your mobile applications on a physical device . Emulators and simulators can help you see how an application may operate within a given environment , but they do not provide you with the actual environment . Physical devices may have memory , CPU , or other physical limitations that an emulator or simulator may not be able to reveal . <p> Secondly , keep in mind that emulators and simulators are created to make your development process easier and faster ( especially if hardware is not readily available for the entire dev team ) . The debugging environment within Flash Builder is designed exactly for that purpose . You can quickly and easily test your applications interface and functionality with a single button click . You can even setup debugging profiles for multiple devices , or use one of the predefined device configurations : <p> While these capabilities make developing for multiple form factors and multiple device types significantly easier and faster , this does not trump my first point . If you @ @ @ @ @ @ @ @ @ @ you test thoroughly on your target platform(s). 
@@106848825 @2248825/ <p> What I 'm about to show you might seem like science fiction from the future , but I can assure you it is not . Actually , every piece of this is available for you to use as a service . - Today . <p> Yesterday- Twilio , an IBM partner whose services are available- via- IBM- Bluemix , announced several new SDKs , including live video chat as a service . - This makes live video very easy to integrate into your native mobile or web based applications , and gives you the power to do some very cool things . For example , what if you could add video chat capabilities between your mobile and web clients ? Now , what if you could take things a step further , and add IBM Watson cognitive computing capabilities to add real-time transcription and analysis ? <p> Jeff and Damion did an awesome job showing of both the new video service and the power of IBM Watson . I can also say first-hand that the new Twilio video services are- pretty easy to integrate into your own projects ( @ @ @ @ @ @ @ @ @ @ client ( physicians app ) - shown in the demo ) ! - You just pull in the SDK , add your app tokens , and instantiate a video chat . - Jeff is pulling the audio stream from the WebRTC client and pushing it up to Watson in real time for the transcription and sentiment analysis services . <p> In this entry were going- to focus- on building Apple Watch apps that can communicate back and forth with- the host application running on the iPhone. - This is extremely important since the Apple Watch provides a second screen/peripheral complimentary experience to the main app running on the iOS device be it a remote control , or quick view/glance into what 's happening within the bigger picture . <p> In my last post I showed how to setup remote logging and **25;4165;TOOLONG in an Apple Watch app using IBM MobileFirst Platform Foundation server . - I used the- methods described below for communicating between the WatchKit and host apps in the- sample app from that- previous post . <p> When were talking about bidirectional communication , were talking about sending data @ @ @ @ @ @ @ @ @ @ to the WatchKit app <p> Sending data to the WatchKit app from the host- app <p> At first thought , you might think " oh that 's easy , just use- NSNotificationCenter to communicate between the separate classes of the application " , but things are n't  exactly that simple . <p> An Apple Watch app is really made of 3 parts : 1 ) the main iOS application binary , 2 ) the user interface on the Apple Watch , and 3 ) the WatchKit extension binary ( on the iOS device ) . <p> Apple Watch App Architectural Components <p> Yep , you read that correctly , the WatchKit extension ( which controls all of the logic inside the Apple Watch UI and resides on the iOS device ) is a separate binary from the " main " iOS application binary . - These are separate processes , so- objects in memory in the main app are not the same objects in memory in the extension , and as a result , these processes do not communicate directly . NSNotificationCenter is n't going to work . <p> However there are @ @ @ @ @ @ @ @ @ @ work . <p> First , WatchKit has methods to invoke actions on the host application from the WatchKit extension . - WatchKits- openParentApplication- or **30;4192;TOOLONG methods both provide the ability to invoke actions and pass data in the containing app , and provide a mechanism to invoke a " reply " code block back in the WatchKit extension after the code in the host application has been completed . <p> For example , in the- WatchKit extension , - this will invoke an action in the host application and handle the reply : <p> Inside the host application we have access to the userInfo NSDictionary that was passed , and we can respond to it accordingly . For example , in the code below I am setting a string value on the userInfo instance , and taking appropriate actions based upon the value of that string . <p> This covers the " pull " scenario , and is great if you want to invoke actions within your host app from your WatchKit extension , and then handle the responses back in- the WatchKit extension to update your Apple Watch UI @ @ @ @ @ @ @ @ @ @ ? - The previous scenario only covers requests that originate inside the WatchKit extension . - What happens if you have a process running inside of your host app , and- have updates that you want to push to the WatchKit extension without an originating request ? <p> There is no shared memory , and it is not a shared process , so neither NSNotificationCenter or direct method invocation will work . However , you *can* use Darwin notifications ( which work across seprate processes by using- CFNotificationCenter ) . - This enables near-realtime interactions across processes , and you can share data as attributes- of a CFdictionary- object based between processes . You can also share larger amounts of data using access- groups , and notify the separate processes using the CFNotificationCenter implementation . <p> Note : CFNotificationCenter is C syntax , not Objective-C syntax . <p> First you 'll need to subscribe for the notifications in the WatchKitExtension . Pay attention to the static i 'd instance " staticSelf " you 'll need this later when invoking Objective-C methods from the C notification callback . <p> We 've now covered scenarios where @ @ @ @ @ @ @ @ @ @ application *from* the WatchKit extension , and also how you can push data from the host application to the WatchKit extension . <p> Now , what if there was a library that encapsulated some of this , and made it even easier for the developer ? - When I wrote the app in my previous post , I used the methods described above . However , I recently stumbled across the open source- MMWormhole , which wraps the Darwin Notifications method ( above ) for ease of use . - I 'm pretty sure I 'll be using this in my next WatchKit app . <h> Helpful Links for inter-process communication between WatchKit and host apps : <p> This is the first entry in a multipart series on powering native iPhone and Apple Watch apps using the IBM MobileFirst Platform . - In this entry we will cover how to setup the MobileFirst Platform for use within Apple WatchKit apps and leverage the operational analytics and remote logging features . <p> So , let 's first take a look at the app were going to build in this video : <p> The app @ @ @ @ @ @ @ @ @ @ a much simpler version of Run Keeper that will allow you to track your location path over a period of time , and show your location on a map . - Were also building a WatchKit app that enables you to quickly start or stop tracking your location without ever having to pull your iPhone out of your pocket . - All of this powered by IBM MobileFirst . <p> When you 're setting up your WatchKit app , you need to follow the exact same steps that you did for the native app target , just apply them to your WatchKit extension target . <p> First you need to add the required frameworks and dependencies ( full list here , also be sure to include **36;4224;TOOLONG that is inside the iOS API ) : <p> Add MobileFirst Frameworks and Dependencies <p> Next , add the " -ObjC " linker flag : <p> Add Linker Flag <p> Then make sure that worklight.plist ( which is inside of the MobileFirst API you generated from either the CLI or Eclipse Studio ) so that it is included in both the native app and @ @ @ @ @ @ @ @ @ @ you to take advantage of MobileFirst APIs within your WatchKit extension , complete with operational analytics. - You cansave remote logs , you can access data adapters , and more. - The server-side security mechanisms also work , so if you want to shut down your API for specific versions , you have that ability . <p> I mentioned earlier , its just like a native iOS app , but with a few exceptions . - The most important and notable exception is that the UI elements ( modal dialogs , alerts , etc ) that you would normally see in the native phone interface do not appear in the WatchKit interface . - You do n't  get errors you just do n't  see the notification . - So , you need to work around any scenarios that rely on this , and make sure you handle errors accordingly . <p> To invoke MobileFirst APIs , you call them as you wold normally in either Objective-C or Swift . - For example : <p> and the remote log search capability , including- logs from the WatchKit extension : <p> MobileFirst @ @ @ @ @ @ @ @ @ @ you need to get started ! <p> Stay tuned ! - Full source code will be released on my github account in a subsequent post. - Also be sure to stay tuned for future entries that cover the MobileFirst platform with offline data , persisting data to the server , push notifications , geo notifications , bidirectional communication between the watch and host app , background processing , and more ! I will update this post to links to each subsequent post as it is made available . <p> Wondering what IBM MobileFirst is ? - Its a platform that enables you to deliver and maintain mobile applications throughout their entire lifecycle. - This includes tools to easily manage data , offline storage , push notifications , user authentication , and more , plus you get operational analytics and remote logging to keep an eye on things once you 've deployed it to the real world , and its available as- either cloud or on-premise solutions . <p> I recently put together some content on building " Apps that Work as Well Offline as they do Online " using IBM MobileFirst @ @ @ @ @ @ @ @ @ @ the original- blog post , I used the content in a presentation at ApacheCon , and now I 've opened everything up for anyone use or learn from . <p> The content now lives on the IBM Bluemix github account , and includes code for the native iOS app , code for the web ( Node.js ) endpoint , a comprehensive script that walks through every step of of the process configuring the application , - and also a video walkthrough of the entire process from backend creation to a complete solution . <p> Key concepts demonstrated in these materials : <p> User authentication using the Bluemix Advanced Mobile Access service <p> Remote app logging and instrumentation using the Bluemix Advanced Mobile Access service <p> Client-side Objective-C code ( you can do this in either hybrid or other native platforms too , but I just wrote it for iOS ) . - The " iOS-native " folder contains the source code for a complete sample application leveraging this workflow . The " GeoPix-complete " folder contains a completed project ( still needs you to walk through backend configuration ) . The @ @ @ @ @ @ @ @ @ @ all MobileFirst/Bluemix code commented out . You can follow the steps inside of the " Step By Step Instructions.pdf " file to setup the backend infrastructure on Bluemix , and setup all code within the " GeoPix-starter " project . 
@@106848826 @2248826/ <h> Inexpensive Clamp-On GoPro Mounts <p> I 've posted about GoPro cameras here before I love mine b/c it is so versatile . You can mount it on just about anything underwater , on the ground , or in the air . There are tons of accessories for mounting it all kinds of different ways . However , the cost of these mounts can quickly add up . The adhesive mounts are essentially 1-use mounts . Once you stick them to something , they are firmly attached , and can not necessarily be easily reused multiple times . If you want a reusable mount , GoPro has their own clamp mount , but is also a bit pricey if you have other accessories . here 's how you can create an inexpensive and extremely versatile clamp mount . <p> I already had a frame mount , and a tripod mount for the GoPro , so I already had half of the equation . ( Yes , if you have to go out and purchase both of these , its just as much as GoPros clamp mount , but this approach is @ @ @ @ @ @ @ @ @ @ ) <p> To turn this into a clamp mount , you just need to add the clamp and a mounting point , which can be purchased at a hardware store for just a few dollars . You 'll need a clamp ; any size will do , you just need one big enough to put a 1/4 inch bolt through the handle . Generally , the bigger the clamp , the stronger the spring will be to hold it closed . The one I used is pretty big you do n't  necessarily need one this big . You 'll also need a 1/4 inch bolt , some washers , and some 1/4 inch nuts . I used a thumb screw with two wing nuts so that it is easy to adjust in any situation without any tools . <p> Clamp Mount Parts <p> Just drill a hole through the clamp handle big enough to fit the 1/4 inch thumb screw . Put a washer on either side , and tighten it with one of the wing nuts . Then put the other wing nut in the opposite direction , followed by the @ @ @ @ @ @ @ @ @ @ the tripod mount in the correct position . Next , just attach the " arms " that come with the GoPro , and attach the camera inside of the frame mount . Voila you know have an extremely versatile clamp mount for the GoPro . This will also work with the waterproof enclosure . <p> DIY Clamp Mount <p> Now , get out there , capture some great images and videos , and then use Creative Cloud to bring out the best in them . You can do everything from lens profile correction , horizon correction , color correction , and much , much , more on both images and videos with Photoshop , Lightroom , and all of the tools that Creative Cloud offers . 
@@106848828 @2248828/ <h> Category Archives : PhoneGap <p> HTML5 Developer Conference just wrapped up in San Francisco , and it was a great event . There was a lot to see and hear from the entire HTML/JS community . Adobe showed off some amazing showcases of the work that is being done with rich layout on the web and released an awesome new SVG library , Snap.svg. - I also had two sessions on PhoneGap . <p> First , news and announcements from Adobe <h> Snap.svg <p> Adobe has released Snap.svg , the JavaScript SVG library for the modern web . - Snap.svg is focused on making the most out of everything that SVG can offer , including masking , clipping , patterns , gradients , groups , and much more . - It is definitely worth checking out . <h> My Sessions on PhoneGap <p> Last , but certainly not least , I had two sessions on PhoneGap : one an intro , and the other a more advanced architecture topic . - Thanks to everyone who came out for my sessions . You were a great audience , and @ @ @ @ @ @ @ @ @ @ recorded by the conference organizers , and will be available at a later date . <p> You can access my presentation slides in the links below ; just use the space bar to advance each slide : <p> However , I must also apologize that a few of my samples in the " Getting Started with PhoneGap and Cross-Platform Mobile Development " did not work . I was connected to the network , but was n't able to receive any data , so I could n't access PhoneGap Build , or even add device features to a PhoneGap project from the command line tools . - I promise , these features do work when you 're on a reliable network connection . Go check out phonegap.com to learn more and get started today . <p> The recording for my session " PhoneGap and Hardware " from PhoneGap Day back in July is now available ! Be sure to check it out . There were apparently some issues with the audio , but you can still hear everything . <p> I 'd like to express a huge Thank You- to everyone who attended , and @ @ @ @ @ @ @ @ @ @ the sample projects I showed in the presentation , including source code . However , keep in mind that all of these examples were written before PhoneGap 3.0 . The native plugin syntax , and inclusion methods have changed . <h> Pressure Sensitive Sketching in PhoneGap <p> In this example , - the pressure-sensitive Pogo Connect Stylus uses a low energy Bluetooth 4 connection to relay touch/pressure information back to the PhoneGap application . This makes for a unique drawing and sketching experience powered with the HTML5 Canvas element . I 've written about this example previously Check out the video below to see it in action , and read the blog post for technical details and source code . <h> Moga Gamepad <p> The second example that I explored is a PhoneGap native plugin that is used to handle input from a Moga game controller inside of a PhoneGap application on Android . <p> This implementation is intended to be a proof of concept demonstrating how you could integrate the gamepad within your application . It currently only supports input from the joysticks ( axisX and axisY ) and the @ @ @ @ @ @ @ @ @ @ all possible input from the controller . <p> This implementation is adapted directly from the **38;4262;TOOLONG example from the Moga developers SDK samples available for download at : - http : **34;4302;TOOLONG 123434 @qwx983434 <p> The game is based on the Universe- prototype that was used as a sub-game inside of the MaxMe app for the recent Adobe MAX conference . I make no guarantees about the code for this game , it was in a huge rush ! <p> One criticism of PhoneGap apps that I sometimes hear is that they often do n't  have " standard " features from the native operating system . - Little things , like iOSs ability to scroll a container back to the top , just by tapping on the operating systems status bar . These types of features are not hard to add to a PhoneGap application , at all . This is more of an " attention to detail " issue , not something that the platform ca n't do . <p> Since this is n't a feature that is applicable on all platforms , and it can vary per PhoneGap app implementation @ @ @ @ @ @ @ @ @ @ . - However , this can be very easily added via a native plugin. - Native plugins enable you to access native code , or augment the capabilities of PhoneGap for a particular platform . <p> If you download the app from the app store today , you wo n't see this yet because I literally *just* submitted it to Apple . <p> So how did I do this ? <p> It was n't hard . The first thing to do is check and see if there was an existing native plugin that has already been created by someone in the PhoneGap/Cordova community . - It turns out , Greg- pointed out- one that already existed . - Since this plugin was built targeting an older version of PhoneGap , and my project was built using PhoneGap 3.0 , I had a few minor updates . - Though , I was able to get everything all set up in a very short period of time . <p> Then , in your PhoneGap application , you just have to add an event listener for the " statusTap " event , which is triggered @ @ @ @ @ @ @ @ @ @ . - It is literally this simple : <p> This just shows an alert that the status bar was tapped . If you want to animate specific containers , you have to do this manually yourself via JavaScript . Again , that is n't hard to do . here 's an excerpt that I used from the Halloween app , using jQuery syntax : <p> With Halloween less than a month away , I figured its about time to update my " Instant Halloween " PhoneGap sound effects app . I 'm happy to say that the latest version is now out for both iOS and Android . It has a few new sounds , a new UI style , and has been updated for iOS 7 . I also updated the low latency audio plugin- so it now supports PhoneGap 3.0 method signatures and supports the command line tooling for installation . <p> This app is fantastic for scaring people Just hook it up to really loud speakers , and start playing the sounds to your hearts content . Its got everything from background/ambient loops to maniacal laughter , screams , ghosts @ @ @ @ @ @ @ @ @ @ It is available now , for FREE , for both iOS ( 5.0+ ) and Android ( 4.0+ ) . <p> - <p> So what has changed in this version ? <p> First , I updated the app to support iOS 7 . For the most part , this is a non-issue . PhoneGap apps are based on web standards , and HTML/JS/CSS work pretty much everywhere . However , you do have to account for a few minor changes . One is that the OS status bar now sits over top of the application . You 'll need to update your UI on iOS 7 , so there are no UI issues . Check out this post from Christophe Coenraets for details regarding creating PhoneGap apps for iOS 7 . <p> iOS 7 also introduces some new UI design paradigms and guidelines . I simplified the user interface , got rid of all textures , and tried to make things as simple and minimalistic and native-feeling as possible . I also got rid of iScroll for touch-based scrolling both the iOS and Android versions now use native inertial based scrolling @ @ @ @ @ @ @ @ @ @ the new Android version is only Android 4.0 and later , but it is also the reason that the app feels much closer to a fully native experience . <p> I updated the low latency audio native plugin to support PhoneGap PhoneGap 3 . There were two parts to this : First , there is the updated method signature on the native interfaces . I just took the old plugin , and updated it for the new method signature . The new method signature- was actually introduced a while back , but I never updated the plugin for it. - Second , I added the appropriate XML metadata to enable CLI-based installation of the plugin for both iOS and Android . Take a look at the PhoneGap documentation for details on creating PhoneGap native plugins- and plugin.xml. - Also check out this tutorial from Holly Schinsky for help creating native plugins for Android . <p> In the process , I also ran into an unexpected issue with Android deployment Back in the spring I had a corrupted hard drive . I was able to recover *most* of my data , @ @ @ @ @ @ @ @ @ @ keys . It turns out that for Android , your apps must not only have the same signing keys , but also the same key store when you sign the APK files , or else Google wont let you distribute the Android APK file as an update ; it must be a new app . It turns out that I had recovered the key , but not the keystore . So , I had no choice but to distribute it as a new app . <p> Go download the free apps , get the source code , and start building your own PhoneGap apps today ! <p> About a year ago I released the Fresh Food Finder , a multi-platform mobile application built with PhoneGap . The Fresh Food Finder provides an easy way to search for farmers markets near your current location , based on the farmers markets listings from the USDA . This app has seen a lot of popularity lately , so I 'm working on a new version for all platforms with a better data feed , better UI , and overall better UX unfortunately , that @ @ @ @ @ @ @ @ @ @ able to bring it to an additional platform this week : Firefox OS ! <p> Fresh Food Finder on iOS , Firefox OS , &amp; Android <p> PhoneGap support is coming for Firefox OS , and in preparation I wanted to become familiar with the Firefox OS development environment and platform ecosystem . So I ported the Fresh Food Finder , minus the specific PhoneGap API calls . The best part ( and this really shows the power of web-standards based development ) is that I was able to take the existing PhoneGap codebase , and turn it into a Firefox OS app AND submit it to the Firefox Marketplace in under 24 hours ! If you 're interested , you can check out progress on Firefox OS support in the Cordova project , and it will be available on PhoneGap.com once its actually released . <p> Basically , I commented out the PhoneGap-specific API calls , added a few minor bug fixes , and added a few Firefox-OS specific layout/styling changes ( just a few minor things so that my app looked right on the device ) . Then you @ @ @ @ @ @ @ @ @ @ , then submit it to the app store . Check it out in the video below to see it in action , running on a Firefox OS device <p> The phone I am using is a Geeksphone Firefox OS developer device . Its not a production/consumer model , so there were a few hiccups using it , but overall it was a good experience . Also , many thanks to Jason Weathersby from Mozilla for helping me get the latest device image running on my phone . <p> You can learn more about getting started with Firefox OS development here : 
@@106848830 @2248830/ <h> Tag Archives : PhotoShop <p> One of my favorite parts of Creative Cloud is that it gives you everything you need to be creative . Whether you are into photography , video , illustration , print design , web design , or just dabble in creativity , Creative Cloud has everything that you need . Ive been doing a lot of photography lately . My main tools for retouching images are Photoshop and Lightroom Photoshop for the heavy edits and re-composition , and Lightroom for retouching/color correcting and bulk edits . <p> Using either of these tools you can turn images that originally looked " blah " into " Awesome ! " hence my tag line " bringing out the awesome " . <p> One great feature that I use in both of these tools is the ability to retouch colors using Adobe Camera RAW ( In Photoshop CC this is a filter , in Lightroom this is under the " Develop " tab ) . This gives you the ability to enhance colors , enhance clarity , add effects , heal , and much more , and @ @ @ @ @ @ @ @ @ @ of it . Check out the images below for before and after shots of a photo I recently captured from a flight over Charlotte , NC . <p> Before and after retouching in Lightroom <p> Now , check out a time-lapse video showing the retouching process , to get an idea how this was done ( more details below ) <p> That entire composition retouch only took a few minutes . Heres what I used inside of Adobe Photoshop Lightroom to put it all together . <p> The " Basic " panel gives you the ability to quickly adjust color temperature , tint , exposure , contrast , highlight/shadows/white balance , and enhance clarity and color saturation as it applies to the entire image . Just drag the sliders to see the impact in real time . <p> Basic Panel in Lightroom <p> The " Effects " panel gives you the ability to add a vignette effect to your images . You can adjust the sliders to increase vignette amount , midpoint size , roudness , feather , and more . <p> Effects panel in Lightroom <p> The " Detail @ @ @ @ @ @ @ @ @ @ noise in your images . In this case I used it to reduce luminance ( brightness ) noise on the image , but you can also apply noise reduction to colors . <p> Detail panel in Lightroom <p> The Graduated Filter and Adjustment Brush allow you easily apply localized adjustments to areas within your image . In this case , I applied two graduated filters : one to bring out contrast and definition in the sky , and another to darken and unsharpen the ground . Read more in the Lightroom documentation to learn how to use both the graduated filters and adjustment brush . <p> Graduated Filter in Lightroom <p> and that 's all I did . I did n't  even use a big fancy camera to take this photo . I used a Panasonic Lumix LX7 , which is a pretty good point and shoot camera , but its definitely not a DSLR . <p> If you want to see a bit more detail , check out a higher resolution variation over on my Flickr page . <p> Skies over Charlotte , NC <p> Now , get out there @ @ @ @ @ @ @ @ @ @ already a member of Creative Cloud , join today at creative.adobe.com . <p> Last week I had the opportunity to present an incredibly fun topic to the DC/MD/VA Creative Professionals user group GoPro Cameras , Quadcopters , and Adobe Creative Cloud . - Thanks to everyone who attended . - This topic is a personal interest of mine , and I had a great time . - There were great questions and great conversations all around . <p> For those who werent able to attend , here 's a video of the full 2-hour presentation . - The audio quality is n't perfect , but you can still catch most of it : <p> FPV : KumbaCam- Great for a remote viewfinder , though the GoPro feed flickers when in time lapse photography mode . - I put it on a tripod at eye level so I can quickly glance between LOS and FPV viewing ( FPV = First Person View ) . I use this as a remote viewfinder , not a primary flight mechanism , and never go beyond line-of-sight . <p> Without the gimbal and FPV , you 'll get @ @ @ @ @ @ @ @ @ @ the gimbal and FPV , I get about 7-8 minutes per flight I 'm currently researching options to extend battery life &amp; flight time . <p> You can definitely get bigger copters with a heavier lift capacity , but this configuration is great for getting started , and is designed specifically for the GoPro . Then , use Creative Cloud to polish your images and video . <p> here 's a quick video I put together showing how to create incredible panorama images in ten easy steps , using Adobe Photoshop CC. - I captured this one on Monday in a sunrise session- in Richmond , VA with a DJI Phantom and GoPro . <p> and the ten steps are : <p> Go get some awesome shots ! Just be sure that you 're in the same location and there is overlap between each shot <p> In my last post , I proclaimed my love for Adobe Creative Cloud . This post will show you the reason why . I was playing around with some of the aerial footage I captured last week in San Francisco . Just for fun , I wanted @ @ @ @ @ @ @ @ @ @ add to the first-person experience of the video . My inspiration was the HUD created for The Avengers &amp; Iron Man , which was created using Adobe After Effects . This turned out far better and far more interesting than I could have possibly hoped , and it is all thanks to the power of Adobe Creative Cloud . here 's the final video complete with special effects , and below I will discuss how I used Creative Cloud to get to this . ( Best experienced at 1080p , with audio preferably loud , with lots of bass . ) <p> First things first , I had to design the HUD . I used Photoshop to pull in a still from the video footage , and started layering elements on top of it . I Googled images of real fighter jet HUD displays , and used those as inspiration . I obviously did n't  have all of the same information , so I could n't make my HUD absolutely real , but I could make it look " good enough " . <p> HUD Design in Adobe Photoshop <p> I got @ @ @ @ @ @ @ @ @ @ , and then it came time to implement it for real in the video . It turns out my initial design did n't  work great in the final implementation , so I came back to Photoshop played with colors , and sizes , and chopped pieces up into separate image assets that I could pull into the final composition . <p> Next , I pulled the video into After Effects , and started overlaying the HUD graphic elements . - I chose After Effects for this instead of Premiere because After Effects has better control over the visual output and effects Premiere is my primary tool for sequencing multiple clips into a larger composition . <p> Editing in Adobe After Effects <p> I added all of the HUD elements and manually animated rotation and position so that it fit well with the actual flight path . Everything seemed in place , but I felt like it needed more . <p> Why not have targeting indicators that follow the cars ? With After Effects Track Motion feature , this was easily done . I created a " target " graphic , @ @ @ @ @ @ @ @ @ @ Motion to create a motion path for the graphic . To do this , select the video layer that you want to use for motion tracking , and go to a frame that has the object that you want to track . Then click on the " Track Motion " button in the " Tracker " panel . Youll have to select an area that will be tracked . When you analyze frames , it will detect the movement of your selection over time , and translate that to x/y coordinates , which are applied to the motion target that you choose ( the " target " symbol ) . <p> I repeated this step for a bunch of vehicles , and it started looking much better . Once I had the red target indicators in the HUD , I thought " that looks cool , but its still not enough , and its not believable . " <p> I added some color correction using After Effects Tritone color correction . - This made the HUD really stand out from the video , and gave it a nice cinematic look @ @ @ @ @ @ @ @ @ @ I thought to myself If you 're going to go " over the top " , you might as well go " way over the top " , so I started getting creative/ridiculous . I had this robotic fighter jet feel in the video , so I figured that something needed to blow up . I found this explosion and that 's when things started getting really interesting . I added one , then two , then three explosions to the scene by leveraging After Effects Linear Color Key effect so that the explosion was overlaid without the background . Add some color correction on the explosion , and voila you have an explosion on top of the video with minimal artifacts . <p> Note : Keying is the process of removing pixels from the background based on pixel colors . You can also remove pieces of a video clip using the rotoscoping tool- its like a Photoshop selection over time . <p> This was really starting to come together . Since I had explosions , I needed more smoke . I first tried the- After Effects particle system- for producing smoke @ @ @ @ @ @ @ @ @ @ specific use case. - I found some stock footage of smoke plumes and ambient smoke , and started going to town . Pairing the stock footage with Track Motion , I was able to add smoke to the landscape that followed buildings as the camera rotated to focus back on the building . <p> Like I said earlier , I wanted to go " way over the top " , so of course , why not add a flyby from some jets . So I added some stock footage of computer generated jets flying overhead , again with Keying to remove the background . <p> At this point , things were really coming together for this scene , so I wanted to add an intro title and some music enter Adobe Premiere . <p> Composing/Sequencing in Adobe Premiere <p> Here I added the title , and started adding the static effect overlaid in the beginning and the ending of the composition . - Next , I needed background music and sound effects . - Sound is critical to the experience of video . - It can help convey emotion , @ @ @ @ @ @ @ @ @ @ background music from Audio Jungle . Things were starting to come together really well , but I needed more sound effects- A while back I stumbled across freesound.org , which has a bunch of Creative Commons sound effects . - This has been a goldmine for me . I pulled in sound effects for the explosions , the ambient aircraft noise , ambient machine guns , and radar beeps . <p> Then I pulled some of the sounds into Adobe Audition for some fine-tuning <p> Audio Processing with Adobe Audition <p> Once I had everything sequenced where I wanted it , I just exported the video from Premiere , uploaded it to Youtube , and started sharing it . <p> The best parts of this entire process : <p> I did this whole thing start to finish in a little over one day . - I started working with the video on Monday night , and uploaded it to YouTube this morning . Creative Cloud has an insanely productive workflow . <p> My background is in software development , not in video production I do that for fun . By @ @ @ @ @ @ @ @ @ @ I needed to put everything together . <p> Ive been spending a lot of time with Photoshop recently Whether it has been retouching video or images , creating panoramas , or working with my aerial photos , it has been a lot of fun . One thing that I 've been doing is exporting really large images to the web . So far this has been a very manual process Export from Photoshop using Zoomify . Then , since the default Zoomify renderer uses Flash ( and I want this consumable on mobile devices ) , take the Zoomify image tiles , and put them into a custom-coded HTML experience using the Leaflet tile- engine with a custom tile layer . <p> Leaflet is normally used for web-based mapping , but it is a perfect solution for rendering image tiles on the web . It already has touch and mouse interactions , inertial scrolling , progressive viewing , and a comprehensive API that can be extended if you so choose . <p> I 've done this enough times that I figured " There has to be an easier way " and there @ @ @ @ @ @ @ @ @ @ allows you to export from Photoshops Zoomify feature directly to HTML , leveraging the Leaflet engine . All of the code and installation instructions are below in this post . Check out the video below to see it in action : <h> Samples <p> Here are few samples from the Zoomify output ; both are the compositions that I showed in the video above . Use the mouse or touch interactions to pan and zoom on each of them . <p> The first is an export from a 10MP aerial panorama ( 4340+2325 pixels ) , which was created by stitching together multiple images captured with a GoPro camera and remote controlled helicopter . <p> The second example is a massive 139MP composite image ( 14561+9570 pixels ) . I created this by stitching together 48 10mp images in Photoshop . Its not perfect , but shows how far you can zoom into an image some images had different exposures , some were out of focus , there is still some perspective warp , and I definitely have some bad stitching seams . This image is so huge that I @ @ @ @ @ @ @ @ @ @ all hard disk space with the memory swap file when creating it ( I had over 100 Gigs of free space ) ! <p> Extract the zip file and copy the following files to Photoshops Presets/Zoomify directory . On OS X , with the default configuration , these files should be located in /Applications/Adobe Photoshop CC/Presets/Zoomify/ <p> L.TileLayer.Zoomify.js <p> Zoomify Leaflet HTML.zvt <p> leaflet.css <p> leaflet.js <p> Restart Photoshop . <p> When you have a file open that you want to export , choose File -&gt; Export -&gt; Zoomify- <p> Then select the " Zoomify Leaflet HTML " template that should now be in the list . Select an output location , base name , and image options , and hit " OK " . Ignore the browser width and height , since the template ignores these . Instead , it takes 100% of the width and height of the browser window . <p> - This will generate all of the image tiles and the HTML structure . From here , do whatever you want with it You can modify it , put it on a server , or anything @ @ @ @ @ @ @ @ @ @ image below . You will have a folder that contains the generated HTML file , the Leaflet JS and CSS files , and a directory that contains the generated tiles and appropriate XML metadata. 
@@106848831 @2248831/ <h> Tag Archives : canvas <p> In a previous post on capturing user signatures in mobile applications , I explored how you capture user input from mouse or touch events and visualize that in a HTML5 Canvas . - Inspired- by activities with my daughter , I decided to take this signature capture component and make it a bit more fun &amp; exciting . - My daughter and I often draw and sketch together whether its a magnetic sketching toy , doodling on the iPad , or using a crayon and a placemat at a local pizza joint , there is always something to draw . ( Note : I never said I was actually good at drawing . ) <p> Olivia &amp; the iPad <p> You can take that exact same signature capture example , make the canvas bigger , and then combine it with a tablet and a stylus , and you 've got a decent sketching application . - However , after doodling a bit you will quickly notice that your sketches leave something to be desired . - When you are drawing on a canvas using @ @ @ @ @ @ @ @ @ @ , y ) , you are somewhat limited in what you can do . You have lines which can have consisten thickness , color , and opacity . You can adjust these , however in the end , they are only lines . <p> If you switch your approach away from moveTo and lineTo , then things can get interesting with a minimal amount of changes . You can use images to create " brushes " for drawing strokes in a HTML5 canvas element and add a lot of style and depth to your sketched content . - This is an approach that I 've adapted to JavaScript from some OpenGL drawing applications that I 've worked on in the past . - Take a look at the video below to get an idea what I mean . <p> Examining the sketches side by side , it is easy to see the difference that this makes . - The variances in stroke thickness , opacity &amp; angle add depth and style , and provide the appearance of drawing with a magic marker . <p> Sketches Side By Side <p> Its hard to @ @ @ @ @ @ @ @ @ @ to try out the apps on your own using an iPad or in a HTML5 Canvas-capable browser : <p> Just click/touch and drag in the gray rectangle area to start drawing . <p> Now , let 's examine how it all works . - Both approaches use basic drawing techniques within the HTML5 Canvas element . - If you are n't  familiar with the HTML5 Canvas , you can quickly get up to speed from the tutorials from Mozilla. <h> moveTo , lineTo <p> The first technique uses the canvass drawing context moveTo ( x , y ) and lineTo ( x , y ) to draw line segments that correspond to the mouse/touch coordinates . - Think of this as playing " connect the dots " and drawing a solid line between two points . <p> The sample output will be a line from point A , to point B , to point C : <p> lineTo ( x , y ) Stroke Sample <h> Brush Images <p> The technique for using brush images is identical in concept to the previous example you are drawing a line from point A @ @ @ @ @ @ @ @ @ @ built-in drawing APIs , you are programmatically repeating an image ( the brush ) from point A to point B. <p> First , take a look at the brush image shown below at 400% of the actual scale . - It is a simple image that is a diagonal shape that is thicker and more opaque on the left side . - By itself , this will just be a mark on the canvas . <p> Brush Image ( 400% scale ) <p> When you repeat this image from point A to point B , you will get a " solid " line . - However the opacity and thickness will vary depending upon the angle of the stroke . - Take a look at the sample below ( approximated , and zoomed ) . <p> Brush Stroke Sample ( simulated ) <p> The question is how do you actually do this in JavaScript code ? <p> First , create an Image instance to be used as the brush source . <p> brush = new Image() ; brush.src = ' assets/brush2.png ' ; <p> Once the image is loaded , @ @ @ @ @ @ @ @ @ @ the drawImage() function . The trick here is that you will need to use some trigonometry to determine how to repeat the image . In this case , you can calculate the angle and distance from the start point to the end point . Then , repeat the image based on that distance and angle . <p> This example uses the twitter bootstrap UI framework , jQuery , and Modernizr. - Both the lineTo.html and brush.html apps use the exact same code , which just uses a separate rendering function based upon the use case . - - Feel free to try out the apps on your own using an iPad or in a HTML5 Canvas-capable browser : 
@@106848833 @2248833/ <h> Monthly Archives : August 2015 <p> That title get your attention ? - Yes , it really read " Adaptive- mobile- apps that- change based on personal context " with near real-time rules application , without much extra development effort . - If that sounds interesting to you , or like a product you might want to use within your own apps , then you might want to check out this site where you can get involved in the products development : - http : **35;4338;TOOLONG <p> IBM is looking for your input on creating these types of mobile app experiences. - User experiences within a single app that can be- dramatically different per user based on location , past behavior , profile information , social media activity , - and so much more . - With this behavior being driven by configurable rules that can be changed without redeploying an app to the app store . <h> How it works for your customer <p> Consider this scenario : <p> Jon and Andrea download the mobile app for S&amp;W , a retailer known for its attention to providing @ @ @ @ @ @ @ @ @ @ and Andrea use the app to browse and discover content and merchandise differently . <p> Jon primarily navigates to sports related content for his favorite teams to find gear and clothes for travel to his favorite team 's games . Andrea scours the app for sales and fashion trends and usually ends up following her favorite designers . <p> Andrea and Jon go to a baseball game together . She 's never enjoyed watching it , so she opens up the S&amp;W app to entertain herself , and her app 's navigation quickly steers her through Spring fashion articles . <p> Jon however , wants to replace the hat he 's worn the last three times the team lost , and since he 's in the stadium , his S&amp;W app opens right up to the team 's gear page . The app knows he 's out of town and tells him how to get to an S&amp;W store . <h> How it works for the dev team <p> Consider another scenario : <p> One of the developers on the team , George , sets up the system and application @ @ @ @ @ @ @ @ @ @ for the customer experience . <p> Janet writes rules defining how the application could adapt and become more personalized based on inputs like , social media , geolocation , app usage , or customer information data . <p> Once Janet has built out her rules , she simply hits Submit ' and can immediately see her clever interactions reflected in the mobile application without having to involve the development team . <p> Analytics let Janet know which adaptations are working best , and helps her find new opportunities to optimize the app 's user experience . <p> Were not talking about a content management system , or translation based on locale , instead a rules-driven product that can adapt literally every aspect of your app : - customize the user interface , enable or disable different features , customized messaging and notifications , and much more , all variable based upon the user context . - This can be used to present- contextually relevant information , drive adoption , provide more/less data depending on your physical context , and so much more . <p> It wont be tied to a @ @ @ @ @ @ @ @ @ @ content management system , is n't attempting to re-create Google Now or Apple Proactive Assistance . - Rather , a set of tools and a rules engine that enable you to customize and tailor the app experience to the individual user . <p> Last week I had the opportunity to present to a great audience at- the- MoDev DC meetup group on " Smarter Apps with Cognitive Computing " . - In this session I focused on how you can create a voice-driven experience in your mobile apps . I gave an introduction to IBM Bluemix and IBM Watson services ( particularly the Watson language services ) , and demonstrated how you can integrate them into your native iOS apps . I also covered IBM MobileFirst for operational analytics and- remote logging to provide insight into your apps performance once it goes live . - Check out a recording of the complete presentation in the video below : 
@@106848835 @2248835/ <p> Last month I had the opportunity to speak at the DevNexus- developer conference in Atlanta on building native iOS apps- IBM MobileFirst . DevNexus is a great event , and it is always a privilege to attend- I highly recommend it for next year . - If you werent able to make it , no worries ! - Most of the sessions were recorded and are available for viewing online via dzone . <p> The recording of my- session is embedded below . - It covers everything you need to know to get started building apps with the MobielFirst platform . <p> Once your app goes live in the app store you will have just entered into an iterative cycle of updates , improvements , and releases . Each successively building on features ( and defects ) from previous versions . IBM MobileFirst Foundation gives you the tools you need to manage every aspect of this cycle , so you can deliver the best possible product to your end user . In this session , we 'll cover the process of integrating a native iOS application with IBM @ @ @ @ @ @ @ @ @ @ has to offer . <p> These two services enable you to quickly add Text-To-Speech or Speech-To-Text capability to any application . - What 's a better way to show them off than by updating my existing app to leverage the new speech services ? <p> I simply added the Text To Speech and Speech To Text services to my existing Healthcare QA application that runs on Bluemix : <p> IBM Bluemix Dashboard <p> These services are available via a REST API . Once youve added them- to your application , you can consume them easily within any of your applications . <p> I updated the code from my previous example- in 2 ways : 1 ) take advantage of the Watson Node.js Wrapper that makes interacting with Watson a lot easier and 2 ) to take advantage of these new services services . <h> Watson Node.js Wrapper <p> Using the Watson Node.js Wrapper , you can now easily instantiate Watson services in a single line of code . - For example : <p> The credentials come from your environment configuration , then you just create instances of whichever services that you @ @ @ @ @ @ @ @ @ @ for consuming a service is now much simpler than the previous version . - When we want to submit a question to the Watson QA service , you can now simply call the " ask " method on the QA service instance . <p> Below is- my server-side code from app.js that accepts a POST submission from the browser , delegates- the question to Watson , and takes the result and renders HTML using a- Jade template. - See the Getting Started Guide for the Watson QA Service to learn more about the wrappers for Node or Java . <p> Compare this to the previous version , and you 'll quickly see that it is much simpler . <h> Speech Synthesis <p> At this point , we- already have a functional service that can take natural language text , submit it to Watson , - and return a search result as text . - The next logical step for me was to add speech synthesis using the Watson Text To Speech Service- ( TTS ) . - Again , the Watson Node Wrapper and Watsons REST services- make this task very @ @ @ @ @ @ @ @ @ @ to set the src of an &lt;audio&gt; instance to the URL for the TTS service : <p> On the server you just need to synthesize the audio from the data in the URL query string . - Heres an example how to invoke the text to speech service directly from the Watson TTS sample app : <p> var textToSpeech = new **32;4375;TOOLONG ; // handle get requests app.get ( ' /synthesize ' , function ( req , res ) // make the request to Watson to synthesize the audio file from the query text var transcript = **34;4409;TOOLONG ; // set content-disposition header if downloading the // file instead of playing directly in the browser transcript.on ( 'response ' , function(response) **29;4445;TOOLONG ; if ( req.query.download ) **36;4476;TOOLONG ' = ' attachment ; filename=transcript.ogg ' ; ) ; // pipe results back to the browser as they come in from Watson transcript.pipe(res) ; ) ; <p> The Watson TTS service supports . ogg and . wav file formats . - I modified this sample is setup only with . ogg files . - On the client side , these are @ @ @ @ @ @ @ @ @ @ <p> Now that were able to process natural language and generate speech , that last part of the solution is to recognize spoken input and turn it into text . - The Watson Speech To Text ( STT ) service handles this for us. - Just like the TTS service , the Speech To Text- service also has a sample app , complete with source code to help you get started . <p> In this post I 'd like to show a fairly simple application that I put together which shows off some of the rich capabilities for IBM MobileFirst for Bluemix that you get out of the box All with an absolute minimal amount of your own developer effort . - Bluemix , of course , being IBMs platform as a service offering . <p> GeoPix is a sample application leveraging IBM MobileFirst for Bluemix to capture data and images on a mobile device , persist that data locally ( offline ) , and replicate that data to the cloud . Since its built with IBM MobileFirst , we get lots of things out of the box , including operational @ @ @ @ @ @ @ @ @ @ ( full source code at the bottom of this post ) <p> Heres what the application currently- does : <p> User can take a picture or select an image from the device <p> App captures geographic location when the image is captured <p> App saves both the image and metadata to a local data store- on the device . <p> App uses asynchronous replication to automatically save any data in local store up to the remote store whenever the network is available <p> Oh yeah , cant forget , the user auth- is via Facebook <p> MobileFirst provides all the analytics we need . - Bluemix provides the cloud based server and Cloudant- NoSQL data store . <p> All captured data is available on a web based front-end powered by Node.js <p> In this sample I 'm using everything but the Push Notifications service . - Im- using user authentication , the Cloudant DB ( offline/local store and remote/cloud store ) , and the node.js backend. - You get the operational analytics automatically . <h> Capturing Images <p> Capturing images from the device is also very straightforward . - In the @ @ @ @ @ @ @ @ @ @ either upload an existing image or capture a new image . - See the presentImagePicker and- **29;4514;TOOLONG below . All of this standard practice using Apples developer SDK : <h> Persisting Data <p> If you notice in the- **29;4545;TOOLONG method above , there is a call to- the DataManagers saveImage withLocation method . This is where we save data locally and rely on Cloudants replication to automatically save data from the local data store up to the Cloudant NoSQL database . - This is powered by the iOS 8 Data service from Bluemix . <p> The first thing that we will need to do is initialize the local and remote data stores . Below you can see my init method from my DataManager class . In this , you can see the local data store is initialized , then the remote data store is initialized . If either data store already exists , the existing store will be used , otherwise it is created . <p> Once the data stores are created , you can see that the replicate method is invoked . - This starts up the replication process @ @ @ @ @ @ @ @ @ @ remote data store " in the cloud " . <p> Therefore , if you 're collecting data when the app is offline , then you have nothing to worry about . - All of the data will be stored locally and pushed up to the cloud whenever you 're back online all with no additional effort on your part . - When using replication with the Cloudant SDK , you just have to start the replication process and let it do its thing fire and forget . <p> In my replicate function , I setup- CDTPushReplication for pushing changes to the remote data store . - You could also setup two-way replication to automatically pull new changes from the remote store . <p> Once weve setup the remote and local data stores and setup replication , we now are ready to save the data the were capturing within our app . <p> Next is my saveImage withLocation method . - Here you can see that it creates a new- **26;4576;TOOLONG object ( this is a generic object for the Cloudant NoSQL database ) , and populates it with the location data and @ @ @ @ @ @ @ @ @ @ UIImage ( passed in from the UIImagePicker above ) and adds the jpg as an attachment to the document revision . - Once the document is created , it is saved to the local data store . - We then let replication take care of persisting this data to the back end . <p> If we want to query data from either the remote or local data stores , we can just use the performQuery method on the data store . Below you can see a method for retrieving data for all of the images in the local data store . <p> At this point we 've now captured an image , captured the geographic location , saved that data in our local offline store , and then use- replication to save that data up to the cloud whenever it is available . <p> AND <p> We did all of this without writing a single line of server-side logic . - Since this is built on top of MobileFirst for Bluemix , all the backend infrastructure is setup for us , and we get operational analytics to monitor everything that is @ @ @ @ @ @ @ @ @ @ <p> App usage <p> Active Devices <p> Network Usage <p> Authentications <p> Data Storage <p> Device Logs ( yes , complete debug/crash logs from devices out in the field ) <p> Push Notification Usage <h> Sharing on the web <p> Up until this point we have n't had to write any back-end code . However the mobile app boilerplate on Bluemix comes with a Node.js server . - We might as well take advantage of it . <p> The Node.js back end comes preconfigured to leverage the- express.js- framework for building web applications . - I added the- jade template engine and Leaflet for web-mapping , and was able to crank this out ridiculously quickly . <p> The first thing we need to do is make sure - we have our configuration variables for accessing the Cloudant service from our node app. - These are environment vars that you get automatcilly if you 're running on Bluemix , but you need to set these for your local dev environment : <p> Next you 'll se the logic for querying the Cloudant data store and preparing the data for our UI templates . You @ @ @ @ @ @ @ @ @ @ refactoring for abstraction , or whatever you want . All interactions with Cloudant are powered by the Cloudant Node.js Client <p> Last week I gave a presentation to the NYC Bluemix Meetup Group on IBM MobileFirst for Bluemix . Not familiar with the branding and have no idea what that means ? - It is a mobile backend as a service , which gives you analytics , remote logging , user auth , data persistence &amp; offline synch , push notification management , and more for your mobile applications . - Yes , as a service you can create a Bluemix account today for free and start building your apps very quickly and very efficiently. - - No problem if you werent able to make it to the meetup. - I recorded my session which you can check out in the embedded video below . <p> I know the video quality is n't fantastic , but its the best I had at the time . - ( I almost always have a GoPro with me. ) - If you want to see the code that makes all of this work in @ @ @ @ @ @ @ @ @ @ on Getting Started with Bluemix Mobile Services it has code , video tutorials and more . - Enjoy ! <p> This post specifically covers- native iOS , though there are also Android and hybrid options available . This should have everything you need to get started . It covers all aspects- from creating the app , to updating the back end , to leveraging Cloudant storage , push notifications , and monitoring &amp; logging . <p> So , without further ado , let 's get started <h> Part 1 : Getting Started with Bluemix Mobile Services <p> In this first video I show how to create a new mobile app on Bluemix , connect to the cloud app instance , and implement- remote logging from the client application . This process is covered in more detail in the Getting Started docs , but below- are the basics from my experience . <p> Youll- first need to- sign into your Bluemix account . If you do n't  already have one , you can create a trial account- for free . Once you 're signed in , you just need to create a new @ @ @ @ @ @ @ @ @ @ , and there is a " wizard " to guide you . The first thing that you need to do is create a new app by clicking the big " Create an App " button on your bluemix dashboard . <p> Create a new app from IBM Bluemix Dashboard <p> Next , select which kind of app you 're going to create . For MBaaS , you 'll want to select the " Mobile " option . <p> Select the type of app <p> Next you 'll need to select your platform target . You can choose either " iOS , Android , Hybrid " , or the " iOS 8 beta " target . In this case I chose the iOS 8 beta , but the process is similar for both targets. - Hybrid apps are built leveraging the Apache Cordova- container . <p> Select your platform target <p> Next , just specify an app name and click " Finish " . <p> Give your app a name <p> Once your app is created , you will be presented with instructions how to connect the app in Xcode . I 'll get to @ @ @ @ @ @ @ @ @ @ been created , you 'll be able to see it on your Bluemix dashboard . This app will consist of several components : a Node.js back-end instance , a Cloudant NoSQL database instance , an Advanced Mobile Access instance , and a Push instance . The Advanced Mobile Access component provides you with app analytics , user auth management , remote logging , and more . The Push component gives you the ability to manage and send push notifications ( either manually , or with a rest-based API ) . <p> You app has been created here are the components and the activity <p> Once your app has been created , you will need to setup the mobile app to connect to Bluemix to consume the services . Again , this is a very straightforward process . <p> The next step is to register your client application . Once your app is created , you will be presented with a screen to do this . If you do n't  complete it right away , you can always come back later and register an application . You 'll need to specify the Bundle @ @ @ @ @ @ @ @ @ @ setup any authentication ( if you choose ) . <p> Register your apps bundle I 'd and version <p> Once your app has been registered , you need to configure Xcode . Youll first need to create a new project in Xcode . There are two options for configuring your Xcode project : 1 ) automated installation using CocoaPods , or 2 ) manual installation . I used the CocoaPods installation simply because it is easier and manages dependencies for you . <p> If you are n't  familiar with CocoaPods , it is much like NPM CocoaPods is a dependency manager for Cocoa projects . It helps you configure- the Bluemix libraries and manages dependencies for you . <p> If you 've got Xcode up and running , then close it and install CocoaPods , if you do n't  already have it . Next open up a terminal/command prompt , go to the directory that contains your Xcode project and initialize CocoaPods- using the " setup " - command : <p> pod setup <p> This will create a new file called " podfile " . Open this file in any text editor @ @ @ @ @ @ @ @ @ @ any lines that you do n't  want to actually use ) : <p> source ' https : **32;4604;TOOLONG ' # Copy the following list as is and # remove the dependencies you do not need pod ' IMFCore ' pod ' IMFGoogleAuthentication ' pod ' **25;4638;TOOLONG ' pod ' IMFURLProtocol ' pod ' IMFPush ' pod ' CloudantToolkit ' <p> Save the changes to the " podfile " file , and close the text editor . Then go back to your command promprt/terminal- - and run the installation process : <p> pod install <p> Your project will be configured , and all dependencies will be downloaded automatically . Once this is complete , open up the newly created . xcworkspace file ( Xcode Workspace ) . <p> You have to initialize the Bluemix inside of your application to connect to the cloud service to be able to take advantage of any Bluemix features ( logging , data access , auth , etc ) . The best place to put this is inside of your AppDelegate.m- class- application **29;4665;TOOLONG method because it is the first code that will be run within @ @ @ @ @ @ @ @ @ @ wanted to take advantage of was remote collection of client-side logs . You can do this using the IMFLogger class , in much the same fashion as you do with OCLogger in MobileFirst Foundation server . Once great feature that requires almost no- additional configuration is the- **25;4696;TOOLONG method , which automatically configures the Advanced Mobile Access component to collect information for all app crashes . <p> Next , launch your app in the iOS simulator , or on a device , and you 'll see everything come together . Log into your Bluemix dashboard , and you 'll be able to monitor app analytics and remote logs . <p> Note : If you experience any issues connecting to the Bluemix mobile app from within the iOS simulator , just clear the iOS Simulator by going to the menu command " iOS Simulator -&gt; Reset Content and Settings " , and everything should connect properly the next time you launch the app . <h> Part 2 : Configuring the Node.js Backend <p> In the next video , I demonstrate how to- grab the code for the backend Node.js application , create a @ @ @ @ @ @ @ @ @ @ for local development . <p> When the app is created , you 'll see an " add git " link under the app name . Using this link , you can create a git repository for the backend code . <p> Add a git repository <p> Once your git repo has been created , you can check out the code using any Git client ( I used the CLI ) . You 'll need to use the " npm install " command to pull down all the app dependencies . The biggest thing you need to know is that it uses express.js for the web application framework. - - You can make any changes that you want , and they will be automatically deployed to your Bluemix server instance upon commit . Yes , this workflow is also configurable b/c this process may not work for everyone . <p> One other thing that you will need to watch out for if you are doing local development : You will want to wrap the following code on line 6 in a try/catch block , otherwise you will hit errors in the local environment @ @ @ @ @ @ @ @ @ @ Part 3 : Consuming Data from Cloudant <p> Another part of Bluemix mobile applications is the Cloudant NoSQL database . The Cloudant NoSQL database is a powerful solution that gives you remote storage , querrying , and client-side- data storage mechanisms with automatic online/offline synchronization , all with monitoring/analytics capabilities . <p> By default , objects within the Cloudant data store are treated as generic objects ( over-simplification : think of it is an extremely powerful JSON store in the cloud ) . However you can also serialize your objects to strong data types within the client code configuration . <p> In your AppDelegate class- application **29;4723;TOOLONG method , you 'll also want to initialize the IMFDataManager class , which is the class used for interacting with all Cloudant data operations . <p> IMFDataManager *manager = IMFDataManager sharedInstance ; <p> In my sample , I setup the database manually with open permissions , but you 'll probably want something more secure . Once your database is created , you can create indexes , search for data , create data , etc <p> In the following code , I create a search index @ @ @ @ @ @ @ @ @ @ You really only need to create the index if it does n't  already exist . You can do this either through the mobile app code , or manually through the Cloudant databases web interface . I did this inline in the following code , just for the sake of simplicity : <h> Part 4 : Push Notifications <p> The IBM Bluemix mobile services app also contains a component for managing push notifications within your mobile applications . With this service , you can send push notifications to a specific device , a group of devices using tags , or all devices , and you can send push notifications either manually via the web interface , or as part of an automated workflow using the REST API . <h> Part 5 : Monitoring and Logging <p> Did I- mention that every action that you perform through Bluemix Mobile Services can be monitored ? Analytics are available for the Advanced Mobile Access component , the Cloudant NoSQL data store , and the Push Notifications service . In addition , you also have remote collection of client logs and crash reports . This @ @ @ @ @ @ @ @ @ @ . 
@@106848838 @2248838/ <h> Category Archives : JavaScript <p> Today Raymond Camden and I hosted another open session on PhoneGap- as part of an Adobe TechLive event . - These sessions are an opportunity for anyone to stop in and ask us questions . <p> The Q&amp;A transcript from todays session is below . - Thanks to everyone for sticking around , and bearing through our technical difficulties The normal Q&amp;A pod was n't working for some reason , so we had to improvise. - Well make sure this is working for next time . <p> This was our third event , and we 've had a great turnout so far , so we will be holding these open sessions once a month . - If you werent able to make it this time , well be having another one soon . Just check the- Adobe TechLive- page for future events . <p> In January 2012 , I started the year with a post on multi-screen applications developed with PhoneGap . In that post , I describe an approach for creating mobile applications that extend the app experience onto a second screen where the @ @ @ @ @ @ @ @ @ @ having an " external monitor " capability on a mobile device . <p> Mobile App Drives External Screen <p> Now , I 'm going to turn things around I 've been experimenting with a few ideas of connected secondary-experience applications , and I figured this would be a great way to come full circle and end 2012 . I see the secondary app experience as having huge potential for our connected/media-centric world . The secondary app experience is different in that the application is your " second screen " , perhaps a companion to something else that you are doing . For example , the secondary screen is a mobile application that augments the experience of watching television . Perhaps it is a mobile application that augments the experience of playing a video game , along the same concept as Xbox Smart Glass- though not tied to a particular platform . The key element is that the mobile application is not only an augmented experience to the television-based content , but that it is also updated in real time as you watch the program , or as you play the game . @ @ @ @ @ @ @ @ @ @ I 'll show a proof-of-concept second screen experience where the content of a mobile PhoneGap application is being driven by an external source ( a video ) using- audio watermarks . In this case , the mobile application is the " second screen " , and your TV is the primary screen . I 'd also like to emphasize that this is just a proof of concept the methods and code in this example are not yet suitable for a production-quality use case for reasons I 'll describe below , but are a great starting point for further exploration . <h> The Concept <p> Let 's start with the core concept : a synchronized experience between a content source ( TV or other ) and a mobile application . Since we are talking about TV or media-based content , you cant rely on being able to create a client-server architecture for synchronization between the media source and the mobile app . This just would n't be possible due to legacy TV hardware , and the fact that there is no way to digitally synchronize the content . However , TVs are great at producing sounds @ @ @ @ @ @ @ @ @ @ to invoke actions within a mobile application . <p> Now let 's focus on audio watermarks : Audio watermarks are markers embedded within an audio signal . They may be either human-imperceptible- or within the range of human hearing . In general , humans can hear frequencies between 20Hz and 20kHz , with that range decreasing with age . While we may not be able to hear the markers , mobile devices are able to detect them . When these markers are " heard " by your device , they can invoke an action within your application . <p> Next , Let 's take a look at my proof of concept application . The proof of concept exemplifies a mobile application themed with content from the HBO series Game of Thrones , - synchronized- with the opening scene from the TV series . As castles &amp; cities are shown in the video , the content within the mobile applications is updated to show details about each location . In a nutshell : <p> Proof of Concept <p> In the video below , you can see the proof of concept in action . @ @ @ @ @ @ @ @ @ @ application , with a brief description from yours truly . <h> The Application Implementation <p> There are several ways that you can do audio watermarks . The most basic of which is to embed a single tone in the audio stream , and check for the presence of that tone . The first thing that I started exploring was how to identify the dominant frequency of a sound . A quick Google search- yielded- an answer in the first result . This post not only describes how to detect the dominant sound frequency on iOS , but also has a downloadable project in github that you can use to get started . No exaggeration , I had this project up and running within minutes . It operates kind of like a guitar tuner the application detects the dominant frequency of a sound and displays that in the UI . <p> At a much later time , I also discovered this sample project from Apple , which also demonstrates how to detect frequencies from an audio stream ( used for the frequency waveform visualization ) . This will be useful for @ @ @ @ @ @ @ @ @ @ Once I had the sample native iOS project up and running , I started exploring inaudible audio " tones " and testing what the devices could- accurately- detect . I initially started using tones above the 20kHz frequency range , so that humans would not be able to hear the watermark . Tones in the range of 20-22kHz worked great for the iPhone , but I quickly realized that the iPad microphone was incapable of detecting these watermarks , so I dropped down to the 18-20kHz range , which the iPad was able to pick up without any problems . Most adults wont be able to hear these frequencies , but small children may hear them , and they may drive your pets crazy . <p> The first thing I did was create " pure " audio tones at specific frequencies using Adobe Audition . In Audition , create a new waveform , then go to the " Effects " menu and select " Generate Tones " . From here , you can create audio tones at any frequency . Just specify your frequency and the tone duration , and @ @ @ @ @ @ @ @ @ @ make sure that the tone was long enough to be- perceived- by the device . <p> Generate Tones in Adobe Audition <p> I did this for tones in the range of 18-22kHz , and saved each in a separate wav file . Some of which you can find in the GitHub sample . These files were used for testing , and were embedded in the final video . <p> To embed the audio watermarks in the video , I fired up Adobe Premiere- and started adding the inaudible tones at specific points in time within the video . <p> Audio Tones in Adobe Premiere <p> By playing specific tones at specific times , you can synchronize events within your application to those specific tones . This means that you can reliably synchronize in-app content with video content . <p> Let me reiterate this is only a proof of concept implementation . These watermarks worked great locally , but would n't work in a real-world solution as-is . I also ran into a few major issues when embedding the watermarks See the " lessons learned " below for details . <h> The @ @ @ @ @ @ @ @ @ @ the native code example and turn it into a PhoneGap native plugin so that it can be used within a PhoneGap application . I stripped out the native user interface and exposed an API that would allow the PhoneGap/JavaScript content to register to listen for specific frequencies . If these frequencies are registered as the dominant frequency of a sound , the native plugin invokes the JavaScript callback JavaScript function that is mapped to that particular frequency . Using this approach , a unique JavaScript function can be assigned to each frequency . <p> The final step was to build a user interface using HTML , CSS , &amp; JavaScript that could respond to the audio watermarks . This was the easy part . First , I created a basic project that showed the reception and handling of specific audio frequencies . Next , I created the actual application content themed around the Game Of Thrones video . <p> Audio Watermark Enabled Applications <h> The Final Product <p> You can view the completed project , the sample tones , and the video containing the embedded tones on GitHub . <h> @ @ @ @ @ @ @ @ @ @ and I definitely learned a lot while doing it . Below are just a few of my findings : <p> Dominant Frequency <p> Dominant Frequency watermarks are not the way to go in a real-world solution for many reasons . The main reason being that the watermark has to be the loudest and most predominant frequency in the captured audio spectrum . If there is a lot of other audio content , such as music , sound effects , talking , etc , then the watermark has to be louder than all of the other content , otherwise it will not be detectable . This alone is problematic . If you are normalizing or compressing your audio stream , this can cause even more problems . A multi-frequency watermark that is within the audible range , but is un-noticeable- would be a more reliable solution . <p> High-Frequency Watermarks <p> High-frequency watermarks are also problematic . High-pitch frequencies may be beyond the capabilities of hardware devices . Speakers may have problems playing these frequencies , or microphones may have problems detecting these frequencies , as I discovered with the iPad @ @ @ @ @ @ @ @ @ @ media . Many compression formats/codecs will remove frequencies that are beyond human hearing , thus removing your watermarks . Without those watermarks , there can be no synchronization of content . <p> Time-Duration or Sequential Tones <p> The current implementation only detects for a dominant frequency , without a duration . If that frequency is encountered , it triggers the listening JavaScript function regardless of how long the sound was actually being played . All of my experimental tones lasted 3 seconds , so I could ensure it played long enough to be detected . However , I noticed that some of my- frequency- listeners would be triggered if I slid my mouse across the desk . While the action of moving my mouse across my desk was very brief and I could not hear it , the action apparently generated a frequency that the- application- could detect . This triggered some of the frequencies that the app was listening for . If there was a minimum duration for the watermark frequency , this- erroneous- triggering of the event would not have occurred . You could also prevent misfires of @ @ @ @ @ @ @ @ @ @ sequence to trigger the action . <h> Media Production and Encoding <p> If you are using audio frequencies that are near the upper-range of human hearing , you have to be careful when you encode your media content . If the " inaudible " sound waveforms are over-amplified and are clipped , it has the potential to cause an extremely unpleasant high frequency noise that you can hear . I strongly- recommend- that you do not do this I learned this from experience . <p> Additionally , if you are using high-frequency tones , be careful if you transcode between 16 and 32 bit formats or if you transcode sample rates . Transcoding between 16/32 bit depth or between sample rates can cause the inaudible sounds to become audible with very unpleasant artifacts . I found that I had the best results if the Sequence settings in Premiere , the export format , and the source waveform all had the exact same bit depth ( 32 ) and sample rate ( 41000kHz ) . <h> Findings <p> From this exploration , some reading , and a lot of trial and @ @ @ @ @ @ @ @ @ @ a multi-frequency watermark for a minimum duration . Rather than having one specific frequency that dominates the audio sample , the application would detect for elevated levels of specific frequencies for a minimum period of time . This way the watermark frequencies do n't  have to overpower any other frequencies , and the watermark frequencies can be within the normal range of human hearing without being noticed . This also gives you the ability to have significantly more watermarks by using combinations of frequencies . Since the watermark tones would be within the normal range of human hearing , you also would be better able to rely on common hardware to be able to- accurately- detect those watermarks . <h> Conclusion <p> The main conclusion : not only is it really cool to control your mobile app from an audio source , this can be incredibly powerful for connected experiences . There are already TV shows and apps out in the real world employing the audio-watermarking technique to achieve a synchronized multi-screen experience . My guess is that you will start to see more of these experiences in the not-so-distant @ @ @ @ @ @ @ @ @ @ potential to work extremely well , and has applications far beyond just the synchronization of an app content with a TV show . <p> Targeted advertising : Imagine you are using an app while in a retail store &amp; you receive advertisements just be being in the store . The watermarks could be embedded within the music playing in the store . <p> Product placement : Imagine that you area watching a movie , and your favorite actor is drinking your favorite soda you look down at your device , and you also see an advertisement for that same brand of soda . <p> Museums : Imagine you have a mobile app for your favorite museum . While in the museum , there is an audio track describing the exhibits , or just playing background music . When you approach an exhibit , your app shows you details about that exhibit , all triggered by the sound being played within the museum . <p> The applications of audio watermarking are only limited by our imaginations . This is a low-cost solution that could enable connected experiences pretty much everywhere that @ @ @ @ @ @ @ @ @ @ see if these techniques are possible within PhoneGap apps , and yes , they are . <p> While PhoneGap is a multi-platform solution , you may have noticed that this proof of concept is iOS only . I 'm planning on developing this idea further on iOS , and if successful , I 'll considering porting it to other platforms . <p> Have you ever been testing or developing an application , and things just do n't  seem to be working as you 've expected ? Just imagine that the data that you are receiving from the server is n't exactly what youd expect even worse , what if this is happening in an application that youve already released to the app store , but not in your local debugging environment ? Debugging this type of error can be difficult , but it can be made MUCH easier by taking advantage of a network proxy . <p> A network proxy server acts as an intermediary between the client and server . In this case , we are talking about using a network proxy server to intercept and analyze all web traffic occurring between a @ @ @ @ @ @ @ @ @ @ that I use very often in development is Charles . <p> Charles let 's you inspect ( and even intercept and change ) all HTTP traffic either on your local machine , or on mobile devices that you control . - In this case I am using it to analyze the HTTP traffic for a PhoneGap application , but it actually intercepts all HTTP traffic for the device even HTTP requests made by native applications or " release " applications obtained via the App Store . - It wo n't give you insight into the code within an application just insight into all HTTP-based network communication . 123434 @qwx983434 <p> From the Charles proxy , you 'll be able to see every HTTP request . Once you select an HTTP request , you 'll be able to view pretty much everything about it round-trip time , request/response size , HTTP headers , query parameters , HTTP POST data , and of course , the response payload . <p> If you want , you can even use the- Breakpoint Tool- to intercept HTTP requests and modify the response payload before its returned back to your @ @ @ @ @ @ @ @ @ @ with your own applications , just follow the instructions provided in the Charles FAQ . You will quickly be " up and running " with the proxy , and able to inspect all HTTP traffic within your applications . <p> Once you 've installed and launched Charles on your computer , you 'll need to set the HTTP proxy settings on your mobile device . For iOS devices , go into settings for your Wi-Fi connection , and set up a " Manual " HTTP proxy . Then enter the IP address for the computer that is running Charles , and port 8888. - Note : The device and your computer should be on the same network segment . <p> This also works for Android devices too . You 'll need to go into the " Advanced Settings " for your Wi-Fi connection , select " Manual " proxy , enter your computers IP address for the proxy hostname , and specify port 8888 as used by Charles . <p> Once you 've got this configured , you can view all HTTP traffic in all of your applications . This is particularly useful for @ @ @ @ @ @ @ @ @ @ If you 're using the PhoneGap Emulator , you can just use Chromes Developer Tools without having to worry about configuring a proxy . <p> The application that I was debugging in the video is my US Census Browser , an open source PhoneGap application for visualizing US Census data . You can download this application for yourself via : <p> About two weeks ago I had a random thought " Wouldnt it be fun to build a free Halloween sound effects app using PhoneGap ? " ( I tend to get excited for any holiday ) . I had already done a lot of work using sounds in PhoneGap apps with my LowLatencyAudio native plugin , so the groundwork had already been laid . - Next thing you know , I 'm searching for sound files ( freesound.org is a fantastic resource for creative commons sound effects ) . Write some JS code here , add some CSS styles code there , and next thing you know I had a complete app . The best part this app went from idea to App Store submission in under 48 hours- ( yes @ @ @ @ @ @ @ @ @ @ . Add one more day , and its ready for Android . - That is the power of PhoneGap . <p> Now , I 'd like to introduce you to " Instant Halloween " . Instant Halloween is a Halloween-themed sound effects app built with PhoneGap . My app store description is : <p> Instant Halloween is a fun holiday application to help you scare the pants off of your friends ! Use it to create a creepy ambiance , or play scary Halloween sound effects to liven up any situation . This app is best experienced turned up really loud ! <p> Check it out in the video below , and make sure you have your sound turned on ! - <p> It is definitely best when turned up really loud . There 's nothing like an ear-splitting shriek combined with the thumping bass of the heartbeat and thunder in the background . I connected it to a home theater with a decent subwoofer , and it was awesome . Setup some speakers in the bushes next to your front door , and you could use this to really scare some @ @ @ @ @ @ @ @ @ @ your actions . ) <p> You can download it for free today for both iOS and Android devices : <p> - <p> It supports both phone and tablet form factors , which you can see in action here : <p> I started out using CSS media queries and Twitter Bootstrap for a responsive layout . - However , I ran into a few issues with column layout and content wrapping , so I ended up going with a totally custom solution . <p> Of course , just releasing a new PhoneGap app in the ecosytems is n't enough . - As usual , I 'm releasing the full source of this application for anyone to explore and learn from . - All HTML/JS/CSS source code between the iOS and Android versions is exactly the same ( note : I 've submitted an updated build pending approval for iOS to bring it in parity with the Android build ) . <h> Sounds <p> The audio assets are not redistributed in GitHub due to copyright law . However , all of these assets are available under Creative Commons licenses from- freesound.org . Be sure to @ @ @ @ @ @ @ @ @ @ to use them in any commercial or non-commercial work . You can access specific download links in the- readme . <p> All sound files were converted to 16bit 22050 Hz MP3 files . You can use a higher quality/bitrate if you want . I chose the lower quality due to skipping and memory issues on older/low-end Android devices . <h> Attribution <p> The font used within this application is- " Creepsville " , available for free . An embedded font was chosen over a hosted web font so that the font will still work in offline scenarios . The TTF font was converted for in-browser use with- Font Squirrel Generator . <p> I normally do n't  write posts to talk about a specific product from another company , but this is the first product that I have seen which let 's you do automated UI testing in PhoneGap applications , so I figured I 'd share <p> Automated testing is often a critical step in enterprise application development . I 'm sure most everyone out there is familiar with automated unit testing , where you write specific code to test and validate business @ @ @ @ @ @ @ @ @ @ done on the client side or on the server side , and there are lots of existing tools and languages that can be leveraged for automated unit testing . - For PhoneGap applications , you can write unit tests that evaluate your JavaScript logic to verify proper behavior in your apps using a number of tools , but these only test JS execution and logic . <p> Automated UI testing enables you to test the GUI of your application . It allows you to make sure proper buttons or controls are displayed wherever they are supposed to be displayed , that they have the proper user interaction , and generally that your applications user interface is functioning properly . There are numerous tools to enable automated UI testing on- the- desktop by recording user input and turning that user input into- reproducible- scripts . However on mobile devices , there are n't  as many options . You can write code to create test scripts for both iOS and Android , but writing these tests for PhoneGap applications could get challenging . <p> Recently , while attending the 360iDev conference , @ @ @ @ @ @ @ @ @ @ . At first glance , I thought " oh , that 's cool , automated ui tests for native iOS apps " , but doubted it would work with PhoneGap apps . It turns out I was wrong . You can use it to record and automate UI tests for native , web , or hybrid iOS applications . Below is a video of it in action with the " Walkable " PhoneGap app that I recently released . - You can record test scripts , and play them back at any time , and it will evaluate whether the test was a success of failure if the UI responds as it did in the original script . <p> Test Studio allows you to capture user input in your iOS application and play it back as a script without touching or interacting with the application yourself . - There are just a few steps to get it up and running : <p> Configure a testable build of your application Note : Do not follow the " quick instructions " in the confirmation email you get when you download the SDK it @ @ @ @ @ @ @ @ @ @ <p> Deploy your app to your device . <p> Launch Test Studio , and follow the UI prompts to " Record A Test " . <p> At any time , access and execute any existing script . <p> However , be forewarned , it is not seamless and error-free . I 've noticed in testing my own applications that when executing recorded scripts , Test Studio often attempts to execute steps before my HTML DOM elements actually exist . - This is a common issue in automated UI testing , - especially- when testing single-page dynamic HTML applications regardless of whether they are PhoneGap or in a desktop browser . You have to be very deliberate and precise when you record your steps , and at certain points , I had to manually introduce a wait period in the script to enable enough time for my HTML DOM elements to be created . - In Test Studio , there are a few ways you can do this : <p> Introduce a finite " wait " duration <p> Test to see if the HTML elements exist before continuing with script execution <p> @ @ @ @ @ @ @ @ @ @ tests . - It can be a painstaking process to record accurate UI test scripts , but can be useful for QA processes for many applications . 
@@106848839 @2248839/ <h> Monthly Archives : August 2013 <p> About a year ago I released the Fresh Food Finder , a multi-platform mobile application built with PhoneGap . The Fresh Food Finder provides an easy way to search for farmers markets near your current location , based on the farmers markets listings from the USDA . This app has seen a lot of popularity lately , so I 'm working on a new version for all platforms with a better data feed , better UI , and overall better UX unfortunately , that version is n't ready yet . However , I have been able to bring it to an additional platform this week : Firefox OS ! <p> Fresh Food Finder on iOS , Firefox OS , &amp; Android <p> PhoneGap support is coming for Firefox OS , and in preparation I wanted to become familiar with the Firefox OS development environment and platform ecosystem . So I ported the Fresh Food Finder , minus the specific PhoneGap API calls . The best part ( and this really shows the power of web-standards based development ) is that I was able to @ @ @ @ @ @ @ @ @ @ a Firefox OS app AND submit it to the Firefox Marketplace in under 24 hours ! If you 're interested , you can check out progress on Firefox OS support in the Cordova project , and it will be available on PhoneGap.com once its actually released . <p> Basically , I commented out the PhoneGap-specific API calls , added a few minor bug fixes , and added a few Firefox-OS specific layout/styling changes ( just a few minor things so that my app looked right on the device ) . Then you put in a mainfest.webapp configuration file , package it up , then submit it to the app store . Check it out in the video below to see it in action , running on a Firefox OS device <p> The phone I am using is a Geeksphone Firefox OS developer device . Its not a production/consumer model , so there were a few hiccups using it , but overall it was a good experience . Also , many thanks to Jason Weathersby from Mozilla for helping me get the latest device image running on my phone . <p> You @ @ @ @ @ @ @ @ @ @ here : <p> Last week , new release-candidate versions of Camera Raw and Lightroom were posted to Adobe Labs that feature additional camera and lens support . I was extremely excited when I found out that one of the new camera profiles supported is the GoPro- Hero 3 . I 'm a huge fan of GoPro- cameras , and this means we now have more ways to get more creative with their usage . <p> I was recently thinking I absolutely love the shots I get off of the GoPro , but I wish there was an easy way to reduce the fisheye distortion . I wanted to try my hand at creating some aerial panoramas , but the distortion was causing issues . You can reduce the distortion using Photoshops Adaptive Wide Angle filter , but that can be tedious to get right . This release makes the process of reducing fisheye dead simple . <p> Reducing the lens distortion is now as simple as selecting the GoPro camera profile in Camera Raws lens correction settings , and you can use it to create some incredible images . Check out @ @ @ @ @ @ @ @ @ @ correction to both images and videos in Adobe Photoshop , Adobe Lightroom , and Adobe Bridge . <h> Images <p> Here are some side-by-side comparisons of before and after applying the GoPro lens correction . <h> Panoramas <p> I 've also been able to use this feature to create some awesome ( in my opinion ) aerial panoramas using the- DJI Phantom- quad-rotor remote controlled helicopter . The easiest way to create one of these panoramas is to select the images you want to merge inside of Adobe Bridge . Then right-click and select " Open in Camera Raw " , and then apply the lens correction to all of the images . Once you 've done that , keep the same images selected and go to Tools -&gt; Photoshop -&gt; Photomerge inside of Adobe Bridge . This will launch the Photomerge process inside Photoshop , and after a few minutes , you will have a nice panorama to work with . Take the generated panorama , turn it into a smart object , and then you can start applying other filters ( including Camera Raw ) to it , and you @ @ @ @ @ @ @ @ @ @ few panoramas Ive created . Click on any one of them to view an interactive panorama , where you can zoom into the full resolution of the image . <p> To make the panoramas interactive , I used Photoshops " Zoomify " export , combined with the Leaflet- mapping library for an interactive HTML experience . You can ignore the HTML it generates , but keep the images and XML configuration file . I then used this open source Zoomify Layer for Leaflet- to make the images fully interactive , without any plugin dependencies . You can pinch/zoom and pan the images , and they are loaded as individual tiles , so its a smooth experience for the end user . <h> Next Steps <p> Go get started , and have fun ! You can download Camera Raw 8.2 and Lightroom 5.2 release candidates from labs.adobe.com just make sure to get the correct Camera Raw plugin for your suite ( CS6 or CC ) . <p> Also , check out this video produced by Adobes own Russell Brown for additional information : 
@@106848840 @2248840/ <h> Category Archives : MobileFirst <p> Last week I gave a presentation to the NYC Bluemix Meetup Group on IBM MobileFirst for Bluemix . Not familiar with the branding and have no idea what that means ? - It is a mobile backend as a service , which gives you analytics , remote logging , user auth , data persistence &amp; offline synch , push notification management , and more for your mobile applications . - Yes , as a service you can create a Bluemix account today for free and start building your apps very quickly and very efficiently. - - No problem if you werent able to make it to the meetup. - I recorded my session which you can check out in the embedded video below . <p> I know the video quality is n't fantastic , but its the best I had at the time . - ( I almost always have a GoPro with me. ) - If you want to see the code that makes all of this work in much , much more detail , check out my post on Getting Started with @ @ @ @ @ @ @ @ @ @ more . - Enjoy ! <p> This post specifically covers- native iOS , though there are also Android and hybrid options available . This should have everything you need to get started . It covers all aspects- from creating the app , to updating the back end , to leveraging Cloudant storage , push notifications , and monitoring &amp; logging . <p> So , without further ado , let 's get started <h> Part 1 : Getting Started with Bluemix Mobile Services <p> In this first video I show how to create a new mobile app on Bluemix , connect to the cloud app instance , and implement- remote logging from the client application . This process is covered in more detail in the Getting Started docs , but below- are the basics from my experience . <p> Youll- first need to- sign into your Bluemix account . If you do n't  already have one , you can create a trial account- for free . Once you 're signed in , you just need to create a new mobile app instance . <p> The process is very simple , and there is @ @ @ @ @ @ @ @ @ @ thing that you need to do is create a new app by clicking the big " Create an App " button on your bluemix dashboard . <p> Create a new app from IBM Bluemix Dashboard <p> Next , select which kind of app you 're going to create . For MBaaS , you 'll want to select the " Mobile " option . <p> Select the type of app <p> Next you 'll need to select your platform target . You can choose either " iOS , Android , Hybrid " , or the " iOS 8 beta " target . In this case I chose the iOS 8 beta , but the process is similar for both targets. - Hybrid apps are built leveraging the Apache Cordova- container . <p> Select your platform target <p> Next , just specify an app name and click " Finish " . <p> Give your app a name <p> Once your app is created , you will be presented with instructions how to connect the app in Xcode . I 'll get to that in a moment <p> Now that- your app has been created , you 'll @ @ @ @ @ @ @ @ @ @ This app will consist of several components : a Node.js back-end instance , a Cloudant NoSQL database instance , an Advanced Mobile Access instance , and a Push instance . The Advanced Mobile Access component provides you with app analytics , user auth management , remote logging , and more . The Push component gives you the ability to manage and send push notifications ( either manually , or with a rest-based API ) . <p> You app has been created here are the components and the activity <p> Once your app has been created , you will need to setup the mobile app to connect to Bluemix to consume the services . Again , this is a very straightforward process . <p> The next step is to register your client application . Once your app is created , you will be presented with a screen to do this . If you do n't  complete it right away , you can always come back later and register an application . You 'll need to specify the Bundle I 'd and version of your app , then you can setup any authentication ( @ @ @ @ @ @ @ @ @ @ I 'd and version <p> Once your app has been registered , you need to configure Xcode . Youll first need to create a new project in Xcode . There are two options for configuring your Xcode project : 1 ) automated installation using CocoaPods , or 2 ) manual installation . I used the CocoaPods installation simply because it is easier and manages dependencies for you . <p> If you are n't  familiar with CocoaPods , it is much like NPM CocoaPods is a dependency manager for Cocoa projects . It helps you configure- the Bluemix libraries and manages dependencies for you . <p> If you 've got Xcode up and running , then close it and install CocoaPods , if you do n't  already have it . Next open up a terminal/command prompt , go to the directory that contains your Xcode project and initialize CocoaPods- using the " setup " - command : <p> pod setup <p> This will create a new file called " podfile " . Open this file in any text editor and paste the following ( note : you can remove any lines that you @ @ @ @ @ @ @ @ @ @ ' https : **32;4754;TOOLONG ' # Copy the following list as is and # remove the dependencies you do not need pod ' IMFCore ' pod ' IMFGoogleAuthentication ' pod ' **25;4788;TOOLONG ' pod ' IMFURLProtocol ' pod ' IMFPush ' pod ' CloudantToolkit ' <p> Save the changes to the " podfile " file , and close the text editor . Then go back to your command promprt/terminal- - and run the installation process : <p> pod install <p> Your project will be configured , and all dependencies will be downloaded automatically . Once this is complete , open up the newly created . xcworkspace file ( Xcode Workspace ) . <p> You have to initialize the Bluemix inside of your application to connect to the cloud service to be able to take advantage of any Bluemix features ( logging , data access , auth , etc ) . The best place to put this is inside of your AppDelegate.m- class- application **29;4815;TOOLONG method because it is the first code that will be run within your application : <p> One of the first features I wanted to take advantage @ @ @ @ @ @ @ @ @ @ do this using the IMFLogger class , in much the same fashion as you do with OCLogger in MobileFirst Foundation server . Once great feature that requires almost no- additional configuration is the- **25;4846;TOOLONG method , which automatically configures the Advanced Mobile Access component to collect information for all app crashes . <p> Next , launch your app in the iOS simulator , or on a device , and you 'll see everything come together . Log into your Bluemix dashboard , and you 'll be able to monitor app analytics and remote logs . <p> Note : If you experience any issues connecting to the Bluemix mobile app from within the iOS simulator , just clear the iOS Simulator by going to the menu command " iOS Simulator -&gt; Reset Content and Settings " , and everything should connect properly the next time you launch the app . <h> Part 2 : Configuring the Node.js Backend <p> In the next video , I demonstrate how to- grab the code for the backend Node.js application , create a git repository on IBM JazzHub , then pull the code for local development . @ @ @ @ @ @ @ @ @ @ " add git " link under the app name . Using this link , you can create a git repository for the backend code . <p> Add a git repository <p> Once your git repo has been created , you can check out the code using any Git client ( I used the CLI ) . You 'll need to use the " npm install " command to pull down all the app dependencies . The biggest thing you need to know is that it uses express.js for the web application framework. - - You can make any changes that you want , and they will be automatically deployed to your Bluemix server instance upon commit . Yes , this workflow is also configurable b/c this process may not work for everyone . <p> One other thing that you will need to watch out for if you are doing local development : You will want to wrap the following code on line 6 in a try/catch block , otherwise you will hit errors in the local environment which will prevent your app from launching locally : <h> Part 3 : Consuming @ @ @ @ @ @ @ @ @ @ is the Cloudant NoSQL database . The Cloudant NoSQL database is a powerful solution that gives you remote storage , querrying , and client-side- data storage mechanisms with automatic online/offline synchronization , all with monitoring/analytics capabilities . <p> By default , objects within the Cloudant data store are treated as generic objects ( over-simplification : think of it is an extremely powerful JSON store in the cloud ) . However you can also serialize your objects to strong data types within the client code configuration . <p> In your AppDelegate class- application **29;4873;TOOLONG method , you 'll also want to initialize the IMFDataManager class , which is the class used for interacting with all Cloudant data operations . <p> IMFDataManager *manager = IMFDataManager sharedInstance ; <p> In my sample , I setup the database manually with open permissions , but you 'll probably want something more secure . Once your database is created , you can create indexes , search for data , create data , etc <p> In the following code , I create a search index and query for data from the remote Cloudant database . You really only need @ @ @ @ @ @ @ @ @ @ . You can do this either through the mobile app code , or manually through the Cloudant databases web interface . I did this inline in the following code , just for the sake of simplicity : <h> Part 4 : Push Notifications <p> The IBM Bluemix mobile services app also contains a component for managing push notifications within your mobile applications . With this service , you can send push notifications to a specific device , a group of devices using tags , or all devices , and you can send push notifications either manually via the web interface , or as part of an automated workflow using the REST API . <h> Part 5 : Monitoring and Logging <p> Did I- mention that every action that you perform through Bluemix Mobile Services can be monitored ? Analytics are available for the Advanced Mobile Access component , the Cloudant NoSQL data store , and the Push Notifications service . In addition , you also have remote collection of client logs and crash reports . This provides- - unparalleled insight into- the health of your applications . <p> This is @ @ @ @ @ @ @ @ @ @ refer to a scalable virtual cluster- of computing or storage resources . - Bluemix is IBMs suite of cloud service offerings , and covers lots of use cases : <p> Bluemix is an open-standards , cloud-based platform for building , managing , and running apps of all types , such as web , mobile , big data , and smart devices . Capabilities include Java , mobile back-end development , and application monitoring , as well as features from ecosystem partners and open source " all provided as-a-service in the cloud . <p> Why is it a hot topic ? - MBaaS- enables growth of mobile applications- with seamless ( and virtually endless ) scalability , all without having to manage individual systems for the application server , database , identify management , push notifications , or platform-specific services . <p> Ive been writing a lot about IBM MobileFirst lately for a seamless API to deliver mobile apps to multiple platforms ; though it has been- in the context of an on-premise installation . - However , did you know that many of the exact same MobileFirst features are available @ @ @ @ @ @ @ @ @ @ The mobile data service includes- a- NOSQL database ( powered by IBM Cloudant ) , file storage- capabilities , and appropriate management and analytics features to measure the number of calls , storage usage , time/activity , and OS distribution . <p> Push Notifications The push notification service allows you to easily push data to the right people at the right time on- either Apple APNS or Google GCM platforms all with a single API. - Notifications can be sent by either an app or backend system , and can be sent to a single device , or a group of devices based on their tags/subscriptions. - Of course , with appropriate analytics for monitoring activity , distribution , and engagement . <p> Many of these are the exact same features that you can host in your own on-premise IBM MobileFirst Platform Foundation server the difference is that you do n't  have to maintain the infrastructure . - You can scale as needed through the Bluemix cloud offering . <p> Push notifications , love them or hate them , are everywhere and there 's no getting around it . Push notifications @ @ @ @ @ @ @ @ @ @ regardless of whether the apps are actually running . They can be used to send reminders , drive engagement with the mobile app , notify completion of long running processes , and more . Push notifications- send information to you in real time , rather than you having to request that information . <p> If you are building a back-end infrastructure to manage your applications data , and you want to leverage push notifications , then guess what ? You also have to build the hooks to manage subscription and distribution of push notifications for each platform . <p> The- unified- push notification API allows you to develop your app against a single API , yet deliver push notifications to multiple platforms , and it works with both hybrid ( HTML/CSS/JS ) apps , as well as native apps . <p> You will still have to build the logic to subscribe devices for messaging , and dispatch push notification messages , but you 'll only have to do it once against the unified API not once for each platform . <p> The apps that I showed in the video above are @ @ @ @ @ @ @ @ @ @ guide for iOS and Android , and can be accessed in their entirety ( with both client and server code ) using the links below : <p> On the client app , you 'll need to subscribe for messages from the event source . See the hybrid or native code- linked to above for syntax and examples . <p> Once your clients are subscribed , you can use a single server-side implementation to distribute messages to client apps . Below is an- excerpt from the sample application which demonstrates sending a push notification to all devices for a particular user ( on any platform ) : <p> From- the MobileFirst console , you will be able to monitor and manage event sources , platforms , and the devices that are consuming push notifications . <p> Push Notifications on the MobileFirst Console <p> If you were wondering , yes , these can be cloud-hosted on IBM BlueMix and yes , it can also be installed on-premise on your own server in your data center . - You have the option to configure- your physical or cloud servers however you want . <p> @ @ @ @ @ @ @ @ @ @ is the ability to capture client-side- logs from mobile devices out in the wild in a central location ( on the server ) . - That means you can capture information from devices *after* you have deployed your app into production . - If you are trying to track down or recreate bugs , this can be incredibly helpful . Let 's say that users on iOS 7.0 , specifically on iPhone 4- models are having an issue . - You can capture device logs at this level of granularity ( or at a much broader scope , if you choose ) . <p> The logging classes in the MobileFirst Platform Foundation are similar in concept to- Log4J. - You have logging classes that you can use to write out- trace , debug , info , log , warn , fatal , or error messages . - You can also optionally specify a package name , which is used to identify which code module the debug statements are coming from . - With the package name , you 'll be able to see if the log message is coming from a user @ @ @ @ @ @ @ @ @ @ view , or any other class based upon how you setup your loggers. - Once the log file reaches the specified buffer size , it will automatically be sent to the server . <p> On the server you can setup log profiles that- determine the level of granularity of messages that are captured on the server . - Let 's say you have 100,000 devices consuming- your app. - You can configure the profiles to collect error or fatal messages for every app instance . - However , you probably do n't  want to capture complete device logs for every app instance ; You can setup the log profiles to only capture complete logs for a specific set of devices . <p> As an- example , take a look at the screenshot below to see how you can setup log collection profiles : <p> Configuring Log Profiles on the MobileFirst Server <p> When writing your code , you just need to create a logger instance , then write to the log . <p> If you 're curious when you might want a trace statement , vs. a log statement , vs. a @ @ @ @ @ @ @ @ @ @ from the docs : <p> Then on the server , you can go into the analytics dashboard and access complete logs for a device , or search through all client-side logs with the ability to filter on application name , app versions , log levels , package name , environment , device models , and OS versions within an optional date range , and with the ability to search for keywords in the log message . 
@@106848843 @2248843/ <p> In this post I 'd like to show a fairly simple application that I put together which shows off some of the rich capabilities for IBM MobileFirst for Bluemix that you get out of the box All with an absolute minimal amount of your own developer effort . - Bluemix , of course , being IBMs platform as a service offering . <p> GeoPix is a sample application leveraging IBM MobileFirst for Bluemix to capture data and images on a mobile device , persist that data locally ( offline ) , and replicate that data to the cloud . Since its built with IBM MobileFirst , we get lots of things out of the box , including operational analytics , user authentication , and much more . <p> ( full source code at the bottom of this post ) <p> Heres what the application currently- does : <p> User can take a picture or select an image from the device <p> App captures geographic location when the image is captured <p> App saves both the image and metadata to a local data store- on the device . <p> App @ @ @ @ @ @ @ @ @ @ store up to the remote store whenever the network is available <p> Oh yeah , cant forget , the user auth- is via Facebook <p> MobileFirst provides all the analytics we need . - Bluemix provides the cloud based server and Cloudant- NoSQL data store . <p> All captured data is available on a web based front-end powered by Node.js <p> In this sample I 'm using everything but the Push Notifications service . - Im- using user authentication , the Cloudant DB ( offline/local store and remote/cloud store ) , and the node.js backend. - You get the operational analytics automatically . <h> Capturing Images <p> Capturing images from the device is also very straightforward . - In the app I leverage Apples- UIImagePickerController- to allow the user to either upload an existing image or capture a new image . - See the presentImagePicker and- **29;4904;TOOLONG below . All of this standard practice using Apples developer SDK : <h> Persisting Data <p> If you notice in the- **29;4935;TOOLONG method above , there is a call to- the DataManagers saveImage withLocation method . This is where we save data locally and @ @ @ @ @ @ @ @ @ @ local data store up to the Cloudant NoSQL database . - This is powered by the iOS 8 Data service from Bluemix . <p> The first thing that we will need to do is initialize the local and remote data stores . Below you can see my init method from my DataManager class . In this , you can see the local data store is initialized , then the remote data store is initialized . If either data store already exists , the existing store will be used , otherwise it is created . <p> Once the data stores are created , you can see that the replicate method is invoked . - This starts up the replication process to automatically push changesfrom the local data store to the remote data store " in the cloud " . <p> Therefore , if you 're collecting data when the app is offline , then you have nothing to worry about . - All of the data will be stored locally and pushed up to the cloud whenever you 're back online all with no additional effort on your part . - When using @ @ @ @ @ @ @ @ @ @ start the replication process and let it do its thing fire and forget . <p> In my replicate function , I setup- CDTPushReplication for pushing changes to the remote data store . - You could also setup two-way replication to automatically pull new changes from the remote store . <p> Once weve setup the remote and local data stores and setup replication , we now are ready to save the data the were capturing within our app . <p> Next is my saveImage withLocation method . - Here you can see that it creates a new- **26;4966;TOOLONG object ( this is a generic object for the Cloudant NoSQL database ) , and populates it with the location data and timestamp. - It then creates a jpg image from the UIImage ( passed in from the UIImagePicker above ) and adds the jpg as an attachment to the document revision . - Once the document is created , it is saved to the local data store . - We then let replication take care of persisting this data to the back end . <p> If we want to query data from @ @ @ @ @ @ @ @ @ @ just use the performQuery method on the data store . Below you can see a method for retrieving data for all of the images in the local data store . <p> At this point we 've now captured an image , captured the geographic location , saved that data in our local offline store , and then use- replication to save that data up to the cloud whenever it is available . <p> AND <p> We did all of this without writing a single line of server-side logic . - Since this is built on top of MobileFirst for Bluemix , all the backend infrastructure is setup for us , and we get operational analytics to monitor everything that is happening . <p> With the operational analytics we get : <p> App usage <p> Active Devices <p> Network Usage <p> Authentications <p> Data Storage <p> Device Logs ( yes , complete debug/crash logs from devices out in the field ) <p> Push Notification Usage <h> Sharing on the web <p> Up until this point we have n't had to write any back-end code . However the mobile app boilerplate on Bluemix comes @ @ @ @ @ @ @ @ @ @ take advantage of it . <p> The Node.js back end comes preconfigured to leverage the- express.js- framework for building web applications . - I added the- jade template engine and Leaflet for web-mapping , and was able to crank this out ridiculously quickly . <p> The first thing we need to do is make sure - we have our configuration variables for accessing the Cloudant service from our node app. - These are environment vars that you get automatcilly if you 're running on Bluemix , but you need to set these for your local dev environment : <p> Next you 'll se the logic for querying the Cloudant data store and preparing the data for our UI templates . You can customize this however you want caching for performance , refactoring for abstraction , or whatever you want . All interactions with Cloudant are powered by the Cloudant Node.js Client <p> A while back I wrote about adding parallax effects to your HTML/JS experiences to make them feel a bit richer and closer to a native experience . - I 've just added this subtle ( key word *subtle* ) effect to @ @ @ @ @ @ @ @ @ @ to share here . <p> If you are wondering what I am talking about with " parallax effects " Parallax movement is where objects in the background move at a different rate than objects in the foreground , thus causing the perception- of depth . - Read more about it if you 're interested . <p> First , here 's a quick video of this latest app in action . - Its a hybrid MobileFirst app , but this technique could be used in any **35;4994;TOOLONG web app experience . - The key is to keep it subtle and not too much " in your face " , and yes , it is very subtle in this video . - You have to watch closely . <p> The techniques that I wrote about in the previous post still apply Ive just added a bit more to cover more use cases . <p> This sets the background image and default position . - The distinct change here is that I set the background size to " auto " width and 120% height . - In this case , you can have a huge @ @ @ @ @ @ @ @ @ @ window size , or a small image that scales up to a larger window size . - This way you do n't  end up with seams in a repeated background or a background that is too big to highlight the parallax effect effectively . <p> In the requestAnimationFrame loop , it only applies changes *if* there are changes to apply . - This prevents needless calls to apply CSS even if the CSS styles had n't  changed . - In this , I also truncate- the numeric CSS string so that it is n't reapplying CSS if the position should shift by 0.01 pixels . Side note : If you are n't  using requestAnimationFrame for HTML animations , you should learn about- it . <p> If you used my old code and were holding the device upside down , it would n't work . - Not even a little bit . - This has that fixed ( see comments inline above ) . <p> This moves the background in CSS , which does n't  cause browser reflow operations , and moves the foreground content ( inside of a div ) using translate3d @ @ @ @ @ @ @ @ @ @ - This helps keep animations smooth and the UX performing optimally . <p> I also added a global variable to turn parallax on and off very quickly , if you need it . <p> The result is a faster experience that is more efficient and less of a strain on CPU and battery . - Feel free to test this technique out on your own . <p> If you use the code above , you can modify the xMovement and yMovement variables to exaggerate the parallax effect . <p> This post specifically covers- native iOS , though there are also Android and hybrid options available . This should have everything you need to get started . It covers all aspects- from creating the app , to updating the back end , to leveraging Cloudant storage , push notifications , and monitoring &amp; logging . <p> So , without further ado , let 's get started <h> Part 1 : Getting Started with Bluemix Mobile Services <p> In this first video I show how to create a new mobile app on Bluemix , connect to the cloud app instance , and implement- @ @ @ @ @ @ @ @ @ @ covered in more detail in the Getting Started docs , but below- are the basics from my experience . <p> Youll- first need to- sign into your Bluemix account . If you do n't  already have one , you can create a trial account- for free . Once you 're signed in , you just need to create a new mobile app instance . <p> The process is very simple , and there is a " wizard " to guide you . The first thing that you need to do is create a new app by clicking the big " Create an App " button on your bluemix dashboard . <p> Create a new app from IBM Bluemix Dashboard <p> Next , select which kind of app you 're going to create . For MBaaS , you 'll want to select the " Mobile " option . <p> Select the type of app <p> Next you 'll need to select your platform target . You can choose either " iOS , Android , Hybrid " , or the " iOS 8 beta " target . In this case I chose the iOS 8 beta @ @ @ @ @ @ @ @ @ @ Hybrid apps are built leveraging the Apache Cordova- container . <p> Select your platform target <p> Next , just specify an app name and click " Finish " . <p> Give your app a name <p> Once your app is created , you will be presented with instructions how to connect the app in Xcode . I 'll get to that in a moment <p> Now that- your app has been created , you 'll be able to see it on your Bluemix dashboard . This app will consist of several components : a Node.js back-end instance , a Cloudant NoSQL database instance , an Advanced Mobile Access instance , and a Push instance . The Advanced Mobile Access component provides you with app analytics , user auth management , remote logging , and more . The Push component gives you the ability to manage and send push notifications ( either manually , or with a rest-based API ) . <p> You app has been created here are the components and the activity <p> Once your app has been created , you will need to setup the mobile app to connect to @ @ @ @ @ @ @ @ @ @ a very straightforward process . <p> The next step is to register your client application . Once your app is created , you will be presented with a screen to do this . If you do n't  complete it right away , you can always come back later and register an application . You 'll need to specify the Bundle I 'd and version of your app , then you can setup any authentication ( if you choose ) . <p> Register your apps bundle I 'd and version <p> Once your app has been registered , you need to configure Xcode . Youll first need to create a new project in Xcode . There are two options for configuring your Xcode project : 1 ) automated installation using CocoaPods , or 2 ) manual installation . I used the CocoaPods installation simply because it is easier and manages dependencies for you . <p> If you are n't  familiar with CocoaPods , it is much like NPM CocoaPods is a dependency manager for Cocoa projects . It helps you configure- the Bluemix libraries and manages dependencies for you . <p> If you 've got @ @ @ @ @ @ @ @ @ @ CocoaPods , if you do n't  already have it . Next open up a terminal/command prompt , go to the directory that contains your Xcode project and initialize CocoaPods- using the " setup " - command : <p> pod setup <p> This will create a new file called " podfile " . Open this file in any text editor and paste the following ( note : you can remove any lines that you do n't  want to actually use ) : <p> source ' https : **32;5031;TOOLONG ' # Copy the following list as is and # remove the dependencies you do not need pod ' IMFCore ' pod ' IMFGoogleAuthentication ' pod ' **25;5065;TOOLONG ' pod ' IMFURLProtocol ' pod ' IMFPush ' pod ' CloudantToolkit ' <p> Save the changes to the " podfile " file , and close the text editor . Then go back to your command promprt/terminal- - and run the installation process : <p> pod install <p> Your project will be configured , and all dependencies will be downloaded automatically . Once this is complete , open up the newly created . xcworkspace file @ @ @ @ @ @ @ @ @ @ the Bluemix inside of your application to connect to the cloud service to be able to take advantage of any Bluemix features ( logging , data access , auth , etc ) . The best place to put this is inside of your AppDelegate.m- class- application **29;5092;TOOLONG method because it is the first code that will be run within your application : <p> One of the first features I wanted to take advantage of was remote collection of client-side logs . You can do this using the IMFLogger class , in much the same fashion as you do with OCLogger in MobileFirst Foundation server . Once great feature that requires almost no- additional configuration is the- **25;5123;TOOLONG method , which automatically configures the Advanced Mobile Access component to collect information for all app crashes . <p> Next , launch your app in the iOS simulator , or on a device , and you 'll see everything come together . Log into your Bluemix dashboard , and you 'll be able to monitor app analytics and remote logs . <p> Note : If you experience any issues connecting to the Bluemix mobile app @ @ @ @ @ @ @ @ @ @ Simulator by going to the menu command " iOS Simulator -&gt; Reset Content and Settings " , and everything should connect properly the next time you launch the app . <h> Part 2 : Configuring the Node.js Backend <p> In the next video , I demonstrate how to- grab the code for the backend Node.js application , create a git repository on IBM JazzHub , then pull the code for local development . <p> When the app is created , you 'll see an " add git " link under the app name . Using this link , you can create a git repository for the backend code . <p> Add a git repository <p> Once your git repo has been created , you can check out the code using any Git client ( I used the CLI ) . You 'll need to use the " npm install " command to pull down all the app dependencies . The biggest thing you need to know is that it uses express.js for the web application framework. - - You can make any changes that you want , and they will be automatically @ @ @ @ @ @ @ @ @ @ , this workflow is also configurable b/c this process may not work for everyone . <p> One other thing that you will need to watch out for if you are doing local development : You will want to wrap the following code on line 6 in a try/catch block , otherwise you will hit errors in the local environment which will prevent your app from launching locally : <h> Part 3 : Consuming Data from Cloudant <p> Another part of Bluemix mobile applications is the Cloudant NoSQL database . The Cloudant NoSQL database is a powerful solution that gives you remote storage , querrying , and client-side- data storage mechanisms with automatic online/offline synchronization , all with monitoring/analytics capabilities . <p> By default , objects within the Cloudant data store are treated as generic objects ( over-simplification : think of it is an extremely powerful JSON store in the cloud ) . However you can also serialize your objects to strong data types within the client code configuration . <p> In your AppDelegate class- application **29;5150;TOOLONG method , you 'll also want to initialize the IMFDataManager class , which is the @ @ @ @ @ @ @ @ @ @ <p> IMFDataManager *manager = IMFDataManager sharedInstance ; <p> In my sample , I setup the database manually with open permissions , but you 'll probably want something more secure . Once your database is created , you can create indexes , search for data , create data , etc <p> In the following code , I create a search index and query for data from the remote Cloudant database . You really only need to create the index if it does n't  already exist . You can do this either through the mobile app code , or manually through the Cloudant databases web interface . I did this inline in the following code , just for the sake of simplicity : <h> Part 4 : Push Notifications <p> The IBM Bluemix mobile services app also contains a component for managing push notifications within your mobile applications . With this service , you can send push notifications to a specific device , a group of devices using tags , or all devices , and you can send push notifications either manually via the web interface , or as part of an automated @ @ @ @ @ @ @ @ @ @ Monitoring and Logging <p> Did I- mention that every action that you perform through Bluemix Mobile Services can be monitored ? Analytics are available for the Advanced Mobile Access component , the Cloudant NoSQL data store , and the Push Notifications service . In addition , you also have remote collection of client logs and crash reports . This provides- - unparalleled insight into- the health of your applications . <p> This is more than just " Cloud Services " which more generally refer to a scalable virtual cluster- of computing or storage resources . - Bluemix is IBMs suite of cloud service offerings , and covers lots of use cases : <p> Bluemix is an open-standards , cloud-based platform for building , managing , and running apps of all types , such as web , mobile , big data , and smart devices . Capabilities include Java , mobile back-end development , and application monitoring , as well as features from ecosystem partners and open source " all provided as-a-service in the cloud . <p> Why is it a hot topic ? - MBaaS- enables growth of mobile applications- @ @ @ @ @ @ @ @ @ @ without having to manage individual systems for the application server , database , identify management , push notifications , or platform-specific services . <p> Ive been writing a lot about IBM MobileFirst lately for a seamless API to deliver mobile apps to multiple platforms ; though it has been- in the context of an on-premise installation . - However , did you know that many of the exact same MobileFirst features are available as- MBaaS services on IBM Bluemix ? <p> Mobile Data The mobile data service includes- a- NOSQL database ( powered by IBM Cloudant ) , file storage- capabilities , and appropriate management and analytics features to measure the number of calls , storage usage , time/activity , and OS distribution . <p> Push Notifications The push notification service allows you to easily push data to the right people at the right time on- either Apple APNS or Google GCM platforms all with a single API. - Notifications can be sent by either an app or backend system , and can be sent to a single device , or a group of devices based on their tags/subscriptions. - @ @ @ @ @ @ @ @ @ @ distribution , and engagement . <p> Many of these are the exact same features that you can host in your own on-premise IBM MobileFirst Platform Foundation server the difference is that you do n't  have to maintain the infrastructure . - You can scale as needed through the Bluemix cloud offering . <p> Push notifications , love them or hate them , are everywhere and there 's no getting around it . Push notifications are short messages that can be sent to mobile devices regardless of whether the apps are actually running . They can be used to send reminders , drive engagement with the mobile app , notify completion of long running processes , and more . Push notifications- send information to you in real time , rather than you having to request that information . <p> If you are building a back-end infrastructure to manage your applications data , and you want to leverage push notifications , then guess what ? You also have to build the hooks to manage subscription and distribution of push notifications for each platform . <p> The- unified- push notification API allows you to @ @ @ @ @ @ @ @ @ @ push notifications to multiple platforms , and it works with both hybrid ( HTML/CSS/JS ) apps , as well as native apps . <p> You will still have to build the logic to subscribe devices for messaging , and dispatch push notification messages , but you 'll only have to do it once against the unified API not once for each platform . <p> The apps that I showed in the video above are sample apps taken straight from the IBM MobileFirst platform developer guide for iOS and Android , and can be accessed in their entirety ( with both client and server code ) using the links below : <p> On the client app , you 'll need to subscribe for messages from the event source . See the hybrid or native code- linked to above for syntax and examples . <p> Once your clients are subscribed , you can use a single server-side implementation to distribute messages to client apps . Below is an- excerpt from the sample application which demonstrates sending a push notification to all devices for a particular user ( on any platform ) : <p> From- @ @ @ @ @ @ @ @ @ @ and manage event sources , platforms , and the devices that are consuming push notifications . <p> Push Notifications on the MobileFirst Console <p> If you were wondering , yes , these can be cloud-hosted on IBM BlueMix and yes , it can also be installed on-premise on your own server in your data center . - You have the option to configure- your physical or cloud servers however you want . 
@@106848844 @2248844/ <h> IBM MobileFirst Platform Foundation <p> IBM MobileFirst Platform Foundation- ( formerly known as Worklight- Foundation ) is a platform for building mobile applications for the enterprise . - It is a suite- of tools and services available either on-premise or in the cloud , which enable you to rapidly build , administer , and monitor secure applications . <p> The MobileFirst Platform Foundation consists- of : <p> MobileFirst Server the middleware tier that provides a gateway between back-end systems and services and the mobile client applications . - The server enables application authentication , data endpoints/services , data optimization and transformation , push notification management ( streamlined API for all platforms ) , consolidated logging , and app/services- analytics . For development purposes , the MobileFirst server is available as either part of the MobileFirst Studio ( discussed below ) , or as command line tools . <p> The server-side API enables you to expose data adapters to your mobile applications these adapters could be consuming data from SQL databases , REST or SOAP Services , or JMS data sources . The Server side API also provides a @ @ @ @ @ @ @ @ @ @ platforms ) , and data **26;5181;TOOLONG services . You can leverage the server-side API in JavaScript , or dig deeper and use the Java implementation . <p> The client-side API is available for native iOS ( Objective-C ) , native Android ( Java ) , J2ME , C# native Windows Phone ( C# ) , and JavaScript for cross-platform hybrid OR mobile-web applications . For the native implementations , this includes user authentication , encrypted storage , push notifications , - logging , geo-notifications , data access , and more . - For hybrid applications , it includes everything from- the native API , plus cross-platform- native UI components and- platform specific application skinning . - With the hybrid development approach , you can even push updates to your applications that are live , out on devices , without having to push an update through an app store . - Does the hybrid approach leverage Apache Cordova ? - YES . <p> MobileFirst Studio - an optional all-inclusive development environment for developing enterprise apps on the MobileFirst platform . - This is based on the Eclipse platform , and includes @ @ @ @ @ @ @ @ @ @ and test all data adapters/services , a browser-based hybrid app simulator , and the ability to generate platform-specific applications for deployment . - However , using the studio- is not required ! Try to convince a native iOS ( Xcode ) developer that they have to use Eclipse , and tell me- how that goes for you - If- you do n't  want to use the- all-inclusive studio , no problem . - You can use the command line tools ( CLI ) . - The CLI provides a command line interface for managing the MobileFirst server , creating data adapters , creating the encrypted JSON store , and more . <p> MobileFirst Console the console provides a dashboard and management portal for everything happening within your MobileFirst applications . - You can view which APIs and adapters have been deployed , set app notifications , - manage or disable your apps , report on connected devices and platforms , monitor push notifications , view analytics information for all- services and adapters exposed through the MobileFirst- server , and manage- remote collection of client app logs . - All together @ @ @ @ @ @ @ @ @ @ managing your applications . <h> MobileFirst Platform Application Scanning <p> MobileFirst Platform Application Scanning is set of tools- that can scan your JavaScript , HTML , Objective-C , or Java code for security vulnerabilities and coding best practices . - Think of it as a security layer in your software development lifecycle . <h> MobileFirst Quality Assurance <p> MobileFirst Quality Assurance- is a set of tools and features to help provide quality assurance to your mobile applications . - It includes automated crash analytics , user feedback and sentiment analysis , in-app bug reporting , over-the-air build distribution to testers , test/bug prioritization , and- more . <p> So , is MobileFirst/Worklight- just for hybrid ( HTML/JS ) apps ? You tell me if you need clarification- more information , please re-read this post and follow all the links . - <p> I 've just wrapped up my presentations for this years DevNexus event in Atlanta it has been a great event , filled with tons of information on web , mobile , and back-end development . I had 3 sessions on PhoneGap One intro , one advanced , and one @ @ @ @ @ @ @ @ @ @ . - I did n't  record them this time , since they were being recorded by the conference organizers , so expect to see a video once they 're released . <p> Just press the space bar , or use the arrow keys to view the presentation in your browser . <p> I was - searching the web earlier this week for an older presentation from a few months back , and just happened to stumble across my recent presentations from HTML5DevConf- from- this past October . Looks like the videos were posted in November , but I 'm just seeing them now . I had two sessions : - Designing and Architecting PhoneGap and Mobile Web Apps and- Getting Started with PhoneGap and Cross-Platform Mobile Development , and if you werent able to attend them , you 're still in luck ! Here are the videos from those sessions : <h> Designing and Architecting PhoneGap and Mobile Web Apps <p> Tired of Hello World ? In this session , we explore best practices to build real-world PhoneGap applications . We investigate the Single Page Architecture , HTML templates , effective Touch events @ @ @ @ @ @ @ @ @ @ compare and contrast the leading JavaScript and Mobile Frameworks . This session is a must If you plan to build a PhoneGap application that has more than a couple of screens . <h> Getting Started with PhoneGap and Cross-Platform Mobile Development <p> Unfortunately , I ran into network issues which prevented some of my samples from working in this one , but you 'll still be able to get the point . <p> HTML has emerged as a powerful alternative to " native " to enable cross-platform mobile application development . In this session , you learn how to leverage your existing HTML and JavaScript skills to build cross-platform mobile applications , how to access the device features ( camera , accelerometer , contacts , file system , etc ) using JavaScript APIs , and how to package your HTML application as a native app for distribution through the different app stores . <p> I wanted to follow up my last post on 3D Parallax effects in HTML or Adobe DPS , I 've decided to release some of the other experiments that I 've been exploring with device motion in DPS publications @ @ @ @ @ @ @ @ @ @ samples , and a corrected version of the strawberries example from my last post ( the plants were going the wrong way in the last post ) . <p> All three of these samples leverage the same basic technique for responding to device motion inside of a DPS publication . The motion-interactive components are implemented using HTML and JavaScript , and are included in publications as web content overlays . In JavaScript , it takes advantage of the ondevicemotion event handler to respond to the physical orientation of the device . <p> In all three of samples , the web content overlay is set to autoplay , with user interaction disabled . This way the HTML &amp; JavaScript automatically loads and the scripting is active , but it does n't  block interaction or gestures for DPS navigation . I also enabled " Scale Content To Fit " so that HTML content scales appropriately between retina and non-retina devices . <h> Adobe San Francisco <p> The Adobe/inline content example is implemented in the same manner as the strawberries example . The large city image It is a two-layer composition created with @ @ @ @ @ @ @ @ @ @ independently from the background image . I used Photoshop to separate the content into layers and made them animate based on device orientation in the exact same fashion as the strawberries sample . All of the text and image content surrounding the cityscape panorama is laid out with InDesign . <h> AT&amp;T Park/San Francisco Giants <p> The AT&amp;T Park/San Francisco Giants example is implemented with basic HTML and JavaScript , no additional tools were used to create this interactive scenario . - The content on the left hand side was all laid out with InDesign . The content on the right side is the interactive HTML . <p> The image used in this example is a vertical panorama captured from a remote control helicopter . This image contains various perspectives that have been composited in Photoshop . The motion of the device is aligned to match the perspectives in the image/viewport ; When the device is facing down , the image is looking down and when the device is vertical , the image faces forward . You can check out the vertical panorama image below . If you 're interested in @ @ @ @ @ @ @ @ @ @ this tutorial from Russell Brown . <p> Vertical Panorama over AT&amp;T Park <p> The HTML and JavaScript used in this example is fairly minimal . The image is applied as the background of the root HTML &lt;body&gt; element , and the position of the background is shifted based upon the device motion event . This approach keeps the HTML DOM as flat and simple as possible . <p> A few weeks ago , a fellow Adobe colleague showed me a DPS publication that had an amazing design . All of the content looked great by itself , but what really made parts of it " pop " was that in certain areas there was a 3D parallax effect , which made it feel like you were looking into an image that had depth . You could rotate the device and see what 's hiding behind a person , or around the corner of a building . <p> Heres what I mean on the surface the image looked static , but as I rotated it , elements shifted to give the illusion of depth . The background and foreground elements all moved @ @ @ @ @ @ @ @ @ @ Device <p> I thought this was an incredible example of added interactivity and immersive- experiences , and its not really that difficult to implement. - In fact , I put together this tutorial to show exactly how you can create these types of effects in your own compositions . <p> To create this kind of an effect , the first thing you need to do is break apart an image into layers note : - you may need to synthesize edges so that there is an overlap in all transparent areas . Then you need to add interactivity in HTML . Align those images so that their default state looks just like the still image , then move the images based upon the device orientation . I move the foreground one way , keep the middle content more or less stationary , and move the background content the opposite direction ( all based upon which way you are rotating the mobile device ) . Since this is all HTML , you can take this content and use it on the web , or import it into Adobe InDesign to export @ @ @ @ @ @ @ @ @ @ Layered Images <p> You can either create your own layers , or break apart an existing image into layers so that each individual layer can be placed over top each other to form a seamless composition . In this case , I separated the strawberries , the rows of plants , my daughter , and the sky out to separate layers . <p> Update 1/7/2014 : I added logic to support both landscape and portrait orientation . <p> Be sure to add both of those JavaScript snippets inside of the creationComplete event for the Stage . - I also over-exaggerated the movement in the timeline. - I think it would look better with slightly less ( more subtle ) movement . <p> At this point , you could publish the composition and use it on the web there 's nothing stopping you at all . In fact , you can check it out here , just load it on an iPad and rotate the device to see the effect . However , please keep in mind that 1 ) I have n't added a preloader , 2 ) the assets are non-optimized @ @ @ @ @ @ @ @ @ @ n't  have it auto scaling for the viewport size , so it will only look right on a retina iPad , and 4 ) I have only tested this on an iPad no other devices . <p> Note : You could also do this without using Edge Animate , but you 'd have to hand code the HTML/JS for it . <h> Step 3 : Include in InDesign/DPS Composition <p> To include this in a DPS publication , all that you need to do is export an Animate Deployment Package ( . oam file ) from Adobe Edge Animate . You can then just drag and drop this into InDesign for inclusion in a DPS publication . <p> If you are n't  already a member of Creative Cloud , join today to take advantage of all of our creative tools ! <p> Update : After publishing this I realized that the movement of the plants should actually be reversed . - If you view this link , you 'll see the updated motion- ( which looks more realistic ) , but I cant update the video that 's already been published . 
@@106848845 @2248845/ <h> Monthly Archives : September 2011 <p> Can you feel the excitement ? ! ? ! ? ! Adobe has just announced the availability of AIR 3 and Flash Player 11 for early October ! This release will bring a wave of change to the Internet and development tooling as you know them . From console-quality hardware accelerated 3D , to AIR captive runtime , to native extensions Adobe tools will enable the next wave of innovation in games , rich media , and multi-device applications . In this release there are a lot of new features to get excited about . <p> Flash Player 11 and AIR 3 will be publicly available in early October . Flash Builder and Flex will offer support for the new features in an upcoming release before the end of the year . We 'll have a lot of news and new content at MAX , and in the meantime , you can download the Flash Player and AIR betas on Labs , and start checking out some of the amazing content that 's already been built by developers ! <p> here 's a @ @ @ @ @ @ @ @ @ @ when Flex 2 was all the rage . - No , seriously , Flex 2 was awesome it is the base of todays Flex framework , helped to revolutionize applications on the web , and heralded the " RIA " frenzy . - The best part is that this post is still very relevant with Flex , AIR &amp; ActionScript for mobile/web/desktop , so I decided to resurrect it from the old blog archive . - Enjoy <p> Ive been asked several times , why would you use get/set functions instead of public variables in your flex components and classes ? Well , there are some great things you can do with getters and setters that you cant do with public variables . On the other hand , there are cases where public variables may be an easy choice . When using these functions and/or public variables , the code for the caller will be the same : <p> mycomponent.myValue = 1 ; <p> First , let 's look at public variables <p> Bindable public var myValue : Number <p> Public variables are useful when there are no addional actions that @ @ @ @ @ @ @ @ @ @ . If you change the value of " myValue " , the bindings will update and everything will be handled accordingly . The value will change , and anything bound to that value will change . In this case , there is no need to use getter/setter methods , keeping code simple and easy to implement . <p> First I 'll explain the- Bindable ( event= " myValueUpdated " ) - statement : This indicates that the data binding to the getters value should be updated when the event of type " myValueUpdated " is dispatched . <p> You 'll notice that when the value is set , this event is dispatched , which would notify and components that are bound to this value . Using a binding event is n't required for all getters and setters , however this approach can allow you to invoke binding events on the " getter " even if you do n't  access the " setter " method . <p> Now , the rest The code that I showed above is consumed in exactly the same way as a public property , but requires more code . @ @ @ @ @ @ @ @ @ @ enable sequential code execution when the value is changed , and also enable inheritance in getter/setter methods . <p> This means that you can create your components so that specific functions are executed any time that the value is accessed using get and/or set functions . <p> In this example , every time the value is set , the numSets Number is incremented , and the myFunction() function is executed . Likewise , every time the value is accessed using the " get " method , the numGets Number is incremented , and the myOtherFunction() function is executed . There is no limit to what kind of code you can execute here . You can have it dispatch custom events , change styles , create new components , etc This turns out to be very handy when creating custom Flex components . <p> As I mentioned earlier , getter/setter accessors also enable inheritance on " properties " of an object . This means that you can change the behavior of a getter/setter in descendant classes , while usage remains the same . A great example of this are the " @ @ @ @ @ @ @ @ @ @ used throughout the Flex framework ( and part of the IDataRenderer interface ) . You can override " get data " or " set data " methods to modify behaviors and/or return values , without changing how those methods are used . <p> In this series , I 'd like to highlight Adobes mobile offerings , and the variety of platforms that can be supported from a single codebase. - - I wrote a simple URL Monitoring utility application that will check the status of various HTTP endpoints using the AIR URLMonitor class . - I now have that application running on 3 OS platforms , in 4 ecosystems ( 5th coming soon ) , supporting phone and tablet interfaces , and all with a single codebase <p> Many Apps , One Platform <p> here 's the official app description that I 've been using : <p> URL Monitor is a simple diagnostic application that will allow you to quickly and easily monitor the status of various URL endpoints . Simply enter a URL into the text box and add it to the list . A polling HTTP request will be made every @ @ @ @ @ @ @ @ @ @ . HTTP codes 200 , 202 , 204 , 205 and 206 will be identified as a success with a green check . All other HTTP codes will indicate a problem as a red X. To remove a listing , simply perform a horizontal swipe on a given row to reveal a delete button . All monitoring is paused when the application is in the background . <p> Flex/AIR on Devices : From Flash Builder to BlackBerry App World ( coming soon ) <p> What 's really interesting about this is the activity that I have seen on each market . - The application is free on each market , and became available over the last few weeks . - I have done absolutely no- promotion- of it This is the first announcement that I have made . <p> As of 9/15/2011 , the application has been live in Apples store for 3 weeks , and has had 554 downloads ; it has been live in the Google Android Market for 3 weeks , and has had 20 installes ( only 11 active ) ; it has been in the Amazon @ @ @ @ @ @ @ @ @ @ , that is a zero ) ; and surprisingly , it has been on the Nook store since Sept 1 , and has had 816 downloads ! - Now , that was unexpected . <p> Once you are a registered developer , you will have access to tools that enable you to put your apps onto a nook device . - First , you must download and install the Nook SDK add-on to the Android SDK . <p> Nook SDK Setup <p> This will download and install the Nook SDK. - Once installed , the installation process will need to restart ADB. - Click " YES " and allow it to proceed through the restart process . <p> Nook SDK Setup - Complete <p> Once you are a registered developer , you will have tools accessible to enable " Developer Mode " on your device , which allows you to provision the device for development . - Go to the " Developer Mode " section of the nook developer portal and enter the device names and serial numbers for your dev devices . - From here you will be able @ @ @ @ @ @ @ @ @ @ <p> The provisioning file will allow you to deploy applications directly to the nook via USB. - - Attach the Nook Color device to your computer via a USB connection . - The Nook Color device will show up as a device in Finder. - Copy the provision.cmd file that you downloaded from the " Developer Mode " provisioning portal directly into the root of the Nook Color device . <p> Once ADB has restarted , use the " adb devices " command to list all connected Android devices ( The Nook Color device must still be connect via USB , even though it has been unmounted ) . <p> . /adb devices- <p> You should see your device listed in the output : <p> ADB - View Device <p> Now that you are able to view your Nook Color device using ADB , Flash Builder will also be able to deploy to it . - You will be able to Run/Debug directly from Flash Builder and launch applications on the Nook Color device . - - This follows the normal Flash Builder debug process . <p> However , there @ @ @ @ @ @ @ @ @ @ to the " apps " menu on the Nook color , you wo n't see any of your applications deployed via USB. - Do n't  fret they actually are on your device . - There is just a trick to view them . - From the " apps " menu , click on the " archived " button . - A popup will be displayed here . - Now , press and hold the " volume up " button . - When the speaker appears on the screen ( continue holding the volume button ) , tap on the speaker . - When you tap on the speaker , the " extras " screen is displayed . - ( You can release the volume button now. ) - The " extras " screen will list all of your manually installed applications , and you will be able to re-launch your installed apps . <p> From here , you can generate your release-build APK as you normally would from Flash Builder. - Once you have your APK generated and tested , you 're ready to prepare it for the Nook Store . - @ @ @ @ @ @ @ @ @ @ the " Applications " section . - Click on the " Add New Application " button . <p> First , you will need to enter the primary metadata for the application ( name , type , price , version , etc ) . <p> Nook Portal - New Application <p> Once you have completed the basic information , click on " Save and Continue " , and you will be redirected to the " Description &amp; EULA " tab . - Enter an application description and an optional license agreement ( I left this blank b/c I do not have any special terms for my app ) . <p> Nook Portal - App Description <p> When ready , click " Save and Continue " to enter the " Keywords and Category " tab . - On this screen , enter descriptive keywords and categories to help categorize your application within the Nook store . <p> Nook Portal - Keywords &amp; Category <p> When you have your categorical information all set , click " Save and- Continue " to proceed to the " Icons &amp; Screenshots " screen . - Here @ @ @ @ @ @ @ @ @ @ application screenshots . <p> Nook Portal - Media &amp; Images <p> Now this is where the application approval process is a little different from other app stores . - You need to click " Send for Application Approval " , and the application metadata must be approved before you can upload a binary APK file . <p> Once the application metadata is approved ( this took about a week for my app ) , you will be able to upload the APK binary . <p> Nook Portal - Upload Binary <p> After you have uploaded and submitted the binary APK file , the binary file will need to be approved before it is actually available within the Nook store . - The binary approval took another week+ , so the overall approval process took over 2 weeks . - Once approved , your application will appear in the nook store , and will be ready for public consumption ! <p> First , navigate to- https : //developer.amazon.com/- and click on the " Amazon Appstore for Android " link . Once there , you will need to walk through the full @ @ @ @ @ @ @ @ @ @ Landing Page <p> Once you click on the " Get Started " button , you will be guided through the Appstore registration process . Once that is complete , you will be directed to the Appstore developer portal home page . - From here , just click on " Add a New App " . <p> Amazon Appstore Home <p> Next , you will being the App upload wizard . - First , you will need to enter primary metadata for the application , including a title , form factor , supported languages , and contact information . - Once you have entered this information , click on the " Save " button . <p> Amazon Appstore App Metadata <p> Next , you need to enter merchandising information . - This includes the app category , keywords , a description , price , and release/availability dates . - Once you have completed all of your merchandising information , click on the " Save " button to proceed . <p> Amazon Appstore App Merchandising Info <p> Next , you will have to specify content rating information . - Just fill out @ @ @ @ @ @ @ @ @ @ " Save " button to proceed . - I did n't  run into any content rating issues in the Amazon Appstore , like I did with the iOS App Store . <p> Amazon Appstore App Content Ratings <p> Next , upload multimedia that will be associated with the application . - This includes application icons ( note : they must match , event though this screenshot does n't  show it I was rejected b/c of this ) , and actual screenshots of the application . <p> Amazon Appstore App Multimedia <p> Scroll down to see more of the " Multimedia Content " form . - You will also be able to enter promotional images and promotional video assets for your application . - Once you have uploaded all necessary multimedia , click the " Done " button . <p> Amazon Appstore App Promotional Media <p> Now you are ready to upload your APK Binary . - Follow the instructions for uploading an APK file . - Once it is uploaded you will see information about the file that was uploaded . <p> Amazon Appstore App Binary <p> Finally , click @ @ @ @ @ @ @ @ @ @ application for approval . - There is an approval process for the Amazon Appstore similar to Apples , and my application was live in less than a week from submission . 
@@106848846 @2248846/ <h> Tag Archives : mobile <p> As promised , here are my presentation slides and extra content from last weeks- RIACon conference . I gave three presentations : " Intro to PhoneGap " , " Data Visualization With Web Standards " , and " PhoneGap Native Plugins " . All of the presentations are freely available at : https : **34;5209;TOOLONG . <p> Data visualization is the art &amp; science of creating a visual representation of data and information . - Really , it can be anything : a bar chart , scatter plot , pie chart , complex flow diagram , 3d model , etc - If your visualization conveys information without having to read a table of data , then its doing what it should . Recently , the emergence of HTML5s dynamic graphics and SVG support have made rich , dynamic , and interactive graphics possible on the web without having to leverage Flash , which was previously the only real option . Be sure to check out this presentation , and read this blog post for more info : Data Visualization With Web Standards <p> @ @ @ @ @ @ @ @ @ @ applications using web technologies . - As a part of this , PhoneGap provides an API to access native operating system functionality from JavaScript . Luckily for everyone , the JavaScript-to-native bridge- is extensible and you can very easily create and expose your own custom native functionality with a JavaScript API. - Basically , all PhoneGap native plugins are made up of two parts : a native implementation , and a JavaScript interface . - Your PhoneGap application calls the JavaScript interface , which leverages cordovoa.exec to communicate with the native layer . - The native layer then performs a native operation and communicates back to the JS layer . <p> Open source PhoneGap plugins- There are lots of open source native plugins for everything from push notifications , screen captures , barcode scanners , to MapKit or even iCould ( among many others ) . <p> and of course , the sample apps : <h> iOS Multi-Screen <p> This sample app demonstrates how to create multi-screen experiences using the UIScreen API on iOS. - You can use AirPlay mirroring on an AppleTV as a second screen , the content @ @ @ @ @ @ @ @ @ @ " main " PhoneGap experience on the device . Check out the links and video below to learn more . <h> LowLatencyAudio <p> The PhoneGap LowLatencyAudio native plugin for Android and iOS allows you to preload audio , and playback that audio quickly , with a very simple to use API. - It overcomes the current limitations of the HTML5 Audio API on many mobile devices . Check out the links and video below to learn more . <p> Earlier this month the PhoneGap team held the first PhoneGap day . - This was in part to celebrate the release of PhoneGap 2.0 , but more importantly to bring together members of the PhoneGap community to share and learn from each other . - There are great recaps of PhoneGap Day from RedMonk , as well as on the PhoneGap blog . One of the new services announced on PhoneGap Day was- emulate.phonegap.com. - Emulate.phonegap.com enables an in-browser simulator for developing and debugging PhoneGap/Cordova applications , complete with Cordova API emulation . - It is built off of the Ripple Emulator , which itself is open source and may even @ @ @ @ @ @ @ @ @ @ launched , the URL that you want to simulate will be displayed within the Ripple operating environment view . <p> Note : This only works with assets that are on a live URL . You can use a local http server with references to localhost , however the emulator will fail if you try to access your application directly from the local file system using a file : // URI . <p> Update : You can enable access to local files by changing a few settings on the Ripple emulator . - See the first comment on this post for additional detail . <p> ( click for full-size image ) <p> The emulator environment gives you the ability to emulate PhoneGap events and API calls , without having to deploy to a device or run inside of the iOS , Android , Blackberry , or other emulator . - Not only can you simulate the PhoneGap/Cordova API , but you can also use Chromes debugging tools to test &amp; debug your code complete with breakpoints , memory inspection , and resource monitoring . This is a handy development configuration. - @ @ @ @ @ @ @ @ @ @ is familiar , fast &amp; easy to use . This does not replace on-device debugging however nothing will replace that . - On-device debugging is extremely important ; this helps increase your productivity as a developer . <p> So how do you use this environment ? The environment will handle Cordova API requests , and you can also simulate device events . <p> First , the " Devices " panel In this panel you can select a device configuration Everything from iOS , to Android , to BlackBerry . - Changing the device configuration will not only change the physical dimensions , but will also change Device/OS/user agent settings reported by the application . - Here you can also select the device orientation , which will change the visual area within the simulator . <p> Within the " Platforms " panel you can choose the platform you wish to emulate . - With respect to PhoneGap applications , you will want to choose " Apache Cordova " , and then select the API version that you are using . - By default , it uses " PhoneGap 1.0.0 ? , @ @ @ @ @ @ @ @ @ @ get the most recent version . - The Ripple emulator also simulates BlackBerry WebWorks and mobile web configurations as well . <p> The " Accelerometer " panel can be used to simulate device **25;5245;TOOLONG events . - Just click and drag on the device icon ( the gray and black boxes ) , and the icon will rotate in 3D. - As you drag , accelerometer events will be dispatched and handled within your application . - From here , you can even trigger a " shake " event . <p> The " Geolocation " panel enables you to simulate your geographic position within your PhoneGap/Cordova application . - You can specify a latitude , longitude , altitude , speed , etc - You can even drag the map and use it to specify your geographic position . The position that you set within the geolocation panel will be reported when using **42;5272;TOOLONG . <p> The " Config " panel is a graphical representation of your PhoneGap BuildConfig.xml file . - You can use this to easily view/analyze what 's in your application configuration . <p> The " Events " panel @ @ @ @ @ @ @ @ @ @ " deviceready " , " backbutton " , " menubutton " , " online " , and " offline " ( among others ) . - Just select the event type , and click on the " Fire Event " button . <p> As I mentioned earlier , this wont replace on-device debugging . - It also wont handle execution of native code for PhoneGap native plugins , however you can test/develop against the JavaScript interfaces for those native plugins. - Emulate.phonegap.com- will definitely help with development of PhoneGap applications in many scenarios , and is a nice complement to the- Chrome Developer Tools . <p> Ill be speaking at a few conferences in the next few months on PhoneGap and web standards-based development . - Here are just a few , with some more pending . - Be sure to come check one out ( or all of them ) ! <h> RIACON <p> Where architects and developers of all levels come to gather , share and learn about creating the next generation of web based applications. - RIAcons goal is to help you network with fellow industry professionals @ @ @ @ @ @ @ @ @ @ be speaking on the following topics at RIACON : <p> Introduction to PhoneGap Interested in developing applications for mobile devices , on multiple platforms ? Interested in leveraging your existing web development skills to build natively installed applications ? Just looking to expand your skill set ? Come join Adobe Technical Evangelist , Andrew Trice , to learn about cross platform mobile development and PhoneGap . In this session , you will get an introduction to PhoneGap ( Apache Cordova ) , be able to see example PhoneGap applications , and walk through the process of building your first PhoneGap application . <p> PhoneGap Native Plugins PhoneGap enables developers to build natively installed applications using traditional web-based development tools ( HTML &amp; JavaScript ) , but what if you want to make your application do more ? In this session , learn how to write native plugins for PhoneGap that enable you to extend the API to tap into native device functionality . <p> Data Visualization with Web Standards Do you have the requirement to create rich visual data dashboard applications , but also have the requirement to use web-standard @ @ @ @ @ @ @ @ @ @ ? Well , you 're in luck ! Come to this session to learn about data visualization strategies and frameworks powered entirely with HTML and JavaScript. <h> 360iDev <p> 360iDev is the first and still the best iPhone developer conference in the world . We 're not a publishing company pushing books , or a media company selling subscriptions . We 're a conference company , focused on community . Our goal is to bring the best and brightest in the developer community together for 3 days of incredible sessions , awesome parties , good times , and learning . If you do n't leave Wednesday night , with more ideas than you know what to do with , we 're not doing our jobs ! <p> Ill be speaking on the following topics at 360iDev : <p> Kick A$$ iOS Apps with PhoneGap Apps do n't  have to be written in native Objective-C to be awesome. - - Get ready for a crash course in PhoneGap , a tool that enables you to build natively installed iOS apps using 100% HTML &amp; JavaScript , complete with access to local APIs. @ @ @ @ @ @ @ @ @ @ to strategies for building highly performant &amp; interactive applications . <h> Dreamforce <p> Every year Dreamforce features stories and presentations from some of the brightest minds in technology , business and beyond . This years Dreamforce promises to be even more informative and dynamic , with our most exciting keynote speaker lineup yet . The cloud computing event of the year is also the Social Enterprise event of the year . This is where you 'll learn everything you need to know " from the industry leaders who are paving the way " about how the Social Enterprise revolution is changing the way we do business . <p> Ill be speaking on the following topics at Dreamforce : <p> Data Visualization with Web Standards Do you have the requirement to create rich visual data-centric applications , but also have the requirement to use web-standard technologies , and do n't  know what to do next ? Well , you 're in luck ! Come to this session to learn about data visualization strategies and frameworks powered entirely with HTML , CSS , and JavaScript . <p> Native-like Apps with PhoneGap Native applications @ @ @ @ @ @ @ @ @ @ valley " effect where they do n't  feel quite right as a native application . In this session well focus on strategies to make your apps feel like native apps , including considerations for a native-feeling UI , platform consistency , and user experience . <p> Last week I posted on the PhoneGap blog " Questions And Myths About PhoneGap " . It has a bunch of content , and if you 've been wondering/ questioning the use of PhoneGap , you should definitely check it out . 
@@106848847 @2248847/ <h> Wearables &amp; IBM MobileFirst Video &amp; Sample Code <p> Last week I attended IBM Insight in Las Vegas . It was a great event , with tons of great information for attendees . I had- a few sessions on mobile applications . In particular , my dev@Insight session on Wearables powered by IBM MobileFirst was recorded . You can check it out here : <h> Key takeaways from the session : <p> Wearables are the most personal computing devices ever . Your users can use them to be notified of information , search/consume data , or even collect environmental data for reporting or actionable analysis . <p> Regardless of whether developing for a peripheral device like the Apple Watch or Microsoft Band , or a standalone device like Android Wear , you are developing an app that runs in an environment that mirrors that of a a native app . So , the fundamental development principles are exactly the same . You write native code , that uses standard protocols and common conventions to interact with the back-end . <p> Caveat to #1 : You user interface is @ @ @ @ @ @ @ @ @ @ services to acomodate for the reduced amount of information that can be displayed . <p> You can share code across both the phone/tablet and watch/wearable experience ( depending on the target device ) . <p> Using IBM MobileFirst you can easily expose data , add authentication , and capture analytics for both the mobile and wearable solutions . 
@@106848850 @2248850/ <h> Monthly Archives : January 2013 <p> I am asked all the time " How do I get started developing PhoneGap applications ? " . My normal answer is to advise people to check out the PhoneGap Getting Started Guides , which provide a great starting point for every platform . However after further thought , I 'm not sure this is always what people are asking . Rather than " how do I get started ? " , I think people are often looking for insight into the workflow for developing PhoneGap applications . Everything from tools to developer flow , to getting the app on devices . The Getting Started Guides are essential for setting up the initial project structure , but once you get that setup , you might be wondering " what do I do next ? " . In this post , I 'll try to she 'd some light on the workflow and tools that I use when developing PhoneGap applications . <h> Know What You 're Going To Build Before You Build It <p> First and foremost it is essential to have at least some kind @ @ @ @ @ @ @ @ @ @ build it . If you just start hacking things together without a plan , the final result is seldomly great . Complete ( pixel perfect ) UI/UX mockups are fantastic , but you do n't  have to have a fully polished design and screen flow . Just having wireframes/sketches are a great start . Heck , even a sketch on a napkin is better than starting with nothing . <p> The UX design/wireframes help you understand what you application should be doing from the users perspective , which in turn helps you make decisions on how you tackle a project . This can be purely from a HTML level , helping you figure out how you should position DOM elements and/or content . Or , it can help you gauge your projects technical complexity How many " moving parts " do you have , how much of the app is dynamic or asynchronus , or how do different visual elements need to work together ? You can leverage this design/mockup to analyze the needs of your application and determine if a particular framework/development methodology is a best fit ( Bootstrap @ @ @ @ @ @ @ @ @ @ Angular.js , etc ) . <p> When working with a designer , I use Adobes Creative Suite Tools for pretty much everything wireframes , UI/UX designs , chopping up assets , etc I 'm currently working on a project that was designed by the talented- Joni from Adobe XD . Joni designed everything in Creative Suite , and I 'm using Photoshop to view screen flows and extract UI assets for the actual implementation . <p> UI Mockups in PhotoshopScreen Flow in Photoshop <p> Note : This app will also be free and open source as a sample/learning resource for PhoneGap , including all of the design assets I 'll follow up with another post on this later , once the app is- available- in the app stores . <p> If you are n't  a " graphics person " , or do n't  have creative suite , there are a bunch of other tools that you can use for wireframing and/or sketching ( but cmon , Creative Cloud is only $50 a month ) . Here are several Ive used with great success , but this is not a comprehensive list at all @ @ @ @ @ @ @ @ @ @ OS X. This is fantastic for wireframing or documenting screen flows . In fact , the screen flow image shown in Photoshop above was originally composed in Omnigraffle , using the mockups created in Photoshop . <p> Visio- A powerful drag &amp; drop wireframing/design tool for Windows much like OmniGraffle , but for windows . <p> PowerPoint- or Keynote- - These are n't  just for presentations . They can be really useful for putting together screen flow diagrams , or annotating images/content . <p> Often people like to sketch out ideas &amp; wireframes on their tablets , here are a few tools that I use for that : <p> iBrainstorm A great app for collaboratively taking notes and sketching . I 'm partial to this one b/c used to be on the dev team , and I wrote a good chunk of the graphics sketching logic . <p> There are a bunch of other tablet sketching apps out there , but I have n't used most of them . <h> Coding Environment <p> Coding environments are a tricky subject . There is no single solution that meets the exact wants and needs @ @ @ @ @ @ @ @ @ @ some people chose large-scale IDEs , some people use designer-centric tools , and many of these choices are- dependant- upon which operating system you use or your background as a designer or developer . Since PhoneGap applications are really just editing HTML , CSS &amp; JavaScript , you can use whatever editor you want . In fact , I know a several people that use vim as their primary editor . <h> Large-Scale IDEs <p> I 'm a bigger fan of of using a complete IDE ( integrated development environment ) than I am of a lightweight editor , simply b/c IDEs tend to have hooks into more features/languages , etc I know people complain about startup time , but there is no startup time if you leave it open all the time . <p> There are a few catches when talking about IDEs with PhoneGap . The first is that if you want to deploy anything locally to devices ( without using PhoneGap Build ) , you have to delpoy using the IDE for the particular platform that you are- targeting . That means Xcode for iOS , Eclipse for @ @ @ @ @ @ @ @ @ @ However if you wish , you can use your editor of choice , and just use the IDE to deploy to devices locally . You can even share source code across several IDE installations using symlinks ( which I describe here ) . I very often use this type of a configuration to share code between Xcode , Eclipse , and WebStorm . <p> My preference for coding PhoneGap applications is to use- WebStorm by JetBrains . WebStorm has great code-hinting ( even for your own custom JS and 3rd party libraries ) , great refactoring , hooks into Git , CVS , or SVN repositories , and is a very mature IDE . <p> WebStorm IDE <p> I tend to use this as my primary coding tool , then switch to Eclipse or Xcode when I want to locally deploy to a device for testing . When using PhoneGap Build to simplify cross-platform compilation , I just push the code to git , then recompile via PhoneGap Build . <p> I 'm not a fan of Xcodes HTML/JS editing , and havent found an HTML/JS plugin for Eclipse that I @ @ @ @ @ @ @ @ @ @ Visual Studio . <h> Lightweight Editors <p> I 'm a bigger fan of larger IDEs than lightweight editors , but- Adobe Edge Code ( also known as Brackets ) is a great lightweight editor for quick edits. - Edge Code/Brackets is an open source HTML/JS editor that supports live editing in the browser and inline editors for CSS styles , without leaving your HTML files . If you tried Edge Code Preview 1 , but werent sold on it , you should try Edge Code Preview 2 . The team has come a long way very quickly . Its fast , easy to use , and there is a- plugin to tie it into PhoneGap Build . I sometimes use this for quick edits . <p> There are tons of other lightweight editors out there , and everyone has their favorite . As long as you 're happy with the tool , and it can edit text ( HTML , CSS , JS ) files , you can use it to build PhoneGap applications . <h> Designer-Friendly Editors <p> I 'm not necessarily the primary target for Dreamweaver , but it has some @ @ @ @ @ @ @ @ @ @ plus a WYSIWYG editor for HTML experiences . It also features PhoneGap Build integration directly inside the coding environment . If you 're used to Dreamweaver for creating web experiences , you can continue to use it and target mobile apps as well . <p> Adobe Dreamweaver <h> Debugging Environments <p> Yes , that is plural Debugging Environments . Due to the cross-platform nature and PhoneGaps leveraging of native web views for each platform , debugging PhoneGap applications can sometimes be tricky . Here are some tips that will make this significantly easier . <h> The PhoneGap Emulator <p> The PhoneGap Emulator is my primary development/debugging tool for all PhoneGap apps . It is a browser-based emulator leveraging the Google Chrome browser and the Ripple Emulation Environment . The PhoneGap Emulator runs inside of Google Chrome , and provides emulation of PhoneGaps core APIs . Since it is built on top of Chrome , it enables you to leverage Chromes Developer Tools , which in my opinion are second to none for web/application development . This is a highly-productive developer environment . <p> PhoneGap Emulator in Google Chrome <p> here 's why @ @ @ @ @ @ @ @ @ @ , - this combination enables you to emulate most core PhoneGap APIs without leaving the desktop environment . It enables you to test various APIs including geolocation ( with simulated locations ) , device events ( deviceready , back , etc ) , sensor events ( accelerometer , compass ) , and even let 's you test with different device aspect ratios all without having to push anything to an actual device . This saves a lot of time in development iterations . You can read about the supported Ripple emulator features here . <p> Second , Chromes Developer Tools are awesome . Here are just a few things that you can do while developing/debugging your app , live within the emulation environment : <p> Analyze all resources consumed by your app , via the resources panel . This includes all scripts , images , html files , cookies , etc it even includes insight into any local data stored via PhoneGaps local storage database ( WebSQL implementation ) . <p> View/query all local databases within your app . You can write your own queries to view/alter data in the WebSQL @ @ @ @ @ @ @ @ @ @ not immediately intuitive . <p> Debug JavaScript with the Scripts/Sources Panel . You can set breakpoints in JS execution , inspect &amp; alter values in JS objects in-memory , and view details and line numbers for any exceptions that occur . <p> Use the console to monitor console.log() statements , inspect properties of objects in memory , or execute arbitrary JavaScript whenever you want . <p> The PhoneGap Emulator enables developers to be extremely productive with development , however I can not emphasize enough that on-device testing is critical for having a successful app . On-device testing can expose performance problems or browser rendering variances that you may not notice in the emulator environment . <h> On-Device Remote Debugging <p> As I mentioned above , on-device testing is critical for successful applications . iOS and BlackBerry have an advantage over other platforms b/c the latest developer tools allow you to remotely debug content live on a device . <p> Since the release of iOS 6 , you can debug content in the iOS simulator using Safaris Developer Tools . Safaris developer tools give you many of the same debugging capabilities @ @ @ @ @ @ @ @ @ @ with Weinre <p> Not every platform supports live remote debugging , especially older versions . Weinre ( pronounced winery ) is a remote web inspector that allows you to inspect/edit DOM and CSS elements on remote devices . Basically , you include some JavaScript in your app , and it communicates back to a server that will tell you what 's happening inside of the app running on the mobile device . It wo n't give you full debugging capabilities like JS breakpoints and memory inspection , but its better than nothing . You can use Weinre by setting up your own instance , or by leveraging debug.phonegap.com . <p> Weinre for On-Device Debugging <h> When All Else Fails <p> If you 're still debugging your apps , and the solutions mentioned above do n't  work , you can always resort to plain-old " alert() " statements to pop up debug messages , or use " console.log() " statements to write to system logs . <p> On Android , all- console.log ( ' ... ' ) ; - messages will appear as printouts in the command-line tool- logcat , which is bundled with @ @ @ @ @ @ @ @ @ @ . <p> On BlackBerry , all- console.log ( ' ... ' ) ; - are printed to the BlackBerrys Event Log . The Event Log can be accessed by pressing- ALT + LGLG . <p> On iOS , all- console.log ( ' ... ' ) ; - are output to the Xcode Debug Area console . <h> Building PhoneGap Apps <p> The PhoneGap getting started guides will point you to the right direction for getting started with a particular platform . If you are just targeting iOS , you can use Xcode for building . If you are just targeting Android , you can use Eclipse , etc It is all very easy to get up and running . <p> However , this process gets much more complicated when targeting multiple platforms at once . When I have to do this , PhoneGap Build becomes really , really handy . <p> PhoneGap Build allows you to either upload your code , or point to a Git repository . PhoneGap Build will then pull your code and build for 7 different platforms , without you having to do anything special or @ @ @ @ @ @ @ @ @ @ is install the cloud-compiled binaries on your device . You can do this by copying/pasting a URL to the binaries , or by capturing a QR code that will directly link to the compiled- application- binary . <p> One other advantage of PhoneGap build is that it let 's designers/developers build mobile applications without having to install any developer tools . If you want to compile a PhoneGap app for iOS , but are on Windows just use PhoneGap build and you wo n't need Xcode or a Mac . <h> PhoneGap UI/Development Frameworks <p> Probably the most common PhoneGap question that I get asked is " what MVC/development framework should I use ? " . If you 've been waiting for me to answer this , do n't  hold your breath . It is impossible to be prescriptive and say that one solution fits all use cases for every- developer . <p> When people ask me this , I like to paraphrase- Brian Leroux from the PhoneGap team : " Use HTML , it works really well . " <p> I think people often overlook the fact that PhoneGaps UI renderer is @ @ @ @ @ @ @ @ @ @ content can be rendered as your applications user interface . This could be something incredibly simple , like text on the screen , or it could be incredibly creative or complex . The important factor is that you need to focus on a quality user experience . If you 're worried about your UX , and are worried that Apple may reject your app , then read this article where I explain Apple rejections in detail . <p> HTML/JS developers come from many different backgrounds , with varying degrees of programming expertise . Some frameworks appeal to some people , other frameworks appeal to other people . There also seem to be new UI &amp; architectural frameworks popping up every week . It would be a disservice to all people who use PhoneGap for us to proclaim that we should only use one singe framework . <p> There are lots , and lots , and lots more options out in the HTML/JS development world I 'm not even taking into account JavaScript generating tools and languages like CoffeeScript , TypeScript , or others <p> Today Raymond Camden and I hosted another open @ @ @ @ @ @ @ @ @ @ . - These sessions are an opportunity for anyone to stop in and ask us questions . <p> The Q&amp;A transcript from todays session is below . - Thanks to everyone for sticking around , and bearing through our technical difficulties The normal Q&amp;A pod was n't working for some reason , so we had to improvise. - Well make sure this is working for next time . <p> This was our third event , and we 've had a great turnout so far , so we will be holding these open sessions once a month . - If you werent able to make it this time , well be having another one soon . Just check the- Adobe TechLive- page for future events . 
@@106848851 @2248851/ <h> Tag Archives : adobe <p> Last week Adobe announced information about the companys evolution and future plans of Flex . It was also announced that Adobe Flex would be contributed to an open source software foundation . The result of which , was mass speculation , fear , uncertainty , and doubt . - Rest- assured , Flash is not dead , nor is Flex . <p> Falcon , the next-generation MXML and ActionScript compiler that is currently under development ( this will be contributed when complete in 2012 ) <p> Falcon JS , an experimental cross-compiler from MXML and ActionScript to HTML and JavaScript . <p> Flex testing tools , as used previously by Adobe , so as to ensure successful continued development of Flex with high quality <p> Is n't Adobe just abandoning Flex SDK and putting it out to Apache to die ? - <p> Absolutely not " we are incredibly proud of what we 've achieved with Flex and know that it will continue to provide significant value for many years to come . We expect active and on-going contributions from the Apache community @ @ @ @ @ @ @ @ @ @ to the projects and we are working with the Flex community to make them contributors as well . <p> Flex has been open source since the release of Flex 3 SDK . What 's so different about what you are announcing now ? <p> Since Flex 3 , customers have primarily used the Flex source code to debug underlying issues in the Flex framework , rather than to actively develop new features or fix bugs and contribute them back to the SDK . <p> With Friday 's announcement , Adobe will no longer be the owner of the ongoing roadmap . Instead , the project will be in Apache and governed according to its well-established community rules.In this model , Apache community members will provide project leadership . We expect project management to include both Adobe engineers as well as key community leaders . Together , they will jointly operate in a meritocracy to define new features and enhancements for future versions of the Flex SDK . The Apache model has proven to foster a vibrant community , drive development forward , and allow for continuous commits from active developers @ @ @ @ @ @ @ @ @ @ Flex applications continuing to run on Flash Player and Adobe AIR ? <p> Adobe will continue to support applications built with Flex , as well as all future versions of the SDK running in PC browsers with Adobe Flash Player and as mobile apps with Adobe AIR indefinitely on Apple iOS , Google Android and RIM BlackBerry Tablet OS . <h> Adobe AIR <p> We are continuing to develop Adobe AIR for both the desktop and mobile devices . Indeed , we have seen wide adoption of Adobe AIR for creating mobile applications and there have been a number of blockbuster mobile applications created using Adobe AIR . <h> Flash Player for Desktop Browsers <p> We feel that Flash continues to play a vital role of enabling features and functionality on the web that are not otherwise possible . As such , we have a long term commitment to the Flash Player on desktops , and are actively working on the next Flash Player version . <p> BlackBerry DevCon 2011 kicked off earlier this week , and surrounding it were some exciting announcements around Adobe tools and BlackBerry platforms . @ @ @ @ @ @ @ @ @ @ available on the PlayBook including Stage3D ( among many other great features ) . Also announced was AIR support for the new BlackBerry BBX operating system , as well as PhoneGap support for BBX/QNX . BBX is the new QNX based operating system for BlackBerry smartphones . <p> Adobe 's VP and General Manager of Interactive Solutions , Danny Winokur , joined RIM 's Alec Saunders , VP of Developer Relations and Ecosystems Development , on stage at BlackBerry DevCon Americas 2011 . Danny spoke about the exciting possibilities that Flash and HTML5 bring to the web and mobile app development " specifically for the BlackBerry PlayBook and BBX in the future . ( read more here , or check out the video below ) <p> The recordings of my presentations do n't  seem to be available yet on Adobe TV , but here is the content , as promised . I spoke at MAX this year on " Multi Device Best Practices using Flex &amp; AIR for Mobile " , and " Create beautiful , immersive content and applications with HTML5 and CSS3 ? , and the content from @ @ @ @ @ @ @ @ @ @ using Flex &amp; AIR for Mobile <p> In the multi-device best practices session I covered the basics for building a **30;5316;TOOLONG application that conforms to device constraints ( phone and tablet ) , using a single codebase that is able to detect device dimensions and orientation . - This was followed by online/offline detection for occasionally-connected applications , and then followed by device-specific layout using CSS media queries and MultiDPIBitmapSource images . - The presentation slides are below . <h> Create beautiful , immersive content and applications with HTML5 and CSS3 <p> In this session , I gave a " crash course " in developing rich content experiences with HTML5 and CSS3. - I started with a general overview presentation , followed by diving directly into code . - I covered &lt;video&gt; , &lt;audio&gt; , dynamic graphics with &lt;canvas&gt; , &lt;svg&gt; , HTML5 Form elements , CSS3 Web Fonts , Visual Styles ( shadows , corners ) , CSS3 Color spaces ( RGBA , HSLA ) , graidents , transforms , animations , and media queries . - In the presentation , I also discussed the necessity of client-side solution- @ @ @ @ @ @ @ @ @ @ in addition to graceful degradation and HTML5 feature detection using Modernizr. - The presentation slides are below : <p> Although there were no official announcements around Flex , Flash &amp; AIR ( other than the release of FP11 &amp; AIR3 ) , do n't  think that the platform is going away or becoming stale In fact , it is quite the opposite . The Flash Platform will continue to thrive and innovate , providing outstanding solutions that set the pace for other technologies to follow . In case you missed the session , here is the " Flash Platform Roadmap " , provided by Scott Castle , Adam Lehman , and Raghu Thricovil , Product Managers for Flash Platform tooling : <p> If that wasnt enough , did you see the new " Monocle " tool , shown by Deepa Subramaniam ? Monocle is the new realtime profiling tool for Flash-based content which will provide additional insight into what 's happening at runtime , and how you can optimize your applications . <p> Did you also see the latest demos showing the Epic Games &amp; the Unreal engine running INSIDE of the Flash Player ? 
@@106848852 @2248852/ <h> Porting A PhoneGap App To Firefox OS <p> About a year ago I released the Fresh Food Finder , a multi-platform mobile application built with PhoneGap . The Fresh Food Finder provides an easy way to search for farmers markets near your current location , based on the farmers markets listings from the USDA . This app has seen a lot of popularity lately , so I 'm working on a new version for all platforms with a better data feed , better UI , and overall better UX unfortunately , that version is n't ready yet . However , I have been able to bring it to an additional platform this week : Firefox OS ! <p> Fresh Food Finder on iOS , Firefox OS , &amp; Android <p> PhoneGap support is coming for Firefox OS , and in preparation I wanted to become familiar with the Firefox OS development environment and platform ecosystem . So I ported the Fresh Food Finder , minus the specific PhoneGap API calls . The best part ( and this really shows the power of web-standards based development ) is that I was @ @ @ @ @ @ @ @ @ @ it into a Firefox OS app AND submit it to the Firefox Marketplace in under 24 hours ! If you 're interested , you can check out progress on Firefox OS support in the Cordova project , and it will be available on PhoneGap.com once its actually released . <p> Basically , I commented out the PhoneGap-specific API calls , added a few minor bug fixes , and added a few Firefox-OS specific layout/styling changes ( just a few minor things so that my app looked right on the device ) . Then you put in a mainfest.webapp configuration file , package it up , then submit it to the app store . Check it out in the video below to see it in action , running on a Firefox OS device <p> The phone I am using is a Geeksphone Firefox OS developer device . Its not a production/consumer model , so there were a few hiccups using it , but overall it was a good experience . Also , many thanks to Jason Weathersby from Mozilla for helping me get the latest device image running on my phone . @ @ @ @ @ @ @ @ @ @ OS development here : 
@@106848853 @2248853/ <h> Tag Archives : data <p> I recently attended the IBM InterConnect conference , and it was great to be there presenting IBM MobileFirst and engaging with our clients and partners . - While there , I also took part in the " Ignite " presentation series . If you are n't  familar with Ignite , each presenter has 20 slides , with the slides auto-advancing every 15 seconds . That gives you 5 minutes to make your point , and in the session you get to see 5 or 6 presentations on different topics within an hour . - I really liked this format , so I re-recorded- it to share here <p> Here are all my sources for the charts/data. - I chose to use publicly accessible- data for everything . - If you have n't been paying attention to mobile usage numbers and adoption , its quite staggering , and definitely worth paying attention to . <p> Recently , I 've been asked more than once which is better : AMF or JSON for AIR mobile applications . This post is to highlight some performance comparisons , and @ @ @ @ @ @ @ @ @ @ , it is important to know what both AMF and JSON are . <h> AMF <p> Action Message Format ( AMF ) is a compact binary format that is used to serialize ActionScript object graphs . Once serialized an AMF encoded object graph may be used to persist and retrieve the public state of an application across sessions or allow two endpoints to communicate through the exchange of strongly typed data . <h> JSON <p> JSON ( JavaScript Object Notation ) is a lightweight data-interchange format . It is easy for humans to read and write . It is easy for machines to parse and generate . It is based on a subset of the JavaScript Programming Language , Standard ECMA-262 3rd Edition December 1999 . JSON is a text format that is completely language independent but uses conventions that are familiar to programmers of the C-family of languages , including C , C++ , C# , Java , JavaScript , Perl , Python , and many others . <p> I put together a very basic test case where a mobile application makes requests of simple data objects from a @ @ @ @ @ @ @ @ @ @ is made for 1 , 10 , 100 , 1000 , and 10000 value objects , in both AMF and JSON formats . The total round trip time from request to deserialization is measured and compared for each case , for a total of 5 iterations through each cycle . - My findings are that AMF and JSON have- comparable- performance in smaller record sets . - However , AMF seems to have better performance as data sets grow . - In my test cases , the 1000+ record results were consistently faster using AMF. - However , in smaller data sets , JSON was often faster ( however not consistently , or by much of a margin ) . I tested these times on both an iPhone 4 and Motorolla Atrix , both running on the carrier networks ( not over wifi ) . <p> Below is a video of the serialization testing application at work . <p> Here are a few screenshots of the application . <h> The Tests <p> For these tests I created two basic CFCs ( ColdFusion Components ) . One is a simple data @ @ @ @ @ @ @ @ @ @ expose a remote service that returns the value objects to the client . I chose a ColdFusion CFC for this case b/c it can easily be serialized as AMF or JSON just by changing the endpoint used to consume the service . <p> Obviously , this is a fictional data object with randomly generated values . However , it still represents a reasonable service payload for data serialization . By accessing the data via the ColdFusion Flex/Remoting gateway , you access the remote services via AMF3 . <p> In the mobile client application , I have a **27;5348;TOOLONG class that handles all of the test logic and communications back and forth with the server . The time for each test is measured from immediately before the the request is made to the server , until after the data has been deserialized to an ArrayCollection . You can view the **27;5377;TOOLONG class below : <p> The answer to the question of " should I use AMF or JSON " is subjective What kind of data are you returning , and how much data is it ? Do you already have AMF @ @ @ @ @ @ @ @ @ @ built ? - Are the services consumed by multiple endpoints , with multiple technologies ? - Do you rely upon strongly typed objects in you development and maintenance processes ? - Both AMF and JSON are viable solutions for mobile applications . 
@@106848854 @2248854/ <h> Tag Archives : nook <p> Here are some interesting and quite surprising statistics for the US Census Browser HTML/PhoneGap showcase application that I released in December , which I wanted to share . The app is a browser for US Census data , full detail available here : http : **35;5406;TOOLONG . The Census Browser application was intended as a showcase app for enterprise-class data visualization in HTML-based applications , and all source code is freely available to the public . <p> What is really surprising is the " health " of my app within the given ecosystems . I offered the app as a free download in each market . The app is focused on Census data , so there is obviously not a ton of consumer demand , however the data is still interesting to play around with . I would not expect the same results for all types of apps in all markets . <p> BlackBerry Playbook downloads were in 3rd , just behind iOS ( BB is 11% of all downloads ) <p> Android traffic was minimal ( 2% of all downloads ) <p> @ @ @ @ @ @ @ @ @ @ iOS market is strongest , followed by Android , and that BB is dead . These numbers show a conflicting reality . Barnes &amp; Noble was the strongest , with iOS in second place , and BlackBerry just behind iOS. 
@@106848856 @2248856/ <h> Category Archives : Creative Cloud <p> here 's a quick video introduction I put together for Adobe Edge Inspect , a *free* tool that enables synchronized browsing and debugging of HTML content between desktop and mobile devices . Its definitely worth checking out , if you are n't  using it already . <p> Adobe Edge Inspect enables synchronized browsing across desktop and mobile devices . By pairing your devices with Edge- Inspect- running on your desktop , any content you view in the desktop browser will be synched to all paired mobile devices . <p> Adobe Edge Inspect also makes it easy for you to track and document visual issues on the remote devices . With just one button click , you can capture screenshots from all connected devices , complete with information about each device ( OS , device name , screen resolution , etc ) . <p> Screen Captures <p> Did I mention Adobe Edge Inspect is also free ? Its part of the free tier of Adobe Creative Cloud . <p> In this post I 'm changing things up a bit Rather than focusing just on developer-centric @ @ @ @ @ @ @ @ @ @ workflow using Adobe Ideas . - Adobe Ideas is an awesome sketching app for the iPad . You can sketch whatever you want , whenever you want . It is all saved as vector content that you can then pull into Adobe Illustrator to integrate into your desktop workflow using Adobe Creative Cloud. - Take a look at the video below to see it in action : <p> Once you save your sketch , it automatically gets synched with your Creative Cloud account . You can then pull the composition directly into Illustrator and use it within other creative suite tools . <p> here 's the output of the exact composition shown in that video : <p> You can export as raster ( png , jpg , etc ) or vector ( ai , svg , etc ) formats , so it can be a great tool for enabling you to hand-sketch high- quality- graphical assets for your creative workflow . <p> Its Valentines day , and here at Adobe were showing our love for creating great HTML experiences Just check out what we released today ! - All of these @ @ @ @ @ @ @ @ @ @ Adobe Edge Reflow ( Preview ) <p> Adobe Edge Reflow is a new responsive design tool that helps designers create and communicate responsive intent in their designs to both customers and developers . - Adobe Edge Reflow enables designers to create responsive HTML experiences in a visual workspace , and leverages web standards technologies to create designs that are- accurate- to the capabilities of the modern web . <h> Stay Informed ! <p> As I mentioned above , all of these are available immediately via Adobe Creative Cloud subscriptions . Even better , you can get Adobe Edge Animate , Adobe Edge Reflow , and Adobe Edge Code as part of the free Creative Cloud tier . Go download them now ! 
@@106848859 @2248859/ <h> Linked Smart Objects in Adobe Photoshop CC <p> Another great feature in the latest release of Adobe Photoshop CC is Linked Smart Objects . Linked Smart Objects enable you to place another file inside of a composition as a smart object . Any time that the linked file is saved , those changes are automatically propagated to the linked documents that you have open . ( If they 're not open , you will be prompted the next time you open the file . ) <p> For example , you can place a . ai Illustrator file inside of a PSD as a Linked Smart Object , and when you update the content in Illustrator , the content in Photoshop is automatically updated . Check out the video below for more detail . <p> Be sure to check out the Creative Cloud learning resources to learn more about Linked Smart Objects . <p> This is a FREE update for- Creative Cloud- members . - If you 're not already a Creative Cloud member , - join- today to get the most out of all of the tools that Adobe @ @ @ @ @ @ @ @ @ @ NOT a great feature in Photoshop and here 's why . When you get a PSD from an outside source with everything linked and the user who created it does n't  know they now have to package the file to include all of those links along with the PSD This is a big problem . Especially since nobody is used to having to do this with PSD files . So Adobes killer " time saver " just became a huge bottleneck for a production workflow . 123435 @qwx983435 <p> This is a new workflow option that can save significant amounts of time . Calling linked smart objects a " huge bottleneck " is misleading and inaccurate . It does not slow down production workflows in any way . The original method of placing smart objects embedded still exists , and as you mentioned , you have the ability to package the PSD for sharing it ( with the linked objects ) if you choose to use the new linked smart objects capabilities . Adobe provides training and documentation to help users understand these new features ( see LONG ... ) , @ @ @ @ @ @ @ @ @ @ to help users understand how to use them properly . If your workflow does not lend itself to using linked smart objects , you have the option to not use them . With this feature Adobe is providing more options that can enable new use cases and workflow processes , for a variety of types users . <p> kerpow69 <p> Its not misleading . It IS a huge bottleneck when the users creating PSDs with links just send the PSD and not the links . That stops production in its tracks until we can get the files ( because my designers need to work on them , not just output ) . This is happening . A lot . <p> I get where the benefits are but for now this is a major pain point for us . Its great that Adobe and people like you have been trying to spread the word but most users do n't  attend user groups and rarely even look in the help files . Sadly , they just see a new feature and use it without thinking too far ahead . <p> gantic <p> @ @ @ @ @ @ @ @ @ @ then <p> This feature is awesome ! <p> kerpow69 <p> To an extent yes , I know I would love it if all my creatives took the time to learn every new feature and know how to incorporate them into a workflow but , alas , that is n't the world we live in . Time is the rare ingredient here . Oh well , this kind of stuff keeps me employed . 
@@106848861 @2248861/ <h> Category Archives : Creative Cloud <p> The latest release of Photoshop has some amazing new features , one of which is 3D printer support . The new 3D printer support makes printing your 3D models easier , regardless of whether you modeled it within Photoshop or some other 3D modeling tool . - Photoshop will even inspect your 3D models for water tightness and generate support scaffolding to ensure a high quality 3D print . <p> Photoshop now supports local 3D printers from Makerbot- and- 3D Systems , but if you do n't  have a 3D printer , do n't  worry , you can still print 3D objects ! Through a partnership between Adobe and Shapeways , you can now package 3D prints and send them to Shapeways 3D printing service directly from Photoshop . <p> I started out by creating a few simple models and downloading existing models from the web . Check out the video below to learn more and see 3D features within Photoshop in action . <p> All of my 3D printing so far has been through the Shapeways service . This service will @ @ @ @ @ @ @ @ @ @ you immediate feedback whether or not you need to make any changes . Be sure to pay attention to the details of Shapeways materials , since they have different characteristics , minimum thicknesses , and associated cost . Once your object is printed , it will be mailed right to your doorstep . <p> My first example is a 3D printed name plate , modeled entirely within Photoshop. - I took a text layer , extruded it into a 3D object , then added a cube ( stretched to the size of the text ) as a base . Just request a 3D print , and out comes a nice 3D model . This one is the " Coral Red Strong &amp; Flexible Polished " material from Shapeways . Check out the video above for details on how I created this . <p> " Adobe " 3D Printed Name Plate <p> My second example is a dragon , which I downloaded from a free 3D models site . This model was not created in Photoshop . I believe this model was originally intended for video games or renderings , but @ @ @ @ @ @ @ @ @ @ can read many common 3D model formats ( . obj , .3ds , . dae , etc ) , and makes the printing process simple , regardless of where the model was created . <p> I had to go through a few iterations to find a material and size that was actually printable because the model is very intricate and delicate , but I was finally able print it using the " White Strong &amp; Flexible " material . <p> 3D Printed Dragon <p> If you 're wondering " did those cost a fortune " , the answer is NO ! - The cost depends on type and amount of material you 've selected . The dragon was $32.65 , and the " Adobe " letters were $24.90 USD , including shipping . <p> Ready to create your own 3D prints yet ? - Check out the videos below to learn more about 3D printing with Adobe Photoshop CC. <h> 3D Printing in Photoshop Series <p> If you 're already a member of Creative Cloud , then you have everything you need to create your own 3D prints with Photoshop . Just download @ @ @ @ @ @ @ @ @ @ If you 're not already a member of Creative Cloud , then become a member today ! <p> Interested in focusing on aerial photography instead of videography ? Stay tuned for the March Adobe Inspire issue next month , which will feature a complimentary article focusing on still images captured with the same helicopter configuration . Subscribe today to be notified automatically when the new version is available . <p> Be warned flying helicopters with cameras attached is highly addictive . You may easily become obsessed with the endless possibilities , as I have . <p> Here are a few videos Ive captured with this setup , and processed with Creative Cloud . <p> The Creative Cloud Packager is a tool for CC Enterprise and CC Team- customers that enables them to easily package Creative Cloud products and updates for deployment within their organizations . - It let 's you select specific Creative Cloud products and/or updates and package them into . pkg or . msi installers ( optionally with a serial number for Enterprise customers ) . - These packages can then be deployed on their own or integrated with third-party @ @ @ @ @ @ @ @ @ @ Creative Cloud Packager even let 's you control Creative Cloud update behaviors and more . <p> With the recent releases of Creative Cloud Packager , you can now edit existing deployment packages , create deployment packages from local media ( DVDs ) , and even create deployment packages for older ( CS6 ) creative applications , if you have the proper license . <p> In addition to my addiction to aerial photography , I 'm also fascinated by time-lapse photography . With time lapse photography , you set up your camera to take pictures on an interval . This could be every few seconds , every few minutes , every few hours , or heck , once a day . Its really up to you how you want to set up your shots and what you want to shoot . In any case , you can end up with a lot images each by itself could be great , but it only tells a limited story . - However , you can put all those images together in a sequence to create some truly amazing visuals . Subtle motion becomes pronounced , @ @ @ @ @ @ @ @ @ @ Often , this ends up with an amazing visual story that would be hard to otherwise capture . <p> All that you need start diving into time-lapse photography is a camera that is capable of capturing images on an interval normally there is some kind of time lapse mode that let 's you set up your image frequency and duration . Then , once you 've got your images , you can process them with Creative Cloud tools to bring out their full potential . <p> Here are two time-lapse sequences I created this weekone a snow storm , one a sunset . <p> Neither sequence required a lot of specialized or expensive equipment . I used a GoPro Hero 3 Black camera , set it on my window sill , and let it do its thing . ( I do want to upgrade to better gear , but this still works fantastically , and I love the GoPro . ) <p> GoPro Hero 3 Black Edition <p> The sunset was a ten second interval captured over about 2 hours and played back in 30 seconds . The snow storm was a @ @ @ @ @ @ @ @ @ @ back in 40 seconds . <p> So , you 've captured the images , what next ? - <p> You can check out the video below , or read on for further explanation how I processed and assembled the images into a video sequence , complete with links to Adobe documentation and tutorials . <p> Before putting everything together as a sequence , I wanted to enhance the photos to bring out as much detail as possible . here 's where Adobe Lightroom comes into the picture . I used Lightroom to import all of my photos , add them to a collection , and then perform bulk/batch processing to enhance all of the images . <p> Editing Photos with Lightroom <p> First , select an image to use as your baseline for adjustments . I would n't start with your darkest image , and I would n't start with your lightest either . I normally start somewhere in the middle . Select the image , and then switch over to the " Develop " module . I use the basic panel to make adjustments to this image . For the GoPro , I @ @ @ @ @ @ @ @ @ @ highlights to pull out details out . If I 'm shooting a landscape , I also like to bring up the clarity and maybe even the vibrance and saturation just do n't  over do it . You could also use one of Lightrooms presets if you want ; its really up to you . Just be extra careful that it is not too dark or too light b/c were going to apply these settings to all images in the sequence . <p> Lightroom Basic Panel <p> If you want to adjust hue , saturation or luminance of specific colors , you can do that within the HSL/Color/B&amp;W panel . Using this you can make specific colors more or less intense . - I normally try to tone down the yellows in my GoPro images after I 've increased overall saturation . <p> Since I used the GoPro , there is a lot of fisheye distortion from the lens the GoPro has a 2.77mm lens whichgives an ultra-wide 170 degree field of view . This makes for some awesome wide angle shots , but sometimes you do n't  want that extreme distortion . @ @ @ @ @ @ @ @ @ @ , I opened up the Lens Correction panel . As soon as you check the " Enable Profile Corrections " checkbox , Lightroom should automatically select the GoPro Hero 3 Black Edition lens profile based upon metadata within the image . I did n't  want to fully flaten the image , just reduce the wide angle , so I turned down the distortion correction using the " Distortion " slider . <p> Lightroom Lens Correction <p> Once you have your baseline image the way you want it , you need to apply these settings to all of your images in the sequence . Just select them all , and then either click on the " Sync " button in the bottom right of the Develop module , or use the Settings -&gt; Synch Settings menu . This will apply you changes on this image to all of the images that were selected . This will happen automatically if you are using auto-sync. - You can learn more about synchronizing metadata between photos in the Lightroom documentation . <p> Next , be sure to view several images in your collection , @ @ @ @ @ @ @ @ @ @ all look decent . If you need to make any changes because they are too light , or too dark , or do n't  have the right contrast , then now is your time to fix it . Once you 're happy with the images in your collection you next need to export them . I exported as JPG with 100% quality at full resolution with sequential names . <p> Now we 've got a lot of processed images . What 's next ? We need to make a video ! <p> If you 're wondering how I got the motion in the time lapse sequence , no I did n't  have the camera moving . There are devices which make this possible , but I just used a video editing trick . The images are 12 MP , or 4000 by 3000 pixels . A " standard " HD video sequence is 1920 by 1080 pixels . The image below reflects this scale the red area represents the 4000 by 3000 still image , and the yellow represents the 1920 by 1080 video . <p> Video &amp; Image Size Comparison <p> You 'll notice @ @ @ @ @ @ @ @ @ @ and pan around the image . I zoom into the image so that it fills the entire horizontal space within the video sequence you can zoom in more if you want . This leaves a fair amount of vertical content outside the clipping rectangle of the video . You can use this to your advantage by panning vertically within this area . - I just made the pan very slow and deliberate so it appears that there is constant motion of the camera throughout the entire video . <p> The final result is that the content in the video ( yellow area ) appears to move because the actual image sequence is moving relative to the video viewport . <p> Photosynth is an impressive service from Microsoft . - It enables you to upload photos and turn them into interactive 360 panoramas , photo walls , spins , or photo walks . The Photosynth team recently announced a new version of Photosynth , and its a really cool web experience . It leverages- WebGL- to visualize the content , and runs great on both desktop and mobile devices ( as @ @ @ @ @ @ @ @ @ @ who know me well or regularly read the blog probably already know I have an obsession with aerial photography using remote controlled multirotor helicopters . Once I discovered Photosynth , my first thought was " Wow , these Photo Walks will be incredible to visualize flights " . - I capture most of my flights in time-lapse photography mode with a GoPro camera attached to a DJI Phantom copter . The time-lapse images are perfect for Photosynth I normally capture on a two second interval , though the Photosynth team suggested trying an even shorter interval for better results . <p> To generate the best Photosynths , you need to start with the best photos . This is where Lightroom comes into the picture . Lightroom is an incredible tool for editing photos and bringing out their details . You can enhance exposure , colors , clarity , saturation , reduce noise , and more . Even better , it excels at bulk image editing . Thus its perfect for processing your photos for preparation to create a Photosynth . <p> Check out the video below to get an overview @ @ @ @ @ @ @ @ @ @ <p> Now , you 're ready to learn more about both Lightroom and Photosynth , right ? <p> Below are Photosynths from a few of my flights . - If your browser supports WebGL , you 'll be able to see the fully interactive experience you 'll be able to scrub through the photos , zoom in , and pan the images at full resolution . Its best viewed in full-screen mode . 
@@106848862 @2248862/ <h> The New Rigid Mask Tracker in After Effects CC <p> I like to think of After Effects as an animated version of Photoshop many of the same general techniques apply : selection , masks , blend modes etc The primary difference being that the content changes over time when working with video . In general , masks work exactly the same as they do in Photoshop you use the mask to determine which part of the image/video should be a part of the composition , and which parts should be hidden from the final output . Using masks you can selectively determine which parts of the video should get special effects , or you can use them to composite clips from multiple videos together . You can animate these masks on the timeline , and that mask will be animated in the video . <p> Note : Not to be confused with Keying or Rotoscoping , which I 'll be covering in subsequent blog posts in the very near future. - <p> In the latest video tools release on Adobe Creative Cloud , more than 150 new features were @ @ @ @ @ @ @ @ @ @ Mask Tracker in After Effects CC . The Rigid Mask Tracker makes it incredibly easy to create masks that automatically track motion in your video compositions . Using the Rigid Mask Tracker boils down to just a few simple steps : <p> Draw your mask on the target video <p> Analyze the video to track motion <p> Done ! The mask will automatically track the motion in the video , based on the content underneath the mask <p> You do n't  have to use the Rigid Mask Tracker , you can still animate things manually , but this new feature makes applying motion to masks incredibly easy . <p> Let 's take look at a simple use case I have a color video , and I want to make *most* of the video black and white , but leave color in certain places to add drama and focused emphasis on certain parts of the video . Take a look at the screen grabs below to get an idea what I mean : <p> Video Effects Applied Using the Rigid Mask Tracker <p> Pay attention to the fire truck in the screen @ @ @ @ @ @ @ @ @ @ made the fire truck stand out from the rest of the video using a mask that was drawn over the fire truck , and employed the Rigid Mask Tracker to automatically track motion of the fire truck in the video . The top ( masked ) video layer has increased saturation to make the reds brighter , and the bottom video layer has decreased saturation to remove colors . <p> Now , take a look at the video below to see how this was done and see the final output ! It really was as simple as the steps I mentioned above . Just draw the mask , analyze/track motion , apply effects , and you 've got it ! <p> Now , go get out there and create great things ! If you are n't  already a member of Creative Cloud , lean more and join now at creative.adobe.com. 
@@106848863 @2248863/ <h> Category Archives : ActionScript <p> Have you noticed when using twitter , google plus , or certain areas of facebook that when you scroll the page , it automatically loads more data ? - You do n't  have to continually hit " next " to go through page after page of data . Instead , the content just " appears " as you need it . In this post we will explore a technique for making Flex list components behave in this exact way . As you scroll through the list , it continually requests more data from the server . Take a look at the video preview below , and afterwards well explore the code . <p> The basic workflow is that you need to detect when you 've scrolled to the bottom of the list , then load additional data to be displayed further in that list . Since you know how many records are currently in the list , you always know which " page " you are viewing . When you scroll down again , just request the next set of results that are subsequent @ @ @ @ @ @ @ @ @ @ you request data , append the list items to the data provider of the list . <p> First things first , you need to detect when you 've scrolled to the bottom of the list . here 's a great example showing how to detect when you have scrolled to the bottom of the list . You can just add an event listener to the lists scroller viewport . Once you have a vertical scroll event where the new value is equal to the viewport max height minus the item renderer height , then you have scrolled to the end . At this point , request more data from the server . <p> One other trick that I am using here is that I am using conditional item renderers based upon the type of object being displayed . I have a dummy " LoadingVO " value object that is appended to the end of the list data provider . The item renderer function for the list will return a LoadingItemRenderer instance if the data passed to it is a LoadingVO . <p> You may have noticed in the fetchNextPage() function that the @ @ @ @ @ @ @ @ @ @ class next . The InfiniteListModel class is simply an ArrayCollection which gets populated by the getNextPage() function . Inside of the getNextPage() function , it calls a remote service which returns data to the client , based on the current " page " . In the result handler , you can see that I disable binding events using disableAutoUpdate() , remove the dummy LoadingVO , append the service results to the collection , add a new LoadingVO , and then re-enable binding events using enableAutoUpdate() . Also , notice that I have a boolean loading value that is true while requesting data from the server . This boolean flag is used to prevent multiple service calls for the same data . <p> Now , let 's take a look at the root view that puts everything together . There is an InfiniteScrollList whose dataProvider is an InfiniteListModel instance . The InfiniteListModel also references a RemoteObject instance , which loads data from a remote server . <p> Here 's another post that I originally wrote way back in 2006 , when object oriented development was a newer concept to client-side web applications @ @ @ @ @ @ @ @ @ @ Flex , AIR &amp; ActionScript for mobile/web/desktop , so I decided to resurrect it from the old blog archive as well . Enjoy <p> Understanding of OOP ( Object Oriented Programming ) is fundamental in being successful with the Flex framework and being able to get the most out of it . Developers who do not possess a computer science-related background may not be aware of the fundamental concepts that comprise OOP and how to apply them correctly , so here is a quick piece to help you out . <p> First , object oriented programming is a programming paradigm where your code is organized into logical objects , and each object has properties and methods . Each object contains similar and/or related functionality , and is organized into classes that logically represent and logically organize its functionality . <p> For Example : Let 's say that we have a class " Automobile " . This class would contain the information and functions necessary for our application to use the Automobile class . We could have a numeric property for the number of wheels , the speed , and the direction @ @ @ @ @ @ @ @ @ @ also contain methods that control the actions of the Automobile object : accelerate , decelerate(break) , turn , start engine , stop engine , etc Our class would look something like this <p> Ok , now that we have a brief explanation of what object oriented programming is , we can get into some more aspects of OOP : inheritance and interfaces . <p> Inheritance is a way to form new objects based on existing objects . When a class inherits from a base class , the new class extends the functionality of the base class , and can utilize public and protected properties and methods from that base class . Inheritance can be used to create different objects that utilize functions within the base class , so that the child classes all utilize the same code base . Inheritance can be used to extend the functionality of existing objects , and inheritance can also be used to override and/ or change functionality from the base class . <p> In Actionscript 3 , you can access the parent class of your class by using the " super " keyword . @ @ @ @ @ @ @ @ @ @ would use " super() " , where accessing a method of the parent class would use something like : " super.myMethodName() " . If a property of the parent class is created with public or protected access , you can access that property in the child class directly by the property name ( you would use this.propertyName , not super.propertyName ) . <p> Now , Let 's take our Automobile example and apply object-oriented inheritance . We already have a base Automobile class that covers the basic functionality . We can create child classes that extend the functionality of the Automobile . <p> public class SportsCar extends Automobile public function SportsCar() super() ; override public function accelerate():void /* we can override the accelerate function so that it accelerates faster than the base Automobile */ <p> These classes extend the base functionality of the Automobile class , and therefore are instances of the Automobile class . If we have a function outside of the Automobile class , which takes an automobile as the parameter , both a SportsCar and Truck will work since they are both Automobiles . We could have a @ @ @ @ @ @ @ @ @ @ a Truck Instance , and a SportsCar instance , both will work , and each will use the functionality of their specific class instead of the base Automobile class . <p> I 'll get into some more fine-grain details about inheritance later in this post now , let 's move on to interfaces <p> Interfaces are slightly different than inheritance . An interface is a set of " rules " which an object must adhere to . The " rules " are actually method signatures that your class must implement . When we define an interface , we define method signatures that are required for classes that implement that interface . There is no actual code in an interface ; it simply defines methods that must exist within your class . Your class that implements the interface must implement the code for the actual function . If you have multiple classes that implement an interface , those classes must have the same functions ( only the ones required by the interface ) , but that is where the similarities of the two classes may stop . They could have completely different logic @ @ @ @ @ @ @ @ @ @ differ . Two objects that inherit from the same base class have a lot in common ( properties and methods ) : two objects that implement the same interface only have those interface method signatures in common . <p> Let 's now make an Automobile Interface that defines the functions required to create an IAutomobile object ( note the " I " stands for " interface " ) : <p> We can use the IAutomobile interface to create objects ( classes ) that behave as Automobile objects . These classes do not necessarily inherit from each other and do not necessarily share any common properties . <p> The previous two components both implement the IAutomobile interface , but have nothing else in common . One is simply a class that implements the interface , the other is a mxml component that implements the interface . The mxml component example extends the mx:Canvas component ( the same thing could be done by creating an AS class that extends mx.containers.Canvas ) . Now , let 's look at a function similar to the " race " function from earlier <p> This example will work @ @ @ @ @ @ @ @ @ @ implement the IAutomobile interface . They do not rely upon functions in the class hierarchy , just those that were implemented for this interface . You can also use multiple interfaces on classes that you create . Implementing multiple interfaces basically means that you are adding more required method signatures to your class , and you will have to implement these methods to satisfy each interface . <p> On the other hand , you can not inherit from multiple classes . Some programming languages allow for multiple inheritance ActionScript 3 does not support multiple inheritance ( so i 'll stop there ) . <p> OK enough of this rambling What does this have to do with Flex ? Inheritance and interfaces are used extensively in AS3 to create the flex framework . All flex framework components that are rendered to the screen extend from the UIComponent class . AbstractService , DataSerice or EventDispatcher object implements the IEventDispatcher Interface . You may be using these concepts every day , but werent aware of them . Inheritance seems easier to take advanatage of at first Let 's say that you want to create several @ @ @ @ @ @ @ @ @ @ variables . It is easy to see that you can create a base class that encapsulates all of the common functionality . You can then create a sub-classes that implement the differing functionality for each class . <p> When putting these concepts into real-world Flex applications you 'll need to get familiar with the following keywords : <p> extends This is used when defining a child class from a parent class . <p> public class MyImage extends Image <p> implements This is used when implementing an interface . <p> public class MyClass implements MyInterface <p> final Classes and methods implented with " final " can not be overridden . <p> final function myFunction() : void <p> static The static keyword is used when creating variables or functions in a class that are specific to the class , not an instance . Static properties and methods do not require variable instantiation to be executed . <p> public static function myStaticFunction() : void //to use it call it directly from **31;5443;TOOLONG <p> internal This is used when creating a method or property that can be accessed by any object within the same package @ @ @ @ @ @ @ @ @ @ <p> override This is used when creating a function that overrides another function from a parent class . <p> override public function myFunction() : void <p> private This is used when creating methods or properties that are only available to the class where it is defined . A private variable can not be accessed by outside classes or from descendant classes . <p> private var myPrivateValue : String ; <p> protected This is used when creating methods or properties that are only available to the class where it is defined and descendant classes . A protected variable can not be accessed by outside classes . <p> protected var myProtectedValue : String ; <p> public This is used when creating properties and methods that are available to any class . <p> here 's a quick tip for detecting device form factor ( tablet vs phone ) within your Flex mobile applications . First , get the stage dimensions for the screen size in pixels , then divide that by the applicationDPI ( screen pixel density ) . - This will give you the approximate size in inches of the devices screen . @ @ @ @ @ @ @ @ @ @ are rounded to 160 , 240 , or 320 , depending on the device . - - In my code , I make the assumption that if the landscape width is greater than or equal to 5 inches , then its a tablet . - I used view states , but you can also layout components manually . <p> here 's a post that I originally wrote way back in 2006 , when Flex 2 was all the rage . - No , seriously , Flex 2 was awesome it is the base of todays Flex framework , helped to revolutionize applications on the web , and heralded the " RIA " frenzy . - The best part is that this post is still very relevant with Flex , AIR &amp; ActionScript for mobile/web/desktop , so I decided to resurrect it from the old blog archive . - Enjoy <p> Ive been asked several times , why would you use get/set functions instead of public variables in your flex components and classes ? Well , there are some great things you can do with getters and setters that you cant do @ @ @ @ @ @ @ @ @ @ are cases where public variables may be an easy choice . When using these functions and/or public variables , the code for the caller will be the same : <p> mycomponent.myValue = 1 ; <p> First , let 's look at public variables <p> Bindable public var myValue : Number <p> Public variables are useful when there are no addional actions that need to take place when the value has been changed . If you change the value of " myValue " , the bindings will update and everything will be handled accordingly . The value will change , and anything bound to that value will change . In this case , there is no need to use getter/setter methods , keeping code simple and easy to implement . <p> First I 'll explain the- Bindable ( event= " myValueUpdated " ) - statement : This indicates that the data binding to the getters value should be updated when the event of type " myValueUpdated " is dispatched . <p> You 'll notice that when the value is set , this event is dispatched , which would notify and components that are bound @ @ @ @ @ @ @ @ @ @ for all getters and setters , however this approach can allow you to invoke binding events on the " getter " even if you do n't  access the " setter " method . <p> Now , the rest The code that I showed above is consumed in exactly the same way as a public property , but requires more code . The benefits of getter and setter functions are that they enable sequential code execution when the value is changed , and also enable inheritance in getter/setter methods . <p> This means that you can create your components so that specific functions are executed any time that the value is accessed using get and/or set functions . <p> In this example , every time the value is set , the numSets Number is incremented , and the myFunction() function is executed . Likewise , every time the value is accessed using the " get " method , the numGets Number is incremented , and the myOtherFunction() function is executed . There is no limit to what kind of code you can execute here . You can have it dispatch custom @ @ @ @ @ @ @ @ @ @ This turns out to be very handy when creating custom Flex components . <p> As I mentioned earlier , getter/setter accessors also enable inheritance on " properties " of an object . This means that you can change the behavior of a getter/setter in descendant classes , while usage remains the same . A great example of this are the " get data " and " set data " accessor methods used throughout the Flex framework ( and part of the IDataRenderer interface ) . You can override " get data " or " set data " methods to modify behaviors and/or return values , without changing how those methods are used . 
@@106848865 @2248865/ <p> I recently gave a presentation at IBM Insight on Cognitive Computing in mobile apps . - I showed two apps : one that uses Watson natural language processing to perform search queries , and another that uses Watson translation and speech to text services to take text in one language , translate it to another language , then even- have the app play back the spoken audio in the translated language . - Its this second app that I want to highlight today . <p> In fact , it gets much cooler than that . - I had an idea : " What if we hook up an OCR ( optical character recognition ) engine to the translation services ? " That way , you can take a picture of something , extract the text , and translate it . - It turns out , its not that hard , and I was able to put together this sample app in just under two days . - Check out the video below to see it in action . <p> To be clear , I ended up using a @ @ @ @ @ @ @ @ @ @ . This is not based on any of the work IBM research is doing with OCR or natural scene OCR , and should not be confused with any IBM OCR work . - This is basic OCR and works best with dark text on a light background . <p> The Tesseract engine let 's you pass in an image , then handles the OCR operations , returning you a collection of words that it is able to extract from that image . - Once you have the text , you can do whatever you want from it . <p> So , here 's where Watson Developer Cloud Services come into play . First , I used the Watson Language Translation Service to perform the translation . - When using this service , I make a request to my- Node.js app running on IBM Bluemix ( IBMs cloud platform ) . - The Node.js app acts as a facade and delegates to- the Watson service for the actual translation . <p> You can check out a sample on the web here : <p> Translate english to : <p> On the mobile client , @ @ @ @ @ @ @ @ @ @ something with the response . The example below uses the IMFResourceRequest API to make a request to the server ( this can be done in either Objective C or Swift ) . IMFResourceRequest is the MobileFirst wrapper for networking requests that enables the MobileFirst/Mobile Client Access service to capture operational analytics for every request made by the app . <p> Once you receive the result from the server , then you can update the UI , make a request to the speech to text service , or pretty much anything else . <p> To generate audio using the Watson Text To Speech service , you can either use the Watson Speech SDK , or you can use the Node.js facade again to broker requests to the Watson Speech To Text Service . In this sample I used the Node.js facade to generate Flac audio , which I played in the native iOS app using the open source Origami Engine library that supports Flac audio formats . <p> You can preview audio generated using the Watson Text To Speech service using the embedded audio below . Note : In this sample @ @ @ @ @ @ @ @ @ @ work in browsers that support OGG . <p> On the native iOS client , I download the audio file and play it using the Origami Engine player . This could also be done with the Watson iOS SDK ( much easier ) , but I wrote this sample before the SDK was available . <p> Cognitive computing is all about augmenting the experience of the user , and enabling the users to perform their duties more efficiently and more effectively . The Watson language services enable any app to greater facilitate communication and broaden the reach of content across diverse user bases . You should definitely check them out to see how Watson services can benefit you . <h> MobileFirst <p> So , I mentioned that this app uses IBM MobileFirst offerings on Bluemix . In particular I am using the Mobile Client Access service to collect logs and operational analytics from the app . This let 's you capture logs and usage metrics for apps that are live " out in the wild " , providing insight into what people are using , how they 're using it , and the @ @ @ @ @ @ @ @ @ @ <h> Source <p> You can access the sample iOS client and Node.js code at https : **38;5476;TOOLONG . Setup instructions are available in the readme document . I intend on updating this app with some more translation use cases in the future , so be sure to check back ! <p> You may have heard a lot of buzz coming out of IBM lately about Cognitive Computing , and you might have also wondered " what the heck are they talking about ? " - You may have heard of- services for data and predictive analytics , services for natural language text processing , services for sentiment analysis , services understand speech and translate languages , but its sometimes hard to see the forest through the trees . <p> I highly recommend taking a moment to watch this video that introduces Cognitive Computing from- IBM : <p> Cognitive computing systems learn and interact naturally with people to extend what either humans or machine could do on their own . <p> They help human experts make better decisions by penetrating the complexity of Big Data . <p> Cognitive systems are often @ @ @ @ @ @ @ @ @ @ that detect- patterns and concepts that can be turned into actionable information for the end users . - Its not " artificial intelligence " in the sense that the services/machines act upon their own ; rather a system that provides the user tools or information that enables them to make better decisions . <p> The benefits of cognitive systems in a nutshell : <p> They augment the users experience <p> They provide the ability to process information faster <p> They- make complex information easier to understand <p> They enable you to do things you might not otherwise be able to do <p> Curious where this will lead ? - Now take a moment and watch this video talking about the industry-transforming opportunities that Cognitive Computing is already beginning to bring to life " <p> So , why is the " mobile guy " - talking about Cognitive Computing ? <p> First , its because Cognitive Computing is big I mean , really , really big . - Cognitive systems are literally transforming industries and providing powerful analytics and insight into the hands of both experts and " normal people " @ @ @ @ @ @ @ @ @ @ , I again- mean this literally ; much of this cognitive ability is being delivered to those end users through their mobile devices . <p> Last , and this is purely just- personal opinion , I see the mobile MobileFirst offerings themselves as providing somewhat of cognitive service for developing mobile apps . - If you look at it from the operational analytics perspective , you have an immediate insight and a snapshot into the health of your system that you would never have seen otherwise . - You can know what types of devices are hitting your system , what services- are being used , how long things are taking , and detect issues , all without any additional development efforts on your end . Its not predictive analytics , but sure is helpful and gets us moving in the right direction . 
@@106848866 @2248866/ <h> Yearly Archives : 2013 <p> Last week , new release-candidate versions of Camera Raw and Lightroom were posted to Adobe Labs that feature additional camera and lens support . I was extremely excited when I found out that one of the new camera profiles supported is the GoPro- Hero 3 . I 'm a huge fan of GoPro- cameras , and this means we now have more ways to get more creative with their usage . <p> I was recently thinking I absolutely love the shots I get off of the GoPro , but I wish there was an easy way to reduce the fisheye distortion . I wanted to try my hand at creating some aerial panoramas , but the distortion was causing issues . You can reduce the distortion using Photoshops Adaptive Wide Angle filter , but that can be tedious to get right . This release makes the process of reducing fisheye dead simple . <p> Reducing the lens distortion is now as simple as selecting the GoPro camera profile in Camera Raws lens correction settings , and you can use it to create some incredible @ @ @ @ @ @ @ @ @ @ to apply GoPro lens correction to both images and videos in Adobe Photoshop , Adobe Lightroom , and Adobe Bridge . <h> Images <p> Here are some side-by-side comparisons of before and after applying the GoPro lens correction . <h> Panoramas <p> I 've also been able to use this feature to create some awesome ( in my opinion ) aerial panoramas using the- DJI Phantom- quad-rotor remote controlled helicopter . The easiest way to create one of these panoramas is to select the images you want to merge inside of Adobe Bridge . Then right-click and select " Open in Camera Raw " , and then apply the lens correction to all of the images . Once you 've done that , keep the same images selected and go to Tools -&gt; Photoshop -&gt; Photomerge inside of Adobe Bridge . This will launch the Photomerge process inside Photoshop , and after a few minutes , you will have a nice panorama to work with . Take the generated panorama , turn it into a smart object , and then you can start applying other filters ( including Camera Raw ) to @ @ @ @ @ @ @ @ @ @ <p> Here are a few panoramas Ive created . Click on any one of them to view an interactive panorama , where you can zoom into the full resolution of the image . <p> To make the panoramas interactive , I used Photoshops " Zoomify " export , combined with the Leaflet- mapping library for an interactive HTML experience . You can ignore the HTML it generates , but keep the images and XML configuration file . I then used this open source Zoomify Layer for Leaflet- to make the images fully interactive , without any plugin dependencies . You can pinch/zoom and pan the images , and they are loaded as individual tiles , so its a smooth experience for the end user . <h> Next Steps <p> Go get started , and have fun ! You can download Camera Raw 8.2 and Lightroom 5.2 release candidates from labs.adobe.com just make sure to get the correct Camera Raw plugin for your suite ( CS6 or CC ) . <p> Also , check out this video produced by Adobes own Russell Brown for additional information : <p> I 'd like to @ @ @ @ @ @ @ @ @ @ Day in Portland last week , and to Colene and the team that put everything together ! The day was loaded with fantastic presentations , and great community interaction tons of information , tons of great questions , and tons of great people ( oh , and beer it would n't be PhoneGap Day without beer ) . <p> My session was about the integration of PhoneGap with hardware . Basically , it was an overview and exploration how you can use native plugins to extend the capabilities of PhoneGap applications and interact with device peripherals . This enables new interaction paradigms , and helps evaluate and evolve what is attainable with web-related technologies . <p> In this session , I covered two use cases The first use case is the use of a pressure-sensitive stylus for interacting with a PhoneGap application on iOS . The second use case is integration of a Moga gamepad with a PhoneGap application on Android . In both cases , the application experience is augmented by the peripheral device , which changes how it is possible for the user to interact and engage with the @ @ @ @ @ @ @ @ @ @ PhoneGap <p> In this example , - the pressure-sensitive Pogo Connect Stylus uses a low energy Bluetooth 4 connection to relay touch/pressure information back to the PhoneGap application . This makes for a unique drawing and sketching experience powered with the HTML5 Canvas element . I 've written about this example previously Check out the video below to see it in action , and read the blog post for technical details and source code . <h> Moga Gamepad <p> The second example that I explored is a PhoneGap native plugin that is used to handle input from a Moga game controller inside of a PhoneGap application on Android . <p> This implementation is intended to be a proof of concept demonstrating how you could integrate the gamepad within your application . It currently only supports input from the joysticks ( axisX and axisY ) and the A and B buttons , and it does not handle all possible input from the controller . <p> This implementation is adapted directly from the **38;5516;TOOLONG example from the Moga developers SDK samples available for download at : : - http : **34;5556;TOOLONG 123434 @qwx983434 @ @ @ @ @ @ @ @ @ @ was used as a sub-game inside of the MaxMe app for the recent Adobe MAX conference . I make no guarantees about the code for this game , it was in a huge rush ! <p> I am very excited to share that the Fresh Food FinderPhoneGap application which I built last summer was recently featured in the June 2013 issue of SELF Magazine . This app was built as a demonstration/sample application for PhoneGap , but has gained popularity for being an easy way to find farmers markets by leveraging the USDA farmers markets directory- and has appeal to the general public . <p> In fact , Ive seen so many downloads that I am working on an updated version of it based on user feedback , with more recent data ( live services instead of embedded ) , and will be leveraging the awesome topcoat.io CSS framework for styling . <p> Fresh Food Finder in SELF Magazine <p> SELF Magazine has huge readership numbers ( almost 1.5 million monthly subscribers according to Wikipedia ) . - While it was just a very small callout in the magazines calendar @ @ @ @ @ @ @ @ @ @ my blog ( peaked at 348 downloads per day ) ! <p> App downloads May 1- July 8 <p> This also brought the app higher up in the iOS market ranks ( peak #57 in the " Food &amp; Drink " category ) : <p> App store ranking May 1- July 8 <p> You can download the Fresh Food Finder app- today in the following markets : <p> I have a new " hobby " , and I must admit , I am completely addicted . Perhaps " obsessed " might be the proper term . Luckily for me , it is a great use of Creative Cloud , so I can get away with it ! What is this new obsession , you ask ? Aerial Photography . <p> However , I 'm not flying airplanes or riding in helicopters . In fact , my feet are n't  even leaving the ground at all just my camera . It all started a few weeks ago when my friend- Tony- approached me about getting some aerial photos for experimentation. - He knew I already had a GoPro camera , and that @ @ @ @ @ @ @ @ @ @ , and are very durable . The only thing was that we needed a reliable way to get it off the ground . <p> Enter part 2 of the equation , and the greatest part of my obsession : the quadcopter. - Basically , it is a multi-rotor remote controlled helicopter. - Multi-rotor copters are mechanically very simple , very stable , and versatile . Just check out this TED video to see how agile and versatile they can be . <p> I had been wanting one of these for a long time . Researching here and there , looking at the DIY and pre-built kits . I wanted something that would be easy to fly , but also capable of outdoor flight and a camera payload . Copters can range from inexpensive childrens toys- to high-end hex/octo-rotor professional rigs for many thousands of dollars , and I looked at nearly every one of them . I finally settled on the DJI Phantom . <p> I chose this copter mainly for two reasons : <p> It has a GPS guidance system that makes it easy to fly , corrects for @ @ @ @ @ @ @ @ @ @ home " feature if the battery gets low or if it loses contact with the controller . Yet , it can be very maneuverable and agile. - <p> DJIs promise of " ease of use " definitely holds true . It was setup and running very quickly , with the longest part of the setup process waiting for the battery to charge . Once we finally got it off the ground , there was no going back Now , with everything that I look at , I wonder " Wouldnt that be cool to take a picture of from the air ? " <p> A word of warning , once you start down this path , it is very easy to become obsessed I will elaborate more on this subject in a bit . For now , check out some of my recent photos with this rig . You can see the full collection ( and any future photos ) on my Flickr photostream . <p> In fact , we were so excited to fly it , that we did n't  even wait for daylight ! The Phantom has a @ @ @ @ @ @ @ @ @ @ problem flying at night , at all . here 's a short video from my first quadcopter flight : <p> While this setup takes amazing still images out-of-the-box , you may have noticed that the video is n't all that smooth . Some of the shaking can be reduced by refining your piloting techniques ( do n't  ascend/descend too fast , etc ) . However , the default configuration is n't ideal for video capture . Though , much of that can be corrected with some ad-ons ( which I will be investing in ) , and software-based video stabilization in After Effects . Again , I 'll cover this more in a bit <h> Lessons Learned <p> My explorations into aerial photography have provided me with some valuable lessons <p> Accept the fact that you will crash. - Even though the Phantom is very easy to fly , at some point you will , without a doubt , crash the copter . Luckily every piece of the copter can be replaced . If your battery is almost dead and you try to take off , the copter will just flip over onto the @ @ @ @ @ @ @ @ @ @ is a strong gust of wind , you will likely crash . If you descend too quickly , you will crash . I mention each of these , because I have done all of them . <p> Immediately replace the nuts that hold the propellers onto the copter. - Within the first week of owning the Phantom , I lost a propeller and the copter came- plummeting- to the ground . Luckily , I was only about ten feet off of the ground when this happened . The " dome " nuts that ship with the copter look pretty , but are not reliable . Get some new nuts with lock washers , Nylock nuts , and/or some thread lock to prevent the nuts from loosening due to vibration . If you use thread lock , just be sure NOT to get any on the plastic propellers . I have heard it will do bad things to them . <p> Be prepared to obsess over the ways that you can use and modify the copter . This leads me to my next section <p> Add-Ons <p> If you are like @ @ @ @ @ @ @ @ @ @ the ways that you can use the Phantom once you get it . This will lead you down the road of watching countless YouTube videos that others have created , reading DIY instructions for modifications , and scouring the web for upgrade options . <p> Must haves : <p> Extra batteries . The Phantom comes with one battery that lasts 10-12 minutes . You will want more batteries for additional flight time . <p> Better nuts to secure the propellers . ( See my lessons learned above ) <p> Of course , not all modifications are " must haves " ; here are the next few options that are on my list . If anyone is looking for a gift idea for me , look at this list ! <p> Travel Case. - Without- a doubt , you will want to protect your copter . I have mine in a cardboard box lined with bubble-wrap to prevent damage from travel and toddlers . If you plan on travelling with your copter ( which I do ) , you will want something like one of these . <p> Camera Gimbal System. @ @ @ @ @ @ @ @ @ @ regardless of the pitch or yaw of the copter . This will make a huge difference if you are producing video . There are lots of options , from DIY kits , to the official DJI/Zenmuse Gimbal system that was just released today . I am drooling over these options . <p> Vibration Isolation Mount. - If you have a gimbal , you probably already have this , but if you do n't  , you might want one . A vibration isolation mount will reduce the copter vibrations that are transferred to the camera . This will result in better quality images and videos , and less " jello effect " . <p> ND/Polarized filters for the GoPro camera. - ND filters and polarized filters can help improve the captured image or video , and can minimize the " jello effect " due to vibrations . <p> FPV Video Navigation System . An FPV ( First Person View ) video system gives you the ability to see exactly what the copter sees when you are flying . This can be really useful for framing of photos , or flying beyond @ @ @ @ @ @ @ @ @ @ your copter . You can use the GoPro app as a limited FPV- system ; although keep in mind that it operates over an ad hoc wifi connection . If you lose the connection , you lose the video feed . <p> I mentioned that people are creating amazing images and videos with this rig ; here are just a few that blew me away <h> Creative Cloud <p> So , Ive written all about the copter so far , but what about Creative Cloud ? - How does it tie into quadcopters ? Creative Cloud enables you to do more with the aerial imagery that you capture from the quadcopter . <p> When taking still images , I setup the camera so that it will- automatically- take two pictures every second . When you 're flying for 10 minutes , this ends up being a lot of images . Lightroom enables you to quickly import , tag , develop , and publish your images . <p> Many of the images need additional editing before they are suitable for your final product . Whether you need to remove objects ( such @ @ @ @ @ @ @ @ @ @ , or you want to tweak clouds or make other dramatic changes , Photoshop enables you to do these things. - Photoshop enables rich editing and retouching of images . <p> Of course , its not all about still images . Premiere and After Effects enable editing and production of your captured videos . You can use Premiere to arrange multiple clips into a broader sequence , and you can add post-processing effects and image stabilization with After Effects . <p> and what is a video without audio ? Audition provides you with a rich environment for audio production . Whether it is editing the captured audio , or producing new music and soundtracks , Audition has you covered . <p> here 's another video that I produced using Premiere for edits , stabilization with After Effects , and Audition for audio tweaks . Granted , I do not have a gimbal or other stabilization system , so bear with the quality ! <p> This is just the proverbial " tip of the iceberg " . Creative Cloud gives you the tools you need to create amazing multimedia content , and @ @ @ @ @ @ @ @ @ @ hobbyist to die-hard professional . <p> Once you 're happy with what you 've got , do n't  forget you can share it on Behance via your Creative Cloud membership ! <p> Adobe Edge Inspect CC is an awesome tool for synchronized browsing experiences across both desktop and mobile devices . It is incredibly powerful , and streamlines the developer &amp; testing workflows by reducing iterations when testing HTML/JS experiences on multiple devices . <p> Adobe Edge Code CC is an awesome code editor for creating HTML/JS experiences . It has a live connection to the browser , so you can see your edits in the browser in real time as you are editing your code all without having to leave the code editor . <p> Wouldnt it be cool if Adobe Edge Code CC could push updates to mobile devices leveraging Adobe Edge Inspect CC ? Oh wait , what ? It already can ? YES , it can ! This is one of the coolest new features in this weeks Creative Cloud release . You can edit your code in Edge Code CC , and preview your changes live , @ @ @ @ @ @ @ @ @ @ on mobile devices . Take a look at the video below to see it in action : <p> OK , so how do you get it ? Become a- member- of Creative Cloud Membership does have its perks ! This is available in all of the plans , even the free option . This plugin for Edge Code CC/Brackets is also open source . Know what else is cool about Adobe Edge Inspect CC ? It has an open source JavaScript API that you can leverage to integrate your own apps into an Edge Inspect workflow or continuous integration environment . Its definitely worth checking out . 
@@106848867 @2248867/ <h> Repurposing PhoneGap Apps as Desktop Apps <p> I was inspired to write this post by several recent conversations . - I was in a debate about whether with the Flex/Flash platform you could easily repurpose content to the desktop using Adobe AIR ( and vice-versa ) , but that you could n't easily do that with PhoneGap applications . ( My stance was that yes , you could repurpose content . ) <p> I wanted to make sure that people were aware that you can repurpose your content , and here 's an example of how . <p> A while back , I wrote a sample PhoneGap application that allows you to- browse information from the 2010 US Census . - You can read more about this application and download the source code here . This application supports lots of platforms iOS , Android , BlackBerry , and web ( everything except IE because I was targetting WebKit browsers ) . <p> While this application is a mobile app wrapped in the PhoneGap container , I actually did n't  use any PhoneGap-specific- libraries , so it was very easy to @ @ @ @ @ @ @ @ @ @ AIR version of this application , which you can download at : <p> I had to use my Android 2. x branch of the US Census Browser code because the WebKit instance inside of AIR does n't  support SVG. - I also changed the container scrolling to use normal CSS " overflow : auto " instead of using iScroll for touch-based scrolling . There were a few other one-off CSS changes to tweak the layout in the AIR web container , but otherwise the code is identical . <p> You just need to create an AIR application XML file and point it to your HTML content , and then package it using ADT . <p> If you were using PhoneGap APIs , you would have to migrate your code to take advantage of AIR APIs , but all other HTML/CSS/JS could be reused with minimal changes . <p> Even though I used AIR for this example , AIR is n't the only game in town for HTML-based desktop applications - Theres an open source project called MacGap , you can use HTA for Windows , and its not hard to write @ @ @ @ @ @ @ @ @ @ been reported that you are going to be able to write apps for Windows 8 purely using HTML &amp; JS , and you would be able to repurpose your code for this as well . 
@@106848868 @2248868/ <h> Tag Archives : MobileFirst <p> In this entry were going- to focus- on building Apple Watch apps that can communicate back and forth with- the host application running on the iPhone. - This is extremely important since the Apple Watch provides a second screen/peripheral complimentary experience to the main app running on the iOS device be it a remote control , or quick view/glance into what 's happening within the bigger picture . <p> In my last post I showed how to setup remote logging and **25;5592;TOOLONG in an Apple Watch app using IBM MobileFirst Platform Foundation server . - I used the- methods described below for communicating between the WatchKit and host apps in the- sample app from that- previous post . <p> When were talking about bidirectional communication , were talking about sending data two ways : <p> Sending data from the host app to the WatchKit app <p> Sending data to the WatchKit app from the host- app <p> At first thought , you might think " oh that 's easy , just use- NSNotificationCenter to communicate between the separate classes of the application " , but @ @ @ @ @ @ @ @ @ @ Watch app is really made of 3 parts : 1 ) the main iOS application binary , 2 ) the user interface on the Apple Watch , and 3 ) the WatchKit extension binary ( on the iOS device ) . <p> Apple Watch App Architectural Components <p> Yep , you read that correctly , the WatchKit extension ( which controls all of the logic inside the Apple Watch UI and resides on the iOS device ) is a separate binary from the " main " iOS application binary . - These are separate processes , so- objects in memory in the main app are not the same objects in memory in the extension , and as a result , these processes do not communicate directly . NSNotificationCenter is n't going to work . <p> However there are definitely ways you can make this type of a scenario work . <p> First , WatchKit has methods to invoke actions on the host application from the WatchKit extension . - WatchKits- openParentApplication- or **30;5619;TOOLONG methods both provide the ability to invoke actions and pass data in the containing app , and provide @ @ @ @ @ @ @ @ @ @ back in the WatchKit extension after the code in the host application has been completed . <p> For example , in the- WatchKit extension , - this will invoke an action in the host application and handle the reply : <p> Inside the host application we have access to the userInfo NSDictionary that was passed , and we can respond to it accordingly . For example , in the code below I am setting a string value on the userInfo instance , and taking appropriate actions based upon the value of that string . <p> This covers the " pull " scenario , and is great if you want to invoke actions within your host app from your WatchKit extension , and then handle the responses back in- the WatchKit extension to update your Apple Watch UI accordingly . <p> What about the " push " scenario ? - The previous scenario only covers requests that originate inside the WatchKit extension . - What happens if you have a process running inside of your host app , and- have updates that you want to push to the WatchKit extension without @ @ @ @ @ @ @ @ @ @ , and it is not a shared process , so neither NSNotificationCenter or direct method invocation will work . However , you *can* use Darwin notifications ( which work across seprate processes by using- CFNotificationCenter ) . - This enables near-realtime interactions across processes , and you can share data as attributes- of a CFdictionary- object based between processes . You can also share larger amounts of data using access- groups , and notify the separate processes using the CFNotificationCenter implementation . <p> Note : CFNotificationCenter is C syntax , not Objective-C syntax . <p> First you 'll need to subscribe for the notifications in the WatchKitExtension . Pay attention to the static i 'd instance " staticSelf " you 'll need this later when invoking Objective-C methods from the C notification callback . <p> We 've now covered scenarios where you you can request data or actions in the host application *from* the WatchKit extension , and also how you can push data from the host application to the WatchKit extension . <p> Now , what if there was a library that encapsulated some of this , and made it even easier for @ @ @ @ @ @ @ @ @ @ my previous post , I used the methods described above . However , I recently stumbled across the open source- MMWormhole , which wraps the Darwin Notifications method ( above ) for ease of use . - I 'm pretty sure I 'll be using this in my next WatchKit app . <h> Helpful Links for inter-process communication between WatchKit and host apps : <p> This is the first entry in a multipart series on powering native iPhone and Apple Watch apps using the IBM MobileFirst Platform . - In this entry we will cover how to setup the MobileFirst Platform for use within Apple WatchKit apps and leverage the operational analytics and remote logging features . <p> So , let 's first take a look at the app were going to build in this video : <p> The app is a simple location tracker. - Think of something like a much simpler version of Run Keeper that will allow you to track your location path over a period of time , and show your location on a map . - Were also building a WatchKit app that enables you to quickly start @ @ @ @ @ @ @ @ @ @ your iPhone out of your pocket . - All of this powered by IBM MobileFirst . <p> When you 're setting up your WatchKit app , you need to follow the exact same steps that you did for the native app target , just apply them to your WatchKit extension target . <p> First you need to add the required frameworks and dependencies ( full list here , also be sure to include **36;5651;TOOLONG that is inside the iOS API ) : <p> Add MobileFirst Frameworks and Dependencies <p> Next , add the " -ObjC " linker flag : <p> Add Linker Flag <p> Then make sure that worklight.plist ( which is inside of the MobileFirst API you generated from either the CLI or Eclipse Studio ) so that it is included in both the native app and WatchKit extension . <p> Worklight.plist Target Membership <p> This allows you to take advantage of MobileFirst APIs within your WatchKit extension , complete with operational analytics. - You cansave remote logs , you can access data adapters , and more. - The server-side security mechanisms also work , so if you want to @ @ @ @ @ @ @ @ @ @ that ability . <p> I mentioned earlier , its just like a native iOS app , but with a few exceptions . - The most important and notable exception is that the UI elements ( modal dialogs , alerts , etc ) that you would normally see in the native phone interface do not appear in the WatchKit interface . - You do n't  get errors you just do n't  see the notification . - So , you need to work around any scenarios that rely on this , and make sure you handle errors accordingly . <p> To invoke MobileFirst APIs , you call them as you wold normally in either Objective-C or Swift . - For example : <p> and the remote log search capability , including- logs from the WatchKit extension : <p> MobileFirst Remote Logging with the WatchKit App <p> That 's all that you need to get started ! <p> Stay tuned ! - Full source code will be released on my github account in a subsequent post. - Also be sure to stay tuned for future entries that cover the MobileFirst platform with offline data @ @ @ @ @ @ @ @ @ @ geo notifications , bidirectional communication between the watch and host app , background processing , and more ! I will update this post to links to each subsequent post as it is made available . <p> Wondering what IBM MobileFirst is ? - Its a platform that enables you to deliver and maintain mobile applications throughout their entire lifecycle. - This includes tools to easily manage data , offline storage , push notifications , user authentication , and more , plus you get operational analytics and remote logging to keep an eye on things once you 've deployed it to the real world , and its available as- either cloud or on-premise solutions . <p> I recently put together some content on building " Apps that Work as Well Offline as they do Online " using IBM MobileFirst and Bluemix ( cloud services ) . - There was the original- blog post , I used the content in a presentation at ApacheCon , and now I 've opened everything up for anyone use or learn from . <p> The content now lives on the IBM Bluemix github account , and includes code @ @ @ @ @ @ @ @ @ @ ( Node.js ) endpoint , a comprehensive script that walks through every step of of the process configuring the application , - and also a video walkthrough of the entire process from backend creation to a complete solution . <p> Key concepts demonstrated in these materials : <p> User authentication using the Bluemix Advanced Mobile Access service <p> Remote app logging and instrumentation using the Bluemix Advanced Mobile Access service <p> Client-side Objective-C code ( you can do this in either hybrid or other native platforms too , but I just wrote it for iOS ) . - The " iOS-native " folder contains the source code for a complete sample application leveraging this workflow . The " GeoPix-complete " folder contains a completed project ( still needs you to walk through backend configuration ) . The " GeoPix-starter " folder contains a starter application , with all MobileFirst/Bluemix code commented out . You can follow the steps inside of the " Step By Step Instructions.pdf " file to setup the backend infrastructure on Bluemix , and setup all code within the " GeoPix-starter " project . <p> Earlier this @ @ @ @ @ @ @ @ @ @ Austin , TX on the topic of data management for apps that work as well offline as they do online . - This is an important topic for mobile apps , since , as we all painfully know already , there is never a case when you are always online on your mobile devices . - There always ends up being a time when you need your device/app , but you cant get online to get the information you need . - Well , this does n't  always have to be the case . There are strategies you can employ to build apps that work just as well offline as they do online , and the strategy- I 'd like to highlight today is based upon data management using the IBM Cloudant NoSQL database as a service , which is based upon Apache CouchDB . <p> here 's a link to the presentation slides ( built using reveal.js ) just use the space bar to advance the presentation slides : <p> The " couch " in CouchDB is actually an acronym for Cluster of Unreliable Commodity Hardware . At the core of @ @ @ @ @ @ @ @ @ @ the most basic of terms means that - data is shared between multiple sources . - Replication is used to share information between nodes of the cluster , which provides for cluster reliability and fault tolerance . <p> Cloudant is a- clustered NoSQL- database services that provides an extremely powerful and searchable data store . - It is- designed to power- the web and mobile apps , and all information is exposed via REST services . Since the IBM Cloudant service is based on CouchDB ( and not so coincidentally , IBM is a major contributor to the CouchDB project ) , replication is also core the the Cloudant service . <p> With replication , you only have to write your data/changes to a single node in the cluster , and replication takes care of propagating- these changes across the cluster . <p> If you are building apps for the web or mobile , there are options to extend the data replication locally either on the device or in the browser. - This means that you can have a local data store that automatically pushes and/or pulls data from the @ @ @ @ @ @ @ @ @ @ either via native languages , or using JavaScript . <p> If you want to have local replication in either a web or hybrid ( Cordova/PhoneGap ) app , you can use PouchDB. - PouchDB- is a local JavaScript database modeled- after CouchDB and implements that CouchDB replication API. - So , you can store your data in the browsers local storage , and those changes will automatically be replicated to the remote Cloudant store . - This works in the browser , in a hybrid ( web view ) app , or even inside of a Node.js instance . Granted , if you 're in-browser you 'll need to leverage the HTML5 cache to have your app cached locally . <p> If you are building a native app , do n't  worry , you can take advantage of the Cloudant Sync API to leverage- the local data store with replication . - This is available for iOS and- Android , and implements the CouchDB replication API . <p> The sample app that I showed in the presentation is a native iOS application based on the GeoPix MobileFirst sample app- that I detailed @ @ @ @ @ @ @ @ @ @ in this case I showed it using the Cloudant Sync API , instead of the MobileFirst data wrapper classes , even though it was pointing at the exact same Cloudant database instance . - You can see a video of the app in action below . <p> All that you have to do is create a local data store instance , and then use replication to synchronize data between the local store and a remote store . <p> Replication be either one-way ( push or pull ) , or two-way . - So , any changes between the local and remote stores are replicated across the cluster . - Essentially , the local data store just becomes a node in the cluster . - This provides complete access to the local data , even if there is no network available . - Just save your data to the local store , and replication takes care of the rest . <p> In the native Objective-C code , you just need to setup the CDTDatastore manager , and initialize your datastore instance . <p> Once your datastore is created , you can @ @ @ @ @ @ @ @ @ @ this case I am creating a generic data object ( basically - like a JSON object ) , and creating a document containing this data . - A document is a record within the data store . <p> You can add attachments to the document or modify the document as your app needs . - In the code below , I add a JPG atttachment to the document . <p> Replication is a fire-and-forget process . - You simply need to initialize the replication process , and any changes to the local data store will be replicated to the remote store automatically when the device is online . <p> By assigning a replicator delegate class ( as shown above ) , your app can monitor and respond to changes in replication state . - For example , you can update status if replication is in progress , complete , or if an error condition was encountered . <p> If you want to access data from the local store , it is always available within the app , regardless of whether or not the device has an active internet connection . @ @ @ @ @ @ @ @ @ @ within the local data store . <p> In this post I 'd like to show a fairly simple application that I put together which shows off some of the rich capabilities for IBM MobileFirst for Bluemix that you get out of the box All with an absolute minimal amount of your own developer effort . - Bluemix , of course , being IBMs platform as a service offering . <p> GeoPix is a sample application leveraging IBM MobileFirst for Bluemix to capture data and images on a mobile device , persist that data locally ( offline ) , and replicate that data to the cloud . Since its built with IBM MobileFirst , we get lots of things out of the box , including operational analytics , user authentication , and much more . <p> ( full source code at the bottom of this post ) <p> Heres what the application currently- does : <p> User can take a picture or select an image from the device <p> App captures geographic location when the image is captured <p> App saves both the image and metadata to a local data store- on @ @ @ @ @ @ @ @ @ @ save any data in local store up to the remote store whenever the network is available <p> Oh yeah , cant forget , the user auth- is via Facebook <p> MobileFirst provides all the analytics we need . - Bluemix provides the cloud based server and Cloudant- NoSQL data store . <p> All captured data is available on a web based front-end powered by Node.js <p> In this sample I 'm using everything but the Push Notifications service . - Im- using user authentication , the Cloudant DB ( offline/local store and remote/cloud store ) , and the node.js backend. - You get the operational analytics automatically . <h> Capturing Images <p> Capturing images from the device is also very straightforward . - In the app I leverage Apples- UIImagePickerController- to allow the user to either upload an existing image or capture a new image . - See the presentImagePicker and- **29;5689;TOOLONG below . All of this standard practice using Apples developer SDK : <h> Persisting Data <p> If you notice in the- **29;5720;TOOLONG method above , there is a call to- the DataManagers saveImage withLocation method . This is where @ @ @ @ @ @ @ @ @ @ automatically save data from the local data store up to the Cloudant NoSQL database . - This is powered by the iOS 8 Data service from Bluemix . <p> The first thing that we will need to do is initialize the local and remote data stores . Below you can see my init method from my DataManager class . In this , you can see the local data store is initialized , then the remote data store is initialized . If either data store already exists , the existing store will be used , otherwise it is created . <p> Once the data stores are created , you can see that the replicate method is invoked . - This starts up the replication process to automatically push changesfrom the local data store to the remote data store " in the cloud " . <p> Therefore , if you 're collecting data when the app is offline , then you have nothing to worry about . - All of the data will be stored locally and pushed up to the cloud whenever you 're back online all with no additional effort on your @ @ @ @ @ @ @ @ @ @ , you just have to start the replication process and let it do its thing fire and forget . <p> In my replicate function , I setup- CDTPushReplication for pushing changes to the remote data store . - You could also setup two-way replication to automatically pull new changes from the remote store . <p> Once weve setup the remote and local data stores and setup replication , we now are ready to save the data the were capturing within our app . <p> Next is my saveImage withLocation method . - Here you can see that it creates a new- **26;5751;TOOLONG object ( this is a generic object for the Cloudant NoSQL database ) , and populates it with the location data and timestamp. - It then creates a jpg image from the UIImage ( passed in from the UIImagePicker above ) and adds the jpg as an attachment to the document revision . - Once the document is created , it is saved to the local data store . - We then let replication take care of persisting this data to the back end . <p> If we @ @ @ @ @ @ @ @ @ @ data stores , we can just use the performQuery method on the data store . Below you can see a method for retrieving data for all of the images in the local data store . <p> At this point we 've now captured an image , captured the geographic location , saved that data in our local offline store , and then use- replication to save that data up to the cloud whenever it is available . <p> AND <p> We did all of this without writing a single line of server-side logic . - Since this is built on top of MobileFirst for Bluemix , all the backend infrastructure is setup for us , and we get operational analytics to monitor everything that is happening . <p> With the operational analytics we get : <p> App usage <p> Active Devices <p> Network Usage <p> Authentications <p> Data Storage <p> Device Logs ( yes , complete debug/crash logs from devices out in the field ) <p> Push Notification Usage <h> Sharing on the web <p> Up until this point we have n't had to write any back-end code . However the mobile @ @ @ @ @ @ @ @ @ @ - We might as well take advantage of it . <p> The Node.js back end comes preconfigured to leverage the- express.js- framework for building web applications . - I added the- jade template engine and Leaflet for web-mapping , and was able to crank this out ridiculously quickly . <p> The first thing we need to do is make sure - we have our configuration variables for accessing the Cloudant service from our node app. - These are environment vars that you get automatcilly if you 're running on Bluemix , but you need to set these for your local dev environment : <p> Next you 'll se the logic for querying the Cloudant data store and preparing the data for our UI templates . You can customize this however you want caching for performance , refactoring for abstraction , or whatever you want . All interactions with Cloudant are powered by the Cloudant Node.js Client 
@@106848869 @2248869/ <h> Category Archives : Adobe <p> I awoke this morning to discover that Machinarium , originally developed as a Flash game and recompiled for iOS using Adobe AIR for mobile , is the #1 paid iPad game in Apples App Store , and the #2 overall paid iPad application . - Adobe Tools make great apps for mobile devices . <p> In this presentation , Adobe Evangelist Andrew Trice will walk through several applications utilizing Flex and AIR for mobile and will highlight application ecosystems , development processes , device considerations , and many of the " nuts and bolts " of multi-platform development using Adobes toolsets . <p> In this session , we will explore mobile application development using Flex and AIR for mobile , powered by ColdFusion backend systems . By the end of this session , you will know how to build natively installed iOS , Android , and BlackBerry Playbook applications , and you will be able to utilize your existing CF skills to power them . <p> On the official Adobe Flex Team blog , there is a great post by Andrew Shorten discussing @ @ @ @ @ @ @ @ @ @ a moment to read it . In that post Andrew points out where Flex is , and where Flex is heading . One thing I want to re-emphasize is that mobile is the next big thing . <p> It has been proclaimed many times , in many publications that mobile devices ( tablets and smartphones ) are the future of computing . This is both in enterprise and consumer products &amp; applications . One of the catches with this growth is that each platform has its own development tooling and language . Wouldnt it be nice if you could just use one programming model &amp; technology stack ? Even better , would n't it be nice if you could use that same programming model &amp; technology stack to also develop applications for the web and the desktop ? Wow , what if you could even share code libraries across mobile , web , and desktop applications ? That would be awesome . <p> Wait you can already do that ! <p> Adobe Flex is the best tool for creating cross-platform , rich experiences in mobile , desktop , and web applications @ @ @ @ @ @ @ @ @ @ biggest enhancements introduced with Flash Builder 4.5.1 was the inclusion of mobile tooling . These tools allow you to easily create rich experiences targeting a variety of mobile devices iOS , Android , BlackBerry Playbook . All of which are natively installed applications that can be shared by the standard distribution models : App Store , Enterprise distribution , etc <p> The best part is that you do n't  need to learn any new programming skills to develop and deploy for these platforms . You will need to learn about the app ecosystems , platform signing and deployment procedures , and device specifics ( soft keyboards , hardware buttons , etc .. ) . However , you can still develop these applications quickly and easily using Flex , ActionScript , and AIR APIs . One code base ; multiple platforms ; lots of devices . Did you know that you could even take that same code and deploy it to a TV ? Even better , there are great new advancements in the Flex/AIR mobile tooling waiting just over the horizon . <p> We 're continuing to focus on runtime @ @ @ @ @ @ @ @ @ @ , adding more platforms and improving tooling workflows , such that in our next major release timeframe we expect that the need to build a fully-native application will be reserved for a small number of use cases . <p> The growth of the mobile market and the challenge of building out applications that work on a range of different form-factors and platforms present us with a huge opportunity to expose Flex to an entirely new audience of developers , while continuing to be relevant for existing Flex developers who are extending their applications to mobile . <p> Flex &amp; AIR for mobile allow you to use the same enterprise class tooling to build cross platform mobile applications . You can still use existing framework components , existing open source libraries , the strongly typed programming language , automated ASUnit testing , build scripts , and many other features that Flex offers , and you can now target mobile devices . <p> Stage3D is starting to look incredibly promising for developers . Whether you are targeting mobile , web , or desktop or developing for gaming or data visualization , the @ @ @ @ @ @ @ @ @ @ in San Francisco last week meeting with the team , and Thibault Imbert showed us some jaw dropping demos Keep an eye out for whenever those become public . <p> but it is not all about games . In a recent post , I showed off some basic stage3d hardware accelerated graphics within AIR on mobile devices . Ive been developing some ideas to show how this can be applied within non-gaming contexts , and I figured I 'd show my progress so far . The previous post was an actionscipt-only project . In enterprise scenarios , you most likely will want to take advantage of the plethora of pre-built components that the Flex framework provides . The progression for me was natural ; How about showing what Stage3D can do within a Flex for mobile application ? <p> Above is a fairly basic visualization application . There is a stage3D-based custom charting component , tied into data that is displayed using a standard spark List with a custom item renderer . The chart is based on my previous example , and obviously is n't fully fleshed out ( no legend , @ @ @ @ @ @ @ @ @ @ Did I mention , this is on a tablet ? Oh yeah , that is pretty awesome . Did I also mention that it will be cross platform once released ? Oh yeah , that is pretty awesome too . <p> The data is morbid , but it works for my scenario . It is based on this data set from CMU.edu , which shows the cancer deaths and the number of cigarrettes consumed per state . <p> Please keep in mind , this is all beta , and not a final build . Ill post the source code once Stage3D for mobile is publicly available . 
@@106848871 @2248871/ <h> Tag Archives : devices <p> here 's a quick video introduction I put together for Adobe Edge Inspect , a *free* tool that enables synchronized browsing and debugging of HTML content between desktop and mobile devices . Its definitely worth checking out , if you are n't  using it already . <p> Adobe Edge Inspect enables synchronized browsing across desktop and mobile devices . By pairing your devices with Edge- Inspect- running on your desktop , any content you view in the desktop browser will be synched to all paired mobile devices . <p> Adobe Edge Inspect also makes it easy for you to track and document visual issues on the remote devices . With just one button click , you can capture screenshots from all connected devices , complete with information about each device ( OS , device name , screen resolution , etc ) . <p> Screen Captures <p> Did I mention Adobe Edge Inspect is also free ? Its part of the free tier of Adobe Creative Cloud . <p> I covered techniques for making your apps feel like " apps " , not " web @ @ @ @ @ @ @ @ @ @ techniques and useful libraries in my recent post on Multi-Device Best Practices . - Note : That post contains references to both Flex and HTML/JS/CSS tools . - In this presentation I focused only on the HTML/JS/CSS tools . <p> In this presentation , I covered PhoneGap Build , a cloud-based compilation tool for PhoneGap apps , and- debug.phonegap.com for remote application debugging . - I also covered iWebinspector for debugging PhoneGap experiences inside of the iOS Simulator . <p> Let 's also not forget the real-world companies that have invested in PhoneGap/Apache Cordova , including Wikipedia , Facebook , Salesforce , IBM , and others . - You can read more about these companies from my recent post " Who Uses PhoneGap " . <p> I am happy to announce the US Census Browser version 2.0 ! - Back in December of 2011 , I released the US Census Browser- as an open source- application intended to demonstrate principles for enterprise-class data visualization and applications developed with web standards . - This version has some fairly substantial changes See the video below to check out features in the latest version @ @ @ @ @ @ @ @ @ @ some substantial changes , including : <p> Completely new &amp; redesigned UI layer , using app-UI. - app-UI is an open source framework for application view-navigators that mimic native mobile applications . - Using the app-UI SplitViewNavigator , the US Census Browser now supports both landscape and portrait orientations . <p> Switched from Google Maps to Open Street Map using OpenLayers. - Users of the Census Browser maxed out my Google Maps account ! - That is 25,000 map loads within a 24 hour period ! WOW ! I switched to the free Open Street Maps solution , which does n't  have any usage/bandwidth limitations . - With this change I was also able to add interactive maps . <p> Updated to Twitter Bootstrap 2.0. - The app is now using new UI styles and components which are now available in Twitter Bootstrap version 2.0 <p> I 'd like to take a moment and introduce app-UI , a new open source application framework that I 've been working on . <p> app-UI is a collection of reusable " application container " user interface components that may be helpful to web and mobile @ @ @ @ @ @ @ @ @ @ especially those targeting mobile devices . app-UI is a continual work in progress it was born out of the necessity to have rich &amp; native-feeling interfaces in HTML/JS experiences , and it works great with PhoneGap applications ( http : //www.phonegap.com ) . app-UI can easily be styled/customized using CSS . <p> Disclaimer : Please keep in mind that things will change as the project is improved and matured this is a beta/early prototype . <p> All of app-UI was created using HTML , CSS , &amp; JavaScript . All animations are rendered using CSS3 translate3d , so that they are hardware accelerated ( where supported ) . app-UI works well on iOS , Android and BlackBerry browsers ( others not tested ) , and works well on the latest releases of most desktop browsers ( I know it does not work on old versions of IE ) . <h> Why ? <p> You might be wondering " why create this ? " when there are other open source alternatives like jQuery Mobile . The primary motivation for creating app-UI was to have reusable application containers that are highly performant @ @ @ @ @ @ @ @ @ @ With respect to animations/transitions , app-UI outperforms the alternatives , particularly on mobile devices . <p> app-UI can be used with many different existing frameworks app-UI only requires jQuery as a solution accelerator framework . It will work with existing UI widget frameworks ( jQuery UI , Twitter Bootstrap , etc ) , and will work with existing templating frameworks ( Moustache , Knockout , Handlebars , etc ) . <h> Application Containers <p> app-UI currently has three application containers , and at this time it is not intended to be a complete UI widget framework . <p> Please see the " samples " directory for usage scenarios there is no documentation yet . <h> ViewNavigator <p> The ViewNavigator component allows you to create mobile experiences with an easily recognizable mobile UI paradigm . You use this to push &amp; pop views from the stack . <h> SplitViewNavigator <p> The SplitViewNavigator component allows you to create tablet experiences with an easily recognizable mobile UI paradigm . The SplitViewNavigator allows you to have side-by-side content in the landscape orientation , and the sidebar is hidden in portrait orientation . <h> SlidingView @ @ @ @ @ @ @ @ @ @ using a horizontal swipe gesture , revealing a navigation container " underneath " . This is very similar to the behavior in Facebooks iPad application . <p> Recently Ive been spending a fair amount of time working on HTML-based applications both mobile web and mobile applications using PhoneGap. - Regardless of whether you are targeting a mobile web browser or a mobile app using the PhoneGap container , you are still targeting a mobile web browser instance . - If you have n't noticed , mobile web browsers can often have- peculiarities- with how content is rendered , or how you interact with that content . - This happens regardless of platform iOS , Android , BlackBerry , etc - All have quirks. - Here are a few tips that I have found useful for improving overall interaction and mobile HTML- experiences . <p> Disclaimer : I 've been targeting iOS and Android primarily , with BlackBerry support on some applications . - I do n't  have a Windows Phone device to test with , so I cant comment on support for the Windows platform . <h> AutoCorrect and AutoCapitalize <p> First @ @ @ @ @ @ @ @ @ @ sometimes drive you to the brink of insanity . - This is especially the case if you have a text input where you are typing in a username , and it keeps " correcting " it for you ( next thing you know , you are locked out of the app ) . - You can disable these features in web experiences by setting the " autocorrect " and " autocapitalize " attributes of an &lt;input&gt; instance . <p> Disabled AutoCorrect : <p> &lt;input type= " text " autocorrect= " off " autocapitalize= " on " /&gt; <p> Disabled AutoCapitalize : <p> &lt;input type= " text " autocorrect= " on " autocapitalize= " off " /&gt; <h> Managing the Keyboard <p> Have you ever experienced an an app or web site on a mobile device where you have to enter numeric data , and the default keyboard pops up . Before entering any text , you have you switch to the numeric input . Repeat that for 100 form inputs , and try to tell me that you are n't  frustrated Luckily , you can manage the keyboard in mobile @ @ @ @ @ @ @ @ @ @ Disable User Selection <p> One way to easily determine that an application is really HTML is that everything on the UI is selectable and can be copied/pasted Every single piece of text , every image , every link , etc Not only is this annoying in some scenarios ( and very useful in others ) , but there may be instances where you explicitly do n't  want the user to be able to easily copy/paste content . You can disable user selection by applying the following CSS styles . Note : This works on iOS , and partially works on BlackBerry/QNX for the PlayBook . It did not work on Android in my testing . <p> The -webkit-touch-callout css rule disables the callout , and the -webkit-user-select rule disables the ability to select content within an element . More details on webkit css rules from the Mobile Safari CSS Reference . More detail about disabling copy/paste on iOS is available at StackOverflow.com . <h> Disable Zoom <p> If you want your content to feel like an app instead of a web page , then I strongly suggest that you disable @ @ @ @ @ @ @ @ @ @ pinch/zoom is not required . The easiest way to do this is to set the viewport size to device-width and and disable user scaling through the HTML metadata tag . <p> This technique works on both Android and iOS devices , and I assume other platforms . However , I do n't  have the devices to test all of them . <h> Touch Based Scrolling <p> Touch-based scrolling is critical to having an application that feels native . I do n't  mean that the whole page should be able to scroll Your browser will be able to take care of that alone . Instead I mean that you should be able to scroll individual elements so that they mimic clipped views , lists , or large blocks of content . You should be able to scroll content where it is , and not have to scroll an entire page to reveal something in only one area of the screen . You should minimize scrolling when it may cause poor UX scenarios . This is especially the case in tablet-based applications which have a larger UI than phone-based applications . <h> @ @ @ @ @ @ @ @ @ @ on HTML elements on mobile devices generally have a delay that is caused by the operating system logic used to capture gestural input based on touch events . Depending on the device , this could be 300-500 MS . While this does n't  sound like much , it is very noticeable . The workaround is to use touch events instead of mouse events : touchStart , touchMove , touchEnd . You can learn more about touch events from html5rocks.com . There 's also a great script from cubiq that adds touch events for you to optimize the experience for onClick event handlers on iOS devices . <h> Add To Home Screen <p> If you want your web app to fee like a real app and take up the full screen without using PhoneGap as an application container , then you can always add it to the devices home screen . Although this can only be done manually through the mobile browser , there are a few open source scripts to guide the user through this processs : cubiq.org or mobile-bookmark-bubble should get you started . <h> Use Hardware Acceleration <p> Animations @ @ @ @ @ @ @ @ @ @ hardware accelerated ( and the device supports hardware acceleration ) . You can make html elements hardware accelerated just by adding the translate3d ( x , y , z ) css style to the element ( be sure to set all three x , y , and z attributes otherwise hardware acceleration may not be applied . If you do n't  want any translation changes , you can use the translate3d CSS rule with all zero values : translate3d(0,0,0) . <p> transform : translate3d(0,0,0) ; -webkit-transform : translate3d(0,0,0) ; <p> In your development/testing , you can even visualize which content is hardware accelerated in both desktop and mobile Safari using the technique shown at http : //mir.aculo.us/ . <h> Make You Apps Fast <p> Last , but certainly not least , make your apps fast . Follow best practices , and be efficient in code execution and the loading of assets ( both local and remote ) . Here are a few links to get you going in the right direction : 
@@106848873 @2248873/ <h> Tag Archives : Bluemix <p> I recently put together some content on building " Apps that Work as Well Offline as they do Online " using IBM MobileFirst and Bluemix ( cloud services ) . - There was the original- blog post , I used the content in a presentation at ApacheCon , and now I 've opened everything up for anyone use or learn from . <p> The content now lives on the IBM Bluemix github account , and includes code for the native iOS app , code for the web ( Node.js ) endpoint , a comprehensive script that walks through every step of of the process configuring the application , - and also a video walkthrough of the entire process from backend creation to a complete solution . <p> Key concepts demonstrated in these materials : <p> User authentication using the Bluemix Advanced Mobile Access service <p> Remote app logging and instrumentation using the Bluemix Advanced Mobile Access service <p> Client-side Objective-C code ( you can do this in either hybrid or other native platforms too , but I just wrote it for iOS ) . @ @ @ @ @ @ @ @ @ @ for a complete sample application leveraging this workflow . The " GeoPix-complete " folder contains a completed project ( still needs you to walk through backend configuration ) . The " GeoPix-starter " folder contains a starter application , with all MobileFirst/Bluemix code commented out . You can follow the steps inside of the " Step By Step Instructions.pdf " file to setup the backend infrastructure on Bluemix , and setup all code within the " GeoPix-starter " project . <p> Earlier this week I had the privilege of speaking at ApacheCon- in Austin , TX on the topic of data management for apps that work as well offline as they do online . - This is an important topic for mobile apps , since , as we all painfully know already , there is never a case when you are always online on your mobile devices . - There always ends up being a time when you need your device/app , but you cant get online to get the information you need . - Well , this does n't  always have to be the case . There are strategies @ @ @ @ @ @ @ @ @ @ well offline as they do online , and the strategy- I 'd like to highlight today is based upon data management using the IBM Cloudant NoSQL database as a service , which is based upon Apache CouchDB . <p> here 's a link to the presentation slides ( built using reveal.js ) just use the space bar to advance the presentation slides : <p> The " couch " in CouchDB is actually an acronym for Cluster of Unreliable Commodity Hardware . At the core of this cluster is the concept of replication , which in the most basic of terms means that - data is shared between multiple sources . - Replication is used to share information between nodes of the cluster , which provides for cluster reliability and fault tolerance . <p> Cloudant is a- clustered NoSQL- database services that provides an extremely powerful and searchable data store . - It is- designed to power- the web and mobile apps , and all information is exposed via REST services . Since the IBM Cloudant service is based on CouchDB ( and not so coincidentally , IBM is a major contributor to @ @ @ @ @ @ @ @ @ @ the Cloudant service . <p> With replication , you only have to write your data/changes to a single node in the cluster , and replication takes care of propagating- these changes across the cluster . <p> If you are building apps for the web or mobile , there are options to extend the data replication locally either on the device or in the browser. - This means that you can have a local data store that automatically pushes and/or pulls data from the remote store using replication , and it can be done either via native languages , or using JavaScript . <p> If you want to have local replication in either a web or hybrid ( Cordova/PhoneGap ) app , you can use PouchDB. - PouchDB- is a local JavaScript database modeled- after CouchDB and implements that CouchDB replication API. - So , you can store your data in the browsers local storage , and those changes will automatically be replicated to the remote Cloudant store . - This works in the browser , in a hybrid ( web view ) app , or even inside of a Node.js @ @ @ @ @ @ @ @ @ @ leverage the HTML5 cache to have your app cached locally . <p> If you are building a native app , do n't  worry , you can take advantage of the Cloudant Sync API to leverage- the local data store with replication . - This is available for iOS and- Android , and implements the CouchDB replication API . <p> The sample app that I showed in the presentation is a native iOS application based on the GeoPix MobileFirst sample app- that I detailed in a previous post . - The difference is that in this case I showed it using the Cloudant Sync API , instead of the MobileFirst data wrapper classes , even though it was pointing at the exact same Cloudant database instance . - You can see a video of the app in action below . <p> All that you have to do is create a local data store instance , and then use replication to synchronize data between the local store and a remote store . <p> Replication be either one-way ( push or pull ) , or two-way . - So , any changes between @ @ @ @ @ @ @ @ @ @ . - Essentially , the local data store just becomes a node in the cluster . - This provides complete access to the local data , even if there is no network available . - Just save your data to the local store , and replication takes care of the rest . <p> In the native Objective-C code , you just need to setup the CDTDatastore manager , and initialize your datastore instance . <p> Once your datastore is created , you can read/write/modify any data in the local store . - In this case I am creating a generic data object ( basically - like a JSON object ) , and creating a document containing this data . - A document is a record within the data store . <p> You can add attachments to the document or modify the document as your app needs . - In the code below , I add a JPG atttachment to the document . <p> Replication is a fire-and-forget process . - You simply need to initialize the replication process , and any changes to the local data store will be replicated @ @ @ @ @ @ @ @ @ @ . <p> By assigning a replicator delegate class ( as shown above ) , your app can monitor and respond to changes in replication state . - For example , you can update status if replication is in progress , complete , or if an error condition was encountered . <p> If you want to access data from the local store , it is always available within the app , regardless of whether or not the device has an active internet connection . - For example , this method will return all documents within the local data store . <p> Last week I gave a presentation to the NYC Bluemix Meetup Group on IBM MobileFirst for Bluemix . Not familiar with the branding and have no idea what that means ? - It is a mobile backend as a service , which gives you analytics , remote logging , user auth , data persistence &amp; offline synch , push notification management , and more for your mobile applications . - Yes , as a service you can create a Bluemix account today for free and start building your apps very @ @ @ @ @ @ @ @ @ @ werent able to make it to the meetup. - I recorded my session which you can check out in the embedded video below . <p> I know the video quality is n't fantastic , but its the best I had at the time . - ( I almost always have a GoPro with me. ) - If you want to see the code that makes all of this work in much , much more detail , check out my post on Getting Started with Bluemix Mobile Services it has code , video tutorials and more . - Enjoy ! <p> This post specifically covers- native iOS , though there are also Android and hybrid options available . This should have everything you need to get started . It covers all aspects- from creating the app , to updating the back end , to leveraging Cloudant storage , push notifications , and monitoring &amp; logging . <p> So , without further ado , let 's get started <h> Part 1 : Getting Started with Bluemix Mobile Services <p> In this first video I show how to create a new mobile app on Bluemix @ @ @ @ @ @ @ @ @ @ remote logging from the client application . This process is covered in more detail in the Getting Started docs , but below- are the basics from my experience . <p> Youll- first need to- sign into your Bluemix account . If you do n't  already have one , you can create a trial account- for free . Once you 're signed in , you just need to create a new mobile app instance . <p> The process is very simple , and there is a " wizard " to guide you . The first thing that you need to do is create a new app by clicking the big " Create an App " button on your bluemix dashboard . <p> Create a new app from IBM Bluemix Dashboard <p> Next , select which kind of app you 're going to create . For MBaaS , you 'll want to select the " Mobile " option . <p> Select the type of app <p> Next you 'll need to select your platform target . You can choose either " iOS , Android , Hybrid " , or the " iOS 8 beta " target @ @ @ @ @ @ @ @ @ @ , but the process is similar for both targets. - Hybrid apps are built leveraging the Apache Cordova- container . <p> Select your platform target <p> Next , just specify an app name and click " Finish " . <p> Give your app a name <p> Once your app is created , you will be presented with instructions how to connect the app in Xcode . I 'll get to that in a moment <p> Now that- your app has been created , you 'll be able to see it on your Bluemix dashboard . This app will consist of several components : a Node.js back-end instance , a Cloudant NoSQL database instance , an Advanced Mobile Access instance , and a Push instance . The Advanced Mobile Access component provides you with app analytics , user auth management , remote logging , and more . The Push component gives you the ability to manage and send push notifications ( either manually , or with a rest-based API ) . <p> You app has been created here are the components and the activity <p> Once your app has been created , you @ @ @ @ @ @ @ @ @ @ Bluemix to consume the services . Again , this is a very straightforward process . <p> The next step is to register your client application . Once your app is created , you will be presented with a screen to do this . If you do n't  complete it right away , you can always come back later and register an application . You 'll need to specify the Bundle I 'd and version of your app , then you can setup any authentication ( if you choose ) . <p> Register your apps bundle I 'd and version <p> Once your app has been registered , you need to configure Xcode . Youll first need to create a new project in Xcode . There are two options for configuring your Xcode project : 1 ) automated installation using CocoaPods , or 2 ) manual installation . I used the CocoaPods installation simply because it is easier and manages dependencies for you . <p> If you are n't  familiar with CocoaPods , it is much like NPM CocoaPods is a dependency manager for Cocoa projects . It helps you configure- the Bluemix libraries @ @ @ @ @ @ @ @ @ @ Xcode up and running , then close it and install CocoaPods , if you do n't  already have it . Next open up a terminal/command prompt , go to the directory that contains your Xcode project and initialize CocoaPods- using the " setup " - command : <p> pod setup <p> This will create a new file called " podfile " . Open this file in any text editor and paste the following ( note : you can remove any lines that you do n't  want to actually use ) : <p> source ' https : **32;5779;TOOLONG ' # Copy the following list as is and # remove the dependencies you do not need pod ' IMFCore ' pod ' IMFGoogleAuthentication ' pod ' **25;5813;TOOLONG ' pod ' IMFURLProtocol ' pod ' IMFPush ' pod ' CloudantToolkit ' <p> Save the changes to the " podfile " file , and close the text editor . Then go back to your command promprt/terminal- - and run the installation process : <p> pod install <p> Your project will be configured , and all dependencies will be downloaded automatically . Once this is @ @ @ @ @ @ @ @ @ @ ( Xcode Workspace ) . <p> You have to initialize the Bluemix inside of your application to connect to the cloud service to be able to take advantage of any Bluemix features ( logging , data access , auth , etc ) . The best place to put this is inside of your AppDelegate.m- class- application **29;5840;TOOLONG method because it is the first code that will be run within your application : <p> One of the first features I wanted to take advantage of was remote collection of client-side logs . You can do this using the IMFLogger class , in much the same fashion as you do with OCLogger in MobileFirst Foundation server . Once great feature that requires almost no- additional configuration is the- **25;5871;TOOLONG method , which automatically configures the Advanced Mobile Access component to collect information for all app crashes . <p> Next , launch your app in the iOS simulator , or on a device , and you 'll see everything come together . Log into your Bluemix dashboard , and you 'll be able to monitor app analytics and remote logs . <p> Note : If @ @ @ @ @ @ @ @ @ @ from within the iOS simulator , just clear the iOS Simulator by going to the menu command " iOS Simulator -&gt; Reset Content and Settings " , and everything should connect properly the next time you launch the app . <h> Part 2 : Configuring the Node.js Backend <p> In the next video , I demonstrate how to- grab the code for the backend Node.js application , create a git repository on IBM JazzHub , then pull the code for local development . <p> When the app is created , you 'll see an " add git " link under the app name . Using this link , you can create a git repository for the backend code . <p> Add a git repository <p> Once your git repo has been created , you can check out the code using any Git client ( I used the CLI ) . You 'll need to use the " npm install " command to pull down all the app dependencies . The biggest thing you need to know is that it uses express.js for the web application framework. - - You can make any @ @ @ @ @ @ @ @ @ @ deployed to your Bluemix server instance upon commit . Yes , this workflow is also configurable b/c this process may not work for everyone . <p> One other thing that you will need to watch out for if you are doing local development : You will want to wrap the following code on line 6 in a try/catch block , otherwise you will hit errors in the local environment which will prevent your app from launching locally : <h> Part 3 : Consuming Data from Cloudant <p> Another part of Bluemix mobile applications is the Cloudant NoSQL database . The Cloudant NoSQL database is a powerful solution that gives you remote storage , querrying , and client-side- data storage mechanisms with automatic online/offline synchronization , all with monitoring/analytics capabilities . <p> By default , objects within the Cloudant data store are treated as generic objects ( over-simplification : think of it is an extremely powerful JSON store in the cloud ) . However you can also serialize your objects to strong data types within the client code configuration . <p> In your AppDelegate class- application **29;5898;TOOLONG method , you 'll also @ @ @ @ @ @ @ @ @ @ class used for interacting with all Cloudant data operations . <p> IMFDataManager *manager = IMFDataManager sharedInstance ; <p> In my sample , I setup the database manually with open permissions , but you 'll probably want something more secure . Once your database is created , you can create indexes , search for data , create data , etc <p> In the following code , I create a search index and query for data from the remote Cloudant database . You really only need to create the index if it does n't  already exist . You can do this either through the mobile app code , or manually through the Cloudant databases web interface . I did this inline in the following code , just for the sake of simplicity : <h> Part 4 : Push Notifications <p> The IBM Bluemix mobile services app also contains a component for managing push notifications within your mobile applications . With this service , you can send push notifications to a specific device , a group of devices using tags , or all devices , and you can send push notifications either manually via @ @ @ @ @ @ @ @ @ @ workflow using the REST API . <h> Part 5 : Monitoring and Logging <p> Did I- mention that every action that you perform through Bluemix Mobile Services can be monitored ? Analytics are available for the Advanced Mobile Access component , the Cloudant NoSQL data store , and the Push Notifications service . In addition , you also have remote collection of client logs and crash reports . This provides- - unparalleled insight into- the health of your applications . <p> Push notifications , love them or hate them , are everywhere and there 's no getting around it . Push notifications are short messages that can be sent to mobile devices regardless of whether the apps are actually running . They can be used to send reminders , drive engagement with the mobile app , notify completion of long running processes , and more . Push notifications- send information to you in real time , rather than you having to request that information . <p> If you are building a back-end infrastructure to manage your applications data , and you want to leverage push notifications , then guess what ? @ @ @ @ @ @ @ @ @ @ and distribution of push notifications for each platform . <p> The- unified- push notification API allows you to develop your app against a single API , yet deliver push notifications to multiple platforms , and it works with both hybrid ( HTML/CSS/JS ) apps , as well as native apps . <p> You will still have to build the logic to subscribe devices for messaging , and dispatch push notification messages , but you 'll only have to do it once against the unified API not once for each platform . <p> The apps that I showed in the video above are sample apps taken straight from the IBM MobileFirst platform developer guide for iOS and Android , and can be accessed in their entirety ( with both client and server code ) using the links below : <p> On the client app , you 'll need to subscribe for messages from the event source . See the hybrid or native code- linked to above for syntax and examples . <p> Once your clients are subscribed , you can use a single server-side implementation to distribute messages to client apps . Below @ @ @ @ @ @ @ @ @ @ a push notification to all devices for a particular user ( on any platform ) : <p> From- the MobileFirst console , you will be able to monitor and manage event sources , platforms , and the devices that are consuming push notifications . <p> Push Notifications on the MobileFirst Console <p> If you were wondering , yes , these can be cloud-hosted on IBM BlueMix and yes , it can also be installed on-premise on your own server in your data center . - You have the option to configure- your physical or cloud servers however you want . 
@@106848875 @2248875/ <h> Category Archives : Creative Cloud <p> I love my GoPro camera . - It takes amazing pictures and captures incredible videos , and can get into some extreme situations that other cameras probably would not survive no wonder it is one of the best selling cameras in the world . - I also love the fisheye lens , but there are times when the fisheye effect is too much . We 've had lens correction in Photoshop and Lightroom for a while , optics compensation in After Effects , but now it is easier than ever to non-destructively remove the fisheye effect from GoPro video footage directly inside of Adobe Premiere Pro . - Check out the video below to see it in action . <p> Applying lens correction ( or lens distortion removal ) is incredibly easy . - There are new effects presets in the effects panel that enable video editors to simply drag an effect onto their clip to have the lens correction applied . - Just select the preset for the resolution and field of view ( FOV ) that match what you used to @ @ @ @ @ @ @ @ @ @ clip . - They under Presets -&gt; Lens Distortion Removal -&gt; GoPro. - For those fellow quadcopter enthusiasts , you may also notice some presets for the DJI Vision cameras ! <p> GoPro Lens Distortion Presets <p> Once you 've applied the preset to your footage , you can tweak it as you like to customize the amount of correction . - You can under-correct , over-correct , or change the center/focal point of the correction . - I normally tend to leave it with the default settings <p> GoPro Lens Distortion Effect Controls <p> Once you 've applied the correct preset for your footage , you 'll be able to see that the lens distortion has been removed . - The straight lines will now appear straight , and everything will line up to scale . <p> My second article on aerial imaging with a remote controlled helicopter is now live in the March 2014 issue of Adobe Inspire ! - The first article focused on aerial videography and Adobe video tools . This time its all about aerial photography with a GoPro camera and DJI Phantom ( and how to bring @ @ @ @ @ @ @ @ @ @ <p> First , you can make so-so video look great with a few simple color correction techniques . Second , a video is only as good as its audio , so you need solid audio to keep viewers engaged . - Hopefully this post helps you improve your videos with simple steps on both of these topics . <p> To give you an idea what I 'm talking about , check out this before and after video . Its the exact same clip played twice . - The first run through is just the raw video straight from the camera and mic. - Colors do n't  " pop " , its a little grainy , and the audio is very quiet . - The second run through has color correction applied to enhance the visuals , and also has processed audio to enhance tone , increase volume , and clean up artifacts . <p> Let 's first look at color correction . - Below you can see a " before " and " after " still showing the effects of color correction . - The background is darker and has less grain @ @ @ @ @ @ @ @ @ @ warmer . <p> Before and After Color Correction <p> The visual treatment was achieved using two simple effects in Adobe Premiere Pro . - First I used the Fast Color Corrector to adjust the input levels . - By bringing up the black and gray input levels , the background became darker , and it reduced grain in the darker areas . - Then , I applied the " Warm Overall " Lumetri- effect to make the video feel warmer this enhances the reds to add warmth to the image . <p> You can get by with a mediocre video with good audio , but nobody wants to sit through a nice looking video with terrible audio . Here are three simple tips for Adobe Audition to help improve your audio , and hopefully keep viewers engaged . <p> In this case , I thought the audio was too quiet and could be difficult to understand . - My goal was to enhance audio volume and dynamics to make this easier to hear . <p> I first used Dynamics Processing to create a noise gate . This process removes quiet @ @ @ @ @ @ @ @ @ @ sounds , and generally cleaner audio . - You could also use Noise Reduction or the Sound Remover effects the effect that works best will depend on your audio source . <p> Dynamics Processing ( Noise Gate ) in Adobe Audition <p> Next I used the 10-band graphic equalizer to enhance sounds in specific frequency ranges . - I brought up mid-range sounds to give more depth to the audio track . <p> 10 Band EQ in Adobe Audition <p> Finally , I used the Multiband Compressor to enhance the dynamic range of the audio . - Quieter sounds were brought up and louder sounds were brought down to create more level audio that is easier to hear and understand . - However , be careful not to make your audio too loud when using the compressor ! - If you 've ever been watching TV and the advertisements practically blow out your eardrums , this is because of overly compressed audio . <p> Multi-band Compressor in Adobe Audition <p> Want to learn more ? - Do n't  miss the Creative Cloud Learn resources to learn more about all of the @ @ @ @ @ @ @ @ @ @ ! - If you are n't  already a member , join Creative Cloud today to access all Adobe media production tools . <p> What makes a great composition in Photoshop ? Well , that all depends on what you 're trying to achieve and personal taste I wont even attempt to answer that . <p> What makes a composition believable ? That one is a little bit easier Weve all seen bad Photoshop jobs you know , those where colors are way off , lighting is terrible , or edges are left jagged and pixellated . For the most part , I 'd attribute a believable Photoshop composition to having qualities that mimic realism . Consistent use of colors , none of those jagged edges , appropriate use of shadows and lighting , proper perspectives , and most importantly , attention to detail . <p> Not quite sure what I mean ? - Check out these examples of some inspiring and impressive Photoshop compositions 
@@106848876 @2248876/ <p> Node.js is an incredible tool for rapidly building highly performant and scalable back end systems , and you develop it using a familiar core language that most front-end developers are already accustomed to , JavaScript. - This acquisition is positioned to greatly enhance Node.js in the enterprise , and StrongLoops offerings will be integrated into IBM Bluemix , IBM MobileFirst , and WebSphere . <p> Even though the acquisition is still " hot off of the presses " , - you can start using these tools together- today : <p> If you have n't heard about StrongLoop 's LoopBack framework , it enables you to easily connect and expose your data as REST services . It provides the ability to visually create data models in a graphical ( or command line ) interface , which are used to automatically generate REST APIs " thus generating CRUD operations for your REST services tier , without having to write any code . <p> Why is this important ? <p> It makes API development easier and drastically reduces time from concept to implementation . - If you have n't yet looked at the @ @ @ @ @ @ @ @ @ @ - You can build API layers for your apps literally in minutes . - Check out the video below for a quick introduction : <p> Again , be sure to check out these posts that detail the integration steps so you can start using these tools together today : 
@@106848877 @2248877/ <p> While working with Kevins code , I started tinkering " what if I change this , what if I tweak that ? " Next thing you know , I put together a sample scenario showing subscription-based realtime data streaming to multiple web clients using web sockets . Check out the video below to see it in action . <p> You are seeing 9 separate browser instances getting realtime push-based updates from a local server using web sockets . When the browser loads , the html-based client makes a web socket connection , then requests all symbols from the server . The server then sends the stock symbol definitions back to the client and displays them within the HTML user interface . From there , the user can click on a stock symbol to subscribe to updates for that particular symbol . DISCLAIMER : All that data is randomly generated ! <p> I put together this example for experimentation , but also to highlight a few technical scenarios for HTML-based applications . Specifically : <p> Realtime/push data in HTML-based apps <p> Per-client subscriptions for realtime data <p> Multi-series realtime @ @ @ @ @ @ @ @ @ @ AIR app started by Kevin , based on the web sockets draft protocol . It is written in JavaScript , and the client is a HTML page to be viewed in the browser . <p> If you do n't  feel like reading the full web sockets protocol reference , you can get a great overview from websocket.org or Wikipedia . <p> One thing to keep in mind is that web sockets are not widely supported in all browsers yet . There is a great reference matrix for web socket support from caniuse.com : <p> If you still are n't  sure if your browser supports web sockets , you can also check simply by visiting websocketstest.com/ . If you want to test for web socket support within your own applications , you can easily check for support using Modernizr . Note : I did n't  add the Modernizr test in this example I only tested in Chrome on OSX . <p> You 'll know the server is started b/c an air window will popup ( you can ignore this , just do n't  close it ) , and you will start seeing @ @ @ @ @ @ @ @ @ @ server is running , open " client/client.html " in your browser . It will connect to the local server , and then request the list of symbols . If you click on a symbol , it will subscribe to that feed . Just click on the symbol name again to unsubscribe . You 'll know the feed is subscribed b/c the symbol will show up in a color ( matching the corresponding feed on the chart ) . Again , let me reiterate that I only tested this in Chrome . <p> You can open up numerous client instances , and all will receive the same updates in real time for each subscribed stock symbol . <p> The " meat " of code for the server starts in **31;5929;TOOLONG . Basically , the server loads a configuration file for the socket server , then creates a ConnectionManager and DataFeed ( both of these are custom JS classes ) . The ConnectionManager class encapsulates all logic around socket connections . This includes managing the ServerSocket as well as all client socket instances and events . The DataFeed class handles data within the @ @ @ @ @ @ @ @ @ @ sets up an interval to generate random data updates . For every data update , the ConnectionManager instances dispatch() method is invoked to send updates to all subscribed clients . Rather than trying to put lots of code snippets inline in this post ( which would just be more confusing ) , check out the full source at : - LONG ... <p> The client code all starts in client.html , with the application logic inside of client/scripts/client.js . Once the client interface loads , it connects to the web socket and adds the appropriate event handlers . Once subscribed to a data feed , realtime data will be returned via the web socket instance , transformed slightly to fit the data visualization structure , then rendered in an HTML canvas using the RGraph data visualization library . RGraph is free to get started with , however if you want to deploy a production app with it , you 'll need a license . You 'll notice that each feed updates independently , based upon the client subscriptions . Note : The data visualization is not temporally aligned if you want the @ @ @ @ @ @ @ @ @ @ work involved in the client-side data transformation . <p> This example is intended to get your minds rolling with the concepts ; it is not *yet* an all-encompassing enterprise solution . You can expect to see a few more data push scenarios here in the near future , based on different enterprise server technologies . <p> In case you had not seen on the Flex team blog , twitter or through some other medium , Flex SDK 4.6 and Flash Builder 4.6 were released today ! - Go get them , if you have not done so already . - Flex 4.6 marks a huge advancement for the Flex SDK , especially regarding mobile applications . <p> Flash Builder 4.6 is a FREE update for Flash Builder 4.5 users . From the Flex team blog : <p> A lot is included in this update , so much so that we could n't deliver it in the Adobe Application Manager . This means Flash Builder 4.5 users wo n't automatically be notified about the update and will have to download the full Flash Builder 4.6 installer and enter their Flash Builder @ @ @ @ @ @ @ @ @ @ Flash Builder releases , new content around Flex and Flash Builder 4.6 have been posted on Adobe TV. - There is a bunch of great new content worth checking out , including fellow evangelist Michael Chaizes adaptive UI for different platforms and device form factors . - In addition , here I am speaking out the new Captive Runtime feature introduced in AIR 3 <p> I 've finally caught up with some of my to-dos and have uploaded code for several of my recent projects to github . You can see a list of all of my projects at https : //github.com/triceam . As I blog , I 'll attempt to maintain a copy of all source code on github . As I get a chance , I 'll try to post some additional samples that I have sitting around . So far , I 've uploaded the following : <h> URL Monitor <p> This includes my URL Monitor application , available for iOS , Android , and BlackBerry , developed using Adobe AIR . The URL Monitor tool is a simple diagnostic application that will allow you to quickly and easily monitor the @ @ @ @ @ @ @ @ @ @ into the text box and add it to the list . A polling HTTP request will be made every 10 seconds to determine the availability of a given endpoint . HTTP codes 200 , 202 , 204 , 205 and 206 will be identified as a success with a green check . All other HTTP codes will indicate a problem as a red X. <h> Mobile Serialization Tester <p> The Flex Mobile Serialization Testing application is a basic scenario for testing performance between AMF and JSON in a Flex application . The mobile app makes requests of simple data objects from a ColdFusion CFC . In each test iteration , a request is made for 1 , 10 , 100 , 1000 , and 10000 value objects , in both AMF and JSON formats . The total round trip time from request to deserialization is measured and compared for each case , for a total of 5 iterations through each cycle . The application displays end-to-end performance metrics for both AMF and JSON requests . <h> Enterprise Tablet Visualization <p> The Enterprise Tablet Visualization application is a sample application built @ @ @ @ @ @ @ @ @ @ a production application . It demonstrates realtime data push to a mobile application using LiveCycle Data Services , realtime multimedia collaboration using LiveCycle Collaboration Services , as well as multi-form-factor UI for both tablet and phone devices , including map integration and interactive data visualizations . <p> Adobe Flash Player on desktop Adobe reaffirmed its commitment to the Adobe Flash Player in desktop browsers , and its role of enabling functionality on the web that is not otherwise possible . Flash Player 11 for PC browsers just introduced dozens of new features , including hardware accelerated 3D graphics for console-quality gaming and premium HD video with content protection . <p> <p> Adobe AIR for mobile Adobe reaffirmed its commitment to Adobe AIR for mobile devices , which allows developers and designers to create standalone applications using Adobe Flash technologies that can be deployed across mobile operating systems , including Apple iOS , Google Android and RIM BlackBerry Tablet OS . <p> Adobe AIR for desktop Adobe reconfirmed its commitment for its continued support for Adobe AIR applications running on the desktop . Adobe is actively working on the next version @ @ @ @ @ @ @ @ @ @ FlexAdobe announced its intention to contribute the Adobe Flex SDK open source project to the Apache Software Foundation for future governance . <p> Last week Adobe announced information about the companys evolution and future plans of Flex . It was also announced that Adobe Flex would be contributed to an open source software foundation . The result of which , was mass speculation , fear , uncertainty , and doubt . - Rest- assured , Flash is not dead , nor is Flex . <p> Falcon , the next-generation MXML and ActionScript compiler that is currently under development ( this will be contributed when complete in 2012 ) <p> Falcon JS , an experimental cross-compiler from MXML and ActionScript to HTML and JavaScript . <p> Flex testing tools , as used previously by Adobe , so as to ensure successful continued development of Flex with high quality <p> Is n't Adobe just abandoning Flex SDK and putting it out to Apache to die ? - <p> Absolutely not " we are incredibly proud of what we 've achieved with Flex and know that it will continue to provide significant value @ @ @ @ @ @ @ @ @ @ on-going contributions from the Apache community . To be clear , Adobe plans on steadily contributing to the projects and we are working with the Flex community to make them contributors as well . <p> Flex has been open source since the release of Flex 3 SDK . What 's so different about what you are announcing now ? <p> Since Flex 3 , customers have primarily used the Flex source code to debug underlying issues in the Flex framework , rather than to actively develop new features or fix bugs and contribute them back to the SDK . <p> With Friday 's announcement , Adobe will no longer be the owner of the ongoing roadmap . Instead , the project will be in Apache and governed according to its well-established community rules.In this model , Apache community members will provide project leadership . We expect project management to include both Adobe engineers as well as key community leaders . Together , they will jointly operate in a meritocracy to define new features and enhancements for future versions of the Flex SDK . The Apache model has proven to foster @ @ @ @ @ @ @ @ @ @ for continuous commits from active developers . <p> What guarantees can Adobe make in relation to Flex applications continuing to run on Flash Player and Adobe AIR ? <p> Adobe will continue to support applications built with Flex , as well as all future versions of the SDK running in PC browsers with Adobe Flash Player and as mobile apps with Adobe AIR indefinitely on Apple iOS , Google Android and RIM BlackBerry Tablet OS . <h> Adobe AIR <p> We are continuing to develop Adobe AIR for both the desktop and mobile devices . Indeed , we have seen wide adoption of Adobe AIR for creating mobile applications and there have been a number of blockbuster mobile applications created using Adobe AIR . <h> Flash Player for Desktop Browsers <p> We feel that Flash continues to play a vital role of enabling features and functionality on the web that are not otherwise possible . As such , we have a long term commitment to the Flash Player on desktops , and are actively working on the next Flash Player version . 
@@106848879 @2248879/ <p> I consider this to be a fairly minimalist rig , and everything is very portable , which is great for travel . <h> Video Capture <p> For all of my front-facing videos I am using a Panasonic Lumix LX7 , and for some of my on-device and secondary camera angles , I use a GoPro Hero 3 Black Edition . The LX7 is my go-to camera for both photography and video work . It captures great images , and allows for fully automatic or manual control of the image capture settings . <p> Panasonic Lumix LX7 and GoPro Hero 3 Black <p> When recording videos , I set the LX7 to manual mode so it wont auto-focus or automatically adjust light balance , and I record everything in full HD ( 1080p , 30 FPS ) . On the GoPro , I normally select 1080p at either 30 or 60 FPS ( depending on the situation and lighting ) . If I am outdoors , I 'll have Protune on , if I 'm indoors I usually have Protune off . <p> Camera settings are only part of the whole @ @ @ @ @ @ @ @ @ @ need proper lighting , and a backdrop that is n't distracting . I try to keep this setup very simple : I 'll place the camera on a tripod on the opposite side of my desk so that it faces me directly . Behind me , I 'll have a black muslin backdrop this helps everything else stand out from the background , and it does n't  reflect any light . Simple tip : Use a clothes steamer on the muslin backdrop , and the wrinkles will fall out pretty quickly . <p> Office/Studio Lighting and Backdrop <p> For lighting , I have darkening blinds that block out nearly all outside light this way you can control the lighting for your video shoots . Often shooting a video may be done over several days , and I cant rely on the weather and natural light to be consistent . With the room darkened , I normally use a single light source above and slightly off to the side of the camera . I try to find an angle that lights me up from the front , but does not reflect in my glasses @ @ @ @ @ @ @ @ @ @ so its not reflecting in my glasses , or dramatically altering the the lighting . <p> Once the lighting has been set , I set up the camera and adjust zoom , focus , and aperture ( exposure ) where I want it for the current video . <p> Framing The Shot <h> Audio Capture <p> Great video is only half of the equation Without clear audio , the videos are n't  nearly as good , and nobody wants to listen to bad audio . I started off using the built-in mics on my cameras , but quickly learned that the internal microphones werent going to cut it . For all of my recent videos Iver started using a Zoom H4N digital audio recorder . The Zoom enables high quality stereo recording . Its very easy to use , and the recording quality is fantastic now if only I could get those birds to stop chirping outside of my window . <p> Zoom H4N Digital Audio Recorder <h> Editing &amp; Post-Production <p> Capturing content is the first part of the process . The second part is editing everything together . @ @ @ @ @ @ @ @ @ @ of the creative tools Adobe has to offer . Most of my video editing is done with- Premiere Pro . This includes clipping &amp; sequencing , color correction , effects , etc All of my audio production is done with Adobe Audition- this includes sound cleanup , and mastering . For graphics , I use Photoshop and Illustrator , depending on the format and content . If you want to insert animations , you can even use Flash Pro as an animation platform , export as video , and pull it into your Premiere project . <p> Video Editing in Adobe Premiere Pro CC <p> Once everything is how I want it , I 'll export to H.264 format ( for the web ) , upload to YouTube , and then start syndicating it however possible/necessary . Normally , its just pulling in a YoutTube- video into a blog post . 
@@106848880 @2248880/ <h> Category Archives : iOS <p> As promised , here are the slides from my presentation " Introduction to Mobile Development with PhoneGap " from the MobileDev@TU meetup last night . Many thanks to Towson University for hosting the event and inviting me to speak ! <p> I was inspired to write this post by several recent conversations . - I was in a debate about whether with the Flex/Flash platform you could easily repurpose content to the desktop using Adobe AIR ( and vice-versa ) , but that you could n't easily do that with PhoneGap applications . ( My stance was that yes , you could repurpose content . ) <p> I wanted to make sure that people were aware that you can repurpose your content , and here 's an example of how . <p> A while back , I wrote a sample PhoneGap application that allows you to- browse information from the 2010 US Census . - You can read more about this application and download the source code here . This application supports lots of platforms iOS , Android , BlackBerry , and web ( everything @ @ @ @ @ @ @ @ @ @ <p> While this application is a mobile app wrapped in the PhoneGap container , I actually did n't  use any PhoneGap-specific- libraries , so it was very easy to repurpose as a desktop application . - I created an AIR version of this application , which you can download at : <p> I had to use my Android 2. x branch of the US Census Browser code because the WebKit instance inside of AIR does n't  support SVG. - I also changed the container scrolling to use normal CSS " overflow : auto " instead of using iScroll for touch-based scrolling . There were a few other one-off CSS changes to tweak the layout in the AIR web container , but otherwise the code is identical . <p> You just need to create an AIR application XML file and point it to your HTML content , and then package it using ADT . <p> If you were using PhoneGap APIs , you would have to migrate your code to take advantage of AIR APIs , but all other HTML/CSS/JS could be reused with minimal changes . <p> Even though I @ @ @ @ @ @ @ @ @ @ game in town for HTML-based desktop applications - Theres an open source project called MacGap , you can use HTA for Windows , and its not hard to write a HTML/Web View wrapper for any platform . Its even been reported that you are going to be able to write apps for Windows 8 purely using HTML &amp; JS , and you would be able to repurpose your code for this as well . <p> If you are manually building PhoneGap projects across multiple platforms , managing source files can sometimes become a little bit tricky . If you are building for Android , you need a project within Eclipse . If you are building for iOS , you need a project within Xcode . If you are building for both , you need to make sure that the code in the Eclipse project is in synch with the code in the Xcode project so that the platform-specific apps are in parity with each other . <p> Keeping the project source code in synch can be achieved using manual copy/paste between projects , but this is messy and error prone @ @ @ @ @ @ @ @ @ @ other scripting language , but this requires an additional script and/or step in your build process . Although scripting is a reliable process , sometimes you just do n't  need the script . <p> If you do n't  want to manually synch things , and you do n't  want a script , you can use a symlink to link directory paths . Basically , create a " src " folder outside of each project , and create a symlink reference to the src folder inside of the " www " folder for each project . - Symlinks allow a logical directory mapping , which actually points to another location on the hard disk . <p> From the command line , you just use the following command : <p> ln -s source target <p> To setup your project , first create your directory structure . - I created a parent folder for the project . Inside of that folder , I created a " project-ios " folder , " project-android " folder , and " src " folder . - The " src " folder will contain the shared HTML/JavaScript for @ @ @ @ @ @ @ @ @ @ contain the Xcode project , and the " project-android " folder will contain the Eclipse project . <p> Project Structure <p> Next , create the actual iOS and Android projects inside of these folders , following the normal setup instructions : <p> Once you have set up both projects , you 'll need to configure the symlinks. - Put a copy of the " index.html " file into your " src " directory . - Next , go to the " www " directory for each project and delete the " index.html " file to remove any ambiguity or chance for error . <p> However , DO NOT DELETE THE PHONEGAP.js files ! <p> The phonegap.1.4.1.js files are platform specific . - The Android version will not work with iOS , and the iOS version will not work with Android . <p> Next , navigate to your root folder that contains the " src " , " project-iOS " , and " project-Android " folders . Here you will- create the actual symlink references . - When doing so , be sure to use the full path to the source and @ @ @ @ @ @ @ @ @ @ symlink reference from the " src " directory to " project-ios/www/src " , and a- symlink reference from the " src " directory to " **30;5962;TOOLONG " . <p> If you try to use a relative path from your current location , it will give you errors and a massive headache . - - You can use " pwd " to get a reference to the full path of your current directory . <p> Here are the commands that I used on my system , where the root directory is LONG ... <p> In the iOS project , open AppDelegate.m . You 'll also need to update it to reference the index.html file inside of " src " . You 'll just need to edit the start page to " src/index.html " inside of the function : ( BOOL ) application : ( UIApplication* ) application LONG ... <p> Also , be sure to update the link inside of " src/index.html " to point to the project-specific PhoneGap JavaScript files in the parent directory " .. /phonegap-1.4.1.js " ( they are not inside of the linked folder ) : <p> While looking @ @ @ @ @ @ @ @ @ @ to see a lot of search phrases similar to " what is phonegap ? " , " how does a phonegap app look ? " , " how to get started in phonegap ? " , among many , many others . - In this post , I hope to she 'd some light on some basic questions to help you understand and start working with PhoneGap . <p> In case you do n't  feel like reading the whole thing , here are quicklinks to each question : <h> What is PhoneGap ? <p> PhoneGap is an application framework that enables you to build natively installed applications using HTML and JavaScript. - The easiest way to think of PhoneGap is a web view container that is 100% width and 100% height , with a JavaScript programming interface that allows you to access underlying operating system features . - You build your user interface using traditional web development skills ( HTML , CSS , &amp; JavaScript ) , and use the PhoneGap container to deploy to different application ecosystems and devices . - When packaged for deployment , the PhoneGap application is @ @ @ @ @ @ @ @ @ @ " normal " application marketplaces ( iTunes , Google App Market , Amazon Market , etc ) . <h> How does a PhoneGap application typically look ? <p> Since the UI rendering engine is the mobile devices web browser , PhoneGap applications can literally look like anything . - You can use standard HTML &amp; CSS to make it look like a normal web page , you can use a UI framework like jQuery UI , Kendo UI , Sencha , - Twitter Bootstrap , or Skeleton- ( or any other HTML/CSS/JS user interface framework ) . You can also use CSS styles/themes to make your web content look like native apps , such as- iUI- to mimic iOS or Android , or- bbUI - to mimic BlackBerry . <p> PhoneGap applications can have static UIs based on normal HTML , or can have dynamic &amp; interactive experiences developed using JavaScript. - It depends upon the specific application , user experience design , target audience , and use cases to dictate how a PhoneGap application will appear . <p> PhoneGap applications can use pinch/zoom gestures to zoom in &amp; out @ @ @ @ @ @ @ @ @ @ viewport metadata tag . - You can have the page scroll using normal browser behaviors , or you can use a library like iScroll to enable touch-based scrolling of specific container elements . <p> There really are lots of ways to create a user interface with HTML , CSS &amp; JavaScript , so there really is n't any " typical " look . - If you do not apply any CSS styles at all , then all user interface elements will use the operating system/browser default for that specific platform . - This includes buttons , links , and color/highlight states . - This behaves in the exact same manner as the operating systems default web browser . <h> How do I get started in PhoneGap ? <p> Getting started in PhoneGap is easy . - For 90% of a PhoneGap application , all you need is a text editor . - PhoneGap also integrates with device-specific development environments very easily . - You can view " getting started " guides for all application platforms at the links below : <p> When developing PhoneGap applications , just keep in mind that @ @ @ @ @ @ @ @ @ @ . - You develop your applications using HTML and JavaScript , not native code , so you do n't  need anything special . - In fact , I personally do most of my development on the desktop using an HTML editor and the Chrome browser. - When I need device-specific functionality , or I need to test on a device , then I switch over the the device-specific environments . <h> How do you debug PhoneGap applications ? <p> Debugging PhoneGap applications can sometimes be the trickiest part of development . - If you are testing on a physical device , you cant always get access to JavaScript exceptions when they happen . - There are a few strategies for debugging PhoneGap applications . <h> Develop as much as possible on the desktop browser <p> Since PhoneGap applications are written with HTML , CSS , and JavaScript , you can develop most of them using any HTML editor and debug them within a desktop web browser. - The latest versions of all major web browsers ( including Chrome , IE , Firefox , Opera and Safari ) provide rich debugging @ @ @ @ @ @ @ @ @ @ you can inspect HTML DOM elements , inspect CSS styles , set breakpoints in JavaScript , and introspect into memory &amp; JavaScript variables . - You can learn more about the desktop browser development tools at : <p> Once you build the main aspects of your application using desktop tools , you can switch over to a device-specific environment to add device-specific behavior and integrate with PhoneGap APIs . <p> It is imperative that you test your applications on actual devices ! - Actual devices will have different runtime performance than desktop browsers and simulators , and may unearth different bugs/issues including API differences and different UX scenarios . <h> Debug With debug.phonegap.com <p> PhoneGap provides a hosted service that allows you to perform remote , on-device debugging through- debug.phonegap.com. - This uses the Weinre ( Web Inspector Remote ) debugging tool to allow you to remotely inspect the DOM , resource loading , network usage , timeline , and console output . - If you have used any of the developer tools listed above , this will look very familiar . - You will not be able to set @ @ @ @ @ @ @ @ @ @ better than nothing at all . <h> Remote Web Inspector Through iOS 5 <p> There is a little known undocumented API introduced in iOS5 that allows you to perform remote debugging through the iOS5 Simulator. - You just need to enable remote debugging <p> Then launch the application in the desktop iOS Simulator . Once the app is running , open a local Safari instance to : http : //localhost:9999/ . This will launch the remote debugger , complete with breakpoints and script introspection . <h> More Debugging Info <h> How do you architect PhoneGap applications ? <p> You generally architect PhoneGap applications the same way that you create mobile web experiences . The difference is that the initial HTML assets are available locally , instead of on a remote server . - The PhoneGap application loads the initial HTML , which can then request resources from a server , or from the local environment . - Since PhoneGap is based in a browser , it behaves exactly as you would expect a web browser to behave . - You can load multiple pages ; however , keep in mind @ @ @ @ @ @ @ @ @ @ data that is stored in memory via JavaScript. - PhoneGap also supports the single-page web experience model . I strongly suggest using the single-page architecture approach . <h> Single-Page Architecture <p> A single-page architecture refers to the practice of having a single HTML page that dynamically updates based upon data and/or user input . - You can think of this as closer to a true client/server architecture where there is a client application ( written with HTML &amp; JS ) and a separate server structure for serving data . - All client-side application logic resides in JavaScript. - The client application may request data and update its views without reloading the current web page . <p> Using a Single-Page architecture allows you to maintain data in-memory , in JavaScript , which allows you to have a stateful , yet dynamic user interface . - You can read more about single-page architectures at : - LONG ... <h> How do you get PhoneGap apps on devices and into application ecosystems ? <p> PhoneGap applications can be deployed using the same guidelines for native applications for each given platform . - You @ @ @ @ @ @ @ @ @ @ there is no way to get around that . - - You can compile the executables for each platform yourself using each platforms specific build process , or you can use build.phonegap.com to compile them for you . - build.phonegap.com is a hosted service that will compile platform-specific application distributable files for you . - In either case , the output of the build process is a platform-specific binary file : IPA for iOS , APK for Android , etc - You can read more about distributing to various application ecosystems , and each systems signing/certificate requirements at : <h> What is the difference between PhoneGap and AIR ? <p> The most fundamental differences between PhoneGap and AIR is that you develop AIR applications using tools rooted in the Flash Platform ( Flex , Flash , ActionScript , MXML ) , and you develop PhoneGap applications using HTML , CSS , &amp; JavaScript. - AIR applications use the AIR runtime , which allows you to have a single code base , with the exact same expected behavior across all supported platforms . - PhoneGap applications run inside of the native @ @ @ @ @ @ @ @ @ @ this reason , a PhoneGap codebase may behave slightly different between separate platforms , and you will need to account for this during your development efforts . <p> Air applications can be built for iOS , Android , BlackBerry Playbook , and the desktop ( mac and windows ) , with future support for Windows Metro ( Windows 8 mobile interface ) . You can read more about AIRs supported platforms at : - LONG ... <p> ActionScript has strongly-typed objects and supports classical inheritance programming models . AIR applications can also be built using the Flex framework , which allows you to rapidly build enterprise-class applications . - Components in AIR applications are logical objects that have behaviors , properties , and a graphics context . <p> JavaScript-based applications support prototypal inheritance , and have numerous open-source frameworks/tools that can be used . - HTML/JS applications are all visualized through HTML DOM elements . - HTML interfaces can be created through basic string concatenation or JavaScript templating , but in the end you are really just creating DOM elements that have properties and styles . <p> There are some @ @ @ @ @ @ @ @ @ @ however the basic concepts of interactive design and interactive development are identical . - Both platforms have valid strengths , which I could write about ad nauseum I 'll save that for another post . <p> here 's a silly/fun app I built after hours using PhoneGap. - It is a childrens drawing app built entirely with the HTML5 Canvas element , using a PhoneGap wrapper , targeting the iPad. - I was inspired by magnetic drawing toys that I often use when drawing with my daughter , and this was really , really easy and a lot of fun to build . - I used the exact HTML5 Canvas brush image/sketching technique that I have previously demonstrated the only change is that I added the new UI style elements and added support for multiple touch points . Otherwise , the drawing logic is identical . <p> Lil Doodle is a great new iPad application for entertaining both you and your children ! If you know how to use a childrens magnetic drawing toy , then you know how to use Lil Doodle . Pick a " pen " shape , and @ @ @ @ @ @ @ @ @ @ If you want to erase everything and start over , just use the slider at the bottom . Doodle and have fun ! <p> Using the HTML5 Canvas inside of PhoneGap has great performance on iOS , and building the application using purely HTML , CSS , and JavaScript made it incredibly simple . After I wrote the core drawing engine for a previous- blog post , I whipped up the UI in one evening , and then started user testing with my little beta tester . - She found some issues that I had overlooked , and a few days later I submitted it to the app store . <p> and yes , she really does play with it : <p> The app is currently available for iPad devices on iTunes I 'm about to start researching/testing performance on other platforms , so maybe soon it will be out in other ecosystems well see . - You can get it now at : 
@@106848881 @2248881/ <h> Video : Intro To PhoneGap from MobileDev@TU <p> Thanks to everyone who came out the the MobileDev@TU meetup last night at Towson University . - I 've posted the video online for those that want to review it , or werent able to attend . - You can check it out below . - Unfortuantely I lost the last 15 minutes of video because I was n't patient enough to let the camera finish what it was doing , and I just forced everything to power off , resulting in file corruption . - Patience would have prevailed . I 'm going to record another video discussing emulator and on-device debugging strategies for PhoneGap apps , which I 'll post here on the blog later . <p> I 'm still learning the camera , and I need to tweak a few audio setting for next time ( sorry about the clipping ) ! 123433 @qwx983433 <p> Thank you for the vidoe which helped me understand a few things which I found a little confusing on the phonegap website . <p> There is one question I still have though . How does the pricing @ @ @ @ @ @ @ @ @ @ in the terms do I pay the monthly fee as long as I use the service to develop preivate apps or do I pay as long as the apps are up for sale ? For example , I create three apps ( not platforms , but distinct apps ) within a couple of months and put them on the app store where I keep them infefinitely , but do n't  create any further apps . Do I just pay for two months or until I pull the apps from the app store ? 123436 @qwx983436 <p> You only pay while you are using the service . If you cancel your PhoneGap Build subscription , the compiled apps will continue to work . <p> Louis Hau+knecht <p> Hey Andrew , <p> nice talk ! We are currently evaluation some possibilities for app development and I want to give our decision-makers an introduction to PhoneGap and PhoneGap Build . <p> ive watched the whole presentation , glad to hear that my skills on web will not be wasted . Phonegap will help all of web developers . Sir i havnt catched up @ @ @ @ @ @ @ @ @ @ can i have a list of them so i can see and use them to get ssome ideas in begining building phonegap apps ! 
@@106848882 @2248882/ <h> Category Archives : Watson <p> You may have heard a lot of buzz coming out of IBM lately about Cognitive Computing , and you might have also wondered " what the heck are they talking about ? " - You may have heard of- services for data and predictive analytics , services for natural language text processing , services for sentiment analysis , services understand speech and translate languages , but its sometimes hard to see the forest through the trees . <p> I highly recommend taking a moment to watch this video that introduces Cognitive Computing from- IBM : <p> Cognitive computing systems learn and interact naturally with people to extend what either humans or machine could do on their own . <p> They help human experts make better decisions by penetrating the complexity of Big Data . <p> Cognitive systems are often based upon massive sets of data and powerful analytics algorithms that detect- patterns and concepts that can be turned into actionable information for the end users . - Its not " artificial intelligence " in the sense that the services/machines act upon their own @ @ @ @ @ @ @ @ @ @ information that enables them to make better decisions . <p> The benefits of cognitive systems in a nutshell : <p> They augment the users experience <p> They provide the ability to process information faster <p> They- make complex information easier to understand <p> They enable you to do things you might not otherwise be able to do <p> Curious where this will lead ? - Now take a moment and watch this video talking about the industry-transforming opportunities that Cognitive Computing is already beginning to bring to life " <p> So , why is the " mobile guy " - talking about Cognitive Computing ? <p> First , its because Cognitive Computing is big I mean , really , really big . - Cognitive systems are literally transforming industries and providing powerful analytics and insight into the hands of both experts and " normal people " . - When I say " into the hands " , I again- mean this literally ; much of this cognitive ability is being delivered to those end users through their mobile devices . <p> Last , and this is purely just- personal opinion @ @ @ @ @ @ @ @ @ @ somewhat of cognitive service for developing mobile apps . - If you look at it from the operational analytics perspective , you have an immediate insight and a snapshot into the health of your system that you would never have seen otherwise . - You can know what types of devices are hitting your system , what services- are being used , how long things are taking , and detect issues , all without any additional development efforts on your end . Its not predictive analytics , but sure is helpful and gets us moving in the right direction . <p> IBMs Watson Developer Cloud speech services just got a whole lot easier for mobile developers . - I myself just learned about these two , and cant wait to integrate them into my own mobile applications . <p> The Watson Speech to Text and Text to Speech services are now available in both native iOS and Android SDKs , making it even easier to integrate language services into your apps . <p> Last week I had the opportunity to present to a great audience at- the- MoDev DC meetup @ @ @ @ @ @ @ @ @ @ - In this session I focused on how you can create a voice-driven experience in your mobile apps . I gave an introduction to IBM Bluemix and IBM Watson services ( particularly the Watson language services ) , and demonstrated how you can integrate them into your native iOS apps . I also covered IBM MobileFirst for operational analytics and- remote logging to provide insight into your apps performance once it goes live . - Check out a recording of the complete presentation in the video below : <p> If you think that voice-driven apps are too complicated , or out of your reach , then I have great news for you : They are not ! Last week , IBM elevated- several IBM Watson voice services from Beta to General Availability that means you can use them reliably in your own systems too ! <p> Let 's examine the two parts of the system , and see what solutions IBM has available right now for you to take advantage of <p> Transcribe audible signal to text transcript <p> Part one of this equation is converting the audible signal into text @ @ @ @ @ @ @ @ @ @ Speech to Text service fits this bill perfectly , and can be called from any app platform that supports REST services which means just about anything . It could be from the browser , it could be from the desktop , and it could be from a native mobile app . The Watson STT service is very easy to use , you simply post a request to the REST API containing an audio file , and the service will return to you a text transcript based upon what it is able to analyze from the audio file . With this API you do n't  have to worry about any of the transcription actions on your own no concern for accents , etc Let Watson do the heavy lifting for you . <p> Perform a system action by parsing text transcript <p> This one is perhaps not quite as simple because it is entirely subjective , and depends upon what you/your app is trying to do . You can parse the text transcript on your own , searching for actionable keywords , or you can leverage something like the IBM Watson @ @ @ @ @ @ @ @ @ @ Watson data corpora . <p> Riding on the heels of the Watson language services promotion , I put together a sample application that enables a voice-driven app- experience on the iPhone , powered by both the Speech To Text and Watson Question &amp; Answer services , and have made the mobile app and Node.js middleware source code available on github . <p> The app communicates to the Speech to Text and Question &amp; Answer services through the Node.js middelware tier , and connects directly to the Advanced Mobile Access service to provide operational analytics ( usage , devices , network utilization ) and remote log collection from the client app on the mobile devices . <p> For the Speech To Text service , the app records audio from the local device , and sends a WAV file to the Node.js in a HTTP post request . The Node.js tier then delegates to the Speech To Text service to provide transcription capabilities . The Node.js tier then formats the respons JSON object and returns the query to the mobile app . <p> For the QA service , the app makes an @ @ @ @ @ @ @ @ @ @ the Node.js server , which delegates to the Watson QA natural language processing service to return search results . The Node.js tier then formats the respons JSON object and returns the query to the mobile app . <p> The general flow between these systems is shown in the graphic below : <p> IBM Watson Speech QA for iOS Logic Flow <h> Code Explained <p> The code for this example is really in 2- main areas : The client side integration in the mobile app ( Objective-C , but could also be done in Swift ) , and the application server/middleware implemented in Node.js . <h> Node.js Middleware <p> The server side JavaScript- code uses the- Watson Node.js Wrapper , which enables you- to- easily instantiate Watson services in- just a few short lines of code <p> The credentials come from your Bluemix environment configuration , then you just create instances of whichever services that you want to consume . <p> I implemented two methods in the Node.js application tier . The first accepts the audio input from the mobile client as an attachment to a HTTP POST request and returns @ @ @ @ @ @ @ @ @ @ Note : I am using the free/open Watson Healthcare data set . However- the Watson QA service can handle other data sets- these- require an engagement with IBM to train the Watson service to understand the desired data sets . <h> Native iOS Objective C <p> On the mobile side- were working with a native iOS application . My code is written in Objective C , however you could also implement this using Swift . I wont go into complete line-by-line code here for the sake of brevity , but you can access the client side code in the ViewController.m file . In particular , this is within the postToServer and requestQA methods . <p> You can see the flow of the application within the image below : <p> App Flow : User speaks , transcript displayed , results displayed <p> The native mobile app first captures audio input from devices microphone . This is then sent to the Node.js servers /transcribe method- as an attachment to- a HTTP POST request- ( postToServer method on line 191 ) . On the server side this delegates to the Speech To Test @ @ @ @ @ @ @ @ @ @ on the client , the transcribed text is displayed in the UI and then a request is made to the QA service . <p> In the requestQA method , the mobile app makes a HTTP GET request to the Node.js apps /ask method ( as shown on line 257 ) . The Node.js app delegates to the Watson QA service as shown above . Once the results are returned to the client they are displayed within a standard UITableView in the native app . <h> MobileFirst Advanced Mobile Access <p> A few other things you may notice if you decide to peruse the native Objective-C code : <p> Within AppDelegate.m you will see calls to IMFClient , IMFAnalytics , and OCLogger classes . These enable operational analytics and log collection within the Advanced MobileAccess service . <p> What I 'm about to show you might seem like science fiction from the future , but I can assure you it is not . Actually , every piece of this is available for you to use as a service . - Today . <p> Yesterday- Twilio , an IBM partner whose services are @ @ @ @ @ @ @ @ @ @ including live video chat as a service . - This makes live video very easy to integrate into your native mobile or web based applications , and gives you the power to do some very cool things . For example , what if you could add video chat capabilities between your mobile and web clients ? Now , what if you could take things a step further , and add IBM Watson cognitive computing capabilities to add real-time transcription and analysis ? <p> Jeff and Damion did an awesome job showing of both the new video service and the power of IBM Watson . I can also say first-hand that the new Twilio video services are- pretty easy to integrate into your own projects ( I helped them integrate these services into the native iOS client ( physicians app ) - shown in the demo ) ! - You just pull in the SDK , add your app tokens , and instantiate a video chat . - Jeff is pulling the audio stream from the WebRTC client and pushing it up to Watson in real time for the transcription and sentiment analysis services . 
@@106848883 @2248883/ <h> Presentations From Dreamforce 2012 <p> I recently returned from a great trip to San Francisco , CA , where I was able to attend Dreamforce- - Salesforce.coms annual conference . - Dreamforce is probably the biggest conference Ive ever attended , with 90,000 registered attendees consuming the entire Moscone Center and all of downtown San Francisco . While only a small portion of those attendees were technical/developers , it was still a massive turnout . The best part Dreamforce attendees are really excited about PhoneGap ! - I had a fantastic turnout for my sessions , just take a look below ! <p> So , why all the excitement ? <p> SalesForce.coms mobile hybrid SDK is built on top of PhoneGap. - This makes consumption of business-critical Salesforce.com data in your mobile applications very easy . - You can also use components from the- Salesforce Touch platform inside of PhoneGap applications , and even some of Salesforces own mobile applications are even built with PhoneGap ( Logger , Dreamforce ) . <p> I had several presentations focused on PhoneGap , and had great audiences at all of them @ @ @ @ @ @ @ @ @ @ below . - Just press the " space " key when viewing a presentation to advance to the next slide . <p> Description : Interested in developing applications for mobile devices , on multiple platforms ? Interested in leveraging your existing web development skills to build natively installed applications ? Just looking to expand your skill set ? Come join Adobes Technical Evangelist , Andrew Trice , to learn about cross platform mobile development and PhoneGap . In this session , you will get an introduction to PhoneGap ( Apache Cordova ) , be able to see example PhoneGap applications , and walk through the process of building your first PhoneGap application . <p> Description : Do you have a need to create rich visual data-centric applications , but also have the requirement to use standard web technologies , and do n't  know what to do next ? Well , you 're in luck ! Come to this session to learn about data visualization strategies and frameworks powered entirely by HTML , CSS , and JavaScript . <p> Description : Native applications built using web technologies can suffer from the " @ @ @ @ @ @ @ @ @ @ right as a native application . In this session well focus on strategies to make your apps feel like native apps , including considerations for a native-feeling UI , platform consistency , and user experience . 123433 @qwx983433 <p> Hi Andy thanks for the postings . I 'm just starting to get more involved with Saleforce integrations and this should help ! <p> I heard from folks that Dreamforce was quite an event . Mark 123436 @qwx983436 <p> Hi Mark , Yes , it was definitely quite an event with that many people parts of it are total chaos , but overall a good conference . Its different than a lot of tech conferences b/c most of the attendees are sales/management , and most of the conference sessions are geared towards those audiences . Salesforce has a native SDK , a web SDK , and a hybrid SDK . The hybrid one is built using PhoneGap . If you 're still doing native development , you can use the native SDK , but if you 're in the HTML world these days , you can access SF using the Hybrid SDK or forcetk.js @ @ @ @ @ @ @ @ @ @ all is well ! <p> http : **27;5994;TOOLONG Giacomo Balli <p> I had no clue of the strong connection with PhoneGap awesome ! <p> Thanks SO much for the post . <p> talk soon , G <p> http : //www.sproutloop.com/ Kevin Cocco <p> Andrew , Big thanks for open sourcing all these great example apps ! Has been extremely helpful in learning PhoneGap . Keep up the excellent work . Thanks -Kevin 
@@106848884 @2248884/ <h> Tag Archives : HTML5 <p> After spending some time playing around sketching with the HTML5 canvas element earlier this week , I figured " why not add some enterprise concepts to this example ? " - Next thing you know we 've got a multi-device shared sketching/collaboration experience . <p> To keep things straightforward , I chose to demonstrate the near-realtime collaboration using a short-interval HTTP poll . - HTTP polling is probably the simplest form of near-realtime data in web applications , however you may experience lag when compared to a socket connection of equivalent functionality . - I 'll discuss the various realtime data options you have in Flex/Flash and HTML/JS and their pros &amp; cons further in this post . <p> What you 'll see in the video below is the sketching example- with realtime collaboration added using short-interval data polling of a ColdFusion application server . - The realtime collaboration is shown between an iPad 2 , a Kindle Fire , and a Macbook Pro . <p> Before we get into the code for this example , let 's first review some realtime data basics <p> First , @ @ @ @ @ @ @ @ @ @ - Here are just a few : <p> Time sensitive information , where any delay could have major- repercussions <p> Realtime financial information <p> Emergency services ( medical , fire , police ) <p> Military/Intelligence scenarios <p> Business critical efficiency/performance- metrics <p> Collaboration <p> Realtime audio/video collaboration <p> Shared experience ( presentations/screen sharing ) <p> Entertainment <p> Streaming media ( audio/video ) <p> Gaming <p> Regardless of whether you are building applications for mobile , the web , or desktop , using any technology ( Flex/Flash , HTML/JS , Java , . NET , Objective C , or C/C++ ( among others ) ) , there are basically 3 methods for streaming/realtime data : <p> Socket Connection <p> HTTP Polling <p> HTTP Push <h> Socket Connections <p> Socket connectionss are basically end-to-end communications channels between two computer processes . - Your computer ( a client ) connects to a server socket and establishes a persistent connection that is used to pass data between the client and server in near-realtime. - Persistent socket connections are generally based upon TCP or UDP- and enable asynchronus bidirectional communication . - Binary or Text-based @ @ @ @ @ @ @ @ @ @ in time , in any sequence , as data is available . - In HTML/JS applications you can use web sockets , which I recently discussed , or use a plugin that handles realtime socket communication . Did you also know that the next version of- ColdFusion- will even have web socket support built in ? - In Flash/Flex/AIR , this can be achieved using the RTMP protocol ( LCDS , Flash Media Server , etc ) or raw sockets ( TCP or UDP ) . <p> Direct Socket Communications <p> In general , direct socket based communication is the most efficient means of data transfer for realtime application scenarios . - There is less back and forth handshaking and less packet encapsulation required by various protocols ( HTTP , etc ) , and you are restricted by fewer network protocol rules . - However , socket based communications often run on non-standard or restricted ports , so they are more likely to be blocked by IT departments or stopped by network firewalls. - - If you are using socket based communication within your applications , which are running on @ @ @ @ @ @ @ @ @ @ , you may want a fallback to another realtime data implementation for failover cases . <h> HTTP Polling <p> HTTP Polling is the process of using standard HTTP requests to periodically check for data updates on the server . - The client application requests information from the server . - Generally , the client will send a timestamp indicating the last data update time . - If there is information available on the server that is newer than the timestamp , that data will be immediately sent back to the client ( and the clients timestamp will be updated ) . - After a period of time , another request will be made , and so forth until the polling is stopped within the application . - Using this approach , the application is more-or-less " phoning home " periodically to the server to see if there are any updates . - You can achieve near-realtime performance by setting a very short polling interval ( less than one second ) . <p> Basic Data Poll Sequence <p> HTTP polling uses standard web protocols and ports , and generally will not @ @ @ @ @ @ @ @ @ @ of standard HTTP ( port 80 ) or HTTPS ( port 443 ) without any issue . - This can be achieved by polling JSON services , XML Services , AMF , or any other data format on top of a HTTP request . - HTTP polling will generally be slower than a direct socket method , and will also utilize more network bandwidth b/c of request/response encapsulation and the periodic requests to the server . - It is also important to keep in mind that the HTTP spec only allows for 2 concurrent connections to a server at any point in time . - Polling requests can consume HTTP connections , thus slowing load time for other portions of your application . - HTTP polling can be employed in HTML/JS , Flex/Flash/AIR , desktop , server , or basically any other type of application using common libraries &amp; APIs . <h> HTTP Push <p> HTTP Push technologies fall into 2 general categories depending upon the server-side **25;6023;TOOLONG - This can refer to- HTTP Streaming , where a connection is opened between the client and server and kept open using @ @ @ @ @ @ @ @ @ @ client , it will be pushed across the existing open HTTP connection . - HTTP Push can also refer to HTTP Long Polling , where the client will periodically make a HTTP request to the server , and the server will " hold " the connection open until data is available to send to the client ( or a timeout occurs ) . - Once that request has a complete response , another request is made to open another connection to wait for more data . - Once Again , with HTTP Long Poll there should be a very short polling interval to maintain near-realtime performance , however you can expect some lag . <p> HTTP Long Poll Sequence <p> HTTP Streaming &amp; HTTP Long polling can be employed in HTML/JS applications using the Comet approach ( supported by numerous backend server technologies ) and can be employed in Flex/Flash/AIR using BlazeDS or LCDS . <h> Collaborative Applications <p> Now back to the collaborative sketching application shown in the video above the application builds off of the sketching example from previous blog posts . - I added logic to monitor @ @ @ @ @ @ @ @ @ @ to share content between sessions that share a common I 'd . <p> Realtime Collaborative Sketches <p> In the JavaScript code , I created an ApplicationController class that acts as an observer to the input from the Sketcher class . - The ApplicationController encapsulates all logic handling data polling and information sharing between sessions. - When the application loads , it sets up the polling sequence . <p> The polling sequence is setup so that a new request will be made to the server 250MS- after receiving a response from the previous request . - Note : this is very different from using a 250MS interval using setInterval. - This approach guarantees 250MS from response to the next request . - If you use a 250MS interval using setInterval , then you are only waiting 250MS between each request , without waiting for a response . - If your request takes more than 250 MS , you will can end up have stacked , or " concurrent " requests , which can cause serious performance issues . <p> When observing the sketch input , the start and end positions and color @ @ @ @ @ @ @ @ @ @ captured transactions that will be pushed to the server . - ( The code supports multiple colors , even though there is no method to support changing colors in the UI . ) <p> The server then stores the pending transactions in memory ( I am not persisting these , they are in-ram on the server only ) . - The server checks the transactions that are already in memory against the last timestamp from the client , and it will return all transactions that have taken place since that timestamp . <p> In a previous post on capturing user signatures in mobile applications , I explored how you capture user input from mouse or touch events and visualize that in a HTML5 Canvas . - Inspired- by activities with my daughter , I decided to take this signature capture component and make it a bit more fun &amp; exciting . - My daughter and I often draw and sketch together whether its a magnetic sketching toy , doodling on the iPad , or using a crayon and a placemat at a local pizza joint , there is always something @ @ @ @ @ @ @ @ @ @ was actually good at drawing . ) <p> Olivia &amp; the iPad <p> You can take that exact same signature capture example , make the canvas bigger , and then combine it with a tablet and a stylus , and you 've got a decent sketching application . - However , after doodling a bit you will quickly notice that your sketches leave something to be desired . - When you are drawing on a canvas using moveTo ( x , y ) and lineTo ( x , y ) , you are somewhat limited in what you can do . You have lines which can have consisten thickness , color , and opacity . You can adjust these , however in the end , they are only lines . <p> If you switch your approach away from moveTo and lineTo , then things can get interesting with a minimal amount of changes . You can use images to create " brushes " for drawing strokes in a HTML5 canvas element and add a lot of style and depth to your sketched content . - This is an approach that I 've @ @ @ @ @ @ @ @ @ @ worked on in the past . - Take a look at the video below to get an idea what I mean . <p> Examining the sketches side by side , it is easy to see the difference that this makes . - The variances in stroke thickness , opacity &amp; angle add depth and style , and provide the appearance of drawing with a magic marker . <p> Sketches Side By Side <p> Its hard to see the subtleties in this image , so feel free to try out the apps on your own using an iPad or in a HTML5 Canvas-capable browser : <p> Just click/touch and drag in the gray rectangle area to start drawing . <p> Now , let 's examine how it all works . - Both approaches use basic drawing techniques within the HTML5 Canvas element . - If you are n't  familiar with the HTML5 Canvas , you can quickly get up to speed from the tutorials from Mozilla. <h> moveTo , lineTo <p> The first technique uses the canvass drawing context moveTo ( x , y ) and lineTo ( x , y ) @ @ @ @ @ @ @ @ @ @ . - Think of this as playing " connect the dots " and drawing a solid line between two points . <p> The sample output will be a line from point A , to point B , to point C : <p> lineTo ( x , y ) Stroke Sample <h> Brush Images <p> The technique for using brush images is identical in concept to the previous example you are drawing a line from point A to point B. - However , rather than using the built-in drawing APIs , you are programmatically repeating an image ( the brush ) from point A to point B. <p> First , take a look at the brush image shown below at 400% of the actual scale . - It is a simple image that is a diagonal shape that is thicker and more opaque on the left side . - By itself , this will just be a mark on the canvas . <p> Brush Image ( 400% scale ) <p> When you repeat this image from point A to point B , you will get a " solid " line . @ @ @ @ @ @ @ @ @ @ the angle of the stroke . - Take a look at the sample below ( approximated , and zoomed ) . <p> Brush Stroke Sample ( simulated ) <p> The question is how do you actually do this in JavaScript code ? <p> First , create an Image instance to be used as the brush source . <p> brush = new Image() ; brush.src = ' assets/brush2.png ' ; <p> Once the image is loaded , the image can be drawn into the canvas context using the drawImage() function . The trick here is that you will need to use some trigonometry to determine how to repeat the image . In this case , you can calculate the angle and distance from the start point to the end point . Then , repeat the image based on that distance and angle . <p> This example uses the twitter bootstrap UI framework , jQuery , and Modernizr. - Both the lineTo.html and brush.html apps use the exact same code , which just uses a separate rendering function based upon the use case . - - Feel free to try out the @ @ @ @ @ @ @ @ @ @ HTML5 Canvas-capable browser : <p> Here are some interesting and quite surprising statistics for the US Census Browser HTML/PhoneGap showcase application that I released in December , which I wanted to share . The app is a browser for US Census data , full detail available here : http : **35;6050;TOOLONG . The Census Browser application was intended as a showcase app for enterprise-class data visualization in HTML-based applications , and all source code is freely available to the public . <p> What is really surprising is the " health " of my app within the given ecosystems . I offered the app as a free download in each market . The app is focused on Census data , so there is obviously not a ton of consumer demand , however the data is still interesting to play around with . I would not expect the same results for all types of apps in all markets . <p> BlackBerry Playbook downloads were in 3rd , just behind iOS ( BB is 11% of all downloads ) <p> Android traffic was minimal ( 2% of all downloads ) <p> The general @ @ @ @ @ @ @ @ @ @ is strongest , followed by Android , and that BB is dead . These numbers show a conflicting reality . Barnes &amp; Noble was the strongest , with iOS in second place , and BlackBerry just behind iOS . <p> One growing trend that I have seen in mobile &amp; tablet applications is the creation of tools that enable your workforce to perform their job better . This can be in the case of mobile data retrieval , streamlined sales process with apps for door-to-door sales , mobile business process efficiency , etc <p> One of the topics that comes up is how do you capture a signature and store it within your application ? This might be for validation that the signer is who they say they are , or for legal/contractual reasons . Imagine a few scenarios : <p> Your cable TV ca n't be installed until you sign the digital form on the installation techs tablet device <p> You agree to purchase a service from a sales person ( door to door , or in-store kiosk ) your signature is required to make this legally binding . <p> @ @ @ @ @ @ @ @ @ @ data is presented to you . <p> These are just a few random scenarios , I 'm sure there are many more . - In this post , I will focus on 2 ( yes , I said two ) cross-platform solutions to handle this task one built with Adobe Flex &amp; AIR , and one built with HTML5 Canvas &amp; PhoneGap. - <p> Watch the video below to see this in action , then well dig into the code that makes it work . <p> The basic flow of the application is that you enter an email address , sign the interface , then click the green " check " button to submit to the signature to a ColdFusion server . - The server then sends a multi-part email to the email address that you provided , containing text elements as well as the signature that was just captured . <p> If you 'd like to jump straight to specific code portions , use the links below : <h> The Server Solution <p> Let 's first examine the server component of the sample application . - The server side is powered by @ @ @ @ @ @ @ @ @ @ by both the Flex/AIR and HTML/PhoneGap front-end applications . - The CFC exposes a single service that accepts two parameters : the email address , and a base-64 encoded string of the captured image data . <p> Note : I used base-64 encoded image data so that it can be a single server component for both user interfaces . In Flex/AIR you can also serialize the data as a binary byte array , however binary serialization is n't quite as easy with HTML/JS read on to learn more . <h> The Flex/AIR Solution <p> The main user interface for the Flex/AIR solution is a simple UI with some form elements . In that UI there is an instance of my SignatureCapture user interface component . This is a basic component that is built on top of UIComponent ( the base class for all Flex visual components ) , which encapsulates all logic for capturing the user signature . The component captures input based on mouse events ( single touch events are handled as mouse events in air ) . The mouse input is then used to manipulate the graphics content of @ @ @ @ @ @ @ @ @ @ think of the drawing API as a language around the childhood game " connect the dots " . In this case , you are just drawing lines from one point to another . <p> When the form is submitted , the graphical content is converted to a base-64 encoded string using the Flex ImageSnapshot class/API , before passing it to the server . <p> You can check out a browser-based Flex version of this in action at LONG ... Just enter a valid email address and use your mouse to sign within the signature area . When this is submitted , it will send an email to you containing the signature . <h> The HTML5/PhoneGap Solution <p> The main user interface for the HTML5/PhoneGap solution is also a simple UI with some form elements . In that UI there is a Canvas element that is used to render the signature . I created a SignatureCapture JavaScript class that encapsulates all logic for capturing the user signature . In browsers that support touch events ( mobile browsers ) , this is based on the touchstart , touchmove and touchend events . @ @ @ @ @ @ @ @ @ @ browsers ) , the signature input is based on mousedown , mousemove and mouseup events . The component captures input based on touch or mouse events , and that input is used to manipulate the graphics content of the Canvas tag instance . The canvas tag also supports a drawing API that is similar to the ActionScript drawing API . To read up on Canvas programmatic drawing basics , check out the tutorials at LONG ... <p> When the form is submitted , the graphical content is converted to a base-64 encoded string using the Canvass toDataURL() method . The toDataURL() method returns a base-64 encoded string value of the image content , prefixed with " data:image/png , " . Since I 'll be passing this back to the server , I do n't  need this prefix , so it is stripped , then sent to the server for content within the email . <p> You can check out a browser-based version of this using the HTML5 Canvas in action at http : **40;6087;TOOLONG Again , just enter a valid email address and use your mouse to sign within the signature @ @ @ @ @ @ @ @ @ @ an email to you containing the signature . However , this example requires that your browser supports the HTML5 Canvas tag . <p> Also a note for Android users , the Canvas toDataURL() method does not work in Android versions earlier than 3.0 . However , you can implement your own toDataURL() method for use in older OS versions using the technique in this link : LONG ... ( I did not update this example to support older Android OS versions . ) <p> While working with Kevins code , I started tinkering " what if I change this , what if I tweak that ? " Next thing you know , I put together a sample scenario showing subscription-based realtime data streaming to multiple web clients using web sockets . Check out the video below to see it in action . <p> You are seeing 9 separate browser instances getting realtime push-based updates from a local server using web sockets . When the browser loads , the html-based client makes a web socket connection , then requests all symbols from the server . The server then sends the stock @ @ @ @ @ @ @ @ @ @ the HTML user interface . From there , the user can click on a stock symbol to subscribe to updates for that particular symbol . DISCLAIMER : All that data is randomly generated ! <p> I put together this example for experimentation , but also to highlight a few technical scenarios for HTML-based applications . Specifically : <p> Realtime/push data in HTML-based apps <p> Per-client subscriptions for realtime data <p> Multi-series realtime data visualization in HTML-based apps <p> The server is an AIR app started by Kevin , based on the web sockets draft protocol . It is written in JavaScript , and the client is a HTML page to be viewed in the browser . <p> If you do n't  feel like reading the full web sockets protocol reference , you can get a great overview from websocket.org or Wikipedia . <p> One thing to keep in mind is that web sockets are not widely supported in all browsers yet . There is a great reference matrix for web socket support from caniuse.com : <p> If you still are n't  sure if your browser supports web sockets , you @ @ @ @ @ @ @ @ @ @ want to test for web socket support within your own applications , you can easily check for support using Modernizr . Note : I did n't  add the Modernizr test in this example I only tested in Chrome on OSX . <p> You 'll know the server is started b/c an air window will popup ( you can ignore this , just do n't  close it ) , and you will start seeing feed updates in the console output . <p> Once the server is running , open " client/client.html " in your browser . It will connect to the local server , and then request the list of symbols . If you click on a symbol , it will subscribe to that feed . Just click on the symbol name again to unsubscribe . You 'll know the feed is subscribed b/c the symbol will show up in a color ( matching the corresponding feed on the chart ) . Again , let me reiterate that I only tested this in Chrome . <p> You can open up numerous client instances , and all will receive the same updates in real @ @ @ @ @ @ @ @ @ @ meat " of code for the server starts in **31;6129;TOOLONG . Basically , the server loads a configuration file for the socket server , then creates a ConnectionManager and DataFeed ( both of these are custom JS classes ) . The ConnectionManager class encapsulates all logic around socket connections . This includes managing the ServerSocket as well as all client socket instances and events . The DataFeed class handles data within the app . First , it generates random data , then sets up an interval to generate random data updates . For every data update , the ConnectionManager instances dispatch() method is invoked to send updates to all subscribed clients . Rather than trying to put lots of code snippets inline in this post ( which would just be more confusing ) , check out the full source at : - LONG ... <p> The client code all starts in client.html , with the application logic inside of client/scripts/client.js . Once the client interface loads , it connects to the web socket and adds the appropriate event handlers . Once subscribed to a data feed , realtime data will @ @ @ @ @ @ @ @ @ @ to fit the data visualization structure , then rendered in an HTML canvas using the RGraph data visualization library . RGraph is free to get started with , however if you want to deploy a production app with it , you 'll need a license . You 'll notice that each feed updates independently , based upon the client subscriptions . Note : The data visualization is not temporally aligned if you want the updates in time-sequence , there is a litte bit more work involved in the client-side data transformation . <p> This example is intended to get your minds rolling with the concepts ; it is not *yet* an all-encompassing enterprise solution . You can expect to see a few more data push scenarios here in the near future , based on different enterprise server technologies . 
@@106848885 @2248885/ <p> Have you ever been testing or developing an application , and things just do n't  seem to be working as you 've expected ? Just imagine that the data that you are receiving from the server is n't exactly what youd expect even worse , what if this is happening in an application that youve already released to the app store , but not in your local debugging environment ? Debugging this type of error can be difficult , but it can be made MUCH easier by taking advantage of a network proxy . <p> A network proxy server acts as an intermediary between the client and server . In this case , we are talking about using a network proxy server to intercept and analyze all web traffic occurring between a mobile device and the application server . The network proxy/monitor that I use very often in development is Charles . <p> Charles let 's you inspect ( and even intercept and change ) all HTTP traffic either on your local machine , or on mobile devices that you control . - In this case I am using it to @ @ @ @ @ @ @ @ @ @ it actually intercepts all HTTP traffic for the device even HTTP requests made by native applications or " release " applications obtained via the App Store . - It wo n't give you insight into the code within an application just insight into all HTTP-based network communication . 123434 @qwx983434 <p> From the Charles proxy , you 'll be able to see every HTTP request . Once you select an HTTP request , you 'll be able to view pretty much everything about it round-trip time , request/response size , HTTP headers , query parameters , HTTP POST data , and of course , the response payload . <p> If you want , you can even use the- Breakpoint Tool- to intercept HTTP requests and modify the response payload before its returned back to your application . <p> To get started using the network proxy with your own applications , just follow the instructions provided in the Charles FAQ . You will quickly be " up and running " with the proxy , and able to inspect all HTTP traffic within your applications . <p> Once you 've installed and launched Charles on your @ @ @ @ @ @ @ @ @ @ on your mobile device . For iOS devices , go into settings for your Wi-Fi connection , and set up a " Manual " HTTP proxy . Then enter the IP address for the computer that is running Charles , and port 8888. - Note : The device and your computer should be on the same network segment . <p> This also works for Android devices too . You 'll need to go into the " Advanced Settings " for your Wi-Fi connection , select " Manual " proxy , enter your computers IP address for the proxy hostname , and specify port 8888 as used by Charles . <p> Once you 've got this configured , you can view all HTTP traffic in all of your applications . This is particularly useful for debugging query parameter and server response issues on devices . If you 're using the PhoneGap Emulator , you can just use Chromes Developer Tools without having to worry about configuring a proxy . <p> The application that I was debugging in the video is my US Census Browser , an open source PhoneGap application for visualizing US Census @ @ @ @ @ @ @ @ @ @ : 123433 @qwx983433 <p> Thank you for the tip off re . Charles , it just saved me a world of pain ! <p> Aanya Dsouza <p> Yes ! Thank you ! This is exactly what my team phonegap developer needed . Itn++ works just the same to get the data on the BB Z10 as well , and probably any device . Just make sure the IP address is the same as the computer that is running Charles . 
@@106848887 @2248887/ <p> This post is inspired by all the comments Ive seen this week about JS in the enterprise . I would have never imagined- this 10- years ago , but JavaScript is now pretty much ubiquitous . Here are a few reasons why you need to paying attention to JavaScript if you are n't  already , and why you should definitely not- write it off . <p> First , I think one of the major reasons for JavaScripts ubiquity is that- JavaScript is approachable . It is relatively easy for beginners to learn JavaScript , and powerful enough for advanced users to build complex and reliable systems . <p> Second , why you need to pay attention , JavaScript is everywhere . <p> You can now use JavaScript to develop on virtually any platform : client side applications , server side logic , embedded chips/IoT devices , manage build scripts and dependencies , and more . <p> This does n't  mean you 'll use the exact same code in every case , rather that you can use the same skill set JavaScript Development to deliver solutions across multiple paradigms . @ @ @ @ @ @ @ @ @ @ power client side apps/user interfaces , and user interactions on numerous platforms and devices . <h> Web <p> Of course JavaScript powers the web , this is a given . JavaScript is the primary scripting language for all web browsers . I wo n't focus on this much b/c its already well known . <h> Mobile <p> JavaScript can also be used to power mobile applications that are natively installed on a device . <p> Apache Cordova/PhoneGap- You can build natively installed apps with web technology using PhoneGap or Cordova . PhoneGap is Adobes branded distribution of Cordova , but from the developers perspective , they are basically the same thing . Your app runs within a webview on the mobile device , and you build your user interface the same way you you build a dynamic web application . Your user interface- is implemented in HTML , styled with CSS , and all interactivity is created with JavaScript . <p> React Native- JavaScript powered web apps do n't  just have to be inside of a a web view . The React Native framework gives developers the ability to write their @ @ @ @ @ @ @ @ @ @ in a native application running on the mobile device . The logic is interpreted JavaScript at runtime , but everything that the user- interacts with ( all UI elements ) is 100% native , providing a very high quality user experience , and it is now available for both iOS and Android applications . <p> Unity 3D You can even develop rich &amp; immersive mobile 3D simulation or gaming experience , entirely powered by JavaScript using the Unity 3D engine . **These can be web , desktop , or mobile , but is often used in mobile gaming . <h> Desktop <p> Yup , desktop apps are not left out of the mix . Most desktop solutions fall into a category similar to Apache Cordova , where the end results is a web view that has access to lower level APIs , whose content is developed with web based technology . <h> The Server Side <p> Most obviously- Node.js a JavaScript runtime buit on Chromes V8 JavaScript Engine has made huge inroads into server side development and the enterprise . Node.js , powered by frameworks like express.js or loopback.io makes @ @ @ @ @ @ @ @ @ @ . <p> Here are some stats that- show the magnitude of growth and adoption for Node.js/npm.js alone . NPM- stats currently shows a total of- 186,946 packages available for download , 94,978,032 package downloads in the last day , and 2,451,734,737 package downloads in the last month . <p> This does n't  mean that JavaScript is the best language at everything . It- also does n't  meant that you can take a single piece of source- code and run it in every device/context imaginable . <p> It means that you can use your skills in JavaScript to develop for just about any kind of device/context out there . Its not going to be write once , run everywhere , rather in the words of the React.js team : learn once , write everywhere . <p> Node.js is an incredible tool for rapidly building highly performant and scalable back end systems , and you develop it using a familiar core language that most front-end developers are already accustomed to , JavaScript. - This acquisition is positioned to greatly enhance Node.js in the enterprise , and StrongLoops offerings will be integrated into @ @ @ @ @ @ @ @ @ @ Even though the acquisition is still " hot off of the presses " , - you can start using these tools together- today : <p> If you have n't heard about StrongLoop 's LoopBack framework , it enables you to easily connect and expose your data as REST services . It provides the ability to visually create data models in a graphical ( or command line ) interface , which are used to automatically generate REST APIs " thus generating CRUD operations for your REST services tier , without having to write any code . <p> Why is this important ? <p> It makes API development easier and drastically reduces time from concept to implementation . - If you have n't yet looked at the LoopBack framework , you should definitely check it out . - You can build API layers for your apps literally in minutes . - Check out the video below for a quick introduction : <p> Again , be sure to check out these posts that detail the integration steps so you can start using these tools together today : <p> That title get your attention ? - @ @ @ @ @ @ @ @ @ @ change based on personal context " with near real-time rules application , without much extra development effort . - If that sounds interesting to you , or like a product you might want to use within your own apps , then you might want to check out this site where you can get involved in the products development : - http : **35;6162;TOOLONG <p> IBM is looking for your input on creating these types of mobile app experiences. - User experiences within a single app that can be- dramatically different per user based on location , past behavior , profile information , social media activity , - and so much more . - With this behavior being driven by configurable rules that can be changed without redeploying an app to the app store . <h> How it works for your customer <p> Consider this scenario : <p> Jon and Andrea download the mobile app for S&amp;W , a retailer known for its attention to providing great customer service . Over the next month , Jon and Andrea use the app to browse and discover content and merchandise differently . <p> @ @ @ @ @ @ @ @ @ @ teams to find gear and clothes for travel to his favorite team 's games . Andrea scours the app for sales and fashion trends and usually ends up following her favorite designers . <p> Andrea and Jon go to a baseball game together . She 's never enjoyed watching it , so she opens up the S&amp;W app to entertain herself , and her app 's navigation quickly steers her through Spring fashion articles . <p> Jon however , wants to replace the hat he 's worn the last three times the team lost , and since he 's in the stadium , his S&amp;W app opens right up to the team 's gear page . The app knows he 's out of town and tells him how to get to an S&amp;W store . <h> How it works for the dev team <p> Consider another scenario : <p> One of the developers on the team , George , sets up the system and application . He then gives access to Janet who is responsible for the customer experience . <p> Janet writes rules defining how the application could adapt @ @ @ @ @ @ @ @ @ @ media , geolocation , app usage , or customer information data . <p> Once Janet has built out her rules , she simply hits Submit ' and can immediately see her clever interactions reflected in the mobile application without having to involve the development team . <p> Analytics let Janet know which adaptations are working best , and helps her find new opportunities to optimize the app 's user experience . <p> Were not talking about a content management system , or translation based on locale , instead a rules-driven product that can adapt literally every aspect of your app : - customize the user interface , enable or disable different features , customized messaging and notifications , and much more , all variable based upon the user context . - This can be used to present- contextually relevant information , drive adoption , provide more/less data depending on your physical context , and so much more . <p> It wont be tied to a specific UI framework , wont be tied to a specific content management system , is n't attempting to re-create Google Now or Apple Proactive Assistance . @ @ @ @ @ @ @ @ @ @ engine that enable you to customize and tailor the app experience to the individual user . <p> Last week I had the opportunity to present to a great audience at- the- MoDev DC meetup group on " Smarter Apps with Cognitive Computing " . - In this session I focused on how you can create a voice-driven experience in your mobile apps . I gave an introduction to IBM Bluemix and IBM Watson services ( particularly the Watson language services ) , and demonstrated how you can integrate them into your native iOS apps . I also covered IBM MobileFirst for operational analytics and- remote logging to provide insight into your apps performance once it goes live . - Check out a recording of the complete presentation in the video below : <p> Back in February I had the opportunity to present " Enabling the Next Generation of Apps with IBM MobileFirst " at the DevNexus developer conference in Atlanta . - It was a great event , packed with lots of useful content . - Luckily for everyone who was n't able to attend , the organizers recorded most of @ @ @ @ @ @ @ @ @ @ . <p> here 's the session Description : Once your app goes live in the app store you will have just entered into an iterative cycle of updates , improvements , and releases . Each successively building on features ( and defects ) from previous versions . IBM MobileFirst Foundation gives you the tools you need to manage every aspect of this cycle , so you can deliver the best possible product to your end user . In this session , we 'll cover the process of integrating a native iOS application with IBM MobileFirst Foundation to leverage all of the capabilities the platform has to offer . 
@@106848888 @2248888/ <p> I recently gave a presentation at IBM Insight on Cognitive Computing in mobile apps . - I showed two apps : one that uses Watson natural language processing to perform search queries , and another that uses Watson translation and speech to text services to take text in one language , translate it to another language , then even- have the app play back the spoken audio in the translated language . - Its this second app that I want to highlight today . <p> In fact , it gets much cooler than that . - I had an idea : " What if we hook up an OCR ( optical character recognition ) engine to the translation services ? " That way , you can take a picture of something , extract the text , and translate it . - It turns out , its not that hard , and I was able to put together this sample app in just under two days . - Check out the video below to see it in action . <p> To be clear , I ended up using a @ @ @ @ @ @ @ @ @ @ . This is not based on any of the work IBM research is doing with OCR or natural scene OCR , and should not be confused with any IBM OCR work . - This is basic OCR and works best with dark text on a light background . <p> The Tesseract engine let 's you pass in an image , then handles the OCR operations , returning you a collection of words that it is able to extract from that image . - Once you have the text , you can do whatever you want from it . <p> So , here 's where Watson Developer Cloud Services come into play . First , I used the Watson Language Translation Service to perform the translation . - When using this service , I make a request to my- Node.js app running on IBM Bluemix ( IBMs cloud platform ) . - The Node.js app acts as a facade and delegates to- the Watson service for the actual translation . <p> You can check out a sample on the web here : <p> Translate english to : <p> On the mobile client , @ @ @ @ @ @ @ @ @ @ something with the response . The example below uses the IMFResourceRequest API to make a request to the server ( this can be done in either Objective C or Swift ) . IMFResourceRequest is the MobileFirst wrapper for networking requests that enables the MobileFirst/Mobile Client Access service to capture operational analytics for every request made by the app . <p> Once you receive the result from the server , then you can update the UI , make a request to the speech to text service , or pretty much anything else . <p> To generate audio using the Watson Text To Speech service , you can either use the Watson Speech SDK , or you can use the Node.js facade again to broker requests to the Watson Speech To Text Service . In this sample I used the Node.js facade to generate Flac audio , which I played in the native iOS app using the open source Origami Engine library that supports Flac audio formats . <p> You can preview audio generated using the Watson Text To Speech service using the embedded audio below . Note : In this sample @ @ @ @ @ @ @ @ @ @ work in browsers that support OGG . <p> On the native iOS client , I download the audio file and play it using the Origami Engine player . This could also be done with the Watson iOS SDK ( much easier ) , but I wrote this sample before the SDK was available . <p> Cognitive computing is all about augmenting the experience of the user , and enabling the users to perform their duties more efficiently and more effectively . The Watson language services enable any app to greater facilitate communication and broaden the reach of content across diverse user bases . You should definitely check them out to see how Watson services can benefit you . <h> MobileFirst <p> So , I mentioned that this app uses IBM MobileFirst offerings on Bluemix . In particular I am using the Mobile Client Access service to collect logs and operational analytics from the app . This let 's you capture logs and usage metrics for apps that are live " out in the wild " , providing insight into what people are using , how they 're using it , and the @ @ @ @ @ @ @ @ @ @ <h> Source <p> You can access the sample iOS client and Node.js code at https : **38;6199;TOOLONG . Setup instructions are available in the readme document . I intend on updating this app with some more translation use cases in the future , so be sure to check back ! <p> Last week I attended IBM Insight in Las Vegas . It was a great event , with tons of great information for attendees . I had- a few sessions on mobile applications . In particular , my dev@Insight session on Wearables powered by IBM MobileFirst was recorded . You can check it out here : <h> Key takeaways from the session : <p> Wearables are the most personal computing devices ever . Your users can use them to be notified of information , search/consume data , or even collect environmental data for reporting or actionable analysis . <p> Regardless of whether developing for a peripheral device like the Apple Watch or Microsoft Band , or a standalone device like Android Wear , you are developing an app that runs in an environment that mirrors that of a a native @ @ @ @ @ @ @ @ @ @ the same . You write native code , that uses standard protocols and common conventions to interact with the back-end . <p> Caveat to #1 : You user interface is much smaller . You should design the user interface and services to acomodate for the reduced amount of information that can be displayed . <p> You can share code across both the phone/tablet and watch/wearable experience ( depending on the target device ) . <p> Using IBM MobileFirst you can easily expose data , add authentication , and capture analytics for both the mobile and wearable solutions . <p> Last week I had the opportunity to present to a great audience at- the- MoDev DC meetup group on " Smarter Apps with Cognitive Computing " . - In this session I focused on how you can create a voice-driven experience in your mobile apps . I gave an introduction to IBM Bluemix and IBM Watson services ( particularly the Watson language services ) , and demonstrated how you can integrate them into your native iOS apps . I also covered IBM MobileFirst for operational analytics and- remote logging to provide @ @ @ @ @ @ @ @ @ @ - Check out a recording of the complete presentation in the video below : <p> Back in February I had the opportunity to present " Enabling the Next Generation of Apps with IBM MobileFirst " at the DevNexus developer conference in Atlanta . - It was a great event , packed with lots of useful content . - Luckily for everyone who was n't able to attend , the organizers recorded most of the sessions which have just- been made available on Youtube . <p> here 's the session Description : Once your app goes live in the app store you will have just entered into an iterative cycle of updates , improvements , and releases . Each successively building on features ( and defects ) from previous versions . IBM MobileFirst Foundation gives you the tools you need to manage every aspect of this cycle , so you can deliver the best possible product to your end user . In this session , we 'll cover the process of integrating a native iOS application with IBM MobileFirst Foundation to leverage all of the capabilities the platform has to offer . <p> @ @ @ @ @ @ @ @ @ @ or out of your reach , then I have great news for you : They are not ! Last week , IBM elevated- several IBM Watson voice services from Beta to General Availability that means you can use them reliably in your own systems too ! <p> Let 's examine the two parts of the system , and see what solutions IBM has available right now for you to take advantage of <p> Transcribe audible signal to text transcript <p> Part one of this equation is converting the audible signal into text that can be parsed and acted upon . The IBM Speech to Text service fits this bill perfectly , and can be called from any app platform that supports REST services which means just about anything . It could be from the browser , it could be from the desktop , and it could be from a native mobile app . The Watson STT service is very easy to use , you simply post a request to the REST API containing an audio file , and the service will return to you a text transcript based upon what it @ @ @ @ @ @ @ @ @ @ this API you do n't  have to worry about any of the transcription actions on your own no concern for accents , etc Let Watson do the heavy lifting for you . <p> Perform a system action by parsing text transcript <p> This one is perhaps not quite as simple because it is entirely subjective , and depends upon what you/your app is trying to do . You can parse the text transcript on your own , searching for actionable keywords , or you can leverage something like the IBM Watson Q&amp;A service , which enables natural language search queries to Watson data corpora . <p> Riding on the heels of the Watson language services promotion , I put together a sample application that enables a voice-driven app- experience on the iPhone , powered by both the Speech To Text and Watson Question &amp; Answer services , and have made the mobile app and Node.js middleware source code available on github . <p> The app communicates to the Speech to Text and Question &amp; Answer services through the Node.js middelware tier , and connects directly to the Advanced Mobile Access @ @ @ @ @ @ @ @ @ @ network utilization ) and remote log collection from the client app on the mobile devices . <p> For the Speech To Text service , the app records audio from the local device , and sends a WAV file to the Node.js in a HTTP post request . The Node.js tier then delegates to the Speech To Text service to provide transcription capabilities . The Node.js tier then formats the respons JSON object and returns the query to the mobile app . <p> For the QA service , the app makes an HTTP GET request ( containing the query string ) to the Node.js server , which delegates to the Watson QA natural language processing service to return search results . The Node.js tier then formats the respons JSON object and returns the query to the mobile app . <p> The general flow between these systems is shown in the graphic below : <p> IBM Watson Speech QA for iOS Logic Flow <h> Code Explained <p> The code for this example is really in 2- main areas : The client side integration in the mobile app ( Objective-C , but could @ @ @ @ @ @ @ @ @ @ server/middleware implemented in Node.js . <h> Node.js Middleware <p> The server side JavaScript- code uses the- Watson Node.js Wrapper , which enables you- to- easily instantiate Watson services in- just a few short lines of code <p> The credentials come from your Bluemix environment configuration , then you just create instances of whichever services that you want to consume . <p> I implemented two methods in the Node.js application tier . The first accepts the audio input from the mobile client as an attachment to a HTTP POST request and returns a transcript from the Speech To Text- service : <p> Note : I am using the free/open Watson Healthcare data set . However- the Watson QA service can handle other data sets- these- require an engagement with IBM to train the Watson service to understand the desired data sets . <h> Native iOS Objective C <p> On the mobile side- were working with a native iOS application . My code is written in Objective C , however you could also implement this using Swift . I wont go into complete line-by-line code here for the sake of brevity , @ @ @ @ @ @ @ @ @ @ ViewController.m file . In particular , this is within the postToServer and requestQA methods . <p> You can see the flow of the application within the image below : <p> App Flow : User speaks , transcript displayed , results displayed <p> The native mobile app first captures audio input from devices microphone . This is then sent to the Node.js servers /transcribe method- as an attachment to- a HTTP POST request- ( postToServer method on line 191 ) . On the server side this delegates to the Speech To Test service as described above . Once the result is received on the client , the transcribed text is displayed in the UI and then a request is made to the QA service . <p> In the requestQA method , the mobile app makes a HTTP GET request to the Node.js apps /ask method ( as shown on line 257 ) . The Node.js app delegates to the Watson QA service as shown above . Once the results are returned to the client they are displayed within a standard UITableView in the native app . <h> MobileFirst Advanced Mobile Access @ @ @ @ @ @ @ @ @ @ decide to peruse the native Objective-C code : <p> Within AppDelegate.m you will see calls to IMFClient , IMFAnalytics , and OCLogger classes . These enable operational analytics and log collection within the Advanced MobileAccess service . 
@@106848890 @2248890/ <h> Aerial Photography with a GoPro Camera and Adobe Creative Cloud tools <p> My second article on aerial imaging with a remote controlled helicopter is now live in the March 2014 issue of Adobe Inspire ! - The first article focused on aerial videography and Adobe video tools . This time its all about aerial photography with a GoPro camera and DJI Phantom ( and how to bring these images to life with Photoshop and Lightroom ) . <p> Its so amazing and nice to see how those little GoPro Cameras can take breath-taking Aerial Photos and Videos . Myself I live in Las Vegas and happy to see the wonderful caption of The Las Vegas Strip . The editing with adobe creative cloud tools are perfect enough to bring the real beauty of potrait . I assume these are taken using one of the Dji Phantom Quadcopters is n't it ? Its quite a doubt whether parrot quadcopters can take quality aerial stuffs like this from reasonable remote distance . Anyways , keep the good work buddy . <p> Quadcopter and GoPro are a great combination for photographers , @ @ @ @ @ @ @ @ @ @ jungle , water or in the mountains . I did n't  knew about Adobe Creative Cloud integration , thanks for the informative article . **32;6239;TOOLONG <p> abhilash <p> i am a fun of Gopro camera and most of drone work great with GoPro camera . i was writ a guide about GoPro accessories on my blog http : //drone.weararena.com/ 
@@106848891 @2248891/ <h> Tag Archives : cordova <p> A few months back I released App-UI , a UI container toolkit for creating HTML experiences . - It gives you common " view navigator " paradigms for mobile experiences a view navigator stack that can push and pop views , a split view navigator , and a " sliding view " like Facebooks iOS experience . - All of which are created entirely with HTML , CSS , and JavaScript , so they are great for mobile web or PhoneGap applications . <p> Although I have n't committed many changes to App-UI recently , I 've been evaluating different types of visual effects and thinking through options for making App-UI more configurable. - Recently I stumbled across OriDomi , a toolkit that easily provides the ability to fold UI elements as though they were made of paper . - There have been a few interesting UI-folding proof of concepts and demos floating around lately ( such as this , this , and this ) , and I realized : <p> This can be done completely in HTML , CSS , &amp; JS <p> This @ @ @ @ @ @ @ @ @ @ for anyone to use <p> Below you can view the result of a days worth of tinkering : <p> I created this as an offshoot of the " Sliding View " demos It is not integrated into the core functionality since not everyone is going to want it in their apps , but it is pretty easy to add . - You can check out both of my demos/samples below , with full source code . - Just be forewarned : they only work on newer browsers that support CSS3 3D transformations and transitions ( WebKit for best results ) . - I 've tested it in Chrome on OS X and mobile Safari , and it works great . - It does not work on Android . <p> I made a few changes to the open source- OriDomi library , most importantly the ability to toggle between " live DOM " and the copied views that have the visual effects applied . - This allows me to have event listeners on DOM elements that can actually respond to user input . - I also added the ability to support DOM @ @ @ @ @ @ @ @ @ @ and added a " destroy " function to have it clean itself up from memory . <p> The first example is really basic. - It is an instance of the SlidingView that uses a modified version of the OriDomi toolkit to add a " fold " effect to the left sidebar. - I just wanted to show a basic use case , demonstrating the effect on " live " DOM elements . - You can check it out in your browser by clicking on the link/image below : <p> The second example is not quite as pretty , but definitely more complex . - It is an extension of my side-by-side ViewNavigators inside of a SlidingView , with the folding effect applied . - This example demonstrates complex objects with DOM manipulation having the fold effect applied. - You can also check this one out in your browser by clicking on the link/image below : <p> Based on my experimentation so far , I 've got big plans for App-UI. - I fully intend on rewriting how effects are applied to all of the container elements to enable them to be @ @ @ @ @ @ @ @ @ @ multiple visual effects options . <p> Yes , there are a few minor issues related to API changes from Apple . - We cant go into detail about them yet due to Apples non-disclosure agreement by which all Apple developers are bound . However , you will want to pay attention to PhoneGap/Cordova 2.1 release , and upgrade accordingly if your application is impacted. - Most existing apps wont be affected , but a small number may encounter an- issue- ortwo. - Be sure to test your apps on the iOS 6 beta to make sure they are ready . - There are workarounds to these issues without having to upgrade to PhoneGap 2.1 , but all new apps targeting iOS 6 will definitely want to use PhoneGap 2.1 or newer . <p> Earlier this month the PhoneGap team held the first PhoneGap day . - This was in part to celebrate the release of PhoneGap 2.0 , but more importantly to bring together members of the PhoneGap community to share and learn from each other . - There are great recaps of PhoneGap Day from RedMonk , as well @ @ @ @ @ @ @ @ @ @ services announced on PhoneGap Day was- emulate.phonegap.com. - Emulate.phonegap.com enables an in-browser simulator for developing and debugging PhoneGap/Cordova applications , complete with Cordova API emulation . - It is built off of the Ripple Emulator , which itself is open source and may even be contributed to the Apache Cordova project . <p> Once launched , the URL that you want to simulate will be displayed within the Ripple operating environment view . <p> Note : This only works with assets that are on a live URL . You can use a local http server with references to localhost , however the emulator will fail if you try to access your application directly from the local file system using a file : // URI . <p> Update : You can enable access to local files by changing a few settings on the Ripple emulator . - See the first comment on this post for additional detail . <p> ( click for full-size image ) <p> The emulator environment gives you the ability to emulate PhoneGap events and API calls , without having to deploy to a device or run inside @ @ @ @ @ @ @ @ @ @ emulator . - Not only can you simulate the PhoneGap/Cordova API , but you can also use Chromes debugging tools to test &amp; debug your code complete with breakpoints , memory inspection , and resource monitoring . This is a handy development configuration. - It enables app development within a rich debugging environment that is familiar , fast &amp; easy to use . This does not replace on-device debugging however nothing will replace that . - On-device debugging is extremely important ; this helps increase your productivity as a developer . <p> So how do you use this environment ? The environment will handle Cordova API requests , and you can also simulate device events . <p> First , the " Devices " panel In this panel you can select a device configuration Everything from iOS , to Android , to BlackBerry . - Changing the device configuration will not only change the physical dimensions , but will also change Device/OS/user agent settings reported by the application . - Here you can also select the device orientation , which will change the visual area within the simulator . <p> Within @ @ @ @ @ @ @ @ @ @ you wish to emulate . - With respect to PhoneGap applications , you will want to choose " Apache Cordova " , and then select the API version that you are using . - By default , it uses " PhoneGap 1.0.0 ? , however you can chose " Apache Cordova 2.0.0 ? to get the most recent version . - The Ripple emulator also simulates BlackBerry WebWorks and mobile web configurations as well . <p> The " Accelerometer " panel can be used to simulate device **25;6273;TOOLONG events . - Just click and drag on the device icon ( the gray and black boxes ) , and the icon will rotate in 3D. - As you drag , accelerometer events will be dispatched and handled within your application . - From here , you can even trigger a " shake " event . <p> The " Geolocation " panel enables you to simulate your geographic position within your PhoneGap/Cordova application . - You can specify a latitude , longitude , altitude , speed , etc - You can even drag the map and use it to specify your geographic @ @ @ @ @ @ @ @ @ @ panel will be reported when using **42;6300;TOOLONG . <p> The " Config " panel is a graphical representation of your PhoneGap BuildConfig.xml file . - You can use this to easily view/analyze what 's in your application configuration . <p> The " Events " panel can be used to simulate PhoneGap specific events , including " deviceready " , " backbutton " , " menubutton " , " online " , and " offline " ( among others ) . - Just select the event type , and click on the " Fire Event " button . <p> As I mentioned earlier , this wont replace on-device debugging . - It also wont handle execution of native code for PhoneGap native plugins , however you can test/develop against the JavaScript interfaces for those native plugins. - Emulate.phonegap.com- will definitely help with development of PhoneGap applications in many scenarios , and is a nice complement to the- Chrome Developer Tools . <p> Yesterday Raymond Camden and I hosted an open session on PhoneGap . It was an opportunity for the community ( you ) to ask us questions . There was a @ @ @ @ @ @ @ @ @ @ of these sessions in the near future . Thanks to everyone who attended ! - Check these links to see the full Q&amp;A transcript , and the full chat transcript , or download them directly from Rays blog. - ( Please excuse the typos , we were answering these live , in real time , without any edits . ) <p> I see questions and comments all the time with the general sentiment " it looks nice , but who really uses PhoneGap/Apache Cordova ? " . There is no way to create a definitive list of everyone who uses it , but the general answer is " more people than you think " . Here are a few organizations that you might recognize who are using either PhoneGap or Apache Cordova in their cross-platform mobile solutions and/or tools . ( PhoneGap is a distribution of Apache Cordova ) <h> Microsoft <p> Microsoft is involved with core Apache Cordova development ( specifically for the Windows Phone platform ) . - Not only are staff from Microsoft committers for the core Apache Cordova project , Microsoft has also used PhoneGap on @ @ @ @ @ @ @ @ @ @ includes the XBox-Live integrated gaming application Halo Waypoint , for both iOS and Android . - Check out Halo Waypoint in the video below , it looks awesome : <h> Adobe <p> I think most people already know how deeply involved Adobe is with PhoneGap , but I 'll try to recap quickly - In late 2011 , Adobe acquired Nitobi , the creators of PhoneGap , and contributed PhoneGap to the Apache Software Foundation as the Apache Cordova project . - Adobe has resources dedicated to furthering PhoneGap and is dedicated to the success of the platform . - Not only are we helping develop and mature PhoneGap/Apache Cordova , we also build some of our own applications with it . - ( Maybe I 'll be able to talk about those some day . ) <h> Zynga <h> Logitech <p> Logitech- used PhoneGap to develop the Logitech Squeezebox Controller application , which uses your home wifi connection to control a Squeezebox Internet radio device from your smart phone . - You can read more about this application on the PhoneGap application showcase , or download it now for iOS or @ @ @ @ @ @ @ @ @ @ ? What about these , among many others ? 
@@106848892 @2248892/ <h> Category Archives : HTML5 <p> here 's a video from the " What Developers Love and Hate about iOS , Android , Windows and HTML5 ? panel discussion that I took part in during MoDev East- back in December . - The panelists represented native app developers , HTML/web developers , and myself-representing the HTML/hybrid app paradigm . <p> I thought it was a great discussion , and nearly everyone acknowledged the benefits of choosing a hybrid solution such as- PhoneGap . ( That 's me in the brown jacket . ) <p> I am asked all the time " How do I get started developing PhoneGap applications ? " . My normal answer is to advise people to check out the PhoneGap Getting Started Guides , which provide a great starting point for every platform . However after further thought , I 'm not sure this is always what people are asking . Rather than " how do I get started ? " , I think people are often looking for insight into the workflow for developing PhoneGap applications . Everything from tools to developer flow , to getting the @ @ @ @ @ @ @ @ @ @ for setting up the initial project structure , but once you get that setup , you might be wondering " what do I do next ? " . In this post , I 'll try to she 'd some light on the workflow and tools that I use when developing PhoneGap applications . <h> Know What You 're Going To Build Before You Build It <p> First and foremost it is essential to have at least some kind of idea what you are going to build before you build it . If you just start hacking things together without a plan , the final result is seldomly great . Complete ( pixel perfect ) UI/UX mockups are fantastic , but you do n't  have to have a fully polished design and screen flow . Just having wireframes/sketches are a great start . Heck , even a sketch on a napkin is better than starting with nothing . <p> The UX design/wireframes help you understand what you application should be doing from the users perspective , which in turn helps you make decisions on how you tackle a project . This can be purely @ @ @ @ @ @ @ @ @ @ you should position DOM elements and/or content . Or , it can help you gauge your projects technical complexity How many " moving parts " do you have , how much of the app is dynamic or asynchronus , or how do different visual elements need to work together ? You can leverage this design/mockup to analyze the needs of your application and determine if a particular framework/development methodology is a best fit ( Bootstrap , Backbone.js , Knockout.js , Sencha , jQuery Mobile , Angular.js , etc ) . <p> When working with a designer , I use Adobes Creative Suite Tools for pretty much everything wireframes , UI/UX designs , chopping up assets , etc I 'm currently working on a project that was designed by the talented- Joni from Adobe XD . Joni designed everything in Creative Suite , and I 'm using Photoshop to view screen flows and extract UI assets for the actual implementation . <p> UI Mockups in PhotoshopScreen Flow in Photoshop <p> Note : This app will also be free and open source as a sample/learning resource for PhoneGap , including all of the design @ @ @ @ @ @ @ @ @ @ , once the app is- available- in the app stores . <p> If you are n't  a " graphics person " , or do n't  have creative suite , there are a bunch of other tools that you can use for wireframing and/or sketching ( but cmon , Creative Cloud is only $50 a month ) . Here are several Ive used with great success , but this is not a comprehensive list at all : <p> OmniGraffle- A drag &amp; drop wireframing tool for OS X. This is fantastic for wireframing or documenting screen flows . In fact , the screen flow image shown in Photoshop above was originally composed in Omnigraffle , using the mockups created in Photoshop . <p> Visio- A powerful drag &amp; drop wireframing/design tool for Windows much like OmniGraffle , but for windows . <p> PowerPoint- or Keynote- - These are n't  just for presentations . They can be really useful for putting together screen flow diagrams , or annotating images/content . <p> Often people like to sketch out ideas &amp; wireframes on their tablets , here are a few tools that I use @ @ @ @ @ @ @ @ @ @ taking notes and sketching . I 'm partial to this one b/c used to be on the dev team , and I wrote a good chunk of the graphics sketching logic . <p> There are a bunch of other tablet sketching apps out there , but I have n't used most of them . <h> Coding Environment <p> Coding environments are a tricky subject . There is no single solution that meets the exact wants and needs for everyone . Some people chose lightweight text editors , some people chose large-scale IDEs , some people use designer-centric tools , and many of these choices are- dependant- upon which operating system you use or your background as a designer or developer . Since PhoneGap applications are really just editing HTML , CSS &amp; JavaScript , you can use whatever editor you want . In fact , I know a several people that use vim as their primary editor . <h> Large-Scale IDEs <p> I 'm a bigger fan of of using a complete IDE ( integrated development environment ) than I am of a lightweight editor , simply b/c IDEs tend to have hooks @ @ @ @ @ @ @ @ @ @ startup time , but there is no startup time if you leave it open all the time . <p> There are a few catches when talking about IDEs with PhoneGap . The first is that if you want to deploy anything locally to devices ( without using PhoneGap Build ) , you have to delpoy using the IDE for the particular platform that you are- targeting . That means Xcode for iOS , Eclipse for Android , or Visual Studio for Windows Phone , etc However if you wish , you can use your editor of choice , and just use the IDE to deploy to devices locally . You can even share source code across several IDE installations using symlinks ( which I describe here ) . I very often use this type of a configuration to share code between Xcode , Eclipse , and WebStorm . <p> My preference for coding PhoneGap applications is to use- WebStorm by JetBrains . WebStorm has great code-hinting ( even for your own custom JS and 3rd party libraries ) , great refactoring , hooks into Git , CVS , or SVN @ @ @ @ @ @ @ @ @ @ WebStorm IDE <p> I tend to use this as my primary coding tool , then switch to Eclipse or Xcode when I want to locally deploy to a device for testing . When using PhoneGap Build to simplify cross-platform compilation , I just push the code to git , then recompile via PhoneGap Build . <p> I 'm not a fan of Xcodes HTML/JS editing , and havent found an HTML/JS plugin for Eclipse that I really like . To target Windows devices , I use Visual Studio . <h> Lightweight Editors <p> I 'm a bigger fan of larger IDEs than lightweight editors , but- Adobe Edge Code ( also known as Brackets ) is a great lightweight editor for quick edits. - Edge Code/Brackets is an open source HTML/JS editor that supports live editing in the browser and inline editors for CSS styles , without leaving your HTML files . If you tried Edge Code Preview 1 , but werent sold on it , you should try Edge Code Preview 2 . The team has come a long way very quickly . Its fast , easy to use , and @ @ @ @ @ @ @ @ @ @ . I sometimes use this for quick edits . <p> There are tons of other lightweight editors out there , and everyone has their favorite . As long as you 're happy with the tool , and it can edit text ( HTML , CSS , JS ) files , you can use it to build PhoneGap applications . <h> Designer-Friendly Editors <p> I 'm not necessarily the primary target for Dreamweaver , but it has some nice features . Dreamweaver gives you a great programming environment plus a WYSIWYG editor for HTML experiences . It also features PhoneGap Build integration directly inside the coding environment . If you 're used to Dreamweaver for creating web experiences , you can continue to use it and target mobile apps as well . <p> Adobe Dreamweaver <h> Debugging Environments <p> Yes , that is plural Debugging Environments . Due to the cross-platform nature and PhoneGaps leveraging of native web views for each platform , debugging PhoneGap applications can sometimes be tricky . Here are some tips that will make this significantly easier . <h> The PhoneGap Emulator <p> The PhoneGap Emulator is my primary development/debugging @ @ @ @ @ @ @ @ @ @ emulator leveraging the Google Chrome browser and the Ripple Emulation Environment . The PhoneGap Emulator runs inside of Google Chrome , and provides emulation of PhoneGaps core APIs . Since it is built on top of Chrome , it enables you to leverage Chromes Developer Tools , which in my opinion are second to none for web/application development . This is a highly-productive developer environment . <p> PhoneGap Emulator in Google Chrome <p> here 's why I like the PhoneGap/Ripple/Google Chrome development environment : <p> First , - this combination enables you to emulate most core PhoneGap APIs without leaving the desktop environment . It enables you to test various APIs including geolocation ( with simulated locations ) , device events ( deviceready , back , etc ) , sensor events ( accelerometer , compass ) , and even let 's you test with different device aspect ratios all without having to push anything to an actual device . This saves a lot of time in development iterations . You can read about the supported Ripple emulator features here . <p> Second , Chromes Developer Tools are awesome . Here are just @ @ @ @ @ @ @ @ @ @ app , live within the emulation environment : <p> Analyze all resources consumed by your app , via the resources panel . This includes all scripts , images , html files , cookies , etc it even includes insight into any local data stored via PhoneGaps local storage database ( WebSQL implementation ) . <p> View/query all local databases within your app . You can write your own queries to view/alter data in the WebSQL database . Thanks to Ray for sharing this , its not immediately intuitive . <p> Debug JavaScript with the Scripts/Sources Panel . You can set breakpoints in JS execution , inspect &amp; alter values in JS objects in-memory , and view details and line numbers for any exceptions that occur . <p> Use the console to monitor console.log() statements , inspect properties of objects in memory , or execute arbitrary JavaScript whenever you want . <p> The PhoneGap Emulator enables developers to be extremely productive with development , however I can not emphasize enough that on-device testing is critical for having a successful app . On-device testing can expose performance problems or browser rendering variances @ @ @ @ @ @ @ @ @ @ <h> On-Device Remote Debugging <p> As I mentioned above , on-device testing is critical for successful applications . iOS and BlackBerry have an advantage over other platforms b/c the latest developer tools allow you to remotely debug content live on a device . <p> Since the release of iOS 6 , you can debug content in the iOS simulator using Safaris Developer Tools . Safaris developer tools give you many of the same debugging capabilities that I mentioned above for Chrome . <h> Remote Debugging with Weinre <p> Not every platform supports live remote debugging , especially older versions . Weinre ( pronounced winery ) is a remote web inspector that allows you to inspect/edit DOM and CSS elements on remote devices . Basically , you include some JavaScript in your app , and it communicates back to a server that will tell you what 's happening inside of the app running on the mobile device . It wo n't give you full debugging capabilities like JS breakpoints and memory inspection , but its better than nothing . You can use Weinre by setting up your own instance , or by leveraging @ @ @ @ @ @ @ @ @ @ Else Fails <p> If you 're still debugging your apps , and the solutions mentioned above do n't  work , you can always resort to plain-old " alert() " statements to pop up debug messages , or use " console.log() " statements to write to system logs . <p> On Android , all- console.log ( ' ... ' ) ; - messages will appear as printouts in the command-line tool- logcat , which is bundled with the Android SDK and integrated into the Android Eclipse plugin . <p> On BlackBerry , all- console.log ( ' ... ' ) ; - are printed to the BlackBerrys Event Log . The Event Log can be accessed by pressing- ALT + LGLG . <p> On iOS , all- console.log ( ' ... ' ) ; - are output to the Xcode Debug Area console . <h> Building PhoneGap Apps <p> The PhoneGap getting started guides will point you to the right direction for getting started with a particular platform . If you are just targeting iOS , you can use Xcode for building . If you are just targeting Android , you can use @ @ @ @ @ @ @ @ @ @ up and running . <p> However , this process gets much more complicated when targeting multiple platforms at once . When I have to do this , PhoneGap Build becomes really , really handy . <p> PhoneGap Build allows you to either upload your code , or point to a Git repository . PhoneGap Build will then pull your code and build for 7 different platforms , without you having to do anything special or setup multiple environments . All that you 'll have to do is install the cloud-compiled binaries on your device . You can do this by copying/pasting a URL to the binaries , or by capturing a QR code that will directly link to the compiled- application- binary . <p> One other advantage of PhoneGap build is that it let 's designers/developers build mobile applications without having to install any developer tools . If you want to compile a PhoneGap app for iOS , but are on Windows just use PhoneGap build and you wo n't need Xcode or a Mac . <h> PhoneGap UI/Development Frameworks <p> Probably the most common PhoneGap question that I get asked is " @ @ @ @ @ @ @ @ @ @ you 've been waiting for me to answer this , do n't  hold your breath . It is impossible to be prescriptive and say that one solution fits all use cases for every- developer . <p> When people ask me this , I like to paraphrase- Brian Leroux from the PhoneGap team : " Use HTML , it works really well . " <p> I think people often overlook the fact that PhoneGaps UI renderer is a system web view . Anything that is valid HTML/CSS content can be rendered as your applications user interface . This could be something incredibly simple , like text on the screen , or it could be incredibly creative or complex . The important factor is that you need to focus on a quality user experience . If you 're worried about your UX , and are worried that Apple may reject your app , then read this article where I explain Apple rejections in detail . <p> HTML/JS developers come from many different backgrounds , with varying degrees of programming expertise . Some frameworks appeal to some people , other frameworks appeal to other people @ @ @ @ @ @ @ @ @ @ frameworks popping up every week . It would be a disservice to all people who use PhoneGap for us to proclaim that we should only use one singe framework . <p> There are lots , and lots , and lots more options out in the HTML/JS development world I 'm not even taking into account JavaScript generating tools and languages like CoffeeScript , TypeScript , or others <p> Today Raymond Camden and I hosted another open session on PhoneGap- as part of an Adobe TechLive event . - These sessions are an opportunity for anyone to stop in and ask us questions . <p> The Q&amp;A transcript from todays session is below . - Thanks to everyone for sticking around , and bearing through our technical difficulties The normal Q&amp;A pod was n't working for some reason , so we had to improvise. - Well make sure this is working for next time . <p> This was our third event , and we 've had a great turnout so far , so we will be holding these open sessions once a month . - If you werent able to make it this time @ @ @ @ @ @ @ @ @ @ the- Adobe TechLive- page for future events . <p> About two weeks ago I had a random thought " Wouldnt it be fun to build a free Halloween sound effects app using PhoneGap ? " ( I tend to get excited for any holiday ) . I had already done a lot of work using sounds in PhoneGap apps with my LowLatencyAudio native plugin , so the groundwork had already been laid . - Next thing you know , I 'm searching for sound files ( freesound.org is a fantastic resource for creative commons sound effects ) . Write some JS code here , add some CSS styles code there , and next thing you know I had a complete app . The best part this app went from idea to App Store submission in under 48 hours- ( yes , I did sleep during this 48 hour period ) . Add one more day , and its ready for Android . - That is the power of PhoneGap . <p> Now , I 'd like to introduce you to " Instant Halloween " . Instant Halloween is a Halloween-themed sound effects @ @ @ @ @ @ @ @ @ @ : <p> Instant Halloween is a fun holiday application to help you scare the pants off of your friends ! Use it to create a creepy ambiance , or play scary Halloween sound effects to liven up any situation . This app is best experienced turned up really loud ! <p> Check it out in the video below , and make sure you have your sound turned on ! - <p> It is definitely best when turned up really loud . There 's nothing like an ear-splitting shriek combined with the thumping bass of the heartbeat and thunder in the background . I connected it to a home theater with a decent subwoofer , and it was awesome . Setup some speakers in the bushes next to your front door , and you could use this to really scare some children on Halloween. - ( I take no responsibility for your actions . ) <p> You can download it for free today for both iOS and Android devices : <p> - <p> It supports both phone and tablet form factors , which you can see in action here : <p> I @ @ @ @ @ @ @ @ @ @ a responsive layout . - However , I ran into a few issues with column layout and content wrapping , so I ended up going with a totally custom solution . <p> Of course , just releasing a new PhoneGap app in the ecosytems is n't enough . - As usual , I 'm releasing the full source of this application for anyone to explore and learn from . - All HTML/JS/CSS source code between the iOS and Android versions is exactly the same ( note : I 've submitted an updated build pending approval for iOS to bring it in parity with the Android build ) . <h> Sounds <p> The audio assets are not redistributed in GitHub due to copyright law . However , all of these assets are available under Creative Commons licenses from- freesound.org . Be sure to review the individual licenses for each file before any attempt to use them in any commercial or non-commercial work . You can access specific download links in the- readme . <p> All sound files were converted to 16bit 22050 Hz MP3 files . You can use a higher quality/bitrate if you @ @ @ @ @ @ @ @ @ @ and memory issues on older/low-end Android devices . <h> Attribution <p> The font used within this application is- " Creepsville " , available for free . An embedded font was chosen over a hosted web font so that the font will still work in offline scenarios . The TTF font was converted for in-browser use with- Font Squirrel Generator . <p> I normally do n't  write posts to talk about a specific product from another company , but this is the first product that I have seen which let 's you do automated UI testing in PhoneGap applications , so I figured I 'd share <p> Automated testing is often a critical step in enterprise application development . I 'm sure most everyone out there is familiar with automated unit testing , where you write specific code to test and validate business logic or processes within your application . This can be done on the client side or on the server side , and there are lots of existing tools and languages that can be leveraged for automated unit testing . - For PhoneGap applications , you can write unit tests that evaluate @ @ @ @ @ @ @ @ @ @ using a number of tools , but these only test JS execution and logic . <p> Automated UI testing enables you to test the GUI of your application . It allows you to make sure proper buttons or controls are displayed wherever they are supposed to be displayed , that they have the proper user interaction , and generally that your applications user interface is functioning properly . There are numerous tools to enable automated UI testing on- the- desktop by recording user input and turning that user input into- reproducible- scripts . However on mobile devices , there are n't  as many options . You can write code to create test scripts for both iOS and Android , but writing these tests for PhoneGap applications could get challenging . <p> Recently , while attending the 360iDev conference , I was introduced to Teleriks Test Studio for iOS- applications . At first glance , I thought " oh , that 's cool , automated ui tests for native iOS apps " , but doubted it would work with PhoneGap apps . It turns out I was wrong . You can use @ @ @ @ @ @ @ @ @ @ web , or hybrid iOS applications . Below is a video of it in action with the " Walkable " PhoneGap app that I recently released . - You can record test scripts , and play them back at any time , and it will evaluate whether the test was a success of failure if the UI responds as it did in the original script . <p> Test Studio allows you to capture user input in your iOS application and play it back as a script without touching or interacting with the application yourself . - There are just a few steps to get it up and running : <p> Configure a testable build of your application Note : Do not follow the " quick instructions " in the confirmation email you get when you download the SDK it left out a step , follow these instructions instead . <p> Deploy your app to your device . <p> Launch Test Studio , and follow the UI prompts to " Record A Test " . <p> At any time , access and execute any existing script . <p> However , be @ @ @ @ @ @ @ @ @ @ noticed in testing my own applications that when executing recorded scripts , Test Studio often attempts to execute steps before my HTML DOM elements actually exist . - This is a common issue in automated UI testing , - especially- when testing single-page dynamic HTML applications regardless of whether they are PhoneGap or in a desktop browser . You have to be very deliberate and precise when you record your steps , and at certain points , I had to manually introduce a wait period in the script to enable enough time for my HTML DOM elements to be created . - In Test Studio , there are a few ways you can do this : <p> Introduce a finite " wait " duration <p> Test to see if the HTML elements exist before continuing with script execution <p> You can also go back and edit/add steps to existing tests . - It can be a painstaking process to record accurate UI test scripts , but can be useful for QA processes for many applications . 
@@106848893 @2248893/ <h> Category Archives : PhoneGap <p> I 'd like to express a huge Thank You- to everyone who attended PhoneGap Day in Portland last week , and to Colene and the team that put everything together ! The day was loaded with fantastic presentations , and great community interaction tons of information , tons of great questions , and tons of great people ( oh , and beer it would n't be PhoneGap Day without beer ) . <p> My session was about the integration of PhoneGap with hardware . Basically , it was an overview and exploration how you can use native plugins to extend the capabilities of PhoneGap applications and interact with device peripherals . This enables new interaction paradigms , and helps evaluate and evolve what is attainable with web-related technologies . <p> In this session , I covered two use cases The first use case is the use of a pressure-sensitive stylus for interacting with a PhoneGap application on iOS . The second use case is integration of a Moga gamepad with a PhoneGap application on Android . In both cases , the application experience is augmented @ @ @ @ @ @ @ @ @ @ possible for the user to interact and engage with the application content and context . <h> Pressure Sensitive Sketching in PhoneGap <p> In this example , - the pressure-sensitive Pogo Connect Stylus uses a low energy Bluetooth 4 connection to relay touch/pressure information back to the PhoneGap application . This makes for a unique drawing and sketching experience powered with the HTML5 Canvas element . I 've written about this example previously Check out the video below to see it in action , and read the blog post for technical details and source code . <h> Moga Gamepad <p> The second example that I explored is a PhoneGap native plugin that is used to handle input from a Moga game controller inside of a PhoneGap application on Android . <p> This implementation is intended to be a proof of concept demonstrating how you could integrate the gamepad within your application . It currently only supports input from the joysticks ( axisX and axisY ) and the A and B buttons , and it does not handle all possible input from the controller . <p> This implementation is adapted directly from the @ @ @ @ @ @ @ @ @ @ download at : : - http : **34;6384;TOOLONG 123434 @qwx983434 <p> The game is based on the Universe- prototype that was used as a sub-game inside of the MaxMe app for the recent Adobe MAX conference . I make no guarantees about the code for this game , it was in a huge rush ! <p> I am very excited to share that the Fresh Food FinderPhoneGap application which I built last summer was recently featured in the June 2013 issue of SELF Magazine . This app was built as a demonstration/sample application for PhoneGap , but has gained popularity for being an easy way to find farmers markets by leveraging the USDA farmers markets directory- and has appeal to the general public . <p> In fact , Ive seen so many downloads that I am working on an updated version of it based on user feedback , with more recent data ( live services instead of embedded ) , and will be leveraging the awesome topcoat.io CSS framework for styling . <p> Fresh Food Finder in SELF Magazine <p> SELF Magazine has huge readership numbers ( almost 1.5 million @ @ @ @ @ @ @ @ @ @ was just a very small callout in the magazines calendar , this has driven both app downloads and visitors to my blog ( peaked at 348 downloads per day ) ! <p> App downloads May 1- July 8 <p> This also brought the app higher up in the iOS market ranks ( peak #57 in the " Food &amp; Drink " category ) : <p> App store ranking May 1- July 8 <p> You can download the Fresh Food Finder app- today in the following markets : <p> Recently Ive undertaken some explorations with fellow evangelist- Kevin Hoyt , trying to determine how far we can push PhoneGap- applications with devices and physical computing . Turns out , you can push things really far and now I 'm delighted to share one of the experiments that we 've been pursuing . <p> Ive been asked on more than one occasion , can you access Bluetooth devices in PhoneGap applications . The answer is YES , you can . There is not a specific " Bluetooth API " in PhoneGap , however you can create native plugins to access any native library . @ @ @ @ @ @ @ @ @ @ native interface ( written in the native language ) , and a JavaScript interface . The native and JavaScript interfaces can leverage the PhoneGap native to JavaScript bridge for bidirectional communication . <p> In this exploration , we researched whether or not you can use a pressure sensitive stylus with a PhoneGap application . Again , the answer is YES , you can . Check out the video below to see a sample application in-action . This example demonstrates the use of a TenOne Pogo Connect Stylus inside of a PhoneGap application . <p> Note : This is not connected to Project Mighty- in any way Kevin and I started exploring completely separately from the big announcements at MAX . <p> The Pogo Connect stylus leverages Bluetooth 4 Smart ( low energy ) connectivity to communicate with the device , and provides pressure sensitivity and a physical button for the user to interact with . The JavaScript interface does n't  interact directly with with the Bluetooth connection . Instead , I leveraged TenOnes Pogo Connect SDK and created a JavaScript bridge layer to delegate pen interaction from the SDK to @ @ @ @ @ @ @ @ @ @ tricks to get this working . First , the SDK is designed to accept touch input at the native layer , and determine whether or not that touch is from the pen . When using the SDK ( in Objective-c ) , you are supposed to implement the touchesBegan , touchesMoved , touchesEnded , and touchesCancelled functions for a view : <p> The first catch with implementing this inside of a PhoneGap application is that you cant override the touch handlers of a web view on iOS . Luckily , there is another way ! I leveraged a UIGestureRecognizer instance to intercept the touch input that is received by the web view , determine if the touches were from the PogoConnect Stylus , and if so , then delegate that input back to the JS layer . <p> UIGestureRecognizer is normally used as a base class for creating custom gestures in iOS applications . If has everything you need to handle touch input , and it gives you that information without having to subclass an actual view . This means you can attach it to any UIView instance . Since @ @ @ @ @ @ @ @ @ @ this , this plugin can be implemented in a PhoneGap native plugin without having to modify *any* code inside the PhoneGap framework . <p> So , here 's how I did it Once the PhoneGap plugin is initialized on load , I create a gesture recognizer instance and attach it to the PhoneGap applications web view . Whenever touch input is received by the gesture recognizer , that input is passed back to the PhoneGap plugin instance , which executes JavaScript on the web view to pass that information to the JavaScript layer in real time as it is received . <p> On the JavaScript layer , stylus/pen input is dispatched to the application as custom events on the window.document object . To subscribe to pen input in JavaScript , you just add event listeners for the Pogo Connect events that I defined in the native plugins JavaScript file . <p> In JavaScript , you can do whatever you want within your application once you receive this information . <p> I created two sample applications to test this functionality . The first is a very basic app that simply outputs the @ @ @ @ @ @ @ @ @ @ intent with this app was really just to prove the concept and determine if it was actually possible to receive and respond to information from the stylus . <p> Basic Pressure Sensitivity Detection in PhoneGap <p> Once I proved it could be done , the next logical step was to create an app that actually takes advantage of the pressure sensitivity information . So , I made a sketching app . <p> Pressure Sensitive Sketching in PhoneGap <p> I started off by expanding on the drawing logic from my Lil Doodle PhoneGap application . This uses a requestAnimationFrame interval to render content in an HTML Canvas element using a " brush image " technique . Next , I added logic to vary the opacity and stroke size based on the pressure information received from the pen plugin , and a few other options to change the pen tip/brush shape and color . <p> The pressure sensitive stylus gives a few interesting interactions that you would n't get without the hardware : <p> First , the obvious , you know the pressure being applied to the pen tip , and the app @ @ @ @ @ @ @ @ @ @ extra input method . The button on the pen allows you to interact with the device without having to actually touch the device . I used the button in two ways : first , if the button is pressed and held , the pen erases instead of draws . Second , if you double-tap the pen button , it brings up the drawing options . These options are where I placed controls to modify the pen color and stroke . <p> Third , the plugin provides bidirectional communication with the Stylus . When you change the pen color , the LED on the pen will display the selected color for a few seconds . <p> Note : This is iOS only The third-party PogoConnect SDK is for iOS devices only . This example will also ONLY work if you have the PogoConnect Stylus . It does not support other stylus devices or finger-only drawing . <p> One of my first tasks upon downloading Android Studio was to get a PhoneGap app up and running in it . here 's how to get started . Note : I used PhoneGap 2.7- to @ @ @ @ @ @ @ @ @ @ however you could use the same steps ( minus the CLI create ) to import an already-existing PhoneGap application . Be sure to backup your existing project before doing so , just in case you have issues ( Android Studio is still in beta/preview ) . <p> Next , you 'll have to select the directory to import . Choose the directory for the PhoneGap project you just created via the command line tools . <p> Once you click " OK " , you will proceed through several steps of the import wizard . On the next screen , make sure that " Create project from existing sources " is selected , and click the " Next " button . <p> You will next specify a project name and project location . Make sure that the project location is the same as the location you selected above ( and used in the PhoneGap command line tools ) . I noticed that the default setting was to create a new directory , which you do not want . Once you 've verified the name and location , click " Next " . <p> @ @ @ @ @ @ @ @ @ @ everything checked ) , and click " Next " . <p> Android Studio should now open the full IDE/editor . You can just double click on a file in the " Project " tree to open it . <p> To run the project , you can either go to the " Run " menu and select " Run project name " , or click on the " Run " green triangle icon . <p> This will launch the application in your configured environment ( either emulator or on a device ) . You can see the new PhoneGap application running in the Android emulator in the screenshot below . If you 'd like to change your " Run " configuration profile , go to the " Run " menu and select " Edit Configurations " , and you can create multiple launch configurations , or modify existing launch configurations . <p> Next week I 'll be representing Adobe at GDC 2013 , and demonstrating how Adobe Creative Cloud , - PhoneGap , and PhoneGap Build- can be great tools for building casual gaming experiences. - In preparation , I 've been working on @ @ @ @ @ @ @ @ @ @ HTML games packaged with PhoneGap . <p> and now I 'd like to introduce you to PhoneGap Legends . PhoneGap Legends is a fantasy/RPG themed demo that leverages HTML DOM animation techniques and targets webkit browsers . I was able to get some really outstanding performance out of this example , so be sure to check out the video and read the details below . The name " PhoneGap Legends " does n't  mean anything ; I just thought it sounded videogame-ish and appropriately fitting . <p> PhoneGap Legends <p> This game demo is an infinitely-scrolling top-view RPG themed game that is implemented entirely in HTML , CSS , and JavaScript . There is a scrolling background , enemy characters , HUD overlays , and of course , our " protagonist " hero all that 's missing is a story , and general game mechanics like interacting with sprites . <p> Again , I was able to get some *really outstanding performance* out of this sample , so I wanted to share , complete with source code , which you 'll find further in this post ( and I encourage you to share @ @ @ @ @ @ @ @ @ @ below to see the game in action on a variety of devices . Every single bit of this is rendered completely with HTML , CSS , and JavaScript there are no native portions of the application . <p> Update 3/24 : - If you 'd like to test this out on your own devices , you can now access it online at- LONG ... - However , it will still only work on webkit browsers ( Chrome , Safari , Android , iOS , etc ) , and is optimized for small-device screens . - If you attempt to use this on a very large screen , you 'll probably see some sprite clipping . <p> Disclaimer : This sample app is by no means a complete game or complete game engine . Ive implemented some techniques for achieving great performance within a PhoneGap application with game-themed content , but it still needs additional game mechanics . I also wrote this code in about 2 days it needs some additional cleanup/optimization before use in a real-world game . <h> Source <p> Full source code for this demo application is available on GitHub @ @ @ @ @ @ @ @ @ @ **37;6420;TOOLONG . I will be making a few updates over the next few days in preparation for GDC next week , but for the most part , it is stable . <p> In general , the DOM is as shallow as possible for achieving the desired experience . All " sprites " , or UI elements are basic DOM nodes with a fixed position and size . - All DOM elements have an absolute position at 0,0 and leverage translate3d for their x/y placement . - This is beneficial for 2 reasons : 1 ) It is hardware accelerated , and 2 ) there are very , very few reflow operations . - Since all the elements are statically positioned and of a fixed size , browser reflow operations are at an extreme minimum . <p> The background is made up a series of tiles that are repeated during the walk/movement sequence : <p> Sprite Sheet for Background Tiles <p> In the CSS styles , each tile is 256+256 square , with a background style that is defined for each " type " of tile : <p> The content displayed @ @ @ @ @ @ @ @ @ @ applied using sprite sheets and regular CSS background styles . Each sprite sheet contains multiple images , the background for a node is set in CSS , and the position for each image is set using the " background-position " css style . - For example , the walking animation for the hero character is applied just by changing the CSS style that is applied to the " hero " &lt;div&gt; element . <p> Sprite Sheet for Hero <p> There is a sequence of CSS styles that are used to define each state within the walking sequence : <p> This game demo extensively uses translate3d for hardware accelerated composition . - However , note that the 3d transforms are all applied to relatively small elements , and are not nested . All of the " textures " are well below the max texture size across all platforms ( 1024+1024 ) , and since it uses sprite sheets and reusable CSS styles , there are relatively few images to load into memory or upload to the GPU . <h> Attribution <p> The following Creative Commons assets were used in the creation of this sample app and the accompanying video : 
@@106848894 @2248894/ <h> Monthly Archives : August 2012 <p> Do you have questions about PhoneGap and/or Apache Cordova that youd like answered from Adobe ? - If so , here 's your chance ! - On August 28th at 1:00 PM EDT- ( 10:00 AM PDT ) Adobes technical evangelists will be hosting a two hour open session on Adobe Connect , where anyone can join and ask questions . Its an open session , so everyone is welcome with any questions anything from how to get started , to specific technical questions . - This session will be hosted by Raymond Camden , Piotr Walczyszyn , and myself . <p> Let 's say that you have a PhoneGap app that has access to documents , but you want to open those documents in a different application outside of your PhoneGap application . - For example , let 's say you have a DRM-protected PDF document that gets authenticated by an Adobe LiveCycle server . - On iOS devices , the DRM can only be authenticated if you are using Adobe Reader . - So , you need to delegate handling of that file @ @ @ @ @ @ @ @ @ @ handling this type of scenario : Either the target app opens the document via a custom URL scheme , or your app delegates to another application that is registered to handle a specific file format . - If the target app has a custom URL scheme , you just make a request following that schemes conventions Simple , right ? - If the target app does not have a custom URL scheme , then what do you do ? <p> iOS supports the ability to use an " open with " menu through the **31;6459;TOOLONG class , but by default this is not available to PhoneGap applications . - Luckily for us , PhoneGap is extensible , and we can create native plugins for whatever we want . <p> Since Adobe Reader does not yet support a custom URL scheme , you could use the- **31;6492;TOOLONG to suggest that the user opens the document with Adobe Reader , and this is exactly what I did <p> I created a PhoneGap native plugin that allows you to specify a file URL and UTI , and it will use the **31;6525;TOOLONG class @ @ @ @ @ @ @ @ @ @ for handling the file . - Check out the video below to see it in action : <p> The appropriate reader app is opened and UI/input is handed over to the target app <p> Plugin deletes the temp file <p> Even though I 've demonstrated this with a PDF and Adobe Reader , this is not limited to just PDF handling . - This plugin should work with any file type that has registered handlers ( although I 've only tested with PDFs &amp; Adobe Reader ) . - You just need to specify the UTI that is supported by the target application(s). - You can view a list of predefined UTI values from Apple , or create your own applications that support custom UTIs . <p> As promised , here are my presentation slides and extra content from last weeks- RIACon conference . I gave three presentations : " Intro to PhoneGap " , " Data Visualization With Web Standards " , and " PhoneGap Native Plugins " . All of the presentations are freely available at : https : **34;6558;TOOLONG . <p> Data visualization is the art &amp; science of @ @ @ @ @ @ @ @ @ @ Really , it can be anything : a bar chart , scatter plot , pie chart , complex flow diagram , 3d model , etc - If your visualization conveys information without having to read a table of data , then its doing what it should . Recently , the emergence of HTML5s dynamic graphics and SVG support have made rich , dynamic , and interactive graphics possible on the web without having to leverage Flash , which was previously the only real option . Be sure to check out this presentation , and read this blog post for more info : Data Visualization With Web Standards <p> PhoneGap provides you with the ability to create natively-installed mobile applications using web technologies . - As a part of this , PhoneGap provides an API to access native operating system functionality from JavaScript . Luckily for everyone , the JavaScript-to-native bridge- is extensible and you can very easily create and expose your own custom native functionality with a JavaScript API. - Basically , all PhoneGap native plugins are made up of two parts : a native implementation , and a JavaScript @ @ @ @ @ @ @ @ @ @ , which leverages cordovoa.exec to communicate with the native layer . - The native layer then performs a native operation and communicates back to the JS layer . <p> Open source PhoneGap plugins- There are lots of open source native plugins for everything from push notifications , screen captures , barcode scanners , to MapKit or even iCould ( among many others ) . <p> and of course , the sample apps : <h> iOS Multi-Screen <p> This sample app demonstrates how to create multi-screen experiences using the UIScreen API on iOS. - You can use AirPlay mirroring on an AppleTV as a second screen , the content of which is entirely controlled by the JavaScript in the " main " PhoneGap experience on the device . Check out the links and video below to learn more . <h> LowLatencyAudio <p> The PhoneGap LowLatencyAudio native plugin for Android and iOS allows you to preload audio , and playback that audio quickly , with a very simple to use API. - It overcomes the current limitations of the HTML5 Audio API on many mobile devices . Check out the links and video below to learn more . 
@@106848895 @2248895/ <h> Tag Archives : realtime <p> After spending some time playing around sketching with the HTML5 canvas element earlier this week , I figured " why not add some enterprise concepts to this example ? " - Next thing you know we 've got a multi-device shared sketching/collaboration experience . <p> To keep things straightforward , I chose to demonstrate the near-realtime collaboration using a short-interval HTTP poll . - HTTP polling is probably the simplest form of near-realtime data in web applications , however you may experience lag when compared to a socket connection of equivalent functionality . - I 'll discuss the various realtime data options you have in Flex/Flash and HTML/JS and their pros &amp; cons further in this post . <p> What you 'll see in the video below is the sketching example- with realtime collaboration added using short-interval data polling of a ColdFusion application server . - The realtime collaboration is shown between an iPad 2 , a Kindle Fire , and a Macbook Pro . <p> Before we get into the code for this example , let 's first review some realtime data basics <p> First , @ @ @ @ @ @ @ @ @ @ - Here are just a few : <p> Time sensitive information , where any delay could have major- repercussions <p> Realtime financial information <p> Emergency services ( medical , fire , police ) <p> Military/Intelligence scenarios <p> Business critical efficiency/performance- metrics <p> Collaboration <p> Realtime audio/video collaboration <p> Shared experience ( presentations/screen sharing ) <p> Entertainment <p> Streaming media ( audio/video ) <p> Gaming <p> Regardless of whether you are building applications for mobile , the web , or desktop , using any technology ( Flex/Flash , HTML/JS , Java , . NET , Objective C , or C/C++ ( among others ) ) , there are basically 3 methods for streaming/realtime data : <p> Socket Connection <p> HTTP Polling <p> HTTP Push <h> Socket Connections <p> Socket connectionss are basically end-to-end communications channels between two computer processes . - Your computer ( a client ) connects to a server socket and establishes a persistent connection that is used to pass data between the client and server in near-realtime. - Persistent socket connections are generally based upon TCP or UDP- and enable asynchronus bidirectional communication . - Binary or Text-based @ @ @ @ @ @ @ @ @ @ in time , in any sequence , as data is available . - In HTML/JS applications you can use web sockets , which I recently discussed , or use a plugin that handles realtime socket communication . Did you also know that the next version of- ColdFusion- will even have web socket support built in ? - In Flash/Flex/AIR , this can be achieved using the RTMP protocol ( LCDS , Flash Media Server , etc ) or raw sockets ( TCP or UDP ) . <p> Direct Socket Communications <p> In general , direct socket based communication is the most efficient means of data transfer for realtime application scenarios . - There is less back and forth handshaking and less packet encapsulation required by various protocols ( HTTP , etc ) , and you are restricted by fewer network protocol rules . - However , socket based communications often run on non-standard or restricted ports , so they are more likely to be blocked by IT departments or stopped by network firewalls. - - If you are using socket based communication within your applications , which are running on @ @ @ @ @ @ @ @ @ @ , you may want a fallback to another realtime data implementation for failover cases . <h> HTTP Polling <p> HTTP Polling is the process of using standard HTTP requests to periodically check for data updates on the server . - The client application requests information from the server . - Generally , the client will send a timestamp indicating the last data update time . - If there is information available on the server that is newer than the timestamp , that data will be immediately sent back to the client ( and the clients timestamp will be updated ) . - After a period of time , another request will be made , and so forth until the polling is stopped within the application . - Using this approach , the application is more-or-less " phoning home " periodically to the server to see if there are any updates . - You can achieve near-realtime performance by setting a very short polling interval ( less than one second ) . <p> Basic Data Poll Sequence <p> HTTP polling uses standard web protocols and ports , and generally will not @ @ @ @ @ @ @ @ @ @ of standard HTTP ( port 80 ) or HTTPS ( port 443 ) without any issue . - This can be achieved by polling JSON services , XML Services , AMF , or any other data format on top of a HTTP request . - HTTP polling will generally be slower than a direct socket method , and will also utilize more network bandwidth b/c of request/response encapsulation and the periodic requests to the server . - It is also important to keep in mind that the HTTP spec only allows for 2 concurrent connections to a server at any point in time . - Polling requests can consume HTTP connections , thus slowing load time for other portions of your application . - HTTP polling can be employed in HTML/JS , Flex/Flash/AIR , desktop , server , or basically any other type of application using common libraries &amp; APIs . <h> HTTP Push <p> HTTP Push technologies fall into 2 general categories depending upon the server-side **25;6594;TOOLONG - This can refer to- HTTP Streaming , where a connection is opened between the client and server and kept open using @ @ @ @ @ @ @ @ @ @ client , it will be pushed across the existing open HTTP connection . - HTTP Push can also refer to HTTP Long Polling , where the client will periodically make a HTTP request to the server , and the server will " hold " the connection open until data is available to send to the client ( or a timeout occurs ) . - Once that request has a complete response , another request is made to open another connection to wait for more data . - Once Again , with HTTP Long Poll there should be a very short polling interval to maintain near-realtime performance , however you can expect some lag . <p> HTTP Long Poll Sequence <p> HTTP Streaming &amp; HTTP Long polling can be employed in HTML/JS applications using the Comet approach ( supported by numerous backend server technologies ) and can be employed in Flex/Flash/AIR using BlazeDS or LCDS . <h> Collaborative Applications <p> Now back to the collaborative sketching application shown in the video above the application builds off of the sketching example from previous blog posts . - I added logic to monitor @ @ @ @ @ @ @ @ @ @ to share content between sessions that share a common I 'd . <p> Realtime Collaborative Sketches <p> In the JavaScript code , I created an ApplicationController class that acts as an observer to the input from the Sketcher class . - The ApplicationController encapsulates all logic handling data polling and information sharing between sessions. - When the application loads , it sets up the polling sequence . <p> The polling sequence is setup so that a new request will be made to the server 250MS- after receiving a response from the previous request . - Note : this is very different from using a 250MS interval using setInterval. - This approach guarantees 250MS from response to the next request . - If you use a 250MS interval using setInterval , then you are only waiting 250MS between each request , without waiting for a response . - If your request takes more than 250 MS , you will can end up have stacked , or " concurrent " requests , which can cause serious performance issues . <p> When observing the sketch input , the start and end positions and color @ @ @ @ @ @ @ @ @ @ captured transactions that will be pushed to the server . - ( The code supports multiple colors , even though there is no method to support changing colors in the UI . ) <p> The server then stores the pending transactions in memory ( I am not persisting these , they are in-ram on the server only ) . - The server checks the transactions that are already in memory against the last timestamp from the client , and it will return all transactions that have taken place since that timestamp. 
@@106848899 @2248899/ <h> Tag Archives : flex <p> I- recently spoke at the 360Flex conference in Devner , CO on " Multi-Device Best Practices " . - This presentation was focused upon multi-device &amp; multi-platform development strategies for both PhoneGap and Flex/AIR applications . - Below you can view my presentation slides and source code , and a brief summary . <p> Next , I emphasized the differences in user experience , display density , usability , and application style/feel between different platforms and device form factors . - You can read more detail on these topics here : <p> I also covered various libraries and techniques for making your creations feel like " native apps " instead of " web pages in a container " , and ways to make your apps look &amp; feel " more native " for a given platform . <p> Adobe has just published a white paper detailing current views and future commitments to Apache Flex . Included topics are contributions to Apache , commitments to the Flash platform , and commitments to Adobe customers . - I strongly recommend that all Flex developers and @ @ @ @ @ @ @ @ @ @ that Flex is the best solution for enterprise and data-centric application development today and is moving Flex into a community-driven open source project to ensure the continued development and success of Flex for years to come . We are currently in the process of contributing the core Flex SDK , automation libraries , AIR SDK binaries , and documentation to the Apache Flex Project . We will also be contributing Falcon , Falcon JS , Mustella , and BlazeDS . <p> In addition to these contributions , Adobe is providing a team of full-time Flex SDK engineers who will contribute to and support the Apache Flex Project . These Adobe engineers will work directly with the highly skilled Flex developer community to maintain , support , and evolve the Flex SDK . We remain committed to enabling the success of all existing and new Flex projects . <p> Back in the summer , I was lucky enough to get my hands on some early builds of Stage3D for mobile . I built some simple examples , including basic geometric shapes and simple 3D bubble charts inside of mobile Flex/AIR applications @ @ @ @ @ @ @ @ @ @ code , and Ive finally given in , and am sharing some source code . <p> I am not posting the full mobile application source code , since Stage3D for mobile is not yet available . However , I have ported the 3D bubble chart example to run in a Flex application targeting the desktop ( Flash Player 11 ) . The bubble chart example extends the concepts explored in the basic geometric shapes example . <p> Before you say " shoot , he did n't  give us the mobile code " , let me explain When I ported the code from the mobile project to the desktop Flex project , all I changed was code specific to the mobile Flex framework . I changed **34;6621;TOOLONG to &lt;s:Application&gt; and the corresponding architecture changes that were required , and I changed the list item renderers to Spark item renderers based on &lt;s:Group&gt; instead of mobile item renderers. - In the mobile item renderers , all my drawing logic was done using the ActionScript drawing API. - For simplicity in the port , I just used &lt;s:Rect&gt; to add the colored regions @ @ @ @ @ @ @ @ @ @ changed ! - <p> The stage3D code between the desktop and mobile implementations is identical . - - You can see the desktop port in action in the video below : <p> The source code was intended to be exploratory at best I was simply experimenting with hardware accelerated content , and how it can be used within your applications . - There is one big " gotcha " that you will have to watch out for if you want Stage3D content within a Flex application Stage3D content shows up behind Flex content on the display list . - By default , Flex apps have a background color , and they will hide the Stage3D content . - If you want to display any Stage3D content within a Flex application ( regardless of web , desktop AIR , or mobile ) , you must set the background alpha of the Flex application to zero ( 0 ) . - Otherwise you will pull out some hair trying to figure out why it does n't  show up . <p> The source code for the web/Flex port of this example is available @ @ @ @ @ @ @ @ @ @ the Flex transition to the Apache Software Foundation ? <p> How can you contribute and help make Flex thrive ? <p> Do you have questions that you would like to voice to Adobe ? <p> As promised , Adobe is kicking off The Flex User Group Tour to discuss recent events surrounding Flex- and the Flash Platform . - These meetings are intended to help you understand the changes happening with Flex and Flash , the impact to related tools , as well as to educate about the process &amp; transition to Apache . - You can learn more about the user group tour and get an up-to-date listing of dates &amp; cities from the Flex Team blog- be sure to check back periodically for updates . Initial cities include New York , Boston , Denver , Seattle , Los Angeles , Sand Diego , and Dallas . - Expect more cities &amp; countries to be announced at a later date . <p> We hope to see you at one of the upcoming events . I 'm scheduled to speak at the Dallas event in April , and I hop to @ @ @ @ @ @ @ @ @ @ around sketching with the HTML5 canvas element earlier this week , I figured " why not add some enterprise concepts to this example ? " - Next thing you know we 've got a multi-device shared sketching/collaboration experience . <p> To keep things straightforward , I chose to demonstrate the near-realtime collaboration using a short-interval HTTP poll . - HTTP polling is probably the simplest form of near-realtime data in web applications , however you may experience lag when compared to a socket connection of equivalent functionality . - I 'll discuss the various realtime data options you have in Flex/Flash and HTML/JS and their pros &amp; cons further in this post . <p> What you 'll see in the video below is the sketching example- with realtime collaboration added using short-interval data polling of a ColdFusion application server . - The realtime collaboration is shown between an iPad 2 , a Kindle Fire , and a Macbook Pro . <p> Before we get into the code for this example , let 's first review some realtime data basics <p> First , why/when would you need realtime data in your applications ? - Here @ @ @ @ @ @ @ @ @ @ where any delay could have major- repercussions <p> Realtime financial information <p> Emergency services ( medical , fire , police ) <p> Military/Intelligence scenarios <p> Business critical efficiency/performance- metrics <p> Collaboration <p> Realtime audio/video collaboration <p> Shared experience ( presentations/screen sharing ) <p> Entertainment <p> Streaming media ( audio/video ) <p> Gaming <p> Regardless of whether you are building applications for mobile , the web , or desktop , using any technology ( Flex/Flash , HTML/JS , Java , . NET , Objective C , or C/C++ ( among others ) ) , there are basically 3 methods for streaming/realtime data : <p> Socket Connection <p> HTTP Polling <p> HTTP Push <h> Socket Connections <p> Socket connectionss are basically end-to-end communications channels between two computer processes . - Your computer ( a client ) connects to a server socket and establishes a persistent connection that is used to pass data between the client and server in near-realtime. - Persistent socket connections are generally based upon TCP or UDP- and enable asynchronus bidirectional communication . - Binary or Text-based messages can be sent in either direction at any point in time @ @ @ @ @ @ @ @ @ @ - In HTML/JS applications you can use web sockets , which I recently discussed , or use a plugin that handles realtime socket communication . Did you also know that the next version of- ColdFusion- will even have web socket support built in ? - In Flash/Flex/AIR , this can be achieved using the RTMP protocol ( LCDS , Flash Media Server , etc ) or raw sockets ( TCP or UDP ) . <p> Direct Socket Communications <p> In general , direct socket based communication is the most efficient means of data transfer for realtime application scenarios . - There is less back and forth handshaking and less packet encapsulation required by various protocols ( HTTP , etc ) , and you are restricted by fewer network protocol rules . - However , socket based communications often run on non-standard or restricted ports , so they are more likely to be blocked by IT departments or stopped by network firewalls. - - If you are using socket based communication within your applications , which are running on non-standard ports , and you do n't  govern the network , you @ @ @ @ @ @ @ @ @ @ failover cases . <h> HTTP Polling <p> HTTP Polling is the process of using standard HTTP requests to periodically check for data updates on the server . - The client application requests information from the server . - Generally , the client will send a timestamp indicating the last data update time . - If there is information available on the server that is newer than the timestamp , that data will be immediately sent back to the client ( and the clients timestamp will be updated ) . - After a period of time , another request will be made , and so forth until the polling is stopped within the application . - Using this approach , the application is more-or-less " phoning home " periodically to the server to see if there are any updates . - You can achieve near-realtime performance by setting a very short polling interval ( less than one second ) . <p> Basic Data Poll Sequence <p> HTTP polling uses standard web protocols and ports , and generally will not be blocked by firewalls. - You can poll on top of standard @ @ @ @ @ @ @ @ @ @ ) without any issue . - This can be achieved by polling JSON services , XML Services , AMF , or any other data format on top of a HTTP request . - HTTP polling will generally be slower than a direct socket method , and will also utilize more network bandwidth b/c of request/response encapsulation and the periodic requests to the server . - It is also important to keep in mind that the HTTP spec only allows for 2 concurrent connections to a server at any point in time . - Polling requests can consume HTTP connections , thus slowing load time for other portions of your application . - HTTP polling can be employed in HTML/JS , Flex/Flash/AIR , desktop , server , or basically any other type of application using common libraries &amp; APIs . <h> HTTP Push <p> HTTP Push technologies fall into 2 general categories depending upon the server-side **25;6657;TOOLONG - This can refer to- HTTP Streaming , where a connection is opened between the client and server and kept open using keep-alives. - As data is ready to send to the client , @ @ @ @ @ @ @ @ @ @ . - HTTP Push can also refer to HTTP Long Polling , where the client will periodically make a HTTP request to the server , and the server will " hold " the connection open until data is available to send to the client ( or a timeout occurs ) . - Once that request has a complete response , another request is made to open another connection to wait for more data . - Once Again , with HTTP Long Poll there should be a very short polling interval to maintain near-realtime performance , however you can expect some lag . <p> HTTP Long Poll Sequence <p> HTTP Streaming &amp; HTTP Long polling can be employed in HTML/JS applications using the Comet approach ( supported by numerous backend server technologies ) and can be employed in Flex/Flash/AIR using BlazeDS or LCDS . <h> Collaborative Applications <p> Now back to the collaborative sketching application shown in the video above the application builds off of the sketching example from previous blog posts . - I added logic to monitor the input sketches and built a HTTP poll-based monitoring service to share @ @ @ @ @ @ @ @ @ @ Realtime Collaborative Sketches <p> In the JavaScript code , I created an ApplicationController class that acts as an observer to the input from the Sketcher class . - The ApplicationController encapsulates all logic handling data polling and information sharing between sessions. - When the application loads , it sets up the polling sequence . <p> The polling sequence is setup so that a new request will be made to the server 250MS- after receiving a response from the previous request . - Note : this is very different from using a 250MS interval using setInterval. - This approach guarantees 250MS from response to the next request . - If you use a 250MS interval using setInterval , then you are only waiting 250MS between each request , without waiting for a response . - If your request takes more than 250 MS , you will can end up have stacked , or " concurrent " requests , which can cause serious performance issues . <p> When observing the sketch input , the start and end positions and color for each line segment get pushed into a queue of captured transactions @ @ @ @ @ @ @ @ @ @ The code supports multiple colors , even though there is no method to support changing colors in the UI . ) <p> The server then stores the pending transactions in memory ( I am not persisting these , they are in-ram on the server only ) . - The server checks the transactions that are already in memory against the last timestamp from the client , and it will return all transactions that have taken place since that timestamp. 
@@106848900 @2248900/ <h> Tag Archives : HTML5 <p> Last week I had the opportunity to present " Data Visualization With Web Standards " to the Data Visualization New York Meetup group . - There was a great turnout , and thanks to everyone who attended . - I 'd like to especially thank Christian Lilley and Paul Trowbridge for organizing the event . <p> My presentation focused on the fundamental techniques of visualizing data within HTML/JS experiences . - You can view my presentation in its entirety below . - Slides and bullet points are below the fold <h> HTML5 &lt;canvas&gt; <p> You can use the HTML5 &lt;canvas&gt; element to programmatically render content based upon data in-memory using JavaScript . The HTML5 Canvas provides you with an API for rendering graphical content via moveTo or lineTo instructions , or by setting individual pixel values manually . - Learn more about the- HTML5 canvas from the MDN tutorials . <p> " One Million Points " Scatter Plot Let the page load , then use the mouse to click and drag regions to " drill into " . - This is a live visualization @ @ @ @ @ @ @ @ @ @ data sets by manipulating individual pixels . <h> HTML DOM Elements <p> Visualizations like interactive maps , or simple charts can be created purely with HTML structures and creative use of CSS styles to control position , visual presentation , etc You can use CSS positioning to control x/y placement , and percentage-based width/height to display relative values based upon a range of data . - For example , the following bar chart/table is created purely using HTML DIV containers with CSS styles . <h> WebGL <p> WebGL- is on the " bleeding edge " of interactive graphics &amp; data visualization across the web . WebGL enables hardware-accelerated 3D graphics inside the browser experience . Technically , it is not a standard , and there is varied and/or incomplete support across different browsers ( http : **27;6684;TOOLONG ) . - There is also considerable debate whether it ever will be a standard ; however there are some incredible samples out on the web worth mentioning : <p> Its Valentines day , and here at Adobe were showing our love for creating great HTML experiences Just check out what we released @ @ @ @ @ @ @ @ @ @ Adobe Creative Cloud- subscriptions . <h> Adobe Edge Reflow ( Preview ) <p> Adobe Edge Reflow is a new responsive design tool that helps designers create and communicate responsive intent in their designs to both customers and developers . - Adobe Edge Reflow enables designers to create responsive HTML experiences in a visual workspace , and leverages web standards technologies to create designs that are- accurate- to the capabilities of the modern web . <h> Stay Informed ! <p> As I mentioned above , all of these are available immediately via Adobe Creative Cloud subscriptions . Even better , you can get Adobe Edge Animate , Adobe Edge Reflow , and Adobe Edge Code as part of the free Creative Cloud tier . Go download them now ! <p> here 's a video from the " What Developers Love and Hate about iOS , Android , Windows and HTML5 ? panel discussion that I took part in during MoDev East- back in December . - The panelists represented native app developers , HTML/web developers , and myself-representing the HTML/hybrid app paradigm . <p> I thought it was a great discussion , @ @ @ @ @ @ @ @ @ @ solution such as- PhoneGap . ( That 's me in the brown jacket . ) <p> I am asked all the time " How do I get started developing PhoneGap applications ? " . My normal answer is to advise people to check out the PhoneGap Getting Started Guides , which provide a great starting point for every platform . However after further thought , I 'm not sure this is always what people are asking . Rather than " how do I get started ? " , I think people are often looking for insight into the workflow for developing PhoneGap applications . Everything from tools to developer flow , to getting the app on devices . The Getting Started Guides are essential for setting up the initial project structure , but once you get that setup , you might be wondering " what do I do next ? " . In this post , I 'll try to she 'd some light on the workflow and tools that I use when developing PhoneGap applications . <h> Know What You 're Going To Build Before You Build It <p> First and foremost it is @ @ @ @ @ @ @ @ @ @ you are going to build before you build it . If you just start hacking things together without a plan , the final result is seldomly great . Complete ( pixel perfect ) UI/UX mockups are fantastic , but you do n't  have to have a fully polished design and screen flow . Just having wireframes/sketches are a great start . Heck , even a sketch on a napkin is better than starting with nothing . <p> The UX design/wireframes help you understand what you application should be doing from the users perspective , which in turn helps you make decisions on how you tackle a project . This can be purely from a HTML level , helping you figure out how you should position DOM elements and/or content . Or , it can help you gauge your projects technical complexity How many " moving parts " do you have , how much of the app is dynamic or asynchronus , or how do different visual elements need to work together ? You can leverage this design/mockup to analyze the needs of your application and determine if a particular framework/development @ @ @ @ @ @ @ @ @ @ Knockout.js , Sencha , jQuery Mobile , Angular.js , etc ) . <p> When working with a designer , I use Adobes Creative Suite Tools for pretty much everything wireframes , UI/UX designs , chopping up assets , etc I 'm currently working on a project that was designed by the talented- Joni from Adobe XD . Joni designed everything in Creative Suite , and I 'm using Photoshop to view screen flows and extract UI assets for the actual implementation . <p> UI Mockups in PhotoshopScreen Flow in Photoshop <p> Note : This app will also be free and open source as a sample/learning resource for PhoneGap , including all of the design assets I 'll follow up with another post on this later , once the app is- available- in the app stores . <p> If you are n't  a " graphics person " , or do n't  have creative suite , there are a bunch of other tools that you can use for wireframing and/or sketching ( but cmon , Creative Cloud is only $50 a month ) . Here are several Ive used with great success , but this @ @ @ @ @ @ @ @ @ @ A drag &amp; drop wireframing tool for OS X. This is fantastic for wireframing or documenting screen flows . In fact , the screen flow image shown in Photoshop above was originally composed in Omnigraffle , using the mockups created in Photoshop . <p> Visio- A powerful drag &amp; drop wireframing/design tool for Windows much like OmniGraffle , but for windows . <p> PowerPoint- or Keynote- - These are n't  just for presentations . They can be really useful for putting together screen flow diagrams , or annotating images/content . <p> Often people like to sketch out ideas &amp; wireframes on their tablets , here are a few tools that I use for that : <p> iBrainstorm A great app for collaboratively taking notes and sketching . I 'm partial to this one b/c used to be on the dev team , and I wrote a good chunk of the graphics sketching logic . <p> There are a bunch of other tablet sketching apps out there , but I have n't used most of them . <h> Coding Environment <p> Coding environments are a tricky subject . There is no single solution @ @ @ @ @ @ @ @ @ @ Some people chose lightweight text editors , some people chose large-scale IDEs , some people use designer-centric tools , and many of these choices are- dependant- upon which operating system you use or your background as a designer or developer . Since PhoneGap applications are really just editing HTML , CSS &amp; JavaScript , you can use whatever editor you want . In fact , I know a several people that use vim as their primary editor . <h> Large-Scale IDEs <p> I 'm a bigger fan of of using a complete IDE ( integrated development environment ) than I am of a lightweight editor , simply b/c IDEs tend to have hooks into more features/languages , etc I know people complain about startup time , but there is no startup time if you leave it open all the time . <p> There are a few catches when talking about IDEs with PhoneGap . The first is that if you want to deploy anything locally to devices ( without using PhoneGap Build ) , you have to delpoy using the IDE for the particular platform that you are- targeting . That @ @ @ @ @ @ @ @ @ @ Visual Studio for Windows Phone , etc However if you wish , you can use your editor of choice , and just use the IDE to deploy to devices locally . You can even share source code across several IDE installations using symlinks ( which I describe here ) . I very often use this type of a configuration to share code between Xcode , Eclipse , and WebStorm . <p> My preference for coding PhoneGap applications is to use- WebStorm by JetBrains . WebStorm has great code-hinting ( even for your own custom JS and 3rd party libraries ) , great refactoring , hooks into Git , CVS , or SVN repositories , and is a very mature IDE . <p> WebStorm IDE <p> I tend to use this as my primary coding tool , then switch to Eclipse or Xcode when I want to locally deploy to a device for testing . When using PhoneGap Build to simplify cross-platform compilation , I just push the code to git , then recompile via PhoneGap Build . <p> I 'm not a fan of Xcodes HTML/JS editing , and havent found @ @ @ @ @ @ @ @ @ @ To target Windows devices , I use Visual Studio . <h> Lightweight Editors <p> I 'm a bigger fan of larger IDEs than lightweight editors , but- Adobe Edge Code ( also known as Brackets ) is a great lightweight editor for quick edits. - Edge Code/Brackets is an open source HTML/JS editor that supports live editing in the browser and inline editors for CSS styles , without leaving your HTML files . If you tried Edge Code Preview 1 , but werent sold on it , you should try Edge Code Preview 2 . The team has come a long way very quickly . Its fast , easy to use , and there is a- plugin to tie it into PhoneGap Build . I sometimes use this for quick edits . <p> There are tons of other lightweight editors out there , and everyone has their favorite . As long as you 're happy with the tool , and it can edit text ( HTML , CSS , JS ) files , you can use it to build PhoneGap applications . <h> Designer-Friendly Editors <p> I 'm not necessarily the primary target @ @ @ @ @ @ @ @ @ @ Dreamweaver gives you a great programming environment plus a WYSIWYG editor for HTML experiences . It also features PhoneGap Build integration directly inside the coding environment . If you 're used to Dreamweaver for creating web experiences , you can continue to use it and target mobile apps as well . <p> Adobe Dreamweaver <h> Debugging Environments <p> Yes , that is plural Debugging Environments . Due to the cross-platform nature and PhoneGaps leveraging of native web views for each platform , debugging PhoneGap applications can sometimes be tricky . Here are some tips that will make this significantly easier . <h> The PhoneGap Emulator <p> The PhoneGap Emulator is my primary development/debugging tool for all PhoneGap apps . It is a browser-based emulator leveraging the Google Chrome browser and the Ripple Emulation Environment . The PhoneGap Emulator runs inside of Google Chrome , and provides emulation of PhoneGaps core APIs . Since it is built on top of Chrome , it enables you to leverage Chromes Developer Tools , which in my opinion are second to none for web/application development . This is a highly-productive developer environment . <p> PhoneGap @ @ @ @ @ @ @ @ @ @ PhoneGap/Ripple/Google Chrome development environment : <p> First , - this combination enables you to emulate most core PhoneGap APIs without leaving the desktop environment . It enables you to test various APIs including geolocation ( with simulated locations ) , device events ( deviceready , back , etc ) , sensor events ( accelerometer , compass ) , and even let 's you test with different device aspect ratios all without having to push anything to an actual device . This saves a lot of time in development iterations . You can read about the supported Ripple emulator features here . <p> Second , Chromes Developer Tools are awesome . Here are just a few things that you can do while developing/debugging your app , live within the emulation environment : <p> Analyze all resources consumed by your app , via the resources panel . This includes all scripts , images , html files , cookies , etc it even includes insight into any local data stored via PhoneGaps local storage database ( WebSQL implementation ) . <p> View/query all local databases within your app . You can write your own @ @ @ @ @ @ @ @ @ @ to Ray for sharing this , its not immediately intuitive . <p> Debug JavaScript with the Scripts/Sources Panel . You can set breakpoints in JS execution , inspect &amp; alter values in JS objects in-memory , and view details and line numbers for any exceptions that occur . <p> Use the console to monitor console.log() statements , inspect properties of objects in memory , or execute arbitrary JavaScript whenever you want . <p> The PhoneGap Emulator enables developers to be extremely productive with development , however I can not emphasize enough that on-device testing is critical for having a successful app . On-device testing can expose performance problems or browser rendering variances that you may not notice in the emulator environment . <h> On-Device Remote Debugging <p> As I mentioned above , on-device testing is critical for successful applications . iOS and BlackBerry have an advantage over other platforms b/c the latest developer tools allow you to remotely debug content live on a device . <p> Since the release of iOS 6 , you can debug content in the iOS simulator using Safaris Developer Tools . Safaris developer tools give @ @ @ @ @ @ @ @ @ @ above for Chrome . <h> Remote Debugging with Weinre <p> Not every platform supports live remote debugging , especially older versions . Weinre ( pronounced winery ) is a remote web inspector that allows you to inspect/edit DOM and CSS elements on remote devices . Basically , you include some JavaScript in your app , and it communicates back to a server that will tell you what 's happening inside of the app running on the mobile device . It wo n't give you full debugging capabilities like JS breakpoints and memory inspection , but its better than nothing . You can use Weinre by setting up your own instance , or by leveraging debug.phonegap.com . <p> Weinre for On-Device Debugging <h> When All Else Fails <p> If you 're still debugging your apps , and the solutions mentioned above do n't  work , you can always resort to plain-old " alert() " statements to pop up debug messages , or use " console.log() " statements to write to system logs . <p> On Android , all- console.log ( ' ... ' ) ; - messages will appear as printouts in the command-line @ @ @ @ @ @ @ @ @ @ and integrated into the Android Eclipse plugin . <p> On BlackBerry , all- console.log ( ' ... ' ) ; - are printed to the BlackBerrys Event Log . The Event Log can be accessed by pressing- ALT + LGLG . <p> On iOS , all- console.log ( ' ... ' ) ; - are output to the Xcode Debug Area console . <h> Building PhoneGap Apps <p> The PhoneGap getting started guides will point you to the right direction for getting started with a particular platform . If you are just targeting iOS , you can use Xcode for building . If you are just targeting Android , you can use Eclipse , etc It is all very easy to get up and running . <p> However , this process gets much more complicated when targeting multiple platforms at once . When I have to do this , PhoneGap Build becomes really , really handy . <p> PhoneGap Build allows you to either upload your code , or point to a Git repository . PhoneGap Build will then pull your code and build for 7 different platforms , without @ @ @ @ @ @ @ @ @ @ . All that you 'll have to do is install the cloud-compiled binaries on your device . You can do this by copying/pasting a URL to the binaries , or by capturing a QR code that will directly link to the compiled- application- binary . <p> One other advantage of PhoneGap build is that it let 's designers/developers build mobile applications without having to install any developer tools . If you want to compile a PhoneGap app for iOS , but are on Windows just use PhoneGap build and you wo n't need Xcode or a Mac . <h> PhoneGap UI/Development Frameworks <p> Probably the most common PhoneGap question that I get asked is " what MVC/development framework should I use ? " . If you 've been waiting for me to answer this , do n't  hold your breath . It is impossible to be prescriptive and say that one solution fits all use cases for every- developer . <p> When people ask me this , I like to paraphrase- Brian Leroux from the PhoneGap team : " Use HTML , it works really well . " <p> I think people often overlook @ @ @ @ @ @ @ @ @ @ view . Anything that is valid HTML/CSS content can be rendered as your applications user interface . This could be something incredibly simple , like text on the screen , or it could be incredibly creative or complex . The important factor is that you need to focus on a quality user experience . If you 're worried about your UX , and are worried that Apple may reject your app , then read this article where I explain Apple rejections in detail . <p> HTML/JS developers come from many different backgrounds , with varying degrees of programming expertise . Some frameworks appeal to some people , other frameworks appeal to other people . There also seem to be new UI &amp; architectural frameworks popping up every week . It would be a disservice to all people who use PhoneGap for us to proclaim that we should only use one singe framework . <p> There are lots , and lots , and lots more options out in the HTML/JS development world I 'm not even taking into account JavaScript generating tools and languages like CoffeeScript , TypeScript , or others <p> Have @ @ @ @ @ @ @ @ @ @ things just do n't  seem to be working as you 've expected ? Just imagine that the data that you are receiving from the server is n't exactly what youd expect even worse , what if this is happening in an application that youve already released to the app store , but not in your local debugging environment ? Debugging this type of error can be difficult , but it can be made MUCH easier by taking advantage of a network proxy . <p> A network proxy server acts as an intermediary between the client and server . In this case , we are talking about using a network proxy server to intercept and analyze all web traffic occurring between a mobile device and the application server . The network proxy/monitor that I use very often in development is Charles . <p> Charles let 's you inspect ( and even intercept and change ) all HTTP traffic either on your local machine , or on mobile devices that you control . - In this case I am using it to analyze the HTTP traffic for a PhoneGap application , but it actually intercepts @ @ @ @ @ @ @ @ @ @ by native applications or " release " applications obtained via the App Store . - It wo n't give you insight into the code within an application just insight into all HTTP-based network communication . 123434 @qwx983434 <p> From the Charles proxy , you 'll be able to see every HTTP request . Once you select an HTTP request , you 'll be able to view pretty much everything about it round-trip time , request/response size , HTTP headers , query parameters , HTTP POST data , and of course , the response payload . <p> If you want , you can even use the- Breakpoint Tool- to intercept HTTP requests and modify the response payload before its returned back to your application . <p> To get started using the network proxy with your own applications , just follow the instructions provided in the Charles FAQ . You will quickly be " up and running " with the proxy , and able to inspect all HTTP traffic within your applications . <p> Once you 've installed and launched Charles on your computer , you 'll need to set the HTTP proxy settings on your mobile @ @ @ @ @ @ @ @ @ @ your Wi-Fi connection , and set up a " Manual " HTTP proxy . Then enter the IP address for the computer that is running Charles , and port 8888. - Note : The device and your computer should be on the same network segment . <p> This also works for Android devices too . You 'll need to go into the " Advanced Settings " for your Wi-Fi connection , select " Manual " proxy , enter your computers IP address for the proxy hostname , and specify port 8888 as used by Charles . <p> Once you 've got this configured , you can view all HTTP traffic in all of your applications . This is particularly useful for debugging query parameter and server response issues on devices . If you 're using the PhoneGap Emulator , you can just use Chromes Developer Tools without having to worry about configuring a proxy . <p> The application that I was debugging in the video is my US Census Browser , an open source PhoneGap application for visualizing US Census data . You can download this application for yourself via : 
@@106848901 @2248901/ <h> Tag Archives : ios <p> Last week I attended IBM Insight in Las Vegas . It was a great event , with tons of great information for attendees . I had- a few sessions on mobile applications . In particular , my dev@Insight session on Wearables powered by IBM MobileFirst was recorded . You can check it out here : <h> Key takeaways from the session : <p> Wearables are the most personal computing devices ever . Your users can use them to be notified of information , search/consume data , or even collect environmental data for reporting or actionable analysis . <p> Regardless of whether developing for a peripheral device like the Apple Watch or Microsoft Band , or a standalone device like Android Wear , you are developing an app that runs in an environment that mirrors that of a a native app . So , the fundamental development principles are exactly the same . You write native code , that uses standard protocols and common conventions to interact with the back-end . <p> Caveat to #1 : You user interface is much smaller . You @ @ @ @ @ @ @ @ @ @ the reduced amount of information that can be displayed . <p> You can share code across both the phone/tablet and watch/wearable experience ( depending on the target device ) . <p> Using IBM MobileFirst you can easily expose data , add authentication , and capture analytics for both the mobile and wearable solutions . <p> IBMs Watson Developer Cloud speech services just got a whole lot easier for mobile developers . - I myself just learned about these two , and cant wait to integrate them into my own mobile applications . <p> The Watson Speech to Text and Text to Speech services are now available in both native iOS and Android SDKs , making it even easier to integrate language services into your apps . <p> This post is inspired by all the comments Ive seen this week about JS in the enterprise . I would have never imagined- this 10- years ago , but JavaScript is now pretty much ubiquitous . Here are a few reasons why you need to paying attention to JavaScript if you are n't  already , and why you should definitely not- write it @ @ @ @ @ @ @ @ @ @ major reasons for JavaScripts ubiquity is that- JavaScript is approachable . It is relatively easy for beginners to learn JavaScript , and powerful enough for advanced users to build complex and reliable systems . <p> Second , why you need to pay attention , JavaScript is everywhere . <p> You can now use JavaScript to develop on virtually any platform : client side applications , server side logic , embedded chips/IoT devices , manage build scripts and dependencies , and more . <p> This does n't  mean you 'll use the exact same code in every case , rather that you can use the same skill set JavaScript Development to deliver solutions across multiple paradigms . <h> The Client Side <p> JavaScript can be used to power client side apps/user interfaces , and user interactions on numerous platforms and devices . <h> Web <p> Of course JavaScript powers the web , this is a given . JavaScript is the primary scripting language for all web browsers . I wo n't focus on this much b/c its already well known . <h> Mobile <p> JavaScript can also be used to power mobile applications @ @ @ @ @ @ @ @ @ @ Cordova/PhoneGap- You can build natively installed apps with web technology using PhoneGap or Cordova . PhoneGap is Adobes branded distribution of Cordova , but from the developers perspective , they are basically the same thing . Your app runs within a webview on the mobile device , and you build your user interface the same way you you build a dynamic web application . Your user interface- is implemented in HTML , styled with CSS , and all interactivity is created with JavaScript . <p> React Native- JavaScript powered web apps do n't  just have to be inside of a a web view . The React Native framework gives developers the ability to write their application using JavaScript and declarative UI elements , and results in a native application running on the mobile device . The logic is interpreted JavaScript at runtime , but everything that the user- interacts with ( all UI elements ) is 100% native , providing a very high quality user experience , and it is now available for both iOS and Android applications . <p> Unity 3D You can even develop rich &amp; immersive mobile @ @ @ @ @ @ @ @ @ @ using the Unity 3D engine . **These can be web , desktop , or mobile , but is often used in mobile gaming . <h> Desktop <p> Yup , desktop apps are not left out of the mix . Most desktop solutions fall into a category similar to Apache Cordova , where the end results is a web view that has access to lower level APIs , whose content is developed with web based technology . <h> The Server Side <p> Most obviously- Node.js a JavaScript runtime buit on Chromes V8 JavaScript Engine has made huge inroads into server side development and the enterprise . Node.js , powered by frameworks like express.js or loopback.io makes server side development and complex enterprise apps with JavaScript possible . <p> Here are some stats that- show the magnitude of growth and adoption for Node.js/npm.js alone . NPM- stats currently shows a total of- 186,946 packages available for download , 94,978,032 package downloads in the last day , and 2,451,734,737 package downloads in the last month . <p> This does n't  mean that JavaScript is the best language at everything . It- also does @ @ @ @ @ @ @ @ @ @ source- code and run it in every device/context imaginable . <p> It means that you can use your skills in JavaScript to develop for just about any kind of device/context out there . Its not going to be write once , run everywhere , rather in the words of the React.js team : learn once , write everywhere . <p> Node.js is an incredible tool for rapidly building highly performant and scalable back end systems , and you develop it using a familiar core language that most front-end developers are already accustomed to , JavaScript. - This acquisition is positioned to greatly enhance Node.js in the enterprise , and StrongLoops offerings will be integrated into IBM Bluemix , IBM MobileFirst , and WebSphere . <p> Even though the acquisition is still " hot off of the presses " , - you can start using these tools together- today : <p> If you have n't heard about StrongLoop 's LoopBack framework , it enables you to easily connect and expose your data as REST services . It provides the ability to visually create data models in a graphical ( or command line @ @ @ @ @ @ @ @ @ @ APIs " thus generating CRUD operations for your REST services tier , without having to write any code . <p> Why is this important ? <p> It makes API development easier and drastically reduces time from concept to implementation . - If you have n't yet looked at the LoopBack framework , you should definitely check it out . - You can build API layers for your apps literally in minutes . - Check out the video below for a quick introduction : <p> Again , be sure to check out these posts that detail the integration steps so you can start using these tools together today : <p> That title get your attention ? - Yes , it really read " Adaptive- mobile- apps that- change based on personal context " with near real-time rules application , without much extra development effort . - If that sounds interesting to you , or like a product you might want to use within your own apps , then you might want to check out this site where you can get involved in the products development : - http : **35;6713;TOOLONG <p> IBM @ @ @ @ @ @ @ @ @ @ mobile app experiences. - User experiences within a single app that can be- dramatically different per user based on location , past behavior , profile information , social media activity , - and so much more . - With this behavior being driven by configurable rules that can be changed without redeploying an app to the app store . <h> How it works for your customer <p> Consider this scenario : <p> Jon and Andrea download the mobile app for S&amp;W , a retailer known for its attention to providing great customer service . Over the next month , Jon and Andrea use the app to browse and discover content and merchandise differently . <p> Jon primarily navigates to sports related content for his favorite teams to find gear and clothes for travel to his favorite team 's games . Andrea scours the app for sales and fashion trends and usually ends up following her favorite designers . <p> Andrea and Jon go to a baseball game together . She 's never enjoyed watching it , so she opens up the S&amp;W app to entertain herself , and her app @ @ @ @ @ @ @ @ @ @ <p> Jon however , wants to replace the hat he 's worn the last three times the team lost , and since he 's in the stadium , his S&amp;W app opens right up to the team 's gear page . The app knows he 's out of town and tells him how to get to an S&amp;W store . <h> How it works for the dev team <p> Consider another scenario : <p> One of the developers on the team , George , sets up the system and application . He then gives access to Janet who is responsible for the customer experience . <p> Janet writes rules defining how the application could adapt and become more personalized based on inputs like , social media , geolocation , app usage , or customer information data . <p> Once Janet has built out her rules , she simply hits Submit ' and can immediately see her clever interactions reflected in the mobile application without having to involve the development team . <p> Analytics let Janet know which adaptations are working best , and helps her find new opportunities to optimize @ @ @ @ @ @ @ @ @ @ about a content management system , or translation based on locale , instead a rules-driven product that can adapt literally every aspect of your app : - customize the user interface , enable or disable different features , customized messaging and notifications , and much more , all variable based upon the user context . - This can be used to present- contextually relevant information , drive adoption , provide more/less data depending on your physical context , and so much more . <p> It wont be tied to a specific UI framework , wont be tied to a specific content management system , is n't attempting to re-create Google Now or Apple Proactive Assistance . - Rather , a set of tools and a rules engine that enable you to customize and tailor the app experience to the individual user . 
@@106848903 @2248903/ <h> IBM Watson Speech Services Just Got A Whole Lot Easier <p> IBMs Watson Developer Cloud speech services just got a whole lot easier for mobile developers . - I myself just learned about these two , and cant wait to integrate them into my own mobile applications . <p> The Watson Speech to Text and Text to Speech services are now available in both native iOS and Android SDKs , making it even easier to integrate language services into your apps . 
@@106848904 @2248904/ <p> Second , a video of my presentation on " Architectural Considerations for PhoneGap and Mobile Web Apps " has been published by the Atlanta HTML5 meetup group . - Check it out in the video below , and if you 're in the Atlanta area , be sure to check out the meetup group ! - Heres the presentation description : <p> Tired of Hello World ? In this session , we explore best practices to build real-world PhoneGap applications . We investigate the Single Page Architecture , HTML templates , effective Touch events , performance techniques , modularization and more . We also compare and contrast the leading JavaScript and Mobile Frameworks . This session is a must If you plan to build a PhoneGap application that has more than a couple of screens . 
@@106848906 @2248906/ <h> Transcript from Open Session On PhoneGap <p> Today Raymond Camden and I hosted another open session on PhoneGap- as part of an Adobe TechLive event . - These sessions are an opportunity for anyone to stop in and ask us questions . <p> The Q&amp;A transcript from todays session is below . - Thanks to everyone for sticking around , and bearing through our technical difficulties The normal Q&amp;A pod was n't working for some reason , so we had to improvise. - Well make sure this is working for next time . <p> This was our third event , and we 've had a great turnout so far , so we will be holding these open sessions once a month . - If you werent able to make it this time , well be having another one soon . Just check the- Adobe TechLive- page for future events . 123433 @qwx983433 <p> Related to the Require.js and Phonegap quesion , here you can find the source code of " EmployeeDirectoryModular " , a demo I developed for the students of my course on Mobile Apps Development in which I @ @ @ @ @ @ @ @ @ @ , jQueryMobile , Handlebars , underscore , Require.JS , all together 
@@106848907 @2248907/ <h> Tag Archives : JavaScript <p> As promised , here are my presentation slides and extra content from last weeks- RIACon conference . I gave three presentations : " Intro to PhoneGap " , " Data Visualization With Web Standards " , and " PhoneGap Native Plugins " . All of the presentations are freely available at : https : **34;6750;TOOLONG . <p> Data visualization is the art &amp; science of creating a visual representation of data and information . - Really , it can be anything : a bar chart , scatter plot , pie chart , complex flow diagram , 3d model , etc - If your visualization conveys information without having to read a table of data , then its doing what it should . Recently , the emergence of HTML5s dynamic graphics and SVG support have made rich , dynamic , and interactive graphics possible on the web without having to leverage Flash , which was previously the only real option . Be sure to check out this presentation , and read this blog post for more info : Data Visualization With Web Standards <p> @ @ @ @ @ @ @ @ @ @ applications using web technologies . - As a part of this , PhoneGap provides an API to access native operating system functionality from JavaScript . Luckily for everyone , the JavaScript-to-native bridge- is extensible and you can very easily create and expose your own custom native functionality with a JavaScript API. - Basically , all PhoneGap native plugins are made up of two parts : a native implementation , and a JavaScript interface . - Your PhoneGap application calls the JavaScript interface , which leverages cordovoa.exec to communicate with the native layer . - The native layer then performs a native operation and communicates back to the JS layer . <p> Open source PhoneGap plugins- There are lots of open source native plugins for everything from push notifications , screen captures , barcode scanners , to MapKit or even iCould ( among many others ) . <p> and of course , the sample apps : <h> iOS Multi-Screen <p> This sample app demonstrates how to create multi-screen experiences using the UIScreen API on iOS. - You can use AirPlay mirroring on an AppleTV as a second screen , the content @ @ @ @ @ @ @ @ @ @ " main " PhoneGap experience on the device . Check out the links and video below to learn more . <h> LowLatencyAudio <p> The PhoneGap LowLatencyAudio native plugin for Android and iOS allows you to preload audio , and playback that audio quickly , with a very simple to use API. - It overcomes the current limitations of the HTML5 Audio API on many mobile devices . Check out the links and video below to learn more . <p> Earlier this month the PhoneGap team held the first PhoneGap day . - This was in part to celebrate the release of PhoneGap 2.0 , but more importantly to bring together members of the PhoneGap community to share and learn from each other . - There are great recaps of PhoneGap Day from RedMonk , as well as on the PhoneGap blog . One of the new services announced on PhoneGap Day was- emulate.phonegap.com. - Emulate.phonegap.com enables an in-browser simulator for developing and debugging PhoneGap/Cordova applications , complete with Cordova API emulation . - It is built off of the Ripple Emulator , which itself is open source and may even @ @ @ @ @ @ @ @ @ @ launched , the URL that you want to simulate will be displayed within the Ripple operating environment view . <p> Note : This only works with assets that are on a live URL . You can use a local http server with references to localhost , however the emulator will fail if you try to access your application directly from the local file system using a file : // URI . <p> Update : You can enable access to local files by changing a few settings on the Ripple emulator . - See the first comment on this post for additional detail . <p> ( click for full-size image ) <p> The emulator environment gives you the ability to emulate PhoneGap events and API calls , without having to deploy to a device or run inside of the iOS , Android , Blackberry , or other emulator . - Not only can you simulate the PhoneGap/Cordova API , but you can also use Chromes debugging tools to test &amp; debug your code complete with breakpoints , memory inspection , and resource monitoring . This is a handy development configuration. - @ @ @ @ @ @ @ @ @ @ is familiar , fast &amp; easy to use . This does not replace on-device debugging however nothing will replace that . - On-device debugging is extremely important ; this helps increase your productivity as a developer . <p> So how do you use this environment ? The environment will handle Cordova API requests , and you can also simulate device events . <p> First , the " Devices " panel In this panel you can select a device configuration Everything from iOS , to Android , to BlackBerry . - Changing the device configuration will not only change the physical dimensions , but will also change Device/OS/user agent settings reported by the application . - Here you can also select the device orientation , which will change the visual area within the simulator . <p> Within the " Platforms " panel you can choose the platform you wish to emulate . - With respect to PhoneGap applications , you will want to choose " Apache Cordova " , and then select the API version that you are using . - By default , it uses " PhoneGap 1.0.0 ? , @ @ @ @ @ @ @ @ @ @ get the most recent version . - The Ripple emulator also simulates BlackBerry WebWorks and mobile web configurations as well . <p> The " Accelerometer " panel can be used to simulate device **25;6786;TOOLONG events . - Just click and drag on the device icon ( the gray and black boxes ) , and the icon will rotate in 3D. - As you drag , accelerometer events will be dispatched and handled within your application . - From here , you can even trigger a " shake " event . <p> The " Geolocation " panel enables you to simulate your geographic position within your PhoneGap/Cordova application . - You can specify a latitude , longitude , altitude , speed , etc - You can even drag the map and use it to specify your geographic position . The position that you set within the geolocation panel will be reported when using **42;6813;TOOLONG . <p> The " Config " panel is a graphical representation of your PhoneGap BuildConfig.xml file . - You can use this to easily view/analyze what 's in your application configuration . <p> The " Events " panel @ @ @ @ @ @ @ @ @ @ " deviceready " , " backbutton " , " menubutton " , " online " , and " offline " ( among others ) . - Just select the event type , and click on the " Fire Event " button . <p> As I mentioned earlier , this wont replace on-device debugging . - It also wont handle execution of native code for PhoneGap native plugins , however you can test/develop against the JavaScript interfaces for those native plugins. - Emulate.phonegap.com- will definitely help with development of PhoneGap applications in many scenarios , and is a nice complement to the- Chrome Developer Tools . <p> Youve probably heard of Adobe Edge , a timeline-based tool for creating interactive and animated HTML content . Edge enables you to easily create interactive experiences that rely only on HTML , CSS , and JavaScript . If you 've used other Adobe Creative Suite tools , such as Flash Professional , Premiere , or After Effects , then Edge will probably look quite familiar . You have a timeline and controls to edit your content . <p> Currently , the " normal " use @ @ @ @ @ @ @ @ @ @ when the page loads . - You can chain animation compositions in sequence , but they have to be in the same wrapper HTML file . - This works great for a number of use cases , but one thing I wanted to do is create an Edge animation and use that as a component that is arbitrarily added to the HTML DOM at any point in time . My findings : It can be done , although with a few gotchas . <p> Using Edge animations as components inside of a larger HTML experience is n't the primary use case which Edge was designed for . However this use case is being evaluated and may end up in Edge at a later date . If that happens , this process will become much easier . <p> If you 're wondering " What was I thinking ? " , Ill try to explain while discussing the process of building HTML-based apps , I had the thought : <p> Wouldnt it be cool to have a really elaborate loading animation while loading data from the server ? We could use Edge to build @ @ @ @ @ @ @ @ @ @ I created a very basic application that loads two separate Edge animations on demand . Before I go into too much detail on what I built , let 's take a look at the running example . This example has two buttons , one shows a car animation , one shows an airplane animation . Its pretty basic and straightforward : <p> The first thing that I did was create two simple Edge animations which you can view here : <p> Once the animations were complete , I started looking at the generated HTML output , and figuring out how I can add it to the HTML DOM of an existing HTML page . I then started putting together the sample application using- Mustache.js as a templating engine to abstract HTML views away from application logic . Note : I also have a simple utility that enables me to include Mustache.js templates in separate HTML files , so that I can keep everything separate . <p> First , I created the basic shell for the application . It is more or less an empty HTML structure , where all content is @ @ @ @ @ @ @ @ @ @ " div , all UI is added to the HTML DOM upon request . Basically , when the user clicks a button , the Edge animation is added to the DOM , and then the animation begins . <p> In order to get this working , I had to change a few things in the generated Edge output : <p> in the *edge.js file , I changed the DOM Ready event handler to use an arbitrary event that I can control . By default , Edge uses the jQuery $ ( window ) . ready() event to start the animation . Since I am adding this to an existing HTML DOM , the $ ( window ) . ready() event is not applicable . - Instead , I changed this to use a custom " animationReady " event : <p> In the *edgePreload.js file , I added a reference to the onDocLoaded function so that I can manually invoke it later , once the Edge animation has been added to the DOM , since again , this wont rely on the " load " event . <p> //added this so @ @ @ @ @ @ @ @ @ @ I also changed the aLoader object to reference the appropriate JavaScript files , since I changed their location in the directory structure : <p> Finally , I created the Mustache.js template , which will be used to generate the HTML DOM elements that will be appended to the existing DOM . - In this there is a wrapper DIV , some HTML content including a button and some text ( the animation number is dynamic for the templating ) , the styles , a " Stage " div , and Edge preload JavaScript files necessary for the animation . <p> Next , let 's look at how this is actually injected into the DOM . - I created a setupAnimationView() function to inject the animations into the DOM . - This function is used by both animations . The first thing that it does is remove any existing DOM content and dereference the AdobeEdge variables in memory . Since Edge was n't originally designed for- asynchronously- loading animations , I found it to be easiest to just wipe-out Edge and reload it for every animation . - The unfortunate side effect is @ @ @ @ @ @ @ @ @ @ at any given point in time . - Next , the setupAnimationView() function generates the HTML DOM elements and event listeners and adds them to the DOM. - Finally , I created an edgeDetectionFunction , which checks to see if Edge is loaded . - If not , it loads the Edge runtime . The edgeDetectionFunction() then checks if the Edge animation is sufficiently loaded . If the animation definition is not loaded , it just waits and tries again . - If the animation definition is loaded , it dispatches the " animationReady " event ( discussed in step 1 ) to invoke the actual animation . <p> Since I am using Edge in a manner for which it was not initially designed , there are a few " gotchas " that I ran into : <p> You cant have multiple instances of the same Edge animation in a single HTML DOM at least , not easily . - Each Edge animation is assigned a unique I 'd . - This I 'd is referenced in the HTML structure and the *edge.js , *edgeActions.js , and *edgePreload.js files . - You @ @ @ @ @ @ @ @ @ @ , and make sure everything is referenced consistently . <p> It will be very tricky- asynchronously- add more than one Edge animation at the same time . - The shortcut that I used to get these to render was to wipe away the Edge variables in JS and reload them this would cause some issues with more than one animation . <p> If the capability to have Edge animations as components gets built into Edge ( which I hope it does ! ) , then you will not have to go through all of these steps , and it will be much easier . - I 'll be sure to share more if this feature develops . <p> Ill be speaking at a few conferences in the next few months on PhoneGap and web standards-based development . - Here are just a few , with some more pending . - Be sure to come check one out ( or all of them ) ! <h> RIACON <p> Where architects and developers of all levels come to gather , share and learn about creating the next generation of web based applications. - @ @ @ @ @ @ @ @ @ @ professionals and expose you to the best content . <p> Ill be speaking on the following topics at RIACON : <p> Introduction to PhoneGap Interested in developing applications for mobile devices , on multiple platforms ? Interested in leveraging your existing web development skills to build natively installed applications ? Just looking to expand your skill set ? Come join Adobe Technical Evangelist , Andrew Trice , to learn about cross platform mobile development and PhoneGap . In this session , you will get an introduction to PhoneGap ( Apache Cordova ) , be able to see example PhoneGap applications , and walk through the process of building your first PhoneGap application . <p> PhoneGap Native Plugins PhoneGap enables developers to build natively installed applications using traditional web-based development tools ( HTML &amp; JavaScript ) , but what if you want to make your application do more ? In this session , learn how to write native plugins for PhoneGap that enable you to extend the API to tap into native device functionality . <p> Data Visualization with Web Standards Do you have the requirement to create rich visual data @ @ @ @ @ @ @ @ @ @ web-standard technologies , and do n't  know what to do next ? Well , you 're in luck ! Come to this session to learn about data visualization strategies and frameworks powered entirely with HTML and JavaScript. <h> 360iDev <p> 360iDev is the first and still the best iPhone developer conference in the world . We 're not a publishing company pushing books , or a media company selling subscriptions . We 're a conference company , focused on community . Our goal is to bring the best and brightest in the developer community together for 3 days of incredible sessions , awesome parties , good times , and learning . If you do n't leave Wednesday night , with more ideas than you know what to do with , we 're not doing our jobs ! <p> Ill be speaking on the following topics at 360iDev : <p> Kick A$$ iOS Apps with PhoneGap Apps do n't  have to be written in native Objective-C to be awesome. - - Get ready for a crash course in PhoneGap , a tool that enables you to build natively installed iOS apps using @ @ @ @ @ @ @ @ @ @ APIs. - Well cover everything from " what is phonegap " to strategies for building highly performant &amp; interactive applications . <h> Dreamforce <p> Every year Dreamforce features stories and presentations from some of the brightest minds in technology , business and beyond . This years Dreamforce promises to be even more informative and dynamic , with our most exciting keynote speaker lineup yet . The cloud computing event of the year is also the Social Enterprise event of the year . This is where you 'll learn everything you need to know " from the industry leaders who are paving the way " about how the Social Enterprise revolution is changing the way we do business . <p> Ill be speaking on the following topics at Dreamforce : <p> Data Visualization with Web Standards Do you have the requirement to create rich visual data-centric applications , but also have the requirement to use web-standard technologies , and do n't  know what to do next ? Well , you 're in luck ! Come to this session to learn about data visualization strategies and frameworks powered entirely with HTML , CSS @ @ @ @ @ @ @ @ @ @ applications built using web technologies can suffer from the " uncanny valley " effect where they do n't  feel quite right as a native application . In this session well focus on strategies to make your apps feel like native apps , including considerations for a native-feeling UI , platform consistency , and user experience . <p> Ive been working on lots of different projects lately . - On several of them , I 've had the need for a reusable list component . - In some cases , it needed to handle a large data set , in others it just needed to be self-contained and easy to use . - Out of these projects came MegaList : a reusable list component for jQuery , which Ive released as open source on Github . <p> MegaList is a jQuery plugin that creates a touch-enabled list component , capable of very large datasets , complete with data virtualization . It was originally intended for touch-enabled devices , however it also works in many desktop browsers . <p> For performance optimizations , the list component uses data virtualization techniques , so there @ @ @ @ @ @ @ @ @ @ what is currently visible on the screen . As the user scrolls through content , the list updates the DOM elements accordingly . This makes scrolling lists of thousands of items extremely fluid . - This works in a very similar way to ItemRenderer classes in Flex list and grid components . <p> You can employ the list component using one of two approaches . - One option is to declare the list structure in HTML markup , another option is to specify a dataProvider array , from which the list will create DOM elements . <h> Why ? <p> Sometimes you need a pre-built list that you can reuse . Sometimes you need to scroll through big data sets , and other times you just need component logic kept away from your app logic . It does n't  fit every scenario , but it certainly fits a few . <p> Data virtualization techniques allow you to quickly scroll through massive lists , without performance degradation . However , if your app really has 100K list items to scroll through , you should fire your UX designer . <h> Samples @ @ @ @ @ @ @ @ @ @ scrollable list component in action . All samples are interactive , and scrollable via touch or mouse events , with function event handlers . <p> Each of these examples can be scrolled using either the mouse or finger , and just tap/click on a row to select it , invoking the selection handler ( alert message ) . On the desktop , you can also scroll with the scrollbar . Note : I originally intended this for mobile on the desktop , I 've only tested in Chrome and Safari . <p> Simple List Created With Inline LI Elements This is a basic example with a list of 50 LI elements . <p> Note : These inline/embedded examples are contained inside of iframes if you mouse-up outside of the iframe , the iframe contents wont receive the event . If MegaList is used in a page , without a wrapping iframe , you do n't  run into this issue . Follow the " View Sample " links above to see them without the iframe issue . <h> Observations <p> Contrary to my expectations , using CSS3 translate3d is actually slower than @ @ @ @ @ @ @ @ @ @ you enable CSS3 translate3d and set backface visibility , there is an extremely noticeable performance degradation on both desktop and mobile devices . <p> I 've experimented with lots of permutations to get the best performance possible . I 'm not finished yet , but I 've found that you can achieve significantly faster performance of DOM manipulation by removing elements from the DOM , manipulating them , then re-adding them . This is what is done within the updateLayout() method . The &lt;ul&gt; is removed from the DOM , &lt;li&gt; elements are added or removed , and then the &lt;ul&gt; is added back to the DOM . You may see a flicker on rare occasions , but I did n't  find this overly intrusive . <p> For small data sets , this may not be much advantage you can get better performance by just using something like iScroll in a &lt;div&gt; containing a &lt;ul&gt; . With large data sets , this is definitely faster . <p> The more complex the HTML inside of your label function , the slower the animation will be . <h> Download <p> The full source code for @ @ @ @ @ @ @ @ @ @ landing page for API documentation and samples . 
@@106848908 @2248908/ <h> Tag Archives : creative cloud <p> Its an exciting day for Creative Cloud members ! Two significant updates have just been released ! <p> The first major update is for- Adobe Photoshop CC . This release introduces Adobe Generator for Photoshop CC . Generator enables real-time generation of image assets , and also enables new workflows based upon its JavaScript-based architecture . Changes include : <p> Real-time image asset generation gives Photoshop CC users the power to eliminate time-consuming production steps while ensuring their designs are properly implemented both on the desktop and mobile screens <p> Tagged layers and groups in Photoshop CC are automatically saved and updated in real-time as individual files , in the format selected <p> Layers can now be exported as JPEG , GIF , or PNG with a variety of options , including scaling for Retina displays and varying levels of compression <p> In one click , Edge Reflow CC can import Photoshop CC assets , including images and text directly into Edge Reflow , allowing designers to immediately begin their responsive design process and reduce manual production steps <p> You can learn @ @ @ @ @ @ @ @ @ @ video below : <p> Generator enables new workflows by simplifying the process of exporting content from Photoshop . This could just be for image extraction , could be used to simplify design processes , or even integrate with a larger workflow . Some example uses are these iOS icon and PhoneGap Build icon generators for mobile applications , by Paul Burnett . You can see these in action in this video from Raymond Camden . <p> How else might this affect your workflow ? Well , check out the new Photoshop to Adobe Edge Reflow workflow powered by Generator in the video below . With Generator , Edge Reflow can now seamlessly connect to Photoshop and extract assets for inclusion in Edge Reflow designs . <p> Read more more about the Edge Reflow and Photoshop CC live integration on html.adobe.com . <p> Seriously , though this is just the- proverbial " tip of the iceberg . " Check out the next video to see how the Loom game engine can leverage Adobe Photoshop CC and Generator for live authoring of content inside of a game running on separate mobile devices @ @ @ @ @ @ @ @ @ @ Pipeline between Adobe Premiere Pro CC and SpeedGrade CC- provides an integrated workflow that allows users to move multi-track timelines seamlessly back and forth ; open Adobe Premiere Pro CC sequences in SpeedGrade quickly ; and see the results as effects in Adobe Premiere Pro CC that are managed by the Lumetri Deep Color Engine . <p> Expanded native support- for UltraHD , 4K and higher resolutions , - high frame rates and RAW formats , enables editors to work with footage from the hottest new high-res cameras natively " without having to wait to transcode and re-wrap files . <p> The Mask Tracker in After Effects- enables video professionals to create masks and apply effects that track automatically frame-by-frame throughout a composition to save countless hours of tedious work . <p> Performance enhancements- punctuate this release with support in Premiere Pro CC for OpenCL , providing editors with the speed and power they need for the most demanding projects ; and new GPU debayering of the Cinema DNG file format for real time playback . <p> A preview of the upcoming Prelude CC Live Logger iPad app , which @ @ @ @ @ @ @ @ @ @ data on their iPad while shooting , including the ability to sync with timecode on set via supported wireless timecode generators , and then sync metadata to footage via Creative Cloud for faster editing . <p> Advanced color grading- with the new SpeedLooks in SpeedGrade CC offers dedicated camera patches . This allows users to match the color spaces even across different camera formats . New multiple masks and linked mask layers capabilities also enable SpeedGrade users more control over complex looks . <p> New production planning features in Adobe Story Plus- provide powerful scheduling and reporting tools for managing productions efficiently , making it easy to modify and share lists between productions and users . <p> Read more about the new updates for video editing tools in the release notes below : <p> Want to get inspired , learn the latest and greatest about Adobe and Creative Cloud , or get tips on all of Adobes creative tools ? - If you answered yes to any of those , then be sure to check out the Create Now World Tour coming to a city near you . - It @ @ @ @ @ @ @ @ @ @ to miss it ! <p> CREATE NOW WORLD TOUR- brings Adobe experts and evangelists to your city , so you can learn new ways to create right from the pros . Join these free events to discover new tips and tricks that will help you work more efficiently and give you more control over all your projects . <p> Some days you are just meant to be creative I think yesterday was that day for me . I saw that Erik Johansson released a new composition , and it sparked a wave of creativity within me . I discovered Eriks work via Adobe Max this year his presentation was awesome and very inspiring . Seriously , do yourself a favor and go watch it . <p> I digress I 've been doing a lot of aerialphotography lately , and after seeing Eriks latest composition I thought to myself : why not take some of my aerial photos and start altering reality ? My immediate idea was to create a surrealistic composition where you are looking down through a crystal ball . Next thing you know , this happened <p> Here are @ @ @ @ @ @ @ @ @ @ v.2 ( without the flames is my favorite ) : <p> Crystal Ball v.1Crystal Ball v.2Crystal Ball v.2 with Flames <p> Attribution : <p> The aerial image was mine , captured with a DJI Phantom and GoPro- camera . Check out the original image on Flickr . <p> Earlier this week I did a post on the new lens profiles for GoPro cameras in Camera Raw 8.2 and Lightroom 5.2 . I mentioned in that post ( and have had several conversations with people since then ) that you can also reduce the fisheye effect using Photoshop CCs Adaptive Wide Angle filter . So I decided to put together a quick post showing the output of the Lens Correction and Adaptive Wide Angle filters side-by-side on the same image . <p> Check out the video see how to apply both techniques , and compare the output . <p> You can check out the final images here ( shrunk to 600+450 for the blog ) : <p> Do you like keeping up with the latest news and updates from Adobe ? Do you like reading the latest posts and articles from @ @ @ @ @ @ @ @ @ @ featured/highlighted content from Adobe ? Do you like reading your content onFlipboard ? <p> The- Adobe Evangelists Magazine- is a manually curated aggregation of all of our blogs , plus articles and posts that we find interesting and applicable , all combined into one great resource . All links point to the original source content , not a copy . The intent is to provide a one-stop-shop for Adobe Evangelists goodness . 
@@106848909 @2248909/ <h> Category Archives : Windows Phone <p> here 's a video from the " What Developers Love and Hate about iOS , Android , Windows and HTML5 ? panel discussion that I took part in during MoDev East- back in December . - The panelists represented native app developers , HTML/web developers , and myself-representing the HTML/hybrid app paradigm . <p> I thought it was a great discussion , and nearly everyone acknowledged the benefits of choosing a hybrid solution such as- PhoneGap . ( That 's me in the brown jacket . ) <p> I am asked all the time " How do I get started developing PhoneGap applications ? " . My normal answer is to advise people to check out the PhoneGap Getting Started Guides , which provide a great starting point for every platform . However after further thought , I 'm not sure this is always what people are asking . Rather than " how do I get started ? " , I think people are often looking for insight into the workflow for developing PhoneGap applications . Everything from tools to developer flow , to getting @ @ @ @ @ @ @ @ @ @ essential for setting up the initial project structure , but once you get that setup , you might be wondering " what do I do next ? " . In this post , I 'll try to she 'd some light on the workflow and tools that I use when developing PhoneGap applications . <h> Know What You 're Going To Build Before You Build It <p> First and foremost it is essential to have at least some kind of idea what you are going to build before you build it . If you just start hacking things together without a plan , the final result is seldomly great . Complete ( pixel perfect ) UI/UX mockups are fantastic , but you do n't  have to have a fully polished design and screen flow . Just having wireframes/sketches are a great start . Heck , even a sketch on a napkin is better than starting with nothing . <p> The UX design/wireframes help you understand what you application should be doing from the users perspective , which in turn helps you make decisions on how you tackle a project . This can be @ @ @ @ @ @ @ @ @ @ how you should position DOM elements and/or content . Or , it can help you gauge your projects technical complexity How many " moving parts " do you have , how much of the app is dynamic or asynchronus , or how do different visual elements need to work together ? You can leverage this design/mockup to analyze the needs of your application and determine if a particular framework/development methodology is a best fit ( Bootstrap , Backbone.js , Knockout.js , Sencha , jQuery Mobile , Angular.js , etc ) . <p> When working with a designer , I use Adobes Creative Suite Tools for pretty much everything wireframes , UI/UX designs , chopping up assets , etc I 'm currently working on a project that was designed by the talented- Joni from Adobe XD . Joni designed everything in Creative Suite , and I 'm using Photoshop to view screen flows and extract UI assets for the actual implementation . <p> UI Mockups in PhotoshopScreen Flow in Photoshop <p> Note : This app will also be free and open source as a sample/learning resource for PhoneGap , including all of the @ @ @ @ @ @ @ @ @ @ later , once the app is- available- in the app stores . <p> If you are n't  a " graphics person " , or do n't  have creative suite , there are a bunch of other tools that you can use for wireframing and/or sketching ( but cmon , Creative Cloud is only $50 a month ) . Here are several Ive used with great success , but this is not a comprehensive list at all : <p> OmniGraffle- A drag &amp; drop wireframing tool for OS X. This is fantastic for wireframing or documenting screen flows . In fact , the screen flow image shown in Photoshop above was originally composed in Omnigraffle , using the mockups created in Photoshop . <p> Visio- A powerful drag &amp; drop wireframing/design tool for Windows much like OmniGraffle , but for windows . <p> PowerPoint- or Keynote- - These are n't  just for presentations . They can be really useful for putting together screen flow diagrams , or annotating images/content . <p> Often people like to sketch out ideas &amp; wireframes on their tablets , here are a few tools that I @ @ @ @ @ @ @ @ @ @ collaboratively taking notes and sketching . I 'm partial to this one b/c used to be on the dev team , and I wrote a good chunk of the graphics sketching logic . <p> There are a bunch of other tablet sketching apps out there , but I have n't used most of them . <h> Coding Environment <p> Coding environments are a tricky subject . There is no single solution that meets the exact wants and needs for everyone . Some people chose lightweight text editors , some people chose large-scale IDEs , some people use designer-centric tools , and many of these choices are- dependant- upon which operating system you use or your background as a designer or developer . Since PhoneGap applications are really just editing HTML , CSS &amp; JavaScript , you can use whatever editor you want . In fact , I know a several people that use vim as their primary editor . <h> Large-Scale IDEs <p> I 'm a bigger fan of of using a complete IDE ( integrated development environment ) than I am of a lightweight editor , simply b/c IDEs tend to have @ @ @ @ @ @ @ @ @ @ about startup time , but there is no startup time if you leave it open all the time . <p> There are a few catches when talking about IDEs with PhoneGap . The first is that if you want to deploy anything locally to devices ( without using PhoneGap Build ) , you have to delpoy using the IDE for the particular platform that you are- targeting . That means Xcode for iOS , Eclipse for Android , or Visual Studio for Windows Phone , etc However if you wish , you can use your editor of choice , and just use the IDE to deploy to devices locally . You can even share source code across several IDE installations using symlinks ( which I describe here ) . I very often use this type of a configuration to share code between Xcode , Eclipse , and WebStorm . <p> My preference for coding PhoneGap applications is to use- WebStorm by JetBrains . WebStorm has great code-hinting ( even for your own custom JS and 3rd party libraries ) , great refactoring , hooks into Git , CVS , or @ @ @ @ @ @ @ @ @ @ <p> WebStorm IDE <p> I tend to use this as my primary coding tool , then switch to Eclipse or Xcode when I want to locally deploy to a device for testing . When using PhoneGap Build to simplify cross-platform compilation , I just push the code to git , then recompile via PhoneGap Build . <p> I 'm not a fan of Xcodes HTML/JS editing , and havent found an HTML/JS plugin for Eclipse that I really like . To target Windows devices , I use Visual Studio . <h> Lightweight Editors <p> I 'm a bigger fan of larger IDEs than lightweight editors , but- Adobe Edge Code ( also known as Brackets ) is a great lightweight editor for quick edits. - Edge Code/Brackets is an open source HTML/JS editor that supports live editing in the browser and inline editors for CSS styles , without leaving your HTML files . If you tried Edge Code Preview 1 , but werent sold on it , you should try Edge Code Preview 2 . The team has come a long way very quickly . Its fast , easy to use , @ @ @ @ @ @ @ @ @ @ Build . I sometimes use this for quick edits . <p> There are tons of other lightweight editors out there , and everyone has their favorite . As long as you 're happy with the tool , and it can edit text ( HTML , CSS , JS ) files , you can use it to build PhoneGap applications . <h> Designer-Friendly Editors <p> I 'm not necessarily the primary target for Dreamweaver , but it has some nice features . Dreamweaver gives you a great programming environment plus a WYSIWYG editor for HTML experiences . It also features PhoneGap Build integration directly inside the coding environment . If you 're used to Dreamweaver for creating web experiences , you can continue to use it and target mobile apps as well . <p> Adobe Dreamweaver <h> Debugging Environments <p> Yes , that is plural Debugging Environments . Due to the cross-platform nature and PhoneGaps leveraging of native web views for each platform , debugging PhoneGap applications can sometimes be tricky . Here are some tips that will make this significantly easier . <h> The PhoneGap Emulator <p> The PhoneGap Emulator is my primary @ @ @ @ @ @ @ @ @ @ browser-based emulator leveraging the Google Chrome browser and the Ripple Emulation Environment . The PhoneGap Emulator runs inside of Google Chrome , and provides emulation of PhoneGaps core APIs . Since it is built on top of Chrome , it enables you to leverage Chromes Developer Tools , which in my opinion are second to none for web/application development . This is a highly-productive developer environment . <p> PhoneGap Emulator in Google Chrome <p> here 's why I like the PhoneGap/Ripple/Google Chrome development environment : <p> First , - this combination enables you to emulate most core PhoneGap APIs without leaving the desktop environment . It enables you to test various APIs including geolocation ( with simulated locations ) , device events ( deviceready , back , etc ) , sensor events ( accelerometer , compass ) , and even let 's you test with different device aspect ratios all without having to push anything to an actual device . This saves a lot of time in development iterations . You can read about the supported Ripple emulator features here . <p> Second , Chromes Developer Tools are awesome . Here are @ @ @ @ @ @ @ @ @ @ your app , live within the emulation environment : <p> Analyze all resources consumed by your app , via the resources panel . This includes all scripts , images , html files , cookies , etc it even includes insight into any local data stored via PhoneGaps local storage database ( WebSQL implementation ) . <p> View/query all local databases within your app . You can write your own queries to view/alter data in the WebSQL database . Thanks to Ray for sharing this , its not immediately intuitive . <p> Debug JavaScript with the Scripts/Sources Panel . You can set breakpoints in JS execution , inspect &amp; alter values in JS objects in-memory , and view details and line numbers for any exceptions that occur . <p> Use the console to monitor console.log() statements , inspect properties of objects in memory , or execute arbitrary JavaScript whenever you want . <p> The PhoneGap Emulator enables developers to be extremely productive with development , however I can not emphasize enough that on-device testing is critical for having a successful app . On-device testing can expose performance problems or browser rendering @ @ @ @ @ @ @ @ @ @ . <h> On-Device Remote Debugging <p> As I mentioned above , on-device testing is critical for successful applications . iOS and BlackBerry have an advantage over other platforms b/c the latest developer tools allow you to remotely debug content live on a device . <p> Since the release of iOS 6 , you can debug content in the iOS simulator using Safaris Developer Tools . Safaris developer tools give you many of the same debugging capabilities that I mentioned above for Chrome . <h> Remote Debugging with Weinre <p> Not every platform supports live remote debugging , especially older versions . Weinre ( pronounced winery ) is a remote web inspector that allows you to inspect/edit DOM and CSS elements on remote devices . Basically , you include some JavaScript in your app , and it communicates back to a server that will tell you what 's happening inside of the app running on the mobile device . It wo n't give you full debugging capabilities like JS breakpoints and memory inspection , but its better than nothing . You can use Weinre by setting up your own instance , or by @ @ @ @ @ @ @ @ @ @ All Else Fails <p> If you 're still debugging your apps , and the solutions mentioned above do n't  work , you can always resort to plain-old " alert() " statements to pop up debug messages , or use " console.log() " statements to write to system logs . <p> On Android , all- console.log ( ' ... ' ) ; - messages will appear as printouts in the command-line tool- logcat , which is bundled with the Android SDK and integrated into the Android Eclipse plugin . <p> On BlackBerry , all- console.log ( ' ... ' ) ; - are printed to the BlackBerrys Event Log . The Event Log can be accessed by pressing- ALT + LGLG . <p> On iOS , all- console.log ( ' ... ' ) ; - are output to the Xcode Debug Area console . <h> Building PhoneGap Apps <p> The PhoneGap getting started guides will point you to the right direction for getting started with a particular platform . If you are just targeting iOS , you can use Xcode for building . If you are just targeting Android , you can @ @ @ @ @ @ @ @ @ @ get up and running . <p> However , this process gets much more complicated when targeting multiple platforms at once . When I have to do this , PhoneGap Build becomes really , really handy . <p> PhoneGap Build allows you to either upload your code , or point to a Git repository . PhoneGap Build will then pull your code and build for 7 different platforms , without you having to do anything special or setup multiple environments . All that you 'll have to do is install the cloud-compiled binaries on your device . You can do this by copying/pasting a URL to the binaries , or by capturing a QR code that will directly link to the compiled- application- binary . <p> One other advantage of PhoneGap build is that it let 's designers/developers build mobile applications without having to install any developer tools . If you want to compile a PhoneGap app for iOS , but are on Windows just use PhoneGap build and you wo n't need Xcode or a Mac . <h> PhoneGap UI/Development Frameworks <p> Probably the most common PhoneGap question that I get asked is @ @ @ @ @ @ @ @ @ @ If you 've been waiting for me to answer this , do n't  hold your breath . It is impossible to be prescriptive and say that one solution fits all use cases for every- developer . <p> When people ask me this , I like to paraphrase- Brian Leroux from the PhoneGap team : " Use HTML , it works really well . " <p> I think people often overlook the fact that PhoneGaps UI renderer is a system web view . Anything that is valid HTML/CSS content can be rendered as your applications user interface . This could be something incredibly simple , like text on the screen , or it could be incredibly creative or complex . The important factor is that you need to focus on a quality user experience . If you 're worried about your UX , and are worried that Apple may reject your app , then read this article where I explain Apple rejections in detail . <p> HTML/JS developers come from many different backgrounds , with varying degrees of programming expertise . Some frameworks appeal to some people , other frameworks appeal to other @ @ @ @ @ @ @ @ @ @ architectural frameworks popping up every week . It would be a disservice to all people who use PhoneGap for us to proclaim that we should only use one singe framework . <p> There are lots , and lots , and lots more options out in the HTML/JS development world I 'm not even taking into account JavaScript generating tools and languages like CoffeeScript , TypeScript , or others <p> Today Raymond Camden and I hosted another open session on PhoneGap- as part of an Adobe TechLive event . - These sessions are an opportunity for anyone to stop in and ask us questions . <p> The Q&amp;A transcript from todays session is below . - Thanks to everyone for sticking around , and bearing through our technical difficulties The normal Q&amp;A pod was n't working for some reason , so we had to improvise. - Well make sure this is working for next time . <p> This was our third event , and we 've had a great turnout so far , so we will be holding these open sessions once a month . - If you werent able to make it this @ @ @ @ @ @ @ @ @ @ check the- Adobe TechLive- page for future events . <p> The Fresh Food Finder is an open source mobile application built using PhoneGap ( Apache Cordova ) that helps users locate local farmers markets that are registered with the FDA. - You can search for specific markets , or find the closest markets to your current location . - Check out the video below to see it in action : <p> It was originally intended to just be a sample application for- app-UI- with full source available here , - but happens to be quite popular and useful in the " real world " as well . <p> In fact , the Fresh Food Finder made it all the way up to #18 in the iPad " Lifestyle " category on iTunes in the first week of its release , and even made one of the featured apps in the Lifestyle category : <p> All of the information displayed within the Fresh Food Finder is freely available from the US Department of Agriculture through data.gov . This data set was last updated on April 25 , 2012 . <p> The @ @ @ @ @ @ @ @ @ @ is improved data , with market schedules . I 've heard everyone loud and clear , and am happy to say that I have some improved data on the way ( including schedules and times ) , so keep an eye out for it in the not-to-distant future . <p> The Fresh Food Finder is written entirely using HTML , CSS , and JavaScript , and runs on numerous platforms . - It is currently available for iOS and Android . I 've submitted it for approval in the Windows Phone Marketplace , but its still awaiting approval . - I 've also tested it on the BlackBerry Playbook , and it works great there too , but I just have n't gotten around to submitting it to BlackBerry App World yet . <p> The Fresh Food Finder can be downloaded today in the following markets : <p> PhoneGap : http : //www.phonegap.com- PhoneGap is an HTML5 app platform that allows you to author native applications with web technologies and get access to APIs and app stores . <p> App-UI : http : **29;6857;TOOLONG App-UI is a free &amp; open source collection of reusable @ @ @ @ @ @ @ @ @ @ helpful to web and mobile developers for creating interactive applications using HTML and JavaScript , especially those targeting mobile devices . <p> Mustache : https : **30;6888;TOOLONG - Mustache is a logic-less template syntax . It can be used for HTML , config files , source code anything . It works by expanding tags in a template using values provided in a hash or object . <p> The entire user interface of the application is created dynamically at runtime based on JavaScript and the Mustache templates. - You can download the full application source code at- https : **38;6920;TOOLONG - Feel free to fork it , or use it as a learning tool to build UI experiences for PhoneGap applications . <p> The code is organized into the following structure : <p> assets This folder contains fonts , images , and CSS styles used within the application . <p> js - This folder contains JavaScript resources and libraries used within the application . <p> When the application loads , all templates are loaded into memory as part of the bootstrapping/startup process . - Once all the data and templates are @ @ @ @ @ @ @ @ @ @ user . The majority of the application logic is inside application.js , all views are rendered from the Mustache templates inside of viewAssembler.js , and all UI styling is applied via CSS within styles.css . <p> Mustache is a templating framework that enables you to easily separate presentation layer ( HTML structure ) from application logic and the data model . - Basically , you create templates that Mustache will parse and convert into HTML strings based upon the data that gets passed in. - I 'll write another post later about Mustache , but it can be extremely useful for larger applications . <p> If you 've ever used a Windows Phone devices web browser , then you have probably noticed the gray highlight that the Windows Phone browser puts on top of the HTML content any time that you click on a link . For static web pages , this is n't a big deal . It indicates that you have clicked on an item . However , if you are building apps with dynamic content , this can become much more frustrating and can have a negative impact in overall @ @ @ @ @ @ @ @ @ @ to help ! <p> In PhoneGap applications , you can build experiences that respond to mouse/touch events , which can be more dynamic than what is supported in the device browser . Note : The device browser does not support mouse events . All touch input gets translated into mousedown , mousemove , or mouseup events inside of the PhoneGap container . This gray highlight can become a major pain point in highly-dynamic applications . Luckily , I 've found a way to get rid of it . Check out the video below to see an example of the gray highlight , as well as the workaround in action : <p> Through lots of trial and error , I 've found that the gray highlight is applied to all &lt;a&gt; anchor elements , and any html element that has a " click " event handler or lingering " mousedown " and " mouseup " event handlers. - - Luckily , you can manage event listeners and add/remove event handlers as necessary so that you minimize the gray box . - This approach does not get rid of the gray box 100% of @ @ @ @ @ @ @ @ @ @ that it gets rid of it 90% of the time . - After applying these techniques , I only see the gray highlight if you tap on multiple clickable items in rapid succession . <p> here 's how I was able to get rid of the gray highlight box : <p> Do not use &lt;a&gt; anchor tags ever . <p> Assign a " mousedown " event handler to the &lt;span&gt; , &lt;div&gt; or other element that you want to be clickable . <p> When the " mousedown " event handler is invoked , remove the " mousedown " event handler , save the mousedown input coordinates , and add a " mouseup " event handler to the window object . <p> When the " mouseup " event handler is invoked , remove the " mouseup " event handler , and then compare the mouse coordinates . If the mouse coordinates have not changed , and its within a reasonable amount of time , you can infer that this should be a " click " event , and invoke an action as desired . <p> Set a timeout to restore the " @ @ @ @ @ @ @ @ @ @ If you re-add the " mousedown " event handler inside of the " mouseup " event handler , you will still get the gray box . <p> To make things even more confusing , if you have dynamic content ( changing DOM elements ) underneath of your touch input , then you get multiple mousedown and mouseup events invoked on different DOM elements underneath where you touched . - This happens even if you only touched the screen once . I was able to intercept mouse events and prevent their default actions to mitigate this behavior . <p> Sounds confusing , right ? - Well , it can certainly be tricky . - Luckily for us all , I was able to encapsulate all of this functionality inside of a reusable JavaScript class that anyone can use without needing to understand all of the ins &amp; outs . - The TouchClick.js class is available in the example on github under " WinPhone-NoGrayBox " . - In the TouchClick.js library , it override the default HTMLElements addEventListener function and intercepts all calls to addEventListener ( " click " ) and replaces @ @ @ @ @ @ @ @ @ @ click action , but without the gray highlight box . - Thus making your app experience better , and keeping your code simple . <p> Once you include TouchClick.js , you add event listeners as you normally would , and it even works with jQuerys event wrappers : 
@@106848911 @2248911/ <h> Flex/AIR on Devices : From Flash Builder to Android Ecosystems <p> In this post , we will cover the workflow for delivering a solution from development in Flash Builder through packaging for various Android ecosystems . - In subsequent posts , this will be followed up with detail for delivering to each individual ecosystem . <p> You can start developing apps for Android using the debugging simulator built into Flash Builder . However , a critical step in your development process is that you must test and evaluate your applications on a physical device ( preferably before you try to ship it ) . - Luckily for all of us , that is really easy with Flash Builder . <p> First , develop your mobile application . - When you are ready to test it on a device , simply go to the " Debug " menu and select " Debug Configurations " . <p> Make sure that your test device(s) are plugged in via USB , and then hit the " Debug " button . - Using this approach , you can deploy to any Android device @ @ @ @ @ @ @ @ @ @ phones and tablets to enable USB debugging , you just need to go to Settings -&gt; Applications -&gt; Development , and then select the checkbox to enable USB debugging . - - You can also use this method to debug on a Nook Color device , however you must have it unlocked already via your Nook developer account . <p> Once you have developed and debugged your application , you are ready to export it for real world consumption . - - The first thing you need to do is specify you applicaiton name , version and i 'd in your app.xml file . - Be sure that these are the final values , as the application i 'd will be the unique identifier for your application in all marketplaces and if you distribute the APK manually . <p> Application XML Configuration <p> Once your application configuration is finalized , you will be ready to actually export the project . - Right-click on your project in the package explorer and go to the " Export " option . - You can also perform the same action by going to the File -&gt; @ @ @ @ @ @ @ @ @ @ select the " Release Build " export option , and click " Next " . <p> Export Release Build <p> Set your export folder , and a base filename for the output file(s) , be sure to select the target platform " Google Android " , and export as " Signed packages for each target platform " . - Click " Next " to setup your platform- signing- options . <p> Export Release Build Options <p> For Android , you can use a purchased signing certificate , or use a self-signed certificate . - Just use the browser button to locate your signing certificate , or click " Create " to crate a certificate . <p> Export Release Build - Platform Signing Options <p> If you create your own certificate , the following window will be displayed . - Just enter a publisher name ( your name ) , a password , and a file location . - Next click " OK " and the certificate will be created and you will return to the Platform Signing Options screen . <p> Self Signed Certificate <p> Next , click " Finish @ @ @ @ @ @ @ @ @ @ Export Release Build - Platform Options <p> Now that you 've generated your APK binary file , you are ready to push to the various app stores Let 's take a look at this process for each ecosystem : 
@@106848913 @2248913/ <h> Creative Cloud Helps Me Fulfill My Creative Vision <p> In my last post , I proclaimed my love for Adobe Creative Cloud . This post will show you the reason why . I was playing around with some of the aerial footage I captured last week in San Francisco . Just for fun , I wanted to create a HUD ( heads up display ) to add to the first-person experience of the video . My inspiration was the HUD created for The Avengers &amp; Iron Man , which was created using Adobe After Effects . This turned out far better and far more interesting than I could have possibly hoped , and it is all thanks to the power of Adobe Creative Cloud . here 's the final video complete with special effects , and below I will discuss how I used Creative Cloud to get to this . ( Best experienced at 1080p , with audio preferably loud , with lots of bass . ) <p> First things first , I had to design the HUD . I used Photoshop to pull in a still from the @ @ @ @ @ @ @ @ @ @ it . I Googled images of real fighter jet HUD displays , and used those as inspiration . I obviously did n't  have all of the same information , so I could n't make my HUD absolutely real , but I could make it look " good enough " . <p> HUD Design in Adobe Photoshop <p> I got the mockup to a point that I thought looked good , and then it came time to implement it for real in the video . It turns out my initial design did n't  work great in the final implementation , so I came back to Photoshop played with colors , and sizes , and chopped pieces up into separate image assets that I could pull into the final composition . <p> Next , I pulled the video into After Effects , and started overlaying the HUD graphic elements . - I chose After Effects for this instead of Premiere because After Effects has better control over the visual output and effects Premiere is my primary tool for sequencing multiple clips into a larger composition . <p> Editing in Adobe After Effects <p> @ @ @ @ @ @ @ @ @ @ rotation and position so that it fit well with the actual flight path . Everything seemed in place , but I felt like it needed more . <p> Why not have targeting indicators that follow the cars ? With After Effects Track Motion feature , this was easily done . I created a " target " graphic , inserted it into the composition , and then used Track Motion to create a motion path for the graphic . To do this , select the video layer that you want to use for motion tracking , and go to a frame that has the object that you want to track . Then click on the " Track Motion " button in the " Tracker " panel . Youll have to select an area that will be tracked . When you analyze frames , it will detect the movement of your selection over time , and translate that to x/y coordinates , which are applied to the motion target that you choose ( the " target " symbol ) . <p> I repeated this step for a bunch of vehicles , and @ @ @ @ @ @ @ @ @ @ red target indicators in the HUD , I thought " that looks cool , but its still not enough , and its not believable . " <p> I added some color correction using After Effects Tritone color correction . - This made the HUD really stand out from the video , and gave it a nice cinematic look and feel , but I still wanted more . <p> I thought to myself If you 're going to go " over the top " , you might as well go " way over the top " , so I started getting creative/ridiculous . I had this robotic fighter jet feel in the video , so I figured that something needed to blow up . I found this explosion and that 's when things started getting really interesting . I added one , then two , then three explosions to the scene by leveraging After Effects Linear Color Key effect so that the explosion was overlaid without the background . Add some color correction on the explosion , and voila you have an explosion on top of the video with minimal artifacts . <p> @ @ @ @ @ @ @ @ @ @ the background based on pixel colors . You can also remove pieces of a video clip using the rotoscoping tool- its like a Photoshop selection over time . <p> This was really starting to come together . Since I had explosions , I needed more smoke . I first tried the- After Effects particle system- for producing smoke , but it did n't  seem real enough for this specific use case. - I found some stock footage of smoke plumes and ambient smoke , and started going to town . Pairing the stock footage with Track Motion , I was able to add smoke to the landscape that followed buildings as the camera rotated to focus back on the building . <p> Like I said earlier , I wanted to go " way over the top " , so of course , why not add a flyby from some jets . So I added some stock footage of computer generated jets flying overhead , again with Keying to remove the background . <p> At this point , things were really coming together for this scene , so I wanted to @ @ @ @ @ @ @ @ @ @ . <p> Composing/Sequencing in Adobe Premiere <p> Here I added the title , and started adding the static effect overlaid in the beginning and the ending of the composition . - Next , I needed background music and sound effects . - Sound is critical to the experience of video . - It can help convey emotion , and tie everything together . <p> I pulled in some background music from Audio Jungle . Things were starting to come together really well , but I needed more sound effects- A while back I stumbled across freesound.org , which has a bunch of Creative Commons sound effects . - This has been a goldmine for me . I pulled in sound effects for the explosions , the ambient aircraft noise , ambient machine guns , and radar beeps . <p> Then I pulled some of the sounds into Adobe Audition for some fine-tuning <p> Audio Processing with Adobe Audition <p> Once I had everything sequenced where I wanted it , I just exported the video from Premiere , uploaded it to Youtube , and started sharing it . <p> The best @ @ @ @ @ @ @ @ @ @ whole thing start to finish in a little over one day . - I started working with the video on Monday night , and uploaded it to YouTube this morning . Creative Cloud has an insanely productive workflow . <p> My background is in software development , not in video production I do that for fun . By using Creative Cloud , I already had all the tools I needed to put everything together . 123433 @qwx983433 <p> Nice work here ! I know you said you do video production for fun , but if you ever worked on a production for someone else and needed inline feedback and timestamped comments for your video you should give Arc9 a try . It easily integrates with Adobe CC and you do n't  have to worry about file conversions or anything like that . Its all done automatically . 
@@106848915 @2248915/ <h> Category Archives : events <p> The keynotes and many of the session videos form Adobe MAX are now available online at **27;6960;TOOLONG , and are free for everyone to view . Keep in mind , not all videos are up yet , but will be added soon . <p> The first-day keynote focuses on Adobes evolution of Creative Cloud and various product updates . The second day keynote focuses upon inspiration do not miss this one . - I especially liked Erik Johansson and Rob Legattos segments . <h> Day 1 Keynote : A Creative Evolution <p> About This Episode:The process of where and how we create is dramatically changing thanks to major advancements in technology , and there has never been a more exciting time . Join Adobe CEO Shantanu Narayen , Adobes SVP and GM of Digital Media David Wadhwani , and a collection of Adobe visionaries across digital photography , web design , illustration , video and more as we unveil brand new creative workflows and capabilities . Well take a look at the present and set our sights on the endless possibilities in our @ @ @ @ @ @ @ @ @ @ Creativity <p> About This Episode : - Join David Wadhwani , Adobe 's SVP and GM of Digital Media , as he welcomes four incredibly creative minds to explore how they foster creativity and approach their work . You will hear from Rob Legato , an Oscar winning visual effects supervisor ; Paula Scher , an iconic graphic designer and illustrator ; Erik Johansson , an up and coming photographer and retouch artist ; and Phil Hansen , a constraint-based artist that believes limitations drive creativity . We think you 'll leave with more than a few new ideas to incorporate in your next creative project . <p> Ill be speaking at a few conferences in the next few months on PhoneGap and web standards-based development . - Here are just a few , with some more pending . - Be sure to come check one out ( or all of them ) ! <h> RIACON <p> Where architects and developers of all levels come to gather , share and learn about creating the next generation of web based applications. - RIAcons goal is to help you network with fellow @ @ @ @ @ @ @ @ @ @ <p> Ill be speaking on the following topics at RIACON : <p> Introduction to PhoneGap Interested in developing applications for mobile devices , on multiple platforms ? Interested in leveraging your existing web development skills to build natively installed applications ? Just looking to expand your skill set ? Come join Adobe Technical Evangelist , Andrew Trice , to learn about cross platform mobile development and PhoneGap . In this session , you will get an introduction to PhoneGap ( Apache Cordova ) , be able to see example PhoneGap applications , and walk through the process of building your first PhoneGap application . <p> PhoneGap Native Plugins PhoneGap enables developers to build natively installed applications using traditional web-based development tools ( HTML &amp; JavaScript ) , but what if you want to make your application do more ? In this session , learn how to write native plugins for PhoneGap that enable you to extend the API to tap into native device functionality . <p> Data Visualization with Web Standards Do you have the requirement to create rich visual data dashboard applications , but also have the requirement to @ @ @ @ @ @ @ @ @ @ do next ? Well , you 're in luck ! Come to this session to learn about data visualization strategies and frameworks powered entirely with HTML and JavaScript. <h> 360iDev <p> 360iDev is the first and still the best iPhone developer conference in the world . We 're not a publishing company pushing books , or a media company selling subscriptions . We 're a conference company , focused on community . Our goal is to bring the best and brightest in the developer community together for 3 days of incredible sessions , awesome parties , good times , and learning . If you do n't leave Wednesday night , with more ideas than you know what to do with , we 're not doing our jobs ! <p> Ill be speaking on the following topics at 360iDev : <p> Kick A$$ iOS Apps with PhoneGap Apps do n't  have to be written in native Objective-C to be awesome. - - Get ready for a crash course in PhoneGap , a tool that enables you to build natively installed iOS apps using 100% HTML &amp; JavaScript , complete with access to @ @ @ @ @ @ @ @ @ @ phonegap " to strategies for building highly performant &amp; interactive applications . <h> Dreamforce <p> Every year Dreamforce features stories and presentations from some of the brightest minds in technology , business and beyond . This years Dreamforce promises to be even more informative and dynamic , with our most exciting keynote speaker lineup yet . The cloud computing event of the year is also the Social Enterprise event of the year . This is where you 'll learn everything you need to know " from the industry leaders who are paving the way " about how the Social Enterprise revolution is changing the way we do business . <p> Ill be speaking on the following topics at Dreamforce : <p> Data Visualization with Web Standards Do you have the requirement to create rich visual data-centric applications , but also have the requirement to use web-standard technologies , and do n't  know what to do next ? Well , you 're in luck ! Come to this session to learn about data visualization strategies and frameworks powered entirely with HTML , CSS , and JavaScript . <p> Native-like Apps with PhoneGap @ @ @ @ @ @ @ @ @ @ " uncanny valley " effect where they do n't  feel quite right as a native application . In this session well focus on strategies to make your apps feel like native apps , including considerations for a native-feeling UI , platform consistency , and user experience . <p> Hi Everyone ! Here are a few events that I 'll be speaking at/attending in the remainder of 2011 . Come check out Adobes cross-platform tooling , and feel free to bring your questions for me . I hope to see you at any one of these events ! <h> MoDevDC Meetup <p> Tonight ! 11/02/2011 This months MoDevDC ( Mobile Developers DC ) tech meetup is all about cross-platform development tools . Stop by to see a high level overview of Adobes cross-platform mobile development offerings , including AIR and PhoneGap . This will include the basics of " what are these tools " , as well as some demo LONG ... <h> DC Droids Meetup <p> 11/15/2011 Here is a chance for you to learn about another method to develop Android applications ( and make them compatible across platforms ) from @ @ @ @ @ @ @ @ @ @ chance to win a copy of Flash Builder 4.5 . I will walk through the processes of building Android &amp; cross-platform applications using both Adobe AIR and PhoneGap ( a cross-platform mobile application development framework ) . This session will cover demo applications , as well as real-world coding and best LONG ... <h> MoDevEast Conference &amp; Hackathon <p> 12/02/2011 12/03/2011 MoDevEast is where mobile developers and marketers gain the upper edge . If you are developing apps or mobile websites , targeting phones or tablets , staying ahead is paramount in this fast moving industry . MoDevEast 2011 will offer five tracks that hone development skills and sharpen mobile business strategy . Ill be speaking on cross-platform mobile development , and I 'll definitely be there for the event and hackathon ! http : //www.modeveast.com/ 
@@106848916 @2248916/ <h> Tag Archives : air <p> On the official Adobe Flex Team blog , there is a great post by Andrew Shorten discussing the future direction of Flex . I highly recommend taking a moment to read it . In that post Andrew points out where Flex is , and where Flex is heading . One thing I want to re-emphasize is that mobile is the next big thing . <p> It has been proclaimed many times , in many publications that mobile devices ( tablets and smartphones ) are the future of computing . This is both in enterprise and consumer products &amp; applications . One of the catches with this growth is that each platform has its own development tooling and language . Wouldnt it be nice if you could just use one programming model &amp; technology stack ? Even better , would n't it be nice if you could use that same programming model &amp; technology stack to also develop applications for the web and the desktop ? Wow , what if you could even share code libraries across mobile , web , and desktop applications ? @ @ @ @ @ @ @ @ @ @ do that ! <p> Adobe Flex is the best tool for creating cross-platform , rich experiences in mobile , desktop , and web applications . <p> That is awesome . <p> One of the biggest enhancements introduced with Flash Builder 4.5.1 was the inclusion of mobile tooling . These tools allow you to easily create rich experiences targeting a variety of mobile devices iOS , Android , BlackBerry Playbook . All of which are natively installed applications that can be shared by the standard distribution models : App Store , Enterprise distribution , etc <p> The best part is that you do n't  need to learn any new programming skills to develop and deploy for these platforms . You will need to learn about the app ecosystems , platform signing and deployment procedures , and device specifics ( soft keyboards , hardware buttons , etc .. ) . However , you can still develop these applications quickly and easily using Flex , ActionScript , and AIR APIs . One code base ; multiple platforms ; lots of devices . Did you know that you could even take that same code @ @ @ @ @ @ @ @ @ @ there are great new advancements in the Flex/AIR mobile tooling waiting just over the horizon . <p> We 're continuing to focus on runtime performance , native extensions , new components , declarative skinning , adding more platforms and improving tooling workflows , such that in our next major release timeframe we expect that the need to build a fully-native application will be reserved for a small number of use cases . <p> The growth of the mobile market and the challenge of building out applications that work on a range of different form-factors and platforms present us with a huge opportunity to expose Flex to an entirely new audience of developers , while continuing to be relevant for existing Flex developers who are extending their applications to mobile . <p> Flex &amp; AIR for mobile allow you to use the same enterprise class tooling to build cross platform mobile applications . You can still use existing framework components , existing open source libraries , the strongly typed programming language , automated ASUnit testing , build scripts , and many other features that Flex offers , and you can now @ @ @ @ @ @ @ @ @ @ incredibly promising for developers . Whether you are targeting mobile , web , or desktop or developing for gaming or data visualization , the future is bright . I was in the Adobe offices in San Francisco last week meeting with the team , and Thibault Imbert showed us some jaw dropping demos Keep an eye out for whenever those become public . <p> but it is not all about games . In a recent post , I showed off some basic stage3d hardware accelerated graphics within AIR on mobile devices . Ive been developing some ideas to show how this can be applied within non-gaming contexts , and I figured I 'd show my progress so far . The previous post was an actionscipt-only project . In enterprise scenarios , you most likely will want to take advantage of the plethora of pre-built components that the Flex framework provides . The progression for me was natural ; How about showing what Stage3D can do within a Flex for mobile application ? <p> Above is a fairly basic visualization application . There is a stage3D-based custom charting component , tied into @ @ @ @ @ @ @ @ @ @ a custom item renderer . The chart is based on my previous example , and obviously is n't fully fleshed out ( no legend , no ticks , no axis labels , etc ) . Did I mention , this is on a tablet ? Oh yeah , that is pretty awesome . Did I also mention that it will be cross platform once released ? Oh yeah , that is pretty awesome too . <p> The data is morbid , but it works for my scenario . It is based on this data set from CMU.edu , which shows the cancer deaths and the number of cigarrettes consumed per state . <p> Please keep in mind , this is all beta , and not a final build . Ill post the source code once Stage3D for mobile is publicly available . <p> Mark your calendars ! Next Thursday , August 18th at 9:00AM to 10:30 AM ( PDT GMT-7 ) , the evangelism team will be hosting a live Q&amp;A chat . Come join us to learn more about Flex and AIR for mobile , and bring your questions with @ @ @ @ @ @ @ @ @ @ There will be no demos , no slides , no speaking just pure Q&amp;A through Adobe Connect ( all you need is a web browser or the mobile app ) . Whether you are actively involved in mobile development , or about to start , join us and bring your questions ! <p> On Thursday morning , just go to http : //flex.org/ask and join us in the chat . I hope to see you there ! <p> How : We will have at least five evangelists in the Connect session to answer questions . This is a bit of a science experiment so we 'll see how it goes ! We will allow as many people in as we can handle , then we 'll start blocking entry and creating a queue . The event will run for 90 minutes so you can come and go as you want . There are no presentations and no demos . It 's purely a Q&amp;A session . <p> Why : For many of us , building apps for mobile devices is a very dramatic shift . We 've never before had @ @ @ @ @ @ @ @ @ @ and input from the GPS and accelerometer . At the same time , we have to be much more conscience than ever before about resources , because our apps run on much slower CPUs with far less memory than we are used to and on OSes that will shut down your app if it crosses the line ! It 's not a trivial transition ! <p> Our team has been building real apps with Flex for several months and we have learned a lot . This is an opportunity for us to share that knowledge in an informal setting . We 've never attempted this before , but if it works well , we 'll do it often . If it 's a bust , we 'll figure out a better way . <p> A few things to keep in mind for the chat : <p> Questions should be as specific as possible and limited to Flex on Mobile . <p> We ca n't promise that every question will be answered , but we 'll do our best . <p> We ca n't debug your code , that 's your @ @ @ @ @ @ @ @ @ @ online resources . <p> Keep it friendly and G-rated . We do n't want to see you on webcam ! <p> No flaming ! We 're here to answer specific questions , not to debate you about technology . <p> If you had n't  heard yet , Beta 2 of AIR 3.0 and Flash Player 11 are now availabe on Adobe Labs . The AIR 3.0 beta release is sporting some great new features , including hardware accelerated video playback for mobile , iOS background audio , android licensing support , front-facing camera support , encrypted local storage for mobile , H.264 software encoding for desktop applications , and last , but not least , captive runtime support for desktop and Android applications . <p> If you are wondering what " captive runtime support " is , then I 'll try to explain Currently all AIR applications that are deployed on the desktop and in Android require the 3rd-party Adobe AIR runtime . If you are familiar with the process for developing mobile AIR applications for Apples iOS devices , then you may already know that these applications do n't  require @ @ @ @ @ @ @ @ @ @ These AIR applications for iOS already take advantage of the captive runtime . All necessary components of the AIR framework are bundled into a self-contained , compiled distributable application that has no dependence upon other frameworks . <p> With AIR 3.0 , you will have the option to bundle the AIR framework into your applications to eliminate the 3rd-party dependency . However , one thing to keep in mind is that you can only export mac application packages on Macs and Windows EXEs on Windows . You ca n't target native installers or bundled runtimes for cross-platform development . You can only have a single app that targets both platforms if you export a . AIR file ( which requires the 3rd-party AIR runtime ) . <p> A great question came out of the comments on my last post " Why Ca n't I Run My App in the iOS Simulator " asking if it is possible to export the mobile simulator from Flash Builder to allow you to distribute builds for functional testing/validation. - The quick answer is yes ( but you do n't  really export the simulator ) . <p> @ @ @ @ @ @ @ @ @ @ directly within Flash Builders environment , you really are running a local desktop application within the ADL executable ( AIR Debug Launcher ) that is part of the AIR SDK. - The size/layout of ADL in this case is locked to- imitate- the experience of a particular device/configuration. - The mobile application is n't running in a device-specific simulator . <p> All Flex/AIR mobile applications can actually be exported as . AIR files that can run within the desktop runtime environment . - This enables you to easily repurpose an application from mobile to desktop . <p> Keep in mind , this does not mean there will be a 100% parity between desktop and mobile features , interaction , and performance . - Interaction paradigms , such mouse vs. touch , keyboard vs. hard-keys vs. soft-keys , form factor , as well as hardware ( CPU/memory ) may all have critical impacts in the overall experience . - Again , if you are targeting mobile devices , then it is imperative that you test your applications on real , physical devices to identify any kind of performance issues , bugs , @ @ @ @ @ @ @ @ @ @ for desktop usage as a . AIR file , just select the " Signed AIR package for installation on desktop " export option within the " Export Release Build " dialog . <p> Flash Builder Export AIR Options <p> The exported AIR file will use the standard AIR runtime , inclusive of cross-platform features and the AIR installer. - Note : I used a local dev certificate in this example , with just the default options . <p> AIR Installer <p> At runtime , it will look like any other AIR application . - You can configure the AIR app-xml descriptor files to customize it . 
@@106848917 @2248917/ <p> This post is inspired by all the comments Ive seen this week about JS in the enterprise . I would have never imagined- this 10- years ago , but JavaScript is now pretty much ubiquitous . Here are a few reasons why you need to paying attention to JavaScript if you are n't  already , and why you should definitely not- write it off . <p> First , I think one of the major reasons for JavaScripts ubiquity is that- JavaScript is approachable . It is relatively easy for beginners to learn JavaScript , and powerful enough for advanced users to build complex and reliable systems . <p> Second , why you need to pay attention , JavaScript is everywhere . <p> You can now use JavaScript to develop on virtually any platform : client side applications , server side logic , embedded chips/IoT devices , manage build scripts and dependencies , and more . <p> This does n't  mean you 'll use the exact same code in every case , rather that you can use the same skill set JavaScript Development to deliver solutions across multiple paradigms . @ @ @ @ @ @ @ @ @ @ power client side apps/user interfaces , and user interactions on numerous platforms and devices . <h> Web <p> Of course JavaScript powers the web , this is a given . JavaScript is the primary scripting language for all web browsers . I wo n't focus on this much b/c its already well known . <h> Mobile <p> JavaScript can also be used to power mobile applications that are natively installed on a device . <p> Apache Cordova/PhoneGap- You can build natively installed apps with web technology using PhoneGap or Cordova . PhoneGap is Adobes branded distribution of Cordova , but from the developers perspective , they are basically the same thing . Your app runs within a webview on the mobile device , and you build your user interface the same way you you build a dynamic web application . Your user interface- is implemented in HTML , styled with CSS , and all interactivity is created with JavaScript . <p> React Native- JavaScript powered web apps do n't  just have to be inside of a a web view . The React Native framework gives developers the ability to write their @ @ @ @ @ @ @ @ @ @ in a native application running on the mobile device . The logic is interpreted JavaScript at runtime , but everything that the user- interacts with ( all UI elements ) is 100% native , providing a very high quality user experience , and it is now available for both iOS and Android applications . <p> Unity 3D You can even develop rich &amp; immersive mobile 3D simulation or gaming experience , entirely powered by JavaScript using the Unity 3D engine . **These can be web , desktop , or mobile , but is often used in mobile gaming . <h> Desktop <p> Yup , desktop apps are not left out of the mix . Most desktop solutions fall into a category similar to Apache Cordova , where the end results is a web view that has access to lower level APIs , whose content is developed with web based technology . <h> The Server Side <p> Most obviously- Node.js a JavaScript runtime buit on Chromes V8 JavaScript Engine has made huge inroads into server side development and the enterprise . Node.js , powered by frameworks like express.js or loopback.io makes @ @ @ @ @ @ @ @ @ @ . <p> Here are some stats that- show the magnitude of growth and adoption for Node.js/npm.js alone . NPM- stats currently shows a total of- 186,946 packages available for download , 94,978,032 package downloads in the last day , and 2,451,734,737 package downloads in the last month . <p> This does n't  mean that JavaScript is the best language at everything . It- also does n't  meant that you can take a single piece of source- code and run it in every device/context imaginable . <p> It means that you can use your skills in JavaScript to develop for just about any kind of device/context out there . Its not going to be write once , run everywhere , rather in the words of the React.js team : learn once , write everywhere . <p> Node.js is an incredible tool for rapidly building highly performant and scalable back end systems , and you develop it using a familiar core language that most front-end developers are already accustomed to , JavaScript. - This acquisition is positioned to greatly enhance Node.js in the enterprise , and StrongLoops offerings will be integrated into @ @ @ @ @ @ @ @ @ @ Even though the acquisition is still " hot off of the presses " , - you can start using these tools together- today : <p> If you have n't heard about StrongLoop 's LoopBack framework , it enables you to easily connect and expose your data as REST services . It provides the ability to visually create data models in a graphical ( or command line ) interface , which are used to automatically generate REST APIs " thus generating CRUD operations for your REST services tier , without having to write any code . <p> Why is this important ? <p> It makes API development easier and drastically reduces time from concept to implementation . - If you have n't yet looked at the LoopBack framework , you should definitely check it out . - You can build API layers for your apps literally in minutes . - Check out the video below for a quick introduction : <p> Again , be sure to check out these posts that detail the integration steps so you can start using these tools together today : <p> These two services enable you to quickly @ @ @ @ @ @ @ @ @ @ What 's a better way to show them off than by updating my existing app to leverage the new speech services ? <p> I simply added the Text To Speech and Speech To Text services to my existing Healthcare QA application that runs on Bluemix : <p> IBM Bluemix Dashboard <p> These services are available via a REST API . Once youve added them- to your application , you can consume them easily within any of your applications . <p> I updated the code from my previous example- in 2 ways : 1 ) take advantage of the Watson Node.js Wrapper that makes interacting with Watson a lot easier and 2 ) to take advantage of these new services services . <h> Watson Node.js Wrapper <p> Using the Watson Node.js Wrapper , you can now easily instantiate Watson services in a single line of code . - For example : <p> The credentials come from your environment configuration , then you just create instances of whichever services that you want to consume . <h> QA Service <p> The code for consuming a service is now much simpler than the previous version @ @ @ @ @ @ @ @ @ @ the Watson QA service , you can now simply call the " ask " method on the QA service instance . <p> Below is- my server-side code from app.js that accepts a POST submission from the browser , delegates- the question to Watson , and takes the result and renders HTML using a- Jade template. - See the Getting Started Guide for the Watson QA Service to learn more about the wrappers for Node or Java . <p> Compare this to the previous version , and you 'll quickly see that it is much simpler . <h> Speech Synthesis <p> At this point , we- already have a functional service that can take natural language text , submit it to Watson , - and return a search result as text . - The next logical step for me was to add speech synthesis using the Watson Text To Speech Service- ( TTS ) . - Again , the Watson Node Wrapper and Watsons REST services- make this task very simple . - On the client side you just need to set the src of an &lt;audio&gt; instance to the URL for @ @ @ @ @ @ @ @ @ @ need to synthesize the audio from the data in the URL query string . - Heres an example how to invoke the text to speech service directly from the Watson TTS sample app : <p> var textToSpeech = new **32;6989;TOOLONG ; // handle get requests app.get ( ' /synthesize ' , function ( req , res ) // make the request to Watson to synthesize the audio file from the query text var transcript = **34;7023;TOOLONG ; // set content-disposition header if downloading the // file instead of playing directly in the browser transcript.on ( 'response ' , function(response) **29;7059;TOOLONG ; if ( req.query.download ) **36;7090;TOOLONG ' = ' attachment ; filename=transcript.ogg ' ; ) ; // pipe results back to the browser as they come in from Watson transcript.pipe(res) ; ) ; <p> The Watson TTS service supports . ogg and . wav file formats . - I modified this sample is setup only with . ogg files . - On the client side , these are played using the HTML5 &lt;audio&gt; tag . <h> Speech Recognition <p> Now that were able to process natural language and generate speech @ @ @ @ @ @ @ @ @ @ spoken input and turn it into text . - The Watson Speech To Text ( STT ) service handles this for us. - Just like the TTS service , the Speech To Text- service also has a sample app , complete with source code to help you get started . <p> In this post I 'd like to show a fairly simple application that I put together which shows off some of the rich capabilities for IBM MobileFirst for Bluemix that you get out of the box All with an absolute minimal amount of your own developer effort . - Bluemix , of course , being IBMs platform as a service offering . <p> GeoPix is a sample application leveraging IBM MobileFirst for Bluemix to capture data and images on a mobile device , persist that data locally ( offline ) , and replicate that data to the cloud . Since its built with IBM MobileFirst , we get lots of things out of the box , including operational analytics , user authentication , and much more . <p> ( full source code at the bottom of this post ) <p> @ @ @ @ @ @ @ @ @ @ take a picture or select an image from the device <p> App captures geographic location when the image is captured <p> App saves both the image and metadata to a local data store- on the device . <p> App uses asynchronous replication to automatically save any data in local store up to the remote store whenever the network is available <p> Oh yeah , cant forget , the user auth- is via Facebook <p> MobileFirst provides all the analytics we need . - Bluemix provides the cloud based server and Cloudant- NoSQL data store . <p> All captured data is available on a web based front-end powered by Node.js <p> In this sample I 'm using everything but the Push Notifications service . - Im- using user authentication , the Cloudant DB ( offline/local store and remote/cloud store ) , and the node.js backend. - You get the operational analytics automatically . <h> Capturing Images <p> Capturing images from the device is also very straightforward . - In the app I leverage Apples- UIImagePickerController- to allow the user to either upload an existing image or capture a new image . - @ @ @ @ @ @ @ @ @ @ standard practice using Apples developer SDK : <h> Persisting Data <p> If you notice in the- **29;7159;TOOLONG method above , there is a call to- the DataManagers saveImage withLocation method . This is where we save data locally and rely on Cloudants replication to automatically save data from the local data store up to the Cloudant NoSQL database . - This is powered by the iOS 8 Data service from Bluemix . <p> The first thing that we will need to do is initialize the local and remote data stores . Below you can see my init method from my DataManager class . In this , you can see the local data store is initialized , then the remote data store is initialized . If either data store already exists , the existing store will be used , otherwise it is created . <p> Once the data stores are created , you can see that the replicate method is invoked . - This starts up the replication process to automatically push changesfrom the local data store to the remote data store " in the cloud " . <p> Therefore , @ @ @ @ @ @ @ @ @ @ then you have nothing to worry about . - All of the data will be stored locally and pushed up to the cloud whenever you 're back online all with no additional effort on your part . - When using replication with the Cloudant SDK , you just have to start the replication process and let it do its thing fire and forget . <p> In my replicate function , I setup- CDTPushReplication for pushing changes to the remote data store . - You could also setup two-way replication to automatically pull new changes from the remote store . <p> Once weve setup the remote and local data stores and setup replication , we now are ready to save the data the were capturing within our app . <p> Next is my saveImage withLocation method . - Here you can see that it creates a new- **26;7190;TOOLONG object ( this is a generic object for the Cloudant NoSQL database ) , and populates it with the location data and timestamp. - It then creates a jpg image from the UIImage ( passed in from the UIImagePicker above ) and adds the @ @ @ @ @ @ @ @ @ @ Once the document is created , it is saved to the local data store . - We then let replication take care of persisting this data to the back end . <p> If we want to query data from either the remote or local data stores , we can just use the performQuery method on the data store . Below you can see a method for retrieving data for all of the images in the local data store . <p> At this point we 've now captured an image , captured the geographic location , saved that data in our local offline store , and then use- replication to save that data up to the cloud whenever it is available . <p> AND <p> We did all of this without writing a single line of server-side logic . - Since this is built on top of MobileFirst for Bluemix , all the backend infrastructure is setup for us , and we get operational analytics to monitor everything that is happening . <p> With the operational analytics we get : <p> App usage <p> Active Devices <p> Network Usage <p> Authentications <p> @ @ @ @ @ @ @ @ @ @ logs from devices out in the field ) <p> Push Notification Usage <h> Sharing on the web <p> Up until this point we have n't had to write any back-end code . However the mobile app boilerplate on Bluemix comes with a Node.js server . - We might as well take advantage of it . <p> The Node.js back end comes preconfigured to leverage the- express.js- framework for building web applications . - I added the- jade template engine and Leaflet for web-mapping , and was able to crank this out ridiculously quickly . <p> The first thing we need to do is make sure - we have our configuration variables for accessing the Cloudant service from our node app. - These are environment vars that you get automatcilly if you 're running on Bluemix , but you need to set these for your local dev environment : <p> Next you 'll se the logic for querying the Cloudant data store and preparing the data for our UI templates . You can customize this however you want caching for performance , refactoring for abstraction , or whatever you want . All interactions with Cloudant are powered by the Cloudant Node.js Client 
@@106848918 @2248918/ <h> Category Archives : Development <p> here 's an interview that I recently did with IBM DeveloperWorks TV at the recent World of Watson conference . In it I discuss a project Ive been working on that analyzes drone imagery to perform automatic damage detection using the Watson Visual Recognition , and generates 3D models from the drone images using photogrammetry processes . The best part the entire thing runs in the cloud on IBM Bluemix . <p> Its been a while since I 've posted here on the blog - In fact , - I just did the math , and- its been over 7 months. - Lots of things have happened since , I 've moved to a new team within IBM , built new developertools , worked directly with clients- on their solutions , worked on a few high profile keynotes , built apps for kinetic motion and activity tracking , built a mobile client for a chat bot , and even completed some new drone projects . - Its been exciting to say the least , but the real reason I 'm writing this post is to share a @ @ @ @ @ @ @ @ @ @ recent conferences . <p> I recently returned from Gartner Symposium and IBMs annual World of Watson conference , and- its been one of the busiest , yet most exciting span of two weeks Ive experienced- in quite a while . <p> At both events , we showed a project Ive been working on with IBMs Global Business Services team that focuses on the use of small consumer drones and drone imagery to transform Insurance use cases . In particular , by leveraging IBM Watson to automatically detect roof damage , in conjunction with photogrammetry to create 3D reconstructions and generate measurements of afflicted areas to expedite and automate claims processing . <p> This application leverages many of the services IBM Bluemix has to offer on-demand CloudFoundry runtimes , a Cloudant NoSQL database , scalable Cloud Object Storage ( S3 compatible storage ) , and BareMetal servers on Softlayer . Bare Metal servers are *awesome* I have a dedicated server in the cloud that has 24 cores ( 48 threads ) , 64 GB RAM , RAID array of SSD drives , and 2 high end multi-core GPUs . Its taken @ @ @ @ @ @ @ @ @ @ to 10 minutes for photogrammetric reconstruction with Watson analysis . <p> Its been an incredibly interesting project , - and you can check it out yourself in the links below . <h> World of Watson <p> World of Watson was a whirlwind of the best kind I had the opportunity to join IBM SVP of Cloud , Robert LeBlanc , on stage as part of the the Cloud keynote at T-Mobile Arena ( a huge venue that seats over 20,000 people ) to show off the drone/insurance demo , plus 2 more presentations , and an " ask me anything " session on the expo floor . <p> You can also check out my session " Elevate Your apps with IBM Bluemix " on UStream to see an overview in much more detail : <p> .. and that 's not all . I also finally got to see a complete working version of the Olympic Cycling teams training app on the expo floor , including cycling/biometric feedback , video , etc I worked with an IBM JStart team and wrote the video integration layer into for the mobile app using IBM @ @ @ @ @ @ @ @ @ @ <h> Drones <p> On this project we 've been working with a partner DataWing , who provides drone image/data capture as a service . However , I 've also been flying and capturing my own data . The app can process virtually any images with appropriate metadata , but I 've been putting both the DJI Phantom and Inspire 1 to work , and they 're working fantastically . <p> MobileFirst Platform Foundation provides a middleware solution and SDK that makes exposing data to mobile apps easier , improves- security through encryption , authentication- and handshaking to guarantee app authenticity , - provides facilities- to easily manage multiple versions of an app , notify and engage users , and , on top of everything else , provides operational analytics so that you can monitor the health of your overall system at any point in time . <p> As a mobile developer catering to the enterprise , it makes your life significantly- easier , and it supports any mobile development- paradigm- that you might want to target : Native platforms , hybrid Xamarin using C# , and hybrid Cordova platforms ( HTML/JS ) . <h> @ @ @ @ @ @ @ @ @ @ ? <p> The recently opened beta has some great new features , AND its now available as a service on Bluemix ( IBMs Cloud platform ) . - The beta program is intended to deliver the next generation of an open , integrated and comprehensive mobile app development platform redesigned for cloud agility , speed , and productivity , that enables enterprises to accelerate delivery of their mobile strategy . <p> In my last post I mentioned some new announcements related to the Swift programming language at IBM . - Upon further thought , I guess- its probably not a bad idea to re-post- more detail here too <p> If you did n't  see/hear it last week , IBM unveiled several projects to advance the Swift language for developers , which we think will have a profound impact on developers &amp; developer productivity in the years to come . You can view a replay of the IBM announcement- in the video embedded below , or just scroll down for direct links : <p> Here are quick links to each of the projects listed : <p> Kitura A light-weight web framework @ @ @ @ @ @ @ @ @ @ services with complex routes , easily . Learn more <p> Last week I was in good ole Las Vegas for IBM InterConnect IBMs largest conference of the year . With over 20,000 attendees , it was a fantastic event that covered everything from technical details for developers to forward-looking strategy and trends for C-level executives . IBM also made some big announcements for developers OpenWhisk serverless computing and bringing the Swift language to the server just to name a few . Both of these are exciting new initiatives- that offer radical changes &amp; simplification to developer workflows . <p> It was a busy week to say the least lots of presentations , a few labs , and even a role in the main stage Swift keynote . You can expect to find more detail on each of these here on the blog in the days/weeks to come . <p> For starters , here are two " lightning talks " I presented in the InterConnect Dev@ developer zone : <h> Smarter apps with Cognitive Computing <p> This session introduces the concept of cognitive computing , and demonstrates how you can use @ @ @ @ @ @ @ @ @ @ you are n't  familiar with cognitive computing , then I strongly recommend that you check out this post : The Future of Cognitive Computing . <p> In the presentation below , I show two apps leveraging services on Bluemix , IBMs Cloud computing platform , and the iOS SDK for Watson . <p> Actually , I 'm using two Watson SDKs The older Speech SDK for iOS , and the new iOS SDK. - I 'm using the older speech SDK in one example because it supports continuous listening for Watson Speech To Text , which is currently still in development for the new SDK . <h> Redefining your personal mobile expression with on-body computing <p> My second presentation highlighted how we can use on-body computing devices to change how we interact with systems and data . - For example , we can use a luxury smart watch ( ex : Apple Watch ) to consume and engage with data in more efficient and more personal ways . - Likewise , we can also use smart/wearable peripherals devices to access and act on data in ways that were- never possible- before . @ @ @ @ @ @ @ @ @ @ upon patterns in raw data transmitted by the on-body devices . - For this , I leveraged the new IBM Wearables SDK. - The IBM Wearables SDK provides a consistent interface/abstraction layer for interacting with wearable sensors . - This allows you to focus on building your apps that interact with the data , rather thank learning the ins &amp; outs of a new device-specific SDK . <p> The wearables SDK also users data interpretation algorithms to enable you to define gestures or patterns in the data , and use those patterns to act upon events when they happen without additional user interaction . - For example : you can determine if someone falls down , you can determine when someone is raising their hand , you can determine anomalies in heart rate or skin temperature , and much more . - The system is capable of learning patterns for any type of action or virtually any data being submitted to the system . - Sound interesting ? - Then check it out here . <p> I also had some other- sessions- on integrating drones with cloud services , integrating @ @ @ @ @ @ @ @ @ @ - I 'll be sure to post updates for this- content- I make them publicly available . - I think you 'll find the session on drones + cloud especially interesting I know I did . 
@@106848920 @2248920/ <h> Tag Archives : development <p> As promised , here are my presentation slides and extra content from last weeks- RIACon conference . I gave three presentations : " Intro to PhoneGap " , " Data Visualization With Web Standards " , and " PhoneGap Native Plugins " . All of the presentations are freely available at : https : **34;7218;TOOLONG . <p> Data visualization is the art &amp; science of creating a visual representation of data and information . - Really , it can be anything : a bar chart , scatter plot , pie chart , complex flow diagram , 3d model , etc - If your visualization conveys information without having to read a table of data , then its doing what it should . Recently , the emergence of HTML5s dynamic graphics and SVG support have made rich , dynamic , and interactive graphics possible on the web without having to leverage Flash , which was previously the only real option . Be sure to check out this presentation , and read this blog post for more info : Data Visualization With Web Standards <p> @ @ @ @ @ @ @ @ @ @ applications using web technologies . - As a part of this , PhoneGap provides an API to access native operating system functionality from JavaScript . Luckily for everyone , the JavaScript-to-native bridge- is extensible and you can very easily create and expose your own custom native functionality with a JavaScript API. - Basically , all PhoneGap native plugins are made up of two parts : a native implementation , and a JavaScript interface . - Your PhoneGap application calls the JavaScript interface , which leverages cordovoa.exec to communicate with the native layer . - The native layer then performs a native operation and communicates back to the JS layer . <p> Open source PhoneGap plugins- There are lots of open source native plugins for everything from push notifications , screen captures , barcode scanners , to MapKit or even iCould ( among many others ) . <p> and of course , the sample apps : <h> iOS Multi-Screen <p> This sample app demonstrates how to create multi-screen experiences using the UIScreen API on iOS. - You can use AirPlay mirroring on an AppleTV as a second screen , the content @ @ @ @ @ @ @ @ @ @ " main " PhoneGap experience on the device . Check out the links and video below to learn more . <h> LowLatencyAudio <p> The PhoneGap LowLatencyAudio native plugin for Android and iOS allows you to preload audio , and playback that audio quickly , with a very simple to use API. - It overcomes the current limitations of the HTML5 Audio API on many mobile devices . Check out the links and video below to learn more . <p> Earlier this month the PhoneGap team held the first PhoneGap day . - This was in part to celebrate the release of PhoneGap 2.0 , but more importantly to bring together members of the PhoneGap community to share and learn from each other . - There are great recaps of PhoneGap Day from RedMonk , as well as on the PhoneGap blog . One of the new services announced on PhoneGap Day was- emulate.phonegap.com. - Emulate.phonegap.com enables an in-browser simulator for developing and debugging PhoneGap/Cordova applications , complete with Cordova API emulation . - It is built off of the Ripple Emulator , which itself is open source and may even @ @ @ @ @ @ @ @ @ @ launched , the URL that you want to simulate will be displayed within the Ripple operating environment view . <p> Note : This only works with assets that are on a live URL . You can use a local http server with references to localhost , however the emulator will fail if you try to access your application directly from the local file system using a file : // URI . <p> Update : You can enable access to local files by changing a few settings on the Ripple emulator . - See the first comment on this post for additional detail . <p> ( click for full-size image ) <p> The emulator environment gives you the ability to emulate PhoneGap events and API calls , without having to deploy to a device or run inside of the iOS , Android , Blackberry , or other emulator . - Not only can you simulate the PhoneGap/Cordova API , but you can also use Chromes debugging tools to test &amp; debug your code complete with breakpoints , memory inspection , and resource monitoring . This is a handy development configuration. - @ @ @ @ @ @ @ @ @ @ is familiar , fast &amp; easy to use . This does not replace on-device debugging however nothing will replace that . - On-device debugging is extremely important ; this helps increase your productivity as a developer . <p> So how do you use this environment ? The environment will handle Cordova API requests , and you can also simulate device events . <p> First , the " Devices " panel In this panel you can select a device configuration Everything from iOS , to Android , to BlackBerry . - Changing the device configuration will not only change the physical dimensions , but will also change Device/OS/user agent settings reported by the application . - Here you can also select the device orientation , which will change the visual area within the simulator . <p> Within the " Platforms " panel you can choose the platform you wish to emulate . - With respect to PhoneGap applications , you will want to choose " Apache Cordova " , and then select the API version that you are using . - By default , it uses " PhoneGap 1.0.0 ? , @ @ @ @ @ @ @ @ @ @ get the most recent version . - The Ripple emulator also simulates BlackBerry WebWorks and mobile web configurations as well . <p> The " Accelerometer " panel can be used to simulate device **25;7254;TOOLONG events . - Just click and drag on the device icon ( the gray and black boxes ) , and the icon will rotate in 3D. - As you drag , accelerometer events will be dispatched and handled within your application . - From here , you can even trigger a " shake " event . <p> The " Geolocation " panel enables you to simulate your geographic position within your PhoneGap/Cordova application . - You can specify a latitude , longitude , altitude , speed , etc - You can even drag the map and use it to specify your geographic position . The position that you set within the geolocation panel will be reported when using **42;7281;TOOLONG . <p> The " Config " panel is a graphical representation of your PhoneGap BuildConfig.xml file . - You can use this to easily view/analyze what 's in your application configuration . <p> The " Events " panel @ @ @ @ @ @ @ @ @ @ " deviceready " , " backbutton " , " menubutton " , " online " , and " offline " ( among others ) . - Just select the event type , and click on the " Fire Event " button . <p> As I mentioned earlier , this wont replace on-device debugging . - It also wont handle execution of native code for PhoneGap native plugins , however you can test/develop against the JavaScript interfaces for those native plugins. - Emulate.phonegap.com- will definitely help with development of PhoneGap applications in many scenarios , and is a nice complement to the- Chrome Developer Tools . <p> Ill be speaking at a few conferences in the next few months on PhoneGap and web standards-based development . - Here are just a few , with some more pending . - Be sure to come check one out ( or all of them ) ! <h> RIACON <p> Where architects and developers of all levels come to gather , share and learn about creating the next generation of web based applications. - RIAcons goal is to help you network with fellow industry professionals @ @ @ @ @ @ @ @ @ @ be speaking on the following topics at RIACON : <p> Introduction to PhoneGap Interested in developing applications for mobile devices , on multiple platforms ? Interested in leveraging your existing web development skills to build natively installed applications ? Just looking to expand your skill set ? Come join Adobe Technical Evangelist , Andrew Trice , to learn about cross platform mobile development and PhoneGap . In this session , you will get an introduction to PhoneGap ( Apache Cordova ) , be able to see example PhoneGap applications , and walk through the process of building your first PhoneGap application . <p> PhoneGap Native Plugins PhoneGap enables developers to build natively installed applications using traditional web-based development tools ( HTML &amp; JavaScript ) , but what if you want to make your application do more ? In this session , learn how to write native plugins for PhoneGap that enable you to extend the API to tap into native device functionality . <p> Data Visualization with Web Standards Do you have the requirement to create rich visual data dashboard applications , but also have the requirement to use web-standard @ @ @ @ @ @ @ @ @ @ ? Well , you 're in luck ! Come to this session to learn about data visualization strategies and frameworks powered entirely with HTML and JavaScript. <h> 360iDev <p> 360iDev is the first and still the best iPhone developer conference in the world . We 're not a publishing company pushing books , or a media company selling subscriptions . We 're a conference company , focused on community . Our goal is to bring the best and brightest in the developer community together for 3 days of incredible sessions , awesome parties , good times , and learning . If you do n't leave Wednesday night , with more ideas than you know what to do with , we 're not doing our jobs ! <p> Ill be speaking on the following topics at 360iDev : <p> Kick A$$ iOS Apps with PhoneGap Apps do n't  have to be written in native Objective-C to be awesome. - - Get ready for a crash course in PhoneGap , a tool that enables you to build natively installed iOS apps using 100% HTML &amp; JavaScript , complete with access to local APIs. @ @ @ @ @ @ @ @ @ @ to strategies for building highly performant &amp; interactive applications . <h> Dreamforce <p> Every year Dreamforce features stories and presentations from some of the brightest minds in technology , business and beyond . This years Dreamforce promises to be even more informative and dynamic , with our most exciting keynote speaker lineup yet . The cloud computing event of the year is also the Social Enterprise event of the year . This is where you 'll learn everything you need to know " from the industry leaders who are paving the way " about how the Social Enterprise revolution is changing the way we do business . <p> Ill be speaking on the following topics at Dreamforce : <p> Data Visualization with Web Standards Do you have the requirement to create rich visual data-centric applications , but also have the requirement to use web-standard technologies , and do n't  know what to do next ? Well , you 're in luck ! Come to this session to learn about data visualization strategies and frameworks powered entirely with HTML , CSS , and JavaScript . <p> Native-like Apps with PhoneGap Native applications @ @ @ @ @ @ @ @ @ @ valley " effect where they do n't  feel quite right as a native application . In this session well focus on strategies to make your apps feel like native apps , including considerations for a native-feeling UI , platform consistency , and user experience . <p> Ive been working on lots of different projects lately . - On several of them , I 've had the need for a reusable list component . - In some cases , it needed to handle a large data set , in others it just needed to be self-contained and easy to use . - Out of these projects came MegaList : a reusable list component for jQuery , which Ive released as open source on Github . <p> MegaList is a jQuery plugin that creates a touch-enabled list component , capable of very large datasets , complete with data virtualization . It was originally intended for touch-enabled devices , however it also works in many desktop browsers . <p> For performance optimizations , the list component uses data virtualization techniques , so there are never more list elements in the HTML DOM than what @ @ @ @ @ @ @ @ @ @ scrolls through content , the list updates the DOM elements accordingly . This makes scrolling lists of thousands of items extremely fluid . - This works in a very similar way to ItemRenderer classes in Flex list and grid components . <p> You can employ the list component using one of two approaches . - One option is to declare the list structure in HTML markup , another option is to specify a dataProvider array , from which the list will create DOM elements . <h> Why ? <p> Sometimes you need a pre-built list that you can reuse . Sometimes you need to scroll through big data sets , and other times you just need component logic kept away from your app logic . It does n't  fit every scenario , but it certainly fits a few . <p> Data virtualization techniques allow you to quickly scroll through massive lists , without performance degradation . However , if your app really has 100K list items to scroll through , you should fire your UX designer . <h> Samples <p> View the " samples " directory to see the scrollable @ @ @ @ @ @ @ @ @ @ and scrollable via touch or mouse events , with function event handlers . <p> Each of these examples can be scrolled using either the mouse or finger , and just tap/click on a row to select it , invoking the selection handler ( alert message ) . On the desktop , you can also scroll with the scrollbar . Note : I originally intended this for mobile on the desktop , I 've only tested in Chrome and Safari . <p> Simple List Created With Inline LI Elements This is a basic example with a list of 50 LI elements . <p> Note : These inline/embedded examples are contained inside of iframes if you mouse-up outside of the iframe , the iframe contents wont receive the event . If MegaList is used in a page , without a wrapping iframe , you do n't  run into this issue . Follow the " View Sample " links above to see them without the iframe issue . <h> Observations <p> Contrary to my expectations , using CSS3 translate3d is actually slower than using CSS top/left when placing the virtualized content . If you @ @ @ @ @ @ @ @ @ @ an extremely noticeable performance degradation on both desktop and mobile devices . <p> I 've experimented with lots of permutations to get the best performance possible . I 'm not finished yet , but I 've found that you can achieve significantly faster performance of DOM manipulation by removing elements from the DOM , manipulating them , then re-adding them . This is what is done within the updateLayout() method . The &lt;ul&gt; is removed from the DOM , &lt;li&gt; elements are added or removed , and then the &lt;ul&gt; is added back to the DOM . You may see a flicker on rare occasions , but I did n't  find this overly intrusive . <p> For small data sets , this may not be much advantage you can get better performance by just using something like iScroll in a &lt;div&gt; containing a &lt;ul&gt; . With large data sets , this is definitely faster . <p> The more complex the HTML inside of your label function , the slower the animation will be . <h> Download <p> The full source code for this component is available on Github . Check out the landing @ @ @ @ @ @ @ @ @ @ Food Finder is an open source mobile application built using PhoneGap ( Apache Cordova ) that helps users locate local farmers markets that are registered with the FDA. - You can search for specific markets , or find the closest markets to your current location . - Check out the video below to see it in action : <p> It was originally intended to just be a sample application for- app-UI- with full source available here , - but happens to be quite popular and useful in the " real world " as well . <p> In fact , the Fresh Food Finder made it all the way up to #18 in the iPad " Lifestyle " category on iTunes in the first week of its release , and even made one of the featured apps in the Lifestyle category : <p> All of the information displayed within the Fresh Food Finder is freely available from the US Department of Agriculture through data.gov . This data set was last updated on April 25 , 2012 . <p> The top request that I 've received for the Fresh Food Finder is improved @ @ @ @ @ @ @ @ @ @ and clear , and am happy to say that I have some improved data on the way ( including schedules and times ) , so keep an eye out for it in the not-to-distant future . <p> The Fresh Food Finder is written entirely using HTML , CSS , and JavaScript , and runs on numerous platforms . - It is currently available for iOS and Android . I 've submitted it for approval in the Windows Phone Marketplace , but its still awaiting approval . - I 've also tested it on the BlackBerry Playbook , and it works great there too , but I just have n't gotten around to submitting it to BlackBerry App World yet . <p> The Fresh Food Finder can be downloaded today in the following markets : <p> PhoneGap : http : //www.phonegap.com- PhoneGap is an HTML5 app platform that allows you to author native applications with web technologies and get access to APIs and app stores . <p> App-UI : http : **29;7325;TOOLONG App-UI is a free &amp; open source collection of reusable " application container " user interface components that may be helpful to @ @ @ @ @ @ @ @ @ @ and JavaScript , especially those targeting mobile devices . <p> Mustache : https : **30;7356;TOOLONG - Mustache is a logic-less template syntax . It can be used for HTML , config files , source code anything . It works by expanding tags in a template using values provided in a hash or object . <p> The entire user interface of the application is created dynamically at runtime based on JavaScript and the Mustache templates. - You can download the full application source code at- https : **38;7388;TOOLONG - Feel free to fork it , or use it as a learning tool to build UI experiences for PhoneGap applications . <p> The code is organized into the following structure : <p> assets This folder contains fonts , images , and CSS styles used within the application . <p> js - This folder contains JavaScript resources and libraries used within the application . <p> When the application loads , all templates are loaded into memory as part of the bootstrapping/startup process . - Once all the data and templates are loaded into memory , the UI is presented to the user . @ @ @ @ @ @ @ @ @ @ all views are rendered from the Mustache templates inside of viewAssembler.js , and all UI styling is applied via CSS within styles.css . <p> Mustache is a templating framework that enables you to easily separate presentation layer ( HTML structure ) from application logic and the data model . - Basically , you create templates that Mustache will parse and convert into HTML strings based upon the data that gets passed in. - I 'll write another post later about Mustache , but it can be extremely useful for larger applications . 
@@106848921 @2248921/ <h> Video : Data Visualization With Web Standards <p> Last week I had the opportunity to present " Data Visualization With Web Standards " to the Data Visualization New York Meetup group . - There was a great turnout , and thanks to everyone who attended . - I 'd like to especially thank Christian Lilley and Paul Trowbridge for organizing the event . <p> My presentation focused on the fundamental techniques of visualizing data within HTML/JS experiences . - You can view my presentation in its entirety below . - Slides and bullet points are below the fold <h> HTML5 &lt;canvas&gt; <p> You can use the HTML5 &lt;canvas&gt; element to programmatically render content based upon data in-memory using JavaScript . The HTML5 Canvas provides you with an API for rendering graphical content via moveTo or lineTo instructions , or by setting individual pixel values manually . - Learn more about the- HTML5 canvas from the MDN tutorials . <p> " One Million Points " Scatter Plot Let the page load , then use the mouse to click and drag regions to " drill into " . - This is @ @ @ @ @ @ @ @ @ @ histogram for massive data sets by manipulating individual pixels . <h> HTML DOM Elements <p> Visualizations like interactive maps , or simple charts can be created purely with HTML structures and creative use of CSS styles to control position , visual presentation , etc You can use CSS positioning to control x/y placement , and percentage-based width/height to display relative values based upon a range of data . - For example , the following bar chart/table is created purely using HTML DIV containers with CSS styles . <h> WebGL <p> WebGL- is on the " bleeding edge " of interactive graphics &amp; data visualization across the web . WebGL enables hardware-accelerated 3D graphics inside the browser experience . Technically , it is not a standard , and there is varied and/or incomplete support across different browsers ( http : **27;7428;TOOLONG ) . - There is also considerable debate whether it ever will be a standard ; however there are some incredible samples out on the web worth mentioning : 
@@106848923 @2248923/ <h> Monthly Archives : February 2014 <p> I 've just wrapped up my presentations for this years DevNexus event in Atlanta it has been a great event , filled with tons of information on web , mobile , and back-end development . I had 3 sessions on PhoneGap One intro , one advanced , and one a mobile frameworks panel . <p> Below are my presentations . - I did n't  record them this time , since they were being recorded by the conference organizers , so expect to see a video once they 're released . <p> Just press the space bar , or use the arrow keys to view the presentation in your browser . <p> What makes a great composition in Photoshop ? Well , that all depends on what you 're trying to achieve and personal taste I wont even attempt to answer that . <p> What makes a composition believable ? That one is a little bit easier Weve all seen bad Photoshop jobs you know , those where colors are way off , lighting is terrible , or edges are left jagged and pixellated . For @ @ @ @ @ @ @ @ @ @ to having qualities that mimic realism . Consistent use of colors , none of those jagged edges , appropriate use of shadows and lighting , proper perspectives , and most importantly , attention to detail . <p> Not quite sure what I mean ? - Check out these examples of some inspiring and impressive Photoshop compositions <p> The latest release of Photoshop has some amazing new features , one of which is 3D printer support . The new 3D printer support makes printing your 3D models easier , regardless of whether you modeled it within Photoshop or some other 3D modeling tool . - Photoshop will even inspect your 3D models for water tightness and generate support scaffolding to ensure a high quality 3D print . <p> Photoshop now supports local 3D printers from Makerbot- and- 3D Systems , but if you do n't  have a 3D printer , do n't  worry , you can still print 3D objects ! Through a partnership between Adobe and Shapeways , you can now package 3D prints and send them to Shapeways 3D printing service directly from Photoshop . <p> I started out @ @ @ @ @ @ @ @ @ @ from the web . Check out the video below to learn more and see 3D features within Photoshop in action . <p> All of my 3D printing so far has been through the Shapeways service . This service will automatically inspect your 3D models for thickness/printability , and gives you immediate feedback whether or not you need to make any changes . Be sure to pay attention to the details of Shapeways materials , since they have different characteristics , minimum thicknesses , and associated cost . Once your object is printed , it will be mailed right to your doorstep . <p> My first example is a 3D printed name plate , modeled entirely within Photoshop. - I took a text layer , extruded it into a 3D object , then added a cube ( stretched to the size of the text ) as a base . Just request a 3D print , and out comes a nice 3D model . This one is the " Coral Red Strong &amp; Flexible Polished " material from Shapeways . Check out the video above for details on how I created this @ @ @ @ @ @ @ @ @ @ My second example is a dragon , which I downloaded from a free 3D models site . This model was not created in Photoshop . I believe this model was originally intended for video games or renderings , but Photoshop had no problem scaling and printing it . Photoshop can read many common 3D model formats ( . obj , .3ds , . dae , etc ) , and makes the printing process simple , regardless of where the model was created . <p> I had to go through a few iterations to find a material and size that was actually printable because the model is very intricate and delicate , but I was finally able print it using the " White Strong &amp; Flexible " material . <p> 3D Printed Dragon <p> If you 're wondering " did those cost a fortune " , the answer is NO ! - The cost depends on type and amount of material you 've selected . The dragon was $32.65 , and the " Adobe " letters were $24.90 USD , including shipping . <p> Ready to create your own 3D prints yet ? @ @ @ @ @ @ @ @ @ @ 3D printing with Adobe Photoshop CC. <h> 3D Printing in Photoshop Series <p> If you 're already a member of Creative Cloud , then you have everything you need to create your own 3D prints with Photoshop . Just download the latest version , and you 're ready to go ! If you 're not already a member of Creative Cloud , then become a member today ! <p> Interested in focusing on aerial photography instead of videography ? Stay tuned for the March Adobe Inspire issue next month , which will feature a complimentary article focusing on still images captured with the same helicopter configuration . Subscribe today to be notified automatically when the new version is available . <p> Be warned flying helicopters with cameras attached is highly addictive . You may easily become obsessed with the endless possibilities , as I have . <p> Here are a few videos Ive captured with this setup , and processed with Creative Cloud . 
@@106848924 @2248924/ <h> Monthly Archives : January 2014 <p> Today new versions of Illustrator , InDesign , and Photoshop were released , and there are some amazing new features added for Creative Cloud members . I 'd like to take this opportunity to show off a really cool new feature in Adobe Photoshop CC Perspective Warp ! Perpsective warp was sneaked at Adobe MAX last year its a new way to manipulate your images by changing their three dimensional perspective . - You can manipulate parts of the image to give the appearance that the camera perspective changes , all without having to create a complex 3D model ! This filter can be used for changing entire images , and is especially useful when aligning perspectives while compositing content from multiple images . <p> here 's a quick video I put together showing how you can use it within your own Photoshop creations . <p> All that you have to do is select the layer that you want to manipulate , then select the Edit -&gt; Perspective Warp menu option. - Using Perspective Warp is a two-step process . <p> First you start @ @ @ @ @ @ @ @ @ @ your images to match areas or geometries of content within your image . This might be the sides of a building , or other areas that you do n't  want deformed . <p> Next , Switch " Warp " mode . - In warp mode you can drag the pins/vertices to warp the content within the image . You can also hold shift and click on a line to lock that line to a horizontal or vertical position . <p> Drag the perspective warping where you want it , and then commit the changes by clicking on the " Commit " check button . <p> Perspective Warp Toolbar <p> Now , let 's look at two scenarios where Ive used perspective warp ( see the video for step by step details ) . <p> The first example shows how you can change the perspective/focal area of the image . - In this case I applied perspective warp to the entire image . - I created planes to match the Adobe building in the center of the image , and then used Perspective Warp to shift the building focal area to the @ @ @ @ @ @ @ @ @ @ truck onto a street . - In the original images , the trucks perspective is close , but does not match the perspective of the street . - Using Perspective Warp , you can easily re-shape content to match perspectives within your composition . <p> A while back , I was asked if its possible to generate PDF documents inside of PhoneGap apps The answer is definitely yes , and its not that hard at all ! - I used the JSPDF library , which has a comprehensive JavaScript API for generating PDF documents . <p> Next , download the JSPDF project code , and copy the jspdf library from the " dist " directory into your PhoneGap project . - I put it into the www/js directory . - Then , be sure to include the library inside of your main HTML file . <p> &lt;script type= " text/javascript " src= " js/jspdf.source.js " &gt;&lt;/script&gt; <p> I used the uncompressed/minified source file in the " dist " directory . <p> Now , let 's generate a PDF document . - Heres a code snippet that will generate a very simple @ @ @ @ @ @ @ @ @ @ on the device using PhoneGaps File API. - This must be called *AFTER* the deviceready event . - All of the console.log statements are just used for debugging this snippet : <p> Notice that the PDF generation is actually the easy part . - Once you have created the document , you can get a string representation of the document using the doc.output() function , and with that you can do whatever you want with it . - You can save it to the local file system like I did , send it to a server , or even send it to a native PDF reader on the device . <p> If you could n't make it to the recently concluded Adobe CreateNow tour do n't  worry , you can still view it online ! - Check out this playlist on YouTube to see a recording of the entire New York CreateNow event . - Youll see the latest and greatest from all of Adobes creative tools . <p> I was - searching the web earlier this week for an older presentation from a few months back , and just happened @ @ @ @ @ @ @ @ @ @ past October . Looks like the videos were posted in November , but I 'm just seeing them now . I had two sessions : - Designing and Architecting PhoneGap and Mobile Web Apps and- Getting Started with PhoneGap and Cross-Platform Mobile Development , and if you werent able to attend them , you 're still in luck ! Here are the videos from those sessions : <h> Designing and Architecting PhoneGap and Mobile Web Apps <p> Tired of Hello World ? In this session , we explore best practices to build real-world PhoneGap applications . We investigate the Single Page Architecture , HTML templates , effective Touch events , performance techniques , modularization and more . We also compare and contrast the leading JavaScript and Mobile Frameworks . This session is a must If you plan to build a PhoneGap application that has more than a couple of screens . <h> Getting Started with PhoneGap and Cross-Platform Mobile Development <p> Unfortunately , I ran into network issues which prevented some of my samples from working in this one , but you 'll still be able to get the point . <p> HTML @ @ @ @ @ @ @ @ @ @ to enable cross-platform mobile application development . In this session , you learn how to leverage your existing HTML and JavaScript skills to build cross-platform mobile applications , how to access the device features ( camera , accelerometer , contacts , file system , etc ) using JavaScript APIs , and how to package your HTML application as a native app for distribution through the different app stores . 
@@106848926 @2248926/ <p> IBMs Watson Developer Cloud speech services just got a whole lot easier for mobile developers . - I myself just learned about these two , and cant wait to integrate them into my own mobile applications . <p> The Watson Speech to Text and Text to Speech services are now available in both native iOS and Android SDKs , making it even easier to integrate language services into your apps . <p> This post is inspired by all the comments Ive seen this week about JS in the enterprise . I would have never imagined- this 10- years ago , but JavaScript is now pretty much ubiquitous . Here are a few reasons why you need to paying attention to JavaScript if you are n't  already , and why you should definitely not- write it off . <p> First , I think one of the major reasons for JavaScripts ubiquity is that- JavaScript is approachable . It is relatively easy for beginners to learn JavaScript , and powerful enough for advanced users to build complex and reliable systems . <p> Second , why you need to pay attention @ @ @ @ @ @ @ @ @ @ JavaScript to develop on virtually any platform : client side applications , server side logic , embedded chips/IoT devices , manage build scripts and dependencies , and more . <p> This does n't  mean you 'll use the exact same code in every case , rather that you can use the same skill set JavaScript Development to deliver solutions across multiple paradigms . <h> The Client Side <p> JavaScript can be used to power client side apps/user interfaces , and user interactions on numerous platforms and devices . <h> Web <p> Of course JavaScript powers the web , this is a given . JavaScript is the primary scripting language for all web browsers . I wo n't focus on this much b/c its already well known . <h> Mobile <p> JavaScript can also be used to power mobile applications that are natively installed on a device . <p> Apache Cordova/PhoneGap- You can build natively installed apps with web technology using PhoneGap or Cordova . PhoneGap is Adobes branded distribution of Cordova , but from the developers perspective , they are basically the same thing . Your app runs within a webview on @ @ @ @ @ @ @ @ @ @ the same way you you build a dynamic web application . Your user interface- is implemented in HTML , styled with CSS , and all interactivity is created with JavaScript . <p> React Native- JavaScript powered web apps do n't  just have to be inside of a a web view . The React Native framework gives developers the ability to write their application using JavaScript and declarative UI elements , and results in a native application running on the mobile device . The logic is interpreted JavaScript at runtime , but everything that the user- interacts with ( all UI elements ) is 100% native , providing a very high quality user experience , and it is now available for both iOS and Android applications . <p> Unity 3D You can even develop rich &amp; immersive mobile 3D simulation or gaming experience , entirely powered by JavaScript using the Unity 3D engine . **These can be web , desktop , or mobile , but is often used in mobile gaming . <h> Desktop <p> Yup , desktop apps are not left out of the mix . Most desktop solutions fall @ @ @ @ @ @ @ @ @ @ end results is a web view that has access to lower level APIs , whose content is developed with web based technology . <h> The Server Side <p> Most obviously- Node.js a JavaScript runtime buit on Chromes V8 JavaScript Engine has made huge inroads into server side development and the enterprise . Node.js , powered by frameworks like express.js or loopback.io makes server side development and complex enterprise apps with JavaScript possible . <p> Here are some stats that- show the magnitude of growth and adoption for Node.js/npm.js alone . NPM- stats currently shows a total of- 186,946 packages available for download , 94,978,032 package downloads in the last day , and 2,451,734,737 package downloads in the last month . <p> This does n't  mean that JavaScript is the best language at everything . It- also does n't  meant that you can take a single piece of source- code and run it in every device/context imaginable . <p> It means that you can use your skills in JavaScript to develop for just about any kind of device/context out there . Its not going to be write once , run everywhere @ @ @ @ @ @ @ @ @ @ learn once , write everywhere . <p> That title get your attention ? - Yes , it really read " Adaptive- mobile- apps that- change based on personal context " with near real-time rules application , without much extra development effort . - If that sounds interesting to you , or like a product you might want to use within your own apps , then you might want to check out this site where you can get involved in the products development : - http : **35;7457;TOOLONG <p> IBM is looking for your input on creating these types of mobile app experiences. - User experiences within a single app that can be- dramatically different per user based on location , past behavior , profile information , social media activity , - and so much more . - With this behavior being driven by configurable rules that can be changed without redeploying an app to the app store . <h> How it works for your customer <p> Consider this scenario : <p> Jon and Andrea download the mobile app for S&amp;W , a retailer known for its attention to providing great @ @ @ @ @ @ @ @ @ @ Andrea use the app to browse and discover content and merchandise differently . <p> Jon primarily navigates to sports related content for his favorite teams to find gear and clothes for travel to his favorite team 's games . Andrea scours the app for sales and fashion trends and usually ends up following her favorite designers . <p> Andrea and Jon go to a baseball game together . She 's never enjoyed watching it , so she opens up the S&amp;W app to entertain herself , and her app 's navigation quickly steers her through Spring fashion articles . <p> Jon however , wants to replace the hat he 's worn the last three times the team lost , and since he 's in the stadium , his S&amp;W app opens right up to the team 's gear page . The app knows he 's out of town and tells him how to get to an S&amp;W store . <h> How it works for the dev team <p> Consider another scenario : <p> One of the developers on the team , George , sets up the system and application . @ @ @ @ @ @ @ @ @ @ the customer experience . <p> Janet writes rules defining how the application could adapt and become more personalized based on inputs like , social media , geolocation , app usage , or customer information data . <p> Once Janet has built out her rules , she simply hits Submit ' and can immediately see her clever interactions reflected in the mobile application without having to involve the development team . <p> Analytics let Janet know which adaptations are working best , and helps her find new opportunities to optimize the app 's user experience . <p> Were not talking about a content management system , or translation based on locale , instead a rules-driven product that can adapt literally every aspect of your app : - customize the user interface , enable or disable different features , customized messaging and notifications , and much more , all variable based upon the user context . - This can be used to present- contextually relevant information , drive adoption , provide more/less data depending on your physical context , and so much more . <p> It wont be tied to a specific @ @ @ @ @ @ @ @ @ @ management system , is n't attempting to re-create Google Now or Apple Proactive Assistance . - Rather , a set of tools and a rules engine that enable you to customize and tailor the app experience to the individual user . <p> Earlier this week I had the privilege of speaking at ApacheCon- in Austin , TX on the topic of data management for apps that work as well offline as they do online . - This is an important topic for mobile apps , since , as we all painfully know already , there is never a case when you are always online on your mobile devices . - There always ends up being a time when you need your device/app , but you cant get online to get the information you need . - Well , this does n't  always have to be the case . There are strategies you can employ to build apps that work just as well offline as they do online , and the strategy- I 'd like to highlight today is based upon data management using the IBM Cloudant NoSQL database as a service , @ @ @ @ @ @ @ @ @ @ link to the presentation slides ( built using reveal.js ) just use the space bar to advance the presentation slides : <p> The " couch " in CouchDB is actually an acronym for Cluster of Unreliable Commodity Hardware . At the core of this cluster is the concept of replication , which in the most basic of terms means that - data is shared between multiple sources . - Replication is used to share information between nodes of the cluster , which provides for cluster reliability and fault tolerance . <p> Cloudant is a- clustered NoSQL- database services that provides an extremely powerful and searchable data store . - It is- designed to power- the web and mobile apps , and all information is exposed via REST services . Since the IBM Cloudant service is based on CouchDB ( and not so coincidentally , IBM is a major contributor to the CouchDB project ) , replication is also core the the Cloudant service . <p> With replication , you only have to write your data/changes to a single node in the cluster , and replication takes care of propagating- these @ @ @ @ @ @ @ @ @ @ apps for the web or mobile , there are options to extend the data replication locally either on the device or in the browser. - This means that you can have a local data store that automatically pushes and/or pulls data from the remote store using replication , and it can be done either via native languages , or using JavaScript . <p> If you want to have local replication in either a web or hybrid ( Cordova/PhoneGap ) app , you can use PouchDB. - PouchDB- is a local JavaScript database modeled- after CouchDB and implements that CouchDB replication API. - So , you can store your data in the browsers local storage , and those changes will automatically be replicated to the remote Cloudant store . - This works in the browser , in a hybrid ( web view ) app , or even inside of a Node.js instance . Granted , if you 're in-browser you 'll need to leverage the HTML5 cache to have your app cached locally . <p> If you are building a native app , do n't  worry , you can take advantage of the @ @ @ @ @ @ @ @ @ @ replication . - This is available for iOS and- Android , and implements the CouchDB replication API . <p> The sample app that I showed in the presentation is a native iOS application based on the GeoPix MobileFirst sample app- that I detailed in a previous post . - The difference is that in this case I showed it using the Cloudant Sync API , instead of the MobileFirst data wrapper classes , even though it was pointing at the exact same Cloudant database instance . - You can see a video of the app in action below . <p> All that you have to do is create a local data store instance , and then use replication to synchronize data between the local store and a remote store . <p> Replication be either one-way ( push or pull ) , or two-way . - So , any changes between the local and remote stores are replicated across the cluster . - Essentially , the local data store just becomes a node in the cluster . - This provides complete access to the local data , even if there is @ @ @ @ @ @ @ @ @ @ the local store , and replication takes care of the rest . <p> In the native Objective-C code , you just need to setup the CDTDatastore manager , and initialize your datastore instance . <p> Once your datastore is created , you can read/write/modify any data in the local store . - In this case I am creating a generic data object ( basically - like a JSON object ) , and creating a document containing this data . - A document is a record within the data store . <p> You can add attachments to the document or modify the document as your app needs . - In the code below , I add a JPG atttachment to the document . <p> Replication is a fire-and-forget process . - You simply need to initialize the replication process , and any changes to the local data store will be replicated to the remote store automatically when the device is online . <p> By assigning a replicator delegate class ( as shown above ) , your app can monitor and respond to changes in replication state . - For example , @ @ @ @ @ @ @ @ @ @ complete , or if an error condition was encountered . <p> If you want to access data from the local store , it is always available within the app , regardless of whether or not the device has an active internet connection . - For example , this method will return all documents within the local data store . <p> Last month I had the opportunity to speak at the DevNexus- developer conference in Atlanta on building native iOS apps- IBM MobileFirst . DevNexus is a great event , and it is always a privilege to attend- I highly recommend it for next year . - If you werent able to make it , no worries ! - Most of the sessions were recorded and are available for viewing online via dzone . <p> The recording of my- session is embedded below . - It covers everything you need to know to get started building apps with the MobielFirst platform . <p> Once your app goes live in the app store you will have just entered into an iterative cycle of updates , improvements , and releases . Each successively @ @ @ @ @ @ @ @ @ @ . IBM MobileFirst Foundation gives you the tools you need to manage every aspect of this cycle , so you can deliver the best possible product to your end user . In this session , we 'll cover the process of integrating a native iOS application with IBM MobileFirst Foundation to leverage all of the capabilities the platform has to offer . 
@@106848927 @2248927/ <h> All posts by Andrew <p> Although there were no official announcements around Flex , Flash &amp; AIR ( other than the release of FP11 &amp; AIR3 ) , do n't  think that the platform is going away or becoming stale In fact , it is quite the opposite . The Flash Platform will continue to thrive and innovate , providing outstanding solutions that set the pace for other technologies to follow . In case you missed the session , here is the " Flash Platform Roadmap " , provided by Scott Castle , Adam Lehman , and Raghu Thricovil , Product Managers for Flash Platform tooling : <p> If that wasnt enough , did you see the new " Monocle " tool , shown by Deepa Subramaniam ? Monocle is the new realtime profiling tool for Flash-based content which will provide additional insight into what 's happening at runtime , and how you can optimize your applications . <p> Did you also see the latest demos showing the Epic Games &amp; the Unreal engine running INSIDE of the Flash Player ? <p> Wednesday concluded another great Adobe MAX event @ @ @ @ @ @ @ @ @ @ design &amp; development communities , as well as check out Adobes latest and greatest . Ill try to summarize as much of the content and announcements as possible here . First , let 's start with the keynotes : <h> MAX Day 1 Keynote <p> Day one focused on new creative tooling , including the Creative Cloud and Adobe Touch suite of applications . You can view the full day one keynote below : <h> MAX Day 2 Keynote <p> Day two focused on new HTML5 tooling and Flash Platform developments . You can view the full day two keynote here : <p> Creative Cloud is a new cloud-based service that allows you to seamlessly share content between creative suite applications , and includes software subscriptions . You can find details on Adobe TV , on the Creative Cloud home page , in the press release , or in the FAQ . <p> Adobe has introduced new tablet-based creative applications for both iOS and Android ( some currently available with iOS coming soon , and others vice versa ) . You can learn more about the Adobe Touch Suite of applications @ @ @ @ @ @ @ @ @ @ a company that sells tile flooring , it would be tough for me to convince someone that tile is the best choice for a particular room because they will expect me to say that tile is best for everything . Conversely , if I worked for a company that sells wood flooring , they expect me to say wood is best . However , if I work for a company that sells both types of flooring , I can have a real discussion about which is best on a room by room basis because I will have credibility in both types of flooring . <p> Adobe announced that it has acquired privately held Typekit Inc. , a leader in the delivery of hosted , high-quality fonts for use on websites . Available as a subscription-based cloud service , You can read details around the Typekit acquisition from the press release . <h> Adobe Announces Digital Publishing Suite , Single Edition <p> Adobes Digital Publishing Suite now offers " Single Edition " , an addition to the DPS tooling that enables small business and single-publishers to create digital publications ( brocheures @ @ @ @ @ @ @ @ @ @ tablet devices . You can read more about DPS Single Edition from the DPS blog , press release , or Adobe TV . <p> Here 's another post that I originally wrote way back in 2006 , when object oriented development was a newer concept to client-side web applications . Again , this post is still very relevant with Flex , AIR &amp; ActionScript for mobile/web/desktop , so I decided to resurrect it from the old blog archive as well . Enjoy <p> Understanding of OOP ( Object Oriented Programming ) is fundamental in being successful with the Flex framework and being able to get the most out of it . Developers who do not possess a computer science-related background may not be aware of the fundamental concepts that comprise OOP and how to apply them correctly , so here is a quick piece to help you out . <p> First , object oriented programming is a programming paradigm where your code is organized into logical objects , and each object has properties and methods . Each object contains similar and/or related functionality , and is organized into classes that @ @ @ @ @ @ @ @ @ @ Example : Let 's say that we have a class " Automobile " . This class would contain the information and functions necessary for our application to use the Automobile class . We could have a numeric property for the number of wheels , the speed , and the direction ( degrees on a compass ) . This class would also contain methods that control the actions of the Automobile object : accelerate , decelerate(break) , turn , start engine , stop engine , etc Our class would look something like this <p> Ok , now that we have a brief explanation of what object oriented programming is , we can get into some more aspects of OOP : inheritance and interfaces . <p> Inheritance is a way to form new objects based on existing objects . When a class inherits from a base class , the new class extends the functionality of the base class , and can utilize public and protected properties and methods from that base class . Inheritance can be used to create different objects that utilize functions within the base class , so that the child @ @ @ @ @ @ @ @ @ @ be used to extend the functionality of existing objects , and inheritance can also be used to override and/ or change functionality from the base class . <p> In Actionscript 3 , you can access the parent class of your class by using the " super " keyword . For instance , calling the constructor of the parent class would use " super() " , where accessing a method of the parent class would use something like : " super.myMethodName() " . If a property of the parent class is created with public or protected access , you can access that property in the child class directly by the property name ( you would use this.propertyName , not super.propertyName ) . <p> Now , Let 's take our Automobile example and apply object-oriented inheritance . We already have a base Automobile class that covers the basic functionality . We can create child classes that extend the functionality of the Automobile . <p> public class SportsCar extends Automobile public function SportsCar() super() ; override public function accelerate():void /* we can override the accelerate function so that it accelerates faster than the base @ @ @ @ @ @ @ @ @ @ the Automobile class , and therefore are instances of the Automobile class . If we have a function outside of the Automobile class , which takes an automobile as the parameter , both a SportsCar and Truck will work since they are both Automobiles . We could have a function such as the following : If we pass in a Truck Instance , and a SportsCar instance , both will work , and each will use the functionality of their specific class instead of the base Automobile class . <p> I 'll get into some more fine-grain details about inheritance later in this post now , let 's move on to interfaces <p> Interfaces are slightly different than inheritance . An interface is a set of " rules " which an object must adhere to . The " rules " are actually method signatures that your class must implement . When we define an interface , we define method signatures that are required for classes that implement that interface . There is no actual code in an interface ; it simply defines methods that must exist within your class . Your class @ @ @ @ @ @ @ @ @ @ actual function . If you have multiple classes that implement an interface , those classes must have the same functions ( only the ones required by the interface ) , but that is where the similarities of the two classes may stop . They could have completely different logic and properties within them this is where inheritance and interfaces differ . Two objects that inherit from the same base class have a lot in common ( properties and methods ) : two objects that implement the same interface only have those interface method signatures in common . <p> Let 's now make an Automobile Interface that defines the functions required to create an IAutomobile object ( note the " I " stands for " interface " ) : <p> We can use the IAutomobile interface to create objects ( classes ) that behave as Automobile objects . These classes do not necessarily inherit from each other and do not necessarily share any common properties . <p> The previous two components both implement the IAutomobile interface , but have nothing else in common . One is simply a class that implements the @ @ @ @ @ @ @ @ @ @ the interface . The mxml component example extends the mx:Canvas component ( the same thing could be done by creating an AS class that extends mx.containers.Canvas ) . Now , let 's look at a function similar to the " race " function from earlier <p> This example will work with either object that I have created because both objects implement the IAutomobile interface . They do not rely upon functions in the class hierarchy , just those that were implemented for this interface . You can also use multiple interfaces on classes that you create . Implementing multiple interfaces basically means that you are adding more required method signatures to your class , and you will have to implement these methods to satisfy each interface . <p> On the other hand , you can not inherit from multiple classes . Some programming languages allow for multiple inheritance ActionScript 3 does not support multiple inheritance ( so i 'll stop there ) . <p> OK enough of this rambling What does this have to do with Flex ? Inheritance and interfaces are used extensively in AS3 to create the flex framework . @ @ @ @ @ @ @ @ @ @ extend from the UIComponent class . AbstractService , DataSerice or EventDispatcher object implements the IEventDispatcher Interface . You may be using these concepts every day , but werent aware of them . Inheritance seems easier to take advanatage of at first Let 's say that you want to create several objects , all of which will have identical functions and variables . It is easy to see that you can create a base class that encapsulates all of the common functionality . You can then create a sub-classes that implement the differing functionality for each class . <p> When putting these concepts into real-world Flex applications you 'll need to get familiar with the following keywords : <p> extends This is used when defining a child class from a parent class . <p> public class MyImage extends Image <p> implements This is used when implementing an interface . <p> public class MyClass implements MyInterface <p> final Classes and methods implented with " final " can not be overridden . <p> final function myFunction() : void <p> static The static keyword is used when creating variables or functions in a class that are @ @ @ @ @ @ @ @ @ @ properties and methods do not require variable instantiation to be executed . <p> public static function myStaticFunction() : void //to use it call it directly from **31;7494;TOOLONG <p> internal This is used when creating a method or property that can be accessed by any object within the same package ( namespace ) <p> internal var foo : String ; <p> override This is used when creating a function that overrides another function from a parent class . <p> override public function myFunction() : void <p> private This is used when creating methods or properties that are only available to the class where it is defined . A private variable can not be accessed by outside classes or from descendant classes . <p> private var myPrivateValue : String ; <p> protected This is used when creating methods or properties that are only available to the class where it is defined and descendant classes . A protected variable can not be accessed by outside classes . <p> protected var myProtectedValue : String ; <p> public This is used when creating properties and methods that are available to any class . <p> here 's @ @ @ @ @ @ @ @ @ @ vs phone ) within your Flex mobile applications . First , get the stage dimensions for the screen size in pixels , then divide that by the applicationDPI ( screen pixel density ) . - This will give you the approximate size in inches of the devices screen . - I say " approximate " because the pixel densities are rounded to 160 , 240 , or 320 , depending on the device . - - In my code , I make the assumption that if the landscape width is greater than or equal to 5 inches , then its a tablet . - I used view states , but you can also layout components manually . <p> Adobe MAX 2011 is rapidly approaching , are you ready ? Next week will bring some exciting announcements , so be sure to watch the MAX Keynotes steamed live online if you are n't  able to be there in person ( Monday AND Tuesday ) . Definitely do not miss them . <p> Adobe MAX 2011 <p> For those of you who will be able to attend , do n't  forget to @ @ @ @ @ @ @ @ @ @ Android at- http : **26;7527;TOOLONG . The MAX Companion is an application that enables MAX attendees to check their session schedules and access general information about the MAX 2011 conference being held October 1-5 , 2011 in Los Angeles , CA . <p> MAX Companion <p> If you are interested in HTML5 , or Flex/AIR for mobile , also do n't  forget to stop by and check out my labs &amp; sessions : <p> Stay up to date on the latest developments in HTML , CSS , and javascript . Learn how to use HTML5 to add interactivity , motion , and video to the web . This lab will start with the fundamentals of HTML5 and how it can be used today for both designers and developers . Get tips , tricks , and best practices from experts , while learning how to apply Adobes tools into your evolving workflow . Discover what can be done with next-generation web standards to create powerfully engaging websites that can transcend screens of all sizes . 
@@106848928 @2248928/ <h> Tag Archives : mobile <p> I am embarking on a new- PhoneGap project that will have to run on many platforms iOS , Android , BlackBerry , and Windows Phone . - This will actually be my first foray into modern Window Phone development . ( I did some experimental work with the Windows Mobile platform many years ago , but a lot has changed since then . ) <p> One of the caveats with Windows Phone development is that it has to be done from Windows , just like iOS development has to be done from OS X ( normally , although some cross platform technologies enable development via other OS/platforms ) . <p> Of course , I did not want to give up OS X , so here 's how I have my environment setup - I have a virtual machine running Windows 7 , in which I can run the Visual Studio development tools . - I am able to deploy to a physical Windows Phone device using the USB connection . <p> Windows Phone Development with PhoneGap , on OS X <p> However , with @ @ @ @ @ @ @ @ @ @ Window Phone emulator , which is a part of the Windows Phone development SDK. - The Windows Phone emulator is not supported inside of a VMWare virtual machine because the emulated operating system environment does not meet the minimum requirements- ( specifically the graphics drivers are not WDDM 1.1 compliant ) . - If you try to use the phone emulator inside the virtual machine , you will just get a blank screen . - I spent a few hours trying to find a workaround , to no avail . - You can use the Windows Phone emulator if you boot your Mac into Windows using Bootcamp , - but I wanted to keep OS X as my primary operating system . <p> Being able to deploy directly to a device works for me , and is ( in my opinion ) better than being able to deploy to an emulator , thus I am happy with this workflow . - I have heard that other people have had trouble deploying to Windows Phone devices through a VMWare emulator , so here are the details on how I have my @ @ @ @ @ @ @ @ @ @ for PhoneGap development on Windows Phone : <p> Note : All of the project setup work will be done through the Windows virtual machine instance . <p> There is a detailed " Getting Started " guide for PhoneGap and Windows Phone available at- http : //phonegap.com/start#wp. - This will provide you with all the information that you need to get started with PhoneGap applications for Windows Phone . <p> To be able to deploy an application to a physical Windows Phone device , you will need to register as a Windows Phone developer on MSDN App Hub at- http : **39;7555;TOOLONG - This is a very similar model to Apples iOS developer program . - There is a $99 annual fee , and once you are registered , you will be able to debug on devices and distribute applications via the Windows Phone Marketplace . - However , debug provisioning is much easier . - Instead of signing each application with a debug certificate , you just have to register your device as a development device . - Once the device is registered , you will be able to deploy @ @ @ @ @ @ @ @ @ @ very similar to the provisioning model for Nook devices . <p> When linking my AppHub account to my Windows Live account ( a required step ) , I ran into a vague error message " Theres a temporary problem with the service . - Please try again . - If you continue to get this error message , try again later . " - After scouring the web for this error message , I found a few threads that mentioned this error is likely the result of an incomplete profile for your Windows Live account . Sure enough , I went into live.com and filled out my profile ( including contact information ) , and this error went away . <h> My Setup <p> Below are the specifics for my setup ; I did not have any issues connecting a Windows Phone device with this configuration . <h> Device Registration <p> To deploy an application to a Windows Phone device , you just have to use the Windows Phone Developer Registration Tool , and walk through a few simple steps to associate your phone with your developer account . - @ @ @ @ @ @ @ @ @ @ SDK. - You can read full details about debugging Windows Phone applications on MSDN . <p> As I mentioned above , I- have heard that others have encountered problems when trying to deploy- applications- to Windows Phone devices via a virtual machine on OSX , but I have not had any problems with this configuration . <p> I was inspired to write this post by several recent conversations . - I was in a debate about whether with the Flex/Flash platform you could easily repurpose content to the desktop using Adobe AIR ( and vice-versa ) , but that you could n't easily do that with PhoneGap applications . ( My stance was that yes , you could repurpose content . ) <p> I wanted to make sure that people were aware that you can repurpose your content , and here 's an example of how . <p> A while back , I wrote a sample PhoneGap application that allows you to- browse information from the 2010 US Census . - You can read more about this application and download the source code here . This application supports lots of platforms iOS @ @ @ @ @ @ @ @ @ @ IE because I was targetting WebKit browsers ) . <p> While this application is a mobile app wrapped in the PhoneGap container , I actually did n't  use any PhoneGap-specific- libraries , so it was very easy to repurpose as a desktop application . - I created an AIR version of this application , which you can download at : <p> I had to use my Android 2. x branch of the US Census Browser code because the WebKit instance inside of AIR does n't  support SVG. - I also changed the container scrolling to use normal CSS " overflow : auto " instead of using iScroll for touch-based scrolling . There were a few other one-off CSS changes to tweak the layout in the AIR web container , but otherwise the code is identical . <p> You just need to create an AIR application XML file and point it to your HTML content , and then package it using ADT . <p> If you were using PhoneGap APIs , you would have to migrate your code to take advantage of AIR APIs , but all other HTML/CSS/JS could be @ @ @ @ @ @ @ @ @ @ AIR for this example , AIR is n't the only game in town for HTML-based desktop applications - Theres an open source project called MacGap , you can use HTA for Windows , and its not hard to write a HTML/Web View wrapper for any platform . Its even been reported that you are going to be able to write apps for Windows 8 purely using HTML &amp; JS , and you would be able to repurpose your code for this as well . <p> While looking at the analytics for my blog , I 've recently started to see a lot of search phrases similar to " what is phonegap ? " , " how does a phonegap app look ? " , " how to get started in phonegap ? " , among many , many others . - In this post , I hope to she 'd some light on some basic questions to help you understand and start working with PhoneGap . <p> In case you do n't  feel like reading the whole thing , here are quicklinks to each question : <h> What is PhoneGap ? <p> PhoneGap @ @ @ @ @ @ @ @ @ @ installed applications using HTML and JavaScript. - The easiest way to think of PhoneGap is a web view container that is 100% width and 100% height , with a JavaScript programming interface that allows you to access underlying operating system features . - You build your user interface using traditional web development skills ( HTML , CSS , &amp; JavaScript ) , and use the PhoneGap container to deploy to different application ecosystems and devices . - When packaged for deployment , the PhoneGap application is a binary distributable file that can be distributed by the " normal " application marketplaces ( iTunes , Google App Market , Amazon Market , etc ) . <h> How does a PhoneGap application typically look ? <p> Since the UI rendering engine is the mobile devices web browser , PhoneGap applications can literally look like anything . - You can use standard HTML &amp; CSS to make it look like a normal web page , you can use a UI framework like jQuery UI , Kendo UI , Sencha , - Twitter Bootstrap , or Skeleton- ( or any other HTML/CSS/JS user interface @ @ @ @ @ @ @ @ @ @ make your web content look like native apps , such as- iUI- to mimic iOS or Android , or- bbUI - to mimic BlackBerry . <p> PhoneGap applications can have static UIs based on normal HTML , or can have dynamic &amp; interactive experiences developed using JavaScript. - It depends upon the specific application , user experience design , target audience , and use cases to dictate how a PhoneGap application will appear . <p> PhoneGap applications can use pinch/zoom gestures to zoom in &amp; out , or you can lock the viewport scale using the viewport metadata tag . - You can have the page scroll using normal browser behaviors , or you can use a library like iScroll to enable touch-based scrolling of specific container elements . <p> There really are lots of ways to create a user interface with HTML , CSS &amp; JavaScript , so there really is n't any " typical " look . - If you do not apply any CSS styles at all , then all user interface elements will use the operating system/browser default for that specific platform . - This includes buttons @ @ @ @ @ @ @ @ @ @ in the exact same manner as the operating systems default web browser . <h> How do I get started in PhoneGap ? <p> Getting started in PhoneGap is easy . - For 90% of a PhoneGap application , all you need is a text editor . - PhoneGap also integrates with device-specific development environments very easily . - You can view " getting started " guides for all application platforms at the links below : <p> When developing PhoneGap applications , just keep in mind that you are running code inside of a web browser instance . - You develop your applications using HTML and JavaScript , not native code , so you do n't  need anything special . - In fact , I personally do most of my development on the desktop using an HTML editor and the Chrome browser. - When I need device-specific functionality , or I need to test on a device , then I switch over the the device-specific environments . <h> How do you debug PhoneGap applications ? <p> Debugging PhoneGap applications can sometimes be the trickiest part of development . - If you @ @ @ @ @ @ @ @ @ @ get access to JavaScript exceptions when they happen . - There are a few strategies for debugging PhoneGap applications . <h> Develop as much as possible on the desktop browser <p> Since PhoneGap applications are written with HTML , CSS , and JavaScript , you can develop most of them using any HTML editor and debug them within a desktop web browser. - The latest versions of all major web browsers ( including Chrome , IE , Firefox , Opera and Safari ) provide rich debugging features . In the developer tools for the browsers , you can inspect HTML DOM elements , inspect CSS styles , set breakpoints in JavaScript , and introspect into memory &amp; JavaScript variables . - You can learn more about the desktop browser development tools at : <p> Once you build the main aspects of your application using desktop tools , you can switch over to a device-specific environment to add device-specific behavior and integrate with PhoneGap APIs . <p> It is imperative that you test your applications on actual devices ! - Actual devices will have different runtime performance than desktop browsers and @ @ @ @ @ @ @ @ @ @ and different UX scenarios . <h> Debug With debug.phonegap.com <p> PhoneGap provides a hosted service that allows you to perform remote , on-device debugging through- debug.phonegap.com. - This uses the Weinre ( Web Inspector Remote ) debugging tool to allow you to remotely inspect the DOM , resource loading , network usage , timeline , and console output . - If you have used any of the developer tools listed above , this will look very familiar . - You will not be able to set breakpoints on the mobile device , but it is certainly better than nothing at all . <h> Remote Web Inspector Through iOS 5 <p> There is a little known undocumented API introduced in iOS5 that allows you to perform remote debugging through the iOS5 Simulator. - You just need to enable remote debugging <p> Then launch the application in the desktop iOS Simulator . Once the app is running , open a local Safari instance to : http : //localhost:9999/ . This will launch the remote debugger , complete with breakpoints and script introspection . <h> More Debugging Info <h> How do you architect @ @ @ @ @ @ @ @ @ @ same way that you create mobile web experiences . The difference is that the initial HTML assets are available locally , instead of on a remote server . - The PhoneGap application loads the initial HTML , which can then request resources from a server , or from the local environment . - Since PhoneGap is based in a browser , it behaves exactly as you would expect a web browser to behave . - You can load multiple pages ; however , keep in mind that once you load/unload a page you may lose any data that is stored in memory via JavaScript. - PhoneGap also supports the single-page web experience model . I strongly suggest using the single-page architecture approach . <h> Single-Page Architecture <p> A single-page architecture refers to the practice of having a single HTML page that dynamically updates based upon data and/or user input . - You can think of this as closer to a true client/server architecture where there is a client application ( written with HTML &amp; JS ) and a separate server structure for serving data . - All client-side application logic @ @ @ @ @ @ @ @ @ @ and update its views without reloading the current web page . <p> Using a Single-Page architecture allows you to maintain data in-memory , in JavaScript , which allows you to have a stateful , yet dynamic user interface . - You can read more about single-page architectures at : - LONG ... <h> How do you get PhoneGap apps on devices and into application ecosystems ? <p> PhoneGap applications can be deployed using the same guidelines for native applications for each given platform . - You must follow the rules of each hardware platform/vendor , and there is no way to get around that . - - You can compile the executables for each platform yourself using each platforms specific build process , or you can use build.phonegap.com to compile them for you . - build.phonegap.com is a hosted service that will compile platform-specific application distributable files for you . - In either case , the output of the build process is a platform-specific binary file : IPA for iOS , APK for Android , etc - You can read more about distributing to various application ecosystems , and each @ @ @ @ @ @ @ @ @ @ between PhoneGap and AIR ? <p> The most fundamental differences between PhoneGap and AIR is that you develop AIR applications using tools rooted in the Flash Platform ( Flex , Flash , ActionScript , MXML ) , and you develop PhoneGap applications using HTML , CSS , &amp; JavaScript. - AIR applications use the AIR runtime , which allows you to have a single code base , with the exact same expected behavior across all supported platforms . - PhoneGap applications run inside of the native web browser component for each supported platform . - For this reason , a PhoneGap codebase may behave slightly different between separate platforms , and you will need to account for this during your development efforts . <p> Air applications can be built for iOS , Android , BlackBerry Playbook , and the desktop ( mac and windows ) , with future support for Windows Metro ( Windows 8 mobile interface ) . You can read more about AIRs supported platforms at : - LONG ... <p> ActionScript has strongly-typed objects and supports classical inheritance programming models . AIR applications can also be built @ @ @ @ @ @ @ @ @ @ build enterprise-class applications . - Components in AIR applications are logical objects that have behaviors , properties , and a graphics context . <p> JavaScript-based applications support prototypal inheritance , and have numerous open-source frameworks/tools that can be used . - HTML/JS applications are all visualized through HTML DOM elements . - HTML interfaces can be created through basic string concatenation or JavaScript templating , but in the end you are really just creating DOM elements that have properties and styles . <p> There are some fundamental difference in the syntax of building these applications , however the basic concepts of interactive design and interactive development are identical . - Both platforms have valid strengths , which I could write about ad nauseum I 'll save that for another post . <p> here 's a silly/fun app I built after hours using PhoneGap. - It is a childrens drawing app built entirely with the HTML5 Canvas element , using a PhoneGap wrapper , targeting the iPad. - I was inspired by magnetic drawing toys that I often use when drawing with my daughter , and this was really , really easy and @ @ @ @ @ @ @ @ @ @ the exact HTML5 Canvas brush image/sketching technique that I have previously demonstrated the only change is that I added the new UI style elements and added support for multiple touch points . Otherwise , the drawing logic is identical . <p> Lil Doodle is a great new iPad application for entertaining both you and your children ! If you know how to use a childrens magnetic drawing toy , then you know how to use Lil Doodle . Pick a " pen " shape , and start doodling . Your imagination is your only limit . If you want to erase everything and start over , just use the slider at the bottom . Doodle and have fun ! <p> Using the HTML5 Canvas inside of PhoneGap has great performance on iOS , and building the application using purely HTML , CSS , and JavaScript made it incredibly simple . After I wrote the core drawing engine for a previous- blog post , I whipped up the UI in one evening , and then started user testing with my little beta tester . - She found some issues that I @ @ @ @ @ @ @ @ @ @ it to the app store . <p> and yes , she really does play with it : <p> The app is currently available for iPad devices on iTunes I 'm about to start researching/testing performance on other platforms , so maybe soon it will be out in other ecosystems well see . - You can get it now at : <p> Back in the summer , I was lucky enough to get my hands on some early builds of Stage3D for mobile . I built some simple examples , including basic geometric shapes and simple 3D bubble charts inside of mobile Flex/AIR applications . I have been asked numerous times for the source code , and Ive finally given in , and am sharing some source code . <p> I am not posting the full mobile application source code , since Stage3D for mobile is not yet available . However , I have ported the 3D bubble chart example to run in a Flex application targeting the desktop ( Flash Player 11 ) . The bubble chart example extends the concepts explored in the basic geometric shapes example . <p> Before @ @ @ @ @ @ @ @ @ @ the mobile code " , let me explain When I ported the code from the mobile project to the desktop Flex project , all I changed was code specific to the mobile Flex framework . I changed **34;7596;TOOLONG to &lt;s:Application&gt; and the corresponding architecture changes that were required , and I changed the list item renderers to Spark item renderers based on &lt;s:Group&gt; instead of mobile item renderers. - In the mobile item renderers , all my drawing logic was done using the ActionScript drawing API. - For simplicity in the port , I just used &lt;s:Rect&gt; to add the colored regions in the desktop variant . <p> That is all I changed ! - <p> The stage3D code between the desktop and mobile implementations is identical . - - You can see the desktop port in action in the video below : <p> The source code was intended to be exploratory at best I was simply experimenting with hardware accelerated content , and how it can be used within your applications . - There is one big " gotcha " that you will have to watch out for if @ @ @ @ @ @ @ @ @ @ shows up behind Flex content on the display list . - By default , Flex apps have a background color , and they will hide the Stage3D content . - If you want to display any Stage3D content within a Flex application ( regardless of web , desktop AIR , or mobile ) , you must set the background alpha of the Flex application to zero ( 0 ) . - Otherwise you will pull out some hair trying to figure out why it does n't  show up . <p> The source code for the web/Flex port of this example is available at : 
@@106848929 @2248929/ <p> In this post I 'd like to show a fairly simple application that I put together which shows off some of the rich capabilities for IBM MobileFirst for Bluemix that you get out of the box All with an absolute minimal amount of your own developer effort . - Bluemix , of course , being IBMs platform as a service offering . <p> GeoPix is a sample application leveraging IBM MobileFirst for Bluemix to capture data and images on a mobile device , persist that data locally ( offline ) , and replicate that data to the cloud . Since its built with IBM MobileFirst , we get lots of things out of the box , including operational analytics , user authentication , and much more . <p> ( full source code at the bottom of this post ) <p> Heres what the application currently- does : <p> User can take a picture or select an image from the device <p> App captures geographic location when the image is captured <p> App saves both the image and metadata to a local data store- on the device . <p> App @ @ @ @ @ @ @ @ @ @ store up to the remote store whenever the network is available <p> Oh yeah , cant forget , the user auth- is via Facebook <p> MobileFirst provides all the analytics we need . - Bluemix provides the cloud based server and Cloudant- NoSQL data store . <p> All captured data is available on a web based front-end powered by Node.js <p> In this sample I 'm using everything but the Push Notifications service . - Im- using user authentication , the Cloudant DB ( offline/local store and remote/cloud store ) , and the node.js backend. - You get the operational analytics automatically . <h> Capturing Images <p> Capturing images from the device is also very straightforward . - In the app I leverage Apples- UIImagePickerController- to allow the user to either upload an existing image or capture a new image . - See the presentImagePicker and- **29;7632;TOOLONG below . All of this standard practice using Apples developer SDK : <h> Persisting Data <p> If you notice in the- **29;7663;TOOLONG method above , there is a call to- the DataManagers saveImage withLocation method . This is where we save data locally and @ @ @ @ @ @ @ @ @ @ local data store up to the Cloudant NoSQL database . - This is powered by the iOS 8 Data service from Bluemix . <p> The first thing that we will need to do is initialize the local and remote data stores . Below you can see my init method from my DataManager class . In this , you can see the local data store is initialized , then the remote data store is initialized . If either data store already exists , the existing store will be used , otherwise it is created . <p> Once the data stores are created , you can see that the replicate method is invoked . - This starts up the replication process to automatically push changesfrom the local data store to the remote data store " in the cloud " . <p> Therefore , if you 're collecting data when the app is offline , then you have nothing to worry about . - All of the data will be stored locally and pushed up to the cloud whenever you 're back online all with no additional effort on your part . - When using @ @ @ @ @ @ @ @ @ @ start the replication process and let it do its thing fire and forget . <p> In my replicate function , I setup- CDTPushReplication for pushing changes to the remote data store . - You could also setup two-way replication to automatically pull new changes from the remote store . <p> Once weve setup the remote and local data stores and setup replication , we now are ready to save the data the were capturing within our app . <p> Next is my saveImage withLocation method . - Here you can see that it creates a new- **26;7694;TOOLONG object ( this is a generic object for the Cloudant NoSQL database ) , and populates it with the location data and timestamp. - It then creates a jpg image from the UIImage ( passed in from the UIImagePicker above ) and adds the jpg as an attachment to the document revision . - Once the document is created , it is saved to the local data store . - We then let replication take care of persisting this data to the back end . <p> If we want to query data from @ @ @ @ @ @ @ @ @ @ just use the performQuery method on the data store . Below you can see a method for retrieving data for all of the images in the local data store . <p> At this point we 've now captured an image , captured the geographic location , saved that data in our local offline store , and then use- replication to save that data up to the cloud whenever it is available . <p> AND <p> We did all of this without writing a single line of server-side logic . - Since this is built on top of MobileFirst for Bluemix , all the backend infrastructure is setup for us , and we get operational analytics to monitor everything that is happening . <p> With the operational analytics we get : <p> App usage <p> Active Devices <p> Network Usage <p> Authentications <p> Data Storage <p> Device Logs ( yes , complete debug/crash logs from devices out in the field ) <p> Push Notification Usage <h> Sharing on the web <p> Up until this point we have n't had to write any back-end code . However the mobile app boilerplate on Bluemix comes @ @ @ @ @ @ @ @ @ @ take advantage of it . <p> The Node.js back end comes preconfigured to leverage the- express.js- framework for building web applications . - I added the- jade template engine and Leaflet for web-mapping , and was able to crank this out ridiculously quickly . <p> The first thing we need to do is make sure - we have our configuration variables for accessing the Cloudant service from our node app. - These are environment vars that you get automatcilly if you 're running on Bluemix , but you need to set these for your local dev environment : <p> Next you 'll se the logic for querying the Cloudant data store and preparing the data for our UI templates . You can customize this however you want caching for performance , refactoring for abstraction , or whatever you want . All interactions with Cloudant are powered by the Cloudant Node.js Client <p> A while back I wrote about adding parallax effects to your HTML/JS experiences to make them feel a bit richer and closer to a native experience . - I 've just added this subtle ( key word *subtle* ) effect to @ @ @ @ @ @ @ @ @ @ to share here . <p> If you are wondering what I am talking about with " parallax effects " Parallax movement is where objects in the background move at a different rate than objects in the foreground , thus causing the perception- of depth . - Read more about it if you 're interested . <p> First , here 's a quick video of this latest app in action . - Its a hybrid MobileFirst app , but this technique could be used in any **35;7722;TOOLONG web app experience . - The key is to keep it subtle and not too much " in your face " , and yes , it is very subtle in this video . - You have to watch closely . <p> The techniques that I wrote about in the previous post still apply Ive just added a bit more to cover more use cases . <p> This sets the background image and default position . - The distinct change here is that I set the background size to " auto " width and 120% height . - In this case , you can have a huge @ @ @ @ @ @ @ @ @ @ window size , or a small image that scales up to a larger window size . - This way you do n't  end up with seams in a repeated background or a background that is too big to highlight the parallax effect effectively . <p> In the requestAnimationFrame loop , it only applies changes *if* there are changes to apply . - This prevents needless calls to apply CSS even if the CSS styles had n't  changed . - In this , I also truncate- the numeric CSS string so that it is n't reapplying CSS if the position should shift by 0.01 pixels . Side note : If you are n't  using requestAnimationFrame for HTML animations , you should learn about- it . <p> If you used my old code and were holding the device upside down , it would n't work . - Not even a little bit . - This has that fixed ( see comments inline above ) . <p> This moves the background in CSS , which does n't  cause browser reflow operations , and moves the foreground content ( inside of a div ) using translate3d @ @ @ @ @ @ @ @ @ @ - This helps keep animations smooth and the UX performing optimally . <p> I also added a global variable to turn parallax on and off very quickly , if you need it . <p> The result is a faster experience that is more efficient and less of a strain on CPU and battery . - Feel free to test this technique out on your own . <p> If you use the code above , you can modify the xMovement and yMovement variables to exaggerate the parallax effect . <p> This is more than just " Cloud Services " which more generally refer to a scalable virtual cluster- of computing or storage resources . - Bluemix is IBMs suite of cloud service offerings , and covers lots of use cases : <p> Bluemix is an open-standards , cloud-based platform for building , managing , and running apps of all types , such as web , mobile , big data , and smart devices . Capabilities include Java , mobile back-end development , and application monitoring , as well as features from ecosystem partners and open source " all provided as-a-service in @ @ @ @ @ @ @ @ @ @ ? - MBaaS- enables growth of mobile applications- with seamless ( and virtually endless ) scalability , all without having to manage individual systems for the application server , database , identify management , push notifications , or platform-specific services . <p> Ive been writing a lot about IBM MobileFirst lately for a seamless API to deliver mobile apps to multiple platforms ; though it has been- in the context of an on-premise installation . - However , did you know that many of the exact same MobileFirst features are available as- MBaaS services on IBM Bluemix ? <p> Mobile Data The mobile data service includes- a- NOSQL database ( powered by IBM Cloudant ) , file storage- capabilities , and appropriate management and analytics features to measure the number of calls , storage usage , time/activity , and OS distribution . <p> Push Notifications The push notification service allows you to easily push data to the right people at the right time on- either Apple APNS or Google GCM platforms all with a single API. - Notifications can be sent by either an app or backend system , and @ @ @ @ @ @ @ @ @ @ group of devices based on their tags/subscriptions. - Of course , with appropriate analytics for monitoring activity , distribution , and engagement . <p> Many of these are the exact same features that you can host in your own on-premise IBM MobileFirst Platform Foundation server the difference is that you do n't  have to maintain the infrastructure . - You can scale as needed through the Bluemix cloud offering . <p> One of the many popular feature of IBM MobileFirst SDK is the ability to capture client-side- logs from mobile devices out in the wild in a central location ( on the server ) . - That means you can capture information from devices *after* you have deployed your app into production . - If you are trying to track down or recreate bugs , this can be incredibly helpful . Let 's say that users on iOS 7.0 , specifically on iPhone 4- models are having an issue . - You can capture device logs at this level of granularity ( or at a much broader scope , if you choose ) . <p> The logging classes in the MobileFirst @ @ @ @ @ @ @ @ @ @ have logging classes that you can use to write out- trace , debug , info , log , warn , fatal , or error messages . - You can also optionally specify a package name , which is used to identify which code module the debug statements are coming from . - With the package name , you 'll be able to see if the log message is coming from a user authentication manager , a data receiver , a user interface view , or any other class based upon how you setup your loggers. - Once the log file reaches the specified buffer size , it will automatically be sent to the server . <p> On the server you can setup log profiles that- determine the level of granularity of messages that are captured on the server . - Let 's say you have 100,000 devices consuming- your app. - You can configure the profiles to collect error or fatal messages for every app instance . - However , you probably do n't  want to capture complete device logs for every app instance ; You can setup the log profiles to @ @ @ @ @ @ @ @ @ @ . <p> As an- example , take a look at the screenshot below to see how you can setup log collection profiles : <p> Configuring Log Profiles on the MobileFirst Server <p> When writing your code , you just need to create a logger instance , then write to the log . <p> If you 're curious when you might want a trace statement , vs. a log statement , vs. a debug statement , etc Here is the usage level- guidance from the docs : <p> Then on the server , you can go into the analytics dashboard and access complete logs for a device , or search through all client-side logs with the ability to filter on application name , app versions , log levels , package name , environment , device models , and OS versions within an optional date range , and with the ability to search for keywords in the log message . <p> The IBM WatsonG Question Answer ( QA ) service provides an API that give you the power of the IBM Watson cognitive computing system . With this service , you can connect @ @ @ @ @ @ @ @ @ @ receive responses that you can use within your application . <p> Just click on the microphone button , allow access to the system mic , and start talking . - Just a warning , lots of background noise might interfere with the APIs ability to recognize &amp; generate a meaningful transcript . <p> This demo uses the Watson For Healthcare data set , which contains information- from HealthFinder.gov , the CDC , National Hear Lung , and Blood Institute , - National Institute of Arthritis and Musculoskeletal and Skin Diseases , National Institute of Diabetes and Digestive and Kidney Diseases , National Institute of Neurological Disorders and Stroke , and Cancer.gov. - Just know that this is a beta service/data set implementing Watson for your own enterprise- solutions requires system training and algorithm development for Watson to be able to understand your data . <p> Using Watson with this dataset , you can ask conditional questions , like : <p> What is X ? <p> What causes X ? <p> What is the treatment for X ? <p> What are the symptoms of X ? <p> Am I at risk @ @ @ @ @ @ @ @ @ @ What should I expect before X ? <p> What should I expect after X ? <p> General health auestions , like : <p> What are the benefits of taking aspirin daily ? <p> Why do I need to get shots ? <p> How do I know if I have food poisoning ? <p> Or , action-related questions , like : <p> How can I quit smoking ? <p> What should I do if my child is obese ? <p> What can I do to get more calcium ? <p> Watson services are exposed through a RESTful API , and can easily be integrated into an existing application . - For example , here 's a snippet demonstrating how- you can consume the Watson QA service inside of a Node.js app : <p> Hooking into the Web Speech API is just as easy ( assuming you 're using a browser that implements the Web Speech API I built this demo using Chrome on OS X ) . On the client side , you just need need to create a SpeechRecognition instance , and add the appropriate event handlers . <p> To make your @ @ @ @ @ @ @ @ @ @ you just need to- create a new SpeechSynthesisUtterance object , and pass it into the **30;7759;TOOLONG function . You can add event listeners to handle speech events , if needed . 
@@106848932 @2248932/ <h> New Swift Offerings from IBM <p> In my last post I mentioned some new announcements related to the Swift programming language at IBM . - Upon further thought , I guess- its probably not a bad idea to re-post- more detail here too <p> If you did n't  see/hear it last week , IBM unveiled several projects to advance the Swift language for developers , which we think will have a profound impact on developers &amp; developer productivity in the years to come . You can view a replay of the IBM announcement- in the video embedded below , or just scroll down for direct links : <p> Here are quick links to each of the projects listed : <p> Kitura A light-weight web framework written in Swift , that allows you to build web services with complex routes , easily . Learn more 
@@106848933 @2248933/ <h> Tag Archives : phonegap <p> While looking at the analytics for my blog , I 've recently started to see a lot of search phrases similar to " what is phonegap ? " , " how does a phonegap app look ? " , " how to get started in phonegap ? " , among many , many others . - In this post , I hope to she 'd some light on some basic questions to help you understand and start working with PhoneGap . <p> In case you do n't  feel like reading the whole thing , here are quicklinks to each question : <h> What is PhoneGap ? <p> PhoneGap is an application framework that enables you to build natively installed applications using HTML and JavaScript. - The easiest way to think of PhoneGap is a web view container that is 100% width and 100% height , with a JavaScript programming interface that allows you to access underlying operating system features . - You build your user interface using traditional web development skills ( HTML , CSS , &amp; JavaScript ) , and use the PhoneGap container @ @ @ @ @ @ @ @ @ @ When packaged for deployment , the PhoneGap application is a binary distributable file that can be distributed by the " normal " application marketplaces ( iTunes , Google App Market , Amazon Market , etc ) . <h> How does a PhoneGap application typically look ? <p> Since the UI rendering engine is the mobile devices web browser , PhoneGap applications can literally look like anything . - You can use standard HTML &amp; CSS to make it look like a normal web page , you can use a UI framework like jQuery UI , Kendo UI , Sencha , - Twitter Bootstrap , or Skeleton- ( or any other HTML/CSS/JS user interface framework ) . You can also use CSS styles/themes to make your web content look like native apps , such as- iUI- to mimic iOS or Android , or- bbUI - to mimic BlackBerry . <p> PhoneGap applications can have static UIs based on normal HTML , or can have dynamic &amp; interactive experiences developed using JavaScript. - It depends upon the specific application , user experience design , target audience , and use cases to dictate @ @ @ @ @ @ @ @ @ @ can use pinch/zoom gestures to zoom in &amp; out , or you can lock the viewport scale using the viewport metadata tag . - You can have the page scroll using normal browser behaviors , or you can use a library like iScroll to enable touch-based scrolling of specific container elements . <p> There really are lots of ways to create a user interface with HTML , CSS &amp; JavaScript , so there really is n't any " typical " look . - If you do not apply any CSS styles at all , then all user interface elements will use the operating system/browser default for that specific platform . - This includes buttons , links , and color/highlight states . - This behaves in the exact same manner as the operating systems default web browser . <h> How do I get started in PhoneGap ? <p> Getting started in PhoneGap is easy . - For 90% of a PhoneGap application , all you need is a text editor . - PhoneGap also integrates with device-specific development environments very easily . - You can view " getting started " guides for @ @ @ @ @ @ @ @ @ @ developing PhoneGap applications , just keep in mind that you are running code inside of a web browser instance . - You develop your applications using HTML and JavaScript , not native code , so you do n't  need anything special . - In fact , I personally do most of my development on the desktop using an HTML editor and the Chrome browser. - When I need device-specific functionality , or I need to test on a device , then I switch over the the device-specific environments . <h> How do you debug PhoneGap applications ? <p> Debugging PhoneGap applications can sometimes be the trickiest part of development . - If you are testing on a physical device , you cant always get access to JavaScript exceptions when they happen . - There are a few strategies for debugging PhoneGap applications . <h> Develop as much as possible on the desktop browser <p> Since PhoneGap applications are written with HTML , CSS , and JavaScript , you can develop most of them using any HTML editor and debug them within a desktop web browser. - The latest versions of @ @ @ @ @ @ @ @ @ @ Firefox , Opera and Safari ) provide rich debugging features . In the developer tools for the browsers , you can inspect HTML DOM elements , inspect CSS styles , set breakpoints in JavaScript , and introspect into memory &amp; JavaScript variables . - You can learn more about the desktop browser development tools at : <p> Once you build the main aspects of your application using desktop tools , you can switch over to a device-specific environment to add device-specific behavior and integrate with PhoneGap APIs . <p> It is imperative that you test your applications on actual devices ! - Actual devices will have different runtime performance than desktop browsers and simulators , and may unearth different bugs/issues including API differences and different UX scenarios . <h> Debug With debug.phonegap.com <p> PhoneGap provides a hosted service that allows you to perform remote , on-device debugging through- debug.phonegap.com. - This uses the Weinre ( Web Inspector Remote ) debugging tool to allow you to remotely inspect the DOM , resource loading , network usage , timeline , and console output . - If you have used any of the @ @ @ @ @ @ @ @ @ @ . - You will not be able to set breakpoints on the mobile device , but it is certainly better than nothing at all . <h> Remote Web Inspector Through iOS 5 <p> There is a little known undocumented API introduced in iOS5 that allows you to perform remote debugging through the iOS5 Simulator. - You just need to enable remote debugging <p> Then launch the application in the desktop iOS Simulator . Once the app is running , open a local Safari instance to : http : //localhost:9999/ . This will launch the remote debugger , complete with breakpoints and script introspection . <h> More Debugging Info <h> How do you architect PhoneGap applications ? <p> You generally architect PhoneGap applications the same way that you create mobile web experiences . The difference is that the initial HTML assets are available locally , instead of on a remote server . - The PhoneGap application loads the initial HTML , which can then request resources from a server , or from the local environment . - Since PhoneGap is based in a browser , it behaves exactly as you would @ @ @ @ @ @ @ @ @ @ load multiple pages ; however , keep in mind that once you load/unload a page you may lose any data that is stored in memory via JavaScript. - PhoneGap also supports the single-page web experience model . I strongly suggest using the single-page architecture approach . <h> Single-Page Architecture <p> A single-page architecture refers to the practice of having a single HTML page that dynamically updates based upon data and/or user input . - You can think of this as closer to a true client/server architecture where there is a client application ( written with HTML &amp; JS ) and a separate server structure for serving data . - All client-side application logic resides in JavaScript. - The client application may request data and update its views without reloading the current web page . <p> Using a Single-Page architecture allows you to maintain data in-memory , in JavaScript , which allows you to have a stateful , yet dynamic user interface . - You can read more about single-page architectures at : - LONG ... <h> How do you get PhoneGap apps on devices and into application ecosystems ? <p> @ @ @ @ @ @ @ @ @ @ native applications for each given platform . - You must follow the rules of each hardware platform/vendor , and there is no way to get around that . - - You can compile the executables for each platform yourself using each platforms specific build process , or you can use build.phonegap.com to compile them for you . - build.phonegap.com is a hosted service that will compile platform-specific application distributable files for you . - In either case , the output of the build process is a platform-specific binary file : IPA for iOS , APK for Android , etc - You can read more about distributing to various application ecosystems , and each systems signing/certificate requirements at : <h> What is the difference between PhoneGap and AIR ? <p> The most fundamental differences between PhoneGap and AIR is that you develop AIR applications using tools rooted in the Flash Platform ( Flex , Flash , ActionScript , MXML ) , and you develop PhoneGap applications using HTML , CSS , &amp; JavaScript. - AIR applications use the AIR runtime , which allows you to have a single code base , @ @ @ @ @ @ @ @ @ @ . - PhoneGap applications run inside of the native web browser component for each supported platform . - For this reason , a PhoneGap codebase may behave slightly different between separate platforms , and you will need to account for this during your development efforts . <p> Air applications can be built for iOS , Android , BlackBerry Playbook , and the desktop ( mac and windows ) , with future support for Windows Metro ( Windows 8 mobile interface ) . You can read more about AIRs supported platforms at : - LONG ... <p> ActionScript has strongly-typed objects and supports classical inheritance programming models . AIR applications can also be built using the Flex framework , which allows you to rapidly build enterprise-class applications . - Components in AIR applications are logical objects that have behaviors , properties , and a graphics context . <p> JavaScript-based applications support prototypal inheritance , and have numerous open-source frameworks/tools that can be used . - HTML/JS applications are all visualized through HTML DOM elements . - HTML interfaces can be created through basic string concatenation or JavaScript templating , but in @ @ @ @ @ @ @ @ @ @ have properties and styles . <p> There are some fundamental difference in the syntax of building these applications , however the basic concepts of interactive design and interactive development are identical . - Both platforms have valid strengths , which I could write about ad nauseum I 'll save that for another post . <p> here 's a silly/fun app I built after hours using PhoneGap. - It is a childrens drawing app built entirely with the HTML5 Canvas element , using a PhoneGap wrapper , targeting the iPad. - I was inspired by magnetic drawing toys that I often use when drawing with my daughter , and this was really , really easy and a lot of fun to build . - I used the exact HTML5 Canvas brush image/sketching technique that I have previously demonstrated the only change is that I added the new UI style elements and added support for multiple touch points . Otherwise , the drawing logic is identical . <p> Lil Doodle is a great new iPad application for entertaining both you and your children ! If you know how to use a childrens magnetic drawing @ @ @ @ @ @ @ @ @ @ . Pick a " pen " shape , and start doodling . Your imagination is your only limit . If you want to erase everything and start over , just use the slider at the bottom . Doodle and have fun ! <p> Using the HTML5 Canvas inside of PhoneGap has great performance on iOS , and building the application using purely HTML , CSS , and JavaScript made it incredibly simple . After I wrote the core drawing engine for a previous- blog post , I whipped up the UI in one evening , and then started user testing with my little beta tester . - She found some issues that I had overlooked , and a few days later I submitted it to the app store . <p> and yes , she really does play with it : <p> The app is currently available for iPad devices on iTunes I 'm about to start researching/testing performance on other platforms , so maybe soon it will be out in other ecosystems well see . - You can get it now at : <p> If you have ever tried to develop @ @ @ @ @ @ @ @ @ @ supported , then you have likely pulled all the hair from your head . In its current state , HTML5 Audio is wrought with issues lack of consistent codec support across browsers &amp; operating systems , no polyphony ( a single audio clip can not be played on top of itself ) , and lack of concurrency ( on some of the leading mobile browsers you can only play one audio file at a time , if at all ) . Even the leading HTML5 games for desktop browsers do n't  even use HTML5 audio ( they use Flash ) . Do n't  believe me ? Just take a look at Angry Birds , Cut the Rope , or Bejeweled in a proxy/resource monitor <h> The Problem <p> HTML5 audio is not *yet* ready for prime-time . There are some great libraries like SoundManager , which can help you try to use HTML5 audio with a failover to Flash , but you are still limited without polyphony or concurrency . In desktop browsers , Flash fixes these issues , and Flash is still vastly superior to HTML5 for audio programming @ @ @ @ @ @ @ @ @ @ can have great audio capabilities by developing apps with AIR . However , what if you are n't  using AIR ? In native applications , you can access the underlying audio APIs and have complete control . <p> If you are developing mobile applications with PhoneGap , you can use the Media class , which works great . If you want polyphony , then you will have to do some work managing audio files for yourself , which can get tricky . You can also write native plugins that integrate with the audio APIs for the native operating systems , which is what i will be covering in this post . <p> Before continuing further , let 's take a minute to understand what I am talking about when I refer to concurrency , polyphony , and low-latency <h> Concurrency <p> Concurrency in audio programming refers to the ability to play multiple audio resources simultaneously . - HTML5 in most mobile devices does not support this not in iOS , not in Android . - In fact , HTML5 Audio does not work *at all* in Android 2. x and earlier @ @ @ @ @ @ @ @ @ @ does PhoneGaps Media class , which is based on Android MediaPlayer and iOS- AVAudioPlayer . <h> Polyphony <p> In this case , polyphony is the production of multiple sounds simultaneously ( I 'm not referring to the concept of polyphany in music theory ) . In describing concurrency , I refered to the ability to play 2 separate sounds at the same time , where with polyphony I refer to the ability to play the same sound " on top " of itself . There can be multiple " voices " of the same sound . In the most literal of definitions concurrency could be considered a part of polyphony , and polyphony a part of concurrency Hopefully you get what I 'm trying to say. - In its current state , HTML5 audio supports neither concurrency or polyphony. - The PhoneGap Media class does not support polyphony , however you can probably manage multiple media instances via javascript to achieve polyphonic behavior this requires additional work in the JavaScript side of things to juggle resources . <h> Low Latency <p> Low latency refers to " human-unnoticeable- delays- between an- input- being @ @ @ @ @ @ @ @ @ @ according to wikipedia. - In this case , I refer to low latency audio , meaning that there is an- imperceptible- delay between when a sound is triggered , and when it actually plays . - This means that sounds will play when expected , not after a wait . - This means a bouncing ball sound should be heard as you see the ball bouncing on the screen . - Not after it has already bounced . <p> In HTML5 , you can auto-load a sound so that it is ready when you need it , but do n't  expect to play more than one at a time . - With the PhoneGap Media class , the audio file is n't actually requested until you invoke " play " . - This occurs inside " startPlaying " on Android , and " play " on iOS. - What I wanted was a way to preload the audio so that it is immediately ready for use at the time it is needed . <h> The Solution <p> PhoneGap makes it really easy to build natively installed applications using a familiar paradim @ @ @ @ @ @ @ @ @ @ you to tie into native code using the native plugin model . - This enables you to write your own native code and expose that code to your PhoneGap application via a JavaScript interface and that is exactly what I did to enable low-latency , concurrent , and polyphonic audio in a PhoneGap experience . <p> I created PhoneGap native plugins for Android and iOS that allow you to preload audio , and playback that audio quickly , with a very simple to use API. - I 'll get into details how this works further in the post , but you can get a pretty good idea of what I mean by viewing the following two videos . <p> The first is a basic " Drum Machine " . - You just tap the pads to play an audio sample . <p> The second is a simple user interface that allows you to layer lots of complex audio , - mimicking- scenarios that may occur within a video gaming context . <p> You may have noticed a slight delay in this second video between the tap and the actual sounds . @ @ @ @ @ @ @ @ @ @ events in the first example , and just using a normal &lt;a href= " javascript:foo() " &gt; link in the second . - There is always a delay for " normal " links in all multi-touch devices/environments because there has to be time for the device to detect a gesture event . You can bypass this delay in mobile web browsers by using touch events for all input . <p> Side Note : - I have also noticed that touch events are slightly slower to be recognized on Android devices than iOS. - My assumption is that this is related to specific device capabilities this is more- noticeable- on the Amazon Kindle Fire than the Motorola Atrix. - The delay does not appear to be a delay in the actual audio playback . <h> How it works <p> The native plugins expose a very simple API for hooking into native Audio capabilities . - The basic usage is : <p> Preload the audio asset <p> Play the audio asset <p> When done , unload the audio asset to conserve resources <p> The basic components of a PhoneGap native plugin are @ @ @ @ @ @ @ @ @ @ <p> You can learn more about getting started with native plugins on the PhoneGap wiki . <p> Let 's start by examining the native plugins JavaScript API. - You can see that it just hands off the JavaScript calls to the native layer via PhoneGap : <p> Next , let 's examine some intricacies of the plugin - One thing to keep in mind is that I do not have callbacks to the phonegap app once a media asset is loaded . - If you need " loaded " callbacks , you will need to add those yourself . <p> The preloadFX function loads an audio file into memory . - These are lower-level audio methods and have minimal overhead . These assets should be short ( less than 5 seconds ) . - These assets are fully concurrent and polyphonic . <p> On Android , assets that are loaded using preloadFX are managed/played using the Android SoundPool class . Sound files longer than 5 seconds may have errors including ( not playing , clipped content , not looping ) all will fail silently on the device ( debug output will be @ @ @ @ @ @ @ @ @ @ , assets that are loaded using preloadFX are managed/played using System Sound Services from the AudioToolbox framework . Audio loaded using this function is played using **28;7791;TOOLONG . These assets should be short , and are not intended to be looped or stopped . <p> The preloadAudio function loads an audio file into memory . - These have more overhead than assets laoded via preloadFX , and can be looped/stopped . By default , there is a single " voice " only one instance that will be stopped &amp; restarted when you hit play . If there are multiple voices ( number greater than 0 ) , it will cycle through voices to play overlapping audio . - You must specify multiple voices to have polyphonic audio keep in mind , this takes up more device resources . <p> On Android , assets that are loaded using preloadAudio are managed/played using the Android MediaPlayer . <p> On iOS , assets that are loaded using preloadAudio are managed/played using AVAudioPlayer . <p> Loops an audio asset infinitely . - On iOS , this only works for assets loaded via preloadAudio. - @ @ @ @ @ @ @ @ @ @ it is- recommended- to keep usage consistent between platforms . <p> Recently Ive been spending a fair amount of time working on HTML-based applications both mobile web and mobile applications using PhoneGap. - Regardless of whether you are targeting a mobile web browser or a mobile app using the PhoneGap container , you are still targeting a mobile web browser instance . - If you have n't noticed , mobile web browsers can often have- peculiarities- with how content is rendered , or how you interact with that content . - This happens regardless of platform iOS , Android , BlackBerry , etc - All have quirks. - Here are a few tips that I have found useful for improving overall interaction and mobile HTML- experiences . <p> Disclaimer : I 've been targeting iOS and Android primarily , with BlackBerry support on some applications . - I do n't  have a Windows Phone device to test with , so I cant comment on support for the Windows platform . <h> AutoCorrect and AutoCapitalize <p> First things first : autocorrect and autocapitalize on Apples iOS can sometimes drive you to the brink @ @ @ @ @ @ @ @ @ @ you have a text input where you are typing in a username , and it keeps " correcting " it for you ( next thing you know , you are locked out of the app ) . - You can disable these features in web experiences by setting the " autocorrect " and " autocapitalize " attributes of an &lt;input&gt; instance . <p> Disabled AutoCorrect : <p> &lt;input type= " text " autocorrect= " off " autocapitalize= " on " /&gt; <p> Disabled AutoCapitalize : <p> &lt;input type= " text " autocorrect= " on " autocapitalize= " off " /&gt; <h> Managing the Keyboard <p> Have you ever experienced an an app or web site on a mobile device where you have to enter numeric data , and the default keyboard pops up . Before entering any text , you have you switch to the numeric input . Repeat that for 100 form inputs , and try to tell me that you are n't  frustrated Luckily , you can manage the keyboard in mobile HTML experiences very easily using HTML5 Form elements . <h> Disable User Selection <p> One way @ @ @ @ @ @ @ @ @ @ that everything on the UI is selectable and can be copied/pasted Every single piece of text , every image , every link , etc Not only is this annoying in some scenarios ( and very useful in others ) , but there may be instances where you explicitly do n't  want the user to be able to easily copy/paste content . You can disable user selection by applying the following CSS styles . Note : This works on iOS , and partially works on BlackBerry/QNX for the PlayBook . It did not work on Android in my testing . <p> The -webkit-touch-callout css rule disables the callout , and the -webkit-user-select rule disables the ability to select content within an element . More details on webkit css rules from the Mobile Safari CSS Reference . More detail about disabling copy/paste on iOS is available at StackOverflow.com . <h> Disable Zoom <p> If you want your content to feel like an app instead of a web page , then I strongly suggest that you disable gestures for pinch/zoom and panning for all use cases where pinch/zoom is not required . The @ @ @ @ @ @ @ @ @ @ size to device-width and and disable user scaling through the HTML metadata tag . <p> This technique works on both Android and iOS devices , and I assume other platforms . However , I do n't  have the devices to test all of them . <h> Touch Based Scrolling <p> Touch-based scrolling is critical to having an application that feels native . I do n't  mean that the whole page should be able to scroll Your browser will be able to take care of that alone . Instead I mean that you should be able to scroll individual elements so that they mimic clipped views , lists , or large blocks of content . You should be able to scroll content where it is , and not have to scroll an entire page to reveal something in only one area of the screen . You should minimize scrolling when it may cause poor UX scenarios . This is especially the case in tablet-based applications which have a larger UI than phone-based applications . <h> Remove " click " Delays <p> " Click " events on HTML elements on mobile devices @ @ @ @ @ @ @ @ @ @ system logic used to capture gestural input based on touch events . Depending on the device , this could be 300-500 MS . While this does n't  sound like much , it is very noticeable . The workaround is to use touch events instead of mouse events : touchStart , touchMove , touchEnd . You can learn more about touch events from html5rocks.com . There 's also a great script from cubiq that adds touch events for you to optimize the experience for onClick event handlers on iOS devices . <h> Add To Home Screen <p> If you want your web app to fee like a real app and take up the full screen without using PhoneGap as an application container , then you can always add it to the devices home screen . Although this can only be done manually through the mobile browser , there are a few open source scripts to guide the user through this processs : cubiq.org or mobile-bookmark-bubble should get you started . <h> Use Hardware Acceleration <p> Animations will generally be smoother and faster if your content is hardware accelerated ( and the device @ @ @ @ @ @ @ @ @ @ hardware accelerated just by adding the translate3d ( x , y , z ) css style to the element ( be sure to set all three x , y , and z attributes otherwise hardware acceleration may not be applied . If you do n't  want any translation changes , you can use the translate3d CSS rule with all zero values : translate3d(0,0,0) . <p> transform : translate3d(0,0,0) ; -webkit-transform : translate3d(0,0,0) ; <p> In your development/testing , you can even visualize which content is hardware accelerated in both desktop and mobile Safari using the technique shown at http : //mir.aculo.us/ . <h> Make You Apps Fast <p> Last , but certainly not least , make your apps fast . Follow best practices , and be efficient in code execution and the loading of assets ( both local and remote ) . Here are a few links to get you going in the right direction : <p> Did you know that apps built on top of iOS can have a multi-screen workflow ? For example in Keynote , you can have an external screen show a presentation while you control @ @ @ @ @ @ @ @ @ @ app , you can view the audio player on an external screen , and in- Real Racing HD , you can view the game on an external screen while the iOS device becomes your controller . ( among others ) <p> Real Racing HD <p> This is all made possible by the UIWindow and UIScreen APIs in iOS . Even better , on the iPad 2 and iPhone 4Gs , this can be done wirelessly using Airplay with an Apple TV device . On other iOS devices , you can have a second screen using a VGA output . <p> One of the benefits of using a cross platform solution like PhoneGap or Flex/Air is that you can build apps with an easier to use/more familiar paradigm . - However , cross platform runtimes do n't  always offer access to every API feature that native development enables . <p> Out of the box , PhoneGap apps are confined to a single screen . - You can use screen mirroring to mirror content on an external screen , but you cant have a second screen experience . - Its a good @ @ @ @ @ @ @ @ @ @ within your applications . <h> ExternalScreen Native Plugin For PhoneGap <p> I recently did exactly that I created a PhoneGap native plugin that enables second screen capability for PhoneGap applications . - The plugin listens for external screen connection notifications , and if an additional screen is available , it creates a new UIWebView for HTML-based content in the external screen complete with functions for injecting HTML , JavaScript , or URL locations . <h> Why ? <p> You might be wondering " Why ? " you would want this plugin within PhoneGap - this plugin enables the multi-screen experiences described in the apps mentioned above . - They extend the interactions and capabilities of the mobile hardware . - With this PhoneGap native plugin , you can create rich multi-screen experiences with the ease of HTML and JavaScript. - Here are a few ideas of the types of apps that you can build with this approach ( scroll down for source code ) : <h> Fleet Manager <p> Let 's first consider a simple Fleet Manager application which allows you monitor vehicles in a mobile app. - This is a @ @ @ @ @ @ @ @ @ @ The basic functionality allows you to see information on the tablet regarding your fleet . - What if this app connected to a larger screen and was able to display information about your vehicles for everyone to see ? - Watch the video below to see this in real life . <p> This application example is powered by Google Maps , and all of the data is randomly generated on the client . <h> Law Enforcement <p> Let 's next consider a mobile law enforcement application application which gives you details to aid in investigations and- apprehension- of criminals . - Let 's pretend that you are a detective who is searching for a fugitive , and you walk into a crowded bar near the last known location of that fugitive . - You connect to the bars Apple TV on their big screen TV , pull up images and videos of the suspect , then say " Have you seen this person ? " . - This could be incredibly powerful . - Check out- the video below to see a prototype in real life . <p> This law enforcement demo @ @ @ @ @ @ @ @ @ @ wanted RSS data feeds . <h> Tip Of The Iceberg <p> There are lots of use cases where a second screen experience could be beneficial and create a superior product or application . - Using PhoneGap allows you to build those apps faster &amp; with the ease of HTML and JavaScript , using traditional web development paradigms . <p> The PhoneGap native plugin is written in Objective C , with a JavaScript interface to integrate with the client application . PhoneGap plugins are actually very easy to develop . - Basically , you have to write the native code class , write a corresponding JS interface , and add a mapping in your PhoneGap.plist file to expose the new functionality through PhoneGap. - There is a great reference on the PhoneGap wiki for native plugins- which includes architecture &amp; structure , as well as platform specific authoring and installation of those plugins. - - Here are quick links to the iOS-specific native plugin content authoring and installation . <p> The ExternalScreen plugin creates a UIWebView for the the external screen , and exposes methods for interacting with the UIWebView. - @ @ @ @ @ @ @ @ @ @ does not have support for all PhoneGap libraries just a standard HTML container . <p> You can read up on multi-screen programming at iOS from these useful tutorials : <h> PGExternalScreen.h <p> The header file shows the method signatures for the native functionality . - The corresponding PGExternalScreen.m contains all of the actual code to make it all work . - Note : If you are using ARC ( Automatic Reference Counting ) , you will need to remove the retain/release calls in PGExternalScreen.m. 
@@106848935 @2248935/ <h> US Census Browser v2.0 <p> I am happy to announce the US Census Browser version 2.0 ! - Back in December of 2011 , I released the US Census Browser- as an open source- application intended to demonstrate principles for enterprise-class data visualization and applications developed with web standards . - This version has some fairly substantial changes See the video below to check out features in the latest version : <p> Version 2.0 of the US Census Browser has some substantial changes , including : <p> Completely new &amp; redesigned UI layer , using app-UI. - app-UI is an open source framework for application view-navigators that mimic native mobile applications . - Using the app-UI SplitViewNavigator , the US Census Browser now supports both landscape and portrait orientations . <p> Switched from Google Maps to Open Street Map using OpenLayers. - Users of the Census Browser maxed out my Google Maps account ! - That is 25,000 map loads within a 24 hour period ! WOW ! I switched to the free Open Street Maps solution , which does n't  have any usage/bandwidth limitations . - With @ @ @ @ @ @ @ @ @ @ . <p> Updated to Twitter Bootstrap 2.0. - The app is now using new UI styles and components which are now available in Twitter Bootstrap version 2.0 
@@106848936 @2248936/ <h> All posts by Andrew <p> Well , hello again everyone ! Its been a while since I 've been active on this blog ( tricedesigns.com ) . This is the first post at this location in nearly 2 years . As I was writing for insideria.com this blog began to suffer . If you 're wondering where the old content has gone , I 've decided to take it down . - All of my content which was formerly at insideria.com is now available at developria.com . <p> I migrated this site from the old Blogger hosting since they decided to no longer support FTP publication , and a lot of the content that I had here was from 2006-2009 , so a fair amount of it was outdated . If there was something that you really miss , please let me know , and I will dig it up , update and re-post . <p> Expect to see this blog become a lot more active , with some significant updates in the near future . I have some exciting announcements coming soon , and you can expect to find a @ @ @ @ @ @ @ @ @ @ and their applicability in the enterprise . 
@@106848937 @2248937/ <h> Category Archives : Adobe <p> Last week Adobe announced information about the companys evolution and future plans of Flex . It was also announced that Adobe Flex would be contributed to an open source software foundation . The result of which , was mass speculation , fear , uncertainty , and doubt . - Rest- assured , Flash is not dead , nor is Flex . <p> Falcon , the next-generation MXML and ActionScript compiler that is currently under development ( this will be contributed when complete in 2012 ) <p> Falcon JS , an experimental cross-compiler from MXML and ActionScript to HTML and JavaScript . <p> Flex testing tools , as used previously by Adobe , so as to ensure successful continued development of Flex with high quality <p> Is n't Adobe just abandoning Flex SDK and putting it out to Apache to die ? - <p> Absolutely not " we are incredibly proud of what we 've achieved with Flex and know that it will continue to provide significant value for many years to come . We expect active and on-going contributions from the Apache community @ @ @ @ @ @ @ @ @ @ to the projects and we are working with the Flex community to make them contributors as well . <p> Flex has been open source since the release of Flex 3 SDK . What 's so different about what you are announcing now ? <p> Since Flex 3 , customers have primarily used the Flex source code to debug underlying issues in the Flex framework , rather than to actively develop new features or fix bugs and contribute them back to the SDK . <p> With Friday 's announcement , Adobe will no longer be the owner of the ongoing roadmap . Instead , the project will be in Apache and governed according to its well-established community rules.In this model , Apache community members will provide project leadership . We expect project management to include both Adobe engineers as well as key community leaders . Together , they will jointly operate in a meritocracy to define new features and enhancements for future versions of the Flex SDK . The Apache model has proven to foster a vibrant community , drive development forward , and allow for continuous commits from active developers @ @ @ @ @ @ @ @ @ @ Flex applications continuing to run on Flash Player and Adobe AIR ? <p> Adobe will continue to support applications built with Flex , as well as all future versions of the SDK running in PC browsers with Adobe Flash Player and as mobile apps with Adobe AIR indefinitely on Apple iOS , Google Android and RIM BlackBerry Tablet OS . <h> Adobe AIR <p> We are continuing to develop Adobe AIR for both the desktop and mobile devices . Indeed , we have seen wide adoption of Adobe AIR for creating mobile applications and there have been a number of blockbuster mobile applications created using Adobe AIR . <h> Flash Player for Desktop Browsers <p> We feel that Flash continues to play a vital role of enabling features and functionality on the web that are not otherwise possible . As such , we have a long term commitment to the Flash Player on desktops , and are actively working on the next Flash Player version . <p> Hi Everyone ! Here are a few events that I 'll be speaking at/attending in the remainder of 2011 . Come check out Adobes @ @ @ @ @ @ @ @ @ @ for me . I hope to see you at any one of these events ! <h> MoDevDC Meetup <p> Tonight ! 11/02/2011 This months MoDevDC ( Mobile Developers DC ) tech meetup is all about cross-platform development tools . Stop by to see a high level overview of Adobes cross-platform mobile development offerings , including AIR and PhoneGap . This will include the basics of " what are these tools " , as well as some demo LONG ... <h> DC Droids Meetup <p> 11/15/2011 Here is a chance for you to learn about another method to develop Android applications ( and make them compatible across platforms ) from an expert from Adobe . You will also have the chance to win a copy of Flash Builder 4.5 . I will walk through the processes of building Android &amp; cross-platform applications using both Adobe AIR and PhoneGap ( a cross-platform mobile application development framework ) . This session will cover demo applications , as well as real-world coding and best LONG ... <h> MoDevEast Conference &amp; Hackathon <p> 12/02/2011 12/03/2011 MoDevEast is where mobile developers and marketers gain the upper @ @ @ @ @ @ @ @ @ @ , targeting phones or tablets , staying ahead is paramount in this fast moving industry . MoDevEast 2011 will offer five tracks that hone development skills and sharpen mobile business strategy . Ill be speaking on cross-platform mobile development , and I 'll definitely be there for the event and hackathon ! http : //www.modeveast.com/ <p> Have you noticed when using twitter , google plus , or certain areas of facebook that when you scroll the page , it automatically loads more data ? - You do n't  have to continually hit " next " to go through page after page of data . Instead , the content just " appears " as you need it . In this post we will explore a technique for making Flex list components behave in this exact way . As you scroll through the list , it continually requests more data from the server . Take a look at the video preview below , and afterwards well explore the code . <p> The basic workflow is that you need to detect when you 've scrolled to the bottom of the list , then load additional @ @ @ @ @ @ @ @ @ @ you know how many records are currently in the list , you always know which " page " you are viewing . When you scroll down again , just request the next set of results that are subsequent to the last results that you requested . Each time you request data , append the list items to the data provider of the list . <p> First things first , you need to detect when you 've scrolled to the bottom of the list . here 's a great example showing how to detect when you have scrolled to the bottom of the list . You can just add an event listener to the lists scroller viewport . Once you have a vertical scroll event where the new value is equal to the viewport max height minus the item renderer height , then you have scrolled to the end . At this point , request more data from the server . <p> One other trick that I am using here is that I am using conditional item renderers based upon the type of object being displayed . I have a dummy " LoadingVO @ @ @ @ @ @ @ @ @ @ the list data provider . The item renderer function for the list will return a LoadingItemRenderer instance if the data passed to it is a LoadingVO . <p> You may have noticed in the fetchNextPage() function that the dataProvider is referenced as an InfiniteListModel class let 's examine this class next . The InfiniteListModel class is simply an ArrayCollection which gets populated by the getNextPage() function . Inside of the getNextPage() function , it calls a remote service which returns data to the client , based on the current " page " . In the result handler , you can see that I disable binding events using disableAutoUpdate() , remove the dummy LoadingVO , append the service results to the collection , add a new LoadingVO , and then re-enable binding events using enableAutoUpdate() . Also , notice that I have a boolean loading value that is true while requesting data from the server . This boolean flag is used to prevent multiple service calls for the same data . <p> Now , let 's take a look at the root view that puts everything together . There is an InfiniteScrollList whose @ @ @ @ @ @ @ @ @ @ a RemoteObject instance , which loads data from a remote server . <p> Perhaps you have heard of the topic " cross platform development " , but are n't  really sure what it is , or you are n't  sure why you would want to use cross-platform technologies . If this is the case , then this post is especially for you . Ill try to she 'd some light onto what it is , and why you would want to use cross-platform development strategies . <h> What is cross-platform development ? <p> Cross platform development is a concept in computer software development where you write application code once , and it runs on multiple platforms . This is very much inline with the " write once , run everywhere " concept pioneered in the 90s , and brought to a mainstream reality with Flash in the browser , and AIR on the desktop . The standard evolution of technology has been to make everything faster , smaller , and more portable , and it is only natural that that this concept has now come into the mobile development world . @ @ @ @ @ @ @ @ @ @ application using a codebase &amp; technology that allows the application to be deployed and distributed across multiple disparate platforms/operating systems/devices . <p> In case you 're wondering why I offered 2 cross platform technologies , that is because Adobe will soon have 2 cross-platform product offerings . - Adobe has entered an- agreement to purchase Nitobi , the creators of PhoneGap . <h> Adobe AIR <p> Adobe AIR is a cross-platform technology with roots in the Flash Player and the AIR desktop runtime. - AIR allows you to build cross-platform mobile applications using ActionScript and the open source Flex framework . - AIR apps can be built from the Flash Professional timeline-based design/animation tool , Flash Builder ( an Eclipse-based development environment ) , or other open source solutions using the freely available AIR SDK. - Applications developed with Adobe AIR can target desktop platforms ( Mac , &amp; Windows ) , smart phone and tablet platforms ( iOS , Android , BlackBerry , soon Windows ) , and even smart televisions . <h> PhoneGap ( Apache Callback ) <p> PhoneGap is an open source cross platform technology with roots in @ @ @ @ @ @ @ @ @ @ of a web view that consumes 100% of the available width &amp; 100% of the available height , taking advantage of web browsers on each platform . - PhoneGap offers a JavaScript to native bridge that enables you to build natively-installed applications using HTML and JavaScript , using the native bridge to interact with the device hardware/APIs. - Note : PhoneGap is also being submitted to the Apache Foundation as the Apache Callback project . <h> More Devices , Less Code <p> The driving factor behind cross-platform technologies is that you will be able to use those technologies to target more devices &amp; platforms , with writing a minimal amount of source code . - There are many advantages with this approach . - Here are a few of the major reasons <h> Lower Barrier of Entry <p> Generally speaking , development with HTML &amp; JavaScript or Flex &amp; ActionScript is easier than developing with Objective-C or Java . - Due to the ease of use of the development tooling and familiarity of the languages , cross platform technologies lower the technical barriers which may have prevented adoption of native @ @ @ @ @ @ @ @ @ @ applications that they may not previously have been able to , and also enables your team to focus on what matters the application ; not the skills required to develop on multiple disparate platforms . <h> Reduce the Number of Required Skills for the Development Team <p> Native development on multiple platforms requires your development team to learn Objective C for iOS applications , Java for Android applications , Silverlight for Windows Phone applications , etc - Finding all of these skills in a single developer is nearly impossible . - Using cross-platform development technologies , your team only needs to be proficient with one language/skillset. - Knowledge of the native development paradigms and languages are always a plus , but are no longer a requirement . - - Many developers transitioning from web development already know either Flex/ActionScript and/or HTML/JavaScript , and making the transition from web to mobile development will not be a major undertaking . <h> Reduced Development &amp; Long Term Maintenance Costs <p> Cross-platform mobile applications can originate from a single codebase , which requires a single development skillset. - You do n't  need to have @ @ @ @ @ @ @ @ @ @ working on the shared codebase can cover all target platforms . - Having a single codebase also reduces long term maintenance costs . - You no longer need to have bug tracking for X number of codebases , and do not need to maintain a larger staff to support each platform . - Did I also mention that you have one codebase to maintain ? <p> Having a single codebase does n't  reduce the need for QA/testing on each target platform nothing can get rid of this . - It is absolutely imperative that you test your codebase on physical devices for all platforms that you intend to support . - Emulators and Simulators can go a long way during development , but they will undoubtedly not cover all scenarios possible on a physical device , and they will not have the same runtime performance as a physical device . <h> Play the Strengths of a Technology <p> Some technologies make tasks easier than others . - For example , programmatic drawing and data visualization are very easy using Flex &amp; ActionScript. - Developing equivalent experiences in native code can @ @ @ @ @ @ @ @ @ @ the the features of the language to their fullest potential , to your advantage- that 's why they exist . <p> BlackBerry DevCon 2011 kicked off earlier this week , and surrounding it were some exciting announcements around Adobe tools and BlackBerry platforms . These announcements include Flash Player 11 and AIR 3 features available on the PlayBook including Stage3D ( among many other great features ) . Also announced was AIR support for the new BlackBerry BBX operating system , as well as PhoneGap support for BBX/QNX . BBX is the new QNX based operating system for BlackBerry smartphones . <p> Adobe 's VP and General Manager of Interactive Solutions , Danny Winokur , joined RIM 's Alec Saunders , VP of Developer Relations and Ecosystems Development , on stage at BlackBerry DevCon Americas 2011 . Danny spoke about the exciting possibilities that Flash and HTML5 bring to the web and mobile app development " specifically for the BlackBerry PlayBook and BBX in the future . ( read more here , or check out the video below ) 
@@106848939 @2248939/ <h> Category Archives : JavaScript <p> here 's a silly/fun app I built after hours using PhoneGap. - It is a childrens drawing app built entirely with the HTML5 Canvas element , using a PhoneGap wrapper , targeting the iPad. - I was inspired by magnetic drawing toys that I often use when drawing with my daughter , and this was really , really easy and a lot of fun to build . - I used the exact HTML5 Canvas brush image/sketching technique that I have previously demonstrated the only change is that I added the new UI style elements and added support for multiple touch points . Otherwise , the drawing logic is identical . <p> Lil Doodle is a great new iPad application for entertaining both you and your children ! If you know how to use a childrens magnetic drawing toy , then you know how to use Lil Doodle . Pick a " pen " shape , and start doodling . Your imagination is your only limit . If you want to erase everything and start over , just use the slider at the bottom . @ @ @ @ @ @ @ @ @ @ inside of PhoneGap has great performance on iOS , and building the application using purely HTML , CSS , and JavaScript made it incredibly simple . After I wrote the core drawing engine for a previous- blog post , I whipped up the UI in one evening , and then started user testing with my little beta tester . - She found some issues that I had overlooked , and a few days later I submitted it to the app store . <p> and yes , she really does play with it : <p> The app is currently available for iPad devices on iTunes I 'm about to start researching/testing performance on other platforms , so maybe soon it will be out in other ecosystems well see . - You can get it now at : <p> The world is changing and oh my , it is changing fast . - In the not-too-distant future , many capabilities that were exclusive to plugin-based content will be accessible to the HTML/JavaScript world without any plugin dependencies. - This includes access to media devices ( microphone and camera ) , as well as @ @ @ @ @ @ @ @ @ @ thinking " no way , that is still years off " , but its not . <p> Just last night I was looking at the new webRTC capabilities that were introduced in the Google Chrome Canary build in January , and I was experimenting with the new getUserMedia API. - WebRTC is an open source realtime communications API that was recently included in Chrome ( Canary , the latest dev build ) , the latest version of Opera , and soon FireFox ( if not already ) , and is built on top of the getUserMedia APIs . Device access &amp; user media APIs are n't  commonly available in most users browsers yet , but you can be sure that they will be commonplace in the not-so-distant future . <p> Below you 'll see a screenshot of a simple example demonstrating camera access . <p> The beauty of this example is that the entire experience is delivered in a whopping total of 17 lines of code . - It uses the webkitGetUserMedia API to grab a media stream from the local webcam and display it within a HTML5 &lt;video&gt; element . @ @ @ @ @ @ @ @ @ @ a foundational building block for more complicated operations , including realtime video enhancement and **35;6050;TOOLONG - - Check out this more advanced example from- http : **25;7821;TOOLONG , which applies effects to the camera stream in real time : <p> If you want to read more about some of the new " Bleeding Edge " features coming to the web , check out this slide deck by Googles Paul Kinlan. - You can also read more about the getUserMedia API from Operas developer site . <p> If you have ever tried to develop any kind of application using HTML5 audio that is widely supported , then you have likely pulled all the hair from your head . In its current state , HTML5 Audio is wrought with issues lack of consistent codec support across browsers &amp; operating systems , no polyphony ( a single audio clip can not be played on top of itself ) , and lack of concurrency ( on some of the leading mobile browsers you can only play one audio file at a time , if at all ) . Even the leading HTML5 games for @ @ @ @ @ @ @ @ @ @ use Flash ) . Do n't  believe me ? Just take a look at Angry Birds , Cut the Rope , or Bejeweled in a proxy/resource monitor <h> The Problem <p> HTML5 audio is not *yet* ready for prime-time . There are some great libraries like SoundManager , which can help you try to use HTML5 audio with a failover to Flash , but you are still limited without polyphony or concurrency . In desktop browsers , Flash fixes these issues , and Flash is still vastly superior to HTML5 for audio programming . <p> If you are building mobile applications , you can have great audio capabilities by developing apps with AIR . However , what if you are n't  using AIR ? In native applications , you can access the underlying audio APIs and have complete control . <p> If you are developing mobile applications with PhoneGap , you can use the Media class , which works great . If you want polyphony , then you will have to do some work managing audio files for yourself , which can get tricky . You can also write native @ @ @ @ @ @ @ @ @ @ operating systems , which is what i will be covering in this post . <p> Before continuing further , let 's take a minute to understand what I am talking about when I refer to concurrency , polyphony , and low-latency <h> Concurrency <p> Concurrency in audio programming refers to the ability to play multiple audio resources simultaneously . - HTML5 in most mobile devices does not support this not in iOS , not in Android . - In fact , HTML5 Audio does not work *at all* in Android 2. x and earlier . - Native APIs do support this , and so does PhoneGaps Media class , which is based on Android MediaPlayer and iOS- AVAudioPlayer . <h> Polyphony <p> In this case , polyphony is the production of multiple sounds simultaneously ( I 'm not referring to the concept of polyphany in music theory ) . In describing concurrency , I refered to the ability to play 2 separate sounds at the same time , where with polyphony I refer to the ability to play the same sound " on top " of itself . There can be multiple @ @ @ @ @ @ @ @ @ @ most literal of definitions concurrency could be considered a part of polyphony , and polyphony a part of concurrency Hopefully you get what I 'm trying to say. - In its current state , HTML5 audio supports neither concurrency or polyphony. - The PhoneGap Media class does not support polyphony , however you can probably manage multiple media instances via javascript to achieve polyphonic behavior this requires additional work in the JavaScript side of things to juggle resources . <h> Low Latency <p> Low latency refers to " human-unnoticeable- delays- between an- input- being processed and the corresponding- output- providing real time characteristics " according to wikipedia. - In this case , I refer to low latency audio , meaning that there is an- imperceptible- delay between when a sound is triggered , and when it actually plays . - This means that sounds will play when expected , not after a wait . - This means a bouncing ball sound should be heard as you see the ball bouncing on the screen . - Not after it has already bounced . <p> In HTML5 , you can auto-load a sound @ @ @ @ @ @ @ @ @ @ but do n't  expect to play more than one at a time . - With the PhoneGap Media class , the audio file is n't actually requested until you invoke " play " . - This occurs inside " startPlaying " on Android , and " play " on iOS. - What I wanted was a way to preload the audio so that it is immediately ready for use at the time it is needed . <h> The Solution <p> PhoneGap makes it really easy to build natively installed applications using a familiar paradim : HTML &amp; JavaScript. - Luckily , PhoneGap also allows you to tie into native code using the native plugin model . - This enables you to write your own native code and expose that code to your PhoneGap application via a JavaScript interface and that is exactly what I did to enable low-latency , concurrent , and polyphonic audio in a PhoneGap experience . <p> I created PhoneGap native plugins for Android and iOS that allow you to preload audio , and playback that audio quickly , with a very simple to use API. - I 'll @ @ @ @ @ @ @ @ @ @ , but you can get a pretty good idea of what I mean by viewing the following two videos . <p> The first is a basic " Drum Machine " . - You just tap the pads to play an audio sample . <p> The second is a simple user interface that allows you to layer lots of complex audio , - mimicking- scenarios that may occur within a video gaming context . <p> You may have noticed a slight delay in this second video between the tap and the actual sounds . - This is because I am using " touchStart " events in the first example , and just using a normal &lt;a href= " javascript:foo() " &gt; link in the second . - There is always a delay for " normal " links in all multi-touch devices/environments because there has to be time for the device to detect a gesture event . You can bypass this delay in mobile web browsers by using touch events for all input . <p> Side Note : - I have also noticed that touch events are slightly slower to be recognized @ @ @ @ @ @ @ @ @ @ this is related to specific device capabilities this is more- noticeable- on the Amazon Kindle Fire than the Motorola Atrix. - The delay does not appear to be a delay in the actual audio playback . <h> How it works <p> The native plugins expose a very simple API for hooking into native Audio capabilities . - The basic usage is : <p> Preload the audio asset <p> Play the audio asset <p> When done , unload the audio asset to conserve resources <p> The basic components of a PhoneGap native plugin are : <p> A JavaScript interface <p> Corresponding Native Code classes <p> You can learn more about getting started with native plugins on the PhoneGap wiki . <p> Let 's start by examining the native plugins JavaScript API. - You can see that it just hands off the JavaScript calls to the native layer via PhoneGap : <p> Next , let 's examine some intricacies of the plugin - One thing to keep in mind is that I do not have callbacks to the phonegap app once a media asset is loaded . - If you need " loaded " @ @ @ @ @ @ @ @ @ @ <p> The preloadFX function loads an audio file into memory . - These are lower-level audio methods and have minimal overhead . These assets should be short ( less than 5 seconds ) . - These assets are fully concurrent and polyphonic . <p> On Android , assets that are loaded using preloadFX are managed/played using the Android SoundPool class . Sound files longer than 5 seconds may have errors including ( not playing , clipped content , not looping ) all will fail silently on the device ( debug output will be visible if connected to debugger ) . <p> On iOS , assets that are loaded using preloadFX are managed/played using System Sound Services from the AudioToolbox framework . Audio loaded using this function is played using **28;7848;TOOLONG . These assets should be short , and are not intended to be looped or stopped . <p> The preloadAudio function loads an audio file into memory . - These have more overhead than assets laoded via preloadFX , and can be looped/stopped . By default , there is a single " voice " only one instance that will be @ @ @ @ @ @ @ @ @ @ are multiple voices ( number greater than 0 ) , it will cycle through voices to play overlapping audio . - You must specify multiple voices to have polyphonic audio keep in mind , this takes up more device resources . <p> On Android , assets that are loaded using preloadAudio are managed/played using the Android MediaPlayer . <p> On iOS , assets that are loaded using preloadAudio are managed/played using AVAudioPlayer . <p> Loops an audio asset infinitely . - On iOS , this only works for assets loaded via preloadAudio. - This works for all asset types for Android , however it is- recommended- to keep usage consistent between platforms . <p> Recently Ive been spending a fair amount of time working on HTML-based applications both mobile web and mobile applications using PhoneGap. - Regardless of whether you are targeting a mobile web browser or a mobile app using the PhoneGap container , you are still targeting a mobile web browser instance . - If you have n't noticed , mobile web browsers can often have- peculiarities- with how content is rendered , or how you interact with that @ @ @ @ @ @ @ @ @ @ Android , BlackBerry , etc - All have quirks. - Here are a few tips that I have found useful for improving overall interaction and mobile HTML- experiences . <p> Disclaimer : I 've been targeting iOS and Android primarily , with BlackBerry support on some applications . - I do n't  have a Windows Phone device to test with , so I cant comment on support for the Windows platform . <h> AutoCorrect and AutoCapitalize <p> First things first : autocorrect and autocapitalize on Apples iOS can sometimes drive you to the brink of insanity . - This is especially the case if you have a text input where you are typing in a username , and it keeps " correcting " it for you ( next thing you know , you are locked out of the app ) . - You can disable these features in web experiences by setting the " autocorrect " and " autocapitalize " attributes of an &lt;input&gt; instance . <p> Disabled AutoCorrect : <p> &lt;input type= " text " autocorrect= " off " autocapitalize= " on " /&gt; <p> Disabled AutoCapitalize : <p> &lt;input @ @ @ @ @ @ @ @ @ @ off " /&gt; <h> Managing the Keyboard <p> Have you ever experienced an an app or web site on a mobile device where you have to enter numeric data , and the default keyboard pops up . Before entering any text , you have you switch to the numeric input . Repeat that for 100 form inputs , and try to tell me that you are n't  frustrated Luckily , you can manage the keyboard in mobile HTML experiences very easily using HTML5 Form elements . <h> Disable User Selection <p> One way to easily determine that an application is really HTML is that everything on the UI is selectable and can be copied/pasted Every single piece of text , every image , every link , etc Not only is this annoying in some scenarios ( and very useful in others ) , but there may be instances where you explicitly do n't  want the user to be able to easily copy/paste content . You can disable user selection by applying the following CSS styles . Note : This works on iOS , and partially works on BlackBerry/QNX for the @ @ @ @ @ @ @ @ @ @ testing . <p> The -webkit-touch-callout css rule disables the callout , and the -webkit-user-select rule disables the ability to select content within an element . More details on webkit css rules from the Mobile Safari CSS Reference . More detail about disabling copy/paste on iOS is available at StackOverflow.com . <h> Disable Zoom <p> If you want your content to feel like an app instead of a web page , then I strongly suggest that you disable gestures for pinch/zoom and panning for all use cases where pinch/zoom is not required . The easiest way to do this is to set the viewport size to device-width and and disable user scaling through the HTML metadata tag . <p> This technique works on both Android and iOS devices , and I assume other platforms . However , I do n't  have the devices to test all of them . <h> Touch Based Scrolling <p> Touch-based scrolling is critical to having an application that feels native . I do n't  mean that the whole page should be able to scroll Your browser will be able to take care of that alone . @ @ @ @ @ @ @ @ @ @ individual elements so that they mimic clipped views , lists , or large blocks of content . You should be able to scroll content where it is , and not have to scroll an entire page to reveal something in only one area of the screen . You should minimize scrolling when it may cause poor UX scenarios . This is especially the case in tablet-based applications which have a larger UI than phone-based applications . <h> Remove " click " Delays <p> " Click " events on HTML elements on mobile devices generally have a delay that is caused by the operating system logic used to capture gestural input based on touch events . Depending on the device , this could be 300-500 MS . While this does n't  sound like much , it is very noticeable . The workaround is to use touch events instead of mouse events : touchStart , touchMove , touchEnd . You can learn more about touch events from html5rocks.com . There 's also a great script from cubiq that adds touch events for you to optimize the experience for onClick event handlers on iOS @ @ @ @ @ @ @ @ @ @ want your web app to fee like a real app and take up the full screen without using PhoneGap as an application container , then you can always add it to the devices home screen . Although this can only be done manually through the mobile browser , there are a few open source scripts to guide the user through this processs : cubiq.org or mobile-bookmark-bubble should get you started . <h> Use Hardware Acceleration <p> Animations will generally be smoother and faster if your content is hardware accelerated ( and the device supports hardware acceleration ) . You can make html elements hardware accelerated just by adding the translate3d ( x , y , z ) css style to the element ( be sure to set all three x , y , and z attributes otherwise hardware acceleration may not be applied . If you do n't  want any translation changes , you can use the translate3d CSS rule with all zero values : translate3d(0,0,0) . <p> transform : translate3d(0,0,0) ; -webkit-transform : translate3d(0,0,0) ; <p> In your development/testing , you can even visualize which content is hardware accelerated @ @ @ @ @ @ @ @ @ @ at http : //mir.aculo.us/ . <h> Make You Apps Fast <p> Last , but certainly not least , make your apps fast . Follow best practices , and be efficient in code execution and the loading of assets ( both local and remote ) . Here are a few links to get you going in the right direction : <p> Did you know that apps built on top of iOS can have a multi-screen workflow ? For example in Keynote , you can have an external screen show a presentation while you control it on your iOS device . In the- Jimi Hendrix app , you can view the audio player on an external screen , and in- Real Racing HD , you can view the game on an external screen while the iOS device becomes your controller . ( among others ) <p> Real Racing HD <p> This is all made possible by the UIWindow and UIScreen APIs in iOS . Even better , on the iPad 2 and iPhone 4Gs , this can be done wirelessly using Airplay with an Apple TV device . On other iOS devices @ @ @ @ @ @ @ @ @ @ output . <p> One of the benefits of using a cross platform solution like PhoneGap or Flex/Air is that you can build apps with an easier to use/more familiar paradigm . - However , cross platform runtimes do n't  always offer access to every API feature that native development enables . <p> Out of the box , PhoneGap apps are confined to a single screen . - You can use screen mirroring to mirror content on an external screen , but you cant have a second screen experience . - Its a good thing you can write native plugins/extensions to enable native functionality within your applications . <h> ExternalScreen Native Plugin For PhoneGap <p> I recently did exactly that I created a PhoneGap native plugin that enables second screen capability for PhoneGap applications . - The plugin listens for external screen connection notifications , and if an additional screen is available , it creates a new UIWebView for HTML-based content in the external screen complete with functions for injecting HTML , JavaScript , or URL locations . <h> Why ? <p> You might be wondering " Why ? " you @ @ @ @ @ @ @ @ @ @ the multi-screen experiences described in the apps mentioned above . - They extend the interactions and capabilities of the mobile hardware . - With this PhoneGap native plugin , you can create rich multi-screen experiences with the ease of HTML and JavaScript. - Here are a few ideas of the types of apps that you can build with this approach ( scroll down for source code ) : <h> Fleet Manager <p> Let 's first consider a simple Fleet Manager application which allows you monitor vehicles in a mobile app. - This is a similar concept which Ive used in previous examples . - The basic functionality allows you to see information on the tablet regarding your fleet . - What if this app connected to a larger screen and was able to display information about your vehicles for everyone to see ? - Watch the video below to see this in real life . <p> This application example is powered by Google Maps , and all of the data is randomly generated on the client . <h> Law Enforcement <p> Let 's next consider a mobile law enforcement application application which @ @ @ @ @ @ @ @ @ @ criminals . - Let 's pretend that you are a detective who is searching for a fugitive , and you walk into a crowded bar near the last known location of that fugitive . - You connect to the bars Apple TV on their big screen TV , pull up images and videos of the suspect , then say " Have you seen this person ? " . - This could be incredibly powerful . - Check out- the video below to see a prototype in real life . <p> This law enforcement demo scenario is a basic application powered by the FBIs most wanted RSS data feeds . <h> Tip Of The Iceberg <p> There are lots of use cases where a second screen experience could be beneficial and create a superior product or application . - Using PhoneGap allows you to build those apps faster &amp; with the ease of HTML and JavaScript , using traditional web development paradigms . <p> The PhoneGap native plugin is written in Objective C , with a JavaScript interface to integrate with the client application . PhoneGap plugins are actually very easy to @ @ @ @ @ @ @ @ @ @ native code class , write a corresponding JS interface , and add a mapping in your PhoneGap.plist file to expose the new functionality through PhoneGap. - There is a great reference on the PhoneGap wiki for native plugins- which includes architecture &amp; structure , as well as platform specific authoring and installation of those plugins. - - Here are quick links to the iOS-specific native plugin content authoring and installation . <p> The ExternalScreen plugin creates a UIWebView for the the external screen , and exposes methods for interacting with the UIWebView. - Note : This is just a normal UIWebView , it does not have support for all PhoneGap libraries just a standard HTML container . <p> You can read up on multi-screen programming at iOS from these useful tutorials : <h> PGExternalScreen.h <p> The header file shows the method signatures for the native functionality . - The corresponding PGExternalScreen.m contains all of the actual code to make it all work . - Note : If you are using ARC ( Automatic Reference Counting ) , you will need to remove the retain/release calls in PGExternalScreen.m. 
@@106848940 @2248940/ <p> If you have ever tried to develop any kind of application using HTML5 audio that is widely supported , then you have likely pulled all the hair from your head . In its current state , HTML5 Audio is wrought with issues lack of consistent codec support across browsers &amp; operating systems , no polyphony ( a single audio clip can not be played on top of itself ) , and lack of concurrency ( on some of the leading mobile browsers you can only play one audio file at a time , if at all ) . Even the leading HTML5 games for desktop browsers do n't  even use HTML5 audio ( they use Flash ) . Do n't  believe me ? Just take a look at Angry Birds , Cut the Rope , or Bejeweled in a proxy/resource monitor <h> The Problem <p> HTML5 audio is not *yet* ready for prime-time . There are some great libraries like SoundManager , which can help you try to use HTML5 audio with a failover to Flash , but you are still limited without polyphony or concurrency . In @ @ @ @ @ @ @ @ @ @ is still vastly superior to HTML5 for audio programming . <p> If you are building mobile applications , you can have great audio capabilities by developing apps with AIR . However , what if you are n't  using AIR ? In native applications , you can access the underlying audio APIs and have complete control . <p> If you are developing mobile applications with PhoneGap , you can use the Media class , which works great . If you want polyphony , then you will have to do some work managing audio files for yourself , which can get tricky . You can also write native plugins that integrate with the audio APIs for the native operating systems , which is what i will be covering in this post . <p> Before continuing further , let 's take a minute to understand what I am talking about when I refer to concurrency , polyphony , and low-latency <h> Concurrency <p> Concurrency in audio programming refers to the ability to play multiple audio resources simultaneously . - HTML5 in most mobile devices does not support this not in iOS , not in @ @ @ @ @ @ @ @ @ @ work *at all* in Android 2. x and earlier . - Native APIs do support this , and so does PhoneGaps Media class , which is based on Android MediaPlayer and iOS- AVAudioPlayer . <h> Polyphony <p> In this case , polyphony is the production of multiple sounds simultaneously ( I 'm not referring to the concept of polyphany in music theory ) . In describing concurrency , I refered to the ability to play 2 separate sounds at the same time , where with polyphony I refer to the ability to play the same sound " on top " of itself . There can be multiple " voices " of the same sound . In the most literal of definitions concurrency could be considered a part of polyphony , and polyphony a part of concurrency Hopefully you get what I 'm trying to say. - In its current state , HTML5 audio supports neither concurrency or polyphony. - The PhoneGap Media class does not support polyphony , however you can probably manage multiple media instances via javascript to achieve polyphonic behavior this requires additional work in the JavaScript side of things @ @ @ @ @ @ @ @ @ @ refers to " human-unnoticeable- delays- between an- input- being processed and the corresponding- output- providing real time characteristics " according to wikipedia. - In this case , I refer to low latency audio , meaning that there is an- imperceptible- delay between when a sound is triggered , and when it actually plays . - This means that sounds will play when expected , not after a wait . - This means a bouncing ball sound should be heard as you see the ball bouncing on the screen . - Not after it has already bounced . <p> In HTML5 , you can auto-load a sound so that it is ready when you need it , but do n't  expect to play more than one at a time . - With the PhoneGap Media class , the audio file is n't actually requested until you invoke " play " . - This occurs inside " startPlaying " on Android , and " play " on iOS. - What I wanted was a way to preload the audio so that it is immediately ready for use at the time it is needed @ @ @ @ @ @ @ @ @ @ to build natively installed applications using a familiar paradim : HTML &amp; JavaScript. - Luckily , PhoneGap also allows you to tie into native code using the native plugin model . - This enables you to write your own native code and expose that code to your PhoneGap application via a JavaScript interface and that is exactly what I did to enable low-latency , concurrent , and polyphonic audio in a PhoneGap experience . <p> I created PhoneGap native plugins for Android and iOS that allow you to preload audio , and playback that audio quickly , with a very simple to use API. - I 'll get into details how this works further in the post , but you can get a pretty good idea of what I mean by viewing the following two videos . <p> The first is a basic " Drum Machine " . - You just tap the pads to play an audio sample . <p> The second is a simple user interface that allows you to layer lots of complex audio , - mimicking- scenarios that may occur within a video gaming context . <p> @ @ @ @ @ @ @ @ @ @ video between the tap and the actual sounds . - This is because I am using " touchStart " events in the first example , and just using a normal &lt;a href= " javascript:foo() " &gt; link in the second . - There is always a delay for " normal " links in all multi-touch devices/environments because there has to be time for the device to detect a gesture event . You can bypass this delay in mobile web browsers by using touch events for all input . <p> Side Note : - I have also noticed that touch events are slightly slower to be recognized on Android devices than iOS. - My assumption is that this is related to specific device capabilities this is more- noticeable- on the Amazon Kindle Fire than the Motorola Atrix. - The delay does not appear to be a delay in the actual audio playback . <h> How it works <p> The native plugins expose a very simple API for hooking into native Audio capabilities . - The basic usage is : <p> Preload the audio asset <p> Play the audio asset <p> When @ @ @ @ @ @ @ @ @ @ The basic components of a PhoneGap native plugin are : <p> A JavaScript interface <p> Corresponding Native Code classes <p> You can learn more about getting started with native plugins on the PhoneGap wiki . <p> Let 's start by examining the native plugins JavaScript API. - You can see that it just hands off the JavaScript calls to the native layer via PhoneGap : <p> Next , let 's examine some intricacies of the plugin - One thing to keep in mind is that I do not have callbacks to the phonegap app once a media asset is loaded . - If you need " loaded " callbacks , you will need to add those yourself . <p> The preloadFX function loads an audio file into memory . - These are lower-level audio methods and have minimal overhead . These assets should be short ( less than 5 seconds ) . - These assets are fully concurrent and polyphonic . <p> On Android , assets that are loaded using preloadFX are managed/played using the Android SoundPool class . Sound files longer than 5 seconds may have errors including ( not playing @ @ @ @ @ @ @ @ @ @ silently on the device ( debug output will be visible if connected to debugger ) . <p> On iOS , assets that are loaded using preloadFX are managed/played using System Sound Services from the AudioToolbox framework . Audio loaded using this function is played using **28;7878;TOOLONG . These assets should be short , and are not intended to be looped or stopped . <p> The preloadAudio function loads an audio file into memory . - These have more overhead than assets laoded via preloadFX , and can be looped/stopped . By default , there is a single " voice " only one instance that will be stopped &amp; restarted when you hit play . If there are multiple voices ( number greater than 0 ) , it will cycle through voices to play overlapping audio . - You must specify multiple voices to have polyphonic audio keep in mind , this takes up more device resources . <p> On Android , assets that are loaded using preloadAudio are managed/played using the Android MediaPlayer . <p> On iOS , assets that are loaded using preloadAudio are managed/played using AVAudioPlayer . <p> @ @ @ @ @ @ @ @ @ @ this only works for assets loaded via preloadAudio. - This works for all asset types for Android , however it is- recommended- to keep usage consistent between platforms . 
@@106848941 @2248941/ <h> Salisbury Festival Time-lapse <p> Every year the town I live in has a weekend-long spring festival . There are rides for the kids , live music , beer , and lots of food . This year I have a great view overlooking the carnival area , so I decided to do a time-lapse video capturing all of the activity . The trucks pulled in before I got to the office on Thursday morning , but I managed to capture most of the set up , all the way until the trucks drove away on Sunday night . <p> I set up two GoPro cameras . One was a stock GoPro Hero 3+ Black edition capturing 7MP narrow FOV stills every 60 seconds . The other was a GoPro Hero 3 Black with a " flat " lens capturing 5MP stills every 60 seconds . Unfortunately the 3+ stopped recording after about 24 hours I 'm not sure if the camera over heated , had a bug in the firmware ( I realized I 'm 1 version back from the latest ) , or if my memory card had a corrupt @ @ @ @ @ @ @ @ @ @ camera . The backup camera kept running all 4 days and captured the entire festival . <p> Assembling this was simple I imported the images as image sequences in Adobe Premiere , arranged them on the timeline , cut out the night sequences ( there was almost no activity during them ) , added some transitions , titles , and color correction ( contrast and saturation ) , then added some background music . - I added slow zooming and panning to each of the shots to add drama , which helped make things a lot more interesting . 
@@106848942 @2248942/ <h> Tag Archives : JavaScript <p> I stumbled upon a really odd bug in my current project , which I can only attribute to the WebKit browser engine , since I was able to recreate this in a UIWebView on iOS ( in PhoneGap ) and in Chrome on OS X WebKit is the common engine in both . Its a bizarre issue that is really easy to fix , but was dumbfounding since the user interface was not displaying what I was seeing in the WebKit debugging tools . I figured I 'd share , in case anyone else runs into the same issue . <h> The problem : <p> I have a horizontal slider ( custom HTML/JS ) component , and content within a separate HTML &lt;span&gt; should be updated when the slider value changes . The JavaScript seemed to be working properly , I could see console.log output that showed events were being dispatched , but the UI would n't display what I was seeing in the debugging tools . Instead , the UI would update sporadically with a value , but not consistently , and not for @ @ @ @ @ @ @ @ @ @ The solution : <p> Set the CSS " display " property on the &lt;span&gt; element to " inline-block " , and everything works properly . This was a really strange issue since I could see the HTML DOM updates , but the actual UI was n't being updated . <p> If my description does n't  seem to be making any sense , check it out in the video below . - Keep an eye on the HTML DOM structure , as well as the rendered output . <p> Unfortunately , it took way longer than I would have hoped to fix such a seemingly simple issue . - Hopefully this saves you some time if you run into it too ! <p> I am happy to announce the US Census Browser version 2.0 ! - Back in December of 2011 , I released the US Census Browser- as an open source- application intended to demonstrate principles for enterprise-class data visualization and applications developed with web standards . - This version has some fairly substantial changes See the video below to check out features in the latest version : <p> Version 2.0 @ @ @ @ @ @ @ @ @ @ including : <p> Completely new &amp; redesigned UI layer , using app-UI. - app-UI is an open source framework for application view-navigators that mimic native mobile applications . - Using the app-UI SplitViewNavigator , the US Census Browser now supports both landscape and portrait orientations . <p> Switched from Google Maps to Open Street Map using OpenLayers. - Users of the Census Browser maxed out my Google Maps account ! - That is 25,000 map loads within a 24 hour period ! WOW ! I switched to the free Open Street Maps solution , which does n't  have any usage/bandwidth limitations . - With this change I was also able to add interactive maps . <p> Updated to Twitter Bootstrap 2.0. - The app is now using new UI styles and components which are now available in Twitter Bootstrap version 2.0 <p> I 'd like to take a moment and introduce app-UI , a new open source application framework that I 've been working on . <p> app-UI is a collection of reusable " application container " user interface components that may be helpful to web and mobile developers for creating interactive @ @ @ @ @ @ @ @ @ @ devices . app-UI is a continual work in progress it was born out of the necessity to have rich &amp; native-feeling interfaces in HTML/JS experiences , and it works great with PhoneGap applications ( http : //www.phonegap.com ) . app-UI can easily be styled/customized using CSS . <p> Disclaimer : Please keep in mind that things will change as the project is improved and matured this is a beta/early prototype . <p> All of app-UI was created using HTML , CSS , &amp; JavaScript . All animations are rendered using CSS3 translate3d , so that they are hardware accelerated ( where supported ) . app-UI works well on iOS , Android and BlackBerry browsers ( others not tested ) , and works well on the latest releases of most desktop browsers ( I know it does not work on old versions of IE ) . <h> Why ? <p> You might be wondering " why create this ? " when there are other open source alternatives like jQuery Mobile . The primary motivation for creating app-UI was to have reusable application containers that are highly performant , and do not @ @ @ @ @ @ @ @ @ @ , app-UI outperforms the alternatives , particularly on mobile devices . <p> app-UI can be used with many different existing frameworks app-UI only requires jQuery as a solution accelerator framework . It will work with existing UI widget frameworks ( jQuery UI , Twitter Bootstrap , etc ) , and will work with existing templating frameworks ( Moustache , Knockout , Handlebars , etc ) . <h> Application Containers <p> app-UI currently has three application containers , and at this time it is not intended to be a complete UI widget framework . <p> Please see the " samples " directory for usage scenarios there is no documentation yet . <h> ViewNavigator <p> The ViewNavigator component allows you to create mobile experiences with an easily recognizable mobile UI paradigm . You use this to push &amp; pop views from the stack . <h> SplitViewNavigator <p> The SplitViewNavigator component allows you to create tablet experiences with an easily recognizable mobile UI paradigm . The SplitViewNavigator allows you to have side-by-side content in the landscape orientation , and the sidebar is hidden in portrait orientation . <h> SlidingView <p> The SlidingView allows @ @ @ @ @ @ @ @ @ @ gesture , revealing a navigation container " underneath " . This is very similar to the behavior in Facebooks iPad application . <p> I see questions and comments all the time with the general sentiment " it looks nice , but who really uses PhoneGap/Apache Cordova ? " . There is no way to create a definitive list of everyone who uses it , but the general answer is " more people than you think " . Here are a few organizations that you might recognize who are using either PhoneGap or Apache Cordova in their cross-platform mobile solutions and/or tools . ( PhoneGap is a distribution of Apache Cordova ) <h> Microsoft <p> Microsoft is involved with core Apache Cordova development ( specifically for the Windows Phone platform ) . - Not only are staff from Microsoft committers for the core Apache Cordova project , Microsoft has also used PhoneGap on public mobile applications that target multiple platforms . - This includes the XBox-Live integrated gaming application Halo Waypoint , for both iOS and Android . - Check out Halo Waypoint in the video below , it looks awesome @ @ @ @ @ @ @ @ @ @ how deeply involved Adobe is with PhoneGap , but I 'll try to recap quickly - In late 2011 , Adobe acquired Nitobi , the creators of PhoneGap , and contributed PhoneGap to the Apache Software Foundation as the Apache Cordova project . - Adobe has resources dedicated to furthering PhoneGap and is dedicated to the success of the platform . - Not only are we helping develop and mature PhoneGap/Apache Cordova , we also build some of our own applications with it . - ( Maybe I 'll be able to talk about those some day . ) <h> Zynga <h> Logitech <p> Logitech- used PhoneGap to develop the Logitech Squeezebox Controller application , which uses your home wifi connection to control a Squeezebox Internet radio device from your smart phone . - You can read more about this application on the PhoneGap application showcase , or download it now for iOS or Android . <p> Still not sure if anyone uses PhoneGap ? What about these , among many others ? 
@@106848943 @2248943/ <p> Last week I had the opportunity to present to a great audience at- the- MoDev DC meetup group on " Smarter Apps with Cognitive Computing " . - In this session I focused on how you can create a voice-driven experience in your mobile apps . I gave an introduction to IBM Bluemix and IBM Watson services ( particularly the Watson language services ) , and demonstrated how you can integrate them into your native iOS apps . I also covered IBM MobileFirst for operational analytics and- remote logging to provide insight into your apps performance once it goes live . - Check out a recording of the complete presentation in the video below : <p> Back in February I had the opportunity to present " Enabling the Next Generation of Apps with IBM MobileFirst " at the DevNexus developer conference in Atlanta . - It was a great event , packed with lots of useful content . - Luckily for everyone who was n't able to attend , the organizers recorded most of the sessions which have just- been made available on Youtube . <p> here 's the session @ @ @ @ @ @ @ @ @ @ store you will have just entered into an iterative cycle of updates , improvements , and releases . Each successively building on features ( and defects ) from previous versions . IBM MobileFirst Foundation gives you the tools you need to manage every aspect of this cycle , so you can deliver the best possible product to your end user . In this session , we 'll cover the process of integrating a native iOS application with IBM MobileFirst Foundation to leverage all of the capabilities the platform has to offer . <p> What I 'm about to show you might seem like science fiction from the future , but I can assure you it is not . Actually , every piece of this is available for you to use as a service . - Today . <p> Yesterday- Twilio , an IBM partner whose services are available- via- IBM- Bluemix , announced several new SDKs , including live video chat as a service . - This makes live video very easy to integrate into your native mobile or web based applications , and gives you the power to do some @ @ @ @ @ @ @ @ @ @ could add video chat capabilities between your mobile and web clients ? Now , what if you could take things a step further , and add IBM Watson cognitive computing capabilities to add real-time transcription and analysis ? <p> Jeff and Damion did an awesome job showing of both the new video service and the power of IBM Watson . I can also say first-hand that the new Twilio video services are- pretty easy to integrate into your own projects ( I helped them integrate these services into the native iOS client ( physicians app ) - shown in the demo ) ! - You just pull in the SDK , add your app tokens , and instantiate a video chat . - Jeff is pulling the audio stream from the WebRTC client and pushing it up to Watson in real time for the transcription and sentiment analysis services . <p> I recently put together some content on building " Apps that Work as Well Offline as they do Online " using IBM MobileFirst and Bluemix ( cloud services ) . - There was the original- blog post , I @ @ @ @ @ @ @ @ @ @ now I 've opened everything up for anyone use or learn from . <p> The content now lives on the IBM Bluemix github account , and includes code for the native iOS app , code for the web ( Node.js ) endpoint , a comprehensive script that walks through every step of of the process configuring the application , - and also a video walkthrough of the entire process from backend creation to a complete solution . <p> Key concepts demonstrated in these materials : <p> User authentication using the Bluemix Advanced Mobile Access service <p> Remote app logging and instrumentation using the Bluemix Advanced Mobile Access service <p> Client-side Objective-C code ( you can do this in either hybrid or other native platforms too , but I just wrote it for iOS ) . - The " iOS-native " folder contains the source code for a complete sample application leveraging this workflow . The " GeoPix-complete " folder contains a completed project ( still needs you to walk through backend configuration ) . The " GeoPix-starter " folder contains a starter application , with all MobileFirst/Bluemix code commented out . @ @ @ @ @ @ @ @ @ @ By Step Instructions.pdf " file to setup the backend infrastructure on Bluemix , and setup all code within the " GeoPix-starter " project . <p> Earlier this week I had the privilege of speaking at ApacheCon- in Austin , TX on the topic of data management for apps that work as well offline as they do online . - This is an important topic for mobile apps , since , as we all painfully know already , there is never a case when you are always online on your mobile devices . - There always ends up being a time when you need your device/app , but you cant get online to get the information you need . - Well , this does n't  always have to be the case . There are strategies you can employ to build apps that work just as well offline as they do online , and the strategy- I 'd like to highlight today is based upon data management using the IBM Cloudant NoSQL database as a service , which is based upon Apache CouchDB . <p> here 's a link to the presentation slides ( @ @ @ @ @ @ @ @ @ @ advance the presentation slides : <p> The " couch " in CouchDB is actually an acronym for Cluster of Unreliable Commodity Hardware . At the core of this cluster is the concept of replication , which in the most basic of terms means that - data is shared between multiple sources . - Replication is used to share information between nodes of the cluster , which provides for cluster reliability and fault tolerance . <p> Cloudant is a- clustered NoSQL- database services that provides an extremely powerful and searchable data store . - It is- designed to power- the web and mobile apps , and all information is exposed via REST services . Since the IBM Cloudant service is based on CouchDB ( and not so coincidentally , IBM is a major contributor to the CouchDB project ) , replication is also core the the Cloudant service . <p> With replication , you only have to write your data/changes to a single node in the cluster , and replication takes care of propagating- these changes across the cluster . <p> If you are building apps for the web or mobile @ @ @ @ @ @ @ @ @ @ either on the device or in the browser. - This means that you can have a local data store that automatically pushes and/or pulls data from the remote store using replication , and it can be done either via native languages , or using JavaScript . <p> If you want to have local replication in either a web or hybrid ( Cordova/PhoneGap ) app , you can use PouchDB. - PouchDB- is a local JavaScript database modeled- after CouchDB and implements that CouchDB replication API. - So , you can store your data in the browsers local storage , and those changes will automatically be replicated to the remote Cloudant store . - This works in the browser , in a hybrid ( web view ) app , or even inside of a Node.js instance . Granted , if you 're in-browser you 'll need to leverage the HTML5 cache to have your app cached locally . <p> If you are building a native app , do n't  worry , you can take advantage of the Cloudant Sync API to leverage- the local data store with replication . - This is available @ @ @ @ @ @ @ @ @ @ API . <p> The sample app that I showed in the presentation is a native iOS application based on the GeoPix MobileFirst sample app- that I detailed in a previous post . - The difference is that in this case I showed it using the Cloudant Sync API , instead of the MobileFirst data wrapper classes , even though it was pointing at the exact same Cloudant database instance . - You can see a video of the app in action below . <p> All that you have to do is create a local data store instance , and then use replication to synchronize data between the local store and a remote store . <p> Replication be either one-way ( push or pull ) , or two-way . - So , any changes between the local and remote stores are replicated across the cluster . - Essentially , the local data store just becomes a node in the cluster . - This provides complete access to the local data , even if there is no network available . - Just save your data to the local store , and replication @ @ @ @ @ @ @ @ @ @ Objective-C code , you just need to setup the CDTDatastore manager , and initialize your datastore instance . <p> Once your datastore is created , you can read/write/modify any data in the local store . - In this case I am creating a generic data object ( basically - like a JSON object ) , and creating a document containing this data . - A document is a record within the data store . <p> You can add attachments to the document or modify the document as your app needs . - In the code below , I add a JPG atttachment to the document . <p> Replication is a fire-and-forget process . - You simply need to initialize the replication process , and any changes to the local data store will be replicated to the remote store automatically when the device is online . <p> By assigning a replicator delegate class ( as shown above ) , your app can monitor and respond to changes in replication state . - For example , you can update status if replication is in progress , complete , or if an error @ @ @ @ @ @ @ @ @ @ data from the local store , it is always available within the app , regardless of whether or not the device has an active internet connection . - For example , this method will return all documents within the local data store . 
@@106848944 @2248944/ <p> These two services enable you to quickly add Text-To-Speech or Speech-To-Text capability to any application . - What 's a better way to show them off than by updating my existing app to leverage the new speech services ? <p> I simply added the Text To Speech and Speech To Text services to my existing Healthcare QA application that runs on Bluemix : <p> IBM Bluemix Dashboard <p> These services are available via a REST API . Once youve added them- to your application , you can consume them easily within any of your applications . <p> I updated the code from my previous example- in 2 ways : 1 ) take advantage of the Watson Node.js Wrapper that makes interacting with Watson a lot easier and 2 ) to take advantage of these new services services . <h> Watson Node.js Wrapper <p> Using the Watson Node.js Wrapper , you can now easily instantiate Watson services in a single line of code . - For example : <p> The credentials come from your environment configuration , then you just create instances of whichever services that you want to consume @ @ @ @ @ @ @ @ @ @ service is now much simpler than the previous version . - When we want to submit a question to the Watson QA service , you can now simply call the " ask " method on the QA service instance . <p> Below is- my server-side code from app.js that accepts a POST submission from the browser , delegates- the question to Watson , and takes the result and renders HTML using a- Jade template. - See the Getting Started Guide for the Watson QA Service to learn more about the wrappers for Node or Java . <p> Compare this to the previous version , and you 'll quickly see that it is much simpler . <h> Speech Synthesis <p> At this point , we- already have a functional service that can take natural language text , submit it to Watson , - and return a search result as text . - The next logical step for me was to add speech synthesis using the Watson Text To Speech Service- ( TTS ) . - Again , the Watson Node Wrapper and Watsons REST services- make this task very simple . - @ @ @ @ @ @ @ @ @ @ src of an &lt;audio&gt; instance to the URL for the TTS service : <p> On the server you just need to synthesize the audio from the data in the URL query string . - Heres an example how to invoke the text to speech service directly from the Watson TTS sample app : <p> var textToSpeech = new **32;7908;TOOLONG ; // handle get requests app.get ( ' /synthesize ' , function ( req , res ) // make the request to Watson to synthesize the audio file from the query text var transcript = **34;7942;TOOLONG ; // set content-disposition header if downloading the // file instead of playing directly in the browser transcript.on ( 'response ' , function(response) **29;7978;TOOLONG ; if ( req.query.download ) **36;8009;TOOLONG ' = ' attachment ; filename=transcript.ogg ' ; ) ; // pipe results back to the browser as they come in from Watson transcript.pipe(res) ; ) ; <p> The Watson TTS service supports . ogg and . wav file formats . - I modified this sample is setup only with . ogg files . - On the client side , these are played using the @ @ @ @ @ @ @ @ @ @ were able to process natural language and generate speech , that last part of the solution is to recognize spoken input and turn it into text . - The Watson Speech To Text ( STT ) service handles this for us. - Just like the TTS service , the Speech To Text- service also has a sample app , complete with source code to help you get started . <p> In this post I 'd like to show a fairly simple application that I put together which shows off some of the rich capabilities for IBM MobileFirst for Bluemix that you get out of the box All with an absolute minimal amount of your own developer effort . - Bluemix , of course , being IBMs platform as a service offering . <p> GeoPix is a sample application leveraging IBM MobileFirst for Bluemix to capture data and images on a mobile device , persist that data locally ( offline ) , and replicate that data to the cloud . Since its built with IBM MobileFirst , we get lots of things out of the box , including operational analytics , user @ @ @ @ @ @ @ @ @ @ code at the bottom of this post ) <p> Heres what the application currently- does : <p> User can take a picture or select an image from the device <p> App captures geographic location when the image is captured <p> App saves both the image and metadata to a local data store- on the device . <p> App uses asynchronous replication to automatically save any data in local store up to the remote store whenever the network is available <p> Oh yeah , cant forget , the user auth- is via Facebook <p> MobileFirst provides all the analytics we need . - Bluemix provides the cloud based server and Cloudant- NoSQL data store . <p> All captured data is available on a web based front-end powered by Node.js <p> In this sample I 'm using everything but the Push Notifications service . - Im- using user authentication , the Cloudant DB ( offline/local store and remote/cloud store ) , and the node.js backend. - You get the operational analytics automatically . <h> Capturing Images <p> Capturing images from the device is also very straightforward . - In the app I leverage @ @ @ @ @ @ @ @ @ @ existing image or capture a new image . - See the presentImagePicker and- **29;8047;TOOLONG below . All of this standard practice using Apples developer SDK : <h> Persisting Data <p> If you notice in the- **29;8078;TOOLONG method above , there is a call to- the DataManagers saveImage withLocation method . This is where we save data locally and rely on Cloudants replication to automatically save data from the local data store up to the Cloudant NoSQL database . - This is powered by the iOS 8 Data service from Bluemix . <p> The first thing that we will need to do is initialize the local and remote data stores . Below you can see my init method from my DataManager class . In this , you can see the local data store is initialized , then the remote data store is initialized . If either data store already exists , the existing store will be used , otherwise it is created . <p> Once the data stores are created , you can see that the replicate method is invoked . - This starts up the replication process to automatically push @ @ @ @ @ @ @ @ @ @ " in the cloud " . <p> Therefore , if you 're collecting data when the app is offline , then you have nothing to worry about . - All of the data will be stored locally and pushed up to the cloud whenever you 're back online all with no additional effort on your part . - When using replication with the Cloudant SDK , you just have to start the replication process and let it do its thing fire and forget . <p> In my replicate function , I setup- CDTPushReplication for pushing changes to the remote data store . - You could also setup two-way replication to automatically pull new changes from the remote store . <p> Once weve setup the remote and local data stores and setup replication , we now are ready to save the data the were capturing within our app . <p> Next is my saveImage withLocation method . - Here you can see that it creates a new- **26;8109;TOOLONG object ( this is a generic object for the Cloudant NoSQL database ) , and populates it with the location data and timestamp. - It @ @ @ @ @ @ @ @ @ @ in from the UIImagePicker above ) and adds the jpg as an attachment to the document revision . - Once the document is created , it is saved to the local data store . - We then let replication take care of persisting this data to the back end . <p> If we want to query data from either the remote or local data stores , we can just use the performQuery method on the data store . Below you can see a method for retrieving data for all of the images in the local data store . <p> At this point we 've now captured an image , captured the geographic location , saved that data in our local offline store , and then use- replication to save that data up to the cloud whenever it is available . <p> AND <p> We did all of this without writing a single line of server-side logic . - Since this is built on top of MobileFirst for Bluemix , all the backend infrastructure is setup for us , and we get operational analytics to monitor everything that is happening . <p> @ @ @ @ @ @ @ @ @ @ <p> Active Devices <p> Network Usage <p> Authentications <p> Data Storage <p> Device Logs ( yes , complete debug/crash logs from devices out in the field ) <p> Push Notification Usage <h> Sharing on the web <p> Up until this point we have n't had to write any back-end code . However the mobile app boilerplate on Bluemix comes with a Node.js server . - We might as well take advantage of it . <p> The Node.js back end comes preconfigured to leverage the- express.js- framework for building web applications . - I added the- jade template engine and Leaflet for web-mapping , and was able to crank this out ridiculously quickly . <p> The first thing we need to do is make sure - we have our configuration variables for accessing the Cloudant service from our node app. - These are environment vars that you get automatcilly if you 're running on Bluemix , but you need to set these for your local dev environment : <p> Next you 'll se the logic for querying the Cloudant data store and preparing the data for our UI templates . You can customize this @ @ @ @ @ @ @ @ @ @ , or whatever you want . All interactions with Cloudant are powered by the Cloudant Node.js Client <p> A while back I wrote about adding parallax effects to your HTML/JS experiences to make them feel a bit richer and closer to a native experience . - I 've just added this subtle ( key word *subtle* ) effect to a new project and made a few changes I wanted to share here . <p> If you are wondering what I am talking about with " parallax effects " Parallax movement is where objects in the background move at a different rate than objects in the foreground , thus causing the perception- of depth . - Read more about it if you 're interested . <p> First , here 's a quick video of this latest app in action . - Its a hybrid MobileFirst app , but this technique could be used in any **35;8137;TOOLONG web app experience . - The key is to keep it subtle and not too much " in your face " , and yes , it is very subtle in this video . - You have to watch @ @ @ @ @ @ @ @ @ @ the previous post still apply Ive just added a bit more to cover more use cases . <p> This sets the background image and default position . - The distinct change here is that I set the background size to " auto " width and 120% height . - In this case , you can have a huge image that shrinks down to just slightly larger than the window size , or a small image that scales up to a larger window size . - This way you do n't  end up with seams in a repeated background or a background that is too big to highlight the parallax effect effectively . <p> In the requestAnimationFrame loop , it only applies changes *if* there are changes to apply . - This prevents needless calls to apply CSS even if the CSS styles had n't  changed . - In this , I also truncate- the numeric CSS string so that it is n't reapplying CSS if the position should shift by 0.01 pixels . Side note : If you are n't  using requestAnimationFrame for HTML animations , you should learn about- it @ @ @ @ @ @ @ @ @ @ holding the device upside down , it would n't work . - Not even a little bit . - This has that fixed ( see comments inline above ) . <p> This moves the background in CSS , which does n't  cause browser reflow operations , and moves the foreground content ( inside of a div ) using translate3d , which also does n't  cause browser reflow operations . - This helps keep animations smooth and the UX performing optimally . <p> I also added a global variable to turn parallax on and off very quickly , if you need it . <p> The result is a faster experience that is more efficient and less of a strain on CPU and battery . - Feel free to test this technique out on your own . <p> If you use the code above , you can modify the xMovement and yMovement variables to exaggerate the parallax effect . <p> This post specifically covers- native iOS , though there are also Android and hybrid options available . This should have everything you need to get started . It covers all aspects- from creating @ @ @ @ @ @ @ @ @ @ leveraging Cloudant storage , push notifications , and monitoring &amp; logging . <p> So , without further ado , let 's get started <h> Part 1 : Getting Started with Bluemix Mobile Services <p> In this first video I show how to create a new mobile app on Bluemix , connect to the cloud app instance , and implement- remote logging from the client application . This process is covered in more detail in the Getting Started docs , but below- are the basics from my experience . <p> Youll- first need to- sign into your Bluemix account . If you do n't  already have one , you can create a trial account- for free . Once you 're signed in , you just need to create a new mobile app instance . <p> The process is very simple , and there is a " wizard " to guide you . The first thing that you need to do is create a new app by clicking the big " Create an App " button on your bluemix dashboard . <p> Create a new app from IBM Bluemix Dashboard <p> Next , select @ @ @ @ @ @ @ @ @ @ MBaaS , you 'll want to select the " Mobile " option . <p> Select the type of app <p> Next you 'll need to select your platform target . You can choose either " iOS , Android , Hybrid " , or the " iOS 8 beta " target . In this case I chose the iOS 8 beta , but the process is similar for both targets. - Hybrid apps are built leveraging the Apache Cordova- container . <p> Select your platform target <p> Next , just specify an app name and click " Finish " . <p> Give your app a name <p> Once your app is created , you will be presented with instructions how to connect the app in Xcode . I 'll get to that in a moment <p> Now that- your app has been created , you 'll be able to see it on your Bluemix dashboard . This app will consist of several components : a Node.js back-end instance , a Cloudant NoSQL database instance , an Advanced Mobile Access instance , and a Push instance . The Advanced Mobile Access component provides you with app @ @ @ @ @ @ @ @ @ @ more . The Push component gives you the ability to manage and send push notifications ( either manually , or with a rest-based API ) . <p> You app has been created here are the components and the activity <p> Once your app has been created , you will need to setup the mobile app to connect to Bluemix to consume the services . Again , this is a very straightforward process . <p> The next step is to register your client application . Once your app is created , you will be presented with a screen to do this . If you do n't  complete it right away , you can always come back later and register an application . You 'll need to specify the Bundle I 'd and version of your app , then you can setup any authentication ( if you choose ) . <p> Register your apps bundle I 'd and version <p> Once your app has been registered , you need to configure Xcode . Youll first need to create a new project in Xcode . There are two options for configuring your Xcode project : 1 @ @ @ @ @ @ @ @ @ @ installation . I used the CocoaPods installation simply because it is easier and manages dependencies for you . <p> If you are n't  familiar with CocoaPods , it is much like NPM CocoaPods is a dependency manager for Cocoa projects . It helps you configure- the Bluemix libraries and manages dependencies for you . <p> If you 've got Xcode up and running , then close it and install CocoaPods , if you do n't  already have it . Next open up a terminal/command prompt , go to the directory that contains your Xcode project and initialize CocoaPods- using the " setup " - command : <p> pod setup <p> This will create a new file called " podfile " . Open this file in any text editor and paste the following ( note : you can remove any lines that you do n't  want to actually use ) : <p> source ' https : **32;8174;TOOLONG ' # Copy the following list as is and # remove the dependencies you do not need pod ' IMFCore ' pod ' IMFGoogleAuthentication ' pod ' **25;8208;TOOLONG ' pod ' IMFURLProtocol ' pod ' @ @ @ @ @ @ @ @ @ @ to the " podfile " file , and close the text editor . Then go back to your command promprt/terminal- - and run the installation process : <p> pod install <p> Your project will be configured , and all dependencies will be downloaded automatically . Once this is complete , open up the newly created . xcworkspace file ( Xcode Workspace ) . <p> You have to initialize the Bluemix inside of your application to connect to the cloud service to be able to take advantage of any Bluemix features ( logging , data access , auth , etc ) . The best place to put this is inside of your AppDelegate.m- class- application **29;8235;TOOLONG method because it is the first code that will be run within your application : <p> One of the first features I wanted to take advantage of was remote collection of client-side logs . You can do this using the IMFLogger class , in much the same fashion as you do with OCLogger in MobileFirst Foundation server . Once great feature that requires almost no- additional configuration is the- **25;8266;TOOLONG method , which automatically configures @ @ @ @ @ @ @ @ @ @ app crashes . <p> Next , launch your app in the iOS simulator , or on a device , and you 'll see everything come together . Log into your Bluemix dashboard , and you 'll be able to monitor app analytics and remote logs . <p> Note : If you experience any issues connecting to the Bluemix mobile app from within the iOS simulator , just clear the iOS Simulator by going to the menu command " iOS Simulator -&gt; Reset Content and Settings " , and everything should connect properly the next time you launch the app . <h> Part 2 : Configuring the Node.js Backend <p> In the next video , I demonstrate how to- grab the code for the backend Node.js application , create a git repository on IBM JazzHub , then pull the code for local development . <p> When the app is created , you 'll see an " add git " link under the app name . Using this link , you can create a git repository for the backend code . <p> Add a git repository <p> Once your git repo has been created , @ @ @ @ @ @ @ @ @ @ ( I used the CLI ) . You 'll need to use the " npm install " command to pull down all the app dependencies . The biggest thing you need to know is that it uses express.js for the web application framework. - - You can make any changes that you want , and they will be automatically deployed to your Bluemix server instance upon commit . Yes , this workflow is also configurable b/c this process may not work for everyone . <p> One other thing that you will need to watch out for if you are doing local development : You will want to wrap the following code on line 6 in a try/catch block , otherwise you will hit errors in the local environment which will prevent your app from launching locally : <h> Part 3 : Consuming Data from Cloudant <p> Another part of Bluemix mobile applications is the Cloudant NoSQL database . The Cloudant NoSQL database is a powerful solution that gives you remote storage , querrying , and client-side- data storage mechanisms with automatic online/offline synchronization , all with monitoring/analytics capabilities . <p> By @ @ @ @ @ @ @ @ @ @ as generic objects ( over-simplification : think of it is an extremely powerful JSON store in the cloud ) . However you can also serialize your objects to strong data types within the client code configuration . <p> In your AppDelegate class- application **29;8293;TOOLONG method , you 'll also want to initialize the IMFDataManager class , which is the class used for interacting with all Cloudant data operations . <p> IMFDataManager *manager = IMFDataManager sharedInstance ; <p> In my sample , I setup the database manually with open permissions , but you 'll probably want something more secure . Once your database is created , you can create indexes , search for data , create data , etc <p> In the following code , I create a search index and query for data from the remote Cloudant database . You really only need to create the index if it does n't  already exist . You can do this either through the mobile app code , or manually through the Cloudant databases web interface . I did this inline in the following code , just for the sake of simplicity : <h> Part @ @ @ @ @ @ @ @ @ @ app also contains a component for managing push notifications within your mobile applications . With this service , you can send push notifications to a specific device , a group of devices using tags , or all devices , and you can send push notifications either manually via the web interface , or as part of an automated workflow using the REST API . <h> Part 5 : Monitoring and Logging <p> Did I- mention that every action that you perform through Bluemix Mobile Services can be monitored ? Analytics are available for the Advanced Mobile Access component , the Cloudant NoSQL data store , and the Push Notifications service . In addition , you also have remote collection of client logs and crash reports . This provides- - unparalleled insight into- the health of your applications . <p> This is more than just " Cloud Services " which more generally refer to a scalable virtual cluster- of computing or storage resources . - Bluemix is IBMs suite of cloud service offerings , and covers lots of use cases : <p> Bluemix is an open-standards , cloud-based platform for building @ @ @ @ @ @ @ @ @ @ such as web , mobile , big data , and smart devices . Capabilities include Java , mobile back-end development , and application monitoring , as well as features from ecosystem partners and open source " all provided as-a-service in the cloud . <p> Why is it a hot topic ? - MBaaS- enables growth of mobile applications- with seamless ( and virtually endless ) scalability , all without having to manage individual systems for the application server , database , identify management , push notifications , or platform-specific services . <p> Ive been writing a lot about IBM MobileFirst lately for a seamless API to deliver mobile apps to multiple platforms ; though it has been- in the context of an on-premise installation . - However , did you know that many of the exact same MobileFirst features are available as- MBaaS services on IBM Bluemix ? <p> Mobile Data The mobile data service includes- a- NOSQL database ( powered by IBM Cloudant ) , file storage- capabilities , and appropriate management and analytics features to measure the number of calls , storage usage , time/activity , and OS @ @ @ @ @ @ @ @ @ @ you to easily push data to the right people at the right time on- either Apple APNS or Google GCM platforms all with a single API. - Notifications can be sent by either an app or backend system , and can be sent to a single device , or a group of devices based on their tags/subscriptions. - Of course , with appropriate analytics for monitoring activity , distribution , and engagement . <p> Many of these are the exact same features that you can host in your own on-premise IBM MobileFirst Platform Foundation server the difference is that you do n't  have to maintain the infrastructure . - You can scale as needed through the Bluemix cloud offering . 
@@106848945 @2248945/ <h> Monthly Archives : January 2014 <p> The Creative Cloud Packager is a tool for CC Enterprise and CC Team- customers that enables them to easily package Creative Cloud products and updates for deployment within their organizations . - It let 's you select specific Creative Cloud products and/or updates and package them into . pkg or . msi installers ( optionally with a serial number for Enterprise customers ) . - These packages can then be deployed on their own or integrated with third-party deployment tools like JAMF Casper or Microsoft SCCM. - The Creative Cloud Packager even let 's you control Creative Cloud update behaviors and more . <p> With the recent releases of Creative Cloud Packager , you can now edit existing deployment packages , create deployment packages from local media ( DVDs ) , and even create deployment packages for older ( CS6 ) creative applications , if you have the proper license . <p> In addition to my addiction to aerial photography , I 'm also fascinated by time-lapse photography . With time lapse photography , you set up your camera to take pictures on an interval @ @ @ @ @ @ @ @ @ @ minutes , every few hours , or heck , once a day . Its really up to you how you want to set up your shots and what you want to shoot . In any case , you can end up with a lot images each by itself could be great , but it only tells a limited story . - However , you can put all those images together in a sequence to create some truly amazing visuals . Subtle motion becomes pronounced , and you can clearly view the passage of time . Often , this ends up with an amazing visual story that would be hard to otherwise capture . <p> All that you need start diving into time-lapse photography is a camera that is capable of capturing images on an interval normally there is some kind of time lapse mode that let 's you set up your image frequency and duration . Then , once you 've got your images , you can process them with Creative Cloud tools to bring out their full potential . <p> Here are two time-lapse sequences I created this weekone a snow @ @ @ @ @ @ @ @ @ @ a lot of specialized or expensive equipment . I used a GoPro Hero 3 Black camera , set it on my window sill , and let it do its thing . ( I do want to upgrade to better gear , but this still works fantastically , and I love the GoPro . ) <p> GoPro Hero 3 Black Edition <p> The sunset was a ten second interval captured over about 2 hours and played back in 30 seconds . The snow storm was a 60 second interval captured over roughly 14 hours , played back in 40 seconds . <p> So , you 've captured the images , what next ? - <p> You can check out the video below , or read on for further explanation how I processed and assembled the images into a video sequence , complete with links to Adobe documentation and tutorials . <p> Before putting everything together as a sequence , I wanted to enhance the photos to bring out as much detail as possible . here 's where Adobe Lightroom comes into the picture . I used Lightroom to import all of my photos @ @ @ @ @ @ @ @ @ @ bulk/batch processing to enhance all of the images . <p> Editing Photos with Lightroom <p> First , select an image to use as your baseline for adjustments . I would n't start with your darkest image , and I would n't start with your lightest either . I normally start somewhere in the middle . Select the image , and then switch over to the " Develop " module . I use the basic panel to make adjustments to this image . For the GoPro , I like to bring up the shadows and bring down the highlights to pull out details out . If I 'm shooting a landscape , I also like to bring up the clarity and maybe even the vibrance and saturation just do n't  over do it . You could also use one of Lightrooms presets if you want ; its really up to you . Just be extra careful that it is not too dark or too light b/c were going to apply these settings to all images in the sequence . <p> Lightroom Basic Panel <p> If you want to adjust hue , saturation or luminance @ @ @ @ @ @ @ @ @ @ HSL/Color/B&amp;W panel . Using this you can make specific colors more or less intense . - I normally try to tone down the yellows in my GoPro images after I 've increased overall saturation . <p> Since I used the GoPro , there is a lot of fisheye distortion from the lens the GoPro has a 2.77mm lens whichgives an ultra-wide 170 degree field of view . This makes for some awesome wide angle shots , but sometimes you do n't  want that extreme distortion . This is where lens correction gets really handy . Next , I opened up the Lens Correction panel . As soon as you check the " Enable Profile Corrections " checkbox , Lightroom should automatically select the GoPro Hero 3 Black Edition lens profile based upon metadata within the image . I did n't  want to fully flaten the image , just reduce the wide angle , so I turned down the distortion correction using the " Distortion " slider . <p> Lightroom Lens Correction <p> Once you have your baseline image the way you want it , you need to apply these settings to @ @ @ @ @ @ @ @ @ @ them all , and then either click on the " Sync " button in the bottom right of the Develop module , or use the Settings -&gt; Synch Settings menu . This will apply you changes on this image to all of the images that were selected . This will happen automatically if you are using auto-sync. - You can learn more about synchronizing metadata between photos in the Lightroom documentation . <p> Next , be sure to view several images in your collection , the lightest to the darkest , and make sure they all look decent . If you need to make any changes because they are too light , or too dark , or do n't  have the right contrast , then now is your time to fix it . Once you 're happy with the images in your collection you next need to export them . I exported as JPG with 100% quality at full resolution with sequential names . <p> Now we 've got a lot of processed images . What 's next ? We need to make a video ! <p> If you 're wondering how I got @ @ @ @ @ @ @ @ @ @ did n't  have the camera moving . There are devices which make this possible , but I just used a video editing trick . The images are 12 MP , or 4000 by 3000 pixels . A " standard " HD video sequence is 1920 by 1080 pixels . The image below reflects this scale the red area represents the 4000 by 3000 still image , and the yellow represents the 1920 by 1080 video . <p> Video &amp; Image Size Comparison <p> You 'll notice that leaves us with a lot of room to zoom and pan around the image . I zoom into the image so that it fills the entire horizontal space within the video sequence you can zoom in more if you want . This leaves a fair amount of vertical content outside the clipping rectangle of the video . You can use this to your advantage by panning vertically within this area . - I just made the pan very slow and deliberate so it appears that there is constant motion of the camera throughout the entire video . <p> The final result is that the @ @ @ @ @ @ @ @ @ @ move because the actual image sequence is moving relative to the video viewport . <p> I 've recently had several conversations with PhoneGap users around processes for automating the compilation of PhoneGap apps . - This could be either in automated tasks using- Grunt , - Ant , - Maven , or any other task manager , or could be in continuous integration environments like Jenkins CI . <p> If you 're interested in this , here are a few options First of all , PhoneGap Build has a REST API . You can use this to programmatically create new projects , update projects , trigger new builds ( even just for specific platforms ) , etc This can integrate with your build scripts and tie into any workflow . <p> If you 're using GitHub , it is possible to tie into hooks triggering PhoneGap Build to recompile every time you commit your code . - Heres an example of it in action , or you can just use this service which is already setup : - http : //autobuild.monkeh.me/- ( from the same author ) - - Just be careful with @ @ @ @ @ @ @ @ @ @ also use the- Autobuild service using a clientID variable instead of sending through username and password details via HTTP . <p> If you are n't  using PhoneGap Build , you 're not out of luck . - All PhoneGap CLI- commands are based on scripts , which themselves can be scripted. - You could use ANTs exec command , the Maven exec plugin , Grunt exec or Grunt shell plugins , Jenkins execute shell , or any other task runner to manually invoke the PhoneGap CLI . You just need to make sure all your environment and path variables are correct to access SDKs and required programs . However , there 's one caveat iOS builds require Xcode/Apple developer tools , which have to be run on a Mac . <p> Photosynth is an impressive service from Microsoft . - It enables you to upload photos and turn them into interactive 360 panoramas , photo walls , spins , or photo walks . The Photosynth team recently announced a new version of Photosynth , and its a really cool web experience . It leverages- WebGL- to visualize the content , and runs @ @ @ @ @ @ @ @ @ @ as the devices support WebGL ) . <p> Those who know me well or regularly read the blog probably already know I have an obsession with aerial photography using remote controlled multirotor helicopters . Once I discovered Photosynth , my first thought was " Wow , these Photo Walks will be incredible to visualize flights " . - I capture most of my flights in time-lapse photography mode with a GoPro camera attached to a DJI Phantom copter . The time-lapse images are perfect for Photosynth I normally capture on a two second interval , though the Photosynth team suggested trying an even shorter interval for better results . <p> To generate the best Photosynths , you need to start with the best photos . This is where Lightroom comes into the picture . Lightroom is an incredible tool for editing photos and bringing out their details . You can enhance exposure , colors , clarity , saturation , reduce noise , and more . Even better , it excels at bulk image editing . Thus its perfect for processing your photos for preparation to create a Photosynth . <p> @ @ @ @ @ @ @ @ @ @ Photosynth , and preparing your photos with Lightroom . <p> Now , you 're ready to learn more about both Lightroom and Photosynth , right ? <p> Below are Photosynths from a few of my flights . - If your browser supports WebGL , you 'll be able to see the fully interactive experience you 'll be able to scrub through the photos , zoom in , and pan the images at full resolution . Its best viewed in full-screen mode . <p> Another great feature in the latest release of Adobe Photoshop CC is Linked Smart Objects . Linked Smart Objects enable you to place another file inside of a composition as a smart object . Any time that the linked file is saved , those changes are automatically propagated to the linked documents that you have open . ( If they 're not open , you will be prompted the next time you open the file . ) <p> For example , you can place a . ai Illustrator file inside of a PSD as a Linked Smart Object , and when you update the content in Illustrator , the content in @ @ @ @ @ @ @ @ @ @ for more detail . <p> Be sure to check out the Creative Cloud learning resources to learn more about Linked Smart Objects . <p> This is a FREE update for- Creative Cloud- members . - If you 're not already a Creative Cloud member , - join- today to get the most out of all of the tools that Adobe has to offer ! 
@@106848946 @2248946/ <h> Tag Archives : Cognitive <p> Its not every day that you get the opportunity to have your work showcased front and center on the main landing page for one of the largest companies in the world . Well , today is definitely my lucky day . I was interviewed last month about a drone-related project that I 've been working on that focuses on insurance use cases and safety/productivity improvement by using cognitive/artifical intelligence via IBM Watson . I knew it was going to be used for some marketing materials , but the last thing that I expected was to have my image right there on ibm.com . I see this as a tremendous honor , and am humbled by the opportunity and exposure . <p> here 's an interview that I recently did with IBM DeveloperWorks TV at the recent World of Watson conference . In it I discuss a project Ive been working on that analyzes drone imagery to perform automatic damage detection using the Watson Visual Recognition , and generates 3D models from the drone images using photogrammetry processes . The best part the entire thing runs @ @ @ @ @ @ @ @ @ @ a while since I 've posted here on the blog - In fact , - I just did the math , and- its been over 7 months. - Lots of things have happened since , I 've moved to a new team within IBM , built new developertools , worked directly with clients- on their solutions , worked on a few high profile keynotes , built apps for kinetic motion and activity tracking , built a mobile client for a chat bot , and even completed some new drone projects . - Its been exciting to say the least , but the real reason I 'm writing this post is to share a few- of the public projects Ive been involved with from recent conferences . <p> I recently returned from Gartner Symposium and IBMs annual World of Watson conference , and- its been one of the busiest , yet most exciting span of two weeks Ive experienced- in quite a while . <p> At both events , we showed a project Ive been working on with IBMs Global Business Services team that focuses on the use of small consumer drones and drone @ @ @ @ @ @ @ @ @ @ by leveraging IBM Watson to automatically detect roof damage , in conjunction with photogrammetry to create 3D reconstructions and generate measurements of afflicted areas to expedite and automate claims processing . <p> This application leverages many of the services IBM Bluemix has to offer on-demand CloudFoundry runtimes , a Cloudant NoSQL database , scalable Cloud Object Storage ( S3 compatible storage ) , and BareMetal servers on Softlayer . Bare Metal servers are *awesome* I have a dedicated server in the cloud that has 24 cores ( 48 threads ) , 64 GB RAM , RAID array of SSD drives , and 2 high end multi-core GPUs . Its taken my analysis processes from 2-3 hours on my laptop down to 10 minutes for photogrammetric reconstruction with Watson analysis . <p> Its been an incredibly interesting project , - and you can check it out yourself in the links below . <h> World of Watson <p> World of Watson was a whirlwind of the best kind I had the opportunity to join IBM SVP of Cloud , Robert LeBlanc , on stage as part of the the Cloud keynote at @ @ @ @ @ @ @ @ @ @ people ) to show off the drone/insurance demo , plus 2 more presentations , and an " ask me anything " session on the expo floor . <p> You can also check out my session " Elevate Your apps with IBM Bluemix " on UStream to see an overview in much more detail : <p> .. and that 's not all . I also finally got to see a complete working version of the Olympic Cycling teams training app on the expo floor , including cycling/biometric feedback , video , etc I worked with an IBM JStart team and wrote the video integration layer into for the mobile app using IBM Cloud Object Storage and Aspera for efficient network transmission . <h> Drones <p> On this project we 've been working with a partner DataWing , who provides drone image/data capture as a service . However , I 've also been flying and capturing my own data . The app can process virtually any images with appropriate metadata , but I 've been putting both the DJI Phantom and Inspire 1 to work , and they 're working fantastically . <p> IBM Watson services , @ @ @ @ @ @ @ @ @ @ you the ability- to handle unstructured data , like text analysis or- translation , speech processing , and more . - This makes consumption , mining , or responding to unstructured data or " dark data " faster , more efficient , and more powerful than ever . <p> The new Watson iOS SDK- provides- developers with an API- to simplify integration of the- Watson Developer Cloud services into their mobile apps , including the Dialog , Language Translation , Natural Language Classifier , Personality Insights , Speech To Text , Text to Speech , Alchemy Language , or Alchemy Vision services " all of which are available today , and can now be integrated with just a few lines of code . <p> The- Watson iOS SDK makes integration with Watson services significantly *really* easy . For example , if you want to take advantage of the Language Translation service , you first have to setup a service instance . Once the translation service is setup , then you 'll be able to leverage translation capabilities within your mobile app : <p> Be sure to check out the sample @ @ @ @ @ @ @ @ @ @ with all of the Watson services , You must have a service instance properly configured , with authentication credentials- in order to be able to consume it within your app . <p> You may have heard a lot of buzz coming out of IBM lately about Cognitive Computing , and you might have also wondered " what the heck are they talking about ? " - You may have heard of- services for data and predictive analytics , services for natural language text processing , services for sentiment analysis , services understand speech and translate languages , but its sometimes hard to see the forest through the trees . <p> I highly recommend taking a moment to watch this video that introduces Cognitive Computing from- IBM : <p> Cognitive computing systems learn and interact naturally with people to extend what either humans or machine could do on their own . <p> They help human experts make better decisions by penetrating the complexity of Big Data . <p> Cognitive systems are often based upon massive sets of data and powerful analytics algorithms that detect- patterns and concepts that can be turned @ @ @ @ @ @ @ @ @ @ not " artificial intelligence " in the sense that the services/machines act upon their own ; rather a system that provides the user tools or information that enables them to make better decisions . <p> The benefits of cognitive systems in a nutshell : <p> They augment the users experience <p> They provide the ability to process information faster <p> They- make complex information easier to understand <p> They enable you to do things you might not otherwise be able to do <p> Curious where this will lead ? - Now take a moment and watch this video talking about the industry-transforming opportunities that Cognitive Computing is already beginning to bring to life " <p> So , why is the " mobile guy " - talking about Cognitive Computing ? <p> First , its because Cognitive Computing is big I mean , really , really big . - Cognitive systems are literally transforming industries and providing powerful analytics and insight into the hands of both experts and " normal people " . - When I say " into the hands " , I again- mean this literally ; much of @ @ @ @ @ @ @ @ @ @ through their mobile devices . <p> Last , and this is purely just- personal opinion , I see the mobile MobileFirst offerings themselves as providing somewhat of cognitive service for developing mobile apps . - If you look at it from the operational analytics perspective , you have an immediate insight and a snapshot into the health of your system that you would never have seen otherwise . - You can know what types of devices are hitting your system , what services- are being used , how long things are taking , and detect issues , all without any additional development efforts on your end . Its not predictive analytics , but sure is helpful and gets us moving in the right direction . 
@@106848947 @2248947/ <p> My javascript toolkit is a set of resources that can help aid in the development of javascript-heavy or AJAX style applications . I encourage the use , enhancement and distribution of this javascript library . I hope that you find it useful as well . <p> There are four major components to the Javascript Toolkit . They are not necessarily " pretty " yet , but they are very useful . Each is described on this page . Use the following links to see a demo of each . <p> The CommandPrompt object is simply a command line interface for javascript , within a web page . Just type javascript commands into the text box and hit enter , and your code is executed . It is built so that it can easily be added to any page . From within it , you can call any javascript command , get or set the value of any existing variable within the page , or evaluate expressions . Although , It is still subject to the rules of your browser 's security model . <p> An instance of the @ @ @ @ @ @ @ @ @ @ <p> &lt;script&gt; var cmdPrompt = new CommandPrompt() ; &lt;/script&gt; <p> ... And it is written such that there can be multiple instances of the CommandPrompt per browser instance ( in the random event that you actually need this ) . CommandArea <p> The CommandArea object is another a command line interface for javascript , within a web page . All internal functionality is identical to the CommandPrompt . The only difference is how you enter the javascript commands . Instead of having a textbox , and listening for the kepress event on the enter key , The CommandArea object uses a multiline textarea control , to allow you to inject longer code chunks and values . It is also built so that it can easily be added to any page . From within the CommandArea , you can also call any javascript command , get or set the value of any existing variable within the page , or evaluate expressions . One thing to keep in mind is that functions defined within the CommandArea are only accessible during code execution . For example , if you define a function within @ @ @ @ @ @ @ @ @ @ the contents of the CommandArea ... your function will be gone . <p> An instance of the CommandArea object can be created as easily as this : <p> &lt;script&gt; var cmdArea = new CommandArea() ; &lt;/script&gt; <p> ... And it is also written such that there can be multiple instances of the CommandArea per browser instance . VarWatch <p> The VarWatch object is a usefull debugging tool . It is designed to watch the values of your javascript variables during code execution . You can either programmatically add variables to the watch list , or you can add them during runtime through the user interface . The VarWatch object will display a list of all variables that you are watching , and their corresponding values will be updated every 500 millisecond . <p> An instance of the VarWatch object can be created , and variables assigned to it as easily as this : 
@@106848948 @2248948/ <h> Tag Archives : drones <p> Its not every day that you get the opportunity to have your work showcased front and center on the main landing page for one of the largest companies in the world . Well , today is definitely my lucky day . I was interviewed last month about a drone-related project that I 've been working on that focuses on insurance use cases and safety/productivity improvement by using cognitive/artifical intelligence via IBM Watson . I knew it was going to be used for some marketing materials , but the last thing that I expected was to have my image right there on ibm.com . I see this as a tremendous honor , and am humbled by the opportunity and exposure . <p> here 's an interview that I recently did with IBM DeveloperWorks TV at the recent World of Watson conference . In it I discuss a project Ive been working on that analyzes drone imagery to perform automatic damage detection using the Watson Visual Recognition , and generates 3D models from the drone images using photogrammetry processes . The best part the entire thing runs @ @ @ @ @ @ @ @ @ @ been so focused on mobile , apps , development , conferences , and more that I have n't posted much besides IBM work news and projects . - Well , I 'm taking a break for just a moment <p> If you 've followed my blog for a while , then you already know that I 'm pretty much obsessed with " drones " . - It is- by far the most fun and exciting recreation- that Ive taken up in a very long time . Not only are they fun to fly , but they get you into some amazing views that were previously inacessible , and have applications far beyond just taking pictures . - I 've written how-tos for aerial photography and videography , taken tons of pictures for fun , and even shot some indoor footage for TV commercials . <p> I 'm always following the news feeds , watching the advances in technology , watching prices drop , and- am continually blown away- by what the industry is offering . - The last week to ten days have been nothing short of amazing . <p> First let 's start with the latest @ @ @ @ @ @ @ @ @ @ consumer- drone with some very impressive specs and performance . <p> The Phantom- 3 - is an easy to fly copter that sports a 3-axis gimbal ( camera stabilizer ) , up to 4K video footage , an integrated rectilinear ( flat ) lens camera , live HD first-person view , integrated iOS and Android apps , a vision positioning system- ( for stabilized indoor flights ) and up to a 1.2 mile flight range . - All for a cost of under $1300 USD. - Thats one heck of a package , and officially makes my old- Phantom look like a dinosaur . <p> 3 Days later , 3D Robotics announced the Solo , a direct competitor to the Phantom . The- Solo- is also very impressive , and has already- won an- award- for Best Drone- at NAB in Las Vegas . <p> The Solo also has a 3-axis gimbal for stabilized footage , and is designed to work with GoPro cameras . - In fact , it is the only copter that integrates with the camera controller and can control the GoPro remotely . The Solo also @ @ @ @ @ @ @ @ @ @ in the copter ) , HD first person view , and has- an upgradeable system that can have- new camera systems or payloads configured . - It does n't  have an optical stabilization system built in , but that can be added to the expansion bay . - What really sets the Solo apart is the intelligent auto-pilot sytem that appears to make complex shots very easy. - All of this with a price tag starting at $1000 USD . <p> I currently own DJI products , but this has gotten me- seriously considering a purchase . <p> Both of these are small aircraft- targeting consumers , but from the look of it they are definitely capable of high end applications . - Their small size make them extremely portable , and a potential add in many industries and use cases . - Larger copters are always available for larger scale applications . <p> Let 's not forget drones for the enterprise - Last week Airware- launched their drone operating system . - Business can now license their operating system for commercial applications and data collection . <p> Meanwhile , people @ @ @ @ @ @ @ @ @ @ , ignoring their utility and positive value . The rules for commercial use continue to shake out , but oh man , its an exciting time . 
@@106848949 @2248949/ <h> Category Archives : Android <p> Stage3D is starting to look incredibly promising for developers . Whether you are targeting mobile , web , or desktop or developing for gaming or data visualization , the future is bright . I was in the Adobe offices in San Francisco last week meeting with the team , and Thibault Imbert showed us some jaw dropping demos Keep an eye out for whenever those become public . <p> but it is not all about games . In a recent post , I showed off some basic stage3d hardware accelerated graphics within AIR on mobile devices . Ive been developing some ideas to show how this can be applied within non-gaming contexts , and I figured I 'd show my progress so far . The previous post was an actionscipt-only project . In enterprise scenarios , you most likely will want to take advantage of the plethora of pre-built components that the Flex framework provides . The progression for me was natural ; How about showing what Stage3D can do within a Flex for mobile application ? <p> Above is a fairly basic visualization @ @ @ @ @ @ @ @ @ @ tied into data that is displayed using a standard spark List with a custom item renderer . The chart is based on my previous example , and obviously is n't fully fleshed out ( no legend , no ticks , no axis labels , etc ) . Did I mention , this is on a tablet ? Oh yeah , that is pretty awesome . Did I also mention that it will be cross platform once released ? Oh yeah , that is pretty awesome too . <p> The data is morbid , but it works for my scenario . It is based on this data set from CMU.edu , which shows the cancer deaths and the number of cigarrettes consumed per state . <p> Please keep in mind , this is all beta , and not a final build . Ill post the source code once Stage3D for mobile is publicly available . <p> Mark your calendars ! Next Thursday , August 18th at 9:00AM to 10:30 AM ( PDT GMT-7 ) , the evangelism team will be hosting a live Q&amp;A chat . Come join us to learn more @ @ @ @ @ @ @ @ @ @ questions with you ! <p> This session will be Q&amp;A only . There will be no demos , no slides , no speaking just pure Q&amp;A through Adobe Connect ( all you need is a web browser or the mobile app ) . Whether you are actively involved in mobile development , or about to start , join us and bring your questions ! <p> On Thursday morning , just go to http : //flex.org/ask and join us in the chat . I hope to see you there ! <p> How : We will have at least five evangelists in the Connect session to answer questions . This is a bit of a science experiment so we 'll see how it goes ! We will allow as many people in as we can handle , then we 'll start blocking entry and creating a queue . The event will run for 90 minutes so you can come and go as you want . There are no presentations and no demos . It 's purely a Q&amp;A session . <p> Why : For many of us , building apps for mobile @ @ @ @ @ @ @ @ @ @ before had to deal with touch interfaces , varying screen densities , and input from the GPS and accelerometer . At the same time , we have to be much more conscience than ever before about resources , because our apps run on much slower CPUs with far less memory than we are used to and on OSes that will shut down your app if it crosses the line ! It 's not a trivial transition ! <p> Our team has been building real apps with Flex for several months and we have learned a lot . This is an opportunity for us to share that knowledge in an informal setting . We 've never attempted this before , but if it works well , we 'll do it often . If it 's a bust , we 'll figure out a better way . <p> A few things to keep in mind for the chat : <p> Questions should be as specific as possible and limited to Flex on Mobile . <p> We ca n't promise that every question will be answered , but we 'll do our best @ @ @ @ @ @ @ @ @ @ 's your job ! However , we can direct you to good online resources . <p> Keep it friendly and G-rated . We do n't want to see you on webcam ! <p> No flaming ! We 're here to answer specific questions , not to debate you about technology . <p> If you had n't  heard yet , Beta 2 of AIR 3.0 and Flash Player 11 are now availabe on Adobe Labs . The AIR 3.0 beta release is sporting some great new features , including hardware accelerated video playback for mobile , iOS background audio , android licensing support , front-facing camera support , encrypted local storage for mobile , H.264 software encoding for desktop applications , and last , but not least , captive runtime support for desktop and Android applications . <p> If you are wondering what " captive runtime support " is , then I 'll try to explain Currently all AIR applications that are deployed on the desktop and in Android require the 3rd-party Adobe AIR runtime . If you are familiar with the process for developing mobile AIR applications for Apples iOS devices @ @ @ @ @ @ @ @ @ @ n't  require the 3rd-party runtime ; they are completely self-contained applications . These AIR applications for iOS already take advantage of the captive runtime . All necessary components of the AIR framework are bundled into a self-contained , compiled distributable application that has no dependence upon other frameworks . <p> With AIR 3.0 , you will have the option to bundle the AIR framework into your applications to eliminate the 3rd-party dependency . However , one thing to keep in mind is that you can only export mac application packages on Macs and Windows EXEs on Windows . You ca n't target native installers or bundled runtimes for cross-platform development . You can only have a single app that targets both platforms if you export a . AIR file ( which requires the 3rd-party AIR runtime ) . <p> I recently had the opportunity to start playing around with Molehill for mobile ( hardware accelerated 3D graphics ) , and I think it is something that many of you will be very excited about ( see video below ) . Everyone thinks that 3D is just for games , but its @ @ @ @ @ @ @ @ @ @ , scientific modeling , or a whole host of other subject areas . <p> I started down the path of creating 3D charts that are capable of rendering on mobile , using the Away3D engine . This is really just exploratory work , and I have it rendering multiple axes with 250 spheres ( individual data points ) , which is about 70K-90K polygons drawn onscreen at any given time . Check out the preview below : <p> Molehill ( Stage3D ) on Mobile <p> This preview is running on a Motorola Atrix , and Samsung Galaxy Tab 10.1 with a prerelease AIR runtime more devices will coming soon . Source code will be available once molehill for mobile has been released . 
@@106848950 @2248950/ <h> All posts by Andrew <p> I recently purchased a Beholder Lite camera gimbal for my DJI Phantom quadcopter , and I am very pleased with it . Bottom line for those that do n't  want to read this entire post The Beholder Lite is hard to beat for the price , as long as you have some time to tune the gimbal. - Output video is very steady when flying reasonably , and still images are far more crisp than they are without a gimbal . The final output is not quite as good as the- H3-2D Zenmuse , but it is still much more than acceptable. - Plus , you can buy almost 4 Beholder Lites for the cost of one Zenmuse . However , when flying aggressively , there is a lot of vibration . Read more for an explanation , pros &amp; cons , plus tips for set up and some sample videos . <p> DJI Phantom with Beholder Lite Gimbal <h> Background <p> Um What is a camera gimbal ? <p> First , a gimbal is : " a- pivoted support that allows the @ @ @ @ @ @ @ @ @ @ set of three gimbals , one mounted on the other with- orthogonal- pivot axes , may be used to allow an object mounted on the innermost gimbal to remain independent of the rotation of its support . " <p> Basic Gimbal ( photo : Wikipedia ) <p> A camera gimbal is a set of these gimbal supports that enables the cameras movement to be independent from the support structure . In this case , the camera gimbal allows the cameras orientation to be independent from the orientation of the helicopter . Check out this video for more detail : <h> The Beholder Lite Gimbal <p> The Beholder Lite gimbal is a direct-drive brushless gimbal . This means : 1 ) the motors are brushless motors , and 2 ) that the motors directly drive the support arms for the gimbal ; there are no servo arms or other moving parts in the gimbal assembly . Brushless motors are faster than traditional/brushed servo motors , so they offer a smoother response and better stabilization . I have used the Phantom with no gimbal , as well as with a brushed/servo gimbal @ @ @ @ @ @ @ @ @ @ best quality . <p> I mentioned that I am very happy with this gimbal , though it has not been without its own hiccups . <h> Pros : <p> Very stable footage ****once properly installed and tuned**** <p> Still images are more crisp <p> Videos are far more stable , though aggressive flying will cause significant vibration ( this happens with all gimbals , though some more than others ) <p> Very affordable compared to similar gimbals on the market . <h> Cons : <p> Installation instructions are not very good . They are based on pictures only , and do not clearly identify motor orientation or wiring . I originally had the motors in the wrong direction , and the motor polarity reversed . This third-party post ( with video ) was very helpful for installation . Ive provided additional installation details at the bottom of this post . <p> The gimbal is marketed as " ready to go " you just assemble it and start flying . Once assembled , I had lots of vibration to the point of being unusable . I seriously considered returning it . @ @ @ @ @ @ @ @ @ @ could get some decent footage . This was both physical ( in the mounting ) , and in software configuration . I had to adjust the output gains before the gimbal would provide stable footage ( details below ) . <p> The vibration absorbing mounts are too soft . You will get vibration from the camera oscillating below the copter . Luckily this can be corrected but sticking some foam ear plugs inside the vibration mounts ( see details below ) . <p> Both aggressive flying and high-wind environments will cause additional vibration in both the copter and gimbal , so keep those in mind when you are filming. - <h> Sample Video Footage <p> I still do n't  feel like I have the gimbal settings ( gains ) 100% dialed-in , but I have it close enough to get some high quality footage . I have found that the 1080p@60fps video mode on the GoPro camera has the best results . 2.7K@30fps has too much rolling shutter effect for the footage to be really usable . I also have not balanced the propellors on my copter , which will @ @ @ @ @ @ @ @ @ @ samples from the camera/gimbal combination . All of this footage was captured with a GoPro Hero 3 Black Edition . <p> 1080p@60fps with Post Processing via Adobe Premiere : I still think I can get it more stable by changing a few editing parameters and changing the sequence of my scaling and effects , but I am very happy with this output . <p> 2.7K@30fps with 10% Warp Stabilization The footage is far better than it is without a gimbal , but there 's still some rolling shutter effect evident ( capturing Tonys hex liftoff ) . <p> Aggressive Flight : This footage is completely raw , without any post-processing or stabilization . This shows output when flying aggressively . This was *very* aggressive flight full speed in ATTI mode . I recommend that you *never* attempt to shoot professional footage flying like this : <p> Pay particular attention to motor orientation and wiring . The motors look symmetrical , but their weight and operation is not . The pitch motor should have the wire coming out the side that is closest to the copter . The roll motor should have @ @ @ @ @ @ @ @ @ @ . If you have your motor polarity reversed , you 'll know b/c the gimbal will vibrate back and forth . This wont hurt it , and you can just reverse the wires easily if this happens . In the picture below you can see my motor orientation : <p> Motor Orientation and Wiring <p> Very important : make sure your gimbal is balanced . If the power is off , the GoPro should not fall in any direction . It should just stay where it is . Having it perfectly balanced is key to having smooth footage . <p> The rubber vibration dampeners are too soft for the gimbal . If you fly it " stock " , you will get a TON of vibration from the gimbal oscillating under the copter . Use some soft foam earplugs , roll them up , and put them inside of the dampeners . I used the cheap foam orange ones you get from any drug store , and they work great . I put one in each of the 6 vibration dampeners . Another picture below , so you can see the @ @ @ @ @ @ @ @ @ @ necessary , but I also put Moongel in between the gimbal and the copter body to absorb vibrations , and I also put it on top of the gimbals gyro board to absorb vibrations that may affect gimbal orientation ( see picture below ) . <p> Beholder Lite on DJI Phantom <p> Do not get the white wires too close to the power wires or to the motors . They are very light weight , and can get interference from power wires , motors , and motor wires . You will get inexplicable vibrations if this occurs . <p> One last thing If you use zip ties on the gimbal for extra safety , keep them very loose . If you compress the dampeners , the bottom plate will hit the controller board , and cause significant interference with gimbal operation . Everything will be fine one second , then completely out of control the next . <h> Battery Life <p> Before having a gimbal , I could easily get 12+ minutes of flight time per battery . With a servo-based gimbal , I could get about 10+ minutes per @ @ @ @ @ @ @ @ @ @ max of 8 minutes per battery . I set a timer for 7 minutes , and be sure to bring it down as soon as the time goes off . However , this seems comparable to battery life with other brushless gimbals that friends/coworkers use . The decreased battery life is due to additional weight of the gimbal , plus the additional power consumption from the gimbal motors . <h> What Next ? <p> Use Creative Cloud to process all of your video and images to make them the best they can be ! - Here are some very useful posts for processing your content with this configuration ( or similar configurations ) <p> One criticism of PhoneGap apps that I sometimes hear is that they often do n't  have " standard " features from the native operating system . - Little things , like iOSs ability to scroll a container back to the top , just by tapping on the operating systems status bar . These types of features are not hard to add to a PhoneGap application , at all . This is more of an " attention @ @ @ @ @ @ @ @ @ @ ca n't do . <p> Since this is n't a feature that is applicable on all platforms , and it can vary per PhoneGap app implementation , it is not part of the core PhoneGap/Cordova download . - However , this can be very easily added via a native plugin. - Native plugins enable you to access native code , or augment the capabilities of PhoneGap for a particular platform . <p> If you download the app from the app store today , you wo n't see this yet because I literally *just* submitted it to Apple . <p> So how did I do this ? <p> It was n't hard . The first thing to do is check and see if there was an existing native plugin that has already been created by someone in the PhoneGap/Cordova community . - It turns out , Greg- pointed out- one that already existed . - Since this plugin was built targeting an older version of PhoneGap , and my project was built using PhoneGap 3.0 , I had a few minor updates . - Though , I was able to get everything all set up @ @ @ @ @ @ @ @ @ @ , in your PhoneGap application , you just have to add an event listener for the " statusTap " event , which is triggered when the user taps on the operating systems status bar . - It is literally this simple : <p> This just shows an alert that the status bar was tapped . If you want to animate specific containers , you have to do this manually yourself via JavaScript . Again , that is n't hard to do . here 's an excerpt that I used from the Halloween app , using jQuery syntax : <p> With Halloween less than a month away , I figured its about time to update my " Instant Halloween " PhoneGap sound effects app . I 'm happy to say that the latest version is now out for both iOS and Android . It has a few new sounds , a new UI style , and has been updated for iOS 7 . I also updated the low latency audio plugin- so it now supports PhoneGap 3.0 method signatures and supports the command line tooling for installation . <p> This app is fantastic for @ @ @ @ @ @ @ @ @ @ , and start playing the sounds to your hearts content . Its got everything from background/ambient loops to maniacal laughter , screams , ghosts , zombies , and other spooky sound effects . <p> It is available now , for FREE , for both iOS ( 5.0+ ) and Android ( 4.0+ ) . <p> - <p> So what has changed in this version ? <p> First , I updated the app to support iOS 7 . For the most part , this is a non-issue . PhoneGap apps are based on web standards , and HTML/JS/CSS work pretty much everywhere . However , you do have to account for a few minor changes . One is that the OS status bar now sits over top of the application . You 'll need to update your UI on iOS 7 , so there are no UI issues . Check out this post from Christophe Coenraets for details regarding creating PhoneGap apps for iOS 7 . <p> iOS 7 also introduces some new UI design paradigms and guidelines . I simplified the user interface , got rid of all textures , and @ @ @ @ @ @ @ @ @ @ as possible . I also got rid of iScroll for touch-based scrolling both the iOS and Android versions now use native inertial based scrolling from the operating system . This is the reason that the new Android version is only Android 4.0 and later , but it is also the reason that the app feels much closer to a fully native experience . <p> I updated the low latency audio native plugin to support PhoneGap PhoneGap 3 . There were two parts to this : First , there is the updated method signature on the native interfaces . I just took the old plugin , and updated it for the new method signature . The new method signature- was actually introduced a while back , but I never updated the plugin for it. - Second , I added the appropriate XML metadata to enable CLI-based installation of the plugin for both iOS and Android . Take a look at the PhoneGap documentation for details on creating PhoneGap native plugins- and plugin.xml. - Also check out this tutorial from Holly Schinsky for help creating native plugins for Android . <p> In @ @ @ @ @ @ @ @ @ @ with Android deployment Back in the spring I had a corrupted hard drive . I was able to recover *most* of my data , and I thought I had all of my app signing keys . It turns out that for Android , your apps must not only have the same signing keys , but also the same key store when you sign the APK files , or else Google wont let you distribute the Android APK file as an update ; it must be a new app . It turns out that I had recovered the key , but not the keystore . So , I had no choice but to distribute it as a new app . <p> Go download the free apps , get the source code , and start building your own PhoneGap apps today ! <p> Adobe has introduced a bunch of new features in the Photoshop CC releases over the course of the year . One feature that I knew about , but had n't  used much myself was the new " preserve details " upsampling algorithm when resizing images , which was introduced @ @ @ @ @ @ @ @ @ @ looking into resizing one of my aerial photos of San Francisco for printing , and I was blown away at the output quality when upsampling my image . <p> Aerial Panorama of San Francisco <p> I stitched together the panorama from 6 or 7 images captured with a GoPro and DJI Phantom quadcopter ( details here ) . The original panorama image size was- 4746+1706 pixels I wanted to see how big I could make it for large-scale printing , and I upscaled the image by 469% to 22256+8000 using Photoshops image resizing features . The results are very impressive . <p> Here are a few side by side comparisons of different resampling/upscaling algorithms , the new " Preserve Details " algorithm is the last one . While none of them is perfect , the output of the new " preserve details " algorithm produced very good/high quality results . You can adjust the " reduce noise " slider to tweak the resizing algorithms output to minimize visual artifacts that may come from the up-sampling process . <p> You can check out the final output by interacting with the image @ @ @ @ @ @ @ @ @ @ just by interacting with the image directly . When you zoom in really far , you can see its not going to create details that werent there in the original picture , but you 'll quickly see this can be immensely useful for upscaling smaller images for printing or other uses . - As a simple game , see if you can spot the Adobe San Francisco office ! <p> Check out more details on the Preserve Details upscaling feature in the video below , from Adobe TV : <p> In this session , we will cover the Adobe Creative Cloud Packager , which makes it easy for IT to create packages that contain Creative Cloud products and updates for their teams or entire organizations . 
@@106848951 @2248951/ <h> Category Archives : Cognitive Computing <p> Its not every day that you get the opportunity to have your work showcased front and center on the main landing page for one of the largest companies in the world . Well , today is definitely my lucky day . I was interviewed last month about a drone-related project that I 've been working on that focuses on insurance use cases and safety/productivity improvement by using cognitive/artifical intelligence via IBM Watson . I knew it was going to be used for some marketing materials , but the last thing that I expected was to have my image right there on ibm.com . I see this as a tremendous honor , and am humbled by the opportunity and exposure . <p> here 's an interview that I recently did with IBM DeveloperWorks TV at the recent World of Watson conference . In it I discuss a project Ive been working on that analyzes drone imagery to perform automatic damage detection using the Watson Visual Recognition , and generates 3D models from the drone images using photogrammetry processes . The best part the entire thing @ @ @ @ @ @ @ @ @ @ been a while since I 've posted here on the blog - In fact , - I just did the math , and- its been over 7 months. - Lots of things have happened since , I 've moved to a new team within IBM , built new developertools , worked directly with clients- on their solutions , worked on a few high profile keynotes , built apps for kinetic motion and activity tracking , built a mobile client for a chat bot , and even completed some new drone projects . - Its been exciting to say the least , but the real reason I 'm writing this post is to share a few- of the public projects Ive been involved with from recent conferences . <p> I recently returned from Gartner Symposium and IBMs annual World of Watson conference , and- its been one of the busiest , yet most exciting span of two weeks Ive experienced- in quite a while . <p> At both events , we showed a project Ive been working on with IBMs Global Business Services team that focuses on the use of small consumer drones and @ @ @ @ @ @ @ @ @ @ , by leveraging IBM Watson to automatically detect roof damage , in conjunction with photogrammetry to create 3D reconstructions and generate measurements of afflicted areas to expedite and automate claims processing . <p> This application leverages many of the services IBM Bluemix has to offer on-demand CloudFoundry runtimes , a Cloudant NoSQL database , scalable Cloud Object Storage ( S3 compatible storage ) , and BareMetal servers on Softlayer . Bare Metal servers are *awesome* I have a dedicated server in the cloud that has 24 cores ( 48 threads ) , 64 GB RAM , RAID array of SSD drives , and 2 high end multi-core GPUs . Its taken my analysis processes from 2-3 hours on my laptop down to 10 minutes for photogrammetric reconstruction with Watson analysis . <p> Its been an incredibly interesting project , - and you can check it out yourself in the links below . <h> World of Watson <p> World of Watson was a whirlwind of the best kind I had the opportunity to join IBM SVP of Cloud , Robert LeBlanc , on stage as part of the the Cloud keynote @ @ @ @ @ @ @ @ @ @ 20,000 people ) to show off the drone/insurance demo , plus 2 more presentations , and an " ask me anything " session on the expo floor . <p> You can also check out my session " Elevate Your apps with IBM Bluemix " on UStream to see an overview in much more detail : <p> .. and that 's not all . I also finally got to see a complete working version of the Olympic Cycling teams training app on the expo floor , including cycling/biometric feedback , video , etc I worked with an IBM JStart team and wrote the video integration layer into for the mobile app using IBM Cloud Object Storage and Aspera for efficient network transmission . <h> Drones <p> On this project we 've been working with a partner DataWing , who provides drone image/data capture as a service . However , I 've also been flying and capturing my own data . The app can process virtually any images with appropriate metadata , but I 've been putting both the DJI Phantom and Inspire 1 to work , and they 're working fantastically . <p> Last week I @ @ @ @ @ @ @ @ @ @ largest conference of the year . With over 20,000 attendees , it was a fantastic event that covered everything from technical details for developers to forward-looking strategy and trends for C-level executives . IBM also made some big announcements for developers OpenWhisk serverless computing and bringing the Swift language to the server just to name a few . Both of these are exciting new initiatives- that offer radical changes &amp; simplification to developer workflows . <p> It was a busy week to say the least lots of presentations , a few labs , and even a role in the main stage Swift keynote . You can expect to find more detail on each of these here on the blog in the days/weeks to come . <p> For starters , here are two " lightning talks " I presented in the InterConnect Dev@ developer zone : <h> Smarter apps with Cognitive Computing <p> This session introduces the concept of cognitive computing , and demonstrates how you can use cognitive services in your own mobile apps . - If you are n't  familiar with cognitive computing , then I strongly recommend that @ @ @ @ @ @ @ @ @ @ Computing . <p> In the presentation below , I show two apps leveraging services on Bluemix , IBMs Cloud computing platform , and the iOS SDK for Watson . <p> Actually , I 'm using two Watson SDKs The older Speech SDK for iOS , and the new iOS SDK. - I 'm using the older speech SDK in one example because it supports continuous listening for Watson Speech To Text , which is currently still in development for the new SDK . <h> Redefining your personal mobile expression with on-body computing <p> My second presentation highlighted how we can use on-body computing devices to change how we interact with systems and data . - For example , we can use a luxury smart watch ( ex : Apple Watch ) to consume and engage with data in more efficient and more personal ways . - Likewise , we can also use smart/wearable peripherals devices to access and act on data in ways that were- never possible- before . <p> For example , determining gestures or biometric status based upon patterns in raw data transmitted by the on-body devices . - For @ @ @ @ @ @ @ @ @ @ The IBM Wearables SDK provides a consistent interface/abstraction layer for interacting with wearable sensors . - This allows you to focus on building your apps that interact with the data , rather thank learning the ins &amp; outs of a new device-specific SDK . <p> The wearables SDK also users data interpretation algorithms to enable you to define gestures or patterns in the data , and use those patterns to act upon events when they happen without additional user interaction . - For example : you can determine if someone falls down , you can determine when someone is raising their hand , you can determine anomalies in heart rate or skin temperature , and much more . - The system is capable of learning patterns for any type of action or virtually any data being submitted to the system . - Sound interesting ? - Then check it out here . <p> I also had some other- sessions- on integrating drones with cloud services , integrating weather services in your mobile apps , and more . - I 'll be sure to post updates for this- content- I make them @ @ @ @ @ @ @ @ @ @ on drones + cloud especially interesting I know I did . <p> IBM Watson services , which are based on machine learning algorithms , - give you the ability- to handle unstructured data , like text analysis or- translation , speech processing , and more . - This makes consumption , mining , or responding to unstructured data or " dark data " faster , more efficient , and more powerful than ever . <p> The new Watson iOS SDK- provides- developers with an API- to simplify integration of the- Watson Developer Cloud services into their mobile apps , including the Dialog , Language Translation , Natural Language Classifier , Personality Insights , Speech To Text , Text to Speech , Alchemy Language , or Alchemy Vision services " all of which are available today , and can now be integrated with just a few lines of code . <p> The- Watson iOS SDK makes integration with Watson services significantly *really* easy . For example , if you want to take advantage of the Language Translation service , you first have to setup a service instance . Once the translation @ @ @ @ @ @ @ @ @ @ leverage translation capabilities within your mobile app : <p> Be sure to check out the sample 's readme for additional detail and setup instructions . As with all of the Watson services , You must have a service instance properly configured , with authentication credentials- in order to be able to consume it within your app. 
@@106848952 @2248952/ <h> Tag Archives : apps <p> That title get your attention ? - Yes , it really read " Adaptive- mobile- apps that- change based on personal context " with near real-time rules application , without much extra development effort . - If that sounds interesting to you , or like a product you might want to use within your own apps , then you might want to check out this site where you can get involved in the products development : - http : **35;8324;TOOLONG <p> IBM is looking for your input on creating these types of mobile app experiences. - User experiences within a single app that can be- dramatically different per user based on location , past behavior , profile information , social media activity , - and so much more . - With this behavior being driven by configurable rules that can be changed without redeploying an app to the app store . <h> How it works for your customer <p> Consider this scenario : <p> Jon and Andrea download the mobile app for S&amp;W , a retailer known for its attention to providing great @ @ @ @ @ @ @ @ @ @ Andrea use the app to browse and discover content and merchandise differently . <p> Jon primarily navigates to sports related content for his favorite teams to find gear and clothes for travel to his favorite team 's games . Andrea scours the app for sales and fashion trends and usually ends up following her favorite designers . <p> Andrea and Jon go to a baseball game together . She 's never enjoyed watching it , so she opens up the S&amp;W app to entertain herself , and her app 's navigation quickly steers her through Spring fashion articles . <p> Jon however , wants to replace the hat he 's worn the last three times the team lost , and since he 's in the stadium , his S&amp;W app opens right up to the team 's gear page . The app knows he 's out of town and tells him how to get to an S&amp;W store . <h> How it works for the dev team <p> Consider another scenario : <p> One of the developers on the team , George , sets up the system and application . @ @ @ @ @ @ @ @ @ @ the customer experience . <p> Janet writes rules defining how the application could adapt and become more personalized based on inputs like , social media , geolocation , app usage , or customer information data . <p> Once Janet has built out her rules , she simply hits Submit ' and can immediately see her clever interactions reflected in the mobile application without having to involve the development team . <p> Analytics let Janet know which adaptations are working best , and helps her find new opportunities to optimize the app 's user experience . <p> Were not talking about a content management system , or translation based on locale , instead a rules-driven product that can adapt literally every aspect of your app : - customize the user interface , enable or disable different features , customized messaging and notifications , and much more , all variable based upon the user context . - This can be used to present- contextually relevant information , drive adoption , provide more/less data depending on your physical context , and so much more . <p> It wont be tied to a specific @ @ @ @ @ @ @ @ @ @ management system , is n't attempting to re-create Google Now or Apple Proactive Assistance . - Rather , a set of tools and a rules engine that enable you to customize and tailor the app experience to the individual user . <p> This post specifically covers- native iOS , though there are also Android and hybrid options available . This should have everything you need to get started . It covers all aspects- from creating the app , to updating the back end , to leveraging Cloudant storage , push notifications , and monitoring &amp; logging . <p> So , without further ado , let 's get started <h> Part 1 : Getting Started with Bluemix Mobile Services <p> In this first video I show how to create a new mobile app on Bluemix , connect to the cloud app instance , and implement- remote logging from the client application . This process is covered in more detail in the Getting Started docs , but below- are the basics from my experience . <p> Youll- first need to- sign into your Bluemix account . If you do n't  already have one , @ @ @ @ @ @ @ @ @ @ you 're signed in , you just need to create a new mobile app instance . <p> The process is very simple , and there is a " wizard " to guide you . The first thing that you need to do is create a new app by clicking the big " Create an App " button on your bluemix dashboard . <p> Create a new app from IBM Bluemix Dashboard <p> Next , select which kind of app you 're going to create . For MBaaS , you 'll want to select the " Mobile " option . <p> Select the type of app <p> Next you 'll need to select your platform target . You can choose either " iOS , Android , Hybrid " , or the " iOS 8 beta " target . In this case I chose the iOS 8 beta , but the process is similar for both targets. - Hybrid apps are built leveraging the Apache Cordova- container . <p> Select your platform target <p> Next , just specify an app name and click " Finish " . <p> Give your app a name <p> Once your @ @ @ @ @ @ @ @ @ @ how to connect the app in Xcode . I 'll get to that in a moment <p> Now that- your app has been created , you 'll be able to see it on your Bluemix dashboard . This app will consist of several components : a Node.js back-end instance , a Cloudant NoSQL database instance , an Advanced Mobile Access instance , and a Push instance . The Advanced Mobile Access component provides you with app analytics , user auth management , remote logging , and more . The Push component gives you the ability to manage and send push notifications ( either manually , or with a rest-based API ) . <p> You app has been created here are the components and the activity <p> Once your app has been created , you will need to setup the mobile app to connect to Bluemix to consume the services . Again , this is a very straightforward process . <p> The next step is to register your client application . Once your app is created , you will be presented with a screen to do this . If you do n't  complete @ @ @ @ @ @ @ @ @ @ and register an application . You 'll need to specify the Bundle I 'd and version of your app , then you can setup any authentication ( if you choose ) . <p> Register your apps bundle I 'd and version <p> Once your app has been registered , you need to configure Xcode . Youll first need to create a new project in Xcode . There are two options for configuring your Xcode project : 1 ) automated installation using CocoaPods , or 2 ) manual installation . I used the CocoaPods installation simply because it is easier and manages dependencies for you . <p> If you are n't  familiar with CocoaPods , it is much like NPM CocoaPods is a dependency manager for Cocoa projects . It helps you configure- the Bluemix libraries and manages dependencies for you . <p> If you 've got Xcode up and running , then close it and install CocoaPods , if you do n't  already have it . Next open up a terminal/command prompt , go to the directory that contains your Xcode project and initialize CocoaPods- using the " setup " - command : <p> @ @ @ @ @ @ @ @ @ @ " podfile " . Open this file in any text editor and paste the following ( note : you can remove any lines that you do n't  want to actually use ) : <p> source ' https : **32;8361;TOOLONG ' # Copy the following list as is and # remove the dependencies you do not need pod ' IMFCore ' pod ' IMFGoogleAuthentication ' pod ' **25;8395;TOOLONG ' pod ' IMFURLProtocol ' pod ' IMFPush ' pod ' CloudantToolkit ' <p> Save the changes to the " podfile " file , and close the text editor . Then go back to your command promprt/terminal- - and run the installation process : <p> pod install <p> Your project will be configured , and all dependencies will be downloaded automatically . Once this is complete , open up the newly created . xcworkspace file ( Xcode Workspace ) . <p> You have to initialize the Bluemix inside of your application to connect to the cloud service to be able to take advantage of any Bluemix features ( logging , data access , auth , etc ) . The best place to put @ @ @ @ @ @ @ @ @ @ because it is the first code that will be run within your application : <p> One of the first features I wanted to take advantage of was remote collection of client-side logs . You can do this using the IMFLogger class , in much the same fashion as you do with OCLogger in MobileFirst Foundation server . Once great feature that requires almost no- additional configuration is the- **25;8453;TOOLONG method , which automatically configures the Advanced Mobile Access component to collect information for all app crashes . <p> Next , launch your app in the iOS simulator , or on a device , and you 'll see everything come together . Log into your Bluemix dashboard , and you 'll be able to monitor app analytics and remote logs . <p> Note : If you experience any issues connecting to the Bluemix mobile app from within the iOS simulator , just clear the iOS Simulator by going to the menu command " iOS Simulator -&gt; Reset Content and Settings " , and everything should connect properly the next time you launch the app . <h> Part 2 : Configuring the Node.js Backend @ @ @ @ @ @ @ @ @ @ grab the code for the backend Node.js application , create a git repository on IBM JazzHub , then pull the code for local development . <p> When the app is created , you 'll see an " add git " link under the app name . Using this link , you can create a git repository for the backend code . <p> Add a git repository <p> Once your git repo has been created , you can check out the code using any Git client ( I used the CLI ) . You 'll need to use the " npm install " command to pull down all the app dependencies . The biggest thing you need to know is that it uses express.js for the web application framework. - - You can make any changes that you want , and they will be automatically deployed to your Bluemix server instance upon commit . Yes , this workflow is also configurable b/c this process may not work for everyone . <p> One other thing that you will need to watch out for if you are doing local development : You will want to @ @ @ @ @ @ @ @ @ @ block , otherwise you will hit errors in the local environment which will prevent your app from launching locally : <h> Part 3 : Consuming Data from Cloudant <p> Another part of Bluemix mobile applications is the Cloudant NoSQL database . The Cloudant NoSQL database is a powerful solution that gives you remote storage , querrying , and client-side- data storage mechanisms with automatic online/offline synchronization , all with monitoring/analytics capabilities . <p> By default , objects within the Cloudant data store are treated as generic objects ( over-simplification : think of it is an extremely powerful JSON store in the cloud ) . However you can also serialize your objects to strong data types within the client code configuration . <p> In your AppDelegate class- application **29;8480;TOOLONG method , you 'll also want to initialize the IMFDataManager class , which is the class used for interacting with all Cloudant data operations . <p> IMFDataManager *manager = IMFDataManager sharedInstance ; <p> In my sample , I setup the database manually with open permissions , but you 'll probably want something more secure . Once your database is created , you can create @ @ @ @ @ @ @ @ @ @ <p> In the following code , I create a search index and query for data from the remote Cloudant database . You really only need to create the index if it does n't  already exist . You can do this either through the mobile app code , or manually through the Cloudant databases web interface . I did this inline in the following code , just for the sake of simplicity : <h> Part 4 : Push Notifications <p> The IBM Bluemix mobile services app also contains a component for managing push notifications within your mobile applications . With this service , you can send push notifications to a specific device , a group of devices using tags , or all devices , and you can send push notifications either manually via the web interface , or as part of an automated workflow using the REST API . <h> Part 5 : Monitoring and Logging <p> Did I- mention that every action that you perform through Bluemix Mobile Services can be monitored ? Analytics are available for the Advanced Mobile Access component , the Cloudant NoSQL data store , and @ @ @ @ @ @ @ @ @ @ have remote collection of client logs and crash reports . This provides- - unparalleled insight into- the health of your applications . <p> Push notifications , love them or hate them , are everywhere and there 's no getting around it . Push notifications are short messages that can be sent to mobile devices regardless of whether the apps are actually running . They can be used to send reminders , drive engagement with the mobile app , notify completion of long running processes , and more . Push notifications- send information to you in real time , rather than you having to request that information . <p> If you are building a back-end infrastructure to manage your applications data , and you want to leverage push notifications , then guess what ? You also have to build the hooks to manage subscription and distribution of push notifications for each platform . <p> The- unified- push notification API allows you to develop your app against a single API , yet deliver push notifications to multiple platforms , and it works with both hybrid ( HTML/CSS/JS ) apps , as well as @ @ @ @ @ @ @ @ @ @ the logic to subscribe devices for messaging , and dispatch push notification messages , but you 'll only have to do it once against the unified API not once for each platform . <p> The apps that I showed in the video above are sample apps taken straight from the IBM MobileFirst platform developer guide for iOS and Android , and can be accessed in their entirety ( with both client and server code ) using the links below : <p> On the client app , you 'll need to subscribe for messages from the event source . See the hybrid or native code- linked to above for syntax and examples . <p> Once your clients are subscribed , you can use a single server-side implementation to distribute messages to client apps . Below is an- excerpt from the sample application which demonstrates sending a push notification to all devices for a particular user ( on any platform ) : <p> From- the MobileFirst console , you will be able to monitor and manage event sources , platforms , and the devices that are consuming push notifications . <p> Push Notifications on @ @ @ @ @ @ @ @ @ @ , these can be cloud-hosted on IBM BlueMix and yes , it can also be installed on-premise on your own server in your data center . - You have the option to configure- your physical or cloud servers however you want . 
@@106848953 @2248953/ <p> Once you are a registered developer , you will have access to tools that enable you to put your apps onto a nook device . - First , you must download and install the Nook SDK add-on to the Android SDK . <p> Nook SDK Setup <p> This will download and install the Nook SDK. - Once installed , the installation process will need to restart ADB. - Click " YES " and allow it to proceed through the restart process . <p> Nook SDK Setup - Complete <p> Once you are a registered developer , you will have tools accessible to enable " Developer Mode " on your device , which allows you to provision the device for development . - Go to the " Developer Mode " section of the nook developer portal and enter the device names and serial numbers for your dev devices . - From here you will be able to download a provisioning file . <p> Nook Developer Provisioning <p> The provisioning file will allow you to deploy applications directly to the nook via USB. - - Attach the Nook Color device @ @ @ @ @ @ @ @ @ @ Nook Color device will show up as a device in Finder. - Copy the provision.cmd file that you downloaded from the " Developer Mode " provisioning portal directly into the root of the Nook Color device . <p> Once ADB has restarted , use the " adb devices " command to list all connected Android devices ( The Nook Color device must still be connect via USB , even though it has been unmounted ) . <p> . /adb devices- <p> You should see your device listed in the output : <p> ADB - View Device <p> Now that you are able to view your Nook Color device using ADB , Flash Builder will also be able to deploy to it . - You will be able to Run/Debug directly from Flash Builder and launch applications on the Nook Color device . - - This follows the normal Flash Builder debug process . <p> However , there is one trick you may notice that if you go to the " apps " menu on the Nook color , you wo n't see any of your applications deployed via USB. - Do @ @ @ @ @ @ @ @ @ @ There is just a trick to view them . - From the " apps " menu , click on the " archived " button . - A popup will be displayed here . - Now , press and hold the " volume up " button . - When the speaker appears on the screen ( continue holding the volume button ) , tap on the speaker . - When you tap on the speaker , the " extras " screen is displayed . - ( You can release the volume button now. ) - The " extras " screen will list all of your manually installed applications , and you will be able to re-launch your installed apps . <p> From here , you can generate your release-build APK as you normally would from Flash Builder. - Once you have your APK generated and tested , you 're ready to prepare it for the Nook Store . - Sign into the Nook Developer Portal , and go to the " Applications " section . - Click on the " Add New Application " button . <p> First , you will need @ @ @ @ @ @ @ @ @ @ , type , price , version , etc ) . <p> Nook Portal - New Application <p> Once you have completed the basic information , click on " Save and Continue " , and you will be redirected to the " Description &amp; EULA " tab . - Enter an application description and an optional license agreement ( I left this blank b/c I do not have any special terms for my app ) . <p> Nook Portal - App Description <p> When ready , click " Save and Continue " to enter the " Keywords and Category " tab . - On this screen , enter descriptive keywords and categories to help categorize your application within the Nook store . <p> Nook Portal - Keywords &amp; Category <p> When you have your categorical information all set , click " Save and- Continue " to proceed to the " Icons &amp; Screenshots " screen . - Here you will need to set the application icon and add application screenshots . <p> Nook Portal - Media &amp; Images <p> Now this is where the application approval process is a little different @ @ @ @ @ @ @ @ @ @ " Send for Application Approval " , and the application metadata must be approved before you can upload a binary APK file . <p> Once the application metadata is approved ( this took about a week for my app ) , you will be able to upload the APK binary . <p> Nook Portal - Upload Binary <p> After you have uploaded and submitted the binary APK file , the binary file will need to be approved before it is actually available within the Nook store . - The binary approval took another week+ , so the overall approval process took over 2 weeks . - Once approved , your application will appear in the nook store , and will be ready for public consumption ! <p> First , navigate to- https : //developer.amazon.com/- and click on the " Amazon Appstore for Android " link . Once there , you will need to walk through the full registration process to create your account . <p> Amazon Appstore Landing Page <p> Once you click on the " Get Started " button , you will be guided through the Appstore registration process @ @ @ @ @ @ @ @ @ @ to the Appstore developer portal home page . - From here , just click on " Add a New App " . <p> Amazon Appstore Home <p> Next , you will being the App upload wizard . - First , you will need to enter primary metadata for the application , including a title , form factor , supported languages , and contact information . - Once you have entered this information , click on the " Save " button . <p> Amazon Appstore App Metadata <p> Next , you need to enter merchandising information . - This includes the app category , keywords , a description , price , and release/availability dates . - Once you have completed all of your merchandising information , click on the " Save " button to proceed . <p> Amazon Appstore App Merchandising Info <p> Next , you will have to specify content rating information . - Just fill out the information about your content , and click on the " Save " button to proceed . - I did n't  run into any content rating issues in the Amazon Appstore , like @ @ @ @ @ @ @ @ @ @ Appstore App Content Ratings <p> Next , upload multimedia that will be associated with the application . - This includes application icons ( note : they must match , event though this screenshot does n't  show it I was rejected b/c of this ) , and actual screenshots of the application . <p> Amazon Appstore App Multimedia <p> Scroll down to see more of the " Multimedia Content " form . - You will also be able to enter promotional images and promotional video assets for your application . - Once you have uploaded all necessary multimedia , click the " Done " button . <p> Amazon Appstore App Promotional Media <p> Now you are ready to upload your APK Binary . - Follow the instructions for uploading an APK file . - Once it is uploaded you will see information about the file that was uploaded . <p> Amazon Appstore App Binary <p> Finally , click on the " Submit App " button to submit your application for approval . - There is an approval process for the Amazon Appstore similar to Apples , and my application was live @ @ @ @ @ @ @ @ @ @ you have successfully uploaded the application APK , you will be prompted with the details about your application . - Click " Save " to being entering application metadata information . <p> Android Market APK Uploaded <p> You will need to upload at least 2 screenshots of your application , as well as an application icon for use within the Android Market . <p> Android Market APK Metadata <p> Next , scroll down to the " Listing Details " area and begin entering metadata for your application . - This includes an application title and description , as well as release notes , classifications , and promotional text . <p> Android Market APK Metadata - Description <p> Next , scroll down to the " Publishing Options " screen , where you can configure copy protection settings , content ratings , pricing , and available markets . <p> Once you have completed the form in its entirety , scroll back up to the top of the page and click on the " Save " button to save content , or " Publish " to save and publish the application to the @ @ @ @ @ @ @ @ @ @ , your application will show up in your home dashboard , as shown below . <p> Android Market APK App Ready <p> That 's it ! - The Android Market makes it extremely easy to publish your application . - My application started showing up in the market after about 30 minutes from pressing the " Submit " button . <p> In this post , we will cover the workflow for delivering a solution from development in Flash Builder through packaging for various Android ecosystems . - In subsequent posts , this will be followed up with detail for delivering to each individual ecosystem . <p> You can start developing apps for Android using the debugging simulator built into Flash Builder . However , a critical step in your development process is that you must test and evaluate your applications on a physical device ( preferably before you try to ship it ) . - Luckily for all of us , that is really easy with Flash Builder . <p> First , develop your mobile application . - When you are ready to test it on a device , simply go @ @ @ @ @ @ @ @ @ @ Configurations " . <p> Make sure that your test device(s) are plugged in via USB , and then hit the " Debug " button . - Using this approach , you can deploy to any Android device that has USB Debugging enabled . - This applies to phones and tablets to enable USB debugging , you just need to go to Settings -&gt; Applications -&gt; Development , and then select the checkbox to enable USB debugging . - - You can also use this method to debug on a Nook Color device , however you must have it unlocked already via your Nook developer account . <p> Once you have developed and debugged your application , you are ready to export it for real world consumption . - - The first thing you need to do is specify you applicaiton name , version and i 'd in your app.xml file . - Be sure that these are the final values , as the application i 'd will be the unique identifier for your application in all marketplaces and if you distribute the APK manually . <p> Application XML Configuration <p> Once your @ @ @ @ @ @ @ @ @ @ actually export the project . - Right-click on your project in the package explorer and go to the " Export " option . - You can also perform the same action by going to the File -&gt; Export menu . <p> Export Menu Options <p> Next , select the " Release Build " export option , and click " Next " . <p> Export Release Build <p> Set your export folder , and a base filename for the output file(s) , be sure to select the target platform " Google Android " , and export as " Signed packages for each target platform " . - Click " Next " to setup your platform- signing- options . <p> Export Release Build Options <p> For Android , you can use a purchased signing certificate , or use a self-signed certificate . - Just use the browser button to locate your signing certificate , or click " Create " to crate a certificate . <p> Export Release Build - Platform Signing Options <p> If you create your own certificate , the following window will be displayed . - Just enter a publisher @ @ @ @ @ @ @ @ @ @ a file location . - Next click " OK " and the certificate will be created and you will return to the Platform Signing Options screen . <p> Self Signed Certificate <p> Next , click " Finish " and your APK file will be generated . <p> Export Release Build - Platform Options <p> Now that you 've generated your APK binary file , you are ready to push to the various app stores Let 's take a look at this process for each ecosystem : <p> In this post , we will cover the workflow for delivering a solution from development in Flash Builder through publishing in Apples App Store . <p> You can start developing apps for iOS without an Apple developer account by using the debugging simulator built into Flash Builder . However , a critical step in your development process is that you must test and evaluate your applications on a physical device ( preferably before you try to ship it ) . <p> To get from Flash Builder onto an iOS device , the first thing that you will need to do is create a developer account @ @ @ @ @ @ @ @ @ @ profiles . To register for a developer account , visit developer.apple.com and walk through the registration process . Note : You can register for free , but you ca n't access any of the iOS deployment tools without the paid enrollment into the iOS developer program . <p> After you have an active iOS developer account , you will need to create a signing certificate and distribution profiles . From the iOS developer centers home page , click on the " iOS Provisioning Portal " link as shown below . <p> iOS Developer Home <p> Next , click on the " Certificates " link on the left side of the screen , as shown in the next screen capture . This will direct you to the certificate management area . <p> iOS Provisioning Portal <p> If you are only concerned about developing and not about distribution ( or if you are not the iOS account team agent and cant access distribution profiles ) , then you simply follow the steps for creating a development certificate . <p> If you will be distributing your application in the app store , via the @ @ @ @ @ @ @ @ @ @ program ) , or if you will be using ad-hoc distribution , then you will have a " Distribution " tab , as shown in the screen below . Distribution certificates are required for distribution profiles . Go ahead and click on the " Distribution " tab and follow the instructions for setting up a certificate . You will need to create a certificate signing request through Keychain Access , and then you will need to submit it through the Apple developer portal . <p> iOS Provisioning Certificates <p> Once you have successfully created a certificate , it will display under your " Current Distribution Certificate " heading within the " Distribution Certificates " tab , as shown below . <p> iOS Provisioning Certificates - With Certificate <p> Download the certificate and either drag or import the " distributionidentity.cer " file into the Keychain Access application . Once you have imported it , you should see the profile listed . - In the next screenshot , you can see that I have selected my " iPhone Distribution " certificate . - You will need to export the certificate to a @ @ @ @ @ @ @ @ @ @ Flash Builder . - Just select the certificate at the root level ( be sure to not just select the key ) . - Next , go to the file menu and click " Export Items " and save the certificate to a file . <p> Once you have created your signing certificate , you 'll next need to obtain your provisioning profile from Apple , in order to be able to actually deploy on any iOS devices . - If you are only using your registered iOS devices for local development and do not plan to distribute your application yet , then you can go to the Provisioning section of the iOS developer portal and download the development " Team Provisioning Profile " . - Click on the download link and save you . mobileprovision file locally , and then proceed on to setting up your debugging environment . <p> Download Development Profile <p> If you will be distributing your application , then you will need to first create an application I 'd . - All distribution provisioning requires an application I 'd . - Within the iOS Provisioning Portal , click @ @ @ @ @ @ @ @ @ @ sidebar , and then click on the " New App I 'd " button on the right side of the screen . <p> Create Application I 'd <p> From here , you need to create the actually application I 'd . - You need to enter a description ( what you commonly refer to the app as ) , and enter a bundle identifier . - For the description on my application , I entered " URL Monitor " for my description , and the bundle identifier is " **27;8511;TOOLONG " . - The bundle identifier is the unique reference to your application . <p> Create Application I 'd <p> Once youve create an App I 'd , next you will need to create a distribution provisioning profile . - Click on the " Provisioning " navigation item on the left sidebar , and then click on the " Distribution " tab across the top . - To create a new distribution profile , click on the " New Profile " button on the right side of the screen . <p> New Distribution Profile <p> Next , select your distribution type . - For most @ @ @ @ @ @ @ @ @ @ " and " Ad Hoc " . - For developers who have access to the enterprise developer program , there will be a third " Enterprise " option . - Note : - " Ad Hoc " distribution means that you can only distribute to a limited number of devices ; all of which need to be registered within your developer account . - Give your distribution profile a name ( I called mine " URL Monitor App Store " ) , and select the application I 'd that you just created . - Once you click submit , the distribution profile will be created . <p> Debugging : First things first , let 's cover debugging . I mean , cmon you cant distribute an app if you have n't debugged it first right ? - To debug , go the the debug menu and select the " Debug Configurations " option to create a new device configuration . <p> From that same screen , cick on the " Configure packaging settings " link to show iOS signing options . - In the screen shown below , select the . p12 certificate @ @ @ @ @ @ @ @ @ @ . mobileprovision provisioning file that you also just downloaded . <p> Apple Debug Settings <p> Click " Apply " to save these settings , then click " OK " to close this dialog . - You will be returned to the " Debug Configurations " dialog , and you can now click " Debug " to actually debug your application on your iOS device . - Flash Builder will not automatically deploy to your device . - It will create an IPA file , which you will need to drag into iTunes , and then synch your device to deploy it . - However , you can still actively debug your application over the local network , directly within Flash Builder . <p> Deployment Packaging : <p> First things first , setup your applications metadata within your app.xml file . - In particular , you will need to specify an application name , version number , and I 'd . - The I 'd MUST match the bundle identifier that you specified when creating the App I 'd through the iOS Provisioning Portal , and the version number must be a valid number @ @ @ @ @ @ @ @ @ @ on your project and select the " Export " menu option , as shown below : <p> Export <p> Select the " Release Build " option , and continue the wizard steps : <p> Export Release Build <p> Next , you will need to specify the export location ( " export to folder " ) and enter a filename . - Also , be sure to have the " Signed packages for each target platform " option selected . <p> iOS Export <p> Next , you will need to configure your distribution options . - Specify your certificate ( . p12 ) file location , your provisioning file ( . mobileprovision ) location , and select the correct package type . - If you use an App Store or Enterprise provisioning profile , select the " Final release package for Apple App Store " option . - If you use an Ad-Hoc provisioning profile , select the " Ad hoc package for limited distribution " option . - Keep in mind that you must provision specific devices in the Apple App Store before- creating the build . - Next , @ @ @ @ @ @ @ @ @ @ IPA file ready for distribution . - Note : If you are using a developer certificate , you will not be able to distribute an IPa file . <p> iOS Export IPA <p> Distribution : <p> If you are using Ad Hoc- distribution , you can now simply send the IPA file to your users , and they will be able to deploy the application to any provisioned devices . - If you are using the enterprise provisioning option , you can just send the IPA without worrying about specific device IDs. - However , keep in mind that you can only distribute those files according to Apples rules surrounding the Enterprise deployment model . <p> If you 've be " chomping at the bit " to upload your application to Apples app store , then this next section is for you . - First things first , you must go to iTunes Connect to setup your application with the App Store . - Simply navigate to itunesconnect.apple.com- and l0g in using the same credentials that you used for the iOS Portal . - iTunes connect allows you to manage and @ @ @ @ @ @ @ @ @ @ etc <p> To get started , click on the " Manage Your Applications " link shown in the screenshot below : <p> iTunes Connect - Home <p> You will be brought to the " manage " screen ; from which you can edit metadata for all of your applications . - To create a new application , click on the blue " Add New App " button in the top left corner . <p> iTunes Connect - Manage Apps <p> iTunes connect will now walk you through a series of questions that will provide appropriate metadata for your application . - First , select your language , and enter your company name . - If you are submitting as an individual ( not a company ) , just put your name . <p> Next , enter the availability date and pricing for your application . - Keep in mind that the availability date only applies after the application has been approved by Apple . - If you back-date this , it will not speed up your time into the app store . - When ready , click " Continue " @ @ @ @ @ @ @ @ @ @ <p> Now , enter the primary metadata that describes your application . - This includes the version number ( try to keep this in synch with your app.xml in your Flex project for organization purposes although not required to match ) , description , primary and secondary categories , keywords , contact information , and review notes ( for the team at Apple that will review your application ) . <p> iTunes Connect - Primary Metadata <p> Next , rate the content within your application . - Apple will verify your app against these ratings , and if they do not match , your app will not make it into the app store . - Mine was actually rejected for this No , I do not have crude content in this case . - For example , if you provide unrestricted access to the internet , such as allowing a user to type in any URL , and you show that content via a stageWebView , then you are rated 17+. - No " ifs " , " ands " , or " buts " . - My only options @ @ @ @ @ @ @ @ @ @ , or to have the app rated 17+. - I chose 17+. - Do n't  be confused by this though ; - If you use StageWebView , that does n't  mean your app will always be rejected this is only b/c I had unrestricted access to the StageWebView control . - In my app , you can enter any URL , and click on the " view " button to view the content of that URL . <p> iTunes Connect - Content Ratings <p> Next , upload your application images . - The large icon is 512+512 , and the phone screenshots should match actual device resolutions . - Note : Your main " large icon " must match the one embedded in your application I also got rejected b/c it did n't  match initially , as shown in the screenshot below . <p> Image requirements from Apple : <p> iPhone and iPod touch Screenshots must be . jpeg , . jpg , . tif , . tiff , or . png file that is 960+640 , 960+600 , 640+960 , 640+920 , 480+320 , 480+300 , 320+480 or 320+460 @ @ @ @ @ @ @ @ @ @ RGB color space . <p> iTunes Connect - Images <p> Hit " Continue " to save all of your changes , and now you are ready to start uploading your application binary . - Click on one of the " Ready To Upload Binary " buttons on either the top of bottom of the screen to move the process along <p> iOS Distribution - Ready to Upload <p> Once you click on the " Ready to Upload " button , you will walk through several questions around export compliance and encryption. - Answer these questions , then hit " Save " to proceed . <p> iOS Distribution - Export Compliance <p> Once you have completed all of the questions , your- application- will now be ready to upload. - After this point , we will be leaving iTunes Connect , and jumping to the OSX desktop to actually upload the application . <p> iOS Distribution - Application Ready to Upload <p> Now , we are ready to upload the application . - In OSX , launch the " Application Uploader " program , which is a part of the XCode @ @ @ @ @ @ @ @ @ @ will need to sign in using your iOS developer credentials . <p> iOS Provisioning Certificates - With Certificate <p> Upon login , the Application Loader tool will allow you to chose which application you want to upload a new build for . - Choose the desired application , and then click on the " Next " button . <p> iOS Provisioning Certificates - With Certificate <p> The Application Loader tool will display metadata about the application that you have selected , and you will need to choose an IPA file from your local filesystem for distribution . - Be sure to select your release-compiled , App Store provisioned IPA , or else the upload process will fail . <p> iOS Provisioning Certificates - With Certificate <p> Once you 've selected the IPA file , click " Send " to being the upload process . <p> iOS Provisioning Certificates - With Certificate <p> After the upload process is complete , there will be a verification step , and then you will receive a message " Uploaded package to iTunes Store " . <p> iOS Provisioning Certificates - With Certificate <p> Click " @ @ @ @ @ @ @ @ @ @ " Thank You " message , and the upload process is done . <p> iOS Provisioning Certificates - With Certificate <p> Now , comes the Apple approval process . - Generally , this takes about a week for new applications , and only a few days for application updates . - However , there are not guaranteed timelines. - Once the application is approved , it can be ready for immediate consumption within Apples App Store . 
@@106848954 @2248954/ <h> Tag Archives : presentations <p> I recently returned from a great trip to San Francisco , CA , where I was able to attend Dreamforce- - Salesforce.coms annual conference . - Dreamforce is probably the biggest conference Ive ever attended , with 90,000 registered attendees consuming the entire Moscone Center and all of downtown San Francisco . While only a small portion of those attendees were technical/developers , it was still a massive turnout . The best part Dreamforce attendees are really excited about PhoneGap ! - I had a fantastic turnout for my sessions , just take a look below ! <p> So , why all the excitement ? <p> SalesForce.coms mobile hybrid SDK is built on top of PhoneGap. - This makes consumption of business-critical Salesforce.com data in your mobile applications very easy . - You can also use components from the- Salesforce Touch platform inside of PhoneGap applications , and even some of Salesforces own mobile applications are even built with PhoneGap ( Logger , Dreamforce ) . <p> I had several presentations focused on PhoneGap , and had great audiences at all of them @ @ @ @ @ @ @ @ @ @ below . - Just press the " space " key when viewing a presentation to advance to the next slide . <p> Description : Interested in developing applications for mobile devices , on multiple platforms ? Interested in leveraging your existing web development skills to build natively installed applications ? Just looking to expand your skill set ? Come join Adobes Technical Evangelist , Andrew Trice , to learn about cross platform mobile development and PhoneGap . In this session , you will get an introduction to PhoneGap ( Apache Cordova ) , be able to see example PhoneGap applications , and walk through the process of building your first PhoneGap application . <p> Description : Do you have a need to create rich visual data-centric applications , but also have the requirement to use standard web technologies , and do n't  know what to do next ? Well , you 're in luck ! Come to this session to learn about data visualization strategies and frameworks powered entirely by HTML , CSS , and JavaScript . <p> Description : Native applications built using web technologies can suffer from the " @ @ @ @ @ @ @ @ @ @ right as a native application . In this session well focus on strategies to make your apps feel like native apps , including considerations for a native-feeling UI , platform consistency , and user experience . <p> Ill be speaking at a few conferences in the next few months on PhoneGap and web standards-based development . - Here are just a few , with some more pending . - Be sure to come check one out ( or all of them ) ! <h> RIACON <p> Where architects and developers of all levels come to gather , share and learn about creating the next generation of web based applications. - RIAcons goal is to help you network with fellow industry professionals and expose you to the best content . <p> Ill be speaking on the following topics at RIACON : <p> Introduction to PhoneGap Interested in developing applications for mobile devices , on multiple platforms ? Interested in leveraging your existing web development skills to build natively installed applications ? Just looking to expand your skill set ? Come join Adobe Technical Evangelist , Andrew Trice , to learn about @ @ @ @ @ @ @ @ @ @ , you will get an introduction to PhoneGap ( Apache Cordova ) , be able to see example PhoneGap applications , and walk through the process of building your first PhoneGap application . <p> PhoneGap Native Plugins PhoneGap enables developers to build natively installed applications using traditional web-based development tools ( HTML &amp; JavaScript ) , but what if you want to make your application do more ? In this session , learn how to write native plugins for PhoneGap that enable you to extend the API to tap into native device functionality . <p> Data Visualization with Web Standards Do you have the requirement to create rich visual data dashboard applications , but also have the requirement to use web-standard technologies , and do n't  know what to do next ? Well , you 're in luck ! Come to this session to learn about data visualization strategies and frameworks powered entirely with HTML and JavaScript. <h> 360iDev <p> 360iDev is the first and still the best iPhone developer conference in the world . We 're not a publishing company pushing books , or a media company selling subscriptions . @ @ @ @ @ @ @ @ @ @ Our goal is to bring the best and brightest in the developer community together for 3 days of incredible sessions , awesome parties , good times , and learning . If you do n't leave Wednesday night , with more ideas than you know what to do with , we 're not doing our jobs ! <p> Ill be speaking on the following topics at 360iDev : <p> Kick A$$ iOS Apps with PhoneGap Apps do n't  have to be written in native Objective-C to be awesome. - - Get ready for a crash course in PhoneGap , a tool that enables you to build natively installed iOS apps using 100% HTML &amp; JavaScript , complete with access to local APIs. - Well cover everything from " what is phonegap " to strategies for building highly performant &amp; interactive applications . <h> Dreamforce <p> Every year Dreamforce features stories and presentations from some of the brightest minds in technology , business and beyond . This years Dreamforce promises to be even more informative and dynamic , with our most exciting keynote speaker lineup yet . The cloud computing event of @ @ @ @ @ @ @ @ @ @ year . This is where you 'll learn everything you need to know " from the industry leaders who are paving the way " about how the Social Enterprise revolution is changing the way we do business . <p> Ill be speaking on the following topics at Dreamforce : <p> Data Visualization with Web Standards Do you have the requirement to create rich visual data-centric applications , but also have the requirement to use web-standard technologies , and do n't  know what to do next ? Well , you 're in luck ! Come to this session to learn about data visualization strategies and frameworks powered entirely with HTML , CSS , and JavaScript . <p> Native-like Apps with PhoneGap Native applications built using web technologies can suffer from the " uncanny valley " effect where they do n't  feel quite right as a native application . In this session well focus on strategies to make your apps feel like native apps , including considerations for a native-feeling UI , platform consistency , and user experience . <p> As promised , here are the slides from my presentation " Introduction to @ @ @ @ @ @ @ @ @ @ night . Many thanks to Towson University for hosting the event and inviting me to speak ! <p> In this presentation , Adobe Evangelist Andrew Trice will walk through several applications utilizing Flex and AIR for mobile and will highlight application ecosystems , development processes , device considerations , and many of the " nuts and bolts " of multi-platform development using Adobes toolsets . <p> In this session , we will explore mobile application development using Flex and AIR for mobile , powered by ColdFusion backend systems . By the end of this session , you will know how to build natively installed iOS , Android , and BlackBerry Playbook applications , and you will be able to utilize your existing CF skills to power them . 
@@106848955 @2248955/ <h> Tag Archives : adobe <p> HTML5 Developer Conference just wrapped up in San Francisco , and it was a great event . There was a lot to see and hear from the entire HTML/JS community . Adobe showed off some amazing showcases of the work that is being done with rich layout on the web and released an awesome new SVG library , Snap.svg. - I also had two sessions on PhoneGap . <p> First , news and announcements from Adobe <h> Snap.svg <p> Adobe has released Snap.svg , the JavaScript SVG library for the modern web . - Snap.svg is focused on making the most out of everything that SVG can offer , including masking , clipping , patterns , gradients , groups , and much more . - It is definitely worth checking out . <h> My Sessions on PhoneGap <p> Last , but certainly not least , I had two sessions on PhoneGap : one an intro , and the other a more advanced architecture topic . - Thanks to everyone who came out for my sessions . You were a great audience , and @ @ @ @ @ @ @ @ @ @ recorded by the conference organizers , and will be available at a later date . <p> You can access my presentation slides in the links below ; just use the space bar to advance each slide : <p> However , I must also apologize that a few of my samples in the " Getting Started with PhoneGap and Cross-Platform Mobile Development " did not work . I was connected to the network , but was n't able to receive any data , so I could n't access PhoneGap Build , or even add device features to a PhoneGap project from the command line tools . - I promise , these features do work when you 're on a reliable network connection . Go check out phonegap.com to learn more and get started today . <p> This week I was in the video studio recording some content for Adobe Inspire magazine on creative uses of quadcopters , GoPros and Creative Cloud for aerial photography and videography . Adobe Inspire is a great place to get new ideas or learn tips and tricks with Adobe Creative Cloud tools . Subscribe to the free Adobe Inspire @ @ @ @ @ @ @ @ @ @ . <p> My series will be released in early 2014 , but here are a few teasers to whet the appetite . I had a blast with this shoot , and cant wait for the articles and videos to be released radio-controlled aerial photography is my latest hobby/obsession . <p> In the studio : <p> Video from the shoot : Skies above the Adobe office in San Francisco . Captured with a DJI Phantom and GoPro : <p> If you look really closely , you can see us standing on top of the parking garage to the back left of the Baker &amp; Hamilton sign . <p> Subscribe- today or download the- iOS app- to be notified once this is live in Inspire . Now , go get creative and do amazing things ! <p> The recording for my session " PhoneGap and Hardware " from PhoneGap Day back in July is now available ! Be sure to check it out . There were apparently some issues with the audio , but you can still hear everything . <p> I 'd like to express a huge Thank You- to everyone who @ @ @ @ @ @ @ @ @ @ <p> Below are the sample projects I showed in the presentation , including source code . However , keep in mind that all of these examples were written before PhoneGap 3.0 . The native plugin syntax , and inclusion methods have changed . <h> Pressure Sensitive Sketching in PhoneGap <p> In this example , - the pressure-sensitive Pogo Connect Stylus uses a low energy Bluetooth 4 connection to relay touch/pressure information back to the PhoneGap application . This makes for a unique drawing and sketching experience powered with the HTML5 Canvas element . I 've written about this example previously Check out the video below to see it in action , and read the blog post for technical details and source code . <h> Moga Gamepad <p> The second example that I explored is a PhoneGap native plugin that is used to handle input from a Moga game controller inside of a PhoneGap application on Android . <p> This implementation is intended to be a proof of concept demonstrating how you could integrate the gamepad within your application . It currently only supports input from the joysticks ( axisX and axisY @ @ @ @ @ @ @ @ @ @ does not handle all possible input from the controller . <p> This implementation is adapted directly from the **38;8540;TOOLONG example from the Moga developers SDK samples available for download at : - http : **34;8580;TOOLONG 123434 @qwx983434 <p> The game is based on the Universe- prototype that was used as a sub-game inside of the MaxMe app for the recent Adobe MAX conference . I make no guarantees about the code for this game , it was in a huge rush ! <p> Ive been spending a lot of time with Photoshop recently Whether it has been retouching video or images , creating panoramas , or working with my aerial photos , it has been a lot of fun . One thing that I 've been doing is exporting really large images to the web . So far this has been a very manual process Export from Photoshop using Zoomify . Then , since the default Zoomify renderer uses Flash ( and I want this consumable on mobile devices ) , take the Zoomify image tiles , and put them into a custom-coded HTML experience using the Leaflet tile- engine with @ @ @ @ @ @ @ @ @ @ for web-based mapping , but it is a perfect solution for rendering image tiles on the web . It already has touch and mouse interactions , inertial scrolling , progressive viewing , and a comprehensive API that can be extended if you so choose . <p> I 've done this enough times that I figured " There has to be an easier way " and there definitely is . I 've created a new Zoomify template that allows you to export from Photoshops Zoomify feature directly to HTML , leveraging the Leaflet engine . All of the code and installation instructions are below in this post . Check out the video below to see it in action : <h> Samples <p> Here are few samples from the Zoomify output ; both are the compositions that I showed in the video above . Use the mouse or touch interactions to pan and zoom on each of them . <p> The first is an export from a 10MP aerial panorama ( 4340+2325 pixels ) , which was created by stitching together multiple images captured with a GoPro camera and remote controlled helicopter . <p> @ @ @ @ @ @ @ @ @ @ 14561+9570 pixels ) . I created this by stitching together 48 10mp images in Photoshop . Its not perfect , but shows how far you can zoom into an image some images had different exposures , some were out of focus , there is still some perspective warp , and I definitely have some bad stitching seams . This image is so huge that I actually maxed out the system RAM , and filled up all hard disk space with the memory swap file when creating it ( I had over 100 Gigs of free space ) ! <p> Extract the zip file and copy the following files to Photoshops Presets/Zoomify directory . On OS X , with the default configuration , these files should be located in /Applications/Adobe Photoshop CC/Presets/Zoomify/ <p> L.TileLayer.Zoomify.js <p> Zoomify Leaflet HTML.zvt <p> leaflet.css <p> leaflet.js <p> Restart Photoshop . <p> When you have a file open that you want to export , choose File -&gt; Export -&gt; Zoomify- <p> Then select the " Zoomify Leaflet HTML " template that should now be in the list . Select an output location , base name , @ @ @ @ @ @ @ @ @ @ Ignore the browser width and height , since the template ignores these . Instead , it takes 100% of the width and height of the browser window . <p> - This will generate all of the image tiles and the HTML structure . From here , do whatever you want with it You can modify it , put it on a server , or anything else . the file output will look something like the image below . You will have a folder that contains the generated HTML file , the Leaflet JS and CSS files , and a directory that contains the generated tiles and appropriate XML metadata . <p> With Halloween less than a month away , I figured its about time to update my " Instant Halloween " PhoneGap sound effects app . I 'm happy to say that the latest version is now out for both iOS and Android . It has a few new sounds , a new UI style , and has been updated for iOS 7 . I also updated the low latency audio plugin- so it now supports PhoneGap 3.0 method signatures and @ @ @ @ @ @ @ @ @ @ app is fantastic for scaring people Just hook it up to really loud speakers , and start playing the sounds to your hearts content . Its got everything from background/ambient loops to maniacal laughter , screams , ghosts , zombies , and other spooky sound effects . <p> It is available now , for FREE , for both iOS ( 5.0+ ) and Android ( 4.0+ ) . <p> - <p> So what has changed in this version ? <p> First , I updated the app to support iOS 7 . For the most part , this is a non-issue . PhoneGap apps are based on web standards , and HTML/JS/CSS work pretty much everywhere . However , you do have to account for a few minor changes . One is that the OS status bar now sits over top of the application . You 'll need to update your UI on iOS 7 , so there are no UI issues . Check out this post from Christophe Coenraets for details regarding creating PhoneGap apps for iOS 7 . <p> iOS 7 also introduces some new UI design paradigms and guidelines @ @ @ @ @ @ @ @ @ @ all textures , and tried to make things as simple and minimalistic and native-feeling as possible . I also got rid of iScroll for touch-based scrolling both the iOS and Android versions now use native inertial based scrolling from the operating system . This is the reason that the new Android version is only Android 4.0 and later , but it is also the reason that the app feels much closer to a fully native experience . <p> I updated the low latency audio native plugin to support PhoneGap PhoneGap 3 . There were two parts to this : First , there is the updated method signature on the native interfaces . I just took the old plugin , and updated it for the new method signature . The new method signature- was actually introduced a while back , but I never updated the plugin for it. - Second , I added the appropriate XML metadata to enable CLI-based installation of the plugin for both iOS and Android . Take a look at the PhoneGap documentation for details on creating PhoneGap native plugins- and plugin.xml. - Also check out this @ @ @ @ @ @ @ @ @ @ Android . <p> In the process , I also ran into an unexpected issue with Android deployment Back in the spring I had a corrupted hard drive . I was able to recover *most* of my data , and I thought I had all of my app signing keys . It turns out that for Android , your apps must not only have the same signing keys , but also the same key store when you sign the APK files , or else Google wont let you distribute the Android APK file as an update ; it must be a new app . It turns out that I had recovered the key , but not the keystore . So , I had no choice but to distribute it as a new app . <p> Go download the free apps , get the source code , and start building your own PhoneGap apps today ! 
@@106848956 @2248956/ <h> Tag Archives : lccs <p> here 's a little demo I put together that I 'm just too excited not to share Fellow Adobe Evangelist Christophe Coenraets has an awesome- Mobile Trader- application that highlights the power of LCCS collaboration and realtime information , using Flex for mobile . - I decided to see if that same collaboration example would work with the Stage3D data visualization component Ive been playing around with , and guess what : - it works fantastic . - Ive given it the nickname " project awesome " b/c it combines realtime collaboration ( audio , video &amp; data ) , Stage3D hardware accelerated graphics , and desktop to mobile paradigms. - Check it out below : <p> If you have n't already seen Christophes Mobile Trader , you can check it out here : <p> Christophe has made the source of the Mobile Trader application freely available on github , so go download it and creating your own collaborative applications ! I know , you *really* want to see Stage3D out in the public I promise you , it is coming , and it will be @ @ @ @ @ @ @ @ @ @ I almost forgot to mention : I wrote less than 200 lines of code to create project awesome ! - It uses the Away3D library , and LCCS the hard stuff is easy with Flex . 
@@106848957 @2248957/ <h> Tag Archives : cross-platform <p> Perhaps you have heard of the topic " cross platform development " , but are n't  really sure what it is , or you are n't  sure why you would want to use cross-platform technologies . If this is the case , then this post is especially for you . Ill try to she 'd some light onto what it is , and why you would want to use cross-platform development strategies . <h> What is cross-platform development ? <p> Cross platform development is a concept in computer software development where you write application code once , and it runs on multiple platforms . This is very much inline with the " write once , run everywhere " concept pioneered in the 90s , and brought to a mainstream reality with Flash in the browser , and AIR on the desktop . The standard evolution of technology has been to make everything faster , smaller , and more portable , and it is only natural that that this concept has now come into the mobile development world . In mobile scenarios , it is @ @ @ @ @ @ @ @ @ @ that allows the application to be deployed and distributed across multiple disparate platforms/operating systems/devices . <p> In case you 're wondering why I offered 2 cross platform technologies , that is because Adobe will soon have 2 cross-platform product offerings . - Adobe has entered an- agreement to purchase Nitobi , the creators of PhoneGap . <h> Adobe AIR <p> Adobe AIR is a cross-platform technology with roots in the Flash Player and the AIR desktop runtime. - AIR allows you to build cross-platform mobile applications using ActionScript and the open source Flex framework . - AIR apps can be built from the Flash Professional timeline-based design/animation tool , Flash Builder ( an Eclipse-based development environment ) , or other open source solutions using the freely available AIR SDK. - Applications developed with Adobe AIR can target desktop platforms ( Mac , &amp; Windows ) , smart phone and tablet platforms ( iOS , Android , BlackBerry , soon Windows ) , and even smart televisions . <h> PhoneGap ( Apache Callback ) <p> PhoneGap is an open source cross platform technology with roots in the HTML world . Essentially , @ @ @ @ @ @ @ @ @ @ 100% of the available width &amp; 100% of the available height , taking advantage of web browsers on each platform . - PhoneGap offers a JavaScript to native bridge that enables you to build natively-installed applications using HTML and JavaScript , using the native bridge to interact with the device hardware/APIs. - Note : PhoneGap is also being submitted to the Apache Foundation as the Apache Callback project . <h> More Devices , Less Code <p> The driving factor behind cross-platform technologies is that you will be able to use those technologies to target more devices &amp; platforms , with writing a minimal amount of source code . - There are many advantages with this approach . - Here are a few of the major reasons <h> Lower Barrier of Entry <p> Generally speaking , development with HTML &amp; JavaScript or Flex &amp; ActionScript is easier than developing with Objective-C or Java . - Due to the ease of use of the development tooling and familiarity of the languages , cross platform technologies lower the technical barriers which may have prevented adoption of native development . - This allows your @ @ @ @ @ @ @ @ @ @ have been able to , and also enables your team to focus on what matters the application ; not the skills required to develop on multiple disparate platforms . <h> Reduce the Number of Required Skills for the Development Team <p> Native development on multiple platforms requires your development team to learn Objective C for iOS applications , Java for Android applications , Silverlight for Windows Phone applications , etc - Finding all of these skills in a single developer is nearly impossible . - Using cross-platform development technologies , your team only needs to be proficient with one language/skillset. - Knowledge of the native development paradigms and languages are always a plus , but are no longer a requirement . - - Many developers transitioning from web development already know either Flex/ActionScript and/or HTML/JavaScript , and making the transition from web to mobile development will not be a major undertaking . <h> Reduced Development &amp; Long Term Maintenance Costs <p> Cross-platform mobile applications can originate from a single codebase , which requires a single development skillset. - You do n't  need to have staff for each individual platform . @ @ @ @ @ @ @ @ @ @ cover all target platforms . - Having a single codebase also reduces long term maintenance costs . - You no longer need to have bug tracking for X number of codebases , and do not need to maintain a larger staff to support each platform . - Did I also mention that you have one codebase to maintain ? <p> Having a single codebase does n't  reduce the need for QA/testing on each target platform nothing can get rid of this . - It is absolutely imperative that you test your codebase on physical devices for all platforms that you intend to support . - Emulators and Simulators can go a long way during development , but they will undoubtedly not cover all scenarios possible on a physical device , and they will not have the same runtime performance as a physical device . <h> Play the Strengths of a Technology <p> Some technologies make tasks easier than others . - For example , programmatic drawing and data visualization are very easy using Flex &amp; ActionScript. - Developing equivalent experiences in native code can be significantly more complex and time @ @ @ @ @ @ @ @ @ @ to their fullest potential , to your advantage- that 's why they exist . 
@@106848958 @2248958/ <p> The reason that you can run a Flex Mobile app in the Android emulator , but not in the iOS Simulator comes down to the fundamental difference between an emulator and a simulator . An emulator emulates a physical device ; The emulator program mimics the hardware , and the device-specific code that will run on the actual device runs within the mimicked environment . A simulator simulates an environment it has a likeness or model of an environment , however it is not identical to the target environment . <p> In this case , the Android emulator mimics the hardware environment and is capable of running a compiled APK for a Flex/AIR mobile application . However , the iOS simulator is not capable of executing the contents of an IPA file . This is n't specific to an IPA file for an AIR mobile app , but any IPA file even those downloaded from Apples own app store . <p> The executable content within an IPA file is compiled targeting the devices A4-ARM processor . Your desktop computer uses an intel-based processor architecture , which is n't compatible and @ @ @ @ @ @ @ @ @ @ file to a ZIP file and extract the contents , it will not work within the iOS Simulator because of the CPU architecture differences . <p> Best Practice : <p> The first , and most important point that I emphasize regarding mobile application development is that nothing is more important than testing your mobile applications on a physical device . Emulators and simulators can help you see how an application may operate within a given environment , but they do not provide you with the actual environment . Physical devices may have memory , CPU , or other physical limitations that an emulator or simulator may not be able to reveal . <p> Secondly , keep in mind that emulators and simulators are created to make your development process easier and faster ( especially if hardware is not readily available for the entire dev team ) . The debugging environment within Flash Builder is designed exactly for that purpose . You can quickly and easily test your applications interface and functionality with a single button click . You can even setup debugging profiles for multiple devices , or use one @ @ @ @ @ @ @ @ @ @ make developing for multiple form factors and multiple device types significantly easier and faster , this does not trump my first point . If you are targeting specific hardware , then it is imperative that you test thoroughly on your target platform(s) . <p> I 'm pleased to announce that as of next week ( July 26th ) I will be joining Adobe as a Technical Evangelist , with an emphasis on enterprise mobile applications ! Many of you know that Ive been passionate about the Adobe ecosystem for quite a few years I became really involved late in the Macromedia days , and have worked with Flex since 1.5 and Flash since version 3 ( now working on version 11 ) . I 've had the opportunity to work on some incredible projects , and a lot has changed throughout this time . I have been fortunate to witness the changes throughout the web that have been made possible by Adobes technologies , and I 'm even more excited that I now get to play a part in it . <p> In the last year I have primarily focused on mobile application @ @ @ @ @ @ @ @ @ @ for iOS , HTML5 , Appcelerator Titanium , PhoneGap , a little bit of native Android , and ( last , but most definitely not the least ) Adobe AIR for mobile ( BlackBerry , Android , and iOS ) . Now , I am excited to use this experience to show how Adobes technologies can be used in web , desktop , and mobile scenarios , for building incredible applications on multiple platforms , with an easy-to-use and extremely powerful development paradigm . <p> I 'd also like to take a moment to thank my current employer , Universal Mind , for the opportunities that I 've had while working here . While with UM I have had a great experience , and I wish them many great successes in the future . It has been an awesome time , and I 've had the good fortune of being a part of some ground-breaking and exciting projects ( while also getting to work with some of the best and brightest in the consulting and design industry ) . <p> This is an exciting opportunity , and I am happy to share it @ @ @ @ @ @ @ @ @ @ need of information or best practices regarding Adobe and/or mobile , please do not hesitate to ask ! 
@@106848959 @2248959/ <p> IBM Watson services , which are based on machine learning algorithms , - give you the ability- to handle unstructured data , like text analysis or- translation , speech processing , and more . - This makes consumption , mining , or responding to unstructured data or " dark data " faster , more efficient , and more powerful than ever . <p> The new Watson iOS SDK- provides- developers with an API- to simplify integration of the- Watson Developer Cloud services into their mobile apps , including the Dialog , Language Translation , Natural Language Classifier , Personality Insights , Speech To Text , Text to Speech , Alchemy Language , or Alchemy Vision services " all of which are available today , and can now be integrated with just a few lines of code . <p> The- Watson iOS SDK makes integration with Watson services significantly *really* easy . For example , if you want to take advantage of the Language Translation service , you first have to setup a service instance . Once the translation service is setup , then you 'll be able to @ @ @ @ @ @ @ @ @ @ sure to check out the sample 's readme for additional detail and setup instructions . As with all of the Watson services , You must have a service instance properly configured , with authentication credentials- in order to be able to consume it within your app. 
@@106848960 @2248960/ <h> Mobile Apps , Cognitive Computing , &amp; Wearables <p> Last week I was in good ole Las Vegas for IBM InterConnect IBMs largest conference of the year . With over 20,000 attendees , it was a fantastic event that covered everything from technical details for developers to forward-looking strategy and trends for C-level executives . IBM also made some big announcements for developers OpenWhisk serverless computing and bringing the Swift language to the server just to name a few . Both of these are exciting new initiatives- that offer radical changes &amp; simplification to developer workflows . <p> It was a busy week to say the least lots of presentations , a few labs , and even a role in the main stage Swift keynote . You can expect to find more detail on each of these here on the blog in the days/weeks to come . <p> For starters , here are two " lightning talks " I presented in the InterConnect Dev@ developer zone : <h> Smarter apps with Cognitive Computing <p> This session introduces the concept of cognitive computing , and demonstrates how you can @ @ @ @ @ @ @ @ @ @ If you are n't  familiar with cognitive computing , then I strongly recommend that you check out this post : The Future of Cognitive Computing . <p> In the presentation below , I show two apps leveraging services on Bluemix , IBMs Cloud computing platform , and the iOS SDK for Watson . <p> Actually , I 'm using two Watson SDKs The older Speech SDK for iOS , and the new iOS SDK. - I 'm using the older speech SDK in one example because it supports continuous listening for Watson Speech To Text , which is currently still in development for the new SDK . <h> Redefining your personal mobile expression with on-body computing <p> My second presentation highlighted how we can use on-body computing devices to change how we interact with systems and data . - For example , we can use a luxury smart watch ( ex : Apple Watch ) to consume and engage with data in more efficient and more personal ways . - Likewise , we can also use smart/wearable peripherals devices to access and act on data in ways that were- never possible- before @ @ @ @ @ @ @ @ @ @ based upon patterns in raw data transmitted by the on-body devices . - For this , I leveraged the new IBM Wearables SDK. - The IBM Wearables SDK provides a consistent interface/abstraction layer for interacting with wearable sensors . - This allows you to focus on building your apps that interact with the data , rather thank learning the ins &amp; outs of a new device-specific SDK . <p> The wearables SDK also users data interpretation algorithms to enable you to define gestures or patterns in the data , and use those patterns to act upon events when they happen without additional user interaction . - For example : you can determine if someone falls down , you can determine when someone is raising their hand , you can determine anomalies in heart rate or skin temperature , and much more . - The system is capable of learning patterns for any type of action or virtually any data being submitted to the system . - Sound interesting ? - Then check it out here . <p> I also had some other- sessions- on integrating drones with cloud services , @ @ @ @ @ @ @ @ @ @ . - I 'll be sure to post updates for this- content- I make them publicly available . - I think you 'll find the session on drones + cloud especially interesting I know I did . 
@@106848961 @2248961/ <h> Tag Archives : bleeding edge <p> The world is changing and oh my , it is changing fast . - In the not-too-distant future , many capabilities that were exclusive to plugin-based content will be accessible to the HTML/JavaScript world without any plugin dependencies. - This includes access to media devices ( microphone and camera ) , as well as real time communications . - You might be reading this thinking " no way , that is still years off " , but its not . <p> Just last night I was looking at the new webRTC capabilities that were introduced in the Google Chrome Canary build in January , and I was experimenting with the new getUserMedia API. - WebRTC is an open source realtime communications API that was recently included in Chrome ( Canary , the latest dev build ) , the latest version of Opera , and soon FireFox ( if not already ) , and is built on top of the getUserMedia APIs . Device access &amp; user media APIs are n't  commonly available in most users browsers yet , but you can be @ @ @ @ @ @ @ @ @ @ . <p> Below you 'll see a screenshot of a simple example demonstrating camera access . <p> The beauty of this example is that the entire experience is delivered in a whopping total of 17 lines of code . - It uses the webkitGetUserMedia API to grab a media stream from the local webcam and display it within a HTML5 &lt;video&gt; element . <p> While this example is really basic , it is a foundational building block for more complicated operations , including realtime video enhancement and **31;6459;TOOLONG - - Check out this more advanced example from- http : **25;8616;TOOLONG , which applies effects to the camera stream in real time : <p> If you want to read more about some of the new " Bleeding Edge " features coming to the web , check out this slide deck by Googles Paul Kinlan. - You can also read more about the getUserMedia API from Operas developer site . 
@@106848962 @2248962/ <h> Why I Love Adobe Creative Cloud <p> Creative Cloud is absolutely incredible . Yes , I work for Adobe , so this may sound like a biased opinion , but even if I did n't  work for Adobe , I would say the same thing . " What 's so incredible ? " you are wondering ? Well , for starters , it has everything I need to bring a creative vision to life . <p> I can hear it now you are thinking " Thats awfully vague . " - Let me try to provide some context <p> My background and all of my formal education is in computer science and software development . My career has been focused on building great applications and immersive experiences for the end user . This has taken me on a perilous journey between the client and the server ; from the depths of C/C++/Objective C , to the outer regions of JavaScript , from the early interactive web with Flash/ActionScript , to the innermost workings of Java , from countless acronyms , realtime data streams , and REST/web services on towards @ @ @ @ @ @ @ @ @ @ thing that is common across all of these roles and languages is that you have to create assets to build your user interface . - Throughout my career , Ive used Photoshop and Illustrator to build the assets . - While my primary use case has been creating and manipulating assets for software , I occasionally would use other tools for other purposes . <p> Creative Cloud has everything that I need to execute on this workflow , plus more . A whole lot more . <p> Creative Cloud has everything I need to bring a creative vision to life . <p> For those cases where I need to switch roles and edit video , cleanup some audio , or retouch a series of images , I already have everything I could possibly need with Creative Cloud . Video tools , audio tools , print design tools , digital publishing tools , development tools Theres something for everyone . <p> On occasion , I need to edit video . - Did you know that with Creative Cloud , you get THE BEST VIDEO EDITING WORKFLOW ON THE PLANET ? No @ @ @ @ @ @ @ @ @ @ , Star Trek , Iron Man , Life of Pi , and many , many more movies and television shows leverage the Adobe toolchain to execute their creative vision . <p> Trying to recreate some of these movie visual effects is actually what inspired me to write this post . I was metaphorically " blown away " by the capability of everything I already had installed on my computer . The end result and video effects I 'll cover in my next post , but here 's a quick screenshot : <p> I 've had an interest in art , photography , computer graphics , music , and audio production my entire life . I took my first Photoshop class back in the mid 90s , and never looked back . I played in a band , and thought music and digital arts would be my entire life . Creative Cloud makes perfect sense to me . I get the tools I need on a daily basis , and so much more . Whatever I need , its there . 
@@106848964 @2248964/ <h> PhoneGap Day : PhoneGap &amp; Hardware <p> I 'd like to express a huge Thank You- to everyone who attended PhoneGap Day in Portland last week , and to Colene and the team that put everything together ! The day was loaded with fantastic presentations , and great community interaction tons of information , tons of great questions , and tons of great people ( oh , and beer it would n't be PhoneGap Day without beer ) . <p> My session was about the integration of PhoneGap with hardware . Basically , it was an overview and exploration how you can use native plugins to extend the capabilities of PhoneGap applications and interact with device peripherals . This enables new interaction paradigms , and helps evaluate and evolve what is attainable with web-related technologies . <p> In this session , I covered two use cases The first use case is the use of a pressure-sensitive stylus for interacting with a PhoneGap application on iOS . The second use case is integration of a Moga gamepad with a PhoneGap application on Android . In both cases , the application experience @ @ @ @ @ @ @ @ @ @ it is possible for the user to interact and engage with the application content and context . <h> Pressure Sensitive Sketching in PhoneGap <p> In this example , - the pressure-sensitive Pogo Connect Stylus uses a low energy Bluetooth 4 connection to relay touch/pressure information back to the PhoneGap application . This makes for a unique drawing and sketching experience powered with the HTML5 Canvas element . I 've written about this example previously Check out the video below to see it in action , and read the blog post for technical details and source code . <h> Moga Gamepad <p> The second example that I explored is a PhoneGap native plugin that is used to handle input from a Moga game controller inside of a PhoneGap application on Android . <p> This implementation is intended to be a proof of concept demonstrating how you could integrate the gamepad within your application . It currently only supports input from the joysticks ( axisX and axisY ) and the A and B buttons , and it does not handle all possible input from the controller . <p> This implementation is adapted directly @ @ @ @ @ @ @ @ @ @ available for download at : : - http : **34;8683;TOOLONG 123434 @qwx983434 <p> The game is based on the Universe- prototype that was used as a sub-game inside of the MaxMe app for the recent Adobe MAX conference . I make no guarantees about the code for this game , it was in a huge rush ! 
@@106848965 @2248965/ <h> Category Archives : Cloud <p> Last week I attended IBM Insight in Las Vegas . It was a great event , with tons of great information for attendees . I had- a few sessions on mobile applications . In particular , my dev@Insight session on Wearables powered by IBM MobileFirst was recorded . You can check it out here : <h> Key takeaways from the session : <p> Wearables are the most personal computing devices ever . Your users can use them to be notified of information , search/consume data , or even collect environmental data for reporting or actionable analysis . <p> Regardless of whether developing for a peripheral device like the Apple Watch or Microsoft Band , or a standalone device like Android Wear , you are developing an app that runs in an environment that mirrors that of a a native app . So , the fundamental development principles are exactly the same . You write native code , that uses standard protocols and common conventions to interact with the back-end . <p> Caveat to #1 : You user interface is much smaller . You @ @ @ @ @ @ @ @ @ @ the reduced amount of information that can be displayed . <p> You can share code across both the phone/tablet and watch/wearable experience ( depending on the target device ) . <p> Using IBM MobileFirst you can easily expose data , add authentication , and capture analytics for both the mobile and wearable solutions . <p> This post is inspired by all the comments Ive seen this week about JS in the enterprise . I would have never imagined- this 10- years ago , but JavaScript is now pretty much ubiquitous . Here are a few reasons why you need to paying attention to JavaScript if you are n't  already , and why you should definitely not- write it off . <p> First , I think one of the major reasons for JavaScripts ubiquity is that- JavaScript is approachable . It is relatively easy for beginners to learn JavaScript , and powerful enough for advanced users to build complex and reliable systems . <p> Second , why you need to pay attention , JavaScript is everywhere . <p> You can now use JavaScript to develop on virtually any platform : client @ @ @ @ @ @ @ @ @ @ , manage build scripts and dependencies , and more . <p> This does n't  mean you 'll use the exact same code in every case , rather that you can use the same skill set JavaScript Development to deliver solutions across multiple paradigms . <h> The Client Side <p> JavaScript can be used to power client side apps/user interfaces , and user interactions on numerous platforms and devices . <h> Web <p> Of course JavaScript powers the web , this is a given . JavaScript is the primary scripting language for all web browsers . I wo n't focus on this much b/c its already well known . <h> Mobile <p> JavaScript can also be used to power mobile applications that are natively installed on a device . <p> Apache Cordova/PhoneGap- You can build natively installed apps with web technology using PhoneGap or Cordova . PhoneGap is Adobes branded distribution of Cordova , but from the developers perspective , they are basically the same thing . Your app runs within a webview on the mobile device , and you build your user interface the same way you you build a dynamic web @ @ @ @ @ @ @ @ @ @ styled with CSS , and all interactivity is created with JavaScript . <p> React Native- JavaScript powered web apps do n't  just have to be inside of a a web view . The React Native framework gives developers the ability to write their application using JavaScript and declarative UI elements , and results in a native application running on the mobile device . The logic is interpreted JavaScript at runtime , but everything that the user- interacts with ( all UI elements ) is 100% native , providing a very high quality user experience , and it is now available for both iOS and Android applications . <p> Unity 3D You can even develop rich &amp; immersive mobile 3D simulation or gaming experience , entirely powered by JavaScript using the Unity 3D engine . **These can be web , desktop , or mobile , but is often used in mobile gaming . <h> Desktop <p> Yup , desktop apps are not left out of the mix . Most desktop solutions fall into a category similar to Apache Cordova , where the end results is a web view that has access @ @ @ @ @ @ @ @ @ @ web based technology . <h> The Server Side <p> Most obviously- Node.js a JavaScript runtime buit on Chromes V8 JavaScript Engine has made huge inroads into server side development and the enterprise . Node.js , powered by frameworks like express.js or loopback.io makes server side development and complex enterprise apps with JavaScript possible . <p> Here are some stats that- show the magnitude of growth and adoption for Node.js/npm.js alone . NPM- stats currently shows a total of- 186,946 packages available for download , 94,978,032 package downloads in the last day , and 2,451,734,737 package downloads in the last month . <p> This does n't  mean that JavaScript is the best language at everything . It- also does n't  meant that you can take a single piece of source- code and run it in every device/context imaginable . <p> It means that you can use your skills in JavaScript to develop for just about any kind of device/context out there . Its not going to be write once , run everywhere , rather in the words of the React.js team : learn once , write everywhere . <p> Last week @ @ @ @ @ @ @ @ @ @ at- the- MoDev DC meetup group on " Smarter Apps with Cognitive Computing " . - In this session I focused on how you can create a voice-driven experience in your mobile apps . I gave an introduction to IBM Bluemix and IBM Watson services ( particularly the Watson language services ) , and demonstrated how you can integrate them into your native iOS apps . I also covered IBM MobileFirst for operational analytics and- remote logging to provide insight into your apps performance once it goes live . - Check out a recording of the complete presentation in the video below : <p> What I 'm about to show you might seem like science fiction from the future , but I can assure you it is not . Actually , every piece of this is available for you to use as a service . - Today . <p> Yesterday- Twilio , an IBM partner whose services are available- via- IBM- Bluemix , announced several new SDKs , including live video chat as a service . - This makes live video very easy to integrate into your native mobile or web @ @ @ @ @ @ @ @ @ @ some very cool things . For example , what if you could add video chat capabilities between your mobile and web clients ? Now , what if you could take things a step further , and add IBM Watson cognitive computing capabilities to add real-time transcription and analysis ? <p> Jeff and Damion did an awesome job showing of both the new video service and the power of IBM Watson . I can also say first-hand that the new Twilio video services are- pretty easy to integrate into your own projects ( I helped them integrate these services into the native iOS client ( physicians app ) - shown in the demo ) ! - You just pull in the SDK , add your app tokens , and instantiate a video chat . - Jeff is pulling the audio stream from the WebRTC client and pushing it up to Watson in real time for the transcription and sentiment analysis services . <p> I recently put together some content on building " Apps that Work as Well Offline as they do Online " using IBM MobileFirst and Bluemix ( cloud services @ @ @ @ @ @ @ @ @ @ I used the content in a presentation at ApacheCon , and now I 've opened everything up for anyone use or learn from . <p> The content now lives on the IBM Bluemix github account , and includes code for the native iOS app , code for the web ( Node.js ) endpoint , a comprehensive script that walks through every step of of the process configuring the application , - and also a video walkthrough of the entire process from backend creation to a complete solution . <p> Key concepts demonstrated in these materials : <p> User authentication using the Bluemix Advanced Mobile Access service <p> Remote app logging and instrumentation using the Bluemix Advanced Mobile Access service <p> Client-side Objective-C code ( you can do this in either hybrid or other native platforms too , but I just wrote it for iOS ) . - The " iOS-native " folder contains the source code for a complete sample application leveraging this workflow . The " GeoPix-complete " folder contains a completed project ( still needs you to walk through backend configuration ) . The " GeoPix-starter " folder contains @ @ @ @ @ @ @ @ @ @ . You can follow the steps inside of the " Step By Step Instructions.pdf " file to setup the backend infrastructure on Bluemix , and setup all code within the " GeoPix-starter " project . 
@@106848966 @2248966/ <p> First , navigate to- https : //developer.amazon.com/- and click on the " Amazon Appstore for Android " link . Once there , you will need to walk through the full registration process to create your account . <p> Amazon Appstore Landing Page <p> Once you click on the " Get Started " button , you will be guided through the Appstore registration process . Once that is complete , you will be directed to the Appstore developer portal home page . - From here , just click on " Add a New App " . <p> Amazon Appstore Home <p> Next , you will being the App upload wizard . - First , you will need to enter primary metadata for the application , including a title , form factor , supported languages , and contact information . - Once you have entered this information , click on the " Save " button . <p> Amazon Appstore App Metadata <p> Next , you need to enter merchandising information . - This includes the app category , keywords , a description , price , and release/availability dates . - @ @ @ @ @ @ @ @ @ @ click on the " Save " button to proceed . <p> Amazon Appstore App Merchandising Info <p> Next , you will have to specify content rating information . - Just fill out the information about your content , and click on the " Save " button to proceed . - I did n't  run into any content rating issues in the Amazon Appstore , like I did with the iOS App Store . <p> Amazon Appstore App Content Ratings <p> Next , upload multimedia that will be associated with the application . - This includes application icons ( note : they must match , event though this screenshot does n't  show it I was rejected b/c of this ) , and actual screenshots of the application . <p> Amazon Appstore App Multimedia <p> Scroll down to see more of the " Multimedia Content " form . - You will also be able to enter promotional images and promotional video assets for your application . - Once you have uploaded all necessary multimedia , click the " Done " button . <p> Amazon Appstore App Promotional Media <p> Now you are @ @ @ @ @ @ @ @ @ @ instructions for uploading an APK file . - Once it is uploaded you will see information about the file that was uploaded . <p> Amazon Appstore App Binary <p> Finally , click on the " Submit App " button to submit your application for approval . - There is an approval process for the Amazon Appstore similar to Apples , and my application was live in less than a week from submission . <p> I have to say Googles Android Market was by far the easiest in terms initial setup , upload and ongoing updates . But the Apple app stores still seems the most popular in terms of user downloads of the app. 
@@106848968 @2248968/ <h> Tag Archives : flash <p> AIR 3.2 and Flash Player 11.2- release candidates are now available for download on Adobe Labs . These latest versions have some exciting new features , including Stage3D for mobile devices , broader Stage3D support on desktop machines , multi-threaded video decoding , and better mouse support for gaming scenarios . - Get ready for some incredible mobile and desktop experiences powered by Adobe Flash &amp; AIR . <p> You can read more about what 's new in Flash Player 11.2 and AIR 3.2 on the Adobe Digital Media blog , or check out the videos below to see some of the new features in action . <p> To complement the white paper released last week covering the future of Flex and the transition to Apache , Adobe has released a white paper covering the the roadmap for the Flash and AIR runtimes . Flash is very much alive and well , and is continuing to evolve to be able to bring uncompromising rich experiences to the web , desktop , and mobile devices . You can read the white paper online at : @ @ @ @ @ @ @ @ @ @ the entire white paper for a clear outline of the future of Flash runtimes. - The future is going to be awesome . <h> Summary <p> For the past decade , Flash Player and , more recently , Adobe AIR have played a vital role on the web by providing consistent platforms for deploying rich , expressive content across browsers , desktops , and devices . Beginning as a platform for enabling animation , the Flash runtimes have evolved into a complete multimedia platform , enabling experiences that were otherwise not possible or feasible on the web . <p> Looking forward , Adobe believes that Flash is particularly suited for addressing the gaming and premium video markets , and will focus its development efforts in those areas . At the same time , Adobe will make architectural and language changes to the runtimes in order to ensure that the Flash runtimes are well placed to enable the richest experiences on the web and across mobile devices for another decade . <p> Back in the summer , I was lucky enough to get my hands on some early builds of Stage3D @ @ @ @ @ @ @ @ @ @ basic geometric shapes and simple 3D bubble charts inside of mobile Flex/AIR applications . I have been asked numerous times for the source code , and Ive finally given in , and am sharing some source code . <p> I am not posting the full mobile application source code , since Stage3D for mobile is not yet available . However , I have ported the 3D bubble chart example to run in a Flex application targeting the desktop ( Flash Player 11 ) . The bubble chart example extends the concepts explored in the basic geometric shapes example . <p> Before you say " shoot , he did n't  give us the mobile code " , let me explain When I ported the code from the mobile project to the desktop Flex project , all I changed was code specific to the mobile Flex framework . I changed **34;8719;TOOLONG to &lt;s:Application&gt; and the corresponding architecture changes that were required , and I changed the list item renderers to Spark item renderers based on &lt;s:Group&gt; instead of mobile item renderers. - In the mobile item renderers , all my drawing logic @ @ @ @ @ @ @ @ @ @ in the port , I just used &lt;s:Rect&gt; to add the colored regions in the desktop variant . <p> That is all I changed ! - <p> The stage3D code between the desktop and mobile implementations is identical . - - You can see the desktop port in action in the video below : <p> The source code was intended to be exploratory at best I was simply experimenting with hardware accelerated content , and how it can be used within your applications . - There is one big " gotcha " that you will have to watch out for if you want Stage3D content within a Flex application Stage3D content shows up behind Flex content on the display list . - By default , Flex apps have a background color , and they will hide the Stage3D content . - If you want to display any Stage3D content within a Flex application ( regardless of web , desktop AIR , or mobile ) , you must set the background alpha of the Flex application to zero ( 0 ) . - Otherwise you will pull out some hair trying @ @ @ @ @ @ @ @ @ @ <p> The source code for the web/Flex port of this example is available at : <p> Adobe Flash Player on desktop Adobe reaffirmed its commitment to the Adobe Flash Player in desktop browsers , and its role of enabling functionality on the web that is not otherwise possible . Flash Player 11 for PC browsers just introduced dozens of new features , including hardware accelerated 3D graphics for console-quality gaming and premium HD video with content protection . <p> <p> Adobe AIR for mobile Adobe reaffirmed its commitment to Adobe AIR for mobile devices , which allows developers and designers to create standalone applications using Adobe Flash technologies that can be deployed across mobile operating systems , including Apple iOS , Google Android and RIM BlackBerry Tablet OS . <p> Adobe AIR for desktop Adobe reconfirmed its commitment for its continued support for Adobe AIR applications running on the desktop . Adobe is actively working on the next version of Adobe AIR for the desktop . <p> <p> Adobe FlexAdobe announced its intention to contribute the Adobe Flex SDK open source project to the Apache Software Foundation for future governance @ @ @ @ @ @ @ @ @ @ evolution and future plans of Flex . It was also announced that Adobe Flex would be contributed to an open source software foundation . The result of which , was mass speculation , fear , uncertainty , and doubt . - Rest- assured , Flash is not dead , nor is Flex . <p> Falcon , the next-generation MXML and ActionScript compiler that is currently under development ( this will be contributed when complete in 2012 ) <p> Falcon JS , an experimental cross-compiler from MXML and ActionScript to HTML and JavaScript . <p> Flex testing tools , as used previously by Adobe , so as to ensure successful continued development of Flex with high quality <p> Is n't Adobe just abandoning Flex SDK and putting it out to Apache to die ? - <p> Absolutely not " we are incredibly proud of what we 've achieved with Flex and know that it will continue to provide significant value for many years to come . We expect active and on-going contributions from the Apache community . To be clear , Adobe plans on steadily contributing to the projects and we @ @ @ @ @ @ @ @ @ @ as well . <p> Flex has been open source since the release of Flex 3 SDK . What 's so different about what you are announcing now ? <p> Since Flex 3 , customers have primarily used the Flex source code to debug underlying issues in the Flex framework , rather than to actively develop new features or fix bugs and contribute them back to the SDK . <p> With Friday 's announcement , Adobe will no longer be the owner of the ongoing roadmap . Instead , the project will be in Apache and governed according to its well-established community rules.In this model , Apache community members will provide project leadership . We expect project management to include both Adobe engineers as well as key community leaders . Together , they will jointly operate in a meritocracy to define new features and enhancements for future versions of the Flex SDK . The Apache model has proven to foster a vibrant community , drive development forward , and allow for continuous commits from active developers . <p> What guarantees can Adobe make in relation to Flex applications continuing to run @ @ @ @ @ @ @ @ @ @ continue to support applications built with Flex , as well as all future versions of the SDK running in PC browsers with Adobe Flash Player and as mobile apps with Adobe AIR indefinitely on Apple iOS , Google Android and RIM BlackBerry Tablet OS . <h> Adobe AIR <p> We are continuing to develop Adobe AIR for both the desktop and mobile devices . Indeed , we have seen wide adoption of Adobe AIR for creating mobile applications and there have been a number of blockbuster mobile applications created using Adobe AIR . <h> Flash Player for Desktop Browsers <p> We feel that Flash continues to play a vital role of enabling features and functionality on the web that are not otherwise possible . As such , we have a long term commitment to the Flash Player on desktops , and are actively working on the next Flash Player version . 
@@106848969 @2248969/ <h> Tag Archives : open source <p> One criticism of PhoneGap apps that I sometimes hear is that they often do n't  have " standard " features from the native operating system . - Little things , like iOSs ability to scroll a container back to the top , just by tapping on the operating systems status bar . These types of features are not hard to add to a PhoneGap application , at all . This is more of an " attention to detail " issue , not something that the platform ca n't do . <p> Since this is n't a feature that is applicable on all platforms , and it can vary per PhoneGap app implementation , it is not part of the core PhoneGap/Cordova download . - However , this can be very easily added via a native plugin. - Native plugins enable you to access native code , or augment the capabilities of PhoneGap for a particular platform . <p> If you download the app from the app store today , you wo n't see this yet because I literally *just* submitted it to Apple . <p> @ @ @ @ @ @ @ @ @ @ hard . The first thing to do is check and see if there was an existing native plugin that has already been created by someone in the PhoneGap/Cordova community . - It turns out , Greg- pointed out- one that already existed . - Since this plugin was built targeting an older version of PhoneGap , and my project was built using PhoneGap 3.0 , I had a few minor updates . - Though , I was able to get everything all set up in a very short period of time . <p> Then , in your PhoneGap application , you just have to add an event listener for the " statusTap " event , which is triggered when the user taps on the operating systems status bar . - It is literally this simple : <p> This just shows an alert that the status bar was tapped . If you want to animate specific containers , you have to do this manually yourself via JavaScript . Again , that is n't hard to do . here 's an excerpt that I used from the Halloween app , using jQuery syntax : @ @ @ @ @ @ @ @ @ @ figured its about time to update my " Instant Halloween " PhoneGap sound effects app . I 'm happy to say that the latest version is now out for both iOS and Android . It has a few new sounds , a new UI style , and has been updated for iOS 7 . I also updated the low latency audio plugin- so it now supports PhoneGap 3.0 method signatures and supports the command line tooling for installation . <p> This app is fantastic for scaring people Just hook it up to really loud speakers , and start playing the sounds to your hearts content . Its got everything from background/ambient loops to maniacal laughter , screams , ghosts , zombies , and other spooky sound effects . <p> It is available now , for FREE , for both iOS ( 5.0+ ) and Android ( 4.0+ ) . <p> - <p> So what has changed in this version ? <p> First , I updated the app to support iOS 7 . For the most part , this is a non-issue . PhoneGap apps are based on web standards , and @ @ @ @ @ @ @ @ @ @ have to account for a few minor changes . One is that the OS status bar now sits over top of the application . You 'll need to update your UI on iOS 7 , so there are no UI issues . Check out this post from Christophe Coenraets for details regarding creating PhoneGap apps for iOS 7 . <p> iOS 7 also introduces some new UI design paradigms and guidelines . I simplified the user interface , got rid of all textures , and tried to make things as simple and minimalistic and native-feeling as possible . I also got rid of iScroll for touch-based scrolling both the iOS and Android versions now use native inertial based scrolling from the operating system . This is the reason that the new Android version is only Android 4.0 and later , but it is also the reason that the app feels much closer to a fully native experience . <p> I updated the low latency audio native plugin to support PhoneGap PhoneGap 3 . There were two parts to this : First , there is the updated method signature on the native @ @ @ @ @ @ @ @ @ @ updated it for the new method signature . The new method signature- was actually introduced a while back , but I never updated the plugin for it. - Second , I added the appropriate XML metadata to enable CLI-based installation of the plugin for both iOS and Android . Take a look at the PhoneGap documentation for details on creating PhoneGap native plugins- and plugin.xml. - Also check out this tutorial from Holly Schinsky for help creating native plugins for Android . <p> In the process , I also ran into an unexpected issue with Android deployment Back in the spring I had a corrupted hard drive . I was able to recover *most* of my data , and I thought I had all of my app signing keys . It turns out that for Android , your apps must not only have the same signing keys , but also the same key store when you sign the APK files , or else Google wont let you distribute the Android APK file as an update ; it must be a new app . It turns out that I had recovered @ @ @ @ @ @ @ @ @ @ I had no choice but to distribute it as a new app . <p> Go download the free apps , get the source code , and start building your own PhoneGap apps today ! <p> CSS regions is a revolutionary- CSS specification draft- that allows a deeper separation of concerns in the way designers and developers structure their content and layout . They can now manage the way content should flow across different regions of the page design ( hence the name CSS Regions ) separately from the content itself . Then content can now be made to flow in different chains of regions , typically laid out differently for a mobile , tablet or desktop/laptop use . <p> Reflow now supports CSS Regions in the user interface/design surface , and Adobe Edge Code now supports CSS Regions in code hinting . Check out the video below to see CSS Regions in Reflow in action : <p> About a year ago I released the Fresh Food Finder , a multi-platform mobile application built with PhoneGap . The Fresh Food Finder provides an easy way to search for farmers markets near @ @ @ @ @ @ @ @ @ @ from the USDA . This app has seen a lot of popularity lately , so I 'm working on a new version for all platforms with a better data feed , better UI , and overall better UX unfortunately , that version is n't ready yet . However , I have been able to bring it to an additional platform this week : Firefox OS ! <p> Fresh Food Finder on iOS , Firefox OS , &amp; Android <p> PhoneGap support is coming for Firefox OS , and in preparation I wanted to become familiar with the Firefox OS development environment and platform ecosystem . So I ported the Fresh Food Finder , minus the specific PhoneGap API calls . The best part ( and this really shows the power of web-standards based development ) is that I was able to take the existing PhoneGap codebase , and turn it into a Firefox OS app AND submit it to the Firefox Marketplace in under 24 hours ! If you 're interested , you can check out progress on Firefox OS support in the Cordova project , and it will be available on PhoneGap.com @ @ @ @ @ @ @ @ @ @ out the PhoneGap-specific API calls , added a few minor bug fixes , and added a few Firefox-OS specific layout/styling changes ( just a few minor things so that my app looked right on the device ) . Then you put in a mainfest.webapp configuration file , package it up , then submit it to the app store . Check it out in the video below to see it in action , running on a Firefox OS device <p> The phone I am using is a Geeksphone Firefox OS developer device . Its not a production/consumer model , so there were a few hiccups using it , but overall it was a good experience . Also , many thanks to Jason Weathersby from Mozilla for helping me get the latest device image running on my phone . <p> You can learn more about getting started with Firefox OS development here : <p> I 'd like to express a huge Thank You- to everyone who attended PhoneGap Day in Portland last week , and to Colene and the team that put everything together ! The day was loaded with fantastic presentations , @ @ @ @ @ @ @ @ @ @ great questions , and tons of great people ( oh , and beer it would n't be PhoneGap Day without beer ) . <p> My session was about the integration of PhoneGap with hardware . Basically , it was an overview and exploration how you can use native plugins to extend the capabilities of PhoneGap applications and interact with device peripherals . This enables new interaction paradigms , and helps evaluate and evolve what is attainable with web-related technologies . <p> In this session , I covered two use cases The first use case is the use of a pressure-sensitive stylus for interacting with a PhoneGap application on iOS . The second use case is integration of a Moga gamepad with a PhoneGap application on Android . In both cases , the application experience is augmented by the peripheral device , which changes how it is possible for the user to interact and engage with the application content and context . <h> Pressure Sensitive Sketching in PhoneGap <p> In this example , - the pressure-sensitive Pogo Connect Stylus uses a low energy Bluetooth 4 connection to relay touch/pressure information back to @ @ @ @ @ @ @ @ @ @ and sketching experience powered with the HTML5 Canvas element . I 've written about this example previously Check out the video below to see it in action , and read the blog post for technical details and source code . <h> Moga Gamepad <p> The second example that I explored is a PhoneGap native plugin that is used to handle input from a Moga game controller inside of a PhoneGap application on Android . <p> This implementation is intended to be a proof of concept demonstrating how you could integrate the gamepad within your application . It currently only supports input from the joysticks ( axisX and axisY ) and the A and B buttons , and it does not handle all possible input from the controller . <p> This implementation is adapted directly from the **38;8755;TOOLONG example from the Moga developers SDK samples available for download at : : - http : **34;8795;TOOLONG 123434 @qwx983434 <p> The game is based on the Universe- prototype that was used as a sub-game inside of the MaxMe app for the recent Adobe MAX conference . I make no guarantees about the code for @ @ @ @ @ @ @ @ @ @ 
@@106848970 @2248970/ <h> Tag Archives : misc <p> I 'm pleased to announce that as of next week ( July 26th ) I will be joining Adobe as a Technical Evangelist , with an emphasis on enterprise mobile applications ! Many of you know that Ive been passionate about the Adobe ecosystem for quite a few years I became really involved late in the Macromedia days , and have worked with Flex since 1.5 and Flash since version 3 ( now working on version 11 ) . I 've had the opportunity to work on some incredible projects , and a lot has changed throughout this time . I have been fortunate to witness the changes throughout the web that have been made possible by Adobes technologies , and I 'm even more excited that I now get to play a part in it . <p> In the last year I have primarily focused on mobile application development ( both phone and tablet paradigms ) native Objective-C for iOS , HTML5 , Appcelerator Titanium , PhoneGap , a little bit of native Android , and ( last , but most definitely not the least @ @ @ @ @ @ @ @ @ @ and iOS ) . Now , I am excited to use this experience to show how Adobes technologies can be used in web , desktop , and mobile scenarios , for building incredible applications on multiple platforms , with an easy-to-use and extremely powerful development paradigm . <p> I 'd also like to take a moment to thank my current employer , Universal Mind , for the opportunities that I 've had while working here . While with UM I have had a great experience , and I wish them many great successes in the future . It has been an awesome time , and I 've had the good fortune of being a part of some ground-breaking and exciting projects ( while also getting to work with some of the best and brightest in the consulting and design industry ) . <p> This is an exciting opportunity , and I am happy to share it with all of you . If you ever are in need of information or best practices regarding Adobe and/or mobile , please do not hesitate to ask ! <p> Well , hello again everyone ! Its been @ @ @ @ @ @ @ @ @ @ tricedesigns.com ) . This is the first post at this location in nearly 2 years . As I was writing for insideria.com this blog began to suffer . If you 're wondering where the old content has gone , I 've decided to take it down . - All of my content which was formerly at insideria.com is now available at developria.com . <p> I migrated this site from the old Blogger hosting since they decided to no longer support FTP publication , and a lot of the content that I had here was from 2006-2009 , so a fair amount of it was outdated . If there was something that you really miss , please let me know , and I will dig it up , update and re-post . <p> Expect to see this blog become a lot more active , with some significant updates in the near future . I have some exciting announcements coming soon , and you can expect to find a lot of content regarding mobile applications , mobile paradigms , and their applicability in the enterprise . 
@@106848972 @2248972/ <p> The reason that you can run a Flex Mobile app in the Android emulator , but not in the iOS Simulator comes down to the fundamental difference between an emulator and a simulator . An emulator emulates a physical device ; The emulator program mimics the hardware , and the device-specific code that will run on the actual device runs within the mimicked environment . A simulator simulates an environment it has a likeness or model of an environment , however it is not identical to the target environment . <p> In this case , the Android emulator mimics the hardware environment and is capable of running a compiled APK for a Flex/AIR mobile application . However , the iOS simulator is not capable of executing the contents of an IPA file . This is n't specific to an IPA file for an AIR mobile app , but any IPA file even those downloaded from Apples own app store . <p> The executable content within an IPA file is compiled targeting the devices A4-ARM processor . Your desktop computer uses an intel-based processor architecture , which is n't compatible and @ @ @ @ @ @ @ @ @ @ file to a ZIP file and extract the contents , it will not work within the iOS Simulator because of the CPU architecture differences . <p> Best Practice : <p> The first , and most important point that I emphasize regarding mobile application development is that nothing is more important than testing your mobile applications on a physical device . Emulators and simulators can help you see how an application may operate within a given environment , but they do not provide you with the actual environment . Physical devices may have memory , CPU , or other physical limitations that an emulator or simulator may not be able to reveal . <p> Secondly , keep in mind that emulators and simulators are created to make your development process easier and faster ( especially if hardware is not readily available for the entire dev team ) . The debugging environment within Flash Builder is designed exactly for that purpose . You can quickly and easily test your applications interface and functionality with a single button click . You can even setup debugging profiles for multiple devices , or use one @ @ @ @ @ @ @ @ @ @ make developing for multiple form factors and multiple device types significantly easier and faster , this does not trump my first point . If you are targeting specific hardware , then it is imperative that you test thoroughly on your target platform(s). 
@@106848974 @2248974/ <h> Tag Archives : projects <p> I 've finally caught up with some of my to-dos and have uploaded code for several of my recent projects to github . You can see a list of all of my projects at https : //github.com/triceam . As I blog , I 'll attempt to maintain a copy of all source code on github . As I get a chance , I 'll try to post some additional samples that I have sitting around . So far , I 've uploaded the following : <h> URL Monitor <p> This includes my URL Monitor application , available for iOS , Android , and BlackBerry , developed using Adobe AIR . The URL Monitor tool is a simple diagnostic application that will allow you to quickly and easily monitor the status of various URL endpoints . Simply enter a URL into the text box and add it to the list . A polling HTTP request will be made every 10 seconds to determine the availability of a given endpoint . HTTP codes 200 , 202 , 204 , 205 and 206 will be identified as a success with @ @ @ @ @ @ @ @ @ @ a problem as a red X. <h> Mobile Serialization Tester <p> The Flex Mobile Serialization Testing application is a basic scenario for testing performance between AMF and JSON in a Flex application . The mobile app makes requests of simple data objects from a ColdFusion CFC . In each test iteration , a request is made for 1 , 10 , 100 , 1000 , and 10000 value objects , in both AMF and JSON formats . The total round trip time from request to deserialization is measured and compared for each case , for a total of 5 iterations through each cycle . The application displays end-to-end performance metrics for both AMF and JSON requests . <h> Enterprise Tablet Visualization <p> The Enterprise Tablet Visualization application is a sample application built using Adobe Flex and AIR . The application is not a production application . It demonstrates realtime data push to a mobile application using LiveCycle Data Services , realtime multimedia collaboration using LiveCycle Collaboration Services , as well as multi-form-factor UI for both tablet and phone devices , including map integration and interactive data visualizations. 
@@106848975 @2248975/ <h> PhoneGap Day : PhoneGap &amp; Hardware <p> I 'd like to express a huge Thank You- to everyone who attended PhoneGap Day in Portland last week , and to Colene and the team that put everything together ! The day was loaded with fantastic presentations , and great community interaction tons of information , tons of great questions , and tons of great people ( oh , and beer it would n't be PhoneGap Day without beer ) . <p> My session was about the integration of PhoneGap with hardware . Basically , it was an overview and exploration how you can use native plugins to extend the capabilities of PhoneGap applications and interact with device peripherals . This enables new interaction paradigms , and helps evaluate and evolve what is attainable with web-related technologies . <p> In this session , I covered two use cases The first use case is the use of a pressure-sensitive stylus for interacting with a PhoneGap application on iOS . The second use case is integration of a Moga gamepad with a PhoneGap application on Android . In both cases , the application experience @ @ @ @ @ @ @ @ @ @ it is possible for the user to interact and engage with the application content and context . <h> Pressure Sensitive Sketching in PhoneGap <p> In this example , - the pressure-sensitive Pogo Connect Stylus uses a low energy Bluetooth 4 connection to relay touch/pressure information back to the PhoneGap application . This makes for a unique drawing and sketching experience powered with the HTML5 Canvas element . I 've written about this example previously Check out the video below to see it in action , and read the blog post for technical details and source code . <h> Moga Gamepad <p> The second example that I explored is a PhoneGap native plugin that is used to handle input from a Moga game controller inside of a PhoneGap application on Android . <p> This implementation is intended to be a proof of concept demonstrating how you could integrate the gamepad within your application . It currently only supports input from the joysticks ( axisX and axisY ) and the A and B buttons , and it does not handle all possible input from the controller . <p> This implementation is adapted directly @ @ @ @ @ @ @ @ @ @ available for download at : : - http : **34;8871;TOOLONG 123434 @qwx983434 <p> The game is based on the Universe- prototype that was used as a sub-game inside of the MaxMe app for the recent Adobe MAX conference . I make no guarantees about the code for this game , it was in a huge rush ! 
@@106848978 @2248978/ <h> Mobile Apps , Cognitive Computing , &amp; Wearables <p> Last week I was in good ole Las Vegas for IBM InterConnect IBMs largest conference of the year . With over 20,000 attendees , it was a fantastic event that covered everything from technical details for developers to forward-looking strategy and trends for C-level executives . IBM also made some big announcements for developers OpenWhisk serverless computing and bringing the Swift language to the server just to name a few . Both of these are exciting new initiatives- that offer radical changes &amp; simplification to developer workflows . <p> It was a busy week to say the least lots of presentations , a few labs , and even a role in the main stage Swift keynote . You can expect to find more detail on each of these here on the blog in the days/weeks to come . <p> For starters , here are two " lightning talks " I presented in the InterConnect Dev@ developer zone : <h> Smarter apps with Cognitive Computing <p> This session introduces the concept of cognitive computing , and demonstrates how you can @ @ @ @ @ @ @ @ @ @ If you are n't  familiar with cognitive computing , then I strongly recommend that you check out this post : The Future of Cognitive Computing . <p> In the presentation below , I show two apps leveraging services on Bluemix , IBMs Cloud computing platform , and the iOS SDK for Watson . <p> Actually , I 'm using two Watson SDKs The older Speech SDK for iOS , and the new iOS SDK. - I 'm using the older speech SDK in one example because it supports continuous listening for Watson Speech To Text , which is currently still in development for the new SDK . <h> Redefining your personal mobile expression with on-body computing <p> My second presentation highlighted how we can use on-body computing devices to change how we interact with systems and data . - For example , we can use a luxury smart watch ( ex : Apple Watch ) to consume and engage with data in more efficient and more personal ways . - Likewise , we can also use smart/wearable peripherals devices to access and act on data in ways that were- never possible- before @ @ @ @ @ @ @ @ @ @ based upon patterns in raw data transmitted by the on-body devices . - For this , I leveraged the new IBM Wearables SDK. - The IBM Wearables SDK provides a consistent interface/abstraction layer for interacting with wearable sensors . - This allows you to focus on building your apps that interact with the data , rather thank learning the ins &amp; outs of a new device-specific SDK . <p> The wearables SDK also users data interpretation algorithms to enable you to define gestures or patterns in the data , and use those patterns to act upon events when they happen without additional user interaction . - For example : you can determine if someone falls down , you can determine when someone is raising their hand , you can determine anomalies in heart rate or skin temperature , and much more . - The system is capable of learning patterns for any type of action or virtually any data being submitted to the system . - Sound interesting ? - Then check it out here . <p> I also had some other- sessions- on integrating drones with cloud services , @ @ @ @ @ @ @ @ @ @ . - I 'll be sure to post updates for this- content- I make them publicly available . - I think you 'll find the session on drones + cloud especially interesting I know I did . 
@@106848979 @2248979/ <p> IBM Watson services , which are based on machine learning algorithms , - give you the ability- to handle unstructured data , like text analysis or- translation , speech processing , and more . - This makes consumption , mining , or responding to unstructured data or " dark data " faster , more efficient , and more powerful than ever . <p> The new Watson iOS SDK- provides- developers with an API- to simplify integration of the- Watson Developer Cloud services into their mobile apps , including the Dialog , Language Translation , Natural Language Classifier , Personality Insights , Speech To Text , Text to Speech , Alchemy Language , or Alchemy Vision services " all of which are available today , and can now be integrated with just a few lines of code . <p> The- Watson iOS SDK makes integration with Watson services significantly *really* easy . For example , if you want to take advantage of the Language Translation service , you first have to setup a service instance . Once the translation service is setup , then you 'll be able to @ @ @ @ @ @ @ @ @ @ sure to check out the sample 's readme for additional detail and setup instructions . As with all of the Watson services , You must have a service instance properly configured , with authentication credentials- in order to be able to consume it within your app. 
@@106848981 @2248981/ <h> Tag Archives : flash <p> AIR 3.2 and Flash Player 11.2- release candidates are now available for download on Adobe Labs . These latest versions have some exciting new features , including Stage3D for mobile devices , broader Stage3D support on desktop machines , multi-threaded video decoding , and better mouse support for gaming scenarios . - Get ready for some incredible mobile and desktop experiences powered by Adobe Flash &amp; AIR . <p> You can read more about what 's new in Flash Player 11.2 and AIR 3.2 on the Adobe Digital Media blog , or check out the videos below to see some of the new features in action . <p> To complement the white paper released last week covering the future of Flex and the transition to Apache , Adobe has released a white paper covering the the roadmap for the Flash and AIR runtimes . Flash is very much alive and well , and is continuing to evolve to be able to bring uncompromising rich experiences to the web , desktop , and mobile devices . You can read the white paper online at : @ @ @ @ @ @ @ @ @ @ the entire white paper for a clear outline of the future of Flash runtimes. - The future is going to be awesome . <h> Summary <p> For the past decade , Flash Player and , more recently , Adobe AIR have played a vital role on the web by providing consistent platforms for deploying rich , expressive content across browsers , desktops , and devices . Beginning as a platform for enabling animation , the Flash runtimes have evolved into a complete multimedia platform , enabling experiences that were otherwise not possible or feasible on the web . <p> Looking forward , Adobe believes that Flash is particularly suited for addressing the gaming and premium video markets , and will focus its development efforts in those areas . At the same time , Adobe will make architectural and language changes to the runtimes in order to ensure that the Flash runtimes are well placed to enable the richest experiences on the web and across mobile devices for another decade . <p> Back in the summer , I was lucky enough to get my hands on some early builds of Stage3D @ @ @ @ @ @ @ @ @ @ basic geometric shapes and simple 3D bubble charts inside of mobile Flex/AIR applications . I have been asked numerous times for the source code , and Ive finally given in , and am sharing some source code . <p> I am not posting the full mobile application source code , since Stage3D for mobile is not yet available . However , I have ported the 3D bubble chart example to run in a Flex application targeting the desktop ( Flash Player 11 ) . The bubble chart example extends the concepts explored in the basic geometric shapes example . <p> Before you say " shoot , he did n't  give us the mobile code " , let me explain When I ported the code from the mobile project to the desktop Flex project , all I changed was code specific to the mobile Flex framework . I changed **34;8983;TOOLONG to &lt;s:Application&gt; and the corresponding architecture changes that were required , and I changed the list item renderers to Spark item renderers based on &lt;s:Group&gt; instead of mobile item renderers. - In the mobile item renderers , all my drawing logic @ @ @ @ @ @ @ @ @ @ in the port , I just used &lt;s:Rect&gt; to add the colored regions in the desktop variant . <p> That is all I changed ! - <p> The stage3D code between the desktop and mobile implementations is identical . - - You can see the desktop port in action in the video below : <p> The source code was intended to be exploratory at best I was simply experimenting with hardware accelerated content , and how it can be used within your applications . - There is one big " gotcha " that you will have to watch out for if you want Stage3D content within a Flex application Stage3D content shows up behind Flex content on the display list . - By default , Flex apps have a background color , and they will hide the Stage3D content . - If you want to display any Stage3D content within a Flex application ( regardless of web , desktop AIR , or mobile ) , you must set the background alpha of the Flex application to zero ( 0 ) . - Otherwise you will pull out some hair trying @ @ @ @ @ @ @ @ @ @ <p> The source code for the web/Flex port of this example is available at : <p> Adobe Flash Player on desktop Adobe reaffirmed its commitment to the Adobe Flash Player in desktop browsers , and its role of enabling functionality on the web that is not otherwise possible . Flash Player 11 for PC browsers just introduced dozens of new features , including hardware accelerated 3D graphics for console-quality gaming and premium HD video with content protection . <p> <p> Adobe AIR for mobile Adobe reaffirmed its commitment to Adobe AIR for mobile devices , which allows developers and designers to create standalone applications using Adobe Flash technologies that can be deployed across mobile operating systems , including Apple iOS , Google Android and RIM BlackBerry Tablet OS . <p> Adobe AIR for desktop Adobe reconfirmed its commitment for its continued support for Adobe AIR applications running on the desktop . Adobe is actively working on the next version of Adobe AIR for the desktop . <p> <p> Adobe FlexAdobe announced its intention to contribute the Adobe Flex SDK open source project to the Apache Software Foundation for future governance @ @ @ @ @ @ @ @ @ @ evolution and future plans of Flex . It was also announced that Adobe Flex would be contributed to an open source software foundation . The result of which , was mass speculation , fear , uncertainty , and doubt . - Rest- assured , Flash is not dead , nor is Flex . <p> Falcon , the next-generation MXML and ActionScript compiler that is currently under development ( this will be contributed when complete in 2012 ) <p> Falcon JS , an experimental cross-compiler from MXML and ActionScript to HTML and JavaScript . <p> Flex testing tools , as used previously by Adobe , so as to ensure successful continued development of Flex with high quality <p> Is n't Adobe just abandoning Flex SDK and putting it out to Apache to die ? - <p> Absolutely not " we are incredibly proud of what we 've achieved with Flex and know that it will continue to provide significant value for many years to come . We expect active and on-going contributions from the Apache community . To be clear , Adobe plans on steadily contributing to the projects and we @ @ @ @ @ @ @ @ @ @ as well . <p> Flex has been open source since the release of Flex 3 SDK . What 's so different about what you are announcing now ? <p> Since Flex 3 , customers have primarily used the Flex source code to debug underlying issues in the Flex framework , rather than to actively develop new features or fix bugs and contribute them back to the SDK . <p> With Friday 's announcement , Adobe will no longer be the owner of the ongoing roadmap . Instead , the project will be in Apache and governed according to its well-established community rules.In this model , Apache community members will provide project leadership . We expect project management to include both Adobe engineers as well as key community leaders . Together , they will jointly operate in a meritocracy to define new features and enhancements for future versions of the Flex SDK . The Apache model has proven to foster a vibrant community , drive development forward , and allow for continuous commits from active developers . <p> What guarantees can Adobe make in relation to Flex applications continuing to run @ @ @ @ @ @ @ @ @ @ continue to support applications built with Flex , as well as all future versions of the SDK running in PC browsers with Adobe Flash Player and as mobile apps with Adobe AIR indefinitely on Apple iOS , Google Android and RIM BlackBerry Tablet OS . <h> Adobe AIR <p> We are continuing to develop Adobe AIR for both the desktop and mobile devices . Indeed , we have seen wide adoption of Adobe AIR for creating mobile applications and there have been a number of blockbuster mobile applications created using Adobe AIR . <h> Flash Player for Desktop Browsers <p> We feel that Flash continues to play a vital role of enabling features and functionality on the web that are not otherwise possible . As such , we have a long term commitment to the Flash Player on desktops , and are actively working on the next Flash Player version . 
@@106848982 @2248982/ <p> The reason that you can run a Flex Mobile app in the Android emulator , but not in the iOS Simulator comes down to the fundamental difference between an emulator and a simulator . An emulator emulates a physical device ; The emulator program mimics the hardware , and the device-specific code that will run on the actual device runs within the mimicked environment . A simulator simulates an environment it has a likeness or model of an environment , however it is not identical to the target environment . <p> In this case , the Android emulator mimics the hardware environment and is capable of running a compiled APK for a Flex/AIR mobile application . However , the iOS simulator is not capable of executing the contents of an IPA file . This is n't specific to an IPA file for an AIR mobile app , but any IPA file even those downloaded from Apples own app store . <p> The executable content within an IPA file is compiled targeting the devices A4-ARM processor . Your desktop computer uses an intel-based processor architecture , which is n't compatible and @ @ @ @ @ @ @ @ @ @ file to a ZIP file and extract the contents , it will not work within the iOS Simulator because of the CPU architecture differences . <p> Best Practice : <p> The first , and most important point that I emphasize regarding mobile application development is that nothing is more important than testing your mobile applications on a physical device . Emulators and simulators can help you see how an application may operate within a given environment , but they do not provide you with the actual environment . Physical devices may have memory , CPU , or other physical limitations that an emulator or simulator may not be able to reveal . <p> Secondly , keep in mind that emulators and simulators are created to make your development process easier and faster ( especially if hardware is not readily available for the entire dev team ) . The debugging environment within Flash Builder is designed exactly for that purpose . You can quickly and easily test your applications interface and functionality with a single button click . You can even setup debugging profiles for multiple devices , or use one @ @ @ @ @ @ @ @ @ @ make developing for multiple form factors and multiple device types significantly easier and faster , this does not trump my first point . If you are targeting specific hardware , then it is imperative that you test thoroughly on your target platform(s). 
@@106848984 @2248984/ <h> Fun With Photoshop <p> Some days you are just meant to be creative I think yesterday was that day for me . I saw that Erik Johansson released a new composition , and it sparked a wave of creativity within me . I discovered Eriks work via Adobe Max this year his presentation was awesome and very inspiring . Seriously , do yourself a favor and go watch it . <p> I digress I 've been doing a lot of aerialphotography lately , and after seeing Eriks latest composition I thought to myself : why not take some of my aerial photos and start altering reality ? My immediate idea was to create a surrealistic composition where you are looking down through a crystal ball . Next thing you know , this happened <p> Here are a few iterations from the creative spark . I think v.2 ( without the flames is my favorite ) : <p> Crystal Ball v.1Crystal Ball v.2Crystal Ball v.2 with Flames <p> Attribution : <p> The aerial image was mine , captured with a DJI Phantom and GoPro- camera . Check out the original image on Flickr. 
@@106848985 @2248985/ <h> Category Archives : Mobile <p> Get ready for 360Flex 2012 ! What better place to network and learn the latest about Flex development for mobile , the desktop , and the browser ? 360Flex is a chance to interact with the community and even get involved in the open-source Apache Flex project . The 360Flex 2012 schedule is now available , and its loaded with great presentations from leaders in the Flex community . <p> Im excited to have been chosen for 360Flex this year , focusing on Multi-Device Best Practices . <p> Multi-Device Best Practices Ready to bring the " write once , run everywhere " dream to a wonderful reality ? In this session we will focus on multi-device deployment considerations and best practices to help bring your applications to as many platforms , devices , and form factors as possible . <p> Adobe Flash Player on desktop Adobe reaffirmed its commitment to the Adobe Flash Player in desktop browsers , and its role of enabling functionality on the web that is not otherwise possible . Flash Player 11 for PC browsers just introduced dozens of @ @ @ @ @ @ @ @ @ @ gaming and premium HD video with content protection . <p> <p> Adobe AIR for mobile Adobe reaffirmed its commitment to Adobe AIR for mobile devices , which allows developers and designers to create standalone applications using Adobe Flash technologies that can be deployed across mobile operating systems , including Apple iOS , Google Android and RIM BlackBerry Tablet OS . <p> Adobe AIR for desktop Adobe reconfirmed its commitment for its continued support for Adobe AIR applications running on the desktop . Adobe is actively working on the next version of Adobe AIR for the desktop . <p> <p> Adobe FlexAdobe announced its intention to contribute the Adobe Flex SDK open source project to the Apache Software Foundation for future governance . <p> Last week Adobe announced information about the companys evolution and future plans of Flex . It was also announced that Adobe Flex would be contributed to an open source software foundation . The result of which , was mass speculation , fear , uncertainty , and doubt . - Rest- assured , Flash is not dead , nor is Flex . <p> Falcon , the next-generation MXML @ @ @ @ @ @ @ @ @ @ will be contributed when complete in 2012 ) <p> Falcon JS , an experimental cross-compiler from MXML and ActionScript to HTML and JavaScript . <p> Flex testing tools , as used previously by Adobe , so as to ensure successful continued development of Flex with high quality <p> Is n't Adobe just abandoning Flex SDK and putting it out to Apache to die ? - <p> Absolutely not " we are incredibly proud of what we 've achieved with Flex and know that it will continue to provide significant value for many years to come . We expect active and on-going contributions from the Apache community . To be clear , Adobe plans on steadily contributing to the projects and we are working with the Flex community to make them contributors as well . <p> Flex has been open source since the release of Flex 3 SDK . What 's so different about what you are announcing now ? <p> Since Flex 3 , customers have primarily used the Flex source code to debug underlying issues in the Flex framework , rather than to actively develop new features or fix @ @ @ @ @ @ @ @ @ @ With Friday 's announcement , Adobe will no longer be the owner of the ongoing roadmap . Instead , the project will be in Apache and governed according to its well-established community rules.In this model , Apache community members will provide project leadership . We expect project management to include both Adobe engineers as well as key community leaders . Together , they will jointly operate in a meritocracy to define new features and enhancements for future versions of the Flex SDK . The Apache model has proven to foster a vibrant community , drive development forward , and allow for continuous commits from active developers . <p> What guarantees can Adobe make in relation to Flex applications continuing to run on Flash Player and Adobe AIR ? <p> Adobe will continue to support applications built with Flex , as well as all future versions of the SDK running in PC browsers with Adobe Flash Player and as mobile apps with Adobe AIR indefinitely on Apple iOS , Google Android and RIM BlackBerry Tablet OS . <h> Adobe AIR <p> We are continuing to develop Adobe AIR for both the @ @ @ @ @ @ @ @ @ @ wide adoption of Adobe AIR for creating mobile applications and there have been a number of blockbuster mobile applications created using Adobe AIR . <h> Flash Player for Desktop Browsers <p> We feel that Flash continues to play a vital role of enabling features and functionality on the web that are not otherwise possible . As such , we have a long term commitment to the Flash Player on desktops , and are actively working on the next Flash Player version . <p> Hi Everyone ! Here are a few events that I 'll be speaking at/attending in the remainder of 2011 . Come check out Adobes cross-platform tooling , and feel free to bring your questions for me . I hope to see you at any one of these events ! <h> MoDevDC Meetup <p> Tonight ! 11/02/2011 This months MoDevDC ( Mobile Developers DC ) tech meetup is all about cross-platform development tools . Stop by to see a high level overview of Adobes cross-platform mobile development offerings , including AIR and PhoneGap . This will include the basics of " what are these tools " , as well @ @ @ @ @ @ @ @ @ @ 11/15/2011 Here is a chance for you to learn about another method to develop Android applications ( and make them compatible across platforms ) from an expert from Adobe . You will also have the chance to win a copy of Flash Builder 4.5 . I will walk through the processes of building Android &amp; cross-platform applications using both Adobe AIR and PhoneGap ( a cross-platform mobile application development framework ) . This session will cover demo applications , as well as real-world coding and best LONG ... <h> MoDevEast Conference &amp; Hackathon <p> 12/02/2011 12/03/2011 MoDevEast is where mobile developers and marketers gain the upper edge . If you are developing apps or mobile websites , targeting phones or tablets , staying ahead is paramount in this fast moving industry . MoDevEast 2011 will offer five tracks that hone development skills and sharpen mobile business strategy . Ill be speaking on cross-platform mobile development , and I 'll definitely be there for the event and hackathon ! http : //www.modeveast.com/ <h> Update ( 3/28/2012 ) : - <p> Hi Everyone , I know there have been lots of questions about @ @ @ @ @ @ @ @ @ @ Google 's Android SDK keeps changing and breaking the Dreamweaver integration . Fret not ! There is an easier way " Adobe has released a plugin to integrate Dreamweaver with PhoneGap Build . With this plugin , you build your experience in Dreamweaver , then push to the PhoneGap Build service for cloud-based compilation of device-specific binaries . Read more about this plugin here : - LONG ... <h> Original Post : <p> PhoneGap apps are built with HTML and JavaScript , and can be created with any IDE or text editor . You can build them in xCode or Eclipse . Did you also know that you can build PhoneGap apps within Dreamweaver , and you can even launch and debug on the iOS Simulator and Android Emulator all from within Dreamweaver ? <p> here 's a video of this in action from Adobe TV . After the video , well walk through this process step by step . <p> In order to use the iOS simulator and Android emulator , you 'll need to download and install xCode and the Android SDK. - Once you 've downloaded those , let 's focus @ @ @ @ @ @ @ @ @ @ first thing that you need to do is create a new " site " within Dreamweaver for your PhoneGap application . - Go to the " Site " menu , and select " New Site " . <p> The site setup/details dialog will be displayed . - Go ahead and give it a name and directory to contain project files and resources . <p> This will create a new HTML file for the mobile project . - Go ahead and save the file you just created . - The first time this file is saved , you will be prompted to copy dependent files . - Click " Copy " to copy the dependent files into your application " site " . <p> Once in Dreamweaver , you can edit the HTML and JavaScript to your hearts content . - You can take full advantage of Dreamweavers code view or design view , live previews , and any other features . <p> Once you are ready to build and deploy to the android and iOS simulators , you 'll need to setup the mobile development configuration . - First , well @ @ @ @ @ @ @ @ @ @ , go to the " Site " menu , select " Mobile Applications " , then select " Configure Application Framework " . <p> The " Configure Application Framework " dialog will be displayed . - Here , you 'll need to enter the full path to your Android SDK , and the path to the iOS Developer tools ( xCode ) . <p> Within the " Native Application Settings " dialog , you 'll need to specify your application bundle i 'd ( the unique i 'd for the application ) , an application name , the author , version , and application icons . - You can also select iOS SDK versions , and which Android emulator to use . <p> Once you 've configured the application settings and application frameworks , you are ready to build your app and run in the emulators. - Just go to the " Site " menu , select " Mobile Applications " , select " Build and Emulate " , then choose a device or platform . <p> Once you chose a platform or device , Dreamweaver will go ahead and launch the appropriate emulator or @ @ @ @ @ @ @ @ @ @ still have issues with deploying to Android and see the error message below in the build log , then you are probably using the latest Android SDK that was recently released in October . <p> Install file not specified . ' ant install ' now requires the build target to be specified as well . ant debug install ant release install ant instrument install This will build the given package and install it . Alternatively , you can use ant installd ant installr ant installi ant installt to only install an existing package ( this will not rebuild the package . ) <p> The latest Android SDK introduced an additional parameter that is not yet supported by the PhoneGap integration kit within Dreamweaver . You can fix this by updating the build.xml file for the application instance to override the " install " target and add the required dependencies which make this error go away . <p> Go to you application build director and open the " build.xml " file . This will be inside a folder named after the bundleID within your target directory . You can find the @ @ @ @ @ @ @ @ @ @ as shown below : <p> In my case , the build directory is LONG ... <p> Find the " import " node below ( at the end of the file ) : <p> &lt;import file= " **28;9019;TOOLONG " /&gt; <p> Add to this line the attribute as= " imported " and a new " install " target that will override the existing " install " target as shown below . This build target will utilize the existing " install " target , and add necessary debug file dependencies to fix the build error shown above . <p> Not only is Dreamweaver CS5.5 a best-of-breed solution for building web content , with PhoneGap support , Dreamweaver is now a best-of-breed solution for building cross-platform mobile applications as well . Using the PhoneGap integration within Dreamweaver allows you to use familiar tools , familiar development processes , and your current web development skills to build exciting new mobile applications . This enables you , as the developer or designer to focus on what matters " the application or content within your mobile scenarios . 
@@106848986 @2248986/ <h> Adobe Evangelists Magazine on Flipboard <p> Do you like keeping up with the latest news and updates from Adobe ? Do you like reading the latest posts and articles from all of the Adobe Evangelists ? Do you like seeing featured/highlighted content from Adobe ? Do you like reading your content onFlipboard ? <p> The- Adobe Evangelists Magazine- is a manually curated aggregation of all of our blogs , plus articles and posts that we find interesting and applicable , all combined into one great resource . All links point to the original source content , not a copy . The intent is to provide a one-stop-shop for Adobe Evangelists goodness . 
@@106848987 @2248987/ <h> Category Archives : Uncategorized <p> Did you know that you can determine whether your mobile device is on wifi or your mobile data connection when using Adobe AIR for mobile ? To be honest , I was n't aware of it either until yesterday when my friend and former colleague Brian OConnor pointed me towards a recent tweet from @adobecookbook that showed an example how to do it . <p> These networking APIs have been around since AIR 2.0 , but I 've seldom had the need to dig into them for the desktop . Now that AIR for mobile is widely available , this can be critically important for your applications . For example , what if you want to minimize network usage while on the mobile network ? This may even prevent you being chastised for eating up expensive mobile bandwidth . <p> Using the NetworkInfo class findInterfaces() method , you can retrieve a list of all network interfaces on your computer/device . If you iterate through these , you can see which are active , what their IP and MAC addresses are , and even which IP @ @ @ @ @ @ @ @ @ @ ( code below the video ) : <p> Network Detection on AIR Mobile <p> I know that video is a little hard to see , so here are some screen captures . First , a capture showing the mobile network connection active : <p> Mobile Network Active <p> Next , a capture showing the WIFI network active : <p> WIFI Network Active <p> Basically , I just have a list that shows all of the network interfaces . When the app loads , or whenever the network connection changes , the content of that list is updated to reflect the current state of the network interfaces . If you want to determine whether you are on wifi , you can compare the name of the active network interface to see if it contains the string " wifi " , as shown in the Adobe Cookbook . <p> One thing not to forget : You must make sure that your application has been provisioned to allow access to network interfaces ! You 'll just need to uncomment the Android permissions in your app.xml for network state : <p> Once you setup the @ @ @ @ @ @ @ @ @ @ ? - Well , almost This is an IIS server , running ColfFusion and PHP. - While the wordpress blog was working great , I realized that all of my AMF remoting on the server stopped working . - It turns out that the rewrite rules were affecting all pages that do n't  physically exist . - This includes the " /Flex2Gateway/ " endpoint used by ColdFusion to resolve all AMF requests . - Note : the " Flex2Gateway " url path does n't  point to a physical folder or file ; it is a virtual mapping . <p> In order for WordPress and ColdFusion to play nicely together on the same server , you will need to setup your rewrite rules to map all URLs that do n't  contain the string " flex2gateway " . - Its not that complicated really , but took a little while to track down what exactly was happening . - I used the ECMAScript pattern syntax for a regular expression that will match all strings that do n't  include " flex2gateway " , and here 's the- final solution : <p> I 'm pleased to @ @ @ @ @ @ @ @ @ @ I will be joining Adobe as a Technical Evangelist , with an emphasis on enterprise mobile applications ! Many of you know that Ive been passionate about the Adobe ecosystem for quite a few years I became really involved late in the Macromedia days , and have worked with Flex since 1.5 and Flash since version 3 ( now working on version 11 ) . I 've had the opportunity to work on some incredible projects , and a lot has changed throughout this time . I have been fortunate to witness the changes throughout the web that have been made possible by Adobes technologies , and I 'm even more excited that I now get to play a part in it . <p> In the last year I have primarily focused on mobile application development ( both phone and tablet paradigms ) native Objective-C for iOS , HTML5 , Appcelerator Titanium , PhoneGap , a little bit of native Android , and ( last , but most definitely not the least ) Adobe AIR for mobile ( BlackBerry , Android , and iOS ) . Now , I am excited to @ @ @ @ @ @ @ @ @ @ used in web , desktop , and mobile scenarios , for building incredible applications on multiple platforms , with an easy-to-use and extremely powerful development paradigm . <p> I 'd also like to take a moment to thank my current employer , Universal Mind , for the opportunities that I 've had while working here . While with UM I have had a great experience , and I wish them many great successes in the future . It has been an awesome time , and I 've had the good fortune of being a part of some ground-breaking and exciting projects ( while also getting to work with some of the best and brightest in the consulting and design industry ) . <p> This is an exciting opportunity , and I am happy to share it with all of you . If you ever are in need of information or best practices regarding Adobe and/or mobile , please do not hesitate to ask ! <p> Well , hello again everyone ! Its been a while since I 've been active on this blog ( tricedesigns.com ) . This is the first post at this @ @ @ @ @ @ @ @ @ @ for insideria.com this blog began to suffer . If you 're wondering where the old content has gone , I 've decided to take it down . - All of my content which was formerly at insideria.com is now available at developria.com . <p> I migrated this site from the old Blogger hosting since they decided to no longer support FTP publication , and a lot of the content that I had here was from 2006-2009 , so a fair amount of it was outdated . If there was something that you really miss , please let me know , and I will dig it up , update and re-post . <p> Expect to see this blog become a lot more active , with some significant updates in the near future . I have some exciting announcements coming soon , and you can expect to find a lot of content regarding mobile applications , mobile paradigms , and their applicability in the enterprise . 
@@106848988 @2248988/ <h> Adaptive mobile apps that change based on personal context <p> That title get your attention ? - Yes , it really read " Adaptive- mobile- apps that- change based on personal context " with near real-time rules application , without much extra development effort . - If that sounds interesting to you , or like a product you might want to use within your own apps , then you might want to check out this site where you can get involved in the products development : - http : **35;9049;TOOLONG <p> IBM is looking for your input on creating these types of mobile app experiences. - User experiences within a single app that can be- dramatically different per user based on location , past behavior , profile information , social media activity , - and so much more . - With this behavior being driven by configurable rules that can be changed without redeploying an app to the app store . <h> How it works for your customer <p> Consider this scenario : <p> Jon and Andrea download the mobile app for S&amp;W , a retailer known for @ @ @ @ @ @ @ @ @ @ next month , Jon and Andrea use the app to browse and discover content and merchandise differently . <p> Jon primarily navigates to sports related content for his favorite teams to find gear and clothes for travel to his favorite team 's games . Andrea scours the app for sales and fashion trends and usually ends up following her favorite designers . <p> Andrea and Jon go to a baseball game together . She 's never enjoyed watching it , so she opens up the S&amp;W app to entertain herself , and her app 's navigation quickly steers her through Spring fashion articles . <p> Jon however , wants to replace the hat he 's worn the last three times the team lost , and since he 's in the stadium , his S&amp;W app opens right up to the team 's gear page . The app knows he 's out of town and tells him how to get to an S&amp;W store . <h> How it works for the dev team <p> Consider another scenario : <p> One of the developers on the team , George , sets up @ @ @ @ @ @ @ @ @ @ Janet who is responsible for the customer experience . <p> Janet writes rules defining how the application could adapt and become more personalized based on inputs like , social media , geolocation , app usage , or customer information data . <p> Once Janet has built out her rules , she simply hits Submit ' and can immediately see her clever interactions reflected in the mobile application without having to involve the development team . <p> Analytics let Janet know which adaptations are working best , and helps her find new opportunities to optimize the app 's user experience . <p> Were not talking about a content management system , or translation based on locale , instead a rules-driven product that can adapt literally every aspect of your app : - customize the user interface , enable or disable different features , customized messaging and notifications , and much more , all variable based upon the user context . - This can be used to present- contextually relevant information , drive adoption , provide more/less data depending on your physical context , and so much more . <p> It wont @ @ @ @ @ @ @ @ @ @ tied to a specific content management system , is n't attempting to re-create Google Now or Apple Proactive Assistance . - Rather , a set of tools and a rules engine that enable you to customize and tailor the app experience to the individual user . 
@@106848989 @2248989/ <h> Monthly Archives : July 2012 <p> Earlier this month the PhoneGap team held the first PhoneGap day . - This was in part to celebrate the release of PhoneGap 2.0 , but more importantly to bring together members of the PhoneGap community to share and learn from each other . - There are great recaps of PhoneGap Day from RedMonk , as well as on the PhoneGap blog . One of the new services announced on PhoneGap Day was- emulate.phonegap.com. - Emulate.phonegap.com enables an in-browser simulator for developing and debugging PhoneGap/Cordova applications , complete with Cordova API emulation . - It is built off of the Ripple Emulator , which itself is open source and may even be contributed to the Apache Cordova project . <p> Once launched , the URL that you want to simulate will be displayed within the Ripple operating environment view . <p> Note : This only works with assets that are on a live URL . You can use a local http server with references to localhost , however the emulator will fail if you try to access your application directly from the @ @ @ @ @ @ @ @ @ @ <p> Update : You can enable access to local files by changing a few settings on the Ripple emulator . - See the first comment on this post for additional detail . <p> ( click for full-size image ) <p> The emulator environment gives you the ability to emulate PhoneGap events and API calls , without having to deploy to a device or run inside of the iOS , Android , Blackberry , or other emulator . - Not only can you simulate the PhoneGap/Cordova API , but you can also use Chromes debugging tools to test &amp; debug your code complete with breakpoints , memory inspection , and resource monitoring . This is a handy development configuration. - It enables app development within a rich debugging environment that is familiar , fast &amp; easy to use . This does not replace on-device debugging however nothing will replace that . - On-device debugging is extremely important ; this helps increase your productivity as a developer . <p> So how do you use this environment ? The environment will handle Cordova API requests , and you can also simulate device events @ @ @ @ @ @ @ @ @ @ this panel you can select a device configuration Everything from iOS , to Android , to BlackBerry . - Changing the device configuration will not only change the physical dimensions , but will also change Device/OS/user agent settings reported by the application . - Here you can also select the device orientation , which will change the visual area within the simulator . <p> Within the " Platforms " panel you can choose the platform you wish to emulate . - With respect to PhoneGap applications , you will want to choose " Apache Cordova " , and then select the API version that you are using . - By default , it uses " PhoneGap 1.0.0 ? , however you can chose " Apache Cordova 2.0.0 ? to get the most recent version . - The Ripple emulator also simulates BlackBerry WebWorks and mobile web configurations as well . <p> The " Accelerometer " panel can be used to simulate device **25;9086;TOOLONG events . - Just click and drag on the device icon ( the gray and black boxes ) , and the icon will rotate in 3D. - @ @ @ @ @ @ @ @ @ @ handled within your application . - From here , you can even trigger a " shake " event . <p> The " Geolocation " panel enables you to simulate your geographic position within your PhoneGap/Cordova application . - You can specify a latitude , longitude , altitude , speed , etc - You can even drag the map and use it to specify your geographic position . The position that you set within the geolocation panel will be reported when using **42;9113;TOOLONG . <p> The " Config " panel is a graphical representation of your PhoneGap BuildConfig.xml file . - You can use this to easily view/analyze what 's in your application configuration . <p> The " Events " panel can be used to simulate PhoneGap specific events , including " deviceready " , " backbutton " , " menubutton " , " online " , and " offline " ( among others ) . - Just select the event type , and click on the " Fire Event " button . <p> As I mentioned earlier , this wont replace on-device debugging . - It also wont handle execution of @ @ @ @ @ @ @ @ @ @ test/develop against the JavaScript interfaces for those native plugins. - Emulate.phonegap.com- will definitely help with development of PhoneGap applications in many scenarios , and is a nice complement to the- Chrome Developer Tools . <p> Youve probably heard of Adobe Edge , a timeline-based tool for creating interactive and animated HTML content . Edge enables you to easily create interactive experiences that rely only on HTML , CSS , and JavaScript . If you 've used other Adobe Creative Suite tools , such as Flash Professional , Premiere , or After Effects , then Edge will probably look quite familiar . You have a timeline and controls to edit your content . <p> Currently , the " normal " use case for Edge is creating interactive experiences that are loaded when the page loads . - You can chain animation compositions in sequence , but they have to be in the same wrapper HTML file . - This works great for a number of use cases , but one thing I wanted to do is create an Edge animation and use that as a component that is arbitrarily added to the @ @ @ @ @ @ @ @ @ @ : It can be done , although with a few gotchas . <p> Using Edge animations as components inside of a larger HTML experience is n't the primary use case which Edge was designed for . However this use case is being evaluated and may end up in Edge at a later date . If that happens , this process will become much easier . <p> If you 're wondering " What was I thinking ? " , Ill try to explain while discussing the process of building HTML-based apps , I had the thought : <p> Wouldnt it be cool to have a really elaborate loading animation while loading data from the server ? We could use Edge to build the animation ! <p> As a proof of concept , I created a very basic application that loads two separate Edge animations on demand . Before I go into too much detail on what I built , let 's take a look at the running example . This example has two buttons , one shows a car animation , one shows an airplane animation . Its pretty basic and straightforward : <p> @ @ @ @ @ @ @ @ @ @ Edge animations which you can view here : <p> Once the animations were complete , I started looking at the generated HTML output , and figuring out how I can add it to the HTML DOM of an existing HTML page . I then started putting together the sample application using- Mustache.js as a templating engine to abstract HTML views away from application logic . Note : I also have a simple utility that enables me to include Mustache.js templates in separate HTML files , so that I can keep everything separate . <p> First , I created the basic shell for the application . It is more or less an empty HTML structure , where all content is added at runtime : <p> Inside of the " contentHost " div , all UI is added to the HTML DOM upon request . Basically , when the user clicks a button , the Edge animation is added to the DOM , and then the animation begins . <p> In order to get this working , I had to change a few things in the generated Edge output : <p> in @ @ @ @ @ @ @ @ @ @ handler to use an arbitrary event that I can control . By default , Edge uses the jQuery $ ( window ) . ready() event to start the animation . Since I am adding this to an existing HTML DOM , the $ ( window ) . ready() event is not applicable . - Instead , I changed this to use a custom " animationReady " event : <p> In the *edgePreload.js file , I added a reference to the onDocLoaded function so that I can manually invoke it later , once the Edge animation has been added to the DOM , since again , this wont rely on the " load " event . <p> //added this so it can be invoked later window.onDocLoaded = onDocLoaded ; <p> I also changed the aLoader object to reference the appropriate JavaScript files , since I changed their location in the directory structure : <p> Finally , I created the Mustache.js template , which will be used to generate the HTML DOM elements that will be appended to the existing DOM . - In this there is a wrapper DIV , some @ @ @ @ @ @ @ @ @ @ animation number is dynamic for the templating ) , the styles , a " Stage " div , and Edge preload JavaScript files necessary for the animation . <p> Next , let 's look at how this is actually injected into the DOM . - I created a setupAnimationView() function to inject the animations into the DOM . - This function is used by both animations . The first thing that it does is remove any existing DOM content and dereference the AdobeEdge variables in memory . Since Edge was n't originally designed for- asynchronously- loading animations , I found it to be easiest to just wipe-out Edge and reload it for every animation . - The unfortunate side effect is that you can only have one Edge animation on screen at any given point in time . - Next , the setupAnimationView() function generates the HTML DOM elements and event listeners and adds them to the DOM. - Finally , I created an edgeDetectionFunction , which checks to see if Edge is loaded . - If not , it loads the Edge runtime . The edgeDetectionFunction() then checks if the Edge @ @ @ @ @ @ @ @ @ @ not loaded , it just waits and tries again . - If the animation definition is loaded , it dispatches the " animationReady " event ( discussed in step 1 ) to invoke the actual animation . <p> Since I am using Edge in a manner for which it was not initially designed , there are a few " gotchas " that I ran into : <p> You cant have multiple instances of the same Edge animation in a single HTML DOM at least , not easily . - Each Edge animation is assigned a unique I 'd . - This I 'd is referenced in the HTML structure and the *edge.js , *edgeActions.js , and *edgePreload.js files . - You would need to assign a unique I 'd to each instance , and make sure everything is referenced consistently . <p> It will be very tricky- asynchronously- add more than one Edge animation at the same time . - The shortcut that I used to get these to render was to wipe away the Edge variables in JS and reload them this would cause some issues with more than one animation . @ @ @ @ @ @ @ @ @ @ gets built into Edge ( which I hope it does ! ) , then you will not have to go through all of these steps , and it will be much easier . - I 'll be sure to share more if this feature develops . <p> Ill be speaking at a few conferences in the next few months on PhoneGap and web standards-based development . - Here are just a few , with some more pending . - Be sure to come check one out ( or all of them ) ! <h> RIACON <p> Where architects and developers of all levels come to gather , share and learn about creating the next generation of web based applications. - RIAcons goal is to help you network with fellow industry professionals and expose you to the best content . <p> Ill be speaking on the following topics at RIACON : <p> Introduction to PhoneGap Interested in developing applications for mobile devices , on multiple platforms ? Interested in leveraging your existing web development skills to build natively installed applications ? Just looking to expand your skill set ? Come join Adobe @ @ @ @ @ @ @ @ @ @ platform mobile development and PhoneGap . In this session , you will get an introduction to PhoneGap ( Apache Cordova ) , be able to see example PhoneGap applications , and walk through the process of building your first PhoneGap application . <p> PhoneGap Native Plugins PhoneGap enables developers to build natively installed applications using traditional web-based development tools ( HTML &amp; JavaScript ) , but what if you want to make your application do more ? In this session , learn how to write native plugins for PhoneGap that enable you to extend the API to tap into native device functionality . <p> Data Visualization with Web Standards Do you have the requirement to create rich visual data dashboard applications , but also have the requirement to use web-standard technologies , and do n't  know what to do next ? Well , you 're in luck ! Come to this session to learn about data visualization strategies and frameworks powered entirely with HTML and JavaScript. <h> 360iDev <p> 360iDev is the first and still the best iPhone developer conference in the world . We 're not a publishing company pushing @ @ @ @ @ @ @ @ @ @ 're a conference company , focused on community . Our goal is to bring the best and brightest in the developer community together for 3 days of incredible sessions , awesome parties , good times , and learning . If you do n't leave Wednesday night , with more ideas than you know what to do with , we 're not doing our jobs ! <p> Ill be speaking on the following topics at 360iDev : <p> Kick A$$ iOS Apps with PhoneGap Apps do n't  have to be written in native Objective-C to be awesome. - - Get ready for a crash course in PhoneGap , a tool that enables you to build natively installed iOS apps using 100% HTML &amp; JavaScript , complete with access to local APIs. - Well cover everything from " what is phonegap " to strategies for building highly performant &amp; interactive applications . <h> Dreamforce <p> Every year Dreamforce features stories and presentations from some of the brightest minds in technology , business and beyond . This years Dreamforce promises to be even more informative and dynamic , with our most exciting keynote @ @ @ @ @ @ @ @ @ @ year is also the Social Enterprise event of the year . This is where you 'll learn everything you need to know " from the industry leaders who are paving the way " about how the Social Enterprise revolution is changing the way we do business . <p> Ill be speaking on the following topics at Dreamforce : <p> Data Visualization with Web Standards Do you have the requirement to create rich visual data-centric applications , but also have the requirement to use web-standard technologies , and do n't  know what to do next ? Well , you 're in luck ! Come to this session to learn about data visualization strategies and frameworks powered entirely with HTML , CSS , and JavaScript . <p> Native-like Apps with PhoneGap Native applications built using web technologies can suffer from the " uncanny valley " effect where they do n't  feel quite right as a native application . In this session well focus on strategies to make your apps feel like native apps , including considerations for a native-feeling UI , platform consistency , and user experience . 
@@106848990 @2248990/ <p> Recently , I 've been asked more than once which is better : AMF or JSON for AIR mobile applications . This post is to highlight some performance comparisons , and a sample testing application that I put together . First , it is important to know what both AMF and JSON are . <h> AMF <p> Action Message Format ( AMF ) is a compact binary format that is used to serialize ActionScript object graphs . Once serialized an AMF encoded object graph may be used to persist and retrieve the public state of an application across sessions or allow two endpoints to communicate through the exchange of strongly typed data . <h> JSON <p> JSON ( JavaScript Object Notation ) is a lightweight data-interchange format . It is easy for humans to read and write . It is easy for machines to parse and generate . It is based on a subset of the JavaScript Programming Language , Standard ECMA-262 3rd Edition December 1999 . JSON is a text format that is completely language independent but uses conventions that are familiar to programmers of the C-family of @ @ @ @ @ @ @ @ @ @ , JavaScript , Perl , Python , and many others . <p> I put together a very basic test case where a mobile application makes requests of simple data objects from a ColdFusion CFC. - In each test iteration , a request is made for 1 , 10 , 100 , 1000 , and 10000 value objects , in both AMF and JSON formats . The total round trip time from request to deserialization is measured and compared for each case , for a total of 5 iterations through each cycle . - My findings are that AMF and JSON have- comparable- performance in smaller record sets . - However , AMF seems to have better performance as data sets grow . - In my test cases , the 1000+ record results were consistently faster using AMF. - However , in smaller data sets , JSON was often faster ( however not consistently , or by much of a margin ) . I tested these times on both an iPhone 4 and Motorolla Atrix , both running on the carrier networks ( not over wifi ) . <p> Below is @ @ @ @ @ @ @ @ @ @ <p> Here are a few screenshots of the application . <h> The Tests <p> For these tests I created two basic CFCs ( ColdFusion Components ) . One is a simple data value object . The other CFC is a gateway to expose a remote service that returns the value objects to the client . I chose a ColdFusion CFC for this case b/c it can easily be serialized as AMF or JSON just by changing the endpoint used to consume the service . <p> Obviously , this is a fictional data object with randomly generated values . However , it still represents a reasonable service payload for data serialization . By accessing the data via the ColdFusion Flex/Remoting gateway , you access the remote services via AMF3 . <p> In the mobile client application , I have a **27;9157;TOOLONG class that handles all of the test logic and communications back and forth with the server . The time for each test is measured from immediately before the the request is made to the server , until after the data has been deserialized to an ArrayCollection . You can view @ @ @ @ @ @ @ @ @ @ question of " should I use AMF or JSON " is subjective What kind of data are you returning , and how much data is it ? Do you already have AMF services built ? - Do you already have JSON services built ? - Are the services consumed by multiple endpoints , with multiple technologies ? - Do you rely upon strongly typed objects in you development and maintenance processes ? - Both AMF and JSON are viable solutions for mobile applications . <p> Have you noticed when using twitter , google plus , or certain areas of facebook that when you scroll the page , it automatically loads more data ? - You do n't  have to continually hit " next " to go through page after page of data . Instead , the content just " appears " as you need it . In this post we will explore a technique for making Flex list components behave in this exact way . As you scroll through the list , it continually requests more data from the server . Take a look at the video preview below , @ @ @ @ @ @ @ @ @ @ workflow is that you need to detect when you 've scrolled to the bottom of the list , then load additional data to be displayed further in that list . Since you know how many records are currently in the list , you always know which " page " you are viewing . When you scroll down again , just request the next set of results that are subsequent to the last results that you requested . Each time you request data , append the list items to the data provider of the list . <p> First things first , you need to detect when you 've scrolled to the bottom of the list . here 's a great example showing how to detect when you have scrolled to the bottom of the list . You can just add an event listener to the lists scroller viewport . Once you have a vertical scroll event where the new value is equal to the viewport max height minus the item renderer height , then you have scrolled to the end . At this point , request more data from the server . <p> One @ @ @ @ @ @ @ @ @ @ am using conditional item renderers based upon the type of object being displayed . I have a dummy " LoadingVO " value object that is appended to the end of the list data provider . The item renderer function for the list will return a LoadingItemRenderer instance if the data passed to it is a LoadingVO . <p> You may have noticed in the fetchNextPage() function that the dataProvider is referenced as an InfiniteListModel class let 's examine this class next . The InfiniteListModel class is simply an ArrayCollection which gets populated by the getNextPage() function . Inside of the getNextPage() function , it calls a remote service which returns data to the client , based on the current " page " . In the result handler , you can see that I disable binding events using disableAutoUpdate() , remove the dummy LoadingVO , append the service results to the collection , add a new LoadingVO , and then re-enable binding events using enableAutoUpdate() . Also , notice that I have a boolean loading value that is true while requesting data from the server . This boolean flag is used to @ @ @ @ @ @ @ @ @ @ Now , let 's take a look at the root view that puts everything together . There is an InfiniteScrollList whose dataProvider is an InfiniteListModel instance . The InfiniteListModel also references a RemoteObject instance , which loads data from a remote server . <p> I was recently asked by a friend and former colleague about the best way to get text within a s:Label to behave and scroll properly , especially in the Flex mobile SDK . In particular , having a large block of text wrap correctly and scroll only in the vertical direction . By default if you do n't  set a size on the label , the behavior of the Flex framework is that the views containing the label will resize , and the text will be displayed as entered ( without word wrap or truncation ) . This may cause some layout issues and confusion as to " what the heck is going on with my text " . <p> I 've found that the best way to achieve the desired behavior is to set a maxWidth on the label to force proper word wrapping , and then @ @ @ @ @ @ @ @ @ @ properly . I chose to set a maxWidth to allow the label to determine its own size , and only to wrap if it needs to . An easy shortcut for proper wrapping is to bind the maxWidth of the label to the width of the scroller component . Also , DO NOT set a static height or a max height . This will cause the text within the label to be truncated , and it will not scroll at all if the static height is less than the height of the scroller . I 've also noticed that setting cacheAsBitmap=true on the label also helps scroll performance in some circumstances , but this is not required . <p> Check out a video showing the scroll behavior of a large text block using this approach : <p> Below is the code that makes it work , which follows the method described above : <p> The recordings of my presentations do n't  seem to be available yet on Adobe TV , but here is the content , as promised . I spoke at MAX this year on " Multi Device Best Practices @ @ @ @ @ @ @ @ @ @ Create beautiful , immersive content and applications with HTML5 and CSS3 ? , and the content from these presentations is below . <h> Multi Device Best Practices using Flex &amp; AIR for Mobile <p> In the multi-device best practices session I covered the basics for building a **30;9215;TOOLONG application that conforms to device constraints ( phone and tablet ) , using a single codebase that is able to detect device dimensions and orientation . - This was followed by online/offline detection for occasionally-connected applications , and then followed by device-specific layout using CSS media queries and MultiDPIBitmapSource images . - The presentation slides are below . <h> Create beautiful , immersive content and applications with HTML5 and CSS3 <p> In this session , I gave a " crash course " in developing rich content experiences with HTML5 and CSS3. - I started with a general overview presentation , followed by diving directly into code . - I covered &lt;video&gt; , &lt;audio&gt; , dynamic graphics with &lt;canvas&gt; , &lt;svg&gt; , HTML5 Form elements , CSS3 Web Fonts , Visual Styles ( shadows , corners ) , CSS3 Color spaces ( RGBA @ @ @ @ @ @ @ @ @ @ and media queries . - In the presentation , I also discussed the necessity of client-side solution- accelerator- frameowrks ( jQuery or other JS framework ) , in addition to graceful degradation and HTML5 feature detection using Modernizr. - The presentation slides are below : 
@@106848991 @2248991/ <h> Tag Archives : time-lapse <p> In addition to my addiction to aerial photography , I 'm also fascinated by time-lapse photography . With time lapse photography , you set up your camera to take pictures on an interval . This could be every few seconds , every few minutes , every few hours , or heck , once a day . Its really up to you how you want to set up your shots and what you want to shoot . In any case , you can end up with a lot images each by itself could be great , but it only tells a limited story . - However , you can put all those images together in a sequence to create some truly amazing visuals . Subtle motion becomes pronounced , and you can clearly view the passage of time . Often , this ends up with an amazing visual story that would be hard to otherwise capture . <p> All that you need start diving into time-lapse photography is a camera that is capable of capturing images on an interval normally there is some kind of @ @ @ @ @ @ @ @ @ @ frequency and duration . Then , once you 've got your images , you can process them with Creative Cloud tools to bring out their full potential . <p> Here are two time-lapse sequences I created this weekone a snow storm , one a sunset . <p> Neither sequence required a lot of specialized or expensive equipment . I used a GoPro Hero 3 Black camera , set it on my window sill , and let it do its thing . ( I do want to upgrade to better gear , but this still works fantastically , and I love the GoPro . ) <p> GoPro Hero 3 Black Edition <p> The sunset was a ten second interval captured over about 2 hours and played back in 30 seconds . The snow storm was a 60 second interval captured over roughly 14 hours , played back in 40 seconds . <p> So , you 've captured the images , what next ? - <p> You can check out the video below , or read on for further explanation how I processed and assembled the images into a video sequence , complete with @ @ @ @ @ @ @ @ @ @ everything together as a sequence , I wanted to enhance the photos to bring out as much detail as possible . here 's where Adobe Lightroom comes into the picture . I used Lightroom to import all of my photos , add them to a collection , and then perform bulk/batch processing to enhance all of the images . <p> Editing Photos with Lightroom <p> First , select an image to use as your baseline for adjustments . I would n't start with your darkest image , and I would n't start with your lightest either . I normally start somewhere in the middle . Select the image , and then switch over to the " Develop " module . I use the basic panel to make adjustments to this image . For the GoPro , I like to bring up the shadows and bring down the highlights to pull out details out . If I 'm shooting a landscape , I also like to bring up the clarity and maybe even the vibrance and saturation just do n't  over do it . You could also use one of Lightrooms presets if you want @ @ @ @ @ @ @ @ @ @ careful that it is not too dark or too light b/c were going to apply these settings to all images in the sequence . <p> Lightroom Basic Panel <p> If you want to adjust hue , saturation or luminance of specific colors , you can do that within the HSL/Color/B&amp;W panel . Using this you can make specific colors more or less intense . - I normally try to tone down the yellows in my GoPro images after I 've increased overall saturation . <p> Since I used the GoPro , there is a lot of fisheye distortion from the lens the GoPro has a 2.77mm lens whichgives an ultra-wide 170 degree field of view . This makes for some awesome wide angle shots , but sometimes you do n't  want that extreme distortion . This is where lens correction gets really handy . Next , I opened up the Lens Correction panel . As soon as you check the " Enable Profile Corrections " checkbox , Lightroom should automatically select the GoPro Hero 3 Black Edition lens profile based upon metadata within the image . I did n't  want to @ @ @ @ @ @ @ @ @ @ , so I turned down the distortion correction using the " Distortion " slider . <p> Lightroom Lens Correction <p> Once you have your baseline image the way you want it , you need to apply these settings to all of your images in the sequence . Just select them all , and then either click on the " Sync " button in the bottom right of the Develop module , or use the Settings -&gt; Synch Settings menu . This will apply you changes on this image to all of the images that were selected . This will happen automatically if you are using auto-sync. - You can learn more about synchronizing metadata between photos in the Lightroom documentation . <p> Next , be sure to view several images in your collection , the lightest to the darkest , and make sure they all look decent . If you need to make any changes because they are too light , or too dark , or do n't  have the right contrast , then now is your time to fix it . Once you 're happy with the images in your @ @ @ @ @ @ @ @ @ @ as JPG with 100% quality at full resolution with sequential names . <p> Now we 've got a lot of processed images . What 's next ? We need to make a video ! <p> If you 're wondering how I got the motion in the time lapse sequence , no I did n't  have the camera moving . There are devices which make this possible , but I just used a video editing trick . The images are 12 MP , or 4000 by 3000 pixels . A " standard " HD video sequence is 1920 by 1080 pixels . The image below reflects this scale the red area represents the 4000 by 3000 still image , and the yellow represents the 1920 by 1080 video . <p> Video &amp; Image Size Comparison <p> You 'll notice that leaves us with a lot of room to zoom and pan around the image . I zoom into the image so that it fills the entire horizontal space within the video sequence you can zoom in more if you want . This leaves a fair amount of vertical content outside the clipping rectangle of the @ @ @ @ @ @ @ @ @ @ panning vertically within this area . - I just made the pan very slow and deliberate so it appears that there is constant motion of the camera throughout the entire video . <p> The final result is that the content in the video ( yellow area ) appears to move because the actual image sequence is moving relative to the video viewport. 
@@106848992 @2248992/ <h> Mobile The Next Big Thing <p> On the official Adobe Flex Team blog , there is a great post by Andrew Shorten discussing the future direction of Flex . I highly recommend taking a moment to read it . In that post Andrew points out where Flex is , and where Flex is heading . One thing I want to re-emphasize is that mobile is the next big thing . <p> It has been proclaimed many times , in many publications that mobile devices ( tablets and smartphones ) are the future of computing . This is both in enterprise and consumer products &amp; applications . One of the catches with this growth is that each platform has its own development tooling and language . Wouldnt it be nice if you could just use one programming model &amp; technology stack ? Even better , would n't it be nice if you could use that same programming model &amp; technology stack to also develop applications for the web and the desktop ? Wow , what if you could even share code libraries across mobile , web , and desktop applications @ @ @ @ @ @ @ @ @ @ already do that ! <p> Adobe Flex is the best tool for creating cross-platform , rich experiences in mobile , desktop , and web applications . <p> That is awesome . <p> One of the biggest enhancements introduced with Flash Builder 4.5.1 was the inclusion of mobile tooling . These tools allow you to easily create rich experiences targeting a variety of mobile devices iOS , Android , BlackBerry Playbook . All of which are natively installed applications that can be shared by the standard distribution models : App Store , Enterprise distribution , etc <p> The best part is that you do n't  need to learn any new programming skills to develop and deploy for these platforms . You will need to learn about the app ecosystems , platform signing and deployment procedures , and device specifics ( soft keyboards , hardware buttons , etc .. ) . However , you can still develop these applications quickly and easily using Flex , ActionScript , and AIR APIs . One code base ; multiple platforms ; lots of devices . Did you know that you could even take that same @ @ @ @ @ @ @ @ @ @ , there are great new advancements in the Flex/AIR mobile tooling waiting just over the horizon . <p> We 're continuing to focus on runtime performance , native extensions , new components , declarative skinning , adding more platforms and improving tooling workflows , such that in our next major release timeframe we expect that the need to build a fully-native application will be reserved for a small number of use cases . <p> The growth of the mobile market and the challenge of building out applications that work on a range of different form-factors and platforms present us with a huge opportunity to expose Flex to an entirely new audience of developers , while continuing to be relevant for existing Flex developers who are extending their applications to mobile . <p> Flex &amp; AIR for mobile allow you to use the same enterprise class tooling to build cross platform mobile applications . You can still use existing framework components , existing open source libraries , the strongly typed programming language , automated ASUnit testing , build scripts , and many other features that Flex offers , and you can now target mobile devices . 
@@106848993 @2248993/ <h> Tag Archives : video <p> The recording for my session " PhoneGap and Hardware " from PhoneGap Day back in July is now available ! Be sure to check it out . There were apparently some issues with the audio , but you can still hear everything . <p> I 'd like to express a huge Thank You- to everyone who attended , and to everyone who watches this video ! <p> Below are the sample projects I showed in the presentation , including source code . However , keep in mind that all of these examples were written before PhoneGap 3.0 . The native plugin syntax , and inclusion methods have changed . <h> Pressure Sensitive Sketching in PhoneGap <p> In this example , - the pressure-sensitive Pogo Connect Stylus uses a low energy Bluetooth 4 connection to relay touch/pressure information back to the PhoneGap application . This makes for a unique drawing and sketching experience powered with the HTML5 Canvas element . I 've written about this example previously Check out the video below to see it in action , and read the blog post for technical details @ @ @ @ @ @ @ @ @ @ example that I explored is a PhoneGap native plugin that is used to handle input from a Moga game controller inside of a PhoneGap application on Android . <p> This implementation is intended to be a proof of concept demonstrating how you could integrate the gamepad within your application . It currently only supports input from the joysticks ( axisX and axisY ) and the A and B buttons , and it does not handle all possible input from the controller . <p> This implementation is adapted directly from the **38;9247;TOOLONG example from the Moga developers SDK samples available for download at : - http : **34;9287;TOOLONG 123434 @qwx983434 <p> The game is based on the Universe- prototype that was used as a sub-game inside of the MaxMe app for the recent Adobe MAX conference . I make no guarantees about the code for this game , it was in a huge rush ! <p> I 'm testing out something new Im calling it " Photoshop Friday " , where every Friday , optimistically assuming that I do n't  have deadlines looming , I 'm going to try and do something fun @ @ @ @ @ @ @ @ @ @ ago I did some " crystal ball " compositions , and today Ive put together what I 'm calling Sky Aquariums. - Check out the timelapse video below <p> If you 're wondering how I created the timelapse , this technique for interval-based screen grabs works beautifully . - Just run this as a shell script from the directory where you want the screen capture images to be stored ( OSX only ) : <p> Just because these tools are often used in making amazing productions on a massive scale , does not mean that they are hard to use , expensive , or are only suitable for these " massive " projects . In fact , it is quite the opposite . If you are a Creative Cloud member , you already have the tools you need to create professional quality videos , right at your fingertips . <p> I put together this simple " Crash Course " video showing the basics of compositing/editing a video in Adobe Premiere Pro CC . Its really easy to get started , and you can have great looking video content in just minutes . @ @ @ @ @ @ @ @ @ @ process is this : <p> Create a new project in Premiere <p> Import your source clips <p> Create one or more sequences <p> Edit you clips or sequences together <p> Add effects , if desired <p> Export to desired format <p> In case you 'd like to see the final video and are too impatient to watch to the end of the tutorial , here is the final video that was created during this tutorial . Many thanks to Greg for grabbing the copter out of the air as my battery levels dropped ( notice the very quick descent at the end ) and yes , the pun was intended for " Crash Course " . <p> Wondering how to get started ? Just become a member of Creative Cloud , download the tools , and start producing your own content today . <p> I consider this to be a fairly minimalist rig , and everything is very portable , which is great for travel . <h> Video Capture <p> For all of my front-facing videos I am using a Panasonic Lumix LX7 , and for some of my on-device and @ @ @ @ @ @ @ @ @ @ Black Edition . The LX7 is my go-to camera for both photography and video work . It captures great images , and allows for fully automatic or manual control of the image capture settings . <p> Panasonic Lumix LX7 and GoPro Hero 3 Black <p> When recording videos , I set the LX7 to manual mode so it wont auto-focus or automatically adjust light balance , and I record everything in full HD ( 1080p , 30 FPS ) . On the GoPro , I normally select 1080p at either 30 or 60 FPS ( depending on the situation and lighting ) . If I am outdoors , I 'll have Protune on , if I 'm indoors I usually have Protune off . <p> Camera settings are only part of the whole process . To have a decent output , you really need proper lighting , and a backdrop that is n't distracting . I try to keep this setup very simple : I 'll place the camera on a tripod on the opposite side of my desk so that it faces me directly . Behind me , I 'll have a black muslin @ @ @ @ @ @ @ @ @ @ , and it does n't  reflect any light . Simple tip : Use a clothes steamer on the muslin backdrop , and the wrinkles will fall out pretty quickly . <p> Office/Studio Lighting and Backdrop <p> For lighting , I have darkening blinds that block out nearly all outside light this way you can control the lighting for your video shoots . Often shooting a video may be done over several days , and I cant rely on the weather and natural light to be consistent . With the room darkened , I normally use a single light source above and slightly off to the side of the camera . I try to find an angle that lights me up from the front , but does not reflect in my glasses . I also dim the display on my computer , so its not reflecting in my glasses , or dramatically altering the the lighting . <p> Once the lighting has been set , I set up the camera and adjust zoom , focus , and aperture ( exposure ) where I want it for the current video . <p> @ @ @ @ @ @ @ @ @ @ only half of the equation Without clear audio , the videos are n't  nearly as good , and nobody wants to listen to bad audio . I started off using the built-in mics on my cameras , but quickly learned that the internal microphones werent going to cut it . For all of my recent videos Iver started using a Zoom H4N digital audio recorder . The Zoom enables high quality stereo recording . Its very easy to use , and the recording quality is fantastic now if only I could get those birds to stop chirping outside of my window . <p> Zoom H4N Digital Audio Recorder <h> Editing &amp; Post-Production <p> Capturing content is the first part of the process . The second part is editing everything together . In the editing process , I take advantage of all of the creative tools Adobe has to offer . Most of my video editing is done with- Premiere Pro . This includes clipping &amp; sequencing , color correction , effects , etc All of my audio production is done with Adobe Audition- this includes sound cleanup , and mastering @ @ @ @ @ @ @ @ @ @ depending on the format and content . If you want to insert animations , you can even use Flash Pro as an animation platform , export as video , and pull it into your Premiere project . <p> Video Editing in Adobe Premiere Pro CC <p> Once everything is how I want it , I 'll export to H.264 format ( for the web ) , upload to YouTube , and then start syndicating it however possible/necessary . Normally , its just pulling in a YoutTube- video into a blog post . <p> Some days you are just meant to be creative I think yesterday was that day for me . I saw that Erik Johansson released a new composition , and it sparked a wave of creativity within me . I discovered Eriks work via Adobe Max this year his presentation was awesome and very inspiring . Seriously , do yourself a favor and go watch it . <p> I digress I 've been doing a lot of aerialphotography lately , and after seeing Eriks latest composition I thought to myself : why not take some of my aerial photos and @ @ @ @ @ @ @ @ @ @ a surrealistic composition where you are looking down through a crystal ball . Next thing you know , this happened <p> Here are a few iterations from the creative spark . I think v.2 ( without the flames is my favorite ) : <p> Crystal Ball v.1Crystal Ball v.2Crystal Ball v.2 with Flames <p> Attribution : <p> The aerial image was mine , captured with a DJI Phantom and GoPro- camera . Check out the original image on Flickr. 
@@106848994 @2248994/ <p> Yes , there are a few minor issues related to API changes from Apple . - We cant go into detail about them yet due to Apples non-disclosure agreement by which all Apple developers are bound . However , you will want to pay attention to PhoneGap/Cordova 2.1 release , and upgrade accordingly if your application is impacted. - Most existing apps wont be affected , but a small number may encounter an- issue- ortwo. - Be sure to test your apps on the iOS 6 beta to make sure they are ready . - There are workarounds to these issues without having to upgrade to PhoneGap 2.1 , but all new apps targeting iOS 6 will definitely want to use PhoneGap 2.1 or newer . <p> Many thanks to everyone who attended todays open session on PhoneGap hosted by myself , Raymond Camden , and Piotr Walczyszyn . Thank you for the great questions and feedback ! You can access a full transcript from the open session below . - Please ignore typos , it was a live Q&amp;A session . <p> Answered Questions ( 78 ) @ @ @ @ @ @ @ @ @ @ splash screen , I am using PG 2.0.0 for Android. ? * Piotr Walczyszyn : You do it in Java code , here is snippet of my code : public class MyGapActivity extends DroidGap <p> * Piotr Walczyszyn(privately) : This line does the trick : **25;9323;TOOLONG " splashscreen " , R.drawable.splash ) ; <p> and you put your splash in drawable/RES/splash.png <p> 2 . Steven Benjamin : Is there a suggested proguard configuation for Phonegap 2.0.0 and Android ? * Piotr Walczyszyn(privately) : What do you mean by proguard ? <p> 3 . Steven Benjamin : Can you sugguest some tools / sites where I can learn about debugging my phonegap 2.0.0 app ? * Andrew Trice : emulate.phonegap.com is an in-browser phonegap emulator ( in chrome ) this gives you full access chrome developer tools and mimics the phonegap api. debug.phonegap.com gives you on-device debugging ( although a bit more limited ) , http : //www.iwebinspector.com/ gives you the ability to debug inside of the iOS simulator * Andrew Trice : those links : http : //emulate.phonegap.com http : //debug.phonegap.com <p> 4 . Steven Benjamin : Is there way @ @ @ @ @ @ @ @ @ @ has installed it ? * Ray Camden : If you deploy an update as normal ( ie to Android Play Store or iOS store ) , they will see the update as normal . If you want it more in your face , your app could ping a network service and respond with an alert of some sort to let them know . * Andrew Trice : You can not push updates " inside of the app " on iOS , or else apple will reject it . Any app that downloads code will be rejected . However , you can always use the app store for updates . <p> 5. boobyWomack : on iOs you can do a one-liner for debugging too to invoke a remote Web Inspector , and do not forget Weinre. * Piotr Walczyszyn(privately) : The one-line is to add NSClassFromString ( @ " WebView " ) enableRemoteInspector ; line in **29;9350;TOOLONG method in AppDelegate.m <p> And you can navigate from Safari to http : //localhost:9999 port and it would lauch web inspector * Piotr Walczyszyn : Reposting it to all : The one-line is to @ @ @ @ @ @ @ @ @ @ line in **29;9381;TOOLONG method in AppDelegate.m <p> And you can navigate from Safari to http : //localhost:9999 port and it would lauch web inspector <p> 6. boobyWomack : will PhoneGap be developing a better javascript debugging system ? It seems to me it should ship with this from the off if possible ! since javascript is the big draw of Phone Gap I think . * Andrew Trice : Yes and no PhoneGap is built upon system web views on all OSes . It will be limited to what the system web views expose . However , http : //eumlate.phonegap.com gives you the ability to mimic all PhoneGap APIs in the desktop chrome browser , which has very good developer tools . You can also use remote debugging , but we wont be creating our own JS engine . <p> 7 . Amanda : i 'm using phonegap build inside dreamweaver 6. do you know of a way to disable the building of some of the platforms ? Right now I 'm only building for Android and it takes a long time for all of them to load . * Ray Camden @ @ @ @ @ @ @ @ @ @ that allows for it but you cant do that via DW . <p> 8 . Robert Winterbottom : Hi everyone , I have a question about how Phonegaps new Hydration feature , how does it get updated builds to my testers and do I have to do anything special to enable it ? * Ray Camden : Robert , when an end user starts the app , it checks for a new version and prompts them to install it . If they say yes , it happens for them . <p> 9 . Nathan Buth : I am having issues where I keep recieving encoding errors in my app . The Android version works perfectly but on iOS it gives me the errors . I will post small segments of the cde below to help . * Andrew Trice : Do you have specific error messages ? I 'm not sure what you mean by " encoding errors " ? What kind of app are you building ? Is this audio/video related , or something else ? <p> 10 . Tac : Is there a way to create a build and @ @ @ @ @ @ @ @ @ @ then deploy it on PhoneGap ? In other words , I 'd like a cordova2.0-apache.js library that exposes the same API * Ray Camden : Tac , I will often run my PG apps first on a web server just to enable quicker tester . So you can definitely *start* there . You can then package it up for PG just fine . Just note that if you make use of any server side lang , like PHP or ColdFusion , that you cant deploy those files into the PG app . ( Although you can surely hit them via Ajax . ) <p> 11 . Steven Benjamin : Q : I own the domain www.phonegap.co and I would like to to use it for a not-for-profit and no -charge site for posting phone gap video tutorials . Would this be permisable under the licensing guidelines ? * Andrew Trice : Steven , please send me an email about this to atrice@adobe.com <p> 12 . Mark Murphy : Do you know of any projects using CordovaWebView in a fragment ? * Andrew Trice : Off the top of my head , @ @ @ @ @ @ @ @ @ @ of a way to tell who uses which features . <p> 13 . Ryan Hanna / Sworkit : Has there been any testing with Phonegap builds and iOS 6 ? If so , anything we should be cautious of ? * Andrew Trice : Yes , but unfortunately we are restricted by the Apple developer NDA , so we cant talk about it that much yet . PhoneGap 2.1 ( which is coming very soon ) will address a few of the issues . I will be posting more content about this on the PhoneGap blog soon . * Andrew Trice : Prior to PhoneGap 2.1 there are a few quirks , but most apps will not see them . With PhoneGap 2.1 and later , those issues will be fixed . here 's one of the bigger ones : https : **39;9412;TOOLONG <p> 14 . David R : Does the PhoneGap cloud supports debug build ? * Ray Camden : Yes . You can see it in the settings of the app on the site . <p> 15 . Steven Benjamin : Proguard is the built in obsfucation program that @ @ @ @ @ @ @ @ @ @ Piotr Walczyszyn : I do n't  know of any configuration that is official , found a config posted by someone from a PhoneGap community : LONG ... <p> 16 . Andrew : I 'm editing my code directly with DW CS5 that does n't  have any phonegap support , when the cordova docs talk about " permissions " for android , ios , etc is there a place that i should manually assign those or does build.phonegap.com take care of all that automatically ? * Andrew Trice : They are managed inside of your config.xml file https : **36;9453;TOOLONG look under " PhoneGap API Features " * Andrew Trice : ( config.xml is required for PhoneGap build projects ) <p> 17 . AurelioDeRosa : Is it possible to detect if GPS is active or not ? If this is not possible ( as I guess ) , will the developers add the possibility to the compatible OS as Android ? * Ray Camden : If you are asking can I use geolocation but only if GPS versus other means I believe the answer is no . You can *request* high accuracy @ @ @ @ @ @ @ @ @ @ the *best* possibly , not *just* the best available if that makes sense . In the result object you get a key for accuracy though . You could use code to throw out results that are not accurate enough for you . <p> 18 . Robert Winterbottom : If i need to edit some options in the Android Manifest xml that I do not see options for in the config file for phonegap build , what 's the best way to do this , currently I am decompiling the apk , editing the manifest , recompiling , resigning , and zipaligning which seems like a lot of work to edit the manifest , in particular I am trying to edit supported screen sizes * Ray Camden : Afaik , you are doing the only thing you can do . I think you would want to reach out to us ( Adobe ) about adding support in for what 's missing . Can you email me and I 'll try to forward it up the chain ? <p> 19 . Wyatt : Is there a way to utilize android fragments in cordova other than @ @ @ @ @ @ @ @ @ @ a plugin ? * Ray Camden : I do n't  know what Android fragments are . I do know you can fire intents and listen for intents in Android/PG via a plugin . So if you just want to -start- the intent , you definitely can . I 've got a blog post on that . <p> 20. sheridon : Can you guys give us any heads up on Phonegap Qt or desktop use ? We would absolutey love that for some internal depts ( non-profit ) * Andrew Trice : There are phonegap ports for Windows 7 ( see LONG ... ) , and there will be one for Windows 8 as well . However , I am not familiar with QT <p> 21 . AurelioDeRosa : Yes , I asked for GPS only . I would like to user a method that allow me to detect if GPS is active , for example . So , will you add this feature in the next releases ? * Ray Camden : I do not know if this is something being added in the future . You may be able to @ @ @ @ @ @ @ @ @ @ . Ill check into this though . <p> 23 . Amanda : I want to save data locally to be uploaded to a server when network becomes available . i.e. when the phone connects to any kind of network , have the data uploaded . Is this possible with phonegap ? * Piotr Walczyszyn : yes it is possible , you can detect network status with PhoneGap : LONG ... <p> 24 . Andrew : is config.xml where i would put the permissions to allow geolocation for android devices and phonegap build ? IEX : * Piotr Walczyszyn : you do it in config.xml with following entry : * Piotr Walczyszyn : with you only configure a plugin but this is not enough to turn on the permission * Piotr Walczyszyn : you can learn more about config.xml here : https : **36;9491;TOOLONG <p> 25 . StephenR : As an intermediate ColdFusion user , I was wondering what is the BEST way to send and retrieve database calls in a PG app ? Most of the apps I have in mind are media based , bringing in , for example @ @ @ @ @ @ @ @ @ @ have detailed in the database . Thanks for answering all these questions guys . * Ray Camden : I hate to say that " X " is the best . Ill just talk about what has worked well for me . First off I 'd make use of CFs built in ability to serve up JSON from CFCs . That makes your code somewhat simpler and allows for the best reuse imo . IN your specific example , I could imagine a case where the CFC returned an array of URLs for the media that you could then pass to PGs " Media " API . <p> 26. sheridon : Is there a " way " /method or manal we can use to port or help port PhoneGap to Mac OS X or Linux ? Two depts use Ubuntu and Mac . * Piotr Walczyszyn : Well PhoneGap is a distribution of open source Cordova project so you can do it on your own , there is was some intial work done on it for Mac but I do n't  it is very active at this point : https : **33;9529;TOOLONG @ @ @ @ @ @ @ @ @ @ the answer . How can I know if it will be added ? Have I to wait ? * Ray Camden : You would have to watch the site . They always announce new updates/etc . <p> 28 . Wyatt : To follow up on my question , fragments are androids concept of segmenting code to change how things are displayed in particular of if it is a phone or tablet . The issue with firing an intent to do this is that an intent gains the full screen as if it is an activity so something like a dialogFragment becoms useless . Android is moving away from dialogs and other of their design concepts in favor of fragments so support for it would be nice . * Ray Camden : Wyatt thank you for that explanation . That is pretty darn cool . Afaik we have no support for that at all . You could use a plugint to try to interact with it . Sorry I do n't  have more for you there . <p> 29 . Nathan Buth : In app I am working on I have @ @ @ @ @ @ @ @ @ @ Both of thse errors come after a **32;9564;TOOLONG and access the file . I am curious on if there ae bugs with this in iOS because i works fine on Android . * Andrew Trice : What kind of file is this happening with ? An image , text , or something binary ? <p> 30 . StephenR : Is pulling in CF rendered HTML via ajax a bad idea or just let 's flexible ? * Ray Camden : In general , your services should return data only . However , there are always exceptions . If the conversion from data to HTML is super complex in JS and easier in CF , then I say make the *practical* decision and do it . ( Just separate the code into two methods to keep em cleaner . ) Also note that templating engines , like Handlebars , make it easier in client apps to do HTML from data . <p> 31 . Steven Benjamin : For Phonegap 2.0.0. on Android does the splash.png reside in the res/drawable-(x)dpi/ folders ? * Piotr Walczyszyn : Yes <p> 32. sheridon : Is there @ @ @ @ @ @ @ @ @ @ tips for PhoneGap in general or the specific builds ? * Ray Camden : In general , these issues come down to the platform for ex Android having weaker support for so and so versus the embedded web view in iOS ( or vice versa ) . So I 'm not aware of a PG doc in that regards , but Ive seen various blog posts . Some tips apply to all like using touch versus click , but that is n't PG specific , its mobile specific . <p> 33 . Amanda : Thank you . I 'm currently using the network feature with **33;9598;TOOLONG to detect if a connection is available when the user submits . Is that no longer used ? * Andrew Trice : **33;9633;TOOLONG is unfortunately sub-optimal , and does n't  always report accurately . I normally attempt to make a request , and do n't  let the user proceeed until it is successful ( or store all data locally and have a manual synchronization routine ) <p> 34 . Ryan Hanna : Which file in the LowLatencyAudio plugin would be a good place to start to allow @ @ @ @ @ @ @ @ @ @ ? * Andrew Trice : LowLatencyAudioAsset.m is the one you want . * Andrew Trice : That is what is used when an asset is loaded via preloadAudio() if you are using preloadFx() , it uses **32;9668;TOOLONG ( system sounds ) <p> 35 . Steven Benjamin : For PG 2.0.0 on Android , I notice that xlargescreens is no longer in the androidManifest , My splash screen on my HTC OneX is not working , and I am thinking that may be the cause , is there a reason that is it no longer there ? * Piotr Walczyszyn : Do n't  think there is a reason for that , currently I have hdpi , ldpi , mdpi and xhdpi resolutions in my project and it works on most of the devices <p> 36 . Tac : You mentioned Handlebars how does that compare to understore ? Were leaning toward backbone as a framework , not sure what templating engine to use , but the more examples out there the better . I 've read through the examples on http : //coenraets.org/ , but what I want is one solution that @ @ @ @ @ @ @ @ @ @ debugging simplicity ) * Ray Camden : Tac , speaking from *limited* experience , Handlebars is just templating , while Underscore is a utility library with lots of other features . So I 'd say it just plain depends . Frankly I 'd say use what is best for your development style . I prefer the template style of Handlebars over every one Ive seen but that 's just me . <p> 37 . Nathan Buth : Well at th moment it is an empty directory I am using it to call a series of functions that will write in an xml file in one case . In the other it is to add an image file taken with the camera . * Andrew Trice : See this message thread from the cordova mailing list : LONG ... it looks like that is invoked when the URI does n't  specify a protocol * Andrew Trice : See this thread too : LONG ... * Andrew Trice : ( add file : // prefix ) <p> 38 . Amanda : Oh so you would recommend attempting to connect to the server and then just @ @ @ @ @ @ @ @ @ @ network connection ? I will do that then . Is a manual sync the only way to sync ? I was hoping to have the app sync with the server when network became available even if the app was n't openIs that possible ? * Andrew Trice : I typically do this : ( capture data , save locally , attempt to push to server ) all in one process . If that is successful , delete the local copy . If not , save it to push later . You can automate it by just using a timer , or using " online " event LONG ... <p> 39 . Shaun : I am about to try phonegap , is there a good tutorial size that you would recommend ? I have experience developing for the android , but thought I would try something different . * Ray Camden : Personally I 'd just do the guide for your platform ( Android ) and get the default ( empty ) app running on your device . Then just starting writing your HTML/JS . THere are a butt load of blog entries @ @ @ @ @ @ @ @ @ @ and so with PG . <p> 40. sheridon : Can you provide us with any information on using PhoneGap for the not-so-famous platforms such as : 1 . Google TV ; 2 . Amazon ; 3 . Barnes &amp; Noble Nook ? * Andrew Trice : Nook and Amazon Kindle fire are no probelm " out of the box " . The developer process for google tv is restricted from what I hear , but I have n't tried it myself . <p> 41 . MarkB : Is it possible to run an entire app that is served from an external source ( i.e. , a web server ) through PhoneGap and keep it in the DroidGap Activity without using plugins like ChildBrowser ( whenever I click a link in PG w/o CB it launches an Intent into the browser ) ? If so , do you know of any good resources or projects that would help ? * Ray Camden : You can do it in Android with this trick : LONG ... Not sure if you can do it with iOS , but if you do it , it @ @ @ @ @ @ @ @ @ @ if you are doing Enterprise deployment . <p> 42. sheridon : Are there plans to support Blackberry OS 10 and any issues with the BB Playbook deployments ? * Andrew Trice : Yes , it is supported . RIM has contributors to the PhoneGap/Cordova project who will continue to evolve the support . See more detail here : LONG ... <p> From that post ( from RIM ) : We are striving to bring BlackBerry WebWorks and Cordova closer together , and are actively part of the Cordova open source project providing contributions . * Andrew Trice : However , I am not aware of exact specifics of what is supported by RIM . <p> 43. berman eyal : what is the best javascript framework for using with PG ? * Andrew Trice : you can use any of them . PhoneGap is not prescriptive of architecture choice . Many people are successful with jQuery , Backbone.js ( . js ) , Sencha , jQuery Mobile , etc If you are familiar and comfortable with one in particular , you can keep using it . <p> 44 . Steven Benjamin @ @ @ @ @ @ @ @ @ @ posted It does work for PG 1.8.0 but not for PG 2.0.0 * Piotr Walczyszyn : do you have in config.xml * Piotr Walczyszyn : ? <p> 45 . Steven Benjamin : Piotr I found the problem The super.loadUrl was missing the delay time . * Piotr Walczyszyn : Happy to hear that <p> 46 . Robert Winterbottom : Are there any advantages to using PhoneGaps storage api and writing to a database compared against HTML5s local Storage ? * Ray Camden : Robert , it is n't better or worse it is different . LocalStorage is good for key/value pairs . Ie , favoritecolor : red . WebSQL is good for content at a larger scale , and for stuff you have to search . Short answer is use the feature that matches the type of data you are storing . <p> 47 . David R : Just curious .. Titanium vs PhoneGap which one is best ? * Piotr Walczyszyn : If you really look into it deeper these are completely two different approaches , so you cant really compare it . With PhoneGap you do HTML5/CSS3/JS development where @ @ @ @ @ @ @ @ @ @ which gets compiled to native code . <p> 48 . Station 8 : I was considering going to the AppMobi development environment using Phonegap . What I am trying to do is get away from having to develop on a Mac for iOS apps . Does anyone have any experience with this ? * Andrew Trice : You can use PhoneGap build ( http : //build.phonegap.com ) for free to develop iOS apps without being on a mac . <p> 49 . Amanda : Ray , what do you mean by " content at a larger scale " ? Sorry for not knowing * Ray Camden : Well , obviously it is n't cut and dry . I use LocalStorage for data that is simple . THings like config settings , or preferences . Like , favcolor : blue or lastSearch:foo . I use WebSQL for ad hoc data that can grow to any size . Like perhaps free form text notes that user enters . <p> 50 . Shaun : This may be a stupid question , but are there any issues with support when using html5 and css3 when @ @ @ @ @ @ @ @ @ @ depends on the target device/OS. - - For example , Android 2. x does n't  support SVG or &lt;Audio&gt; or &lt;Video&gt;. - - It is not a phonegap limitation , rather the capabilities of the OS b/c PhoneGap uses the system web browser. - - For PhoneGap audio , you can use the Media class to reliably play audio across devices. - - For HTML/CSS3 implementations , check out http : //caniuse.com/ to see if the feature is supported on your target OS . <p> 51. berman eyal : i want to learento create plugin , what plugin is a necesary plugin ? * Piotr Walczyszyn : You need plugins when out-of-the-box functionality is missing something or current plugins which you can find here https : **38;9702;TOOLONG do n't  do what you need . Here is docs how to create plugins : LONG ... <p> 52 . Amanda : OK . We are using the local storage api to store form data temporarily and then pushing it to an SQL database with coldfusion . I want the app to be able to grow without getting bogged down . Does this @ @ @ @ @ @ @ @ @ @ We have discussed couchdb and sqlite , etc * Ray Camden : I think using LocaleStorage to store the form makes perfect sense . Its one form probably just 5-6 fields . <p> 53. berman eyal : how to implement push notification for android ? * Piotr Walczyszyn : * Piotr Walczyszyn : Well you can do as you would do with standard Android development , you can also use http : //urbanairship.com/ * Piotr Walczyszyn : there is also a ready plugin for you : LONG ... <p> 54 . David R : How do I call native PG functions through Javascript ? * Andrew Trice : There is " out of the box " functionality exposed through the PhoneGap API . See details here : http : **39;9742;TOOLONG If you want to integrate with custom native code , you can create what 's known as " native plugins " , see detail for creating those here : LONG ... There are also a bunch of open source native plugins that you can use/reference here : https : **38;9783;TOOLONG <p> 55. sheridon : Is there any matrix of what 's supported @ @ @ @ @ @ @ @ @ @ the other " main " platforms ? * Ray Camden : for that sheridon you need to check with the product sites . for ex , nook does n't  support gps. that 's not something PG would normally document . make sense ? <p> 56. sheridon : And also , any emulators you know of for those devices ? * Andrew Trice : For Kindle , its just the Kindle Fire device ( you have to enter a code for it to be recognized ) . For Nook , download the Nook development kit : LONG ... * Andrew Trice : kindle fire emuilator guide : LONG ... <p> 57 . Nathan Buth : Is there a way to make phonegap 2.0 use the front facing camera of a device like the Nexus 7 that has no rear facing camera ? * Ray Camden : Nathan , it should " just work " ie , it will get the only camera it has access to . <p> 58 . Mikel : Any suggestions on the best way to tell what type of device that you 're on ? Such as phone vs @ @ @ @ @ @ @ @ @ @ for use ? * Ray Camden : There is a Device API in the docs . It tells you well the device . You can also use JS to check things like screen size too . * Andrew Trice : you can also use JS to test CSS media queries to make a determination programmatically. see : https : **37;9823;TOOLONG <p> 59 . Preston : Any suggestions on snappiest JS/PG Framework/Tools ? * Andrew Trice : It depends on the developer and their preferences . Some people like jQuery , some people like Sencha , some people like no frameworks at all . One of the challenges with HTML development in general is that there are hundreds of frameworks , and thousands of ways of approaching a problem . We try not to be prescriptive in tech/architure inside of your phonegap experience . All HTML frameworks will work . <p> 60. sheridon : Any apps using PG shipped on iPad that you can share ? * Andrew Trice : Lots see the showcase here : http : //phonegap.com/app <p> 61 . Preston : PG seems dramtically slower in apps then @ @ @ @ @ @ @ @ @ @ of the lack of hardware acceleration in apps ? is this something to just wait for or are there work-arounds ? * Piotr Walczyszyn : You can make really perfomant apps with PhoneGap , one issue is that on some platforms WebView component that is used by PhoneGap is restricted to use older/slower JavaScript viruatl machines . * Piotr Walczyszyn : You can have hardware acceleration in PhoneGap apps also , and you enforce with same techniques as in browser like for example : translateZ(0) etc <p> 62. tony : what are the difference between application done by phonegap vs native applicatinon done by obj c and if there thing canit done by phongap ? * Ray Camden : Tony its a big question . Obviously if you *know* native dev , it may be best for you . If you do n't  and know HTML already , then PG is a great way to get on device . Even better , it allows you to go on *all* the devices at once . As for things that ca n't be done in PG , in general , you can use @ @ @ @ @ @ @ @ @ @ * Andrew Trice : Its also important to understand that PhoneGaps UI rendering is based upon a web view . You can create any html experience for your app , but do n't  expect things like 3D games coming to PhoneGap quite yet . You can read more about " phonegap ui " here : LONG ... <p> 63 . Shaun : Do I have to use Eclipse or can I use dreamweaver ? * Piotr Walczyszyn : You can use any editor you want , notepad , vi , Eclipse anything <p> 64 . MarkB : Android Any recommendations for handling page transitions ( e.g. , a " loading " dialog of sorts ) when moving between two different URLs ? * Piotr Walczyszyn : You can use jQuery Mobile for that , also you can use my framework called BackStack : http : **34;9862;TOOLONG <p> 65 . Spencer : Is there a way to cancel a PhoneGap plugin call that is already in progress ? For instance if the app is waiting for data from a Bluetooth device , can the user cancel this action and kill the @ @ @ @ @ @ @ @ @ @ Out of the box , there is no way to do this . However , you can add functionality to your native plugin that will manage this for you ( queue pending requests , kill pending requests ) . This would have to be managed in the native code layer . <p> 67 . Amanda : Andrew , how do you handle the data on your fresh food app ? is the information bundled with the app or is it pulling from a server ? if a server , what kind of db ? * Andrew Trice : it is bundled . Its just a REALLY big JS array , and I 'm doing client side filtering . I embedded it b/c I wanted it to work even when offline . * Andrew Trice : To create the array , i imported the data from the FDA ( excel export ) into a MySQL database , did some minor data transforms , then used ColdFusion to generate the JS string . Once I exported it , its a " plain-old " JS array . To publish new data , I have @ @ @ @ @ @ @ @ @ @ , the FDA only publishes their data once a year . <p> 68 . Preston : Is jquery a dependacy of BackStack ? * Piotr Walczyszyn : Well BackStack is dependent on Backbone and Backbone is dependent on jQuery or Zepto <p> 69 . MarkB : As a clarification to my earlier question , my content is to be served from an external web server and the resulting HTML will use HREF links to move between pages . Is it possible to have some sort of " loading " dialog that persists between the unloading of the first page and the loading of the second ? ( forgive my ignorance of Backbone/Backstack if it does this , my brief overlook of it seemed to focus on AJAX or dynamically loaded content ) * Ray Camden : There are a couple of ways you could handle this . One possibly simple way would be a jQuery event handler for anchor/click events in general . You could automatically display a loading type div that would go away when the page loads . Not sure if that helps , but its definitely possible @ @ @ @ @ @ @ @ @ @ . <p> 70 . Tac : Andrew : Wouldnt storing the food data in a SQLite database be better ? At least better for your readers , who would use it as an example ! * Andrew Trice : Take a look at this example from Ray : LONG ... I just went for the simple approach since the data is read-only . <p> 71 . Tac : Also , if you were to expand on the Fresh Food app , I 'd vote for integrating an Open Source Places API , so you could find Fresh Food close to where y ou were at the time * Andrew Trice : good idea , thanks ! <p> 72 . Preston : Is there uglify or compile code when using BackBone/BackStack ? I wouldn * Piotr Walczyszyn : there is a BackStack minified version here : LONG ... you can also find one for backbone : http : **32;9898;TOOLONG <p> 73 . Preston : t think closure complier would work but would you know of any that do * Piotr Walczyszyn : * Piotr Walczyszyn : Yeah closure compiler should work <p> @ @ @ @ @ @ @ @ @ @ or compile code when using BackBone/BackStack ? * Andrew Trice : You can minify or uglify JS code , but since it is interpreted , not compiled , so true " compilation " is not an option , just obfuscation . <p> 75. jared : My team has proven they can use phonegap on android and switch over to native code to do things that are not possible in phonegap like PDF annotation , ect . We are very comfortable on android but little experience with iOS . Does anyone know that it is possilbe on iOS to switch back and forth between phonegap and native XCODE ? * Piotr Walczyszyn : On iOS as on Android you can extend your PhoneGap application with native plugins which are written in native code . * Piotr Walczyszyn : So answering your question yest it is possible and it is done similar way . <p> 76. jared : Do you have any recommendations for a good cross platform PDF sdk ? Looking at branchfires * Andrew Trice : What do you mean ? Normally just linking to a pdf works . Or @ @ @ @ @ @ @ @ @ @ jared : Open the PDF manipulate like add comments , draw , highlight ect. * Piotr Walczyszyn : Do n't  have a good one but branchfire looks decent and it should work for both . <p> 78 . Preston : BackStack does n't  include iScroll4 as a dependency . This is included in App-UI , does this mean that mobile lists scrolling is handled differently ? * Andrew Trice : BackStack does n't  have scrolling built in . You can use iOSs default scrolling , or use iScroll . <p> Open Questions ( 7 ) 79. boobyWomack : I can not get the one-liner to work though 80. boobyWomack : thanks Piotr 81. boobyWomack : thanks andrew , I had n't  come across the emulator before but I am very new to this . It sounds great ! 82 . Nathan Buth : I am having issues where I keep recieving encoding errors in my app . The Android version works perfectly but on iOS it gives me the errors . I will post small segments of the code below to help . The two **32;9932;TOOLONG lines seem to not @ @ @ @ @ @ @ @ @ @ not in scope , but it works fine on Android . Any idea on why it is not working ? Here is a link to the code . LONG ... 83 . Shaun : site sorry 84 . Preston : Webapps once in phonegap seem slower , mistyped a little 85 . Preston : is jQuery a dependency of BackStack <p> Here are my presentation slides and extra content from last nights DC Droids meetup . I gave the presentation " Enterprise Android Applications With PhoneGap " . - There were excellent questions and conversations at last nights event Thank you everyone for attending , and making it a great meetup . <p> As promised , here are my presentation slides and extra content from last weeks- RIACon conference . I gave three presentations : " Intro to PhoneGap " , " Data Visualization With Web Standards " , and " PhoneGap Native Plugins " . All of the presentations are freely available at : https : **34;9966;TOOLONG . <p> Data visualization is the art &amp; science of creating a visual representation of data and information . - Really , @ @ @ @ @ @ @ @ @ @ plot , pie chart , complex flow diagram , 3d model , etc - If your visualization conveys information without having to read a table of data , then its doing what it should . Recently , the emergence of HTML5s dynamic graphics and SVG support have made rich , dynamic , and interactive graphics possible on the web without having to leverage Flash , which was previously the only real option . Be sure to check out this presentation , and read this blog post for more info : Data Visualization With Web Standards <p> PhoneGap provides you with the ability to create natively-installed mobile applications using web technologies . - As a part of this , PhoneGap provides an API to access native operating system functionality from JavaScript . Luckily for everyone , the JavaScript-to-native bridge- is extensible and you can very easily create and expose your own custom native functionality with a JavaScript API. - Basically , all PhoneGap native plugins are made up of two parts : a native implementation , and a JavaScript interface . - Your PhoneGap application calls the JavaScript interface , which @ @ @ @ @ @ @ @ @ @ The native layer then performs a native operation and communicates back to the JS layer . <p> Open source PhoneGap plugins- There are lots of open source native plugins for everything from push notifications , screen captures , barcode scanners , to MapKit or even iCould ( among many others ) . <p> and of course , the sample apps : <h> iOS Multi-Screen <p> This sample app demonstrates how to create multi-screen experiences using the UIScreen API on iOS. - You can use AirPlay mirroring on an AppleTV as a second screen , the content of which is entirely controlled by the JavaScript in the " main " PhoneGap experience on the device . Check out the links and video below to learn more . <h> LowLatencyAudio <p> The PhoneGap LowLatencyAudio native plugin for Android and iOS allows you to preload audio , and playback that audio quickly , with a very simple to use API. - It overcomes the current limitations of the HTML5 Audio API on many mobile devices . Check out the links and video below to learn more . <p> Earlier this month the PhoneGap @ @ @ @ @ @ @ @ @ @ in part to celebrate the release of PhoneGap 2.0 , but more importantly to bring together members of the PhoneGap community to share and learn from each other . - There are great recaps of PhoneGap Day from RedMonk , as well as on the PhoneGap blog . One of the new services announced on PhoneGap Day was- emulate.phonegap.com. - Emulate.phonegap.com enables an in-browser simulator for developing and debugging PhoneGap/Cordova applications , complete with Cordova API emulation . - It is built off of the Ripple Emulator , which itself is open source and may even be contributed to the Apache Cordova project . <p> Once launched , the URL that you want to simulate will be displayed within the Ripple operating environment view . <p> Note : This only works with assets that are on a live URL . You can use a local http server with references to localhost , however the emulator will fail if you try to access your application directly from the local file system using a file : // URI . <p> Update : You can enable access to local files by changing a @ @ @ @ @ @ @ @ @ @ first comment on this post for additional detail . <p> ( click for full-size image ) <p> The emulator environment gives you the ability to emulate PhoneGap events and API calls , without having to deploy to a device or run inside of the iOS , Android , Blackberry , or other emulator . - Not only can you simulate the PhoneGap/Cordova API , but you can also use Chromes debugging tools to test &amp; debug your code complete with breakpoints , memory inspection , and resource monitoring . This is a handy development configuration. - It enables app development within a rich debugging environment that is familiar , fast &amp; easy to use . This does not replace on-device debugging however nothing will replace that . - On-device debugging is extremely important ; this helps increase your productivity as a developer . <p> So how do you use this environment ? The environment will handle Cordova API requests , and you can also simulate device events . <p> First , the " Devices " panel In this panel you can select a device configuration Everything from iOS , to @ @ @ @ @ @ @ @ @ @ will not only change the physical dimensions , but will also change Device/OS/user agent settings reported by the application . - Here you can also select the device orientation , which will change the visual area within the simulator . <p> Within the " Platforms " panel you can choose the platform you wish to emulate . - With respect to PhoneGap applications , you will want to choose " Apache Cordova " , and then select the API version that you are using . - By default , it uses " PhoneGap 1.0.0 ? , however you can chose " Apache Cordova 2.0.0 ? to get the most recent version . - The Ripple emulator also simulates BlackBerry WebWorks and mobile web configurations as well . <p> The " Accelerometer " panel can be used to simulate device **25;10002;TOOLONG events . - Just click and drag on the device icon ( the gray and black boxes ) , and the icon will rotate in 3D. - As you drag , accelerometer events will be dispatched and handled within your application . - From here , you can even trigger @ @ @ @ @ @ @ @ @ @ " panel enables you to simulate your geographic position within your PhoneGap/Cordova application . - You can specify a latitude , longitude , altitude , speed , etc - You can even drag the map and use it to specify your geographic position . The position that you set within the geolocation panel will be reported when using **42;10029;TOOLONG . <p> The " Config " panel is a graphical representation of your PhoneGap BuildConfig.xml file . - You can use this to easily view/analyze what 's in your application configuration . <p> The " Events " panel can be used to simulate PhoneGap specific events , including " deviceready " , " backbutton " , " menubutton " , " online " , and " offline " ( among others ) . - Just select the event type , and click on the " Fire Event " button . <p> As I mentioned earlier , this wont replace on-device debugging . - It also wont handle execution of native code for PhoneGap native plugins , however you can test/develop against the JavaScript interfaces for those native plugins. - Emulate.phonegap.com- will definitely @ @ @ @ @ @ @ @ @ @ and is a nice complement to the- Chrome Developer Tools . 
@@106848995 @2248995/ <h> Category Archives : PhoneGap <p> A few months back I released App-UI , a UI container toolkit for creating HTML experiences . - It gives you common " view navigator " paradigms for mobile experiences a view navigator stack that can push and pop views , a split view navigator , and a " sliding view " like Facebooks iOS experience . - All of which are created entirely with HTML , CSS , and JavaScript , so they are great for mobile web or PhoneGap applications . <p> Although I have n't committed many changes to App-UI recently , I 've been evaluating different types of visual effects and thinking through options for making App-UI more configurable. - Recently I stumbled across OriDomi , a toolkit that easily provides the ability to fold UI elements as though they were made of paper . - There have been a few interesting UI-folding proof of concepts and demos floating around lately ( such as this , this , and this ) , and I realized : <p> This can be done completely in HTML , CSS , &amp; JS <p> This @ @ @ @ @ @ @ @ @ @ for anyone to use <p> Below you can view the result of a days worth of tinkering : <p> I created this as an offshoot of the " Sliding View " demos It is not integrated into the core functionality since not everyone is going to want it in their apps , but it is pretty easy to add . - You can check out both of my demos/samples below , with full source code . - Just be forewarned : they only work on newer browsers that support CSS3 3D transformations and transitions ( WebKit for best results ) . - I 've tested it in Chrome on OS X and mobile Safari , and it works great . - It does not work on Android . <p> I made a few changes to the open source- OriDomi library , most importantly the ability to toggle between " live DOM " and the copied views that have the visual effects applied . - This allows me to have event listeners on DOM elements that can actually respond to user input . - I also added the ability to support DOM @ @ @ @ @ @ @ @ @ @ and added a " destroy " function to have it clean itself up from memory . <p> The first example is really basic. - It is an instance of the SlidingView that uses a modified version of the OriDomi toolkit to add a " fold " effect to the left sidebar. - I just wanted to show a basic use case , demonstrating the effect on " live " DOM elements . - You can check it out in your browser by clicking on the link/image below : <p> The second example is not quite as pretty , but definitely more complex . - It is an extension of my side-by-side ViewNavigators inside of a SlidingView , with the folding effect applied . - This example demonstrates complex objects with DOM manipulation having the fold effect applied. - You can also check this one out in your browser by clicking on the link/image below : <p> Based on my experimentation so far , I 've got big plans for App-UI. - I fully intend on rewriting how effects are applied to all of the container elements to enable them to be @ @ @ @ @ @ @ @ @ @ multiple visual effects options . <p> Yes , there are a few minor issues related to API changes from Apple . - We cant go into detail about them yet due to Apples non-disclosure agreement by which all Apple developers are bound . However , you will want to pay attention to PhoneGap/Cordova 2.1 release , and upgrade accordingly if your application is impacted. - Most existing apps wont be affected , but a small number may encounter an- issue- ortwo. - Be sure to test your apps on the iOS 6 beta to make sure they are ready . - There are workarounds to these issues without having to upgrade to PhoneGap 2.1 , but all new apps targeting iOS 6 will definitely want to use PhoneGap 2.1 or newer . <p> Many thanks to everyone who attended todays open session on PhoneGap hosted by myself , Raymond Camden , and Piotr Walczyszyn . Thank you for the great questions and feedback ! You can access a full transcript from the open session below . - Please ignore typos , it was a live Q&amp;A session . <p> Answered @ @ @ @ @ @ @ @ @ @ can I configure a splash screen , I am using PG 2.0.0 for Android. ? * Piotr Walczyszyn : You do it in Java code , here is snippet of my code : public class MyGapActivity extends DroidGap <p> * Piotr Walczyszyn(privately) : This line does the trick : **25;10073;TOOLONG " splashscreen " , R.drawable.splash ) ; <p> and you put your splash in drawable/RES/splash.png <p> 2 . Steven Benjamin : Is there a suggested proguard configuation for Phonegap 2.0.0 and Android ? * Piotr Walczyszyn(privately) : What do you mean by proguard ? <p> 3 . Steven Benjamin : Can you sugguest some tools / sites where I can learn about debugging my phonegap 2.0.0 app ? * Andrew Trice : emulate.phonegap.com is an in-browser phonegap emulator ( in chrome ) this gives you full access chrome developer tools and mimics the phonegap api. debug.phonegap.com gives you on-device debugging ( although a bit more limited ) , http : //www.iwebinspector.com/ gives you the ability to debug inside of the iOS simulator * Andrew Trice : those links : http : //emulate.phonegap.com http : //debug.phonegap.com <p> 4 . Steven Benjamin @ @ @ @ @ @ @ @ @ @ app after a user has installed it ? * Ray Camden : If you deploy an update as normal ( ie to Android Play Store or iOS store ) , they will see the update as normal . If you want it more in your face , your app could ping a network service and respond with an alert of some sort to let them know . * Andrew Trice : You can not push updates " inside of the app " on iOS , or else apple will reject it . Any app that downloads code will be rejected . However , you can always use the app store for updates . <p> 5. boobyWomack : on iOs you can do a one-liner for debugging too to invoke a remote Web Inspector , and do not forget Weinre. * Piotr Walczyszyn(privately) : The one-line is to add NSClassFromString ( @ " WebView " ) enableRemoteInspector ; line in **29;10100;TOOLONG method in AppDelegate.m <p> And you can navigate from Safari to http : //localhost:9999 port and it would lauch web inspector * Piotr Walczyszyn : Reposting it to all : @ @ @ @ @ @ @ @ @ @ " ) enableRemoteInspector ; line in **29;10131;TOOLONG method in AppDelegate.m <p> And you can navigate from Safari to http : //localhost:9999 port and it would lauch web inspector <p> 6. boobyWomack : will PhoneGap be developing a better javascript debugging system ? It seems to me it should ship with this from the off if possible ! since javascript is the big draw of Phone Gap I think . * Andrew Trice : Yes and no PhoneGap is built upon system web views on all OSes . It will be limited to what the system web views expose . However , http : //eumlate.phonegap.com gives you the ability to mimic all PhoneGap APIs in the desktop chrome browser , which has very good developer tools . You can also use remote debugging , but we wont be creating our own JS engine . <p> 7 . Amanda : i 'm using phonegap build inside dreamweaver 6. do you know of a way to disable the building of some of the platforms ? Right now I 'm only building for Android and it takes a long time for all of them to load @ @ @ @ @ @ @ @ @ @ They have an API that allows for it but you cant do that via DW . <p> 8 . Robert Winterbottom : Hi everyone , I have a question about how Phonegaps new Hydration feature , how does it get updated builds to my testers and do I have to do anything special to enable it ? * Ray Camden : Robert , when an end user starts the app , it checks for a new version and prompts them to install it . If they say yes , it happens for them . <p> 9 . Nathan Buth : I am having issues where I keep recieving encoding errors in my app . The Android version works perfectly but on iOS it gives me the errors . I will post small segments of the cde below to help . * Andrew Trice : Do you have specific error messages ? I 'm not sure what you mean by " encoding errors " ? What kind of app are you building ? Is this audio/video related , or something else ? <p> 10 . Tac : Is there a way to @ @ @ @ @ @ @ @ @ @ under apache ) and then deploy it on PhoneGap ? In other words , I 'd like a cordova2.0-apache.js library that exposes the same API * Ray Camden : Tac , I will often run my PG apps first on a web server just to enable quicker tester . So you can definitely *start* there . You can then package it up for PG just fine . Just note that if you make use of any server side lang , like PHP or ColdFusion , that you cant deploy those files into the PG app . ( Although you can surely hit them via Ajax . ) <p> 11 . Steven Benjamin : Q : I own the domain www.phonegap.co and I would like to to use it for a not-for-profit and no -charge site for posting phone gap video tutorials . Would this be permisable under the licensing guidelines ? * Andrew Trice : Steven , please send me an email about this to atrice@adobe.com <p> 12 . Mark Murphy : Do you know of any projects using CordovaWebView in a fragment ? * Andrew Trice : Off the top @ @ @ @ @ @ @ @ @ @ . I 'm not aware of a way to tell who uses which features . <p> 13 . Ryan Hanna / Sworkit : Has there been any testing with Phonegap builds and iOS 6 ? If so , anything we should be cautious of ? * Andrew Trice : Yes , but unfortunately we are restricted by the Apple developer NDA , so we cant talk about it that much yet . PhoneGap 2.1 ( which is coming very soon ) will address a few of the issues . I will be posting more content about this on the PhoneGap blog soon . * Andrew Trice : Prior to PhoneGap 2.1 there are a few quirks , but most apps will not see them . With PhoneGap 2.1 and later , those issues will be fixed . here 's one of the bigger ones : https : **39;10162;TOOLONG <p> 14 . David R : Does the PhoneGap cloud supports debug build ? * Ray Camden : Yes . You can see it in the settings of the app on the site . <p> 15 . Steven Benjamin : Proguard is the built @ @ @ @ @ @ @ @ @ @ for eclipse . * Piotr Walczyszyn : I do n't  know of any configuration that is official , found a config posted by someone from a PhoneGap community : LONG ... <p> 16 . Andrew : I 'm editing my code directly with DW CS5 that does n't  have any phonegap support , when the cordova docs talk about " permissions " for android , ios , etc is there a place that i should manually assign those or does build.phonegap.com take care of all that automatically ? * Andrew Trice : They are managed inside of your config.xml file https : **36;10203;TOOLONG look under " PhoneGap API Features " * Andrew Trice : ( config.xml is required for PhoneGap build projects ) <p> 17 . AurelioDeRosa : Is it possible to detect if GPS is active or not ? If this is not possible ( as I guess ) , will the developers add the possibility to the compatible OS as Android ? * Ray Camden : If you are asking can I use geolocation but only if GPS versus other means I believe the answer is no . You @ @ @ @ @ @ @ @ @ @ the device will use the *best* possibly , not *just* the best available if that makes sense . In the result object you get a key for accuracy though . You could use code to throw out results that are not accurate enough for you . <p> 18 . Robert Winterbottom : If i need to edit some options in the Android Manifest xml that I do not see options for in the config file for phonegap build , what 's the best way to do this , currently I am decompiling the apk , editing the manifest , recompiling , resigning , and zipaligning which seems like a lot of work to edit the manifest , in particular I am trying to edit supported screen sizes * Ray Camden : Afaik , you are doing the only thing you can do . I think you would want to reach out to us ( Adobe ) about adding support in for what 's missing . Can you email me and I 'll try to forward it up the chain ? <p> 19 . Wyatt : Is there a way to utilize android fragments @ @ @ @ @ @ @ @ @ @ as an intent from a plugin ? * Ray Camden : I do n't  know what Android fragments are . I do know you can fire intents and listen for intents in Android/PG via a plugin . So if you just want to -start- the intent , you definitely can . I 've got a blog post on that . <p> 20. sheridon : Can you guys give us any heads up on Phonegap Qt or desktop use ? We would absolutey love that for some internal depts ( non-profit ) * Andrew Trice : There are phonegap ports for Windows 7 ( see LONG ... ) , and there will be one for Windows 8 as well . However , I am not familiar with QT <p> 21 . AurelioDeRosa : Yes , I asked for GPS only . I would like to user a method that allow me to detect if GPS is active , for example . So , will you add this feature in the next releases ? * Ray Camden : I do not know if this is something being added in the future . You @ @ @ @ @ @ @ @ @ @ writing Java code there . Ill check into this though . <p> 23 . Amanda : I want to save data locally to be uploaded to a server when network becomes available . i.e. when the phone connects to any kind of network , have the data uploaded . Is this possible with phonegap ? * Piotr Walczyszyn : yes it is possible , you can detect network status with PhoneGap : LONG ... <p> 24 . Andrew : is config.xml where i would put the permissions to allow geolocation for android devices and phonegap build ? IEX : * Piotr Walczyszyn : you do it in config.xml with following entry : * Piotr Walczyszyn : with you only configure a plugin but this is not enough to turn on the permission * Piotr Walczyszyn : you can learn more about config.xml here : https : **36;10241;TOOLONG <p> 25 . StephenR : As an intermediate ColdFusion user , I was wondering what is the BEST way to send and retrieve database calls in a PG app ? Most of the apps I have in mind are media based , bringing @ @ @ @ @ @ @ @ @ @ music album that I have detailed in the database . Thanks for answering all these questions guys . * Ray Camden : I hate to say that " X " is the best . Ill just talk about what has worked well for me . First off I 'd make use of CFs built in ability to serve up JSON from CFCs . That makes your code somewhat simpler and allows for the best reuse imo . IN your specific example , I could imagine a case where the CFC returned an array of URLs for the media that you could then pass to PGs " Media " API . <p> 26. sheridon : Is there a " way " /method or manal we can use to port or help port PhoneGap to Mac OS X or Linux ? Two depts use Ubuntu and Mac . * Piotr Walczyszyn : Well PhoneGap is a distribution of open source Cordova project so you can do it on your own , there is was some intial work done on it for Mac but I do n't  it is very active at this point @ @ @ @ @ @ @ @ @ @ you very much for the answer . How can I know if it will be added ? Have I to wait ? * Ray Camden : You would have to watch the site . They always announce new updates/etc . <p> 28 . Wyatt : To follow up on my question , fragments are androids concept of segmenting code to change how things are displayed in particular of if it is a phone or tablet . The issue with firing an intent to do this is that an intent gains the full screen as if it is an activity so something like a dialogFragment becoms useless . Android is moving away from dialogs and other of their design concepts in favor of fragments so support for it would be nice . * Ray Camden : Wyatt thank you for that explanation . That is pretty darn cool . Afaik we have no support for that at all . You could use a plugint to try to interact with it . Sorry I do n't  have more for you there . <p> 29 . Nathan Buth : In app I am @ @ @ @ @ @ @ @ @ @ errors on iOS . Both of thse errors come after a **32;10314;TOOLONG and access the file . I am curious on if there ae bugs with this in iOS because i works fine on Android . * Andrew Trice : What kind of file is this happening with ? An image , text , or something binary ? <p> 30 . StephenR : Is pulling in CF rendered HTML via ajax a bad idea or just let 's flexible ? * Ray Camden : In general , your services should return data only . However , there are always exceptions . If the conversion from data to HTML is super complex in JS and easier in CF , then I say make the *practical* decision and do it . ( Just separate the code into two methods to keep em cleaner . ) Also note that templating engines , like Handlebars , make it easier in client apps to do HTML from data . <p> 31 . Steven Benjamin : For Phonegap 2.0.0. on Android does the splash.png reside in the res/drawable-(x)dpi/ folders ? * Piotr Walczyszyn : Yes <p> 32. @ @ @ @ @ @ @ @ @ @ blog that has optimization tips for PhoneGap in general or the specific builds ? * Ray Camden : In general , these issues come down to the platform for ex Android having weaker support for so and so versus the embedded web view in iOS ( or vice versa ) . So I 'm not aware of a PG doc in that regards , but Ive seen various blog posts . Some tips apply to all like using touch versus click , but that is n't PG specific , its mobile specific . <p> 33 . Amanda : Thank you . I 'm currently using the network feature with **33;10348;TOOLONG to detect if a connection is available when the user submits . Is that no longer used ? * Andrew Trice : **33;10383;TOOLONG is unfortunately sub-optimal , and does n't  always report accurately . I normally attempt to make a request , and do n't  let the user proceeed until it is successful ( or store all data locally and have a manual synchronization routine ) <p> 34 . Ryan Hanna : Which file in the LowLatencyAudio plugin would be a good place @ @ @ @ @ @ @ @ @ @ AV Audio Session Category ? * Andrew Trice : LowLatencyAudioAsset.m is the one you want . * Andrew Trice : That is what is used when an asset is loaded via preloadAudio() if you are using preloadFx() , it uses **32;10418;TOOLONG ( system sounds ) <p> 35 . Steven Benjamin : For PG 2.0.0 on Android , I notice that xlargescreens is no longer in the androidManifest , My splash screen on my HTC OneX is not working , and I am thinking that may be the cause , is there a reason that is it no longer there ? * Piotr Walczyszyn : Do n't  think there is a reason for that , currently I have hdpi , ldpi , mdpi and xhdpi resolutions in my project and it works on most of the devices <p> 36 . Tac : You mentioned Handlebars how does that compare to understore ? Were leaning toward backbone as a framework , not sure what templating engine to use , but the more examples out there the better . I 've read through the examples on http : //coenraets.org/ , but what I want @ @ @ @ @ @ @ @ @ @ desktop browser ( for debugging simplicity ) * Ray Camden : Tac , speaking from *limited* experience , Handlebars is just templating , while Underscore is a utility library with lots of other features . So I 'd say it just plain depends . Frankly I 'd say use what is best for your development style . I prefer the template style of Handlebars over every one Ive seen but that 's just me . <p> 37 . Nathan Buth : Well at th moment it is an empty directory I am using it to call a series of functions that will write in an xml file in one case . In the other it is to add an image file taken with the camera . * Andrew Trice : See this message thread from the cordova mailing list : LONG ... it looks like that is invoked when the URI does n't  specify a protocol * Andrew Trice : See this thread too : LONG ... * Andrew Trice : ( add file : // prefix ) <p> 38 . Amanda : Oh so you would recommend attempting to connect to the @ @ @ @ @ @ @ @ @ @ instead of checking the network connection ? I will do that then . Is a manual sync the only way to sync ? I was hoping to have the app sync with the server when network became available even if the app was n't openIs that possible ? * Andrew Trice : I typically do this : ( capture data , save locally , attempt to push to server ) all in one process . If that is successful , delete the local copy . If not , save it to push later . You can automate it by just using a timer , or using " online " event LONG ... <p> 39 . Shaun : I am about to try phonegap , is there a good tutorial size that you would recommend ? I have experience developing for the android , but thought I would try something different . * Ray Camden : Personally I 'd just do the guide for your platform ( Android ) and get the default ( empty ) app running on your device . Then just starting writing your HTML/JS . THere are a butt @ @ @ @ @ @ @ @ @ @ how to do so and so with PG . <p> 40. sheridon : Can you provide us with any information on using PhoneGap for the not-so-famous platforms such as : 1 . Google TV ; 2 . Amazon ; 3 . Barnes &amp; Noble Nook ? * Andrew Trice : Nook and Amazon Kindle fire are no probelm " out of the box " . The developer process for google tv is restricted from what I hear , but I have n't tried it myself . <p> 41 . MarkB : Is it possible to run an entire app that is served from an external source ( i.e. , a web server ) through PhoneGap and keep it in the DroidGap Activity without using plugins like ChildBrowser ( whenever I click a link in PG w/o CB it launches an Intent into the browser ) ? If so , do you know of any good resources or projects that would help ? * Ray Camden : You can do it in Android with this trick : LONG ... Not sure if you can do it with iOS , but if you @ @ @ @ @ @ @ @ @ @ Not an issue though if you are doing Enterprise deployment . <p> 42. sheridon : Are there plans to support Blackberry OS 10 and any issues with the BB Playbook deployments ? * Andrew Trice : Yes , it is supported . RIM has contributors to the PhoneGap/Cordova project who will continue to evolve the support . See more detail here : LONG ... <p> From that post ( from RIM ) : We are striving to bring BlackBerry WebWorks and Cordova closer together , and are actively part of the Cordova open source project providing contributions . * Andrew Trice : However , I am not aware of exact specifics of what is supported by RIM . <p> 43. berman eyal : what is the best javascript framework for using with PG ? * Andrew Trice : you can use any of them . PhoneGap is not prescriptive of architecture choice . Many people are successful with jQuery , Backbone.js ( . js ) , Sencha , jQuery Mobile , etc If you are familiar and comfortable with one in particular , you can keep using it . <p> @ @ @ @ @ @ @ @ @ @ configurations exactly as previously posted It does work for PG 1.8.0 but not for PG 2.0.0 * Piotr Walczyszyn : do you have in config.xml * Piotr Walczyszyn : ? <p> 45 . Steven Benjamin : Piotr I found the problem The super.loadUrl was missing the delay time . * Piotr Walczyszyn : Happy to hear that <p> 46 . Robert Winterbottom : Are there any advantages to using PhoneGaps storage api and writing to a database compared against HTML5s local Storage ? * Ray Camden : Robert , it is n't better or worse it is different . LocalStorage is good for key/value pairs . Ie , favoritecolor : red . WebSQL is good for content at a larger scale , and for stuff you have to search . Short answer is use the feature that matches the type of data you are storing . <p> 47 . David R : Just curious .. Titanium vs PhoneGap which one is best ? * Piotr Walczyszyn : If you really look into it deeper these are completely two different approaches , so you cant really compare it . With PhoneGap you @ @ @ @ @ @ @ @ @ @ as a programing language which gets compiled to native code . <p> 48 . Station 8 : I was considering going to the AppMobi development environment using Phonegap . What I am trying to do is get away from having to develop on a Mac for iOS apps . Does anyone have any experience with this ? * Andrew Trice : You can use PhoneGap build ( http : //build.phonegap.com ) for free to develop iOS apps without being on a mac . <p> 49 . Amanda : Ray , what do you mean by " content at a larger scale " ? Sorry for not knowing * Ray Camden : Well , obviously it is n't cut and dry . I use LocalStorage for data that is simple . THings like config settings , or preferences . Like , favcolor : blue or lastSearch:foo . I use WebSQL for ad hoc data that can grow to any size . Like perhaps free form text notes that user enters . <p> 50 . Shaun : This may be a stupid question , but are there any issues with support when using @ @ @ @ @ @ @ @ @ @ Andrew Trice : it depends on the target device/OS. - - For example , Android 2. x does n't  support SVG or &lt;Audio&gt; or &lt;Video&gt;. - - It is not a phonegap limitation , rather the capabilities of the OS b/c PhoneGap uses the system web browser. - - For PhoneGap audio , you can use the Media class to reliably play audio across devices. - - For HTML/CSS3 implementations , check out http : //caniuse.com/ to see if the feature is supported on your target OS . <p> 51. berman eyal : i want to learento create plugin , what plugin is a necesary plugin ? * Piotr Walczyszyn : You need plugins when out-of-the-box functionality is missing something or current plugins which you can find here https : **38;10452;TOOLONG do n't  do what you need . Here is docs how to create plugins : LONG ... <p> 52 . Amanda : OK . We are using the local storage api to store form data temporarily and then pushing it to an SQL database with coldfusion . I want the app to be able to grow without getting bogged @ @ @ @ @ @ @ @ @ @ a better route ? We have discussed couchdb and sqlite , etc * Ray Camden : I think using LocaleStorage to store the form makes perfect sense . Its one form probably just 5-6 fields . <p> 53. berman eyal : how to implement push notification for android ? * Piotr Walczyszyn : * Piotr Walczyszyn : Well you can do as you would do with standard Android development , you can also use http : //urbanairship.com/ * Piotr Walczyszyn : there is also a ready plugin for you : LONG ... <p> 54 . David R : How do I call native PG functions through Javascript ? * Andrew Trice : There is " out of the box " functionality exposed through the PhoneGap API . See details here : http : **39;10492;TOOLONG If you want to integrate with custom native code , you can create what 's known as " native plugins " , see detail for creating those here : LONG ... There are also a bunch of open source native plugins that you can use/reference here : https : **38;10533;TOOLONG <p> 55. sheridon : Is there any @ @ @ @ @ @ @ @ @ @ like you have with the other " main " platforms ? * Ray Camden : for that sheridon you need to check with the product sites . for ex , nook does n't  support gps. that 's not something PG would normally document . make sense ? <p> 56. sheridon : And also , any emulators you know of for those devices ? * Andrew Trice : For Kindle , its just the Kindle Fire device ( you have to enter a code for it to be recognized ) . For Nook , download the Nook development kit : LONG ... * Andrew Trice : kindle fire emuilator guide : LONG ... <p> 57 . Nathan Buth : Is there a way to make phonegap 2.0 use the front facing camera of a device like the Nexus 7 that has no rear facing camera ? * Ray Camden : Nathan , it should " just work " ie , it will get the only camera it has access to . <p> 58 . Mikel : Any suggestions on the best way to tell what type of device that you 're on ? @ @ @ @ @ @ @ @ @ @ features you have available for use ? * Ray Camden : There is a Device API in the docs . It tells you well the device . You can also use JS to check things like screen size too . * Andrew Trice : you can also use JS to test CSS media queries to make a determination programmatically. see : https : **37;10573;TOOLONG <p> 59 . Preston : Any suggestions on snappiest JS/PG Framework/Tools ? * Andrew Trice : It depends on the developer and their preferences . Some people like jQuery , some people like Sencha , some people like no frameworks at all . One of the challenges with HTML development in general is that there are hundreds of frameworks , and thousands of ways of approaching a problem . We try not to be prescriptive in tech/architure inside of your phonegap experience . All HTML frameworks will work . <p> 60. sheridon : Any apps using PG shipped on iPad that you can share ? * Andrew Trice : Lots see the showcase here : http : //phonegap.com/app <p> 61 . Preston : PG seems dramtically @ @ @ @ @ @ @ @ @ @ hear this is because of the lack of hardware acceleration in apps ? is this something to just wait for or are there work-arounds ? * Piotr Walczyszyn : You can make really perfomant apps with PhoneGap , one issue is that on some platforms WebView component that is used by PhoneGap is restricted to use older/slower JavaScript viruatl machines . * Piotr Walczyszyn : You can have hardware acceleration in PhoneGap apps also , and you enforce with same techniques as in browser like for example : translateZ(0) etc <p> 62. tony : what are the difference between application done by phonegap vs native applicatinon done by obj c and if there thing canit done by phongap ? * Ray Camden : Tony its a big question . Obviously if you *know* native dev , it may be best for you . If you do n't  and know HTML already , then PG is a great way to get on device . Even better , it allows you to go on *all* the devices at once . As for things that ca n't be done in PG , in general @ @ @ @ @ @ @ @ @ @ in those holes . * Andrew Trice : Its also important to understand that PhoneGaps UI rendering is based upon a web view . You can create any html experience for your app , but do n't  expect things like 3D games coming to PhoneGap quite yet . You can read more about " phonegap ui " here : LONG ... <p> 63 . Shaun : Do I have to use Eclipse or can I use dreamweaver ? * Piotr Walczyszyn : You can use any editor you want , notepad , vi , Eclipse anything <p> 64 . MarkB : Android Any recommendations for handling page transitions ( e.g. , a " loading " dialog of sorts ) when moving between two different URLs ? * Piotr Walczyszyn : You can use jQuery Mobile for that , also you can use my framework called BackStack : http : **34;10612;TOOLONG <p> 65 . Spencer : Is there a way to cancel a PhoneGap plugin call that is already in progress ? For instance if the app is waiting for data from a Bluetooth device , can the user cancel this @ @ @ @ @ @ @ @ @ @ * Andrew Trice : Out of the box , there is no way to do this . However , you can add functionality to your native plugin that will manage this for you ( queue pending requests , kill pending requests ) . This would have to be managed in the native code layer . <p> 67 . Amanda : Andrew , how do you handle the data on your fresh food app ? is the information bundled with the app or is it pulling from a server ? if a server , what kind of db ? * Andrew Trice : it is bundled . Its just a REALLY big JS array , and I 'm doing client side filtering . I embedded it b/c I wanted it to work even when offline . * Andrew Trice : To create the array , i imported the data from the FDA ( excel export ) into a MySQL database , did some minor data transforms , then used ColdFusion to generate the JS string . Once I exported it , its a " plain-old " JS array . To publish new @ @ @ @ @ @ @ @ @ @ the app . Luckily , the FDA only publishes their data once a year . <p> 68 . Preston : Is jquery a dependacy of BackStack ? * Piotr Walczyszyn : Well BackStack is dependent on Backbone and Backbone is dependent on jQuery or Zepto <p> 69 . MarkB : As a clarification to my earlier question , my content is to be served from an external web server and the resulting HTML will use HREF links to move between pages . Is it possible to have some sort of " loading " dialog that persists between the unloading of the first page and the loading of the second ? ( forgive my ignorance of Backbone/Backstack if it does this , my brief overlook of it seemed to focus on AJAX or dynamically loaded content ) * Ray Camden : There are a couple of ways you could handle this . One possibly simple way would be a jQuery event handler for anchor/click events in general . You could automatically display a loading type div that would go away when the page loads . Not sure if that helps , @ @ @ @ @ @ @ @ @ @ remote content to load . <p> 70 . Tac : Andrew : Wouldnt storing the food data in a SQLite database be better ? At least better for your readers , who would use it as an example ! * Andrew Trice : Take a look at this example from Ray : LONG ... I just went for the simple approach since the data is read-only . <p> 71 . Tac : Also , if you were to expand on the Fresh Food app , I 'd vote for integrating an Open Source Places API , so you could find Fresh Food close to where y ou were at the time * Andrew Trice : good idea , thanks ! <p> 72 . Preston : Is there uglify or compile code when using BackBone/BackStack ? I wouldn * Piotr Walczyszyn : there is a BackStack minified version here : LONG ... you can also find one for backbone : http : **32;10648;TOOLONG <p> 73 . Preston : t think closure complier would work but would you know of any that do * Piotr Walczyszyn : * Piotr Walczyszyn : Yeah closure @ @ @ @ @ @ @ @ @ @ a way to uglify or compile code when using BackBone/BackStack ? * Andrew Trice : You can minify or uglify JS code , but since it is interpreted , not compiled , so true " compilation " is not an option , just obfuscation . <p> 75. jared : My team has proven they can use phonegap on android and switch over to native code to do things that are not possible in phonegap like PDF annotation , ect . We are very comfortable on android but little experience with iOS . Does anyone know that it is possilbe on iOS to switch back and forth between phonegap and native XCODE ? * Piotr Walczyszyn : On iOS as on Android you can extend your PhoneGap application with native plugins which are written in native code . * Piotr Walczyszyn : So answering your question yest it is possible and it is done similar way . <p> 76. jared : Do you have any recommendations for a good cross platform PDF sdk ? Looking at branchfires * Andrew Trice : What do you mean ? Normally just linking to a @ @ @ @ @ @ @ @ @ @ pdfs ? <p> 77. jared : Open the PDF manipulate like add comments , draw , highlight ect. * Piotr Walczyszyn : Do n't  have a good one but branchfire looks decent and it should work for both . <p> 78 . Preston : BackStack does n't  include iScroll4 as a dependency . This is included in App-UI , does this mean that mobile lists scrolling is handled differently ? * Andrew Trice : BackStack does n't  have scrolling built in . You can use iOSs default scrolling , or use iScroll . <p> Open Questions ( 7 ) 79. boobyWomack : I can not get the one-liner to work though 80. boobyWomack : thanks Piotr 81. boobyWomack : thanks andrew , I had n't  come across the emulator before but I am very new to this . It sounds great ! 82 . Nathan Buth : I am having issues where I keep recieving encoding errors in my app . The Android version works perfectly but on iOS it gives me the errors . I will post small segments of the code below to help . The two **32;10682;TOOLONG @ @ @ @ @ @ @ @ @ @ think it just is not in scope , but it works fine on Android . Any idea on why it is not working ? Here is a link to the code . LONG ... 83 . Shaun : site sorry 84 . Preston : Webapps once in phonegap seem slower , mistyped a little 85 . Preston : is jQuery a dependency of BackStack <p> I recently worked on an application that was used to demonstrate the process of capturing data from a mobile application created using PhoneGap , and pushing that data into a LiveCycle servers workflow engine . Since my data was being captured and stored locally as JSON objects , I asked myself " why cant I use a template to generate the XML for sending to LiveCycle ? " It turns out you can , and its not that hard at all . <p> The entire mobile experience was created using HTML , CSS , and JavaScript . Since the prototype application was intended to work in offline scenarios , it did n't  push to a server automatically . Rather you had to manually push data @ @ @ @ @ @ @ @ @ @ API service invocation . Since I could n't guarantee that the device was on the network , all data was collected on the client using a standard HTML form and data was saved locally as JSON strings using PhoneGaps local storage imlementation . <p> Since the data was stored with " plain old " JSON , it is really easy to manipulate using JavaScript . You can transform it into HTML , raw text , XML , or whatever format you want . To present the data back to the user within the application experience I took advantage of the templating features from underscore.js . Templating in HTML/JS apps enables you to separate dynamically generated HTML structure from your application logic , and it keeps your JavaScript code clean without having to manage lots of string concatenation . I used these templating features- to generate all views within the application , including tabular data views and even HTML forms that auto populated with data . Basically , anywhere there was an HTML-based view , I used a template to generate that HTML . <p> The REST API for LiveCycle expects you @ @ @ @ @ @ @ @ @ @ that match the information captured within the workflow process . The exact structure of the XML depends upon how the LiveCycle server/document workflow is configured , but in any case , it is still XML . You can use- templates- to generate this XML , or any text-based structure for use in any service call . <p> This technique can work in many scenarios , and with any endpoint whether it is REST , SOAP , some other JSON string format ( transformed ) , or something entirely different altogether . Templates enable you to cleanly generate text strings from JSON objects . <p> here 's basically how I used templating : <p> JSON to HTML I used teampltes to generate my HTML views within the application . <p> JSON to XML I used templates to generate the XML structure that was passed as parameters to the LiveCycle servers REST API . <p> Below is a really simple example showing how you can use templates to transform JSON objects for use within your applications . It uses jQuery for quick and easy DOM manipulation , Twitter Bootstrap for UI styling , @ @ @ @ @ @ @ @ @ @ button to display a string version of the JSON object using JSON.stringify() ( without any templating ) , click the second to display a HTML table that was generated using a template , and click the last button to generate an XML string using a template . <p> This example is intended to be pretty simple all of the JavaScript and the templates are included in one file , and it only demonstrates how to generate XML strings . I 'm not posting it to a server in this example . Once you 've generated the XML string , you could do anything with it , I posted it to the LiveCycle server using a simple jQuery $. ajax() call . <h> Generating HTML <p> Next , let 's examine how the HTML structure gets generated . The generateHTML() function uses an underscore.js template to generate an HTML string , which gets loaded as the content HTML of the " output " div . <p> The HTML table is generated using the following template . Notice that underscore.js template uses a JSP-like syntax for injecting values . This template will loop over the @ @ @ @ @ @ @ @ @ @ entry . <p> Here are my presentation slides and extra content from last nights DC Droids meetup . I gave the presentation " Enterprise Android Applications With PhoneGap " . - There were excellent questions and conversations at last nights event Thank you everyone for attending , and making it a great meetup. 
@@106848996 @2248996/ <p> I covered techniques for making your apps feel like " apps " , not " web pages " . - You can read more about these techniques and useful libraries in my recent post on Multi-Device Best Practices . - Note : That post contains references to both Flex and HTML/JS/CSS tools . - In this presentation I focused only on the HTML/JS/CSS tools . <p> In this presentation , I covered PhoneGap Build , a cloud-based compilation tool for PhoneGap apps , and- debug.phonegap.com for remote application debugging . - I also covered iWebinspector for debugging PhoneGap experiences inside of the iOS Simulator . <p> Let 's also not forget the real-world companies that have invested in PhoneGap/Apache Cordova , including Wikipedia , Facebook , Salesforce , IBM , and others . - You can read more about these companies from my recent post " Who Uses PhoneGap " 