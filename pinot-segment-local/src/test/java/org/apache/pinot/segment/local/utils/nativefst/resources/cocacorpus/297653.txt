
@@44332454 @4332454/ <h> Netflix Master Of None <p> Does anyone have any info about production and/or post production on NETFLIX 's MASTER OF NONE ? Imdb tech page not say much .. <p> i 've watched it all and i though the color work is actually very very good for a show that is supposed to look " PLAIN " , technically quite precise and really pleasant to me ( aside for episode 9 ( that is all over the place for some reason ) , and aside for lots of noise that sometimes show up even in bright scenes ( that could have been denoised easily .. ) <p> somehow i have the feeling that has been shot on F55 ( also because of the random noise ) but ca n't find any info about it ... nor i can find about who @ what company colored ... thanks g <p> Alex Bickel and Mike Howell . Both are at Color Collective in NY I think . Do n't think that was noise , probably cinegrain or another grain overlay . All netflix shows are mandated to be shot on @ @ @ @ @ @ @ @ @ @ originals . <p> All netflix shows are mandated to be shot on red last I heard having worked on a few netflix originals. 43971 @qwx453971 <p> The mandate , I believe , is that their shows be captured in 4K , as in 17:9 DCI rather than UHD . My son recently shot a comedy special for them with F55s . In a pre-production conference call he asked the question : How does the network deal with a 4096x2160 raster inside a 16:9 display ? Are the sides cropped or is it letterboxed ? After a pregnant silence a voice finally came back : We letterbox it . So there you go - Netflix policy , for that one show at least . <p> And yes , I agree the show has a subtly sophisticated grade though not necessarily pretty . I find myself trying to analyze it . <p> All netflix shows are mandated to be shot on red last I heard having worked on a few netflix originals. 43971 @qwx453971 <p> That is completely untrue . Netflix shows are mandated to be captured on a camera that has @ @ @ @ @ @ @ @ @ @ RAW . Netflix shows are being shot on Red , F55 , F65 , Panasonic Varicam 35 , and some others , and additional cameras on those shows commonly include GoPro 4K , Blackmagic 4K , Panasonic GH4 , and others . Delivery is mandated to be either 4K ( 4096x2160 ) or UHD ( 3840x2160 ) , but they do prefer the " full " 4K . Examples of F55 shows would include Sense8 , Wet Hot American Summer , and others . Examples of Red shows would include House of Cards , Narcos , Daredevil , Grace and Frankie , and a number of others . An example of a Varicam 35 show is Orange Is the New Black ( the upcoming season ) . <p> That is completely untrue . Netflix shows are mandated to be captured on a camera that has a capture resolution of at least 4K and can record RAW . Netflix shows are being shot on Red , F55 , F65 , Panasonic Varicam 35 , and some others , and additional cameras on those shows commonly include GoPro 4K , Blackmagic 4K @ @ @ @ @ @ @ @ @ @ to be either 4K ( 4096x2160 ) or UHD ( 3840x2160 ) , but they do prefer the " full " 4K . Examples of F55 shows would include Sense8 , Wet Hot American Summer , and others . Examples of Red shows would include House of Cards , Narcos , Daredevil , Grace and Frankie , and a number of others . An example of a Varicam 35 show is Orange Is the New Black ( the upcoming season ) . 
@@44332455 @4332455/ <h> Cinematographer seeks understanding of curves functions <p> That title may be a little misleading but it is the best way that I could find to shortly describe what I 'm trying to understand . <p> I am a cinematographer , I am not a colorist . While I strive everyday to understand more about color theory and science I have the most experience you 'd lysing film negative in image acquisition . Therefore I continually try to duplicate the types of images that I might be able to capture on set in the computer . <p> What I would like to know is why all of the sudden desaturation of Shadows is the new fad . As I strive to make my digital footage behave like film negative , I see all too often internet sites catering to the new or beginning or low budget filmmaker present techniques discovered on a single YouTube video , and like wildfire the techniques spread . I have tried to understand previous to this a technique of highlight recovery was all the rage instead of just proper exposure . <p> So I @ @ @ @ @ @ @ @ @ @ she 'd some light or point me in a direction of some reading so that I might understand if these tools would help me in exposing digital images better . <p> As a cinematographer we are constantly learning how to deal with the tools that we are presented , and I would like to do my job in camera as best as possible so that when post-production happens , the colourist does n't have to work so hard to correct mistakes but can use their creative abilities to enhance the final Image . <p> This is why I am seeking clarification on why desaturate Shadows has become all the rage in making a more film like image . Usually when making workprints my negatives are printed in the 40s on the printer lights . I am very appreciative that DaVinci Resolve has a printer light tool and which I might use to understand and better Translate the techniques and workflow I am used to dealing with . <p> If anything you ca n't see through my shadow areas on the negative because it 's has a good quality density . Anyhow @ @ @ @ @ @ @ @ @ @ if anyone has a color science book or resource that might help me to understand the curve functions of luminance versus saturation in the digital realm and how that I might best utilize these tools to get a better highlight rolloff without sacrificing negative density . <p> Really what I want to do is learn how to make DaVinci Resolve behave like film negative . I do not want a " film look " , whatever that is , I want the behavior of negative such as it may be achieved or limited . <p> It 's not sudden . We 've been using black desat going back to the 1980s . <p> Visual trends come and go , and there 's a lot of stuff happening with " modern " feature images . I would n't say desaturated blacks are necessarily a thing , but I think there is a trend among some to desaturate a lot of the image , sometimes in selective ways . It 's a good question who 's asking for it , but my take is that directors and DPs are shown a variety @ @ @ @ @ @ @ @ @ @ what they want . I think a lot of them want a visually striking look that 's the opposite of beautiful , something arresting and with a lot of impact and contrast , but not lush and colorful . <p> I personally only grab black desat if I have a matching problem , but I use it very carefully because of the potential for artifacts . There are those that like a lot of color in the low-lights , but I 'm not a fan of that look . ( I have had DPs put a gun to my head and force me to do it , but it 's not fun for either of us . ) <p> BTW , it would be the camera you use that would be like the film negative . Think of Resolve as only the film lab . Resolve ( or any other color corrector ) merely refines the look that the camera captures . Glenn Kennel 's book Color &amp; Mastering for Digital Cinema goes into a lot of the theory very well . <p> Personally , I try to avoid black @ @ @ @ @ @ @ @ @ @ neutral when needed . I think it leads to more natural looking shadows . 43971 @qwx453971 <p> I think that 's a wise move , but I 'll use black desat as a touch-up when I have really problematic film material with bad black shading ( which does happen ) . I think you have to use it very , very carefully . <p> Curves overall can be very useful , particularly for gracefully yanking down the intensity of certain colors and skewing others where they need to be . <p> It 's not sudden . We 've been using black desat going back to the 1980s . <p> BTW , it would be the camera you use that would be like the film negative . Think of Resolve as only the film lab . Resolve ( or any other color corrector ) merely refines the look that the camera captures . Glenn Kennel 's book Color &amp; Mastering for Digital Cinema goes into a lot of the theory very well . 43971 @qwx453971 <p> Interesting what you say , I guess I 've never really thought of desaturation in @ @ @ @ @ @ @ @ @ @ I guess flashing the negative would have the same effect . <p> I agree with you that it is the camera that reacts like film negative , that 's why I like the Alexa so much . I also like my Blackmagic Pocket very much , I am surprised by it everytime I shoot a test . I 'm trying to decide if I want to purchase the new line of cameras from BM , as they really have the only color I actually like . Not that the other manufacturers have bad color science , I just prefer BM . <p> I 'll continue to refine my use of color correction software in ways that work like a lab does - I really hate relighting in post .... I 'm amazed by the abilities of the software these days , but I would rather get it right in camera . <p> I 'll continue to refine my use of color correction software in ways that work like a lab does - I really hate relighting in post .... I 'm amazed by the abilities of the software these days @ @ @ @ @ @ @ @ @ @ . 43971 @qwx453971 <p> Often , I tend to kind of " unllight " in post : add a flag to a background or a shadow to a face because I know the DP just did n't have the time to do it on set , or there was literally no place you could hang the flag without getting it in the shot . In TV commercials , it 's pretty typical to add just a skosh of fill light to faces and eyes just to draw the audience 's attention . <p> But it all starts from the work of the DP and how they initially light and expose the image . I often say , " we ca n't add an eyelight or a backlight , but we can enhance the work that 's there . " Adding filtration and style is another thing that works very well in post . <p> I often say , " we ca n't add an eyelight or a backlight , but we can enhance the work that 's there . " Adding filtration and style is another thing that works very @ @ @ @ @ @ @ @ @ @ thinking of The Dark Knight , and how distracting the overemphasised eyelights in the dinner scene are . I could n't figure out how one might achieve that on set because the eye is not normally that reflective so it had to be enhanced . And , because I do n't actually know if it was done - I sure would like to see the setup if its in camera .... <p> I 'm gearing up to shoot a television pilot next week . Its being done in an older , hard light style , deep focus , and I 've been constantly testing out the best way to work with the digital sensors which is why all this came about . I 've studied the techniques and talked with living cinematographers who worked in the era about best practices , but my frustration comes from trying to talk with users of new cameras , or DP 's who have left the old school completely behind . It 's quite hard to find working DP 's writing about using the tools as I assume we 're all busy ! That @ @ @ @ @ @ @ @ @ @ - boring .... 
@@44332457 @4332457/ <h> ownership of finished grade <p> I do a lot of work with grading older films for blu-ray releases . I 'm in the position now where a couple of companies may end up hiring me to do movies utilizing the same raw scan for films I 've already done in the past for a different company and market . <p> I believe I can not just use my old grade . I do n't have any contracts dealing with who owns what , so I would just assume that a company 's payment for my work means they own the final grade . Does anyone know if this is correct ? <p> I could negotiate with the original company and offer a percentage deal to give the other company the grade . But if the first company owns the grade outright , what incentive do they have to not just cut me out and sell it to the other company themselves ? <p> I guess my safest option would be to just regrade the film without referring to my old project in any way ( except possibly its @ @ @ @ @ @ @ @ @ @ on the market , like I might do with any blu-ray ) and keep my mouth shut to both companies about duplicate jobs . Is this the correct thinking ? Am I missing something ? <p> When you say grade basically you 're talking about a series of settings , curves and other adjustments ? I have a hard time seeing someone owning the rights to that ... <p> We 've done commercials for different brands with the same photographer but different production companies and agencies and they will hold similar look and feels as it 's how the specific photographer works . I ca n't see anything wrong there IF that 's what the client wants . <p> If however you 're using the exact same source / picture this can have rights connected to it . Then resuming the exact same grade might not be " rights breaking " but possibly questionable . <p> typicaly the cleint owns the master , but not the idea 's or workflow that went into creating the master , those are the artists , and feel no restraints on useing powergrades i @ @ @ @ @ @ @ @ @ @ had the same situation , i 'd likely restore the older project , drop in the new media , use that as my starting point and see what i could do better today ... <p> Well , that 's the problem , these companies would be providing the same raw scan , as they are different markets leasing rights to their regional market . The lab would be providing the same exact file they provided to someone else a year ago or so . So I end up with the same movie and same raw footage . If it was a different scan , it would n't be so much an issue to me . <p> Netflix are a great example . I 'm sure there 's a reason why they are archiving everything from the rushes through to the ACES intermediate negatives and I 'm also pretty certain that it has nothing to do with the directors original intent - although I would also love for that to be archived in some sort of way if we are to archive everything forever . <p> - Edit : No slight @ @ @ @ @ @ @ @ @ @ of archival that has never been attempted to this degree before . We wo n't know the results of what this entails for some time yet . <p> If you went into a facility and used their equipment , my opinion would be that the facility owns all of your work : the session , the corrections , the final renders , everything . <p> If you do the session in your own facility , then I would say you own the work and the client only gets the results . It 's kind of like a great meal : the customers get to enjoy the food , but they do n't get to own the recipe . <p> There are always exceptions , and a lot depends on the specific deal you make . Editors have argued that clients frequently want their entire edit list , representing the work that went into the final edit . But re-recording mixers generally will not hand over the final Pro Tools session to clients , because there 's proprietary work in there . 
@@44332458 @4332458/ <h> Pricing Indy Feature <p> They want a project rate , and I 'm not really used to quoting such rates . Hence my question , is there some kind of industry standard expectation/allocation percentage for the post production based on production budget ? <p> Or rather , what 's fair/reasonable for color on an indie feature like this ? <p> I have to admit that I 've always found it unsettling when indy features can afford Alexa and Cooke , but expect sharply discounted labor , whether in production or post . <p> When people come with those requests , I usually just tell them to tell me how much they can offer . I give them my usual daily rate , and a rough idea of how many days it takes to grade a feature , and let them come up with a price . <p> To me , it depends a lot on the complexity of the film , the quality of the cinematography ( or lack thereof ) , and the expectations of the client . I would start at $10,000 all-in ( for , @ @ @ @ @ @ @ @ @ @ if you have to , but be sure to place limitations on the number of redos and fixes , and get it in writing . That 's assuming you have your own room and all the gear , and are responsible for all conforming and deliverables . I think this is a reasonable rate for a film in this budget range . <p> A typical mid-sized LA facility would charge at least $300/hour ( $2400/day ) , which would only give them 4 days . If you 're a freelance colorist with far less overhead , you could at least double the number of days to 8 and work with that . The problem occurs when they decide to re-edit the film , slug in 9 new sequences , or come back and decide they want the last third of the movie green instead of neutral . Changes that broad need to be paid for . Little fixes , I 'll let slide to a point . <p> I 've thought about the possibility of building a rate for these indie film grades based on program run time instead of @ @ @ @ @ @ @ @ @ @ changes are then charged again for those particular minutes of runtime . <p> I 've thought about the possibility of building a rate for these indie film grades based on program run time instead of an hourly/daily charge . Any additional re-grades due to editorial changes are then charged again for those particular minutes of runtime. 43971 @qwx453971 <p> I 've thought about that , too , but how would you charge for a commercial , where you can spend days on : 30 ? Whatever your formula , it really needs to take into account the effort required . <p> I 've thought about the possibility of building a rate for these indie film grades based on program run time instead of an hourly/daily charge . Any additional re-grades due to editorial changes are then charged again for those particular minutes of runtime. 43971 @qwx453971 <p> Program runtime is probably the least accurate way of pricing a feature ... Number of shots might be a better estimate , but even then there 's so much that can affect how long these things take . <p> To me , it @ @ @ @ @ @ @ @ @ @ the quality of the cinematography ( or lack thereof ) , and the expectations of the client . I would start at $10,000 all-in ( for , say , 10 days of work ) and negotiate down if you have to , but be sure to place limitations on the number of redos and fixes , and get it in writing . That 's assuming you have your own room and all the gear , and are responsible for all conforming and deliverables . I think this is a reasonable rate for a film in this budget range . <p> A typical mid-sized LA facility would charge at least $300/hour ( $2400/day ) , which would only give them 4 days . If you 're a freelance colorist with far less overhead , you could at least double the number of days to 8 and work with that . The problem occurs when they decide to re-edit the film , slug in 9 new sequences , or come back and decide they want the last third of the movie green instead of neutral . Changes that broad need to be @ @ @ @ @ @ @ @ @ @ to a point . 43971 @qwx453971 <p> Random Thoughts : <p> Cuts , Cuts , Cuts ! A nicely paced drama vs a " cutty " for the sake of being " cutty " piece are two very different animals ! I ALWAYS ( almost , see below ) specify that a rate is based on range of cuts . I did a 22min piece last week that was 635 cuts for the same price I normally charge for 350 - 450 , shame on me for not asking , never ASSUMED that the show would lend itself to that amount of slicing . <p> You can spend a day cleaning up a FCP or Premiere project if the " editor " is not a real editor . A one layer time line ( with rendered and replaced speed changes / ramps ) that actually matches the final pic lock is a thing of beauty . I have spent my fair share of time being a " video secretary " fixing 5 and 6 layer timelines , some of it even billable . <p> I like having about half the @ @ @ @ @ @ @ @ @ @ sharing stills and getting feedback . Then being supervised . Found this to be the most efficient way and allows me time to really get to know the project and " fix " any issues that may have been mentioned by the DP . <p> Set " Power Window " expectations too - probably not going to be able to re-light every scene on a 90 min show in 10 days . <p> One of my worst experiences involved a " FX " guy that NEVER worked on anything but his own AE projects and I had to teach him about color space issues , sizing issues , and how important MATCHING TC was - got to bill the main client for this and they back-charged the " FX " guy . <p> Finally , shot with an Alexa RAW ( ProRes4444 or Arri ) makes me breathe a sigh of relief too . Not looking for a flame war , but this seems to set some sort of floor on what the level of production it will be . <p> Program runtime is probably the least accurate way of @ @ @ @ @ @ @ @ @ @ better estimate , but even then there 's so much that can affect how long these things take . 43971 @qwx453971 <p> Yeah I generally do it on number of shots but even then sometimes I get tripped up ... on a film I have coming up the last 8/9 minutes is one shot but its a steadicam shot moving from outside to inside with tracked power windows a requirement . <p> Cuts , Cuts , Cuts ! A nicely paced drama vs a " cutty " for the sake of being " cutty " piece are two very different animals ! I ALWAYS ( almost , see below ) specify that a rate is based on range of cuts . I did a 22min piece last week that was 635 cuts for the same price I normally charge for 350 - 450 , shame on me for not asking , never ASSUMED that the show would lend itself to that amount of slicing . 43971 @qwx453971 <p> That is very true . But ... imagine if somebody came to you and said , " I have a 2-hour feature @ @ @ @ @ @ @ @ @ @ ( Birdman . ) A movie like that could easily take you six weeks because of the complexity of the shot structure and lighting , not just cuts . There could be 10,000 keyframes in a film like that . <p> At Complete Post in the 1990s , we once had a client call us late on a Friday night and say , " we have an emergency telecine session we have to book -- it 's just one : 30 second commercial with one shot . " I figured , " how hard could that be ? " so we let them come in at 10PM . But we discovered when they said " one shot , " they meant , " one shot of single-frame animation , each of which had to be individually color-corrected due to flash frames . " That turned into the 10 hour job from hell . And I knocked 2 hours off the session out of sympathy , and they still complained about having to pay the bill . And I just found this 1986 commercial on YouTube ( awful quality , but @ @ @ @ @ @ @ @ @ @ Complexity " includes quality of the overall photography , specific lighting/color challenges , and number of cuts or scene changes . I can watch a couple of reels and get a good gut reaction as to how long it will take , but in general , I think you can do one reel of 400-500 cuts in a single 10-hour day with no problem . If it 's very nit-picky correction , double the time . If they bring in several people to nit-pick , double it again . I have literally done features where we only got 4 minutes of material done per day , for months on end . As far as I 'm concerned , the client has control of the gas pedal and the brakes on the session , but managing client expectations is a big part of the job these days . <p> I 've thought about that , too , but how would you charge for a commercial , where you can spend days on : 30 ? Whatever your formula , it really needs to take into account the effort required . 43971 @ @ @ @ @ @ @ @ @ @ down from a flat or " All-in " as Marc called it , be it broken down by number of cuts , or by runtime , etc , needs to be contractually amended to reflect the reality of the complexities required/requested by the given work . I have n't worked out the details of this proposal but yes , there would be a LOT of requirements in the contract , and the runtime would be the BASE of the pricing . ( read : minimum ) <p> i 've been really busy finishing indie features , so it 's taken awhile to get the time to think about it all .. <p> for me it 's much like Marc ... day 1 ; 1/2 day to review by myself and add markers to problem scenes , then a 1/2 day spotting session with whomever has creative sign-off day 2-7 ; 1 day per reel by myself day 8-9 ; tweaks and addressing notes with whomever has sign-offs day 10 ; 1/2 day to drop in/grade vfx , pick-ups &amp; titles , then 1/2 day playout <p> i do n't really @ @ @ @ @ @ @ @ @ @ the number of cuts in a show , but i do care that the person who signs off at the end of day 9 takes that seriously <p> this assumes that i get a DRP that has been split screened and tweaked to perfection + a disk with the entire media pool in one place whole and complete , then use davinci to create an AAF to use on other software ... resolve seems to have become the standard conform tool , and every assistant editor seems to be comfortable working with it . <p> i do n't fix conform issues in the color sessions with this " low budget " model , that stuff is for shows with budgets .... <p> it does take me 1/2 day to copy the source media across to my local array , and open up an AAF and do a quick check that the conform was done more or less correctly , that i do not charge for <p> on shows with budgets day 1 stays the same , day 2-7 turns into day 2-15 or 2-22 , and the insert vfx can @ @ @ @ @ @ @ @ @ @ 694 VFX shots in it that i 've graded and i 'm now waiting for the finals to come back to me ... it will be a while , but it will be days not hours to drop all those back in and QC each one , so that is excluded from the gradeing deal ... it will tie up one of my rooms and lock out other gigs ... again we are taking this work on in Resolve with an the show 's assistant driveing. , then sending the aaf to another system for gradeing once it all looks good in the small suite <p> i will address any notes color from QC after that , and drop in other QC fix 's and insert them all into the master N/C , again as long as the production has done it 's best to sort those issues before shipping .. i have had post sups look at a shot say ; " it will likely fail QC , but send it as is and we will fix it afterward " - that i am far less likely to drop @ @ @ @ @ @ @ @ @ @ <p> but once it goes to QC no-one is changing the show without incurring charges for a new QC pass , so it 's done and dusted at the end of day 9 for pratical purposes <p> i tend to use Nucoda as my go-to choice for features , and DS for comericals , and i do n't do much in the way of tv or web shows with short turnarounds <p> and i do grade a few MoW 's in Resolve as one producer i work with has a workflow that uses resolve for both conform and packageing , and they want a DRP back so they can change the cut as needed for diffrent markets , but the color tools in Nucoda are much more to my taste <p> still give them a estmate based on 10 days @$xxx/per day , and if we go over they will know about it and approve it early as possiable , or we will find options that will work for everyone ... but one option that does not work well for me is working for free ... 
@@44332459 @4332459/ <h> The look of Partymonster <p> I was hoping that some of you could tell me how te look of Party Monster by The Weeknd was made . Looks like there was a lot of composing , hand drawing and the use of real VHS . It also looks like they added some 16mm or 8mm grain . <p> Beside that there was a lot of art direction and that they did an amazing job with lighting and colored gels , I 'm still wondering what they could have done inside a program like Resolve or Baselight to get this 80 's kind of look . <p> I worked a few years ago with them on some commercial stuff and was really impressed with there skills . Nice young cats with a lot of talent and fun to work with . At the time they had heavy creative cloud use AE , Premiere , magic bullet looks , plugins etc in fcp7 - these guys had skills editing , doing vfx , color , you name it , etc . Not sure if they still have a heavy hand @ @ @ @ @ @ @ @ @ @ been done by them or a colorist in a dedicated grading app but yeah given all the vfx clearly a decent amount done outside of grading app . " IMHO . <p> I worked a few years ago with them on some commercial stuff and was really impressed with there skills . Nice young cats with a lot of talent and fun to work with . At the time they had heavy creative cloud use AE , Premiere , magic bullet looks , plugins etc in fcp7 - these guys had skills editing , doing vfx , color , you name it , etc . Not sure if they still have a heavy hand in all that or if an initial grade may have been done by them or a colorist in a dedicated grading app but yeah given all the vfx clearly a decent amount done outside of grading app . " IMHO. 43971 @qwx453971 <p> Thank you Colin ! <p> As you can see , they put a lot of hours in that video . The company directed , edited and graded the video according to their site @ @ @ @ @ @ @ @ @ @ example After Effects for the fast blur and Magic Bullet maybe for the chromatic aberration . <p> Not sure about this one but Bryan Smaller out of CO3 LA 's done a few of The Weeknd 's vids , he has a contact page on his site so could be worth asking him directly . 43971 @qwx453971 <p> Thanks Sam ! <p> CO3 colored a couple of clips for The Weeknd . Bryan worked on ' Hills ' , ' Ca n't feel my face ' and ' Starboy ' from what I know . The grade for ' M A N I A ' was done by Moving Picture LA and the grading for this one , ' Party Monster ' , was done by BRTHR as well as the directing and editing . <p> The DoP , Erik Henriksson , is actually a friend of mine and I just spoke to him . They shot it on Red , FS7 and Hi8 . The guys at BRTHR did the grading in fcp 7 Lots of effects in camera , lots of effects in post ! 
@@44332460 @4332460/ <h> House of Cards <p> Had a chance to watch the first few episodes of David Fincher and Kevin Spacey 's new series House of Cards on Netflix tonight . This is probably one of the most beautifully shot tv series I have seen in quite a while , and the grade , while subtle and not super over-the-top , is really nice . From what I can tell it was graded at A52 , Angus Wall 's VFX studio in Santa Monica by Paul Yacono ... <p> I 'm really enjoying the episodes . Maybe it 's my TV and the projector upstairs but to me there is a slight yellowish cast to the grade . I really like the overall look . Regardless I think I 'm going to be staying up late watching more episodes then I should tonight . <p> Yes , noticed it too . Beautiful job , although it certainly looks as if colorist , dp , lighting and production design all got together for a chat before shooting commenced . Still a very tasteful grade all the way through . I 'm @ @ @ @ @ @ @ @ @ @ . <p> oh indeed ! ... so well done ! acting , camera work .... and even grading .... i actually was surprised how close the RED footage came to be like ALEXA awesome ( just half kidding its more than just a camera thing ) 
@@44332462 @4332462/ <h> Apply Correction to a Combination of Qualfiers ? <p> What I would like to do is use a combination of qualifiers to create a key . And then apply a single grade to the combination of all of these qualifiers . <p> Here 's an example just to help explain what I 'm getting at : Let 's say I have an unevenly lit reddish-yellow wall that I want to change to teal . But I have people of various skin tones standing in front of the wall . Even with the 3D qualifier ( which allows for non contiguous selections ) I ca n't get a good key for the entire wall using just one qualifier . But I could get the entire wall by using a combination of multiple qualifiers . Maybe two HSL and RGB and a 3D qualifier all combined would give me a perfect key of just the wall . <p> The problem I 'm having is that I do n't see a way to use multiple qualifiers in a single node . I can chain together serial nodes , but each node @ @ @ @ @ @ @ @ @ @ reddish-yellow to teal . And that means every time I want to tweak the teal , I have to change four different serial nodes in exactly the same manner . <p> I believe maybe the answer lays with using a layer mixer node and passing the key from each node to the node below it . But I ca n't seem to get that to work exactly right . I 've tried inverting the key input , output and qualifier and selecting mask , but I just have n't been able to get it to work exactly right . <p> Here 's an example just to help explain what I 'm getting at : Let 's say I have an unevenly lit reddish-yellow wall that I want to change to teal . But I have people of various skin tones standing in front of the wall . Even with the 3D qualifier ( which allows for non contiguous selections ) I ca n't get a good key for the entire wall using just one qualifier . But I could get the entire wall by using a combination of multiple qualifiers @ @ @ @ @ @ @ @ @ @ all combined would give me a perfect key of just the wall . 43971 @qwx453971 <p> I think you 're going to need a combination of keys ( with the key mixer ) and masks &amp; windows in order to pull this off . <p> Note that when you push a correction too hard -- particularly if you take a wall and force it to be the opposite of what it is in real life -- there 's always the danger of artifacts . <p> At some point you may need to just say " fuck it " . It may become a VFX shot and it may call for use a more appropriate tools- Fusion , AE , Nuke etc . I ca n't imagine a situation , where my clients would sit for more than 10 minutes , waiting for me to pull a decent key ... <p> At some point you may need to just say " fuck it " . It may become a VFX shot and it may call for use a more appropriate tools- Fusion , AE , Nuke etc . I ca n't @ @ @ @ @ @ @ @ @ @ more than 10 minutes , waiting for me to pull a decent key ... 43971 @qwx453971 <p> I like the way this guy thinks . Very well said . <p> I 'm known for sometimes turning to clients and saying , " there 's a point where a color-correction session kind of blurs into visual effects , and beyond a certain point we need to hire a VFX artist to do this right . " Repainting a wall might be one of those times . <p> My strategy would often be to try to find a reasonable middle ground for the client 's expectations and say , " I ca n't give you that , but I can give you this and it wo n't cost any more , " and we split the difference . But spinning the color of a wall 180- ... that 's a recipe for trouble . <p> Agree with Jake and Marc . Also second Brandon that key mixer will do what you 've asked . <p> Sometimes with these sorts of thing its preferable to work upside down though . That is it @ @ @ @ @ @ @ @ @ @ n't want to effect . And apply those in as many nodes as you want on a layer mixer . And make the background in the layer mixer the modification you want to apply to the wall . <p> That is , start with two nodes feeding a layer mix , make the top node say a shape for the parts outside the wall , then add as many layers of keys and masks as you want to get everything that crosses in front of the wall . At the end all that 's left in the first layer is the wall . <p> Thanks for all of your helps ! ! The layer mixer is indeed the tool designed for this . I also was able to get the layer mixer to work ( it 's the default inverting of the key that 's been throwing me ) . But the layer mixer is cleaner to use . And I 'll try Juan 's upside down approach too . <p> Well , it looks like there is an even simpler option that can work in some instances . Essentially @ @ @ @ @ @ @ @ @ @ Key Key Inputs . Alternatively , you can just string together serial nodes , connect their keys and again do the un-uverting and un-matting . ( The advantage of using outside nodes is that the node keys are already connected for you , so it saves a step . ) <p> After doing that , just apply your grade to the last node in the chain . 
@@44332463 @4332463/ <h> Organization of layers and comparison mode in Nucoda <p> I am using Nucoda Filmmaster 2013.2 for some film restoration projects . It seems me that the organization of the layer is very functional for a color correction workflow ( in the input layer you would have a first general correction and in the master layer you would have a general final correction ) . <p> But , how does it work for film restoration ? What am I supposed to do in the input layer and in the master ? What I usually do is : Base ( I do n't do anything , only a general Pan Scan , if necessary ) DVO Steady ( for stabilisation ) Pan Scan ( for manual stabilisation ) D-Flicker ( if necessary ) DVO Dust+Fix DVO Scratch DVO Fix ( a couple of those layers , eventually ) Master ( usually I do n't do anything here either ) <p> Do you think it is a practical system ? <p> But , there is a problem . I need to check always what the DVO Dust and the DVO Scratch @ @ @ @ @ @ @ @ @ @ Red ' ' comparison mode . To avoid artifacts , of course . <p> But how does it work ? I can play the Source , or the Input layer in Comparison mode with the Output . But not my current DVO Dust layer . And because I made a stabilisation before the DVo Dust , all the frame is changed , and I can not spot what the DvO Dust only has removed . I saw ther eis one option inside the DVO Dust tool , there is a view mode " Durt on Red output " . It underlines me what the DVO Dust has removed , but when I switch it off , it has to render again ! And anyways , how can I check what the Sracth removal is doing ? <p> I hope it is all clear , have a nice day and thanks in advance for your help ! <p> Partly what I 'm doing . You are right that ' input fx ' is processed first and always is for the whole image . I put there : steady , clarity , @ @ @ @ @ @ @ @ @ @ are right that there is no option to compare just ' this effect ' to the source , uncheck ' enable ' for the ones you do n't wan na see ... You can put the dvo fx on event layers to of you need to mask them or prefer a layer bypass . <p> I 'm not involved with Restoration work , but I know several people who use Phoenix ( very similar to Nucoda ) , and they do all of their image processing ( clarity , denoise , deflicker , sharpen , etc ) in the Input layer . Also , most people I know use the Master layer for pan and scan , which is often looked at as a final step before delivery . It may be different in Restoration though . <p> However , another idea to consider would be adding each DVO as it 's own Effects layer , that way ( like Daniel mentioned ) you would also have control over where the DVOs effect your image with shapes and keys . That could be really useful for things like de-noising shadows @ @ @ @ @ @ @ @ @ @ in it 's own layer , comparing before/ after might be easier , and rendering would go faster . <p> Also , I think Nucoda/ Phoenix both have an option where you can turn off background rendering , but still render whatever frame is under the playhead . See if that helps avoid complete re-render of the layer when you use bypass . <p> Hallo , thank you for your answer ! Yes , defenetly I am now trying to put DVO Steady , Pan Scan ( for manual stab ) , in the Input layer . This works good for me , so I can compare the later automatic Dust Bust with the Base layer . Thank you guys ! 
@@44332464 @4332464/ <h> The Colour of Pomegranates Analysis ? <p> For my money it 's one of the most visually beautiful films ever made and Paradjanov 's use of colour is remarkable . <p> I was wondering if anyone had any comments about achieving this look with digital/video ? <p> Obviously a lot comes down to art direction but there still seems to be something special about the cinematographic process . ( Similar perhaps in its specificity to the beautiful Technicolor look achieved by Jack Cardiff et al ) . <p> Grading wise there is n't a whole lot going on here , Its more just the natural texture and effect from the film NEG-Print process . You can find heaps of online tutorials about this and film effects . What makes this film very lovely and almost contemporary in its beauty is the " flat " image ( no lens vignetting ) and amazing composition of the shots . That 's the hard part . <p> Grading wise there is n't a whole lot going on here , Its more just the natural texture and effect from the film NEG-Print process @ @ @ @ @ @ @ @ @ @ and film effects . What makes this film very lovely and almost contemporary in its beauty is the " flat " image ( no lens vignetting ) and amazing composition of the shots . That 's the hard part . <p> Just my 2C 43971 @qwx453971 <p> Thanks for your two cents David . <p> I suppose it 's the film stock and art direction/composition that really make this look as you suggest . His later film which is similar in some ways The Legend of Suami Fortress does n't look anywhere near as good and that was made much later ( progress ? Apparently not ) . <p> The reds are particularly strong , contrasted with those creamy , tan plaster walls . The colour reminds me a bit of those very early colour photographs by Prokudin-Gorsky ( same part of the world too , amazing textile colours . ) <p> Surprised to find little mention of films like this by colourists , seems many are dazzled by the bright , teal and orange lights of Hollywood . Shame as I think there is a lot to learn from cinema ( i.e. not just commercial movies. ) 
@@44332465 @4332465/ <h> Pros and Cons of Quantel Rio and Davinci Resolve ... <p> So I do n't want to start a pointless discussion of which platform is " better " , but I 'd love to hear some other people 's calm balanced opinions about the pros and cons of each platform from folks who have spent considerable time using both platforms . Strengths of each ? Weaknesses of each ? <p> My background is deep with Davinci Resolve and Assimilate Scratch as well as Nuke and After Effects , and a little bit of Smoke/Flame . <p> All I can tell you is my personal experience . I think I spent equal amount of years on both now , although I stopped using Rio a few years ago . We switched from Rio to Resolve at that time because we were able get better real time processing . Things probably got better now . <p> Having said that , I found Resolve has better tracking , I personally got better results on skin tones in Resolve than with Rio , overall I felt that Resolve in terms of color @ @ @ @ @ @ @ @ @ @ a bit complex and you have to have some good training . So there was a lot of self discovery and learning by error . <p> But , I also think that Rio has a genius user interface , that I 'm surprised nobody tried to emulate , the whole system is pen based , you can use the mouse , but if you get used to the Wacom you literally fly . And that I miss greatly . Editing , conforming , even doing some basic DVE was pure joy , it was really fun and fast , and I would go back to Rio just because of that . <p> Pablo Rio has some interesting design features that seem good for VFX as well . <p> I think as time goes on , Baselight , Lustre , Mistika , Nucoda , Pablo , and Resolve all wind up sharing more similarities than differences . I would say Mistika has some very unusual feature sets not found in the other programs , but the cost advantage of Resolve is hard to beat . If I had the choice of @ @ @ @ @ @ @ @ @ @ Linux systems , it would make more business sense to do the latter . Cost no object , I think Baselight might be the best of all of them in many ways . <p> I 've used both Resolve and Pablo ( though not Rio much ) a lot so I 'll chime in ... <p> Obviously the entry cost of Resolve is impossible to beat . If you compare top-end versions though I think they 're not so far apart on cost , I do n't know - and they 're both excellent . I do much prefer the Neo panel over the BMD panel - I 've only used the BMD panel once but found I kept having to reach over for the mouse or pull out the keyboard . On the Neo I could work with the pen in my hand the whole time and one hand over the keyboard . It 's really fast . <p> Where I prefer Pablo/Rio is nearly everything that is n't colour , and one or two things which are . But on the whole I 've found that copying/rippling grades @ @ @ @ @ @ @ @ @ @ Rio . The Group tools in Rio are really good , possibly better than Resolve ( I really should have used them more when I had the chance ! ) . The thing is that you need to have defined things as groups before you benefit from them , whereas I like the ' Ripple to Selected ' feature in Resolve for example . Also missing from Rio is Match Bin ( or Reveal in Media Pool , whatever you 'd call it ) and you need to workaround for Versions which is a feature Rio lost a while ago . <p> Pluses for Rio are the interface and how you can easily access animation curves , compare and cut between many different edits , the three axises of 2.5D DVE , text , graphics , and generally moving around the timeline . Also good is that you can disable any individual menu or parameter in a cascade ( node ) to just see what it 's doing while keeping the rest of the cascade turned on . I find myself doing much more in a single cascade because of @ @ @ @ @ @ @ @ @ @ timesaver . <p> Agree on the Neo panel opinion - I 've said that before on this forum , but I think it 's the greatest , most thought through and most versatile panel on the market - and it is not even the most expensive one ( being on the " cheaper " end of the high end panels , actually ) . <p> I use to love quantel , but in the last 5-6 years did not get the love it should have . instead to waste money in racing cars , it would be much wise to add 10-15 good programmers to the pool in order to catch up with the competition . <p> goods : the editing side is still faster than resolve expecially with multiple timelines , but too many thing require hands massage . and it still bloody crash sometimes just stepping between shots . <p> bad : the color corrector has a 2006 feature set . a whole decade behind . the panel is cheap build , not flexible at all , mechanically not very good , baselight blackboard is 1000 times better @ @ @ @ @ @ @ @ @ @ color , we stopped use it for color . resolve is better . <p> a side some special case , we actually swapped almost all the SAM gear for BM ones ... <p> bad : the color corrector has a 2006 feature set . a whole decade behind . the panel is cheap build , not flexible at all , mechanically not very good , baselight blackboard is 1000 times better . 43971 @qwx453971 <p> I said it 5 or 6 years ago : the Baselight Blackboard 2 is the best color-correction panel I 've ever seen , hands-down . The OLED buttons blew my little mind : <p> For many working colorists , the control surface kind of IS the color-corrector , and it has a lot to do with dictating personal preference . On the other hand , as Gray Marshall reminded me the other day , there are people who do world-class work every day on a Lustre with just a mouse and keyboard ( and maybe a Wacom surface ) . <p> My understanding is they are not OLED buttons . They are individually projected images @ @ @ @ @ @ @ @ @ @ so many OLED buttons on the board would have been prohibitively expensive . <p> My understanding is they are not OLED buttons . They are individually projected images from the bottom of the BB2 unit . To have so many OLED buttons on the board would have been prohibitively expensive . 43971 @qwx453971 <p> There are some OLED keyboards out there , but I seem to recall they 're about $1500 for a normal 101-key keyboard . I have n't used a Blackboard 2 , so I ca n't say except that I like the idea of every key being mappable and visually changing in context , which is very cool . <p> Fences , Revenant , Rogue One , Bridge of Spies , BFG , Birdman , Skyfall were all graded with a mouse and keyboard . 43971 @qwx453971 <p> And those are just the ones done on Baselight . I seem to recall there 's " a few " Marvel films and other fairly big blockbusters done on Lustre with just a mouse and keyboard . <p> The problem for those not working for a facility with vast @ @ @ @ @ @ @ @ @ @ , I do n't think working without a control surface is productive . I know the guys at eFilm pioneered using their own Colorfront/Lustre system with just a mouse and keyboard , but from what I observed , it 's painfully slow in the real world . I think Baselight and Lustre are faster with control surfaces , and a lot boils down merely to individual colorist philosophy -- not so much results . It 's more about speed and practicality . <p> If it were painfully slow , the people doing the films we 're talking about would not be employed . 43971 @qwx453971 <p> Let me know when network shows with a 2-day turnaround are being done in Lustre . Or indie films that have to be finished in 5 days . It 's relatively easy to use Lustre for finishing when you have a Flame guy , an assistant , a staff engineer , a conform guy , a roto guy , and a second colorist working around the clock , but even that takes a month . I 'm sure you would agree that it 'd @ @ @ @ @ @ @ @ @ @ project like this in your own Lustre workstation under your roof completley by yourself in a reasonable amount of time -- say , 50-60 hours . <p> I think Lustre is a wonderful system capable of fantastic work , and I 've said so many times . But it 's not practical for a lot of different kinds of work out there , particularly for mission-critical projects for which time and money are huge factors . <p> Let me know when network shows with a 2-day turnaround are being done in Lustre . Or indie films that have to be finished in 5 days . It 's relatively easy to use Lustre for finishing when you have a Flame guy , an assistant , a staff engineer , a conform guy , a roto guy , and a second colorist working around the clock , but even that takes a month . I 'm sure you would agree that it 'd be hard for you , Jason , to do a project like this in your own Lustre workstation under your roof completley by yourself in a reasonable amount of @ @ @ @ @ @ @ @ @ @ Lustre is a wonderful system capable of fantastic work , and I 've said so many times . But it 's not practical for a lot of different kinds of work out there , particularly for mission-critical projects for which time and money are huge factors . 43971 @qwx453971 <p> Just for the record , with everything being equal , I certainly prefer a panel . Said that , for very long time I used Nucoda with just a Wacom , keyboard and a ShuttlePro . After a while it 's not a big deal to work without a panel . The difference , some software is designed to be used with a pen and some is n't . There is only one platform , that I would never use without a panel ... <p> I 'm perfectly aware of what you can do with mouse and keyboard , I 'm a fan of printer lights myself , it all depend on two things : <p> how much time you have to do a feature how many minions do all the extra lifting for you . <p> if you 're @ @ @ @ @ @ @ @ @ @ feature , balance a shot with ball and ring offset will bring you in the ballpark way way faster than printer lights with exactly the same quality plus the added split the difference button . <p> Let me know when network shows with a 2-day turnaround are being done in Lustre . 43971 @qwx453971 <p> I would , but I do n't have time to list every television show done at Universal . Or Mad Men , Good Wife , Criminal Minds , Secrets and Lies , or Mozart In the Jungle ( all finished at Technicolor ) . Or Agents of Shield , Greys Anatomy , Shades of Blue , and Designated Survivor ( all finished at Arsenal ) . <p> I honestly do n't know where you got the idea that Lustre is only used on features , or that it is " too slow " for broadcast shows , especially when you know better . 
@@44332466 @4332466/ <h> Audio out of sync between reference monitor and client monitor <p> I have a decklink studio 4k connected to fsi cm250 ( reference monitor ) by SDI and HDMI output to oled tv ( client monitor ) and I have experience audio out of sync from my playback and client monitor . Audio is from windows output not decklink <p> So , what it you workflow for audio/video sync with blackmagic output cards ? <p> I was exploring the idea to output audio from CM250 to Mic plug in my PC in order to output audio only from blackmagic decklink studio instead windows component . That 's make sense ? , is there another workaround ? <p> In my experience consumer OLED TVs have a fairly high input lag . To get around this in our facility we have a way to switch between audio that is delayed for client viewing and audio that syncs with our grading monitor for when we are working . Obviously not the optimal solution , but that is the trade off when using consumer products in a professional environment . Also one @ @ @ @ @ @ @ @ @ @ is greater when accepting a 23.98 or 24 signal . Doing a cross convert to 30i will help lessen lag . <p> The mail culprit tends to be 4K TV 's . What model and mode are you using ? Our Sony TFT-screen as roughly 40 ms in its game-mode which in PAL-country is 1 frame . I can notice that , most clients ca n't . More modern displays are in the 20-30 ms range . <p> If you 're running an older OLED with all bells and whistles on I think they are in the 80-90 ms range which definitely will be noticeable . One solution is to route the sound from the TV , but then sound will be out of sync with the GUI and REF-monitor . <p> Some apps like Premiere offer an offset option for a/v output . <p> Game-modes tend to bring far lower input lag but might offer fewer settings . Make sure you turn off as much processing by the TV as possible . That 's for sure . <p> I think the confusion comes from mixing 3 different things here @ @ @ @ @ @ @ @ @ @ resolve . There is only one option and that is use blackmagic hardware to pull the audio from the same pathway as your video . ( SDI/hdmi direct/loopthrough/good signal splitters/etc ) Using a separate audio card ( even top end does not matter ) from the video stream does not work . It can technically , but resolve just ca n't at the moment Been there tried it , does not work . No problem for other programs , even with same BM hardware in combi with separate audio stream , but resolve does not . Problem gets ever worse with multiple audio channels if not using same path . 2 . Once you have a synced video and audio stream , if you tap from there 10 monitors and 10 sound systems , these individual monitors and audio systems could be but will not necesarily be in sync with each other , but each individualy can have perfectly in sync audio and video . They can be out of sync with each other due to delays in their indivdual signal paths , processing power/technology/etc. 3 . Each individual combination @ @ @ @ @ @ @ @ @ @ with each other if the audio path has a different delay than the video path . And or follows decoding routines with have different latencies . An example of that you see often with consumer TV 's playing video with a high bandwith DTS or better audio channel , streaming from a source where the audio runs via a dedicated Cinema audio amp . The DTS decoding then often takes a little bit longer causing sync issues . Or the reverse for high bitrate video material like full 3D stereo left and right images . Good AV decoders can compensate with delays / buffering but is more tricky to deal with . <p> For tackling 3 , the least chances of this happening are if you let the same device that does the video decoding do the audio decoding and then if needed loop the decoded audio directly to an amp. ( like Pepijns example where he pulls audio from the analog audio output of his grading mon ) If encoded audio follows a completely different path then the video , you may have to look for audio delay/buffer devices @ @ @ @ @ @ @ @ @ @ sync scenarios . Some high end audio stuff like from RME can work in standalone mode and then you have an onboard digital mixer that can delay the audio . Also as mentiod somewhat before , switch of any " smart " video processing stuf on consumer tv 's , which can throw of sync as well . Lastly , i have no experiences with LUT boxes , but these could theoreticaly also introduce processing delays in the signal path . <p> For tackling 2 , not sure what scenario that is important as if you have multiple av systems running at the same time , they should be outside each others audio range otherwise you go crazy anyway ; - ) But if one is in a client room and the other in another room , who cares if they are in sync between them as long as the audio and video is in sync on each individual set . If it does matter and they are in close proximity , an idea could be to use one separate source that does the video and audio decoding and then @ @ @ @ @ @ @ @ @ @ target . So 1 decoder and multiple playback . 
@@44332468 @4332468/ <h> Current State of Restoration Software <p> for those of you going to NAB , could I ask a favor ? If you happen to be looking at the various resto packages , could you please post comments of interest to those of us still looking ? hs-art Diamant is going to be there . Apparently PFclean is not . Phoenix is there , and MTI . This year , I can not go . So any comments thoughts recommendations , etc . If they will quote prices , that would be great . I think prices may be a moving target , but who knows . <p> for those of you going to NAB , could I ask a favor ? If you happen to be looking at the various resto packages , could you please post comments of interest to those of us still looking ? hs-art Diamant is going to be there . Apparently PFclean is not . Phoenix is there , and MTI . This year , I can not go . So any comments thoughts recommendations , etc . If they will quote prices @ @ @ @ @ @ @ @ @ @ be a moving target , but who knows . 43971 @qwx453971 <p> Grace <p> I will be in LA after NAB , happy to give you a tour nd a catch up on Phoenix if you'r like . <p> Back from NAB . While there I got demos of the current versions of MTI DRS Nova , Phoenix Touch and Diamant . <p> MTI has made some progress on a few things : They have a nice tool for dealing with lab mis-timings , where you 'd get 1/4 or so of the frame with the previous shot 's grade . We see this a fair bit on older films and it can be a pain to repair , so it 's nice to have a dedicated tool for that . They also showed a tech demo of a new automatic dust tool that 's really semi-automatic . You define several points of dust and then it looks for matches , you can then add or remove false positives manually before running the pass . This is a nice way to clean up a lot of stuff at one time @ @ @ @ @ @ @ @ @ @ means you 're QCing as you work , which is something I think is really important to save time . So I like this tool , but I think that automatic cleanup algorithms are getting better and soon we 'll be at a point where they can be trusted to do the right thing without having to do an extensive QC pass to look for false positives . MTI remains a largely manual toolset , and it does a fantastic job of that , but it 's also a slower workflow , and we now have more clients who want us to do less expensive fully-automatic cleanup , so we 're keeping an eye on this for now . <p> Phoenix Touch looks really nice . It 's a bit expensive compared to what you get from PFClean and MTI in terms of overall toolset , but it 's fast and it does a nice job . Like MTI they showed a demo of a soon-to-be-released automatic dirt fixing tool that was really impressive . This is more of a fully automatic tool than the MTI , and it was @ @ @ @ @ @ @ @ @ @ and clean them seamlessly . We 're very seriously considering adding a Phoenix system to our lineup this summer . <p> Diamant is promising , but I did n't get a very long demo on it . I kind of feel like if we 're going to spend that kind of money , it 's probably going to be on Phoenix , because it is kind of a funky user interface . It looks a lot more streamlined than it was when I last used it , several years ago , and I think it 's a contender , but I 'm not sure it 's the tool for us . <p> Regarding PFClean : They were n't at the show , but I did get a message from them that they 're closing down their Facebook page since they never use it , and that they 'll soon be making new announcements . We bought a system last summer , but it has been a complete disaster with constant crashing , project file corruption , weeks of lost time ( seriously - we 've lost weeks of meticulous restoration @ @ @ @ @ @ @ @ @ @ really get their act together , we 're done spending any money on PFClean . The 2016 upgrade had better be something really special , and they 'd better offer us a good deal on it , because it 's been a complete mess from the moment we got the system up and running . <p> Thanks for that report , Perry ! I have to say , 1-perf optical mislights have vexed me for 30 years , and since we started having power windows about 20 years ago , it was very time-consuming to have to pop one of those out and figure out how to compensate for those flash-frames . I 'm glad to see that MTI Film has stayed on top of this with a solution . <p> Thanks for that report , Perry ! I have to say , 1-perf optical mislights have vexed me for 30 years , and since we started having power windows about 20 years ago , it was very time-consuming to have to pop one of those out and figure out how to compensate for those flash-frames . I 'm glad @ @ @ @ @ @ @ @ @ @ this with a solution . 43971 @qwx453971 <p> There are ways to deal with that in PFClean , and in some instances , in the previous versions of MTI , using the paint or deflicker tools . What 's nice about this is that you basically just draw a box around the defect and it 's gone . poof ! That 's the kind of tool I like . When you 're spending more than 20-30 seconds on cleanups for a single frame , you 're losing money ... <p> So far three emails , each featuring a new video about the next version . Perhaps I am getting these because I have not yet purchased . Here is a link to the LONG ... I am very interested in your thoughts , Perry , as you 're experience with PFClean has me very wary . <p> Hmm . Well , from what I see in those videos , it looks like they 've made some major UI changes . Some look like they 're for the better . others look like they 're just trying to be fancy with @ @ @ @ @ @ @ @ @ @ they seem to be hung up on . None of it looks particularly new other than the UI changes . <p> One of the big problems with PFClean is that it tries to be a tool that does everything from restoration to grading to mastering . We 've never touched the grading tools since we have a separate system for that . We do n't use it for outputting final masters because that 's largely broken . That is , if you want to Export something to multiple formats , you can do it , but it 's likely to either fail ( Quicktime ) or do something wrong . At least , on Windows that has been the case , but PF folks have told me that it should n't make a difference what platform you 're on . <p> The UI is built on an overly complex node system , which might be useful for some people , but with restoration we typically work in one of two scenarios : <p> 1 ) We have a set of clips we bring in , clean up , export and @ @ @ @ @ @ @ @ @ @ Resolve <p> 2 ) We have an entire film , which we break up into cuts , then go through linearly from beginning to end . <p> We never have more than one node pipeline , and even that seems like overkill ( to export , you have to first drag your bin to an ' Item List ' node then drag the Item List node to a " File Out " node ) . This is a completely extraneous step . You should be able to simply export the bin with the option to export it as a continuous timeline , or as the separate clips . <p> Basically , they 're sharing the core UI with their other tools , PFTrack and PFDepth , which seem to be where they 're really concentrating their efforts . Maybe the nodes make more sense there - I do n't know because I do n't use those . But within PFClean it 's clunky . <p> Truthfully , all they need to do to make it into a perfectly good restoration tool is make it not crash all the time . @ @ @ @ @ @ @ @ @ @ reported these as bugs , some of them many many months ago . Some bugs were fixed . Some were fixed and rebroken . Some they say they 're not fixing in our version . We paid $10,000 for software with a lot of bugs in it . I realize software evolves , but crashes are n't given very high priority - instead , we get " do n't do that " from the developers , which is just arrogant and obnoxious . <p> My big problem with 2016 , from what I see here , is that we 're going to have to deal with another learning curve on the UI , after it took us months to get into a groove with the current interface . They seem most fixated on the way the application looks , rather than the way it behaves . If you 're going to drop $6000 on software ( the current price ) , I kind of feel like you have a right to expect it to not blow up in your face multiple times a day ... <p> I may be a @ @ @ @ @ @ @ @ @ @ to a year with PFClean and it 's been a complete disaster , so I 'm not optimistic they 're going to get 2016 right . <p> * Does the system still eat up all your memory when you 're using auto dirt fixes on 4k footage ? ( we can max out the 64GB RAM in our machine in a few hours ) * Does the application still crash if you click on the project manager button from inside the edit panel ? ( that 's the only way to get to the project manager , so the workaround is to quit and restart the app every time you want to do this ) * Does the application still crash and corrupt your project if you click on something in the Paint effect while the current fix is processing ? * Does the application understand that 29.97fps ! = Drop Frame Timecode ? * Does the application allow you to change the timecode type from drop to non-drop or vice versa ? * Does the application understand that it should be exporting at 23.976 when pulldown has been removed @ @ @ @ @ @ @ @ @ @ screwing up the project duration ) * Does the application still insert pure green as filler in some fixes ( that was supposed to be fixed , but it 's re-broken in a recent build ) * Does the application allow you to export to an uncompressed Quicktime file on Windows without crashing ? * Does the application let you import a Quicktime file for a feature without crashing and corrupting your project ? * Does the application still make certain clips disappear ( the " Gap " problem ) when it crashes ? ( the workaround is to reimport your source , cut out the clips you need to re-insert , then edit them back in manually . That may be the only thing the node pipeline is good for , though simply relinking the media would be easier ) . * Do you have a manual that does n't require being inside the computer or in a web browser to see ? ( It 's unsearchable ) . * Is the manual still full of UI examples for interfaces that no longer exist ? <p> First of all @ @ @ @ @ @ @ @ @ @ to make . I do n't speak English very well so i hope you 'll do your best to understand me and to forgive me . <p> Perry , you absolutly right about pfclean. i 've been working on film restoration for more than 17 years , i know pretty well and used every restoration softwares except algosoft viva and i 've never seen such a baffling software ! <p> Very bad workflow , lots of crashes ...... all you say is absolutly right ! it 's too bad because their algorythmes are very good ! <p> i ' m a phoenix refine and mti nova big fan . i know them very very well ! they complement one another . the first one is more automatic the other is manual . Refine does what nova do n't and nova does what refine do n't ! <p> But i have to say that i use also pfclean because for some defects it does a very good job and have options that refine and nova do n't have . <p> here 's how i work . i do stabilisation with refine @ @ @ @ @ @ @ @ @ @ nova . the refine stabilisation tool does n't handle with rotation ! ! dewarp with nova ! the refine dewarp tool is 100% automatic and is not efficient at all Then i do the deflicker ( the best ever ! ) auto dust ( best ever ! ) and paint with refine . the refine paint tools are not bad but could be much better for the scratches i use also refine but a lot of options are missing so i also use nova because sometimes you want to show manually what scratch you want to get rid of .... only nova gives you that option ! if refine does n't see the scratch there is no way to show it where the scratch is ! ....... and you 're f ... d ! if the scratch is moving on a few frame from left to right or right to left , that scratch wo n't be 100% vertical and if the scratch is not 100% vertical refine does n't see it at all ..... and you 're f .... d one more time ! <p> for small scratches ( @ @ @ @ @ @ @ @ @ @ use pfclean because you can tell it what kind of scratch you want to get rid of . you can set the width the heigth the contrast etc ..... and pfclean will do the job automatically and you can manually undo the wrong fix on each frame or a range of frames with a paint brush or a rectangle and so on ........ Refine does n't care about small scratches and does n't have any option to undo a wrong fix . If you want to avoid artefacts you need tu use masks ...... time consuming ! ! ! ! ! <p> there 's only one thing that i 'm sure of ........... when these guys from digital vision mti pixel farm etc ... make their softwares they do what they think we need but they never ask us what we really need . The first company that will ask us what kind of options we really need will gain the restoration market <p> i did n't talk about diamant because it never does things to the perfection it 's good for stabilizing but not the best it 's good @ @ @ @ @ @ @ @ @ @ fixing scratches but not the best ......... <p> i did n't talk about revival because it 's dead even if the dewarp and deflick Tools are very good <p> if you have question about phoenix refine do n't hesitate asking me . Do n't be afraid of this software .... it 's not full to the brim with bugs ! <p> As a Nucoda user with full DVO I agree with you how fantastic it is . I do n't really do much restoration but I would love to tell you how much Digital Vision are listening to their customer base at the moment - it 's fantastic ! <p> Drop Patrick Morgan an email or a PM on LGG as he would love to hear what ideas are out there . He even came to visit me today to ask for my thoughts and feedback . DV are really going out of their way to help their customers more than ever ! <p> Thanks for that feedback . My experience is with MTI ( 11+ years ) and PFClean . Phoenix is something I 've had demos of at @ @ @ @ @ @ @ @ @ @ Touch , as a way to do some automated and manual work in a more stable environment . There 's a new auto dirt removal tool for Phoenix coming out , which I saw at NAB , that was most impressive . <p> PFClean does a great job at a lot of fixes . In fact , we bought it because of a specific project that we could n't clean up easily in the MTI ( which we were leasing in 6-month intervals ) . PFClean 's Fix Frame simply made the problem go away in seconds , but in the MTI it took 20 minutes of manual work . And there are hundreds , if not more , of these defects in that film . But it 's so damned unstable that it 's lost us a ton of money . I ca n't tell you how many times it corrupted files or lost days worth of fixes and we had to start over from scratch . All I want is for them to make it work without crashing . As you say , the underlying algorithms are just @ @ @ @ @ @ @ @ @ @ with UI and ignoring the underlying stability issues . It 's really frustrating . <p> The MTI is a fantastic tool for manual work . And that 's their philosophy - that you get the best results with manual work . It was mine too , for many years , until I started seeing what some of the automatic tools like Phoenix and PFClean were capable of . I hope they can catch up , because they 're a great group of people , and I really love how seamless you can make a manual fix in their tools . But they 've always lagged on the automatic side . <p> I 've used a very early version of Algosoft Viva , years ago when it was brand new . It 's a really interesting looking tool and I 'm hoping to get some more time on it at some point , but we 're mostly interested in a toolset like what PFClean has ( at least for restoration ) , but that actually works . I think you 're right that it has to be a combination of tools @ @ @ @ @ @ @ @ @ @ more storage resources . It also makes it harder to revisit a project a few years from now if you need to make adjustments . It means backing up two sets of restoration projects , one for each application you use , and then remembering what was done in which application . It 's a lot easier if it 's all there in one project file ... <p> Please drop me a line if you want to chat or have specific requirements for Phoenix , we love to hear form users . Especially an post like the one you did , thank you very much for the compliments , I 'll take 50% of the workflow for now ; - ) <p> Paint is being worked on , as is Scratch , Dust 2 - it will hopefully change the world ; - ) <p> &gt; Hi , &gt; &gt; My name 's Jean Marc Pourchel and i 've been a Refine user since 2010. i 've been working in film restoration since 1999 . I 've been working for EclairGroup in Paris France and i used refine to restore @ @ @ @ @ @ @ @ @ @ " . &gt; I also know and use MTI correct , PFClean , Diamant , Revival and 2d/3d software like After Effects , Photoshop , Nukex , Fusion , Combustion , Toxik , Media Illusion , Cyborg , Matador &gt; I just wanted to know who i have to contact for new features request . &gt; We are very limited in our restoration process because some dvo effects do n't have the needed options to achieve a full restoration . &gt; &gt; thank you very much in advance &gt; &gt; Best Regards &gt; &gt; Jean Marc Pourchel <p> i sent you some diagrams to explain what Tools i would like to have in refine . you did answer me that 's true ! <p> what i wanted to say is that companies like pixel farm , DV , MTI etc .... have their own philosophy about restoration and it 's sometimes far away from ours ( users ) ! Keep in mind ( and i know you are aware of it ) that restoration tools have to be automatic AND manual . <p> For instance your DVO FIX did n't @ @ @ @ @ @ @ @ @ @ companies use nova to fix dirts . Now your tool have these options but it 's too late because they got into a " mti paint " groove and they wo n't change their workflow even if your autofix algorythme is really brilliant . That 's why i think ( but i may be wrong ) you should first ask users for the options they need before releasing a new tool and always give us the option to do things manually if the automatic way does n't work <p> Send me an email jmpourchel@Hotmail.com if you want to know why i use refine for this nova for that pfclean for this compositing software for that etc .... i 'll answer with pleasure ! And if you come to Paris do n't hesitate asking Bertrand Chevaugeon from SAV the french reseller to arrange a meeting . He knows me well ! 
@@44332469 @4332469/ <h> Premiere " out of memory " Q <p> Asking for a client . Not sure about all the details but an 85 minute film with many different HD codecs and some Red 4K . They are trying to media manage the project before delivering to me for finishing in Resolve . They have 64gb of RAM and keep crashing during a media manage with an " out of memory " message . I can get more info but just trying to get a general idea about what the issue might be . <p> You might want to check if they are on the CC2017 version some of the late CC 2015 versions had a memory leak if the scopes where on , which is fixed in the 2017 version . By the way I am not necessarily recommending the CC2017 version I have a lot of crashes with 10bit 4:2:2 H.264 material . <p> You might want to check if they are on the CC2017 version some of the late CC 2015 versions had a memory leak if the scopes where on , which is fixed in the @ @ @ @ @ @ @ @ @ @ recommending the CC2017 version I have a lot of crashes with 10bit 4:2:2 H.264 material . 
@@44332471 @4332471/ <h> House of Others - ASC Spotlight award <p> About year ago or bit more i worked as a colourist on film House of Others , director Russudan Glurjidze and cinematographer Gorka G+mez Andreu . <p> After ASC event we got information that Gorka G+mez Andreu has been awarded Spotlight Award , award for Outstanding Achievement in Cinematography by the American Society of Cinematographers . <p> It was very interesting project to work with and i 'm really happy that the film got this kind of award . Love the exceptional work Gorka did on the set and the dynamics we had with Russudan in grading . Always happy to work with professionals . <p> This film has more to it and can not be shared in few still frames but not every day i work on such film so i wanted to share . Las one on similar scale to me was the one that got Academy nomination . Some days are brighter than others . 
@@44332472 @4332472/ <h> Best way of handling foreground and background corrections <p> I 'm relatively new to Resolve and recently did a short project that challenged my simple knowledge of Resolve . While I was able to get a look close to what was needed I 'm wondering if there was a better and more efficient way of doing it . The project was a simple promo for a charity benefit event . I was n't around for the shoot but from the video you can tell that it is a little overcast and raining in some shots . The client of course wanted a brighter and warmer look . The shots were of kids in different places holding blank pieces of poster board . The intention was then to track and insert logos of different charities that were benefiting from the event . The poster boards need to be very white and bright to stand out , I 'd like the peaks in it to be around 940-960 or so . This shot at the pool was tricky . The kid is in the shade , with a dark skin @ @ @ @ @ @ @ @ @ @ it . The background is a little warmer . White balancing the poster to make it a more pure white , throws the background off to being way to yellow . Raising the gain to get the poster nice and bright blows the background out . There is some detail , not much , in the sky but there are other background elements such as the water in the pool and the white fence in the background that does have detail . Since there is n't much to the background anyway , I 'd like to salvage the detail to have some visual interest . I think I managed to get something close using a series of qualifiers and powerwindows for the poster , his skin and the sky . But I 'm curious to know how others with more experience would handle something like this . The uncorrected shot as it was given to me is below . DPX file is available LONG ... <p> Several power windows would do it : grad for the sky ( adding color and reducing the white-ness ) , window for the face @ @ @ @ @ @ @ @ @ @ window on the white board to keep it white , and maybe a vignette over the whole thing . A lot depends on intent and context . <p> Erik and Marc , may I ask when you 'd use Parallel Nodes vs Serial Nodes in a situation like this ? Both would be able to get you to where you need to go , AFAICT . Parallel 's have the advantage ( or disadvantage ) of each node referencing the same image . But I 'm wondering if there is something else that I 'm missing . Thanks ! <p> Several power windows would do it : grad for the sky ( adding color and reducing the white-ness ) , window for the face &amp; shoulders00 to get the kid to pop more , window on the white board to keep it white , and maybe a vignette over the whole thing . A lot depends on intent and context . 43971 @qwx453971 <p> Hi Marc , just wondering why you 'd go the window route over using qualifiers ? Or maybe you 'd do both but just meant " @ @ @ @ @ @ @ @ @ @ criticizing at all , of course , just trying to understand various approaches . Thanks much ! <p> Hi Marc , just wondering why you 'd go the window route over using qualifiers ? Or maybe you 'd do both but just meant " power window " in a more general sense . Not criticizing at all , of course , just trying to understand various approaches . Thanks much ! 43971 @qwx453971 <p> I tend to use power windoes + the 3D keyer on skintones , but there 's a lot of " it depends " in the equation . I would tend to bring down the sky in your example so the sky has a vague hint of color in it ( maybe blue ) , and that could help make the stark white of the empty sign stand out more and draw the eye there . You could also throw a tad more light in the kid 's face and maybe just a whiff of very mild sharpening . <p> I tend to use parallel nodes only in specific situations where I 'm using several qualifiers at @ @ @ @ @ @ @ @ @ @ them interfere with each other . For a lot of stuff , I think serial nodes are just fine , but I tend to lean towards a fixed node structure with either 12 or 18 nodes depending on how complex each shot is . <p> I used parallel nodes so that I had a clean RGB feed to base my qualifiers on supposedly should result in a cleaner key . I used a combination of power windows and qualifiers , mainly to limit the correction to a specific area as the qualifier was picking up small parts of the background in some places as well . I did a power window on the poster which was then tracked , as in the actual shot the poster moves from in front of his face down to this position , a parallel node worked better here with the correction because it seemed to blend better and less of a noticeable difference between the background and the poster along the edges of the power window . The serial node at the end was just so I could do some overall adjustments . I @ @ @ @ @ @ @ @ @ @ used a bunch of serial nodes , but did n't like the results . I do n't know if this was the correct or best way to do it . I 'm still learning this stuff . <p> Using noised source or source from consumer cameras or in complex lighting and on nature cloudy scene the complex qualifiers will not work as expected . We may get many artifacts . <p> So I prefer to use simple qualifiers to separate contrast color or level areas only . Or working with hue directly without masks . For example , in your case separate is possible with skin tone level . Every scene requires different analysing how to separate colors . Of course , if you can use qualifiers because the quality , try it , but then see carefully the result in 100% pixel level . <h> Attached Files : <p> Erik , one thing I would say , FWIW , is to maybe also lighten his hands a bit as well . Not as much as the face , but the disparity in lightness between his face and hands can @ @ @ @ @ @ @ @ @ @ amt and his hands are left alone . JMHO . <p> Some trick to grade without Qualifiers . Here is a Luminosity Composite Mode is used to get skin brighter . 43971 @qwx453971 <p> Andrew , what Composite Mode did you use in the Layer Mixer ? <p> Also , I tried your RGB Mixer values , but my lowest value is -2 , not 0 , so I 'm a little confused as to how I should match your values for when the color bar has no height . For example , is Red Output 0.18 , 0.0 , 0.0 or is it 0.18 , -2.00 , -2.00 ? Thanks ! <p> Peter , it is a Luminosity Composite Mode . This means all color changes in the bottom mode will affect luma only . See checkboxes to switch to BW mode for RGB mixer . <p> PS . This trick fixes Resolve hue vs luma curve limitation that gets the curve complete useless to use . BMD uses a wrong color science model for it so it just brings any compression artifact not a brightness . You can @ @ @ @ @ @ @ @ @ @ curves with exceptional quality and precise . I prefer now to use Fusion to make grades there ( and result LUT ) and then Resolve for basic color correction and fine tune . <p> I posted a feature request to the support to include Fusion curves to Resolve next version . <p> Erik , one thing I would say , FWIW , is to maybe also lighten his hands a bit as well . Not as much as the face , but the disparity in lightness between his face and hands can stand out if his face is brightened by a considerable amt and his hands are left alone . JMHO. 43971 @qwx453971 <p> Thanks . Good point . I realized after that I had done a qualifier on the skin tones but then limited it to the face with a power window forgetting about the hands , which I could have also included with power windows . I left it rather than risking screwing something up since I was already being nagged about delivering the video . I 'll need to be more careful next time . <p> Peter @ @ @ @ @ @ @ @ @ @ all color changes in the bottom mode will affect luma only . See checkboxes to switch to BW mode for RGB mixer . <p> PS . This trick fixes Resolve hue vs luma curve limitation that gets the curve complete useless to use . BMD uses a wrong color science model for it so it just brings any compression artifact not a brightness . You can also create custom LUT in BMD Fusion that has hue curves with exceptional quality and precise . I prefer now to use Fusion to make grades there ( and result LUT ) and then Resolve for basic color correction and fine tune . <p> I posted a feature request to the support to include Fusion curves to Resolve next version . 43971 @qwx453971 <p> BTW , can I ask why you choose to do the hue shifting in the first serial node ( 01 Pre ) instead of in the top parallel node ( 02 Skin for Lum ) ? Thanks so much . <p> BTW , I would put any kind of extreme foreground/background correction in the category of " advanced " Resolve @ @ @ @ @ @ @ @ @ @ towards MixingLight.com , where they 've done a lot of insight tutorials on this specific area . Note that this is a pay site and I 'm just an interested observer , not an employee . <p> I think there are a lot of ways to increase separation between foreground and background , and I think this pretty much cuts to the core of commercial-style correction , where they really want to draw the viewer 's eye to the car in the spot or the box of detergent or the model 's face or whatever . But every situation is different and it 's hard to come up with a method that works for every scene or every shot . I try a bunch of stuff and go with whatever works in the time available . I often warn clients that the harder we push the foreground/background split , the more potential for the seams to show . <p> BTW , I would also point towards Steve Scott 's excellent lecture on the Lustre techniques he used in last year 's The Revenant , where quite a few foreground details @ @ @ @ @ @ @ @ @ @ in order to color-time them separately . You can take this to a very fine art , particularly when there could be over a dozen masks and/or windows going on ( or using external mattes for even finer control ) . Anything is possible given enough time or money , but usually in my world there are limits to both . <p> Peter , 01 Pre contains no hue shifting . It is for contrast and color temperature correction only . Hue shift is on the bottom node . And a final fine selective hue tune correction is in the last node . The top node ( 02 Skin for Lum ) contains no any correction . I can complete delete it . It is used here just to create a parallel node structure using one keyboard command ( Alt-P ) . <p> My logic is to divide all processing to three separate steps : Pre correction to get realistic look ( color temperature , MD , contrast curve ) G Luma correction ( used to get skin brighter ) G Final fine tune correction ( orange shirt , some @ @ @ @ @ @ @ @ @ @ teal color ) . These steps can be edited any time without affect to different steps . Doing many different changes in one node makes me hard to compare before/after changes , because it affects many different changes . So I use many nodes for different tasks . <p> Also this logic is for this case only . Because one actual issue here is a very dark face . There is also a white sky , but I think it is here because the sample has been created incorrectly ( made Rec709 for high dynamic range Log source without highlight compression ) . <p> Different scene may require different methods . The idea is to find a way to harmonize look using scene characteristics . This way allows me to minimize using of qualifiers to get less masking artifacts . Most work can be done without masking and object separation . 
@@44332473 @4332473/ <h> Saving grade from last frame of shot <p> I have a shot of a car moving by , and its cut in 2 parts , when I save the grade from part 1 and apply to part 2 , it is applying the grade from frame 1 instead of the last frame , which is messing up my power windows . <p> I used to be able to do this by saving the grade at the part I wanted and applying it to the next part but that does not seem to be working ? <p> Right click the blank area of the stills gallery , there is an option to select whether to apply grade using source tc or starting frame to align keyframes . I thought it was meant for this situation , but I tried it did n't seem to work as supposed to . <p> Another workaround is to join the contiunous clips to a compound clip and re-apply the grade . <p> I have a shot of a car moving by , and its cut in 2 parts , when I save the @ @ @ @ @ @ @ @ @ @ it is applying the grade from frame 1 instead of the last frame , which is messing up my power windows . <p> I used to be able to do this by saving the grade at the part I wanted and applying it to the next part but that does not seem to be working ? 43971 @qwx453971 <p> Resolve will look at the current keyframe values when saving a still , but I think you are looking for power window positioning which you have moved using tracking . You could join the two shots , finish your trackings and split them . 
@@44332474 @4332474/ <h> Any way of forcing backwards compatibility ? <p> I had a project today I needed to do on 12.5 . I have 2 MacOS system disks , one that 's still on 12.5 , and one on the 14b4 . Plan was to boot the old partition and start working . <p> Problem is I was having issues booting the drive , so I had to do something pretty silly , which was to open the 12.5 version of Resolve , while I was booted on the drive that actually runs 14 . I know , I 'm an idiot . <p> To avoid messing up the database , etc , I created a new database from 12.5 just for that project . <p> All was going fine but I had to reboot the computer around midday , and stupidly opened the wrong version of Resolve ( 14 ) and kept working . <p> Now the strange thing is that both version of Resolve see the same database , and the same single project , but when I open it in 12.5 , I only see the work @ @ @ @ @ @ @ @ @ @ version . If I open it in 14 , it 's all there , but I ca n't send the DRP to client . <p> This is all super simple grades from a flat render , so I 'm hoping I can force the backwards compatibility somehow , and save a few hours redoing 300 shots . <p> What I would n't mind figuring out , though , is how to isolate the databases from the 2 versions ? <p> Currently , when I open either 12.5 or 14 , they both see the databases created in the other version , and default to opening whichever was last open , even if in the other version . That 's why I did n't realise I had changed version , and there 's probably a risk of 14 corrupting the 12.5 database ( even though it apparently has n't happened ) . <p> By the time you figure stuff out you are probably better of spending 3 hours again or use stills as suggested . In general regarding your question about isolation and having dual boot can be a good idea @ @ @ @ @ @ @ @ @ @ dual boot is that when you boot A , you have zero access to B and visa versa . A simple way besides physical separation would be to " unmount " B just after you finished booting A and visa versa . That way you are sure you not accidentally acces and even better , if you think you are in A but by mistake booted in B , you can not unmount B. It wont allow you to unmount the active drive . So one more reminder that you are either in A or in B. Also name the partitions RESOLVE12 and RESOLVE14 as extra reminder . <p> Regarding Resolve DB separation it can become messy very fast if you are not carefull . Connecting to separate DB 's with SQL is easy . With Disk based DB 's can be tricky , but i guess as long as you put the 2 DB 's in completely different location ( not same subdir etc ) and define only 1 path in each version you should be safe as well . Also here name them clear so you have yet @ @ @ @ @ @ @ @ @ @ little mental post-it i did is create a dummy project named RESOLVE12 in the 12 DB and RESOLVE14 in the 14 DB , so when connecting again you see in your face what you connect to . But the moment you start messing up yourselve and mixing connections , even once ( as they remember ) etc its game over and you can get into what you just experienced unfortunately . <p> Yeah I did n't spend long trying to get back the work I lost . Went straight back to the working copy . But boot drive with Resolve 12 wo n't boot so I had no choice but do it this way this time . I 'm just exporting DRPs at regular intervals in case anything gets messed up again , and will definitely have learnt my lesson ... <p> Good to know . Lots of people use DRP exports ( incl myself ) as an extra backup mechanisme next to others as a last safeguard , but not foolproof . I tend to name the DRP including the resolve version with which i created it . Then @ @ @ @ @ @ @ @ @ @ used resolve versions to increase my chances of getting old stuff back when not compatible anymore with newer versions and visa versa. ( both directions do happen ) . When changing the whole shebang , like now when i had to move from old faithfull OSX Yosemity / Resolve 12.5 to Sierra / Resolve 14 , i keep a bootable disk copy of the defunct version ready to go when needed . Its the most complex problem to solve with project backups for any software . How to open this project in a few years in a usefull state ..... But for short term " oooops " moments DRP 's are a nice extra tool in the bag without having to resort to restores etc . <p> Then the only recourse I can think of is to do as Margus suggested above and open the DRP up in 12.5 , export all the stills as DPX files with DRX corrections , then reboot , launch Resolve 14 , and import all those stills in and manually copy over the corrections from the stills . Be sure to retrack all the @ @ @ @ @ @ @ @ @ @ like this that changing software in the middle of a job is not a good idea . I do n't know when I can make the switch to 14 -- I 'm slammed at the moment , probably at least until August . 
@@44332476 @4332476/ <h> Inquiry Regarding The History of Projection Standardization <p> Something 's been bothering me . I know that film projectors now are standardized to a XYZ color space with a P3 gamut . My question is if there was such a standard for celluloid projections . I swore I remember reading that film stock companies would sell studios color formulas for their film stock positives , so the color timers could see what the positive film stock would look like when going through said projector in advance ; I always thought of that as the precursor to LUTs . I do n't know where I found it , but the information stuck with me for some reason . I could be completely wrong , so any correction or confirmation would be highly appreciated . <p> Projectors are n't actually operating in XYZ , the DLP chip ( most common ) are RGB gained . The XYZ portion is the DCP image , and possibly the stream fed to the projector . Once in the projector that gets converted to linear RGB that 's used to drive the chip . @ @ @ @ @ @ @ @ @ @ interesting and complicated . Celluloid is pretty versatile , so it can be anything , and the negative , interpositive and print stock all have an effect on the final image . So it was the specific combinations of stocks that would result in the final look . The " colorist " or color timers usually worked for the lab , and over saw not just the intermediary steps but also the final prints . The whole process was very inconsistent , we forget in fact how many issues could be presented during a traditional film presentation . <p> In terms of " licensing " you might be referring to color processes like " technicolor " which is used to require the production to hire a specialist to advise on art direction etc for their process . Interesting , not to long a go , I saw some dye-transfer prints of a few classic technicolor films , and was taken aback by the drastic changes in color from reel to reel . <p> Yes , there was ( and is ) a SMPTE standard for film projection . The Lucasfilm @ @ @ @ @ @ @ @ @ @ paper that references a dozen or more SMPTE standards and recommended practices for film projection : <p> Basically , the film lamp is specified for 5400- whites at 14 fL ( 16 fL for open gate ) , and there are allegedly minimum standards for steadiness and focus in terms of resolving line pairs . I have rarely seen a film projector that could hit those standards . <p> The process of color-timing film did not use LUTs , but instead used a calibrated monitor and an experienced lab timer who could predict what he saw on the monitor ( typically a Conrac 20 " color display ) and how it would translate to film timing lights . It 's a fascinating process , but my observation was that there was a lot more luck and skill and guesswork than there was science per se . The results could be very inconsistent . The film stock companies sold the labs nothing -- it was up to the lab to figure out their own " secret sauce " in order to get the desired results . Basically , it was a @ @ @ @ @ @ @ @ @ @ , the guys performed miracles and could do amazing things in the lab , but bear in mind they basically only had four knobs : red , green , blue , and density ( all three at once ) . The LAD AIMS density patch at the head and tail of the negative and print generally told them if the print was doing what they wanted it to do . <p> Dominic Case 's 2001 book Film Technology in Post Production was the most thorough examination of how film color timing worked , and there was a chapter or two vaguely relating the process to digital timing : <p> Thank you for answering my inquiry and filling in the blanks for me . I also just talked with a rep from Technicolor and realized that I made a huge mistake ; I confused analogue to digital film transfers of the ' 90s with some weird thought that film stock copies were selling special color-code LUTs to production houses in the ' 70s and ' 80s . That literally makes no sense , but everything makes perfect sense to me now @ @ @ @ @ @ @ @ @ @ ' 90s were specializing in scanning film prints thousands of times to the point of emulating their deepest characteristics and then creating LUTs out of them so once the analogue negative was scanned to digital to be shown on digital projectors , the LUT could be dropped on and the digital copy would look extremely similar , if not to the same , to its analogue print counterpart . They were also digitally scanned for VFX work . A few companies were creating these LUTs and selling them including Technicolor and Light Iron . <p> My complete , 100-percent bad . I needed to get my facts straight . I was asking my teacher this , and he looked super confused ; the idea sounded reasonable in my head , and I had n't read the article in three years . I still do n't know where it is . Scary stuff . Anyway , thank you so much for taking the time to write back . If you have any corrections for the paragraph above , always feel free to correct me because life is all about learning , @ @ @ @ @ @ @ @ @ @ understand , color houses in the ' 90s were specializing in scanning film prints thousands of times to the point of emulating their deepest characteristics and then creating LUTs out of them so once the analogue negative was scanned to digital to be shown on digital projectors , the LUT could be dropped on and the digital copy would look extremely similar , if not to the same , to its analogue print counterpart . They were also digitally scanned for VFX work . A few companies were creating these LUTs and selling them including Technicolor and Light Iron . 43971 @qwx453971 <p> Naw , that 's not how it works . Juan said a couple of years ago , " the colorist is the LUT , " and that 's basically the thing . <p> About all that Deluxe and Fotokem and Technicolor did was to quantify their scanning processes so that things were more predictable . We had similar systems in place at Cinesite and Lowry Digital , but the reality is I 'd sometimes have to yell at the Scanning &amp; Recording guys and say , " @ @ @ @ @ @ @ @ @ @ ! " They did not like the idea that essentially every film scan has to be optimized based on that particular piece of film : a 1960 film negative is not the same as a 1970 negative or a 1980 negative or a 1990 negative . Kodak was n't the same as Fuji , Fuji was n't the same as Agfa , and on and on and on . They had to make a little tweak at the beginning to optimize the levels so it was n't completely lopsided . <p> Once I had a balanced film scan as a Cineon ( or DPX ) Log file , then I could get to work figuring out how to normalize it for Rec709 . While I can say there was n't necessarily a LUT involved , we did use colorspace conversion in some cases , particularly in cases where we went back to film . But for just home video , it 's just figuring out a starting point that you can work with quickly and efficiently . <p> As much as I think I know about film , I learned @ @ @ @ @ @ @ @ @ @ David Stump 's book Digital Cinematography , which relates a lot about what used to be done in film and what is now done in digital . Mr. Stump is a very sharp guy who really knows this area very well , though bear in mind the book is a few years old now . <p> I want to second everything Marc said , and add an interesting aside , while the projector lamp was eventually standardized , during the Technicolor era they often used Carbon arc lamps , which are significantly warmer than the xenon lamps now used , oddly the prints stayed the same , and so the same print of Wizard of Oz , projected today , would look much cooler than in it 's original exhibition . <p> Very interesting . It 's good to hear everyone 's side on this . I 'll take a look into the Digital Cinematography book as well . Marc and Juan , you have both been a very big help to me . I like to think of myself as the LUT as well . I know students who @ @ @ @ @ @ @ @ @ @ a starting point , and that 's not a bad thing ; it can speed up or hinder the process . I , myself , like to start in a Log space , color on a REC.709 monitor and create my own look because that 's where I find my most personal creative growth . I think it all depends on the colorist . Once again , thank you both . <p> You scan the film element -- typically the cut camera negative , sometimes an IP ( finegrain color intermediate ) struck from the negative , or a fine-grain B&amp;W ( for B&amp;W movies ) -- on a scanner at an optimum level to digital files , usually DPX or some other lossless format . Once the digital files are done , those are brought into a color-correction platform and you color correct it as if it was any digital film . There are generally secondary passes done by other people who do dirt &amp; scratch removal , grain management , de-flicker , and even frame-by-frame reconstruction if necessary . I know of cases for very old films where @ @ @ @ @ @ @ @ @ @ to go through , find the best-looking ( and most complete ) scene available , then conform that into a final " best copy " comprised of several different elements . <p> In some cases , studios will screen the " best available " print to give the colorist an idea of how the film was intended to look at the time of release . The reality is that sometimes , the print has aged so badly , it 's too whacked-out to rely on . I can recall some Paramount projects where the studio exec booked a screening , we sat down to watch it , and it was some magenta beat-to-hell mess . Five minutes in , he 'd stop the projector , shrug , then turn to me and say , " well , just do your best and do n't F it up . " There 's some judgement and experience involved here . <p> I was told at Warner MPI that Citizen Kane was done from a total of six different fine-grain prints , one loaned from the BFI in England , and I think @ @ @ @ @ @ @ @ @ @ problems . It 's harder with old Technicolor movies because those involve 3-strip restoration . They have to digitally align each color " record " so that it forms one seamless image , which is difficult because sometimes the three different images have shrunken or become misaligned over the years . Once the 3-strip is combined , we color-correct the final image as if it were just a regular film . <p> Once the final color-correction is done , each reel is rendered and a conform artist stitches each reel together to form a seamless motion picture . Other staff people work on restoring the sound , and that 's then synced up with the digital image files to form the final film . The renders from there depend on what the distributor wants to send out , and that could include videotape or a lower-res file format . 
@@44332477 @4332477/ <h> " Why Do Marvel 's Movies Look Kind of Ugly ? " <p> You know nothing John snow ....... colour Grading started with oh brother where art tho ..... red is better than Alexa ? This guy not only does n't know what he 's talking about , he ca n't do his research well . Yes I find the colour in marvel movies to be rather dulled down but I think it 's in a effort to make the fantastical still based in our world . but like the man says that 's just my opinion . <p> It 's bullshit . The Marvel movies look fine -- I just saw Dr. Strange a week or two ago , and my immediate reaction was , " wow , it 's great to see a huge Hollywood blockbuster that has a bright , sharp , colorful image with great blacks , good skintones , and lots of detail . " Of all of the modern film franchises out there , I think the Marvel guys do their thing very well . <p> Funny video But ... are the @ @ @ @ @ @ @ @ @ @ to check it out for myself . <p> I 've never noticed anything wrong with Marvels image . It does n't stand out but that does n't mean it 's bad . It looked correct to me and I never considered questioning it . But probably maybe because when I go watch this films it 's pure fun and I switch my brain off ( kinda mostly what I do when I watch holivud blockbuster - you do n't really think about texture , and stuff when you go to Mcdonalds ... ) <p> It 's bullshit . The Marvel movies look fine -- I just saw Dr. Strange a week or two ago , and my immediate reaction was , " wow , it 's great to see a huge Hollywood blockbuster that has a bright , sharp , colorful image with great blacks , good skintones , and lots of detail . " Of all of the modern film franchises out there , I think the Marvel guys do their thing very well . 43971 @qwx453971 <p> I saw Dr. Strange and nothing jumped out to me as @ @ @ @ @ @ @ @ @ @ compliments I can give something . Now that I think about it , the " dark world " stuff , or whatever that alternate universe was called , had a nice Pop to it . Solid movie . All the Marvel movies feel very " same-y " , but they 're all good , and you get what you pay for . <p> Compared to a certain project I worked on on-set last year , that just came out , that had a few scenes that did n't match shot-to-shot . I was a bit disappointed in the grade . But I was also being hyper-critical of the project as a whole considering it was the hardest job I 've ever had ; I 'd like it to be flawless as a final project . So it goes . <p> Such a terribly misinformed , misguided , and disingenuous " video essay " . This self-proclaimed nit-picking nerd with no work experience or authority history reminds us that the difference between an ugly movie and one that rocks is a heavy-toe ' s ' curve and 130% saturation . Not @ @ @ @ @ @ @ @ @ @ the cinematography and DI pipeline of being inspired by laziness . <p> It 's a 7 and a half minute opinion piece , with loose arm-chair arguments that just barely demonstrates first-page Google-deep knowledge about digital cinematography . Just another Tony Zhou wannabe exercising the right to be a critic with no license . <p> Such a terribly misinformed , misguided , and disingenuous " video essay " . This self-proclaimed nit-picking nerd with no work experience or authority history reminds us that the difference between an ugly movie and one that rocks is a heavy-toe ' s ' curve and 130% saturation . Not only calling the color grading dull , but also accusing the cinematography and DI pipeline of being inspired by laziness. 43971 @qwx453971 <p> And we know that the DI team sweated blood for months and months over this film , which is typical for any of the major blockbusters . It 's rare you see a project like this where they did n't spend 1000+ man-hours color-timing the whole thing . <p> I really wonder if you know what the hell are you talking about @ @ @ @ @ @ @ @ @ @ of a movie has nothing to do with the camera that has been used to capture the light but it is an artistic choice on how the production decide to present it to the public ? <p> Which are your sources ? Trailers heavily compressed downloaded from internet ? Did even cross your mind that " trailers " do not follow the color of the feature too close because they are , you know , trailers ! Advertisement pieces . <p> " proper black value " I almost choke on my coffee on this one ... You have a serious video scaling issue in your AE data , in some of your " demonstrations " You can see that the black bars top and bottom end of the 2.39 image get darker as you go with your " correction " , and the " eye dropper " is a true joke . Also the image you 're putting up as first example is full of smoke : it will be a HUGE mistake to add so much contrast to make " Black black " , you will unbalance the tonal @ @ @ @ @ @ @ @ @ @ <p> The reference with the comics drawing are also bullish : those are reflective light and the image sensation on your eye depend on the illuminant : try to read a comic book on a dark room , good luck with that . In digital projection ( ATM ) there is only so much dark you can go and the same if for bright , what it counts is the tonal distribution between **35;0;TOOLONG areas . <p> Also , when you are in a very dark room , your eyes adapt and you still see some details : there is no true black in your vision . That will be unnatural and very videoish . That is one of the reasons why most Cinematographer ( Yes , Capital C ) like always see some details in their images . <p> The print process in the comic got better and the paper/ink/printing process allow now for a much better reproduction of dark details and shades of gray that where a bit out of reach in early days . <p> Your color correction of " Guardian of the Galaxy " is truly @ @ @ @ @ @ @ @ @ @ your monitor is uncalibrated , and after your " improvement " ( that BTW , the **35;37;TOOLONG did not ask for ) it look like a over-crunchy Videoish Jondas image . I wonder if you actually ever seat in a proper calibrated Big screen environment . <p> Then you go on to talk about cameras .... Oh dear . The way you capture data has NO CORRELATION with the way you present it . Sin City II was shot in Alexa : is it dark and contrast enough ? Do you need more ? Historically , due to technology and artistic choices , we present between 8 and 10 stops of latitude in the final screen ( with some notably exceptions ) , the camera capture in excess of 13-15 . OF BLOODY COURSE you have to do a correction to map it , but if you mention Alexa , you can also try to use the LUT 's provided by arri at their website just to give you an idea on how you can map it : it look good without spending 1 minute coloring . That because the @ @ @ @ @ @ @ @ @ @ ! I forgot ! you have to actually know how to light the set ... <p> The fact the Marvel created a universe , and they are sticking to it , it make sense : They are making billions , and their universe is COHERENT and CONSISTENT . Why in the hell they want to go back to the silly ' 80 video look that is so passe ? <p> And do you know what have in common 9 out of the 10 last two years Academy Oscar Nominee for cinematography ? They are ALL shot with alexa . The only outlier was " Hateful 8 " , shot in 65mm 5perfs film . red , sorry , It sucks . and that is not a fact but just my opinion . 
@@44332478 @4332478/ <p> The only reason I 'd but one of those , is if I had a tiny DIT cart on set and only needed to make primary LGG corrections on the footage . For serious on-set grading or grading in a suite I 'd save up for a bigger panel . The rings behind the trackballs are n't reachable that well and I 'm switching between mouse , keyboard and panel so much that it often feels more like an encumbrance with the panel than without it , because you can really just control LGG with it . <p> OxygenTec ProPanel Price : ca. 700G <p> Have n't worked with that one . Seems a little more useful than the Tangent Ripple with its additional buttons , but not much . Might be discontinued ( ? ) . <p> Have n't held that one in my hands yet ( only the Mini Panel below ) , but it seems very good for one-light/primary grades . It 's barely cheaper than the Tangent Wave though , which offers way more features , although not a metal body . So for @ @ @ @ @ @ @ @ @ @ a grading suite the Tangent Wave or the more expensive offerings would be a better solution . <p> This is a pretty good panel for the price , with a good menu structure and a decent amount of knobs and buttons to access grading features . It 's very large though and feels cheap . Not a big fan of the rings above the trackballs again , but this would be my recommended entry level grading panel . <p> Tangent announced the Wave II recently , which should have the same features as the Wave , but better build quality and a smaller footprint : ( Picture by Jonny Elwyn ) <p> This is also a good panel for the price , which does n't feel too cheap and only takes a reasonable amount of desk space . Most buttons are n't labelled though , so you have to have a cheat sheet beside it , or learn all of the button mappings by heart . Because of this , I would n't buy it for an educational institution or a workplace with many external freelancers , as people wo @ @ @ @ @ @ @ @ @ @ do n't know what the buttons do , or have to read up on it all the time . <p> This is a great panel , not just for DaVinci Reaolve . It does n't feel cheap , has lots of labelled buttons and knobs and does n't take much space , while being quite heavy and sturdy . It 's good for serious color grading work and easy to learn , so people can use its full feature-set already after a very short amount of time . The price is reasonable , especially if you buy the whole bundle together , which you should do . <p> It 's very sturdy and offers lots of features with a pretty good mapping . Buttons feel wobbly without a nicely defined point of pressure unfortunately and the layout a little cramped , especially with the encoders under the screens . Since the panel is somewhat narrow and deep , your hands move in front of your body all the time , which feels different than on a wide and shallow Elements panel set . If you only need to control DaVinci @ @ @ @ @ @ @ @ @ @ need the big panel , this would be my recommendation for a grading suite . <p> JLCooper Eclipse CX Price : ca. 3.500G I have n't worked with this one yet , but it seems like a mix between the Artist Color and Element Bundle . Lots of unlabelled buttons there , so you 'll have to learn more stuff by heart . <p> This is obviously the best grading panel for DaVinci Resolve . The difference between grading feature-length films or complex commercials on this panel , compared to any one of the others is huge , as you can become incredibly fast and precise with it . So if that 's your daily business , investing in this panel is a good idea . The many knobs and buttons take some time to learn of course and you might not get the full speed and efficiency benefit out of it , if you do n't work with it frequently . The unlit slide-out keyboard is a bad joke unfortunately . <p> General thoughts : <p> Durability : All panels will show wear after being used a lot ( @ @ @ @ @ @ @ @ @ @ For example the cheaper panels ' plastic will rub off on points of contact and become shiny . On the Element panels , the rubber of the LGG rings will peel off and on the Blackmagic control surface the coating on the buttons will rub off , making the label text disappear . <p> Amount of buttons/knobs : This should be the priority . The more things you can control ( at once ) with the panel the better , because you wo n't have to reach for the mouse and keyboard . The better mapped the panels are , the fewer button presses it takes to reach a grading feature . While I wrote " good mapping " on most of the panels above , they are not perfect and could all be improved ! This needs to come from Blackmagic Design though , as they do the DaVinci Resolve panel mappings themselves and not the panel manufacturers . With the BMD Micro and Mini panels released now , we hopefully wo n't see a decline in third party panel support quality . <p> Ergonomics : This is very @ @ @ @ @ @ @ @ @ @ size and arrangement - you still need a keyboard , mouse and maybe a pen tablet around them on the desk . Button size and grouping/layout - the bigger the buttons the better you can " feel " and press them in the dark . Tangent does this really well , with buttons that are spaced out nicely and stick out from the surface noticeable . <p> That 's it for now . Feel free to post your experiences and opinions too <p> I have n't worked enough with the other panels , so I really ca n't compare . But I can answer questions about the CX if people have them . We 've got one of the older ( non-black ) models . <p> I did play briefly with an Avid panel , and I felt the Eclipse was more solidly built . Ethernet was a primary driver for us , because of the setup of our office . I prefer it because it 's simpler to wire , cheaper , and does n't require USB extenders . Our room is about 30 ' from the actual workstation @ @ @ @ @ @ @ @ @ @ We can use the built-in ethernet drops in our office to connect this , right through our existing switch . there 's no latency , and it has n't been problematic at all to have the control surface on the network . <p> So far there has n't been a situation where I 'm bouncing back and forth between the panel and the UI/mouse . Then again , our grading tends to be kind of old-school : minimal nodes , mostly L/G/G controls , offsets , saturation , etc . We do n't do a lot with windows , but have used the knobs and dials to address things like streaks in a film scan that was done on a Spirit with a dirty sensor . dozens of shots needed work to hide those streaks . It was pretty quick and easy , and all doable on the panel itself . <p> I 'd love to try out some of the bigger panels though , but for us , the Eclipse CX really everything we need . <p> I also use the Jl Cooper Eclipse and I absolutely love it @ @ @ @ @ @ @ @ @ @ it feels less " cheap " . It 's all made from heavy metal , not plastic . Having worked with the two panels I find that the Eclipse is less confusing . All the tools are infront of you , with the element if you have to make a shape you have to press three buttons and get into submenus . It 's not as straghtforward . The olny drawback is that it works only with Resolve . I would like to use it for Scratch , Baselight or Nucoda .. <p> That 's a good point . It 's kept us from using Scratch for grading , to be honest . We have Scratch installed mostly as a way to make ProRes files from Resolve 's output , but I 'd love to try it out for day to day grading at some point . Without a control surface , though , it does n't seem worth the effort . <p> if you 're running SCRATCH on Windows , there might be a chance we could even get it to work . SCRATCH used to support the MCS @ @ @ @ @ @ @ @ @ @ I still have the config files . Actually , if you look into the program folder , you can still find the driver-files for the MCS there , so I guess that leaves a chance . <p> However , as far as I 'm aware , there was some twist in the header bits of the ethernet packets , that changed between the MCS and the Eclipse , which you could overcome by enabling some kind of compatability mode in the Eclipse ... I think that was in the manual somewhere . <p> Anyhow - shoot me an email if you wan na give it a try . <p> For the opinions on the panels - it 's funny , I just wrote a little article on the Ripple , including a little info on the next best alternatives to it for Digital Production Magazine ( to be published in 3 weeks ) and I see plenty of overlap between Felix ' and my observations . <p> I think Felix gives short shrift to the Ripple panels , and to me , what they were intended for -- for very @ @ @ @ @ @ @ @ @ @ they 're fine . As I said in my review elsewhere , I would n't want to grade an entire supervised feature for two weeks with one , but it 'll get you by . And I was surprised that ( for $300 ) it was as heavy-duty and solid as it was . I did n't think it was cheap or plasticky at all ( removable balls notwithstanding ) . <p> I 'd also would like to mention , that DaVinci 2k panels also work with Linux Resolve . Those are wonderful panels and these days are extremely economical , if you can find them in a good working order . 43971 @qwx453971 <p> Not the original 2K ethernet panels . Those are flakey and weird -- I 've freelanced at a couple of houses in LA that are trying to use those , and there 's a lot of intermittent issues with Resolve . The ones I used also had missing buttons and a sliding keyboard practically falling out of the panel . <p> Not the original 2K ethernet panels . Those are flakey and weird -- I @ @ @ @ @ @ @ @ @ @ are trying to use those , and there 's a lot of intermittent issues with Resolve . The ones I used also had missing buttons and a sliding keyboard practically falling out of the panel . 43971 @qwx453971 <p> Original 2k+ ethernet panels . Mine look like they just came off the factory floor . These things are beautiful and they are all courtesy MVF ... <p> I think Felix gives short shrift to the Ripple panels , and to me , what they were intended for -- for very light-duty jobs , for students , for DIT 's -- they 're fine . As I said in my review elsewhere , I would n't want to grade an entire supervised feature for two weeks with one , but it 'll get you by . And I was surprised that ( for $300 ) it was as heavy-duty and solid as it was . I did n't think it was cheap or plasticky at all ( removable balls notwithstanding ) . 43971 @qwx453971 <p> For me grading panels are all about productivity and in my opinion the Ripple does n't @ @ @ @ @ @ @ @ @ @ afford a 300$ panel then you can also save up for a 1200$ one ( the difference in productivity is huge here ) . And if you ca n't save for a 1200$ panel , or if that 's just unreasonably expensive for the projects you do , then you probably should n't spend 300$ on one either and you 'll be fine with the mouse and keyboard . I 've done a lot of grading with mouse and keyboard alone and I could n't imagine many grading jobs where the Ripple would be that much of an improvement , after having worked with it . On-Set DIT work on cramped sets , especially when we talk about live-grading where you do n't need to jump between clips and grading features that much is a different story . <p> I do like Tangent Devices and their products a lot and I 'm sure the Ripple works for them to embrace the very low-end grading market , but I just do n't see the appeal myself , even at that low price point . Again , this is just my personal @ @ @ @ @ @ @ @ @ @ and also disagree on ) these things here <p> The Tangent Element has only 12 rotary knobs . So each knob has multiple uses , and you must move through a series of menus to get to the one you want . I have a Wave with 9 knobs and you must in some cases hit the menu key 7 times to get to the desired menu . It seems all but the Resolve panel have this problem . <p> So I 've started programming X-Keys panels and the results so far are quite good . You can have multiple panels and as many buttons as you like , each for a particular function . You can replace the function of a knob with a button which moves the mouse to the appropriate point on the screen , e.g. saturation , and a mouse or trackball which you use to adjust the setting until the key is released . <p> I will likely still use the Wave or Element Tk Panel for LGG , but the rest could be X-Keys . <p> For me grading panels are all about productivity @ @ @ @ @ @ @ @ @ @ 300$ worth of that . If you can afford a 300$ panel then you can also save up for a 1200$ one ( the difference in productivity is huge here ) . And if you ca n't save for a 1200$ panel , or if that 's just unreasonably expensive for the projects you do , then you probably should n't spend 300$ on one either and you 'll be fine with the mouse and keyboard . I 've done a lot of grading with mouse and keyboard alone and I could n't imagine many grading jobs where the Ripple would be that much of an improvement , after having worked with it . On-Set DIT work on cramped sets , especially when we talk about live-grading where you do n't need to jump between clips and grading features that much is a different story . <p> I do like Tangent Devices and their products a lot and I 'm sure the Ripple works for them to embrace the very low-end grading market , but I just do n't see the appeal myself , even at that low price point @ @ @ @ @ @ @ @ @ @ I 'm happy that we can discuss ( and also disagree on ) these things here 43971 @qwx453971 <p> Very good points ... for me personally , in my home setup , I 'm glad to have moved from mouse-only to Ripple and at a low price . Space and budget mean I would n't be getting a $1200 panel in the foreseeable future . The Ripple is a great fit for me ( literally ) . <p> Did I hear something about the Avid Artist ( or one of the versions of it ) not being supported by Avid anymore ? <p> The Tangent Element has only 12 rotary knobs . So each knob has multiple uses , and you must move through a series of menus to get to the one you want . I have a Wave with 9 knobs and you must in some cases hit the menu key 7 times to get to the desired menu . It seems all but the Resolve panel have this problem . 43971 @qwx453971 <p> On the Element it 's a maximum of two button presses to reach each @ @ @ @ @ @ @ @ @ @ 's three ( press UP &amp; DOWN together and then one button to jump to a menu directly or up/down to switch between the two Master Menus ) . I find this not too bad , but you certainly click almost twice as much as on the big panel , which you really start to feel on those feature-length projects when you 're under time pressure . <p> Not the original 2K ethernet panels . Those are flakey and weird -- I 've freelanced at a couple of houses in LA that are trying to use those , and there 's a lot of intermittent issues with Resolve . The ones I used also had missing buttons and a sliding keyboard practically falling out of the panel . 43971 @qwx453971 <p> My understanding of these panels is that you have to be very careful about the wiring - 50Ohm Coax , the right number of T-junctions , and proper passive termination on the cables , old-school ethernet style . <p> I think that 's the cause of a lot of the woe associated with these units . <p> The Tangent @ @ @ @ @ @ @ @ @ @ has multiple uses , and you must move through a series of menus to get to the one you want . I have a Wave with 9 knobs and you must in some cases hit the menu key 7 times to get to the desired menu . It seems all but the Resolve panel have this problem . So I 've started programming X-Keys panels and the results so far are quite good . You can have multiple panels and as many buttons as you like , each for a particular function . You can replace the function of a knob with a button which moves the mouse to the appropriate point on the screen , e.g. saturation , and a mouse or trackball which you use to adjust the setting until the key is released . 43971 @qwx453971 <p> The ideal solution for me ( at least on a small desk without room for the daVinci Panels ) is using the Tangent Elements plus the XKeys 24 . That combination can do about 90% of everything I need to do without mouse-click commands . Once you 've been on @ @ @ @ @ @ @ @ @ @ memory to very quickly go from one mode to another . I lean on the XKeys pretty heavily , particularly for recalling previous corrections , making windows visible , pulling up the Lightbox , changing Curve modes , and a bunch of other stuff I use every day . <p> My understanding of these panels is that you have to be very careful about the wiring - 50Ohm Coax , the right number of T-junctions , and proper passive termination on the cables , old-school ethernet style . <p> I think that 's the cause of a lot of the woe associated with these units . 43971 @qwx453971 <p> What woes ? Panels work just fine . The mapping on the other hand for these panels , that 's another story . Many things are just not there and keycaps are mostly too old . As I understand it , these panels have a limited amount of RAM for the mapping of the new features . 
@@44332479 @4332479/ <h> A change of heart ( advice needed please ) . <p> it has now been 3 years since working with my nMP 2013 . It has , for the most part , been rather stressful . <p> As I write this it 's in the shop yet again at Jigsaw24 for repairs , for the 5th time ! Again related to GPU issues , which have been replaced 3 times prior just last year . I remember when I first got it , it showed early symptoms in the first 6 months ; such as the notorious horizontal glitches when rendering from Davinci . In the past year alone it has given me constant malfunctions ; such as Davinci failing to save gallery stills and cache properly , full system crashes , data loss , power failures , audio cut outs , one GPU undetected by the system , general irrelugar performance , I could go on . <p> That being said , when it works , when it performs like it should , its a joy , its design , concept and innovation is wonderful and I @ @ @ @ @ @ @ @ @ @ as an Editor and Colourist . At the time it was convenient for me to purchase this as my new machine to carry me through my start-up as a freelancer , my options were limited and this offered me a lot . I wished it gave me a better experience , I really looked after it well . It was all I had that helped me to attract work . <p> But a computer this unreliable in its hardware is n't worth it anymore and I feel I have been conned by Apple . And not seeing any evidence from them they are to take us seriously anymore , with no new mention of an updated Mac Pro and the fact my Applecare is about to run out soon ( without it I would have been screwed ) I am now exploring new options . Jigsaw24 have been great with me and we spoke about the possibility of having the entire machine replaced as mine has been constantly problematic evidenced by the amount of recorded repairs ; they will argue for me and they agree I have a strong case @ @ @ @ @ @ @ @ @ @ my applecare runs out for good . <p> I am in a transitory period too with my business having recently re-branded to EMANATE Studios ( formerly psychedelic offspring productions ) . I have had a soft launch recently and plan later on in the year ( summertime ) for a hard launch with a brand new website , URL and email to follow including a Studio suite upgrade . <p> Thus I am now considering a few options for a new machine for this Summer . I am on a budget of course but nothing too constricting . I wo n't delve into a specific system as of yet , but more of what my options overall are and hopefully get some much needed advice from you guys if possible . <p> Option 1 : Build a powerful Hacintosh system . <p> With my planned budget I can really make a powerful single CPU system , an ultimate gamers machine which can run Mac OS X. <p> Pros - I get to keep using ProRes natively , can use most of my external hardware , I retain a familiarity and @ @ @ @ @ @ @ @ @ @ CPU system can limit me on the amount of GPUs , will be tough to find the right motherboard . Also maintenance can be complex and arduous . Compatibility restricts hardware options in relation to workflow requirements , such as Thunderbolt 2 . <p> Option 2 : Build a Windows system . ( W7 Pro 64 or W10 Pro 64 ) <p> With my same planned budget , I can build a decent powerful dual CPU system . A super workstation . I wo n't be able to max it out but it would be considerably more powerful than a Hackintosh . I would require to install MacDrive , no issue there . <p> Pros - I am familiar with a Windows operating system . Adobe and Davinci are my main programs and work well with Windows . Can fit more GPUs , and do n't require SLI with Davinci or Adobe . Its very hardware friendly so can easily upgrade RAM , CPU , GPUs , PCIe cards ( USB-C , eSATA , expansion bays and harddrive options anytime . <p> Cons - Ca n't export ProRes , unless @ @ @ @ @ @ @ @ @ @ WindowsProRes . No more Mac OS X which as an operating system is far more stable and user-friendly than Windows . Windows 10 forces updates but there are ways to control its behaviour , or use Windows 7 instead . Possible issue with UltraStudio 4k compatibility with Windows 7 or 10 via TB . <p> --- <p> For both options the machines will need Thunderbolt 2 . ( I have an UltraStudio 4k and an External RAID Array , both use Thunderbolt 1 &amp; 2 ) . <p> I have a friend well versed in Hackintosh and PC builds who will help me . <p> With either option Storage solutions are numerous . Can be considered after everything else . Ultimately the Boot drive will be an SSD or preferably a PCIe Flash Storage . <p> I can not use a Linux system , I lack the knowledge , resources and funding to have a Davinci Linux license . I am also aware it is possible to build a dual CPU Hackintosh but with even more difficulty and restrictions . <p> Like many users , they have a powerful iMac @ @ @ @ @ @ @ @ @ @ over a DNxHD file and transcode it to ProRes if required . Which in my case I will have that option . <p> I am sure there are other things I have neglected , hence this post , any advice is welcomed . <p> With your help , once I decide what option to go with , I will make a new post in CPU/GPU forum with the actual build choice and its hardware breakdown list to get further advice . <p> Arthur , I assume you have already contacted Apple about it . I would have done on my second problem within warranty . Actually , I have had a one time problem just 10 days to finish my extended applecare , which basically fried my whole computer but the processors , wrote to tim cook ( yeah , I know ) , and I got a call back from San Jose in less than 2 days . He assured me that if it was n't fixed in 7 days , they would replace it . They made sure everything was alright with my machine after if was fixed @ @ @ @ @ @ @ @ @ @ be a lucky one but I just have had another episode of this kind of care , last week . <p> I understand if you need to move on , but you should fight to get this machine replaced , at least for prores duties . <p> I 've just bought a custom configured Windows PC ( what people would describe as a ' gaming ' PC ) . I got my Thunderbolt UltraStudio Mini Monitor to work over the USB-C via a $29 adaptor . DNxHD mxf is working very nicely as a ProRes replacement and I always have my old iMac to transcode if I need to . I 'm very pleased with the value for money , it 's a better spec than a nMPro and it cost less than half the price . <p> I have an IT guy build a Hackintosh for me . It 's medium powered with a single 6-core CPU and one single TitanX Maxwell . All this is based on an Asus X99E-WS mainboard with lane sharing and 7 full length PCIe slots . If all goes well , I can @ @ @ @ @ @ @ @ @ @ security reasons there 's a WIN partition with the basic programs installed . <p> But : It 's not a pro system really . I 'd say jump over to Win and enjoy the huge hardware selection . <p> That 's what most if us Mac guys need to do one day . Pro Apple hardware is just not there <p> Cons - Ca n't export ProRes , unless I use the new software recently released in Beta ; WindowsProRes . No more Mac OS X which as an operating system is far more stable and user-friendly than Windows . Windows 10 forces updates but there are ways to control its behaviour , or use Windows 7 instead . Possible issue with UltraStudio 4k compatibility with Windows 7 or 10 via TB . 43971 @qwx453971 <p> The ProRes exporter , as I understand it , does n't work with Resolve . It requires access to the generic Quicktime windows , to choose a format , and Resolve only provides predefined output types . Also , this method was tried before ( miraizon ) , and Apple shut it down . @ @ @ @ @ @ @ @ @ @ could continue to use your MacPro , networked , as a ProRes exporter . It 's not going to be super fast , because the network will become a bottleneck . If you do n't mind the extra step , you could export to some flavor of DNx and then get a monthly scratch license ( as-needed ) , to render to ProRes . This works well , it 's fast , it 's licensed by Apple so there are no issues with the resulting ProRes files . $75/mo . There are other tools as well , but this works for us , for making ProRes from DPX sequences . <p> Windows 7 is plenty stable . It 's not as elegant as the mac , but you get past that once you get used to it . There are annoyances with Windows , but for the most part , it 's a good OS and it 's reliable . Win7 does n't force updates . <p> Thunderbolt - we do n't have any machines with it , but our motherboards ( ASROCK X99 and some SuperMicro boards , can @ @ @ @ @ @ @ @ @ @ I believe ASUS makes them as well , for their boards . <p> I 'm in the same ( sinking ) boat as other Trashcan Mac users : I 'm weary of dealing with all the render issues , and all that Apple support has been able to do is to reduce the problem , not eliminate it . I have a half-dozen workarounds to minimize it , but none are better than just junking the piece of crap and going with Windows . But ... I 'm gon na wait until NAB to pull the plug . Then , I think the Trashcan will go into an actual trashcan . <p> WindowsProRes is an interesting idea , but I 'm not sure how long Apple will let them keep going ... <p> I cut over to windows 10 64bit , but realistically there 's still a bunch of things I still run on the mac trashcan ( I still have a bunch of little things that need to stay on the mac ) . I think with the trashcan it 's critical to get a fan app , also @ @ @ @ @ @ @ @ @ @ Anyway , we are all feeling your pain but I 'd keep the trashcan around after you get your windows box running . Prores on windows has n't been a problem for me , since the application packages I use on windows come with a internal prores reader/writers . <p> I cut over to windows 10 64bit , but realistically there 's still a bunch of things I still run on the mac trashcan ( I still have a bunch of little things that need to stay on the mac ) . I think with the trashcan it 's critical to get a fan app , also I do all my big cpu/gpu jobs on windows . Anyway , we are all feeling your pain but I 'd keep the trashcan around after you get your windows box running . Prores on windows has n't been a problem for me , since the application packages I use on windows come with a internal prores reader/writers. 43971 @qwx453971 <p> I would still get rid of that very expensive POS and get myself a good ole 5.1 . Much better value without @ @ @ @ @ @ @ @ @ @ and advice , it has been very helpful and insightful . I have decided to go for a Windows 10 Pro 64bit super workstation build . I shall start a new post in a week or so detailing that to get more technical advice from all of you . <p> Regarding ProRes encoding , there seem to be several options out there which is great . <p> I have a friend who will help me design and build it which is great , should be a lot of fun . Hopefully I shall hear back from Jigsaw24 soon regarding if Apple will replace my nMP for the time being . 
@@44332480 @4332480/ <h> Funny story : A group email I sent to the CEO of Nvidia and Tim Cook led to Pascal support on MacOS <p> On March 3rd I sent a pretty desperate ( and poorly worded , apologies ) group email out to Jensen H. Huang ( CEO of Nvidia ) and Tim Cook ( CEO of Apple ) essentially begging for Pascal chipset support on Macs . To be fair I did n't have much hope that it would go anywhere , it was really late and I was n't feeling particularly wordy or eloquent . I have read hundreds of pages of threads waiting on Pascal support for Mac all over the web , lot of dead ends , lot of sad people giving up and selling chipsets . In an effort I considered to be completely futile , I decided to start digging around and seeing if I could find contacts and reach out to at least one of these guys , maybe they 'd simply delete the email , maybe it 'd never get through a filter , or best case scenario they 'd at @ @ @ @ @ @ @ @ @ @ write the idea on a post it note . <p> What happened was even better . Tonight the CEO of Nvidia emailed me back informing me that my email was the decision Pascal has come to MacOS , that previously he had no idea it was wanted . Thanks to communities like eGPU.io , the various Hackintosh communities , the creative editors and VFX artists , and the colorists over here at LiftGammaGain.com , I was able to present that there was certainly a corner of the world that wanted this . Though I have n't heard back from Tim Cook and likely wo n't , I ca n't say how much it means to hear back from Nvidia 's CEO so candidly . Reaching out made this happen , and I never thought the day would come . My initial effort was basically a shot in the dark , a literal " eh ca n't hurt to send an email " . <p> I really do get the notion that Nvidia 's team wants to be out here with us , delivering support for MacOS probably does n't make @ @ @ @ @ @ @ @ @ @ deep learning crowd out there would , but the fact that they did this for us is incredible . Continue to show Nvidia your love , people ! <p> Excuse the cringey first email , I was really just trying to paint a picture of how important Nvidia/Mac usability is thru the only angle I 'm familiar with as a student : creatives/filmmaking ( I 'm no hackintosh aficianado , I only understand how important Nvidia cards are for software like Resolve , the Adobe suite , etc ) . Was an honor to hear back from him , I ca n't believe my email is what finally made it happen after a year or more of our communities starting to lose hope . <p> What happened was even better . Tonight the CEO of Nvidia emailed me back informing me that my email was the decision Pascal has come to MacOS , that previously he had no idea it was wanted . 43971 @qwx453971 <p> It 's very nice that you got an email . Very nice indeed . <p> But I think there is a saying describing how @ @ @ @ @ @ @ @ @ @ whole company can miss a good part of a year 's worth of Twitter barrages and a couple of the worlds largest Mac Pro communities writing in thread after desperate thread that surely " drivers will be out soon " . And after that pleading for it That saying is " asleep at the wheel " . <p> I sincerely hope Nvidia is n't that deaf to the world and that at least someone over there has a finger on our community 's pulse . If not , just to pull a random quote out of a random movie : " God help us all ! " <p> EDIT : Let me make a small remark in order to not sound unintentionally bitter : I 'm very happy that the drivers are out and that Nvidia listened to us . There are no official Macs out at this time that warrant the drivers , so it 's a service to those of us who need them " thanks Nvidia ! 
@@44332481 @4332481/ <h> The correct way of normalizing SGamut <p> I have some footage from A7S II shot with Slog2 and Slog3 gammas and SGamut3.cine color space . I tried to use LUTs provided by Sony and custom LUTs generated on LUTCalc to convert it into REC709 gamma and color space , but I do n't like the results . The contrast looks ok with some LUTs but colors are always off . The reds become pinkish , the green-yellow turn into yellow and skies look cyan . I 've managed to get a lot better results by using curves and saturation , but I wonder if it 's a technically correct approach for dealing with this color space . Thanks ! <p> I wonder if it 's a technically correct approach for dealing with this color space . 43971 @qwx453971 <p> newest version of ACES ( 1.03 ) has a revised Slog3 IDT , that 's about as " technicaly correct " as is possiable <p> ACES was my starting point for the only project i 've worked on with a Sony camera in the last year or so , @ @ @ @ @ @ @ @ @ @ camera under pretty harsh conditions , alot of desert and only a bit of bounce for fill , pretty simple grades , mainly L*a*b , no curves , no keys , no masks , so leaning on the camera 's sensor range <p> newest version of ACES ( 1.03 ) has a revised Slog3 IDT , that 's about as " technicaly correct " as is possiable <p> ACES was my starting point for the only project i 've worked on with a Sony camera in the last year or so , a short film , it went well , nice looking camera under pretty harsh conditions , alot of desert and only a bit of bounce for fill , pretty simple grades , mainly L*a*b , no curves , no keys , no masks , so leaning on the camera 's sensor range 43971 @qwx453971 <p> Thanks for suggestion , but I would like to avoid working in ACES . I did n't see any benefits of ACES in my tests . <p> well if " correct " is the goal , then Sony 's published IDT , and @ @ @ @ @ @ @ @ @ @ Baselight , Nucoda etc etc etc .... and it will appear as Sony thinks it should look under optimal conditions , adn they are the one 's who know what is going on at the sensor &amp; internal processing level , no one else can or does ... <p> so if " correct " is the goal then maybe load a few shots into ACES , do n't touch anything , save stills , then go back to YRGB and try to match them to create an offset <p> In that case any tool that works for you is a good tool ... <p> i do n't use curves in Resolve ( other than the basic curves in L*a*b ) , but aside from the " -vs- " curves that are a bag of trouble , the basic curves are as workable as anything else , go crazy , be happy , make your day with the tools that work for you <p> as to ; " can curves be correct " the answer is not really ... the way to be " correct " is to use the tool @ @ @ @ @ @ @ @ @ @ in YRGB are not it <p> I second Dermot 's suggestion of using the correct Sony-supplied IDT and working in ACES or at least using the IDT to get a few reference images that you could match to create your own " input grade " . Another option is if you 're set against grading in ACES space you could just use the Sony IDT and ACES to convert to rec709 and then grade from there . <p> I second Dermot 's suggestion of using the correct Sony-supplied IDT and working in ACES or at least using the IDT to get a few reference images that you could match to create your own " input grade " . Another option is if you 're set against grading in ACES space you could just use the Sony IDT and ACES to convert to rec709 and then grade from there . 43971 @qwx453971 <p> Again , I do n't understand why should I go ACES instead of using LUTs provided by Sony . <p> ACES is perfectly suitable for things like this without the need of LUTs . <p> When I need @ @ @ @ @ @ @ @ @ @ I throw either the SLog3 to LC709A or the Arri Log C to 709 LUT at it , Then use the balls for a fast LGG and that 's it . <p> ACES is perfectly suitable for things like this without the need of LUTs . <p> When I need a quick way of showing a client normalized S-Gamut3.cine/S-Log3 footage I throw either the SLog3 to LC709A or the Arri Log C to 709 LUT at it , Then use the balls for a fast LGG and that 's it . 43971 @qwx453971 <p> I can do it without using any LUTs or ACES and I do n't have problem doing that . My original question was if it 's a correct way . And if not , why . <p> Also , I described what happens to color when I apply those LUTs . The same that you mentioned . <p> ACES IDT 's are n't split up in gamma and colorspace , so if you shot slog3 gamut , but with slog2 gamma the IDT is n't really matching . You could try Resolve Color Managed workflow or @ @ @ @ @ @ @ @ @ @ gamma and colorspace seperately . Personally , I 'd do what you already did and just create a grade with sat and curves and use that . Essentially every lut can be recreated in a grade . <p> I can do it without using any LUTs or ACES and I do n't have problem doing that . My original question was if it 's a correct way . And if not , why . 43971 @qwx453971 <p> Dermot already gave you the answers to both of those questions . Seems to me you 've already decided what you want to do and are looking for others to confirm that it 's a good approach . The problem is that a number of us - some of whom are quite experienced - feel it is n't . <p> There is one ( other ) technical issue that may have an impact on the way the LUTs transform the slog2 and slog3 video clips . <p> I have no experience with the Sony A7S but it is my understanding that the log video clips can be recorded both with data or video @ @ @ @ @ @ @ @ @ @ on both at the same time as log space is not linear space . So this may be the reason colors are not correct . <p> Data versus video levels may actually be an issue for ACES as well or does the IDT automatically adjust to it ? <p> Again , I do n't understand why should I go ACES instead of using LUTs provided by Sony . 43971 @qwx453971 <p> Because it seems that the ACES route produces a more esthetically pleasing transform . I agree it seems like the result should be the same since you say the LUTs are coming from Sony too , but maybe they used a slightly different approach with their ACES code or maybe its the combo of the IDT and RRT that makes it look " better " . <p> I can do it without using any LUTs or ACES and I do n't have problem doing that . My original question was if it 's a correct way . And if not , why . 43971 @qwx453971 <p> If it works for you and does n't stomp on your image in @ @ @ @ @ @ @ @ @ @ would n't worry too much if it 's correct or not . For me , I would rather use the ACES transform because someone has already done the work to get me to a good starting point and I trust they 've done it in a way that they consider technically correct . <p> Also consider , if you end up using your own transform or a LUT transform , you might want to put that last in the chain . That way you can grade the log image . Good luck ! <p> Another thing to consider is that the Sony Prosumer share the SLog encoding but do n't have the same native gamut size nor enough bits . They all bake log into 8bits which can produce a lot of problems with smooth graduations . <p> Dermot already gave you the answers to both of those questions . Seems to me you 've already decided what you want to do and are looking for others to confirm that it 's a good approach . The problem is that a number of us - some of whom are quite @ @ @ @ @ @ @ @ @ @ I 've decided the best way to work with it and I wanted to make sure that I wo n't run in any issues <p> Because it seems that the ACES route produces a more esthetically pleasing transform . I agree it seems like the result should be the same since you say the LUTs are coming from Sony too , but maybe they used a slightly different approach with their ACES code or maybe its the combo of the IDT and RRT that makes it look " better " . <p> If it works for you and does n't stomp on your image in a way that makes secondaries and fine-tuning difficult then I would n't worry too much if it 's correct or not . For me , I would rather use the ACES transform because someone has already done the work to get me to a good starting point and I trust they 've done it in a way that they consider technically correct . <p> Also consider , if you end up using your own transform or a LUT transform , you might want to put @ @ @ @ @ @ @ @ @ @ grade the log image . Good luck ! 43971 @qwx453971 <p> Thanks ! <p> I would say that ACES gives me better starting point than using Sony LUTs , but working with primaries gives me better result without complicating my workflow . <p> Also , for some reason the footage from C300 looks bad in ACES . Maybe it 's a Resolve conversion thing , I do n't know . <p> There is absolutely a correct way , and it is different for different log curves and color spaces . All modern cameras are capturing linear light , and how that information is transformed for display purposes , and how it is transformed for storage purposes are mathematically defined and usually two different things . Manufacturers know the characteristics of the cameras they produce , and they know the actual response . They use that to derive transforms that recreate the captured scene " properly " for specific display targets , and they use it to develop log curves that allow the largest amount of the captured information to be stored with proper , perceptually accurate precision . To deny @ @ @ @ @ @ @ @ @ @ undo " those encodings and display them on a particular monitoring system , and claim that this is something that is somehow an " artist 's " prerogative is to ignore scientific and photographic reality . Now , that 's not to say that you ca n't simulate the " proper " transforms by hand , or even change them for aesthetic purposes . However , for the vast , vast majority of people ( and colorists ) , doing that from scratch , with the source being something that was never even intended to be a viewable image but rather an information container ( that 's what log encoding is ) is usually not going to produce a better result - particularly in the mid and upper mid ranges of the greyscale - than using a proper transform derived from the actual capture and log encoding values . 
@@44332482 @4332482/ <h> Find color space of my footage . <p> Hi everybody , Is there any method to find what is the color space used in camera settings for the clips I 'm grading ? Please do n't ask to answer to people who were on the set during the shooting ... It was me ... <p> Hi everybody , Is there any method to find what is the color space used in camera settings for the clips I 'm grading ? Please do n't ask to answer to people who were on the set during the shooting ... It was me ... <p> Thx 43971 @qwx453971 <p> If you are lucky it is in the stream headers or the metadata . <p> To get info from the stream headers you could use MediaInfoXP or ffprobe , meta information you could get from exiftool <p> If you could share a few frames of the original video ( without re-encoding ) we can take a look at it . <p> If so , just check it on the scopes and get the overall neutralized on the vectorscope and get everything kind @ @ @ @ @ @ @ @ @ @ curves and/or Primaries , and get it to a reasonable starting point in a couple of nodes . <p> Hi guys , thx for answers , Cary - I ever use media info but not find in the metadata , is mediainfo XP better ? Marc - Heuu , no I mean color space , I 'm not so far . Pepijn- It 's Sony F5 . Is Catalyst working ? I will try to find . <p> Hi guys , thx for answers , Cary - I ever use media info but not find in the metadata , is mediainfo XP better ? Marc - Heuu , no I mean color space , I 'm not so far . Pepijn- It 's Sony F5 . Is Catalyst working ? I will try to find . <p> For the F5 I think it 's Sgamut or Sgamut3 or Sgamut cine3 <p> Is there a big difference ? 43971 @qwx453971 <p> Sony F5 should be shown correctly in Catalyst Browse/prepare if you have the original media structure in tact , with colour and gamut correctly detected . Even detects cine ei @ @ @ @ @ @ @ @ @ @ usability . The . cine colour space is bigger than p3 or rec709 but it is still ' in line ' in such a way there are n't hue shifts like there are in sgamut3 or the older sgamut though they are easy to work with in ACES ( Resolve does n't include a . cine transform by default ) . It 's possible for it to be sgamut and slog2 as well , I 've certainly received footage shot on the f55 where the operator chose that for some reason . <p> Yep i 've try catalyst , it 's a fantastic tool thx for that . But I ca n't see any indication for color space , I just see in monitoring description : SonyProf3:Slog2-709 . Is there the lut use for monitoring during the recording ? What is SonyProf3 mean ? I can see in capture gamma equation : S-Log2 . It 's mean that the color space is S-gamut no ? No possible to shoot Slog2 with Sgamut3 or . cine3 ? ? If the footage was shot in S gamut or S gamut 3 and @ @ @ @ @ @ @ @ @ @ still good to work and recovery my color sapce for grading ? ? <p> Yep i 've try catalyst , it 's a fantastic tool thx for that . But I ca n't see any indication for color space , I just see in monitoring description : SonyProf3:Slog2-709 . Is there the lut use for monitoring during the recording ? What is SonyProf3 mean ? I can see in capture gamma equation : S-Log2 . It 's mean that the color space is S-gamut no ? No possible to shoot Slog2 with Sgamut3 or . cine3 ? ? If the footage was shot in S gamut or S gamut 3 and I apply a lut in S gamut.cine3 , I 'm still good to work and recovery my color sapce for grading ? ? <p> Thx again 43971 @qwx453971 <p> Really , the best way to figure this out is by simply trying the different options ( slog vs slog3.cine ) and see what looks best . Just pick a well exposed shot of a face or something else familiar . Having a slog2 transform on a slog3.cine captured imaged @ @ @ @ @ @ @ @ @ @ versa ; putting a slog3.cine lut on slog2 shot footage will render skintone quite yellow/brownish . <p> Yep i 've try catalyst , it 's a fantastic tool thx for that . But I ca n't see any indication for color space , I just see in monitoring description : SonyProf3:Slog2-709 . Is there the lut use for monitoring during the recording ? What is SonyProf3 mean ? I can see in capture gamma equation : S-Log2 . It 's mean that the color space is S-gamut no ? No possible to shoot Slog2 with Sgamut3 or . cine3 ? ? If the footage was shot in S gamut or S gamut 3 and I apply a lut in S gamut.cine3 , I 'm still good to work and recovery my color sapce for grading ? ? <p> Thx again 43971 @qwx453971 <p> if in the meta data says SLOG2 then your Slog2 / sgamut <p> if in says SLOG3 then your SLOG3 / sgamut3 or Sgamut.cine <p> so no matter what you think you shot if the meta data say SLOG2 then your sgamut 
@@44332484 @4332484/ <h> Reducing the Luminance of Red Colors <p> As you can see Camera on the Left has darker reds from the cup , scissors and the box , I tried my best to not only match the skintones but also getting the luminance of the red channel on Camera on the right lower without introducing artifacts in the dark box . <p> I could n't figure out how to do it . <p> How can you reduce the red colors for the camera on the right without introducing artifacts ? <p> This is best done in camera , if it 's filmed with a Sony . The color depth option under the picture profile settings adjusts the luminance of the color channels individually . <p> Your other option , in Resolve , would be to split the channels into 3 ( splitter/combiner node . sorry , not at my workstation right now to get a screenshot ) , one of each channel and adjust it that way . Although , that 's for an entire channel , not just a specific hue of red . <p> Secondary keys are @ @ @ @ @ @ @ @ @ @ . As far as less artifacts , the only solution I can think of is transcoding the footage , which will help even if just a little bit . <p> Keys and hue/lum curves are potentially noisy and best done as a final tweak . I am not in front of a system to try this but my first approach would be something more basic , cleaner and global so that you can use it on up front without too much hassle . <p> So my proposal would be go to the channel mixer , go to the blue channel and add a little red . The result would be that all the reds now have a touch of blue , which will shift the hue and look darker . I think the hair and skin on the left are in fact slightly cooler in the reds , so its worth a try . Note : in resolve the channel mixer is too late in the path of a node , so you should do this by itself in the first node without any prmaries and then do everything in @ @ @ @ @ @ @ @ @ @ distort your " camera matrix " ( eg what you are doing with the channel mixer ) 
@@44332485 @4332485/ <p> iResolve Mobile . Features FaceBook integration , sources all images from your phone 's camera or photo album , quick application of filters , a timeline to see what your friends are grading where you can make your own comments or just hit a button that reductively declares that you like an image . <p> ..... it 's Instagram . They made Instagram . A horse , designed by committee , is Instagram . <p> I 'm leaning towards " The daVinci Post-Production Suite , " where they 'll have Resolve , Fusion , and maybe one more thing . 43971 @qwx453971 <p> This makes a lot of sense to me , also for those of us in stereo workflow the dongles are getting kind of complex . I 'd like just a single dongle for all " studio " version ... so a single " studio " purchase for both fusion studio and resolve studio that works across all platforms ( mac/linux/windows ) . In general I 've noticed from comments by people like Peter Doyle and also comments from people that have worked on the martian @ @ @ @ @ @ @ @ @ @ are also involved with vfx integration so I think this suite concept has a lot of legs . <p> I always thought that the DaVinci name was best . Everyone knew DaVinci name . I never really liked the Resolve name ... 43971 @qwx453971 <p> Yeah , Grant Petty has said in interviews that he was always a big fan of daVinci when he was an engineer in the 1990s and for the post business down under , they were the gold standard . I think it was a big deal for him to go from being a post engineer to being in a position to actually buy the assets of daVinci Systems and keep them going . He clearly loves the business , and I would bet they have a huge team of programmers working tirelessly on the software ( and new hardware like the panels ) . <p> There 's some interesting history with daVinci , and when I consulted for them a little bit in 2003-2004 , they were torn between doing another round of hardware ( " daVinci 4K " ) or having a resolution-independent software @ @ @ @ @ @ @ @ @ @ wars at the company on that decision and you could make good arguments both ways . I do n't think anybody imagined 10-12 years ago that there would someday be hundreds of thousands of Resolve users with the $995 version . <p> They 've really come a long way ... here 's how I first saw Resolve , when it was a nightmare to use as Resolve 6 : 
@@44332486 @4332486/ <h> The look of Billy Lynn <p> Have n't got a chance to see Billy Lynn in the cinema so recently grabbed a Bluray , I was surprised at how " videoy " the images looked with harshly blown out highlights and overly magenta skintones that accentuate blemishes . To me it felt like watching an expensive TV drama shot on broadcast camera . <p> The Bluray release is 24fps , I ca n't imagine what it would look like with the ultra smooth 120fps/60fps . It got a Dolby Vision release as well , could it be any better ? <p> Lucy , also shot ( mostly ) on F65 , has a similar look . I know F65 has excellent sensitivity and SNR but average highlight latitude , but Oblivion managed to get some very organic and pleasing images . Well I guess F65 is a beast that only few could tame . <p> I saw the " limited engagement " 120fps version of Billy Lynn at the Ceramic Dome in LA , and I thought it had an odd look . The high frame rate was @ @ @ @ @ @ @ @ @ @ the 3D presentation looked terrific , though I do n't think it 's a film that needed to be in 3D . Color was fine , brightness was fine , the whole thing was sharp as a pin , but the story was n't engaging at all . Technically , it was amazing but it was like a 2-hour demo film for something I 'm just peripherally interested in. 
@@44332488 @4332488/ <h> RED SCARLET NOISE <p> Hi everyone .. i recently shot a music video for a friend and had to put down ISO to 320 but still had a very huge amount of noise throughout the video . please i really do need your help as i am new to R3d Cameras . What do i do to deal with this noise ? <p> Avoiding noise is a lot more complicated than adjusting ISO to lowest possible . You may have gotten less noise using the native ISO of the camera , which I believe is 800asa ( someone correct me who works w more RED footage if I 'm mistaken ) , even if it is higher compared to 320 . There could be an entire book on this subject , but it also depends which RED you shot with ( Scarlet , MX , One , Dragon etc ) , and what typelevel of lighting you used , what frame rate , and several other factors . I 'm going to guess the sensor is a little starved for lighting , causing some sensor noise . Not much @ @ @ @ @ @ @ @ @ @ in the shadows , and gently noise reduce if its else where . Best of luck ! <p> Due to a feature I have just finished with FPN noise issues I would suggest heavy noise reduction and then a re-grain over the top . But to avoid this in the future and to get used to a unfamiliar camera/format . I would say do a test shoot with lighting and lenses with the ISO/ FPS/ Frame sizes to be used would be a good idea . That way you can see at what point you 're getting into trouble . Also nothing like first hand knowledge . <p> All red cameras up to the epic are 320 asa at daylight and 250 asa at tungsten . So you put that kind of right , even if there is no real ' right ' . Perhaps you are dealing with footage from a camera that has not been black balanced , then you get the fpn ( fixed pattern noise ) where only denoising can help to a certain degree ... <p> Thanks alot for your tips , problem is im @ @ @ @ @ @ @ @ @ @ each and everyone of you could tell me what exactly to do with the footage because some terms are new to me . I have attached screenshoots below . These was shot with the R3D scarlet at 25fps 320 ISO , and walimax lense <h> Attached Files : <p> Avoiding noise is a lot more complicated than adjusting ISO to lowest possible . You may have gotten less noise using the native ISO of the camera , which I believe is 800asa ( someone correct me who works w more RED footage if I 'm mistaken ) , even if it is higher compared to 320 . There could be an entire book on this subject , but it also depends which RED you shot with ( Scarlet , MX , One , Dragon etc ) , and what typelevel of lighting you used , what frame rate , and several other factors . I 'm going to guess the sensor is a little starved for lighting , causing some sensor noise . Not much you can do besides try and crush it if its in the shadows , and gently @ @ @ @ @ @ @ @ @ @ ! 43971 @qwx453971 <p> Hi Jason , i appreciate your help . please would you be so kind to tell me how to crash it in the shadows and gently noise reduce ? am just a new film student . thanks <p> Due to a feature I have just finished with FPN noise issues I would suggest heavy noise reduction and then a re-grain over the top . But to avoid this in the future and to get used to a unfamiliar camera/format . I would say do a test shoot with lighting and lenses with the ISO/ FPS/ Frame sizes to be used would be a good idea . That way you can see at what point you 're getting into trouble . Also nothing like first hand knowledge . <p> All red cameras up to the epic are 320 asa at daylight and 250 asa at tungsten . So you put that kind of right , even if there is no real ' right ' . Perhaps you are dealing with footage from a camera that has not been black balanced , then you get the fpn ( @ @ @ @ @ @ @ @ @ @ a certain degree ... 43971 @qwx453971 <p> hi Daniel thanks for your help . so how exactly do i denoise the image ? thanks i attached screenshoots above so you can clearly see where the noise is at . thnx <p> I own a Scarlet and I understand your pain . I 've learned to tame the beast through extensive testing and frustration . <p> Temp and fan control are more critical than iso setting Set the fan to a target tempeture of 64- Never use the auto or manual settings To calibrate run the camera to get it to a stable temp . Put the body cap on and put the camera in a darkened area Record for 8-10 min Format media Then BLACKSHADE <p> You should be able to shoot up to 1600 with no issue . Daylight is always better since the sensor is native to about 5000k . Going lower introduces noise in the blue channel . <p> If while shooting your TE light is yellow or red , stop and Black shade or deal with the noise the hard way . <p> Nuke can clean @ @ @ @ @ @ @ @ @ @ n't tried Neat but it sounds like it might work . Resolve , and AE NR do n't cut it in this case , lumpy . <p> Neat video would go one of the first nodes ( which you can disable when it gets slow while grading but enable it for the render ) . Adjust the settings in neat video for each clip as needed , starting w chroma reduction , and adding some luminance ( y ) noise reduction as needed . These can all be adjusted directly in neat video settings . Def Google neat video and scarlet and I 'm sure you 'll find some good suggestions , or maybe even a neat video profile specifically made got a noisy red ( camera ) situation . <p> This will be an exercise in playing around to see what settings works best so do n't get frustrated if the first few tries are n't 100% successful <p> Thnx a lot . I was doubting if I could follow this work flow due to the frustration I get from importing those files from premiere to davinci lite . @ @ @ @ @ @ @ @ @ @ davinci then export as proress 444 edit in premiere and send to after effects to Denoise with Neat video back to premiere for effects and credits ? Please help <p> I think some of this stuff is overblown , but I come from the days when we had to routinely deal with film grain . It 's pretty rare I see any digital material these days get as grungy as film did , especially when the film was pushed or the ISO went past 500 . <p> But I think the bottom line is that you need good exposure , a good black balance ( to reduce FPN ) , and a reasonable input LUT to get good results . If you just need to see a reasonable picture for editing , the stock RedColor/RedGamma3 setting ( or the new Dragon setting if you have the new chip ) work OK as a one-light . Otherwise , read the tips on the Red User group . There 's many , many , many discussions about exposure , noise , contrast , dynamic range , and the compromises involved . It @ @ @ @ @ @ @ @ @ @ limits of the camera are . <p> In my experience many people just have n't invested the time to understand how to shoot with it . Some are completely lucky and get good results without technical effort . But , what I 've also seen is those same people with problems are coming from a video background and not even using a light meter . <p> I agree Marc that people have forgotten how much grain used to be in what the majority would consider excellent looking footage . Entourage for example used 5229 ( and others ) and shows lots of grain in some shots . I do n't recall anyone ever mentioning it . <p> When used to shoot on film , I never bought anything faster than 800 . We 're all spoiled now 
@@44332490 @4332490/ <h> Export a LUT of a single eye <p> I am doing a grading of a stereo project in the full Baselight and when I tried to export the LUT of one of the eyes I got an error ( " Render failed : Unknown colour space a NULL " ) . <p> It does n't matter if I set the scene settings to be a single stack stereo or Left/Right eye on top . Also it does n't matter if I have both tracks combined or not ... Also If I have the stereo colour matching operator on the input layer or if I insert the operator under the input layer as a new layer . <p> I tried to save the grade on the Gallery and applied this grade to a non-stereo project and of course this does not work . <p> My steps : <p> Set the scene as a stereo project with Left eye on top . I have two tracks , one of top of the other . I combine both eyes in a single stack and I did the " fix geometry " @ @ @ @ @ @ @ @ @ @ match one eye to the other one with the stereo color matching operator . Then when I apply a grade to one eye I have the same on both eyes . Everything is perfect but when I tried to export the LUT is not working . <p> My colour journey is well setup and when I export the LUT my output colour space needs to be CGI linear/Rec 709 to send this LUT to the VFX guys ( FLAME ) . In a non stereo scene the LUT export works fine . <p> I am glad Mark Tucker is on this forum Please maybe someone can put some light here . Thanks ! 
@@44332491 @4332491/ <h> UHD Consumer monitors and upscaling of interlace <p> Trying to get a handle on how the typical consumer UHD monitor handles interlaced material . As the UHD spec is all progressive , I am guessing that all upscaling is also de-interlacing as well ? ? I have had very little luck Googling this . <p> Anybody have any insight ? <p> The reason for the interest is mainly getting new client monitors for edit rooms which will predominately be working on HD with the normal mix of interlace and progressive . But we want to future proof for UHD work . <p> Why not use modern 4K TVs instead ? We run SDI to a high end 55 " 4K Sony X9000 B TV ( native UHD but will do Cinema 4K at 24P ) for client viewing via an SDI to HDMI 2.0 converter . Ours handles all forms of HD , UHD in Progressive , Interlaced as well as PsF at most common legacy PAL/NTSC frame rates . <p> My Panasonic DX900 has a real good upscaler , but I do n't have interlaced footage to @ @ @ @ @ @ @ @ @ @ look all that different from UHD if you have a normal viewing distance . YMMV <p> If the content is properly detected as interlaced , then the set will usually do a per field upscale and sequential draw . That is , it 'll show fld1/fld1/fld2/fld2 . If it 's incorrectly flagged and the set does not recognize it as interlaced , it will scale the whole frame and this usually looks fairly ugly with comb pattern along edges . <p> Occasionally there will be content which is flagged as interlaced but has field dominance incorrectly placed . In that case 1 of two things will happen . Most often the upscaling chip ( there are actually only a few used by manufacturers ) will correctly identify the field order and then upscale correctly . Or will fail to recognize the error in which case the set will display very steppe motion fld2/fld2/fld1/fld1 . This used to be quite common but has become rarer , in fact it 's kind of a problem because news rooms for example frequently use consumer displays for edit monitors , and they fix these @ @ @ @ @ @ @ @ @ @ converted by a device that does n't fix this you end up with a very annoying and glaring artifact . <p> If the content is properly detected as interlaced , then the set will usually do a per field upscale and sequential draw . That is , it 'll show fld1/fld1/fld2/fld2 . If it 's incorrectly flagged and the set does not recognize it as interlaced , it will scale the whole frame and this usually looks fairly ugly with comb pattern along edges . <p> Occasionally there will be content which is flagged as interlaced but has field dominance incorrectly placed . In that case 1 of two things will happen . Most often the upscaling chip ( there are actually only a few used by manufacturers ) will correctly identify the field order and then upscale correctly . Or will fail to recognize the error in which case the set will display very steppe motion fld2/fld2/fld1/fld1 . This used to be quite common but has become rarer , in fact it 's kind of a problem because news rooms for example frequently use consumer displays for edit monitors @ @ @ @ @ @ @ @ @ @ spotted . When displayed or converted by a device that does n't fix this you end up with a very annoying and glaring artifact. 43971 @qwx453971 <p> Hello Juan , Thanks for the info . Exactly what I was looking for . <p> I am a bit confused by one statement , " ... it 'll show fld1/fld1/fld2/fld2 . " How does that information integrate with effective frame rate of what the UHD monitor is showing ? By that , I mean if a 1080i 59.94 program is fed into a UHD monitor what are you seeing ? Is fld1/fld1/fld2/fld2 being thrown up on the screen all at one time ? Or something else ? <p> What happens when a 1080i show with 2:3 pull down and mixed cadence is the source ? <p> Each is usually flashed twice ( 120hz ) , on it 's own . With 2:3PD material one of two things can happen . Chip either uses same process as video . Or if it properly detects PD source it 'll deinterlace back to 24P . Or more likely with modern sets in consumers home , @ @ @ @ @ @ @ @ @ @ Each is usually flashed twice ( 120hz ) , on it 's own . With 2:3PD material one of two things can happen . Chip either uses same process as video . Or if it properly detects PD source it 'll deinterlace back to 24P . Or more likely with modern sets in consumers home , use some motion smoothing to make 240Hz garbage . 43971 @qwx453971 <p> Ok . Well , in some regards , this might be better than some of forms of de-interlacing .... <p> Well , with this info , the next question . Is anybody out there using a consumer UHD as a client monitor in a controlled ( HDMI from a Blackmagic or AJA IO ) predominately HD environment ? If so , how is it working for you ? 
@@44332494 @4332494/ <p> Hey - thanks for the reply . In addition approx 900 shots and the only shots that really copy over are the I/v 's which make up about 40 shots - almost everything else is balance and grade shot by shot ( archive etc is all over the place ) . 1- Grade only 2- No real fixes , other than correction ( huge hue washes and very low lit ) 2- any significant camera issues ? Not really , see above 3 - Pretty straight forward natural doc style 4 - broadcast 5 - HD <p> i graded this doco in 2 x 10hr days , so 20 hr all up , production gave me a DRP , i sent back a DRP linked back to rendered media + rendered media <p> on 7.5 hr days that 20hr would have been about 3days <p> it had a pretty normal stack of problems , nothing crazy tho , and it got past QC , 100% pass first time ( this trailer cam staright off the timeline , i did not grade this , it was not really graded @ @ @ @ @ @ @ @ @ @ shows a bit of start point ; <p> I can generally do 300 shots a day , maybe 500 if it is n't too complicated , and in a documentary situation with a lot of talking heads , it could be faster . I did a 90-minute documentary in the last year where it was basically three days from conform to delivery , and we made it OK , though there were some 12-hour days there . I think it was 30 billable hours total . I could 've done it in 20 , easily , if we 'd let a bunch of little things go unfixed ( which I hate to do , but time is money ) . <p> You could easily take a month with this kind of thing if you wanted to be very exacting , but few documentary people have the resources to do that . <p> Personally I 'd say 2 days is a minimum , 4 days a luxury , so somewhere in there . 43971 @qwx453971 <p> TV is worse . Many , many reality shows are being done in a day @ @ @ @ @ @ @ @ @ @ show . I 've had some that went to 1200 shots , easily . I have sometimes gotten the schedule pushed to two long days , but a lot hinges on client expectations . <p> I have done a few TV movies where we worked 32 hours without stopping , but those were unusual circumstances where we had a drop-dead air date we could n't move . I think I had that problem on the second V movie for Warner Bros. , and also the George Washington II mini-series , both of which were 4 hours each . We could have easily gone 2-3 weeks on them , but 1.5 days is all we had . <p> One thing I think is very helpful : ask them to give you time to go through the entire film and " prep " it , to at least throw down some temporary looks and do all the preliminary grading and matching . Once that 's done , then have the DP and director come in to set looks . At least that way , their relative boredom will be reduced and they @ @ @ @ @ @ @ @ @ @ I 've had many DPs come in and just be astonished at a ) the tediousness of the initial work and b ) the amount of time needed to whip the material into shape . I 'm lucky if the DP even survives one reel starting from ground zero . It 's much easier dealing with the creatives if you 're already about 2/3 of the way there . <p> Even for a low-budget project , if you can do broad strokes on all the material in an 80-minute piece in one day , then have them come in for two or three more days until completion , that can work . 
@@44332496 @4332496/ <h> Any of you use IRC , Slack , Hangouts , WhatsApp ? <p> Do any of you use apps like Slack , IRC , or Hangouts ? I recently installed Hangouts on my iPhone since a few people I work with prefer it , and it 's actually pretty nice . Like a lot of people , I use iMessage and email constantly for work , and was wondering if any of you have gained real , day-to-day traction using anything else . <p> Slack is pretty damn nifty . For small things that need to get taken care of on a larger job that takes place over weeks or months , Slack is great . One of the companies that brings me in for Dailies work uses it , and it 's really great alongside the endless stream of emails on a feature job . <p> God , the emails . I think I saw a spreadsheet at one point on this current gig , of at least 15 different daily emails that are sent out over the course of a day by production . " Which @ @ @ @ @ @ @ @ @ @ Oh god , the bear minimum . I need that one and that one . These other two will already be on the drives I get so please do n't send them to me in email . Please . For the love of all that is holy , please do n't . " <p> I still wake up to an average of 7 emails a day , a full 3rd of which are resolved before I wake up . With Slack , I walk into the office , see an orange bubble on the icon , and know to check it , read up on the conversation , see if I need to deliver anything of change anything . It 's nice because it 's Work at Work . It 's nice because there 's a link to a spreadsheet that gets updated throughout the shoot with out LTO status . It 's a nice little collection of those types of things and I do n't need to go rooting through my email to find them . <p> I definitely use hangouts very regularly . I only use IRC to @ @ @ @ @ @ @ @ @ @ pretty slick little IRC replacement for small teams , especially if they are n't technical enough to want to deal with something like IRC . Easy to setup , web based , decent mobile apps with notifications that work well enough . There 's nothing monumentally new or unique in there - but its a great implementation . <p> Haha , not yet , but a lot of sites seem to mention getting in touch via IRC . If you take a look at something like Kiwi ( https : //kiwiirc.com/ ) , IRC actually seems like it might have emerged from the basement into something more useful and mainstream . <p> God , the emails . I think I saw a spreadsheet at one point on this current gig , of at least 15 different daily emails that are sent out over the course of a day by production . " Which ones do you want to be on ? " " Oh god , the bear minimum . 43971 @qwx453971 <p> That 's what 's actually prompted my question . I 'm really weary of how overused email @ @ @ @ @ @ @ @ @ @ respond to anything important , while filtering through a wall of unnecessary noise . Things have become so bad , if an email is really important , you now get a text message saying that you 've been sent an email . <p> It seems like Hangouts has caught on with a lot of you guys so maybe that 's an idea worth exploring . <p> I have been trying to switch to slack but it 's tough when you need to combine internal and external communication . I see how it can work in organizations where people do n't communicate much directly with clients as we tend to do . Any anecdotal experience on rolling out slack in a small organization is welcome . <p> Usually I only use email - Whatsapp , iMessage and direct phone call only for emergencies . However , you can reach me on my email pretty much as fast as on whatsapp . I only open Skype if I have an " appointment " there . But that 's pretty much all I use <p> I have been trying to switch to slack @ @ @ @ @ @ @ @ @ @ and external communication . I see how it can work in organizations where people do n't communicate much directly with clients as we tend to do . Any anecdotal experience on rolling out slack in a small organization is welcome . 43971 @qwx453971 <p> I think Slack shines for Internal things , or maybe " Internal+ " like my situation , where I 'm 1 person brought in as a dailies op , so I 'm in on the Slack conversation , but no one else in Production is . In such a use , it does n't eliminate all email , but it does cut down on it . <p> We switched to Slack about a year and a half ago for a team that expands from a core of six up to about a dozen . Prior to that , we were all on Hangouts , but Slack allows a lot more organization and integration than Hangouts ever did . <p> There are a lot of " little " standard features that make a big difference versus a less-focused instant message system . We organize conversations by projects @ @ @ @ @ @ @ @ @ @ on top of important information and follow-up items . Search , file previews , reminders , good keyboard shortcuts , Do Not Disturb mode ... there 's actually quite a lot of functionality packed into what seems on the surface like a very simple idea . <p> A lot of Slack functionality comes from third parties , too . We use a service called GeekBot which provides a chat-bot that conducts Agile-style standup meetings ; this has really helped get our **26;74;TOOLONG team stay on the same page . We use off-the-shelf integrations with Dropbox , Google Drive , and Trello , and we 've developed a few of our own integrations via Zapier . <p> We have entirely replaced internal email with Slack . We have greatly reduced email with clients and associate vendors also on Slack . It was a little hard to get people started on Slack , but once they got going , it did n't take long at all for them to find some good value for themselves . When all else fails , just show them /giphy . <p> It sounds goofy and salesy @ @ @ @ @ @ @ @ @ @ moved there , I 've started thinking of Slack as almost the OS for our business . 
@@44332497 @4332497/ <p> There 's no option in Scratch to grade groups like in Resolve , however you could create a collector of the clips , grade that , and then save that grade and apply to clips . You could also grade the output ( like a timeline grade in resolve ) , save that grade and copy to clips . <p> There 's no option in Scratch to grade groups like in Resolve , however you could create a collector of the clips , grade that , and then save that grade and apply to clips . You could also grade the output ( like a timeline grade in resolve ) , save that grade and copy to clips. 
@@44332498 @4332498/ <h> House of Cards ( yellow &amp; blue ? ) <p> I finally caught up to the Netflix series House of Cards , and I 'm enjoying it very much . They do a lot of very unusual and distinctive things with composition and lighting , and it 's an amazingly well-done show , absolutely feature-quality work for TV . <p> I did catch a few critics on the web chiding the show for pushing kind of a " yellow/blue " look throughout . I see what they 're talking about , but it does n't bother me at all , and to me it kind of enhances the storytelling . I 'm a little more taken aback by the kind of non-standard 2.00 aspect ratio , but eh ... you can get used to anything . <p> I can see why the show has done so well with the various awards over the past couple of years . Spacey is pretty amazing to watch , and all the technical details -- art direction , lighting , camera movement , sound , etc. -- are really fantastic . The @ @ @ @ @ @ @ @ @ @ 's a legitimate creative decision like anything else . I think it 's an extraordinarily well-crafted show , a really clean look ( all shot on Red in 4K ) . <p> i think is the best looking show out there ... first time ever ( for me ) that something shot on RED looks perfect as is and ( still in my opinion ) would have not been better in none of the other available media ( film , or Alexa etc .. ) off course art department and lighting are outstanding and big part of the look , but i feel that the color correction work is impeccable most of the times ( impeccable as in " works perfectly for the show ) , sharp , silky , dark , rich clean i really like i wonder how come is so clean and silky , feels like every shots has been denies post grading ... g <p> I was wondering what lenses do they use on S03 as it seems pin sharp in the front but back feels like anamorphic . <p> S03 seems even more fine tuned @ @ @ @ @ @ @ @ @ @ hand in hand and some shots with talking head in the front seem not lit at all but they are but in the minuscule scale . Looks like medical precision . <p> i think is the best looking show out there ... first time ever ( for me ) that something shot on RED looks perfect as is and ( still in my opinion ) would have not been better in none of the other available media ( film , or Alexa etc .. ) off course art department and lighting are outstanding and big part of the look , but i feel that the color correction work is impeccable most of the times ( impeccable as in " works perfectly for the show ) , sharp , silky , dark , rich clean i really like i wonder how come is so clean and silky , feels like every shots has been denies post grading ... g 43971 @qwx453971 <p> Have you seen new Hannibal series . Shot also on red and it looks amazing to me . <p> question : since i basically only watch things on Blu @ @ @ @ @ @ @ @ @ @ quality these days is very very close to blu ray quality ) and i do n't have cable for the above reason , does anybody knows if the quality of ON-DEMAND stuff is the same as when is aired ? i am asking because things such HBOGO look terrible , SHOWTIME ANYTIME looks bit better but still Bad .. etc ... so i wonder if i subscribe for cable and i watch things on demand because already aired ( let 's say " the knick " ) , i wonder if that would look as good as when aired or like HBOGO or SHOWTIME ANYTIME ... i have a 58 plasma at home so compression quality is really pronounced ... <p> and for some reason , watching in HD season 3 looks quite richer shaper and cleaner than the previous 2 seasons ... perhaps has something to do with the 4K finish and the way has been encoded ... 43971 @qwx453971 <p> I have n't read into the production of Season 3 , but I am assuming that S01 and S02 were shot on Red Epic MX , and S03 @ @ @ @ @ @ @ @ @ @ the timelines of when the seasons started filming versus when Dragon was released , the dates line up . I know S01 and S02 were the MX sensor . <p> I 'm 7 episodes into S03 , and it 's much darker than S01 and S02 . A lot more characters in silhouette . Not a complaint , mind you , just an observation . Dragon is billed as being better in low-light , with a higher dynamic range , so this plays into my bet that S03 was shot on Dragon . <p> Yes , Dragon was used in S03 . And , no , Dragon is not billed as better in low-light . Especially if STH OLPF is used . Dragon handles better the highlights rolls off , especially with STH OLPF and it has a better DR , than Epic . Dragon is a better overall camera , but frankly it 's not dramatically better , as it was initially indicated- 20 stops of DR , " ASA 1600 is the new 800 " , etc ... <p> Before we start throwing bets around , let 's @ @ @ @ @ @ @ @ @ @ Regardless of his role- director or a producer , he 's still the most controlling professional out there . Nothing he does is done by happenstance , every decision is very deliberate . He , along with DP is very much hands on with the color grading . They used PIX to exchange the notes . So , in his past films , that were shot on Red , starting with Social Network , he never used HDRx . How do we know that ? Because on his most recent film- Gone Girl , HDRx was never mentioned and even if Yan wanted to use HDRx , he could n't , because the conformed material was delivered for grading as DPX . And finally , it is a known fact , that all grading of 13 episodes was done in two weeks . There is not enough time to futz around with HDRx . Besides , based in the information from many sources , that show has anything they could possibly want . If they needed to light up the whole city of Washington , they could . There is @ @ @ @ @ @ @ @ @ @ done in camera . <p> I was wondering what lenses do they use on S03 as it seems pin sharp in the front but back feels like anamorphic. 43971 @qwx453971 <p> No , they 're shooting in 5K and reframing in 4K , which is the standard Fincher post workflow . We would 've also seen anamorphic lens flares if they had been shooting with those lens . I got ta say , this is a pin-sharp show , though the depth of field is very shallow , indicating they 're shooting at fairly low light levels , wide open ( like F2.2 or even lower ) . <p> As to HDR , it does say : <p> " Colourist Laura Jans-Fazio was able to use the HDR to get real detail in the window and composite that into the overall picture to achieve a more natural result . " <p> I suspect this might have been a one-shot thing , so it 's not like they used it throughout the show . I find it 's often possible even with a 3- or 4-stop difference to use windows and @ @ @ @ @ @ @ @ @ @ bring down the background . If the windows are blown out further than that , they 're going to either have to use HDR , filter the windows , or use VFX to replace the windows . I agree with Jake : to me , HDRx is too much trouble , and I 've seen issues with motion blurring in certain kinds of shots . I think they can hold detail if they just light the scene in such a way as to control the dynamic range in terms of illumination . I rarely ( if ever ) run into this problem with Alexa. 
@@44332499 @4332499/ <p> But garbage in , garbage out , right ? You ca n't magically create more detail ( or more colour data ) in an image . Well , you can " Google produced detailed face images from pixellated source images " but philosophically it is no longer the same image . When a film is cropped for TV broadcast , or you receive a blocky low-bitrate stream from Netflix , or Flickr changes the JPEG profile on an uploaded photo ... are those the same image as the **28;102;TOOLONG intended ? Or are they different ? <p> Does it even matter ? If you 're a broadcaster with a ton of archived SDR footage and millions of colour-thirsty potential customers who might pay for a special HDR channel , surely the only question is whether it 's technically possible to convert SDR content to HDR , and whether that converted footage is subjectively enjoyable to viewers . Remaining objectively faithful to the original is just an added bonus . 43971 @qwx453971 <p> Added bonus : attach a generator to the grave of every deceased cinematographer . With all @ @ @ @ @ @ @ @ @ @ solve the world 's energy crisis ! 
@@44332500 @4332500/ <p> Umedia is an international film group specializing in the production of international feature-film projects . Its activities include production , co-production , financing ( through internal funds ) and visual effects ( VFX ) . It has offices in Los Angeles , London , Paris and Brussels . As part of its development in North America , Umedia is launching a Post-production studio in Vancouver , focused on visual effects , animation , color finishing and mastering for feature films and platform TV series . <p> To support this new entity , Umedia is looking for a Head of Finishing . By joining us , you integrate an international group continuously striving for excellence thanks to a strong corporate culture and values . <p> YOUR MISSION You will be a key player in securing digital intermediate and other finishing projects for the studio and managing those projects through the facility , supervising the finishing team on a day to day basis . You will ensure that the best practices and the utmost quality are maintained , ensuring that client satisfaction is always at its highest . You will @ @ @ @ @ @ @ @ @ @ plan and establish long lasting client relationships . You will create and carry out strategic plans , short to long term , for future growth and development , which lead to increased revenue . <p> Your direct responsibilities include : <p> Strategic sales planning for the finishing department . <p> Day to day management of the department , including hiring new staff members and supervision of the team . <p> Implementing and maintaining procedures , best practices and best workflows for all finishing services from dailies through delivery . <p> Working closely with the Head of VFX , develop strategies for packaging services across the business to increase the companies ' competitiveness in the global market place . <p> Coordinate with Brussels , Paris , London and Los Angeles offices on collaboration . <p> Oversee the finishing department 's budget and financial planning . <p> Collaborate with Head of Marketing on marketing strategies . <p> YOUR PROFILE We 're looking for someone who is full of charisma and charm , who is proactive and has a positive " can-do " attitude . <p> You have at least 10 years experience @ @ @ @ @ @ @ @ @ @ film DI finishing and delivery . <p> You have at least 5 years leadership experience in a well-respected globally recognised post-production facility , having full responsibility for at least 20 staff and the department financials . <p> You have a proven track record for leading a picture post-department to increased revenue and growth . <p> You have solid experience with the US feature film and TV market , with established relationships at the studio level . <p> You have previous experience in sales and marketing at an international level . <p> You have a deep knowledge of post production workflows and technology , covering all aspects of picture post-production from dailies through to delivery . <p> You have a solid knowledge of VFX and the integration of VFX during the finishing process . <p> You have first hand experience with leading innovation within digital post-production technologies . <p> You are autonomous , rigorous and demonstrate an entrepreneurial mindset with a high sense of responsibility.OUR OFFER We offer an attractive package and a position within a fast growing environment with clear career development opportunities . Salary range is $150,000 - $170,000 @ @ @ @ @ @ @ @ @ @ benefits are paid for by the company for you and your dependents and includes dental , drugs &amp; health . 
@@44332501 @4332501/ <h> Moving Large Amounts of Data <p> I am doing some testing for very basic DIT duties on an feature length indie shoot , 11-12 days , in about 6 weeks . Mostly single camera ( BM Production Camera ) ProRes HQ , 4K . Recording to 256G and 480G SSD 's . DP would like to offload both audio ( 24bit , 96K , 3-4 tracks from Zoom H6 ) and video by scene , then copy the days ' work to secured storage at 2 locations each night . See images below for what I am suggesting . Ingest is on the left , a Convergent Design USB3 card reader that can read about 500MBS . Using Shot Put Pro 6 on a MacBook Pro , I am transferring simultaneously to ( 2 ) 1T SSD 's , using the XXHash 64 Checksum verification . The 1T SSD 's are in a OWC disc caddy via Thunderbolt . After ordering the OWC unit I was disappointed to find that it only contains 1 SATA controller for the 2 slots , resulting in a 250 MBS write speed @ @ @ @ @ @ @ @ @ @ the two in the caddy . Is there a better way to write to two discs simultaneously that I have missed ? I 'm getting close to 1:1 copy time , that is , 30 minutes of footage takes about 30 minutes to transfer ( getting 250-280MBS ) which means the tech will have time to do the copy then review files briefly , be sure they open in QT 7 for example before we move company . Then the ( 2 ) 1T SSD 's will go to two different locations to be backed up over night . On my end storage is to a RAID5 SmallTree 8T Server that gets backed up to a NAS ( also Raid 5 , 8T ) every 24 hours . Files checked again before SSD 's get formatted for the next day . If there are any corrupted files we could go back to shooting SSD 's in the AM and off load again before format for the days work . Comments and suggestions welcomed . <p> Black magic SSD 's are often long to copy , do to a lack of @ @ @ @ @ @ @ @ @ @ there is only one SATA bus un your backup " toaster " , I would recommand to move a faster storage solution . Two drives with each one a Thunderbolt connector would be faster . There is no way you can run a 500mb/s two bakcup with that setup , specially coming from there Black Magic SSD 's Card . <p> Black magic SSD 's are often long to copy , do to a lack of good card reader . Well in my opinion . If there is only one SATA bus un your backup " toaster " , I would recommand to move a faster storage solution . Two drives with each one a Thunderbolt connector would be faster . There is no way you can run a 500mb/s two bakcup with that setup , specially coming from there Black Magic SSD 's Card . 43971 @qwx453971 <p> Are BM SSD 's slower to read ? I was testing with my Convergent Design SSD 's and get a steady 500MBS out with the card reader pictured . I get 500 MBS from the CD SSD to the Samsung 1T @ @ @ @ @ @ @ @ @ @ add another drive to the OWC and copy to both simultaneously that I drop to 250MBS ( which is not terrible for this workflow ) . <p> I looked for an SSD T-Bolt drive with additional T-Bolt connection for daisy chaining but could find none . That would mean going to a T-Bolt Hub and connecting each receiving T-Bolt SSD individually , or copying to T-Bolt RAID 0 drives with HDD 's , no ? <p> Are BM SSD 's slower to read ? I was testing with my Convergent Design SSD 's and get a steady 500MBS out with the card reader pictured . I get 500 MBS from the CD SSD to the Samsung 1T if doing a single copy . It 's when I add another drive to the OWC and copy to both simultaneously that I drop to 250MBS ( which is not terrible for this workflow ) . 43971 @qwx453971 <p> I do n't  think that their SSD are slower than others , its just that a USB 3 reader is often not enough . If the camera runs a lot on a day , @ @ @ @ @ @ @ @ @ @ often really long . <p> I guess it 's normal that the 500mb/s is split in half with only one Sata Bus on the toaster . If you have two seperate drives with each is connector , you should be able to get a better speed transfer . But hey , 250bs/s can be ok for a short shoot like this , given the fact that you have only one camera to offload . <p> A 20Gbps Thunderbolt dock splits the bandwidth among the USB 3.0 connections , so theoretically you are able to attach 4 SSDs via USB 3.0 like you did with the 256G drive pictured above . All 4 SSDs would transfer at full speed at around 500MB/s each . <p> I do n't  think that their SSD are slower than others , its just that a USB 3 reader is often not enough . If the camera runs a lot on a day , downloading 200-300Gb of Data with a USB 3 reader is often really long . 43971 @qwx453971 <p> I actually did testing with a bunch of USB card readers like the Atmos readers @ @ @ @ @ @ @ @ @ @ cabling again- supposedly USB3 , and only got 250-280 MBS read . After conversation with Mitch Gross at Convergent Design I was advised that just because a reader has a blue USB3 cable does n't mean it has a controller inside that can do a full speed read of an SSD at 500MBS . He recommended the CD brand reader , a deal at $53 , and moves data consistently over LONG ... <p> I actually did testing with a bunch of USB card readers like the Atmos readers that had blue USB3 connectors , and some other SATA cabling again- supposedly USB3 , and only got 250-280 MBS read . After conversation with Mitch Gross at Convergent Design I was advised that just because a reader has a blue USB3 cable does n't mean it has a controller inside that can do a full speed read of an SSD at 500MBS . He recommended the CD brand reader , a deal at $53 , and moves data consistently over LONG ... 43971 @qwx453971 <p> The reason is some USB 3 to SATA adapters use SATA 3Gbps bridge , which @ @ @ @ @ @ @ @ @ @ the CD brand adapter uses SATA 6Gbps to USB 3.0 interface . <p> A 20Gbps Thunderbolt dock splits the bandwidth among the USB 3.0 connections , so theoretically you are able to attach 4 SSDs via USB 3.0 like you did with the 256G drive pictured above . All 4 SSDs would transfer at full speed at around 500MB/s each . 43971 @qwx453971 <p> Sorry I was unclear . Yes a TBolt Dock with 2-3 USB3 buses , if discreet , would work . It looks like a lot of these devices take a single USB3 bus and split it over several outlets . I will do a little research in that area . <p> Thanks for the outline above . SilverStack looks like a great tool . I might look more into that . <p> The fastest read/write from the Samsung EVO 1T SSD 's seems to be about 500MBS , so I do n't see what advantage I would get from more elaborate dock arrangements and 10 or 20 GBS Thunderbolt bus . I think I will order in the USB3 Hub and 2 CD USB3 cables and @ @ @ @ @ @ @ @ @ @ 
@@44332502 @4332502/ <p> I was as gung-ho as anybody about Macs for video post over the last 20 years , but lately I 've been very bummed-out by their current lackadaisical attitude towards pro audio/video/design users . When the next upgrade cycle comes round , I 'll probably ( reluctantly ) migrate to Windows and only keep a Mac around for rendering ProRes . <p> Of course i 'm probably completely wrong ... But I wonder if they are going to upgrade their professional line because that 's what you 're going to need to make VR content . Of which I believe they want to be a big player . Of course it will only work with VRPRORES . <p> I have a feeling if we do n't get an update on it on June 11th at WWDC it 's a sign they 've abandoned the macpro. 43971 @qwx453971 <p> People have been saying this every year for the last 6 years or so . It 's been 888 days since the last time the MacPro was updated . Making it the longest serving configuration in the history of the @ @ @ @ @ @ @ @ @ @ nMP was an after thought , they were going to scrap the whole line , until that Facebook group got 500K likes and they reconsidered . Announcing the nMP before it was close to ready , then taking 8months to actually ship it while they worked out manufacturing . The product they shipped was faulty and many still suffer from a GPU issue . The design itself is less than ideal . Apple still makes very fine systems for many pro applications . And for editorial and lightweight post even the iMac is a stellar machine . But for heavy duty finishing ... <p> I was as gung-ho as anybody about Macs for video post over the last 20 years , but lately I 've been very bummed-out by their current lackadaisical attitude towards pro audio/video/design users . When the next upgrade cycle comes round , I 'll probably ( reluctantly ) migrate to Windows and only keep a Mac around for rendering ProRes. 43971 @qwx453971 <p> Yup . Considering the debacle with the D700 's and the current form factor of the trashcan I am literally ordering a Supermicro @ @ @ @ @ @ @ @ @ @ it 's absolutely useless if the hardware sucks . I 'm not naive , I know there will be issues - there 's always issues . I just ca n't run a business reliably with a Mac any longer . See you on the other side ! <p> Apple 's approach to soldered components and long update cycles makes it a very dangerous investment for any post production setup . On one hand I 'm happy that Apple is dropping Prores for Windows , seems like it 's forcing us to move on . <p> If we do n't see the nmp I 'm probably going to Windows . Then again I could get an old mac pro with a cubix and a bunch of nvidia 1080s ? I mean , those systems still crank through red files and Alexa footage pretty solidly ? <p> when the MacSemiPro came out I did the maths on a spec for spec machine . If you go the refurb z820 route you can get the same spec for around half the price , not much more and you get more , faster storage @ @ @ @ @ @ @ @ @ @ . I just got very annoyed at a friend 's husband who bought a mac pro ; I mean he has kids to feed ! I worked at quite a high end branding company recently working in 4k and 5k on mac pro 5.1s and it was a nightmare . They were downressing to ProResProxy and it was still like walking through treacle carrying a Buik and they just could not preview the work properly . Get out while you can . It 's pretty easy . <p> I recently switched to a supermicro Win10 based system from a MP5.1 . I 'd say overall the only thing I really miss is being able to preview videos with spacebar . It also sucks to have to batch hex edit prores XQ footage ( ap4x to ap4h ) <p> Honestly , I 'm a mac guy going back to 1984 , and Apple II before that . I loathe windows . But we 've been inching towards being a completely windows based shop for years now . At this point , there are three macs in house : two identical early @ @ @ @ @ @ @ @ @ @ typing this on , on my desk . I much prefer to use the mac for my day to day stuff , so the next , and possibly last Mac upgrade we do here will most likely be to this imac . The Mac Pros run FCP 7 , and are wired up to our decks ( HD and SD ) , primarily for capture and tape layback , and for editing at resolutions of 2k or less . <p> Windows 7 is solid , reasonably reliable , and not at all crash-prone like XP or earlier versions were . All our Windows machines are on Win7 . Any new Windows boxes we build cost us about $1200 , and that 's for an X99 motherboard , i7 CPU , SSD system drive , gobs of RAM , decent GPUs and at least a small RAID , depending on what the machine is for . We have several with this spec now , and they 're super reliable , cheap to make and we can have a common system drive clone on the shelf just in case something goes totally @ @ @ @ @ @ @ @ @ @ Windows for the way it looks and its general clunkiness , it 's been a reliable and steady OS for a while now , and there 's plenty of room to tweak , unlike the Mac . It makes me sad , but Apple is about phones , watches and laptops now , little else . <p> It 's not as if it 's a strictly binary choice . A stable OS X set-up is not dependant on Apple hardware . If you wait for the solution to come to you then you will always ultimately be beholden to the decisions and endeavours of other parties . Actually make an effort in the direction of the solution and you might surprise yourself . <p> It 's not rocket science , and you do not need to be a computer engineer to make it work . The only thing stopping people is fear ( specifically relating to their capabilities ) , which is an especially prevalent and unfortunate trait in the colourist world . If you try and fail then you 're no worse off than you are now , but @ @ @ @ @ @ @ @ @ @ or not will be no more than a minor concern . You will already be sorted . <p> Apple makes 64% of their profit ( that 's net , not gross ) on phones . This does not include iPads and iPods and iWatches and the App Store . Only the phones themselves . It may suck to be us , that rely on the upper end of the spectrum , but damn , if you were them , where would you put YOUR effort ? <p> I 'm hoping we get a refresh at WWDC . That 's only in a few weeks . Bear in mind , prior to the nMP being announced , there was a significant lag in updates to the previous gen . If you look it up , it does n't appear so , but the latest refresh was just , I believe , a CPU bump . I remember being anxious for a refresh for a long time back in 2011 and 2012 , eagerly awaiting TB1 and USB 3.0 . I just last week finished a dailies gig using the older tower @ @ @ @ @ @ @ @ @ @ , we could have avoided almost all of it with a few ThunderBolt 2 ports and a few USB 3.0 ports . Our slots were taken up with connecting mini-SAS to LTO drives , a 16-bay RAID , and an eSata fan cable . Oh , and the SDI out . Could easily do everything we did without needing any PCIe slots on a newer setup . In my world , if it were n't for the massive reliability issues I 've had with the D700 's , the nMP is actually pretty spiffy . . . <p> And sure , someone mentioned that you can get a spec-for-spec PC for about half the price . But take a look over in the Scratch section of LGG : Chris Ratledge is getting about half the expected performance rendering Dailies to DNxHD . If half the cost = half the real world performance , what have you gained ? <p> I do feel abandoned by Apple in my professional work . But 1 ) if I were them , I would do the same . And 2 ) what else is @ @ @ @ @ @ @ @ @ @ alternative , at the very least for DIT/Dailies work given Chris 's recent months-long troubleshooting . And I was asked to troubleshoot my cousin 's Windows computer for some weird issues and found that their laptop was doing some particularly funky ghost-in-the-machine stuff . So I hear that newer Windows OS 's are better than they were compared to when I abandoned them in the XP/Vista days , but I just . . . it 's like burning your hand on the stove , and then going to rest your hand on the burner again . <p> So I guess that leaves Linux . Which is much more customizable and controllable , but that means that when you mess something up , you might be on your own . I 'm often " on my own " out in the field anyway , but I usually at least have an internet search on my phone . Usually . <p> Add to that that things like Pomfort SilverStack is OSX only , and I 'm pretty locked in for the foreseeable future . Again , the needs of a DIT are @ @ @ @ @ @ @ @ @ @ I could just get my D700 's to stop kernel panic-ing , I 'd be in no rush for a refresh of any sort . 
@@44332503 @4332503/ <h> How to solve Moire issues in post <p> I am currently working on a film where we have run across a few moire issues . Wardrobe seems to be using a lot more striped shirts than usual which we have advised against , and the budget has kept us from getting any diffusion filters for the camera . Regardless of having the diffusion filters , I was wondering what the best practice for removing moire in post was . I have found a plugin for FCP , but we work in Premiere and Resolve . Obviously we need to do everything we can in production to avoid this issue , but it 's not always possible . With that being said , what have been your professional experiences dealing with this issue ? Thanks <p> I 'm looking on a 23 inch computer monitor at the moment . I will have to look tonight on a larger screen as well as use my computer at home to look at it on a broadcast monitor . I do n't have that capability on set . I am just labeling @ @ @ @ @ @ @ @ @ @ Wonderful . Thank you for the information . Just wanted to make sure we had a plan now before we get to post and run out of time to deliver it properly . I 'll definitely check tonight and see what I find out . In the event that it is there , or I ever run across footage in the future with moire , what is the best practice to remove it ? <p> I have to deal with moire constantly as much of my work now involves patterns and textures . Color NR helps . The best tool I will use if I absolutely have to is the color NR in Lightroom but that 's time consuming . Reducing sharpness at the offending frequencies helps as well . <p> Juan is correct in that viewing at 100% is critically important when diagnosing moire issues . <p> I am currently working on a film where we have run across a few moire issues . Wardrobe seems to be using a lot more striped shirts than usual which we have advised against , and the budget has kept us from @ @ @ @ @ @ @ @ @ @ <p> Diffusion filters are like $5/day to rent . What 's the problem ? <p> Three observations : <p> 1 ) the production needs to shoot tests prior to the start of filming so they can look at costumes and makeup and make sure they 'll work on screen . Tight patterns , stripes , and certain colors cause problems on many formats , particularly on broadcast TV . <p> 2 ) lots of this stuff ( but not all ) will show up on the monitor on set , so that might be a good warning of trouble to come . Have replacement costumes available for a quick swap-out . I have worked on many , many commercials where they 'll get halfway through Take 1 and somebody will say , " lose the tie ! Too much moire for TV ! " And they bring out another tie . Does n't even take 3 minutes . <p> 3 ) As a last resort , you could draw a PPW around the coat or shirt in Resolve , track it , then pull an appropriate luminance key and use @ @ @ @ @ @ @ @ @ @ , but it could take the edge of it . <p> My memory is that when I worked on League of Their Own , we had tons of moire problems in standard-def ( especially letterbox ) with the bleachers , because it was tons and tons of horizontal shapes in the BG . We wound up defocusing that and using split-screen to combine it with the foreground material -- sort of the primitive early-1990s version of a defocus power window . So this problem has existed going back many years . <p> Another great way to get rid of moire is to blur channel A or B ( sometime both ) in LAB . That way , you wo n't affect the sharpness of the image . If you got luminosity moire as well , it is good to know that applying an inverted version of channel A or B in softlight over channel L is usually enough to knock moire down while preserving a lots of details . <p> That being said , the first part can be accomplished very quickly , and wo n't need much adjustments from @ @ @ @ @ @ @ @ @ @ pretty time consuming sometimes , and will definitely involve keying and power windows . 
@@44332504 @4332504/ <h> Alpha out of Resolve <p> I-ve read a lot that Resolve can not handle alpha output ... Is there a workaround for After Effects ? Maybe something like a white solid in the background and use this as a matte or something like that ? <p> I-ve read a lot that Resolve can not handle alpha output ... Is there a workaround for After Effects ? Maybe something like a white solid in the background and use this as a matte or something like that ? <p> i might set my key , when i was happy i would set exposure ( offset in Resolve speak ) to make the image outside the matte black , and then use the same control to make the image inside the matte white , then export as video flavor of one 's choice ... <p> i 've never done this as i have keyers i prefer in other software already , ( IBK in Nuke , Kak in Fusion , Primatte in DS come to mind ) so have not seen the need to export a key from Resolve , but @ @ @ @ @ @ @ @ @ @ this .... <p> In Resolve 12 you now have the option to feed the alpha from one node into the RGB input if another node so you can easily make and render a matte from a selection now . Not the same as adding an alpha channel to an export but using versions you could make a separate matte pass quite easily . <p> Create a new version of your grade . Find the node which you want to take the alpha from its selection and drag a connection from its alpha-out into the RGB-input of the node graph output , at the end of the graph . Now , you have two versions of the grade - one graded version and one showing the black &amp; white matte of your selection . Do this to any other clips you need and then take a look at the Deliver page options , if you select ' Individual clips ' you get some options under ' Use commercial workflow ' ... I think this will allow the clips to render each of your versions to a different place so that you @ @ @ @ @ @ @ @ @ @ a new version of your grade . Find the node which you want to take the alpha from its selection and drag a connection from its alpha-out into the RGB-input of the node graph output , at the end of the graph . Now , you have two versions of the grade - one graded version and one showing the black &amp; white matte of your selection . Do this to any other clips you need and then take a look at the Deliver page options , if you select ' Individual clips ' you get some options under ' Use commercial workflow ' ... I think this will allow the clips to render each of your versions to a different place so that you have a matte and a graded clip . 43971 @qwx453971 <p> Two years after the fact , can confirm that this works . Thanks for sharing Jamie . <p> I 've key 'd a few green screen shots in Resolve , and sending published EXR files for the Colourist to use in another session . Needs to have an external matte applied to match the @ @ @ @ @ @ @ @ @ @ helped me achieve this 
@@44332505 @4332505/ <p> " Here 's a series of reasons that the mac underperforms . Although you can kinda get away with it on smaller projects . By the way , PCs are faster and cheaper and we 'd love it if a mac was as modular . But it 's not . Conclusion : why we use macs is really an open-ended amorphous concept that I made no attempt to discuss in this article . I rest my case " <p> The amount of positive thinking and dear I say it - grasping for straws - in Michael Cioni presentation is so high it 's almost refreshing . I Love how He has a " Hooks for staying with apple and for leaving apple section " . I was intrested to hear how the hooks for leaving will look like but , that section never came - despite being represented graphically in the presentation . If this is due to editing and that section was presented during actual event , then the editor clearly had an agenda of his own Mr Cioni never directly gave an answer to the question @ @ @ @ @ @ @ @ @ @ think the answer might be " Yes " . Do n't  get me wrong , He had couple of good points but the amount of wishfull thinking for a Man with his kind of expertise is striking . It maybe , of course that he just knows things , and just pretends that this is his purely personal ( bystanderish ) point of view . Either that or he 's " Peter Shiff " of the Pro Desktop Computers market and Apple will dominate once again in couple of years . <p> I like my dual 12 core PC , it 's powerful and has a ton of slots . That 's where I do all my work day in and day out . <p> That said , I 'll be honest nothing beats the Mac for easy of setup and things just working . No fiddling with drivers and other nonsense . I just did a fresh install of OSX Sierra on my 5.1 macpro . Took all of 1 hour to be up and running again . Trying doing that on windows . <p> Sure the trash can @ @ @ @ @ @ @ @ @ @ with a ton of slots I may just get one ... 
@@44332507 @4332507/ <h> Out of gamut excursion errors <p> I was watching a video from Patrick Inhofer yesterday on correcting out of gamut errors before delivering for broadcast . I ca n't link to it as it 's a members only video on mixinglight.com . I do n't understand how can a digital rec709 deliverable have colours out of gamut , how is this possible ? He said that this happens when the signal gets converted to analogue . <p> In Rec709 , color values are not stored as RGB , they are stored as YCbCr - which is luminance , luminance minus blue , and luminance minus red . When converting to RGB for display , the Green channel is extrapolated from all three . <p> Because of the way that conversion from YCbCr to RGB works " it 's possible to have a set of values which ( while fine in YCbCr ) - correspond to RGB values which contain either negative values , or values above the upper limit - which pushes them outside RGB gamut . <p> Joey I think the problem happens when going to analogue @ @ @ @ @ @ @ @ @ @ . Besides , while the math is done in YRGB in Resolve , if the file is in rec709 and you export in rec709 there is no change ? I think the problem is in playout , when the signal gets broadcasted no ? <p> The problem happens when converting the signal to RGB , irrespective of analog or digital . The signal will eventually become RGB to be shown on a display . <p> There are legal values in Rec709 that do not exist when the signal is subsequently converted to RGB - so many broadcasters specify RGB gamut limits in their tech specs . <p> Their QC is n't measuring the signal in Rec709 - its measuring the Rec709 signal converted to RGB ( the same thing happens if you look at an RGB parade on your scopes of an HD-SDI signal - that original signal is *not* RGB , but it is being converted to RGB so it can be represented on that scope ) <p> That said - many broadcasters are now revising their specs for RGB gamut to allow minor violations , since they do @ @ @ @ @ @ @ @ @ @ signal - and can be a huge pain to legalize away , which comes with it 's own set of headaches . <p> Oh I understand . We are basically working with a 709 image through post but when this gets converted to RGB for the screen you get problems . But I do n't understand , if it 's not a thing of playout and analogue brodcasting , then why do n't we get problems when working with this material ? We are viewing on LCDs after all . I 'm just thinking that this is a broadcast specific problem ... <p> Any modern display will just ignore the out of range RGB values and be fine - Older CRT TVs had the potential to get bit wonky though , hence broadcasters requiring RGB gamut limits in their specs ( remember , when HDTV began - most TVs , even HD ones , were still CRT ) <p> These days - it really is mostly a non issue . In fact the EBU has changed their recommendation on this to be much more lenient then it was in the @ @ @ @ @ @ @ @ @ @ still need to be aware of it and follow their specs . <p> If you edit video material you have to make sure your color stays within gamut that is the main concern and that is regardless if you edit in RGB or YUV color space . You can use the vectorscope to see if your colors are within gamut . <p> I never heard of a problem where valid YUV Rec 709 gamma values get rejected for being outside of RGB gamut , that is just minimal . <p> The old NTSC standard did have a wider gamut but nobody uses that one anymore . <p> Rec 709 was partly a compromise between European and USA standards going from 601 SD to HD . <p> You can see - there are no extremes of color or saturation in that image . I just intentionally pushed the blue channel out of RGB gamut to demonstrate how legal YCbCr values can translate to out of gamut RGB values . <p> Yeah , Patrick Inhofer also said that these excursions are not necesseraly seen on a vectroscope . He used a nother @ @ @ @ @ @ @ @ @ @ kinda brutal . He used soft clip mostly and desat . But soft clip kinda messed up the image . So he advised for hardware legalizers . <p> Yup - Scopebox has the channel plots , which work great - Tektronix and Harris both have their own proprietary gamut displays as well . The mixinglight series on legalization is definitely a great resource with a lot of good info . <p> Just remember there is no single universal " broadcast legal " standard - so what some networks allow , others may not and vice-versa . Definitely consult your tech specs for whoever you are delivering to . <p> In Rec709 , color values are not stored as RGB , they are stored as YCbCr - which is luminance , luminance minus blue , and luminance minus red . When converting to RGB for display , the Green channel is extrapolated from all three . 43971 @qwx453971 <p> Although images are often stored in Y'CbCr in formats like ProRes or DNxHD , and transmitted as Y'CbCr over single-link 1.5G SDI , it is not accurate to say that " in @ @ @ @ @ @ @ @ @ @ . The ITU-R BT.709 ( to give it its full name ) spec defines RGB and Y'CbCr coding of image data . If you work in DPX , EXR , ProRes4444 or DNxHR , you may well be storing RGB image data . <p> Grading systems normally work in RGB , not Y'CbCr ( Resolve 's YRGB may lead people to think otherwise , but it 's still fundamentally processing in RGB ) . In theory , if you clamp your RGB output to 0-1 ( which generally happens automatically ) even if you deliver in a Y'CbCr format it should not produce any Y'CbCr values which map to out of gamut RGB . However , because Y'CbCr is 4:2:2 there are two Y ' values for every Cb and Cr value . This means that although the first Y ' value combined with the Cb and Cr values may map back to the original " legal " RGB values , if the second Y ' value differs significantly , it may combine with those same Cb and Cr values to produce out of gamut RGB . So an in-gamut @ @ @ @ @ @ @ @ @ @ transformed to 4:2:2 Y'CbCr and back . <p> Although images are often stored in Y'CbCr in formats like ProRes or DNxHD , and transmitted as Y'CbCr over single-link 1.5G SDI , it is not accurate to say that " in Rec709 , color values are not stored as RGB " . The ITU-R BT.709 ( to give it its full name ) spec defines RGB and Y'CbCr coding of image data . If you work in DPX , EXR , ProRes4444 or DNxHR , you may well be storing RGB image data . 43971 @qwx453971 <p> Yea this is absolutely correct - sorry , I was oversimplifying a bit and assuming source material encoded in YCbCr . <p> Media Composer actually has a pretty solid RGB gamut limiter - but its basically a hard clip , so you got ta be careful with it because if stuff is way out it can introduce its own set of image problems . <p> Media Composer 's limiter wo n't remove the kind of gamut excursions caused by the RGB -&gt; Y'CbCr -&gt; RGB process I describe above . These will generally be @ @ @ @ @ @ @ @ @ @ at all . But a very stringent QC could fail them . Fortunately tolerances are being relaxed in acknowledgement of the reality of the situation " see the recent EBU recommendation . <p> Just remember there is no single universal " broadcast legal " standard - so what some networks allow , others may not and vice-versa . Definitely consult your tech specs for whoever you are delivering to . 43971 @qwx453971 <p> My philosophy is to err on the conservative side whenever possible , when it comes to levels . This works about 99% of the time for me , but I do get caught once in a great while . I 'm big on not allowing credits above about 95ire -- I 've been doing that for 30+ years . <p> this is actually the only well the main reason my stuff fails at QC as you can , t see the problem in resolves normal scopes however . i may invest in that scope box . 43971 @qwx453971 <p> I tell you , Gavin , between Jason Bowdach and the MixingLight guys , they convinced me that @ @ @ @ @ @ @ @ @ @ changed my way of working , and I think it 's helped quite a bit . It 's a pain to set up , but once it 's set , it 's pretty rock-solid . <p> My philosophy is to err on the conservative side whenever possible , when it comes to levels . This works about 99% of the time for me , but I do get caught once in a great while . I 'm big on not allowing credits above about 95ire -- I 've been doing that for 30+ years . <p> I tell you , Gavin , between Jason Bowdach and the MixingLight guys , they convinced me that Scopebox was the way to go . It 's really changed my way of working , and I think it 's helped quite a bit . It 's a pain to set up , but once it 's set , it 's pretty rock-solid. 43971 @qwx453971 <p> Marc yeah i , m going give that a go when i , m back in the office . I hate sending things off with unknowns in them like @ @ @ @ @ @ @ @ @ @ helpful letting me jump in and run it thru the Tektronix until it clears <p> but i would rather be confident with it going out from my system that it was fine ... so it scopebox for me 
@@44332508 @4332508/ <h> I 'm tiRED of having to deal with Dragon footage . What am I doing wrong ? <p> For the last one year , I 've been shooting all my projects on RED Dragon . But from day one , I 've been having issues with noise . I 've been shooting in full 6K and have been doing everything possible to expose the sensor properly . But I 'm just not having any luck . Every project , even in afternoon daylight , turns up with a ton of noise , which , I then have to use neat video to clean it up . While neat video does a great job , it 's takes away the sharpness of the footage and that 's something I do n't really like . <p> I 'm at the end of my wits here . And could really use some advice on what I 'm doing wrong . I 've attached a screenshot from my recent music video . I 've attached the raw , grade and cleaned-up version of the footage . This was shot in 320 ISO @ @ @ @ @ @ @ @ @ @ to do all kind of gymnastics to get the footage to where I 'd like it to be , it was not a pleasant experience in post . After grading , the footage looks like it was shot on my A7S at 10,000 ISO . What 's the point of hiring this monster of a camera , if the output is indistinguishable from the likes of A7S ? I 'm sort of positive that I 'm the one who 's doing something wrong and I really need to figure things out before my next project . Any help will be immensely appreciated . <p> The DPs I 've worked with generally advise that they rate the camera lower than the spec ( 800 ISO ) . If you rate it at 600 or even 500 , you 'll provide a fatter histogram , more signal , and ultimately less noise . You still have to watch the key-to-fill ratio and expose for the highlights while still providing enough illumination for the shadows . <p> What do the histograms look for the Raw , say in DragonColor and RedLogFilm ? It @ @ @ @ @ @ @ @ @ @ shot that just needs more light and more exposure on set . <p> Post snipped R3D frame and then someone may have a look at it . Obviously , applying NV to ALL your material is pretty insane and there is clearly something wrong . All Red cameras are capable of producing wonderful images . <p> The DPs I 've worked with generally advise that they rate the camera lower than the spec ( 800 ISO ) . If you rate it at 600 or even 500 , you 'll provide a fatter histogram , more signal , and ultimately less noise . You still have to watch the key-to-fill ratio and expose for the highlights while still providing enough illumination for the shadows . <p> What do the histograms look for the Raw , say in DragonColor and RedLogFilm ? It could well be that what you have is an underexposed shot that just needs more light and more exposure on set . 43971 @qwx453971 <p> That DP needs to go back to school , pronto . Exposing electronic cameras is not the same as exposing film . Making @ @ @ @ @ @ @ @ @ @ especially Helium . All those cameras are very different , even from the same manufacturers , like Red . Helium , Dragon , Mysterium-X- they all different sensors and they require different type of exposure . On top of that , there is a need to vary your ISO setting , depending on lighting conditions . You may want to rate your Helium at , say , ASA 2000 or higher while shooting outside , as it is known to easily clip the highlights , where Dragon is better at handling brighter exposures , but being more noisy in the dark regions of the image . The bottom line , DPs need to spend time testing the hell out of their cameras ... <p> Very bright man , delightful to deal with , and a guy who understands the limits of digital images better than most people I 've dealt with . <p> I totally agree : camera tests are a huge part of any production , and they have to go through and check the limits of the cameras under different battle conditions to get a grasp on how @ @ @ @ @ @ @ @ @ @ . <p> Very bright man , delightful to deal with , and a guy who understands the limits of digital images better than most people I 've dealt with . <p> I totally agree : camera tests are a huge part of any production , and they have to go through and check the limits of the cameras under different battle conditions to get a grasp on how far they can push things and where the pictures break . 43971 @qwx453971 <p> I do n't care if it 's Roger Deakins himself said that . it 's just flat wrong to rate camera at fixed ASA settings . It 's not film . Somehow , I suspect something got lost in translation , as David is an amazingly talented and experienced DP . Rating something like Helium camera for bright outside for a " fat negative " at ASA 500 is a recipe for disaster . Can you say clipped highlights ? What is an ASA 600 anyway ? Should it be at least ASA 640 ? <p> As we all know , the iso setting in red and other @ @ @ @ @ @ @ @ @ @ signal in Rec709 . I usually rate cameras at their so called native iso , if I want to overexpose , I overexpose , If I want to underexpose , I underexpose . Much easier to see what 's going on . For me the only reason to use this option is to help the people on set see an image closer to what it is supposed to look like after grading or for some reason I expose via the Rec709 image Rec709 waveform or Rec709 histogram and want to constantly over- or underexpose . But in most cases that would only make sense if the scenes I 'm shooting have a constant contrast ratio . If they do n't you might have to decide shot by shot , at least if your goal is to create the " thickest " digital negative without clipping anything ( or just the parts you decide you want to clip ) . <p> but ... even if someone does n't care about precise exposure ( because he wants to upset his colorist , does n't use a meter and has never heard of something @ @ @ @ @ @ @ @ @ @ the Dragon in its native iso and push record as soon as it looks good in the viewfinder/on the monitor with some kind of Rec709 LUT in place there is no way you get that kind of noise . Or that image was heavily underexposed or there is some hardware issue/postproduction error . <p> Pretty common for Dragon footage . At least the noise is mostly monochromatic , consistent across the tonal range and not too coarse . You 'd appreciate this if you had ever dealt with Sony RAW from F5/FS7 . <p> To me , it 's totally understandable that some cinematographers underexpose a bit when shooting RAW to protect highlight . <p> Just make sure you enable D.E.B and use the highest quality debayer setting . If the noise were profiled properly in Neat Video , there would be zero degradation in terms of sharpness and detail . <p> Ok ... As of now , DeBayering is not part of my workflow . I 've never seen a use for this step in particular as Resolve 's Debayering is pretty good . But if DeBayering in RCX @ @ @ @ @ @ @ @ @ @ As of now , this is my workflow . <p> 1 . I get all the . R3D files and put it on a single master hard disk . 2 . I Import all the footage , every single take , into premiere and let it create proxies . ( which takes about 4 days ) 3 . I then edit . And once the edit is closed , I export an XML from premiere . 4 . I import the XML into my Davinci Resolve and do some fine adjustments , like re-framing or speed ramping. 5 . I then grade it and take an output of 3840x2160 . This is my master color file that I take into premiere . 6 . I import my file into premiere , apply Neat Video , titles and transitions , if required , and export . <p> That 's about it . <p> So ..... obviously i 'm not doing something that I 'm supposed to be doing . Yes the Dragon I shoot with has the Skin Tone OLPF but I have no idea what DEB is . Anyway , @ @ @ @ @ @ @ @ @ @ know what you guys think . <p> I 've been having many issues with RED cameras but the main problem I 've noticed is OLPF . By default the RED camera has one for avoiding clipped highlights but if you are shooting dark enviroments you ought to change it to the lowlight OLPF . Just my advice . <p> I am only looking at it on a crappy laptop , and given the environment I would expect some noise , you 've exposed to the very lower end of the image . However I have a strong feeling your issues are with your proxy workflow . <p> If I were to hazard a guess , you probably have your r3d debayer settings set to half res , not full resolution premium . In my experience lower debayer settings really bring out the noise and I believe it 's a large contributor to people thinking it 's a noisy camera . <p> It 's pretty easy to see for yourself , open the clip in the redcine player and change the debayer settings from full to 1/2 . <p> Some things @ @ @ @ @ @ @ @ @ @ the the proxies you 've created in premiere ? If so what are your debayer settings and proxy file settings ? 2 ) If you 're re-linking to r3d files in resolve , what are your default debayer settings ( project settings &gt; camera raw RED ) , and on export are you ticking the boxes " force debayer to highest resolution " " force sizing to highest quality " ? As an aside to that , I would stop taking 4 days out of your life to create proxies . You should be able to get realtime playback in premiere just playing around with the playback resolution settings . <p> Ok ... As of now , DeBayering is not part of my workflow . I 've never seen a use for this step in particular as Resolve 's Debayering is pretty good . But if DeBayering in RCX reduces noise then , that 's news to me . As of now , this is my workflow . <p> 1 . I get all the . R3D files and put it on a single master hard disk . 2 . @ @ @ @ @ @ @ @ @ @ into premiere and let it create proxies . ( which takes about 4 days ) 3 . I then edit . And once the edit is closed , I export an XML from premiere . 4 . I import the XML into my Davinci Resolve and do some fine adjustments , like re-framing or speed ramping. 5 . I then grade it and take an output of 3840x2160 . This is my master color file that I take into premiere . 6 . I import my file into premiere , apply Neat Video , titles and transitions , if required , and export . <p> That 's about it . <p> So ..... obviously i 'm not doing something that I 'm supposed to be doing . Yes the Dragon I shoot with has the Skin Tone OLPF but I have no idea what DEB is . Anyway , I 'm posting a . R3D snapshot , let me know what you guys think . <p> &gt; underexposed , main bulk is in lower third - Weapon is cleaner in that regard , Helium even more <p> &gt; make sure @ @ @ @ @ @ @ @ @ @ also when changing environments ( temperature change ) - excessive or extra noise is usually a blackshade issue - validate ur blackshade after u 've done it ! <p> U 've mentioned in ur OP that even in afternoon daylight u get noise issues . I own Red Dragon and been shooting on it forever . In afternoon daylight ( when exposed properly ) the cam is very , very , very clean . If u get noise even in those shots where u feed the sensor fat light across the stimuli range then u either have a blackshade issue or a sensor issue ! Btw , just making sure : after u 've done the blackshade u need to manually select the blackshade cal that u want to use , in order to activate it ! <p> &gt; I 'm not sure where u wanted to place the focus plane , but if it was the horse 's head then the shot is NOT in focus <p> &gt; add MORE LIGHT to the shot on set - u are starving the sensor a bit here <p> re ur workflow @ @ @ @ @ @ @ @ @ @ comrpessed . r3d raw file gets decompressed and then debayered - this happen in ANY software that supports . r3d files , so debayering has inadvertently been " part of ur workflow " - make sure to always debayer FULL RESOLUTION PREMIUM when doing color work ! ! ! There is no need to do anything special in RCX , unless u wan na try A.D.D . which will get rid of the noise but needs exporting to DPX/EXR files and now u deal with those massive files ( only go this route if ADD is really needed for a given shot ) ... <p> &gt; no need to create proxies , especially not for Premiere - I 've been editing straight off the . r3d files in PP for the last 4 years ... u can simply reduce the debayer resolution to 1/4 or even 1/8 if u have a slower machine ... no need to deal with proxies , since it 's just editing 1/8 res is absolutely enough - I edit Dragon files in 1/2 res and PP is very , very good in handling . r3d @ @ @ @ @ @ @ @ @ @ debayering in full premium resolution <p> &gt; I would apply noise reduction - if necessary - as the first step in the signal processing chain - that means do it in DVR . NV has a DVR OFX plugin . Denoising AFTER final color will change ur image , which means ur color work was not final ... ; - ) <p> &gt; I would try to avoid bringing anything back to PP . PP 's MediaEncoder ( CC version ) has produced horrible , horrible results for us ( the CS6 one was the last decent ) , export from DVR is of much , much higher quality . <p> Try one export from DVR to h.264 and compare with PP export . Night and day . <p> &gt; underexposed , main bulk is in lower third - Weapon is cleaner in that regard , Helium even more <p> &gt; make sure to blackshade the cam properly before every shoot , but also when changing environments ( temperature change ) - excessive or extra noise is usually a blackshade issue - validate ur blackshade after u 've done it @ @ @ @ @ @ @ @ @ @ in afternoon daylight u get noise issues . I own Red Dragon and been shooting on it forever . In afternoon daylight ( when exposed properly ) the cam is very , very , very clean . If u get noise even in those shots where u feed the sensor fat light across the stimuli range then u either have a blackshade issue or a sensor issue ! Btw , just making sure : after u 've done the blackshade u need to manually select the blackshade cal that u want to use , in order to activate it ! <p> &gt; I 'm not sure where u wanted to place the focus plane , but if it was the horse 's head then the shot is NOT in focus <p> &gt; add MORE LIGHT to the shot on set - u are starving the sensor a bit here <p> re ur workflow : <p> &gt; first off : debayering happens when the comrpessed . r3d raw file gets decompressed and then debayered - this happen in ANY software that supports . r3d files , so debayering has inadvertently been @ @ @ @ @ @ @ @ @ @ always debayer FULL RESOLUTION PREMIUM when doing color work ! ! ! There is no need to do anything special in RCX , unless u wan na try A.D.D . which will get rid of the noise but needs exporting to DPX/EXR files and now u deal with those massive files ( only go this route if ADD is really needed for a given shot ) ... <p> &gt; no need to create proxies , especially not for Premiere - I 've been editing straight off the . r3d files in PP for the last 4 years ... u can simply reduce the debayer resolution to 1/4 or even 1/8 if u have a slower machine ... no need to deal with proxies , since it 's just editing 1/8 res is absolutely enough - I edit Dragon files in 1/2 res and PP is very , very good in handling . r3d files <p> &gt; in DVR , make sure u 're debayering in full premium resolution <p> &gt; I would apply noise reduction - if necessary - as the first step in the signal processing chain - that @ @ @ @ @ @ @ @ @ @ OFX plugin . Denoising AFTER final color will change ur image , which means ur color work was not final ... ; - ) <p> &gt; I would try to avoid bringing anything back to PP . PP 's MediaEncoder ( CC version ) has produced horrible , horrible results for us ( the CS6 one was the last decent ) , export from DVR is of much , much higher quality . <p> Try one export from DVR to h.264 and compare with PP export . Night and day . 
@@44332509 @4332509/ <h> Calibration of a TV <p> I was quite surprised today to find out that there is no straightforward way of calibrating a LED , OLED , LCD , or plasma TVs . <p> I know I bumped onto a pretty basic colorimeter sometime ago from Datacolor , Spyder4TVHD , but it seems that one has been discontinued . <p> So since there are calibrators costing " well over $20,000 " , are there any of those that do not require you to use a computer , a disc with calibration patterns of some sort , and software similar to CalMAN ? <p> I 'm a calibration newbie , but this is my understanding : The five basic things you need for calibration are ... <p> Probe/Spectroradiometer ( to take readings ) <p> Pattern Generator ( as implied ) <p> Calibration Software ( to interpret the results and make adjustments ) <p> Display ( the one you are calibrating ) <p> Computer ( where the software runs from ; this can be built-in the display , but very rarely ) <p> There is no such thing as ' straightforward @ @ @ @ @ @ @ @ @ @ track of , especially backlight technology . But it does n't have to be as expensive as you imply- just buy what you need . There 's also ' calibrating ' with color bars , but that 's not really calibration .. that 's more like , ' re-aligning ' a display . <p> The real pain is ( for novices ) to get the very best results from their gear and calibration software packages ... it can be very frustrating and time consuming ... <p> some sw packages are very unstable and buggy as hell and you need to make sure to set up the gear correctly ... but once you understand that and work with good software , it is just a matter of time to set up and then just let it run 5,000 - 12,000 patches or whatever you 're doing ... <p> Last note : also you need to understand the limitation of the hardware you 're dealing with : your display . a 3D LUT can NOT magically fix hardware problems of cheapo screens ... but it will ( if done correctly ) give @ @ @ @ @ @ @ @ @ @ <p> Has anyone had any issues with routing a 3D LUT box through a A/V receiver then to the display ? My LG Plasma only has one HDMI and I 'd like to keep my home theater intact while using it occasionally as a client monitor . <p> I guess I could put the LUT box on the hdmi out from the reciever then all of my devices signals would be calibrated on the display . Any issues with this ? <p> Has anyone had any issues with routing a 3D LUT box through a A/V receiver then to the display ? My LG Plasma only has one HDMI and I 'd like to keep my home theater intact while using it occasionally as a client monitor . <p> I guess I could put the LUT box on the hdmi out from the reciever then all of my devices signals would be calibrated on the display . Any issues with this ? 43971 @qwx453971 <p> I never used A/V receiver ( so I do n't know if it affects picture quality any way ) , but I guess you should @ @ @ @ @ @ @ @ @ @ anyone had any issues with routing a 3D LUT box through a A/V receiver then to the display ? My LG Plasma only has one HDMI and I 'd like to keep my home theater intact while using it occasionally as a client monitor . <p> I guess I could put the LUT box on the hdmi out from the reciever then all of my devices signals would be calibrated on the display . Any issues with this ? <p> ( 1 ) unknown what the AV switch does to the signal ( 2 ) u must then also profile with the LUT box behind the switch <p> best is to keep the LUT box as the last device in the chain right before the TV ... if multiple devices go to the TV , get a LUT box like the eeColor with 6 LUT slots so u can make a calibration for each individual device or simply switch to a Unity LUT slot ( --&gt; no signal processing applied ) when other devices are routed to the TV ... <p> I was quite surprised today to find out that @ @ @ @ @ @ @ @ @ @ OLED , LCD , or plasma TVs . <p> I know I bumped onto a pretty basic colorimeter sometime ago from Datacolor , Spyder4TVHD , but it seems that one has been discontinued . <p> So since there are calibrators costing " well over $20,000 " , are there any of those that do not require you to use a computer , a disc with calibration patterns of some sort , and software similar to CalMAN ? 43971 @qwx453971 <p> I found this very inexpensive pattern generator from QuestTel very effective to calibrate Sony HXRMC2500 in our Lab <p> I use our 55 " Sony X9005B 4K TV as a ' client monitor ' , running it SDI from our Resolve 12G Decklink 4K Pro PCIe card via BMD 's 6G SDI to HDMI 4K converter positioned right at the set . I have critically ' calibrated ' the big Sony by eye with a dozen or more hours spent switching off the various menu ' enhancements ' , adjusting WB and Bias settings and so getting the TV back to where it should be by matching it with our @ @ @ @ @ @ @ @ @ @ not view both monitors at the same time due to suite layout and monitor shroud ) With Backlight on minimum at Brightness at 30 , LED edge bleed is virtually non existent . <p> Blackmagic 's recently released high speed ( 12G ) Terenex SDI to HDMI 4K LUT box will allow 4K at up to 60p frame rate and may even offer even more accurate calibration . <p> PS : I use a series of LightIllusion and Calman charts , displayed in Resolve on a 4K timeline to send various 10bit 4:4:4 signals to both monitors simultaneously - 4K direct to the 55 " Sony whilst SDI output #2 from the 12G Decklink Pro provides a real time 4K to HD down conversion for our HD grading monitor . <p> Without using a probe to read back measured values from known colour patches displayed on the monitor , you are not ' calibrating ' the display . The human eye is just not ' critical ' enough to enable accurate display alignment . <p> Without using a probe to read back measured values from known colour patches displayed on @ @ @ @ @ @ @ @ @ @ display . The human eye is just not ' critical ' enough to enable accurate display alignment .... 43971 @qwx453971 <p> True and handy links from LightIllusion there which is why I put the word calibrated in inverted commas . The human eye may not be a critical measuring device but I 'll tell you this , my eye would appear significantly more ' critical ' than 90% of the people I work with . <p> I do wonder how may people in this business get their eyes tested for colour accuracy . Sure we have measuring displays on our desktop but I would think 20/20 vision with zero or minimal ' colour blindness ' would be a prerequisite yet , I never see mention of this with respect to the art of the Colorist . What if you work with eye glasses which use lens coatings ? How many Colorists will insist it is a ' magenta ' cast when a client says ' red ' or ' pink ' ? 
@@44332510 @4332510/ <p> Pat Inhofer over at Tao of Color has a great Grading Fundamentals video that goes along with his DaVinci Resolve MasterClass . One of my big take-aways from that video was that " you have about one minute to evaluate an image for the first time , before your eyes start to deceive you . " <p> And , that 's exactly what happens . I try to be really mindful of that , but the biggest issue comes when my clients come over for final review . If I find them spending ages scrutinizing a scene , I 'll mention that if we do n't move on , we 'll lose our objectivity . Then the next day , when we review it again , they wonder why everything looks so different ! 
@@44332511 @4332511/ <h> getting rid of a lens dust spot on a close up <p> The final shot of the music video I 'm working on is spotted with lens dust in the very plain center of the face . The spot looks like a slightly dark question mark and is still on the image . As she turns her head , it moves from between her eyes , gets through her left eye into her hair above the left ear . <p> ( See images WGA - lamb meat - spot - 01 to 04 , in 16:9 ) <p> How would you deal with that ? <p> I tried a power window with a curve but of course , since the source area is changing in luminosity along its path , a setting that kinda gets rid of the spot at one place does not work for the next . Moreover , I said " kinda gets rid " of it because despite playing with the softness parameters of the power window , I still gets a fringe around the spot . <p> ( See images WGA - lamb @ @ @ @ @ @ @ @ @ @ cinemascope ) <p> There are also several smaller other spots on other parts of the image . <p> I usually do this in Fusion rather than Resolve - create a motion tracker , then a transform with a small mask that has an offset and is connected to the motion tracker . <p> Theoretically you could put it together in Resolve as well . Create a copy of the clip on a second video layer , offset the copy so that a clean skin segment overlays the dust spot . Create a power window on the copy and connect it to the alpha channel . Add a point tracker to the power window . That has the same effect as a clone stamp in Photoshop but will work across an entire clip as long as you keep the power window over a proper close source area . <p> I usually do this in Fusion rather than Resolve - create a motion tracker , then a transform with a small mask that has an offset and is connected to the motion tracker . <p> Theoretically you could put it together in @ @ @ @ @ @ @ @ @ @ on a second video layer , offset the copy so that a clean skin segment overlays the dust spot . Create a power window on the copy and connect it to the alpha channel . Add a point tracker to the power window . That has the same effect as a clone stamp in Photoshop but will work across an entire clip as long as you keep the power window over a proper close source area . 43971 @qwx453971 <p> There 's actually a quicker way to achieve the same as described above : 1 . Select the dust with shape/mask 2 . Track it 3 . Go to sizing , node sizing , check ' key lock ' and adjust pan/tilt <p> I 'm confused ; why is motion tracking needed ? The spot is stationary in the frame . The subject is moving , but the problem is on the lens , which does not move relative to the film plane . Unless the lens is zooming in and out , in which case the spot would grow larger or smaller and move away or towards the center @ @ @ @ @ @ @ @ @ @ that looks like a set focal length , what would be the need for tracking the mask ? <p> To add to that , it sounds like Ron 's issue is n't in selecting the right pixels to effect , but rather , what to adjust once those pixels are selected . What he has tried has n't been up to his standards , so he 's looking for other suggestions on which knobs and doodads to twist . <p> To add to that , it sounds like Ron 's issue is n't in selecting the right pixels to effect , but rather , what to adjust once those pixels are selected . What he has tried has n't been up to his standards , so he 's looking for other suggestions on which knobs and doodads to twist . 43971 @qwx453971 <p> Either of the suggested methods copy pixels from another region that are not contaminated by lens dust into place to replace the affected pixels . There is very little you can adjust on the original pixels that will be satisfactory because the lens dust has caused information @ @ @ @ @ @ @ @ @ @ is to use very careful cloning or healing brush tools with appropriate source regions ; the quick and dirty method is to use blur tools ( scratch &amp; dust ) . <p> The described methods do the same , but can operate on an entire clip instead of fixing each frame separately . <p> I 'm confused ; why is motion tracking needed ? The spot is stationary in the frame . The subject is moving , but the problem is on the lens , which does not move relative to the film plane . Unless the lens is zooming in and out , in which case the spot would grow larger or smaller and move away or towards the center of the frame . But on a shot like this that looks like a set focal length , what would be the need for tracking the mask <p> I 've just started learning fusion , debating moving from nuke to fusion ( both have rotopain type tools ) . Anyway I normally do this kind of thing with roto/paint tool by selecting a small section i want to copy @ @ @ @ @ @ @ @ @ @ line , way better then going over to adobe photoshop . Also with the a paint tool I can blur/smudge and move the direction/distance of the copy point as needed . 
@@44332515 @4332515/ <h> Life of a Colorist <p> Thought I 'd start a new thread for some lighthearted fun ; keeping it color-related of course ! Here are some gifs that I know most of you can relate with . <p> When the Director asks you to make it look " filmic " . When you see your name on the credits of a really bad short film . When your RAID crashes right before the client shows up . When the DP says you 're his new best friend . When the client has no budget . When you leave the suite after a 16 hour grading session . <p> When you finish a grade and you remember why you do it in the first place . 
@@44332516 @4332516/ <h> Color spaces - in the real world ( Resolve ) <p> There 's so much science about the color spaces , the gamuts , gamut remapping and LUTs/matrices . But how to use it in the everyday ? <p> When shooting h.264 , Prores 422 ( HQ ) or even RAW , how will I know what my " starting " color space is ? - ( in which color space is my image recorded ) <p> When my footage is imported to DaVinci Resolve , and the color science is set to DaVinci YRGB , is all footage converted to work in this color space ? I 'm guessing this is a big color space , and no color information is lost - but I could n't find any diagrams that shows the size with the CIE model . <p> When working with raw , I can chose color spaces to work in , in the raw tab . If I chose BMDfilm e.g. will that be my working colorspace ( instead of YRGB ? ) ? <p> Last question is about delivery . I ca n't @ @ @ @ @ @ @ @ @ @ chose output colorspace . Is the output colorspace determined by the output codec ? - in that case , do you know where to find a list of which codecs have which colorspaces ? -&gt; and , when delivering in say rec. 709 color space within resolve , and my clip has colors that are out of the rec.709 gamut , what happens to the colors ? are they clipped when rendering ? <p> 4.5 ... Is there a way to check/be warned when you 're going out of gamut for the chosen delivery codec/color space ? <p> RGB , YUV , HSL are color spaces . They designate what the code values mean per color channel . RGB colors can be plotted geometrically on a cube . HSL colors can be plotted on a cone . HSL red would be 0 , 100 , 50. 8 bit RGB red would be 255 , 0 , 0 . Color correction software generally converts these spaces for you under the hood , so you do n't really need to worry about it . <p> Color gamuts are all the colors that @ @ @ @ @ @ @ @ @ @ printed paper ) or artificial specification ( like 709 or DCI P3 ) . So when you color correct you typically pick a specific display output gamut . <p> RGB , CMYK , HSL are color components , they define what the values represent . But they are n't color spaces . A color space is the range and behavior of colors within a gamut . A color gamut is the outer permiter of an available range of colors , in a tristimulous display environment that perimeter is defined by the colorimetry of the red , green and blue primaries . In other words , a display set to 709 should have RGB values that at their respective extreme hit the specified values for the color space . Enabling the range of values available within that color space . That 's the gamut . Whether those values are manipulated as RGB or Yuv or any other encoding format does n't change what color they represent . <p> Colorspaces are needed in part because color encoding is relative . The colorspace definitions are like a map for those relative values , @ @ @ @ @ @ @ @ @ @ meant to represent 709 or another way if they are meant to represent P3 , and on . <p> Resolve YRGB is not a color space , it 's color science . But because those RGB values are relative , and not absolute like xyz values , YRGB science can work in various different color spaces . Generally the colorspace you work in is defined by what the display device you 're using for critical evaluation is set to . 709 monitor , you 're in 709 . P3 projector , your in P3 . Source side color spaces like slog2 ( in this case s-gamut being the gamut and slog2 being the gamma ) are just for source , you 're meant to transform these for display . I.e. grade them . <p> And there are still some people who think that all it takes to become a colorist is to apply some Magic Bullet Looks to footage ... 43971 @qwx453971 <p> Maybe some people think like that , but being just a technical geek doesn-t give you a colorist job ... or maybe it does , that-s why @ @ @ @ @ @ @ @ @ @ hardware instead of considering themselves -artists- . Doesn-t matter how much technical knowledge you have if your final result looks crappy . Don-t hate me , I-m just saying .... <p> You have to be highly artistic and have an incredibly deep knowledge of the science and tech of color correction to be a valuable asset in the pipeline these days . Its pretty tough to survive without being a technical geek. the artistic part is just a given . Just sayin . <p> Also , if you download the LightSpace demo program you can directly compare different colour spaces via the CIE charts within the Calibration Interface . <p> But , the one thing I will say here is that image source colour space is virtually irrelevant . Many cameras do not adhere to any know colour space ... <p> The key is the display on which you are grading - that really must be ' calibrated ' to a known colour space ( or at least have its own colour space known ) as that is what the images are being viewed on . So , if you @ @ @ @ @ @ @ @ @ @ look good , they are ' Rec709 ' . <p> As far as I understand points for saturation limit in vectorscope for YCbCr should be organized similar to a symmetric hexagon . In Davinci Resolve its more like a squeezed hexagon which should be found with YUV . Also the angle of HUE of the color wheels confirm to YUV . What I do n't understand is that Y(luminance) seems to be based on YCbCr Rec-709 . That would mean that Color Space in Davinci Resolve would be mixed up . <p> That would mean that Color Space in Davinci Resolve would be mixed up . 43971 @qwx453971 <p> How do you figure ? It would be nice to have switchable anticipated colorspaces on the vectorscope . But the standard for vector scopes is 709 ... you do n't see a lot of other colorspaces using vectorscopes . <p> I did figured out on different ways . Short story is that I currently develope a color picker . If calculation is based on YUV everything fits for Resolves vectorscope . Simple way to prove is freeware from http : //www.couleur.org/ @ @ @ @ @ @ @ @ @ @ examples I created with either YUV and YCbCr . If I blend a screenshot of the vectorscopes with these images I come to following results : YCbCr vs Resolve <p> YUV vs Resolve <p> Test pattern with HUE angle results for YUV/YCbCr which I measured by means of color picker : ( 3 o'clock is 0- , angle is measured anti-clockwise ) <p> How are you using this " color picker " ? on the GUI display ? The gui itself is color managed . i 'm not sure what discrepancy you 're trying to account for . If you throw a dpx of a color bar in resolve , that has 75% rgbcmy they line up just fine on the vectorscope . I 'm not sure I understand what your issue/concern is . <p> That was the reason why to use the color bar for the 2nd method . I have a defined R , G , B , C , M , Y color at the edges of the hexagon . Hence at these edges a defined HUE and Saturation . They will line up fine in YUV and @ @ @ @ @ @ @ @ @ @ HUE angle as well as vector lenght SQRT(U2+V2) or SQRT(Cb2+Cr2) which is a measure of Saturation . <p> To make it simple I only mentioned above results of HUE but Saturation results also confirm that it should be YUV . <p> I used my self made color picker program hence the GUI is color managed by myself . All is based on RGB colors which can be picked up at the monitor . But the Color Picker of Resolve can also be used to detect correct RGB . Then start to calculate it manually and you end up with different symmetries Hexagon/Squeezed Hexagon . Compare these data with data of Resolve vectorscope and you should see it . <p> Just to simplify it more , please read the angle of green or magenta at your vectorscope as they show the biggest difference of 9- and compare it with above results . <p> The interesting thing is that if I use the waveform monitor in combination with grey bar I can analyze Luminance behavior ( Y ) . All calculations work fine when I use YCbCr Rec-709 as a base . @ @ @ @ @ @ @ @ @ @ This behavior was of course expected . 
@@44332517 @4332517/ <h> House of cards : How to achieve that look ? <p> For the past two days I have been mesmerized by the show House of cards . Particularly the look of the show . I am wondering how one is to achieve that look . I shoot with the red one camera which is in the ball park of the epic camera they use . I would really love to get that blue , creamy look of the show . I do n't  even know where to begin . For the record I use Davinci Resolve . <p> The art direction and really consistent production design is about 40% of it . DP and lighting at least another 40% . Otherwise , you would never be able to just pull that look off in Grade . The colorist then has the basis to add the magic , I 'm assuming working with log files from the epic . <p> To my eye , getting those peaks just right all the way through is what makes it really sing . I would first try to desat and soften them @ @ @ @ @ @ @ @ @ @ Then bring down the blacks , but not too chunky , use gamma to get definition in the midtones just right . A bit of warmth in the upper gamma and some blue in the lower gamma would probably give you the overall look . Ah , but then those skin tones that are ever so slightly pushed . Although , given the high production value , some of that could be makeup . Or , could be a bit of qualifier work done there . After that a very soft vignette to focus attention , some saturation falloff in the highs and lows , and that might do it . I think that 's the cool thing about it , not too overly complex or heavy-handed but rather very tastefully " just right " . To see something similar , look at the interior shots in Tom Ford 's " A Single Man " . Similar approach . <p> Not saying that 's how it was done , and without the footage loaded up , very hard to say exactly , just one possible approach . Good luck . @ @ @ @ @ @ @ @ @ @ shot and art directed , I also do n't feel like the grade is overly complex I like the way they keep allot of the show dark while keeping the shadows from being too thick . 
@@44332520 @4332520/ <h> The theory of everything : creamy look <p> Hello i 'm working on a short film actually and the result we want for color grading is pretty close to what is done in the theory of everything 's life scene i can match this look in nuke but in resolve I have more problems i have try with bluring highlight the mix with layer node but i do n't have good results at all <p> any advice on how to have the creamy look in resolve ? here is a before after of my image , then the same in better resolution <p> Peter Doyle was the supervising digital colorist on The Theory Of Everything , as he was on Inside Llewyn Davis which featured a somewhat similar ' creamy ' look . A French chap had a go at recreating the effect in Resolve . It may be work checking out . <p> i 'm working on a short film actually and the result we want for color grading is pretty close to what is done in the theory of everything 's life scene 43971 @qwx453971 <p> @ @ @ @ @ @ @ @ @ @ highlights , I do n't see anything really difficult here . Lighting with diffusion also helps , and there are those who like shooting with some lens diffusion as well ( like a 1/8 black ProMist or the equivalent ) . It 's not always about the correction ; it 's often more about the lighting and the exposure . <p> thanks everyone Paul this video is really interesting , RGB mixer trick is clever Marc , yes i have a black frost 1/4 but it was too strong so we decided to not use it and try in post . Yes diffuse hightlight is easy , make it artistic like for example Damon Loble work ( NSFW ) , not that much . <p> I 'd go half as much as the above still . But it 's all very subjective , content-driven , and based on the creative decisions of the director . Windows and split-screens might help you control the effect better . A 1/4 black frost is a lot more severe than an 1/8th ProMist . There are also interesting variations out there ; Tiffen GlimmerGlass is another really good one . 
@@44332522 @4332522/ <h> Feature Film - The Legend Of Ben Hall <p> I 've mainly been a reader as only recently found this forum . I am a DP and Colourist and wanted to share my first feature film that 's getting released on December 1st in Australian Cinemas . I was both DP and Colourist . Happy to hear some feedback or questions about the production . There are BTS videos already as well . <p> yep was a pain from the first round , 2nd round still had a couple of issues but that was more to do with the fact that the post house did n't give me the colour chart clips from each scene that shot on the feature film shoot . 
@@44332524 @4332524/ <h> What do you think of making lens flares ? <p> My answer is yes ( we have the Sapphire plugin ) , but you can better ask a editor/compositer , because they have more tools to do so . At the other hand , flares are part of look . But what do you guys think ? <p> I have to say , mostly I do n't see the beauty of cosmetic flares , but that 's just my taste . <p> I think lens flares are a well established stylistic element in the history of the cinema and when created in camera and bound to the narrative can convey a certain mood and add to the story . I guess that 's one of the reasons why many people are so crazy about them . I am not talking about J.J. Abrams and his abusive overuse " apparently 721 lens flares - in Star Trek Into Darkness ( 2011 ) for which he later kindly appologized . <p> Personally I ca n't stand the anamorphic lens flares randomly slammed on in post in hopes to make the @ @ @ @ @ @ @ @ @ @ obviously be achieved with great result , but the source material must allow for it and the implementation needs to bend to the physical basis of the given scene ( e.g. matching the lighting environment ) . IMO that 's definitely a compositors job and not the one of a colourist . It 's like you 've said they just have better tools to do so and more importantly are masters of combining the artificial with the real . <p> My answer is yes ( we have the Sapphire plugin ) , but you can better ask a editor/compositer , because they have more tools to do so . At the other hand , flares are part of look . But what do you guys think ? <p> I have to say , mostly I do n't see the beauty of cosmetic flares , but that 's just my taste . 43971 @qwx453971 <p> I have made them ( maybe 5 in the last year ) and thought they added to the scenes as did the clients . I guess I would recommend always saying yes to the client and @ @ @ @ @ @ @ @ @ @ tools and skills to do it . I am not part of a larger post house so I like keeping all the billable hours I can , as long as it serves the clients interest and keeps them coming back ! <p> I think flicker and light leaks can be nice when used subtley and are motivated . I 'm not offended by actual motivated flares . The main purpose of post flares to me is to cover up compositing mistakes from VFX . <p> I think flicker and light leaks can be nice when used subtley and are motivated . I 'm not offended by actual motivated flares . The main purpose of post flares to me is to cover up compositing mistakes from VFX. 43971 @qwx453971 <p> Eh , if a director says , " hey , we need a little moment here -- do you have a just a 1-second lens flare we could throw in right about here ? " I 'll try a few things and if they like it ... hey , it 's a creative choice like anything else . If we 're @ @ @ @ @ @ @ @ @ @ highlight , I do n't have a problem since at least it 's sorta/kinda motivated . <p> Eh , if a director says , " hey , we need a little moment here -- do you have a just a 1-second lens flare we could throw in right about here ? " I 'll try a few things and if they like it ... hey , it 's a creative choice like anything else . If we 're dropping in a lens flare on top of an existing highlight , I do n't have a problem since at least it 's sorta/kinda motivated . 43971 @qwx453971 <p> Motivated - a key word for me ! Do n't see that coming in to play so much anymore ! Awful hyper-speed motion when a cut was called for to move from place to place being my least favorite . <p> I think flicker and light leaks can be nice when used subtley and are motivated . I 'm not offended by actual motivated flares . The main purpose of post flares to me is to cover up compositing mistakes from VFX . @ @ @ @ @ @ @ @ @ @ when used subtley and are motivated . I 'm not offended by actual motivated flares . The main purpose of post flares to me is to cover up compositing mistakes from VFX . <p> The main purpose of post flares to me is to cover up compositing mistakes from VFX. 43971 @qwx453971 <p> Hmm if I see mistakes in VFX shots , I directly call the compositor to fix it . Properly it will cost him a few minutes to solve it , while you can use the time for the nexts shots instead of making a lens flare which is actually not needed . But I assume it depends on the time &amp; budget . <p> Eh , if a director says , " hey , we need a little moment here -- do you have a just a 1-second lens flare we could throw in right about here ? " I 'll try a few things and if they like it ... hey , it 's a creative choice like anything else . If we 're dropping in a lens flare on top of an existing highlight , @ @ @ @ @ @ @ @ @ @ 's sorta/kinda motivated . 43971 @qwx453971 <p> True , but which tools you use to make them ? The 1-second lens flares I know look really cheap to me . It always does me thinking of just a image of a star with a alpha-channel , like it was n't there , but added in the post . <p> Hmm if I see mistakes in VFX shots , I directly call the compositor to fix it . Properly it will cost him a few minutes to solve it , while you can use the time for the nexts shots instead of making a lens flare which is actually not needed . But I assume it depends on the time &amp; budget . 43971 @qwx453971 <p> I meant the flares coming from vfx . I 'm not actually inclined to add flares to things , unless the client specifically requests . 
@@44332525 @4332525/ <h> The Cinematography of Bruno Delbonnel <p> The people at Wolfcrow just posted a fascinating video analyzing the cinematography of Bruno Delbonnel , who 's done such films as Amelie , Harry Potter &amp; The Half-Blood Prince , Infamous , Dark Shadows , Inside Llewyn Davis , and several other really striking films . I do n't always like these as movies , but visually they 're very interesting , and the analysis of Delbonnel 's technique is worth listening to for a few minutes : <p> The look of some of these images is really stunning and beautiful , and it 's interesting to mull over how much of that look was determined with lighting , through filtration , through the color-correction , or all of the above . And I like that Delbonnel makes such an effort to ( generally ) keep the backgrounds down and the characters ' key lights up , which has got to be a struggle given the size of some of these sets . Just amazing work . <p> His lighting and shooting approach has changed somewhat since he began collaborating with @ @ @ @ @ @ @ @ @ @ video above yet arguably contributes just as much to the look of the films that they 've worked on together . 
@@44332527 @4332527/ <p> " a high quality LCD TV will only show a dynamic range of around 6 stops " . <p> " Sony 's X300 HDR monitor can show I believe around an 11 stop range " . <p> In a controlled environment , a LCD display with 1000:1 native static contrast ratio should be able to show a dynamic range of 8 stops , correct ? 2 to the power of 8 equals 1024 . DCI projectors with 2000:1 contrast ratio can show 9 stops , can they ? <p> Why the OLED X300 with almost infinite contrast ratio can show " only " 11 stops ? <p> " a high quality LCD TV will only show a dynamic range of around 6 stops " . <p> " Sony 's X300 HDR monitor can show I believe around an 11 stop range " . <p> In a controlled environment , a LCD display with 1000:1 native static contrast ratio should be able to show a dynamic range of 8 stops , correct ? 2 to the power of 8 equals 1024 . DCI projectors with 2000:1 contrast ratio can @ @ @ @ @ @ @ @ @ @ OLED X300 with almost infinite contrast ratio can show " only " 11 stops ? <p> Please enlighten me. 43971 @qwx453971 <p> This would make a lovely comment on Alister 's post . <p> One gotcha you 're not factoring , is that while the display is cable of incredibly high contrast thanks to it 's true black base point ... the bit depth of the panel and the signal processing chain , would n't be able to support that level of definition , so that does limit the number of stops . <p> That said 6 stops on a SDR set does n't really ring true . I 've always thought of SDR as a 8-9 stop display . With HDR adding another 2-5 stops . <p> Lucas , just curious , what your background is . You seem to only have eyes for HDR. 
@@44332529 @4332529/ <p> Unresponsive because he probably does n't know . You need to view the dpx at the mentioned standards on a calibrated display until one looks right . Highest probability is rec 709 2.2 legal , but it could be any . And if the colorist did not use a calibrated system , it 's none of the above and the project will only look " correct " on his system . If you paid the colorist , he needs to give you this info . If you did n't pay him , well then , you got what you paid for . Sorry ... 
@@44332530 @4332530/ <h> Timeout , dropped frames out of nowhere ... <p> I 've lurked for awhile but this is a good time to join in . I 'm grading a low budget feature shot on Sony 4K raw . I have been outputting Prores 4444 4K reels as one type of backup and have suddenly run into the dreaded " Timeout , waiting for frame ### , frame dropped " error message . The program freezes but I can quit ( I have to force quit ) . This is the thing ; I have made no changes to my system in the last month and have successfully output reels a dozen times . <p> 2009 Mac Pro 5.1 , OS 10.11.5 , 24gigs Ram , GTX680 and GT120 ( that I also pulled but that did nothing ) . I have sense updated Nvida drivers , Cuda 7.5 and to resolve 12.5.1 ( I was on 12.5 ) but still no luck . <p> Stops on different frames but about 20- 28% in on a 20 minute reel . I have tried different raids , 1080 renders , even @ @ @ @ @ @ @ @ @ @ send out screeners this week . <p> Hmmm , I have n't seen those error messages myself , but I assume they are related to something slowing the process down . I know my raid seriously slows down when I have less then 25% free space , so that 's why I suggested the other drive . 4k dpx 16 bit is quite taxing . <p> And I think a rule of thumb is to have at least 4GB of graphic card memory for rendering 4K . Although you would get ' out of memory ' messages if that was the problem , I think . <p> I find those are fairly hefty file formats that require very fast drives and extremely fast processors . How do your drives measure out with the BMD Speed Test utility ? <p> I also find you need more than 10% overhead for any RAID array , or else it starts slowing down quite a bit . I 've dealt with F55 XAVC 4K files , and they generally work pretty well , even on a fairly puny trashcan Mac . But I 'm @ @ @ @ @ @ @ @ @ @ What if you installed a second blank internal RAID and rendered to that ? I 've also used ProRes 444 4K ( and HD ) for delivery and VFX on indies and generally have done OK . DPX files were significantly harder to render . <p> Also forgot to mention I have a flashed 4gig macvidcards gtx680 . I think drive space could be the problem . It 's just weird I have n't run into the problem before . I will try empty raid tomorrow . Time for bed . <p> Could try picking up where you have left off with the dpx render and seeing if it 's that particular section ( the frame may vary but try skipping that whole section , or rendering Just that section ) . You may have to combine very short sections of successfully rendered frames together after to get something complete . Messy but doable until the greater issue is Resolved . <p> Could try source caching , or even output caching , the whole timeline . Render the output using the caches , of course . <p> Could also try @ @ @ @ @ @ @ @ @ @ to render 1080 out of the 4k project . Combining that with caching the output would be the most likely workable scenario in my mind but obviously a lesser outcome than the others might provide . <p> I 've rendered things that had no right to work on my Macbook at the 11th hour by lowering the project res , output caching the timeline and setting a low render speed . <p> I 've tried all of the suggestions here with no luck - even to my surprise . Tried an empty raid ( speed of 450mb ) , caching , even difference reels . They all stop at around 25% in , as if something is filling up . This has never happened before . I 'm wondering if I turned on the stupid button or something . I will go to a backup reel , make screeners then order a new GPU tomorrow ( sure wish there were drivers for the GTX 1080 ready ) then try to get the bottom of this again . <p> I 've tried all of the suggestions here with no luck - @ @ @ @ @ @ @ @ @ @ speed of 450mb ) , caching , even difference reels . They all stop at around 25% in , as if something is filling up . This has never happened before . I 'm wondering if I turned on the stupid button or something . I will go to a backup reel , make screeners then order a new GPU tomorrow ( sure wish there were drivers for the GTX 1080 ready ) then try to get the bottom of this again . <p> I 've tried all of the suggestions here with no luck - even to my surprise . Tried an empty raid ( speed of 450mb ) , caching , even difference reels . They all stop at around 25% in , as if something is filling up . This has never happened before . I 'm wondering if I turned on the stupid button or something . 43971 @qwx453971 <p> I swear , that 's happened to me . <p> In moments of extreme emergency , my advice would be to rent a machine or find somebody in your area with a Resolve system and just @ @ @ @ @ @ @ @ @ @ you 're up against a deadline , this is a better option than beating your head against the wall trying to solve some kind of throughput issue . <p> I was able to divide the reel into 3 min sections and get through a 1080 prores version . I had to restart the computer after each two passes . Four minutes would freeze . Weird . I can get the screeners out then try to figure out what is the deal . I have been using Resolve since version 9 and have had no problems ( that is until version 12 and OS 10.11 ) . We shall see what happens . 
@@44332531 @4332531/ <h> Soon and it will but in its own natural time . <h> Soon but should be encouraged by leading industry professionals . <p> this is a discussion I wanted to start up for some time now . To start off I 'm not here to berate or protest , just want to start a healthy debate amongst us and get your perspective on it . I am aware that its somewhat controversial this topic but just to state I have no emotional investment or yearning for the title , just interested in getting a broader view from the community . <p> It concerns whether Colourists should be publicly recognised and receive their own category at Award ceremonies which are internationally televised , such as The Golden Globes , Baftas and Oscars . Colourists already do in some areas have their own award category , but nothing substantial on the level as other post production roles , HoDs and departments have . <p> Here are some of the possible reasons I can think of why it has n't already happened yet or been properly broached , against the title @ @ @ @ @ @ @ @ @ @ Colourists have only been around a short period of time like DITs have since Digital Cameras have been accepted as an alternative to shoot at a high quality standard on par with film . It needs more time to age and develop . 2 . DoPs could feel undervalued and their position challenged as an artist . Some DoPs dislike the role of DIs . 3 . Colourists have the highest paid position in post production so they do n't need the extra praise . 4 . It 's hard to explain to the general public what we do . <p> All good points of argument but to rebuttle those , for the title : <p> 1 . Film in general is the youngest platform of entertainment and artistry ; its just a little over 100 years old . It 's not as established like Music , Art &amp; Sculpture , Theatre and Literature is , which are considerably older . A weakness in some ways but a strength too in which that its progressed so fast because of its very nature ; that its influenced as equally as it @ @ @ @ @ @ @ @ @ @ , Politcally , Artistically , Technologically ) , especially computer technology . - So if computer-based VFX and Digital Cameras , as young as they are , have been accepted into the industry and are given their own Categories , why ca n't Colourists who are responsible for the Finish of a film ? <p> 2 . Colourists are artists too , many DoPs love DIs , how they can manipulate what they could n't at the time of shooting it due to budget and time constraints , whether its to balance the shots but also to introduce something they had not thought of or did n't think possible . - So are DoPs meant to be regarded as precious little snowflakes or can DoPs publicly acknowledge what they already acknowledge in the Studio ; that Colourists are a crucial cog in machine when making a film and give valuable artistic merit ? As Roger Deakins put it : To make something look good with Colour is easy , to tell a story with Colour is the challenge . <p> 3 . It still takes hard work , time and @ @ @ @ @ @ @ @ @ @ , albeit far more accessible financially now , it is still an expensive investment in equipment and there is a steep ongoing learning curve to it , as its about the science of colour . Just like a DoP is about the science of Optics and Light . Additionally , there are no public courses in grading at film schools unlike Cinematography , Editing and VFX have , you can only learn by searching on the internet for courses and/or learning it by doing it in its own working environment . <p> 4 . We can teach the public , they are n't stupid - they would probably find it fascinating , many categories at award ceremonies that exist are explained already on their websites . Also , the public can work it out by the name alone : Colourists . But an example to give them is one they are already familiar with ; its like Photoshop but for film . Explaining the magic does n't make it less beautiful , it has the opposite effect . <p> Further good points for the title ; now I will rebuttle @ @ @ @ @ @ @ @ @ @ have so many categories now it gets out of hand , not to mention many other opportunities Colourists gets at smaller Award ceremonies and Film Festivals . Then you have the problem of other roles to consider , who else should be afforded their own category ? When should it stop ? - A particular department would no matter what ride along the bus when its nominated and/or wins for a film . Such as VFX , it benefits the whole department and individuals involved to promote themselves further . <p> 2 . It could raise the Colourist position to an HoD level , their decision is as valid as a DoP , it can become overly complex . A Colourist is there to achieve the Director 's and DoP 's vision , not to invent it for them , but of course are allowed to give their own professional input technically and creatively . A collaboration with the three players is already taking place , Colourists are general not undervalued , so it does not need to be made public . <p> 3 . Colourists do n't really want @ @ @ @ @ @ @ @ @ @ to be the silent maverick who makes others look good and achieve a universal goal as a collective entity . Like the Blood Splatter analyst and DNA analyst make a Detective solve cases or the JPL Scientist who make the Astronaut launch into space . <p> 4 . Do we want the public to know ? It 's almost magical when we witness the Log-C tranform into a fully Graded 3D LUT , but that 's how it should be seen , not as Log-C , Log-C is just a technical nescessity for digital cameras . Do we want to explain the magic away so frivolously ? Before DIs came into play , we shot on film and that film would be processed and coloured at the lab then each shot 's entire frame only balanced once cut with just brightness and CMYK . Have those technicians ever received their own award category ? <p> I could go on , so many other points to make , but really I just wanted to be as democratic about it as possible and not appear biased . <p> It concerns whether Colourists should @ @ @ @ @ @ @ @ @ @ ceremonies 43971 @qwx453971 <p> I think a large part of the issue is that grading in it 's current form is so new compared to directing , cinematography , art direction , costume design , etc . The path for grading toward awards recognition will probably be similar to VFX , where more specialized awards were given out initially that evolved into something more permanent . An Oscar for Outstanding Achievement in Color Grading wo n't just appear one year , it will happen over a period of several ( many ? ) years . <p> I think a large part of the issue is that grading in it 's current form is so new compared to directing , cinematography , art direction , costume design , etc. 43971 @qwx453971 <p> For me , the larger issue is that color grading is very dependent on production design , cinematography , and the director 's ( in features , producers in television ) desires . This means that the creative contribution is usually not solely the colorist 's , and that the " degree of difficulty " is hard to ascertain @ @ @ @ @ @ @ @ @ @ " best color grading , " do you give this to a production that was shot very well and required little augmentation , or do you give it to something that was heavily manipulated , to the point that the original cinematography is largely unrepresented in the final product ? Without " before/after " comparisons it 's very difficult to tell what was done , even for an experienced colorist . Visual effects are much easier to judge because for the most part , their integration into the original photography is itself a large part of the artistry . Not to mention that the visual effects design is largely a creation of the visual effects team , not production . <p> As for " highest paid , " I would point out that A-list editors are very , very highly paid as well . And they generally spend a lot more time on a single project than the colorist does . <p> I think it should be shared with a DoPs on Best Cinematography . There are cases , like Mike mentioned , where the colorist has very little to @ @ @ @ @ @ @ @ @ @ of situations where the colorist job goes a long way in creating the look , and that input should be recognized as well . Then 's there 's the argument of " you have a director and that 's the DoP " although many times the Director takes charge and overwrites the DPs decision . <p> There 's certainly a lot of room for argument , but something to consider is that most awards are also based on a workflow involving film , which is pretty much on it 's last run . A Director once told me , there are no more cinematographers , we have videographers now . <p> I think it should be shared with a DoPs on Best Cinematography . There are cases , like Mike mentioned , where the colorist has very little to do if its shot well , but there are tons of situations where the colorist job goes a long way in creating the look , and that input should be recognized as well . Then 's there 's the argument of " you have a director and that 's the DoP @ @ @ @ @ @ @ @ @ @ the DPs decision . <p> There 's certainly a lot of room for argument , but something to consider is that most awards are also based on a workflow involving film , which is pretty much on it 's last run . A Director once told me , there are no more cinematographers , we have videographers now . 43971 @qwx453971 <p> I see your point it could be shared but I doubt a DoP would like that and it 'll be a bit of a culture shock , but maybe you mean the Colourist has their own category but the DoP accepts the award with the Colourist ? <p> There are cases , like Mike mentioned , where the colorist has very little to do if its shot well 43971 @qwx453971 <p> I 'm kind of shocked that anyone would think this . If anything , the best photography I 've ever graded comes with the highest expectation of performance . Sure occasionally the look on set is very close to the final intended look , but it is our jobs as colorists to find ways to contribute and @ @ @ @ @ @ @ @ @ @ thought possible . <p> There 's certainly a lot of room for argument , but something to consider is that most awards are also based on a workflow involving film , which is pretty much on it 's last run . A Director once told me , there are no more cinematographers , we have videographers now . 43971 @qwx453971 <p> My opinion is that that 's a meaningless , superficial observation . To me , the craft of the cinematographer is all about lighting , exposure , lenses , and shadows , and all that still happens no matter what camera is used . Whether or not it 's film has no bearing on whether the cinematographer did a good job . <p> But also with well shot material the job is just to not F it up . The award should be given to the most salvaged job . 43971 @qwx453971 <p> I 've been on a few of those . The most difficult I think is when you have a really well-shot project , and then six months later somebody else goes and does pickups that have @ @ @ @ @ @ @ @ @ @ hard . I 've generally been reluctant to criticize other colorists ' work , because you never know how awful the material was to begin with . Once you see the original and understand the changes involved , then you can judge pretty accurately ... but few get to see that . I think it is possible to objectively look at an image and say , " eh , the look is horrible or the look is OK or the look is great , " but we have no idea how much work it took to get there . I 've seen million-dollar commercials that took an hour to do , and others that took days of work . <p> There are cases , like Mike mentioned , where the colorist has very little to do if its shot well 43971 @qwx453971 <p> I did n't really say that . I said " requires little augmentation , " which means it does n't require the colorist to add a lot of things , or make the look significantly different than the way it was shot . I did n't say @ @ @ @ @ @ @ @ @ @ although admittedly that is sometimes the case , especially in television ) . <p> I understand Ian 's comment , but I would counter that if a shot comes up correctly and satisfies the creative desire " out of the box , " or at least when put through a proper color pipeline , it is not always productive to change it for the sake of changing it , particularly in television work where time - or lack of it - is always a significant factor . All color grading is inherently destructive , so unless something productive is being added as a result of the manipulation , better to keep the destruction to a minimum .... <p> It concerns whether Colourists should be publicly recognised and receive their own category at Award ceremonies which are internationally televised , such as The Golden Globes , Baftas and Oscars . 43971 @qwx453971 <p> I do n't think it 's going to happen , partly because of the lack of a colorist organization ( in the U.S. or worldwide ) , and partly because the trend of the American Oscars and Emmys @ @ @ @ @ @ @ @ @ @ particularly do n't care about technical awards . <p> I 'm reminded of how Mike Meyers denigrated the Oscars for Best Sound and Best Sound Editing at the 2001 Academy Awards , which to me just illustrated how detached big stars are from the enormous difficulty , craft , and expense of technical picture &amp; sound work : <p> Technically , color correction comes under the authority of IATSE Local 700 in LA , which is the Editors Guild . As far as I know , even the Editor 's Guild does n't present an award for best color ... so you have to wonder if there would be any industry support or not for it . My observation is no . And I would bet there are more colorists in LA who are not in the union than are in the union . <p> Bear in mind there still is n't an Oscar for best Stunt , and movie stunts have been going on for at least since the invention of film . <p> I think there would be an enormous amount of financial and political factors that you @ @ @ @ @ @ @ @ @ @ of colorist award for the Oscars and the Emmys . And in truth , I think the reaction of the various Academies , guilds , and organizations like the ASC would be " what 's in it for us ? " If the other organizations do n't benefit , then I think they 're not going to want to push it through . <p> My opinion is that that 's a meaningless , superficial observation . To me , the craft of the cinematographer is all about lighting , exposure , lenses , and shadows , and all that still happens no matter what camera is used . Whether or not it 's film has no bearing on whether the cinematographer did a good job . 43971 @qwx453971 <p> You 're leaving out choosing the right film stock which had a lot to do in creating the look , and that 's gone . <p> You 're leaving out choosing the right film stock which had a lot to do in creating the look , and that 's gone . 43971 @qwx453971 <p> Eh , choosing a specific camera basically @ @ @ @ @ @ @ @ @ @ , Alexa , and Sony F55 each have different looks . I was part of many film tests in the late 1990s and early 2000s at Technicolor where they 'd compare 3 or 4 different Kodak emulsions and usually a couple of Fuji stocks , then show them back-to-back in a projected test to see if anybody could tell the difference . <p> I do n't think the colorist can make a massive difference on that order , but they can make a difference and sometimes an improvement . But it all builds on the original work of the DP . If they have n't lit it well , we still do n't have a magic switch ( or a LUT , if you will ) to fix it . <p> I 'm near completing color correction on a film which I photographed . At this point in the process , I feel like I deserve some kind of award ... Something like the " nobody to blame , but himself " award perhaps ! <p> I will say this : It 's a lot of work and I have @ @ @ @ @ @ @ @ @ @ time ! <p> I think a huge problem here is the lack of information as to how much the colorist has done . I totally accept that there are films where the look has been created by the colorist and not the Cinematographer . Just as there are films that have been , err , adversely affected by the post team Yes there have been awards given to cinematographers that should have been distributed amongst the post team . <p> The big issue was summed up by Jeff Okun when he was discussing VFX Oscars and he said that the problem had become one of people not realising how much VFX work had been done when it was done right and that maybe the Oscar for VFX should be renamed as the Oscar for most visible/worst VFX ... <p> The big issue was summed up by Jeff Okun when he was discussing VFX Oscars and he said that the problem had become one of people not realising how much VFX work had been done when it was done right and that maybe the Oscar for VFX should be renamed as @ @ @ @ @ @ @ @ @ @ This goes back roughly 50 years . For example , while 2001 ( released in 1968 ) won an Oscar for Best Special Effects , it was passed over for Best Makeup . One theory is that the Academy members actually thought they used real apes in the " dawn of man " scene . <p> There 's been a bunch of movies that used all kinds of visual effects where the audience was never aware when they were being used . There 's also movies that are believed to have had very little color-correction , where a lot of the work was theoretically done in-camera , but it turns out that actually 75% of the " virtual lighting " was done in post . And that even goes for non-effects movies that do n't involve explosions , spaceships , or car chases . <p> Gustavo Bermudas said " The way I see it , if the work of the DoP is the most important part , show it without color grading . Let 's see how far it gets . " <p> I 'm up for that I did @ @ @ @ @ @ @ @ @ @ a modified ACES IDT to produce rushes that showed the intent of the Cinematographer without the intervention of anyone . It essentially was just the normal IDT with contrast reduced so that highlights and shadows held , I showed examples of my work shot on F65 , C500 &amp; Alexa with this approach . <p> Believe me , there are a lot of cinematographers who would love this , we 're really fed up with people messing with our pictures . <p> And yes ! we are the very same cinematographers who love taking things further with a skilled colorist. 
@@44332532 @4332532/ <h> Bit of a lurker - just landed in Toronto - who wants a free coffee ? <p> Hello everyone ! My name is David J. Fulde - Colorist and online editor ( And basically everything else in post production but audio and story editing ha ha ) , just moved to Toronto reently and would love to grab some of you wonderful people coffee - shoot the shit , pick your brain , talk about what the post scene in Toronto is like , etc. 
@@44332534 @4332534/ <p> Excuse me , that orange polygon on Powercurve means you have auto keyframe turned on for power curve . Uncheck powercurve polygon , check corrector 17 polygon , go make changes to output gain along time line as needed ( noting its creating keyframes EVERY time you change ANYTHING ) , then uncheck the polygon . <p> enabling it for the entire corrector 17 node instead of on the specific powercurve , or other specific function enables auto keyframe to control ANY change you make : opacity , color wheels , windows , anything . You want to control opacity of the node . This will allow you to do so , as it creates keyframes for ANY changes . Give it a try and you 'll see keyframes pop up for any change after you enable autoKF for the node . 
@@44332535 @4332535/ <h> A quick Scope survey by the makers of Scopebox <p> We 're running a quick survey to find out how people are using ScopeBox , other scopes and QC tools . Please take a few minutes to answer some questions and help shape the future of Software QC . As a thank you we 'll give you a coupon code for 20% off ScopeBox and EditReady . <p> Hi Mike , received the email and plan to send feedback but I also wanted to add some thoughts here as it may lead to some more interesting ideas from other users . <p> I 've only just recently transitioned to scopebox and am liking it more and more . That being said there are some features I hope to see in future updates . <p> 1 . Ability to save current layout without having to type in a new name , then manage the old layouts etc 2 . When increasing the intensity of a scope the traces retain information and do n't scale linearly . I.e on parade scope I sometimes increase the intensity so I can see @ @ @ @ @ @ @ @ @ @ the bright portions of the scope blow out and all the detail is lost . It would be great to be able to see the faintest traces along with the brightest traces without losing detail on either end . Resolves scopes while not the prettiest do this very nicely . 3 . Ability to add refence levels in the waveforms . I.e for this particular job I like my blacks and whites to be at a certainly level so being able to set custom reference levels would be very helpful . 4 With the vectorscope set to color even though the picture is static the traces still dance around . <p> When increasing the intensity of a scope the traces retain information and do n't scale linearly . I.e on parade scope I sometimes increase the intensity so I can see the more faint traces . However when I do that the bright portions of the scope blow out and all the detail is lost . It would be great to be able to see the faintest traces along with the brightest traces without losing detail on either end . @ @ @ @ @ @ @ @ @ @ . 
@@44332536 @4332536/ <h> Make sense of this ... <p> I 've been messing around with this project since last October . It 's bad enough I upgraded to Adobe CC on a 1-year school subscription and it 's about to end soon . I will lose all access to this project being that I planned on going back to CS6 because I can not afford to pay the subscription and the upgrade to Creative Cloud Subscription does n't allow for backwards compatibility - although that really makes no sense . The reason why this is pretty bad is that I gave the couple 4-8 weeks to finish the project , but I have been coordinating with the couple because of the mishaps and they understand ... it 's months past my deadline and I can not make sense of why bad things keep happening to my project files all the way to rendering out a portion of the sequence to export , which looked horrible . The quality of the clip looks muddy and murky and top everything off , it takes an ungodly amount of time for me to render @ @ @ @ @ @ @ @ @ @ from RED GIANT in order to fix noise levels boosted in the primary color correction . A few clips that has this plugin applied appears to be really distorted and jerky -almost the same aesthetic quality of how a corrupted raw image would show up before being processed . It makes the system really slow . I do want to notate that the raw audio and video is currently sitting on a 3TB external drive by Seagate . I 've installed the latest CUDA Driver Version : 7.0.52 . I 'm trying to make sense of optimal settings while in Premiere Pro which will allow for faster processing , but I 'm failing miserably . Can someone point me in the right direction - maybe possibly show guide me through a workflow scenario ? <p> Just a thought : what if you use no denoise at all and just render it out as is as a single file ? And then once that 's done , pull that file back into the system and just do a noise-reduction pass with nothing else going on ? <p> A lot will also @ @ @ @ @ @ @ @ @ @ . I would also add that a MacBook Pro is not ideal to do this kind of work on . Editing is fine , but anything GPU-intensive will be a problem . ( Not sure how Red Giant 's NR works . ) <p> I think just getting the cut right is the most important thing . Prioritize what you have to do , get the piece right artistically , and then worry about the technical stuff . Do n't worry about all the frills , just worry about timing and performance . Once you have that right , then do the other stuff in a separate pass . Red Giant may not be your best option -- there are lots of NR tools out there . In fact , if the color-correction is done optimally , it 's possible you may not need NR or you might need less of it . <p> Just a thought : what if you use no denoise at all and just render it out as is as a single file ? And then once that 's done , pull that file back into @ @ @ @ @ @ @ @ @ @ else going on ? <p> A lot will also depend on resolution , file types , and similar factors . I would also add that a MacBook Pro is not ideal to do this kind of work on . Editing is fine , but anything GPU-intensive will be a problem . ( Not sure how Red Giant 's NR works . ) <p> I think just getting the cut right is the most important thing . Prioritize what you have to do , get the piece right artistically , and then worry about the technical stuff . Do n't worry about all the frills , just worry about timing and performance . Once you have that right , then do the other stuff in a separate pass . Red Giant may not be your best option -- there are lots of NR tools out there . In fact , if the color-correction is done optimally , it 's possible you may not need NR or you might need less of it . 43971 @qwx453971 <p> Thanks for getting back to me ASAP and I really appreciate the advice ! I @ @ @ @ @ @ @ @ @ @ GoPro 3+Black and the problem comes when I 'm in low light situations . Are you saying to export the entire sequence with the color correction then re-import the sequence into the timeline with the Denoiser plugin in maybe an adjustment layer ? OR are you suggesting to render and export each clip separately with Denoiser attached so that I may re-import the media to apply the LUT I 'm using ? Let me know what you think whenever you get a moment . Thanks again Marc ! <p> export the entire sequence with the color correction then re-import the sequence into the timeline with the Denoiser plugin in maybe an adjustment layer 43971 @qwx453971 <p> This one . If it does n't work , I think you should give up on that plugin . If you 're willing to spend money , I 'd suggest downloading Neatvideo instead . I have n't had any bad performances using that . <p> Also , in Premiere go to your sequence settings and uncheck Maximum Bit Depth and maximum Render Quality . In Preferences -&gt; Memory , lower the amount of Ram @ @ @ @ @ @ @ @ @ @ Premiere . <p> Are you exporting within Premiere or are you sending your export to Adobe Media Encoder ? Also , do you have an Nvidia GPU in your macbook pro ? <p> I 'll mirror what everyone else said in that you need to either render out the video with noise reduction before you grade , or just do the noise reduction after you 've finished grading as noise reduction is extremely resource intensive . Are you grading in Premiere or Speedgrade ? If you 're having problems affording the cc subscription but you need it , have you considered switching to month by month pay ? Additionally if you 're only using Premiere you could just subrscribe just to premiere cc as oppossed to the entire suite . <p> I would also say , do n't try to edit or color correct or noise-reduce the GoPro footage directly . It 's all H.264 material that is very processor-intensive . Convert it to something more benign like ProRes 422 . Strictly my opinion . <p> There 's also cheaper solutions to editing out there . You can pick up @ @ @ @ @ @ @ @ @ @ , and Resolve Lite ( technically ) is free for color correction , though $995 if you want the version with NR ( which I highly recommend ) . Do n't forget , you also have the option of adding noise to the project if that suits it . Noise is a tool like anything else , and it may be something that works for the images -- provided you can control it and make it consistent . <p> I actually do n't know the answer to this question , but would transcoding to 422 have any negative effects in regards to data rate given he 's reading from an external 3tb ( I assume USB ) drive ? Or in your experience does the processor needed for decode override this ? <p> There 's also cheaper solutions to editing out there . You can pick up Final Cut Pro 7 dirt cheap if you shop around , and Resolve Lite ( technically ) is free for color correction , though $995 if you want the version with NR ( which I highly recommend ) . Do n't forget , @ @ @ @ @ @ @ @ @ @ project if that suits it . Noise is a tool like anything else , and it may be something that works for the images -- provided you can control it and make it consistent . <p> They presented it that way at the demo to an LA editors ' meetup last month . I think Premiere , Apple , and Avid ( as well as Sony and some other companies ) will have to look on Resolve as some more editing competition in the months ahead . <p> Be carefull with the Denoiser , I had some similar problems with it back in the day , and I m using Neat Video since then , never looked back . Denoisers are eating a lot of power so tbh it 's a lot better to use it when you are done with anything else . My opinion only , I would leave the Red Giant Denoiser alone . These days I usually render out a 10-bit 444 DnxHR - or something similar - and I do the denoise on that , with a help of an adjustment layer . What you @ @ @ @ @ @ @ @ @ @ previous version , is to use an xml/aaf , or pay for 1 month Premier licence . For me it seems like rendering DnxHR is a lot faster than ProRess , you could try that to . Also Davinci Resolve 12 is just around the corner - hopefully . <p> Not sure of this helps the OP , but in PrP , you can " render and replace " where effects are processed out to file and the file populates the timeline , nice for swallowing up the processing cost of certain plugins. 
@@44332537 @4332537/ <p> With respect to LightSpace CMS , we have spoken to Digital Vision about this , and have provided them the LightSpace network integration protocol , as used by Resolve , Mistika , Scratch , etc . But , as yet , they have not performed the required integration work . <p> So , as you have been using , you presently need to use DIP mode within LightSpace , which syncs with a patch sequence played directly from the Nucoda timeline . The patch sequence can be built using the provided EDLs , which can be downloaded from the Light Illusion website. 
@@44332538 @4332538/ <h> What if Man of Steel was in Color ? <p> it was great as it was delivered , as the director intended " not to be second guessed by people that miss the point . Maybe there was less colour than there could have been , and some scenes do have quite a lot of colour in them but the " restoration " is out of context , cartoonish and in some places downright awful . Different ! = better . <p> Just turn the TV set into Vivid mode and one gets all the color one wants ... Instant color " restoration " ... 43971 @qwx453971 <p> Naaa , the movie is extremely desaturated and biased towards blue/cyan . This is a heavily-styled look , and all popping the sat would do is just emphasize that . These people had to basically take a B&amp;W image and then colorize that in order to get anything close to normal fleshtones and skies and grass . <p> I see their point , but I do n't necessarily see it as a slam towards the colorist(s) who were part of @ @ @ @ @ @ @ @ @ @ on the part of the director , DP , and producers . I have to confess , it 's nice to see a " normal " looking movie sometimes with white whites , black blacks , reasonable midtones , and natural-looking fleshtones . Not everything has to be slammed and stomped on , particularly with a 2-1/2 hour epic that 's real loud and has a lot of fast cuts and explosions . To me , the Marvel movies are a lot closer to a reasonable look as comic-book movies . <p> But I concede it 's a creative choice and ultimately , it 's the job of the colorist to make the client happy . <p> To be fair , I did n't think the video was in any way a critique of the work or skill of the colorist , but rather the choices made by Snyder and WB. 43971 @qwx453971 <p> Yes , exactly . And I kind of see their point : the previous Superman movies were a lot more rich and colorful , not as grim and downbeat as Man of Steel . I was @ @ @ @ @ @ @ @ @ @ 's me . I think the movie showed some hardcore differences between Marvel 's approach to superheroes and DC 's approach . It 'll be very interesting to see how their various movies do over the next 2-3 years . <p> With very powerful grading and vfx tools democratized and in the hands of many , this is the new norm . I fully expect to see people re-grading movies and posting their work to the internet ... Kinda like fans editing Jar-Jar out of Star Wars episode one . <p> No point in getting worked up about it , it 's just another Internet video looking for views , trying to make a living through ad revenue . This will be the new norm , like TMZ for production and post . The only thing that makes me angry is hearing they 're making another Superman movie . What 's the definition of insanity again ? <p> Naaa , the movie is extremely desaturated and biased towards blue/cyan . This is a heavily-styled look , and all popping the sat would do is just emphasize that . These people @ @ @ @ @ @ @ @ @ @ that in order to get anything close to normal fleshtones and skies and grass . 43971 @qwx453971 <p> I was being sarcastic regarding the saturation idea behind the video . IMHO , the original grade is very interesting and gives the movie a bleak , graphic novel look . <p> I hate TV sets that ship in Vivid mode just so that they stand out from the others at the stores . I 'm grading a TV series for which the client asked for a highly saturated look . I went as far as I thought was prudent , but the client asked me to go even further . I told them about the Vivid TV sets , but they were actually evaluating the grade on laptops on two different countries and they thought it should be more saturated , anyway . So I pushed it to the limit but I was very unhappy abou the grade . To my luck , one of the crew members watched the tests on a regular TV and convinced the client to let me tone it down . I was having nightmares about @ @ @ @ @ @ @ @ @ @ blind from excessive chroma ... <p> They just turned a beautiful subtle grade into a cheap TV movie grade . 43971 @qwx453971 <p> I think Man of Steel was many things , but " beautiful and subtle " are not the words I would use . I think it 's an extremely stylized grade , which is what the producers , the director , the DP , and the studio wanted . But I think having to be slammed over the head with a grade like this for 2-1/2 hours is very tiring . My personal opinion is that grades this extreme are better in short bursts -- not for features . I also think too many filmmakers get so wrapped up in looks that they kind of tend to slough off the hard stuff , like character and story . <p> As a long time comic book/sci-fi fan , my take is , you already have a premise that defies belief -- an alien that looks like he 's human , who has superpowers and can fly -- and then you take all the image and bend it so @ @ @ @ @ @ @ @ @ @ on earth . To me , it would make more sense to take a very subtly stylized version of reality to make people believe a fantastic premise like this is happening here , now . Having both a far-fetched premise and an extreme look fights against each other too much . Just my opinion . <p> I do n't deny there are stories that are absolutely appropriate for this look , and that includes TV series , commercials , and features . But Superman is a wholly American mythology that I do n't think needs to look this grim , dark , and desaturated . On the other hand , if I were sitting in the bay and the client asked for a look like this , I 'd naturally give them what they wanted ( plus some options that were less extreme ) . <p> I am a little fascinated with the rough and dark superhero trend of the past ten years . It is like they think it automatically adds an instant layer of reality and complexity to the character . But the interesting part I think is @ @ @ @ @ @ @ @ @ @ anti hero stuff . Like it actually works . Of course I could be completely wrong . 
@@44332539 @4332539/ <h> The History of daVinci Resolve <p> It 's a different thing when you have purpose-built hardware and software designed together . The daVinci 2K was limited to 2K ( and we primarily did HD on it ) , but it was a fast , efficient platform for its time . At the same time , because of its reliance on hardware , you were very limited in how far you could go with it . For example , most of our daVinci 2K 's could only do a maximum of 2 power windows , because each Power Tier ( a physical board ) that supplied additional windows cost $50,000 . And a Defocus Tier was another $50,000 , though it also provided another window . <p> Now , we have virtually unlimited power windows plus ColorTrace and resolution independence for $995 . It 's hard to argue with that . <p> Somebody should write The History of daVinci Systems and talk about the internal war in the company over whether the next-generation color-corrector in 2004 should have been hardware or software . I think they were leaning towards @ @ @ @ @ @ @ @ @ @ able to come out with anything inbetween the 2K and Resolve . People also forget that the early Resolve -- like v4.0 , which I remember with horror at Technique , the original Technicolor D.I . Burbank facility -- took like five seconds to react every time you turned a knob . To me , the program was unusable until about v6 , and I actually got some shows done with v8 . <p> Going back , it 's interesting to see what Grant Petty said right around the time Blackmagic bought daVinci Systems and announced the new version of Resolve back in 2009 . It was interesting that back then , he said he could imagine cutting the price of Resolve from $850,000 down to $500,000 : <p> You probably will see price reduction on the high end systems . I think at the moment the highest end systems are $850,000 , I do n't think we 'll ever sell a product for $850,000 . That 's just my feeling . I just do n't think the cost of the hardware that works with the new software DaVinci just @ @ @ @ @ @ @ @ @ @ GPUs or something like that . And you need three fast Linux PCs , and a bunch of Infiniband stuff . I do n't think that adds up to 800,000 , I think you come in more likely at $500,000 or something like that , $600,000 maybe . But then you 've got some pretty high speed disks to do stereoscopic 4K . We 're checking that out now . <p> The Creative Cow interviewer jokes as to whether we 'd ever see a $995 version of Resolve , which Grant Petty got a good laugh out of . Nobody was laughing in 2010 when that actually happened at NAB in April . <p> Ah , and I just discovered this late-1980s demo for what was eventually called " daVinci Classic " : <p> If nothing else , it shows how primitive things were during this era . I had a moment of deja vu seeing that user interface , which I have n't glimpsed for more than 20 years . Note that even this very simple standard-def color correction system cost well over $100,000 back in the day -- @ @ @ @ @ @ @ @ @ @ Complete Post ( where I worked at the time ) was on the list at the end of the demo video ; it 's sad to reflect that of the 50+ post houses shown in this list , very few of them survive today . <p> I never used a Copernicus , but we did have one of its predecessors , Corporate Communications ' " The System XL , " which never worked . Just as I was starting at Complete Post in 1984 , they yanked it out and replaced it with a Rank-Cintel Amigo , and we used that all the way to the bitter end . That was a flakey system , but I usually could reason out where and when to give it a kick to force it to work . ( Note that inventor Armand Sarabia tied up the whole concept of secondary color correction throughout most of the 1980s with an iron-clad patent , the " Rainbow Patent " ; eventually , everybody was able to get around the patent by doing all the work in the digital domain . ) <p> The Dubners @ @ @ @ @ @ @ @ @ @ in the 1980s and early 1990s . I did n't like the slide-pot interface and always preferred joysticks or trackballs . <p> You know you 're a colorist who 's been around when you go back to TOPSY . I 'll see if I can find some pics and post those ... that was really a " stone knives and bear skins " era . <p> The Copernicus worked pretty well , not as sophisticated as the 888 but functional and nice secondaries , the windows were basically useless . The Dubner was something else , basically sticks and stones with 8 " Floppies ; - ) I did some decent work with it like the following : <p> TOPSY was my first introduction to telecine grading when I made the switch from film timing . I trained on it for only about two weeks , with a Mk III B. It was magical to me , though the guy who was training me advised me that I was lucky I 'd be getting a III C with Amigo . I do n't remember a whole lot about TOPSY operation @ @ @ @ @ @ @ @ @ @ sticker on the panel that said " Topsy Turvy " , which seemed to sum up people 's feelings about the system . <p> When I sat down at " my " new III C with Amigo , I was pretty pleased , although I did have some stomach-clenching system freezes with Amigo , inevitably at the worst possible times . Our secondary color corrector was months late arriving due to the RCA/Sarabia lawsuit but eventually I got that , too . <p> I used Amigo from I believe late 83 until sometime in the early 90s when I first got hold of what I think was an early DaVinci Renaissance . It felt like a huge step forward . I read the manual like it was a novel . <p> TOPSY was my first introduction to telecine grading when I made the switch from film timing . I trained on it for only about two weeks , with a Mk III B. It was magical to me , though the guy who was training me advised me that I was lucky I 'd be getting a III C with @ @ @ @ @ @ @ @ @ @ TOPSY operation , to be honest . 43971 @qwx453971 <p> Nobody who used a TOPSY can forget the infamous " Skull-and-Crossbones " button . Two pushes , and you blow out all the corrections in memory . <p> One huge flaw with TOPSY was that although it shipped with a floppy drive for storage , the drive rarely worked in my experience . Around 1983 or so , Rank-Cintel came out with the VDU display , a CRT that would actually let you see a scrolling list of corrections , which at the time was very innovative . I did a ton of TV and home video work on that system at Modern Videofilm back in the day . <p> When I sat down at " my " new III C with Amigo , I was pretty pleased , although I did have some stomach-clenching system freezes with Amigo , inevitably at the worst possible times . Our secondary color corrector was months late arriving due to the RCA/Sarabia lawsuit but eventually I got that , too . 43971 @qwx453971 <p> Yep , same experience here . Sarabia owned the @ @ @ @ @ @ @ @ @ @ to sue anybody who sold analogue color-correction gear with secondaries , and even facilities that used such equipment . I think he paid $100K for the patent and made more than $10M on it before Copernicus kind of faded away in the early 1990s . <p> I used Amigo from I believe late 83 until sometime in the early 90s when I first got hold of what I think was an early DaVinci Renaissance . It felt like a huge step forward . I read the manual like it was a novel . 43971 @qwx453971 <p> We were horrified with the initial daVinci Renaissance we installed around 1990/1991 , and the manual was really thin , like 10 pages . It barely scratched the surface on what the thing could do , and our engineers absolutely despised it and the company . We eventually switched all the rooms over to Rank URSAs and ARCAS color-correctors in the 1990s , but by the middle of the decade we had gone to daVinci 888 , which was a fantastic , reliable system . Our boss bought about 9 or 10 of them @ @ @ @ @ @ @ @ @ @ credit on new 2K HD correctors by the end of the ' 90s , which proved to be a very smart deal on his part . The daVinci U.S. staff were great to deal with by the time the Renaissance came out , and Dwaine Maggard at the Van Nuys office saved us many , many times in sticky situations . ( And still does to this very day ! ) <p> Nobody who used a TOPSY can forget the infamous " Skull-and-Crossbones " button . Two pushes , and you blow out all the corrections in memory . 43971 @qwx453971 <p> Indeed , I do remember that now that you mention it ! I found it both amusing and evident of a dark sense of humor at Cintel . <p> We were horrified with the initial daVinci Renaissance we installed around 1990/1991 , and the manual was really thin , like 10 pages . 43971 @qwx453971 <p> Hmm , maybe I used a later one , or at least had a more fleshed-out manual , I seem to recall it weighing in around a whopping 100 pages ! Which @ @ @ @ @ @ @ @ @ @ but then again the feature set was puny compared to what we have now . Which brings us back to your original point - we 've come a long way ! <p> The manual is 142 pages long not counting the index , and has no photos . The only graphics are boxes with text inside , showing the contents of various menus . The date at the bottom of every page is 10/01/89 . ? <p> ? <p> When I have more time , I 'll look it over more and maybe scan some pages if anyone is interested . ? <p> Ah , and I just discovered this late-1980s demo for what was eventually called " daVinci Classic " : <p> If nothing else , it shows how primitive things were during this era . I had a moment of deja vu seeing that user interface , which I have n't glimpsed for more than 20 years . Note that even this very simple standard-def color correction system cost well over $100,000 back in the day -- it might have been $200K ! . Glad to see Complete @ @ @ @ @ @ @ @ @ @ on the list at the end of the demo video ; it 's sad to reflect that of the 50+ post houses shown in this list , very few of them survive today . <p> Literally , yesterday a client came in with a vhs tape with a 90 second dark noisy clip that he wanted to clean up ! We have a vhs deck with sVideo that we can route through a DSR2000 DV deck and go SDI from there ! <p> Literally , yesterday a client came in with a vhs tape with a 90 second dark noisy clip that he wanted to clean up ! We have a vhs deck with sVideo that we can route through a DSR2000 DV deck and go SDI from there ! <p> Oh , I 've done tons and tons and tons . Heck , I did two entire 22-episode seasons of an NBC series called World 's Most Amazing Videos that was about 80% VHS surveillance videotapes . Very , very , very hard work , 900-1200 cuts per episode , all done in 12 hours or less . Very @ @ @ @ @ @ @ @ @ @ loved the work . My fingertips were bloody at the end of every session . This was around 1998-2000 or so , on daVinci 888 . <p> Oh , I 've done tons and tons and tons . Heck , I did two entire 22-episode seasons of an NBC series called World 's Most Amazing Videos that was about 80% VHS surveillance videotapes . Very , very , very hard work , 900-1200 cuts per episode , all done in 12 hours or less . Very big ratings ( for reality ) , and the producers loved the work . My fingertips were bloody at the end of every session . This was around 1998-2000 or so , on daVinci 888. 
@@44332540 @4332540/ <h> gammaPATT : little tool to check quality of monitor calibration <p> Hi , I would like to share the little software tool gammaPATT which I just developed for fun . It 's free for everybody . It creates checkerboard like gamma patterns . You can check gamma calibration of your monitor at different Luminance stages . It also shows how your monitor performs at different viewing angles . <p> Please Note : Square in the middle of color frame should appear invisible . Left square should be slightly darker and right square slightly brighter ( RGB or CMY ) . For that purpose both Li and Lo ( Luminance inner and outer ) values should be equal . Click on Li value ( for inner Square ) to adjust to the upper Lo ( for outer checkerboard ) automatically . <p> Hi , Mathias . Assume you have installed German regional setting of Windows . It 's the problem of Windows settings with German decimal numbers " , " instead of " . " You have to got to Systemsteuerung/Zeit , Sprache und Region/Region und Sprache/Weitere Einstellungen Change @ @ @ @ @ @ @ @ @ @ Attached Files : <p> Hi Pepijn , did develop this first on Windows 7 by lazarus . Created subsequently run time version for Mac on Yosemite . Hope there is no support issue for older OS versions by routines I used . Which OS version do you use ... <p> Hi , I would like to share the little software tool gammaPATT which I just developed for fun . It 's free for everybody . It creates checkerboard like gamma patterns . You can check gamma calibration of your monitor at different Luminance stages . It also shows how your monitor performs at different viewing angles . <p> Please Note : Square in the middle of color frame should appear invisible . Left square should be slightly darker and right square slightly brighter ( RGB or CMY ) . For that purpose both Li and Lo ( Luminance inner and outer ) values should be equal . Click on Li value ( for inner Square ) to adjust to the upper Lo ( for outer checkerboard ) automatically . <p> Hi Pepijn , nice to hear that it runs on @ @ @ @ @ @ @ @ @ @ the gammaPATTMac.zip into the same folder . As you mentioned start by means of open up the smaller file . <p> Did n't work with Dispcalgui/Argyll before . Nice app , will try whether it works together with my Spyder5Pro on either Win/Mac . How can I interpret the graph . What corresponds to the green , red and open circles . <p> Math for calculation of Luminance based on the three different colors for gammaPATT is as follows : Li = ( R1/255 ) g+ ( G1/255 ) g+ ( B1/255 ) g/3 Lo= LONG ... <p> L:Luminance g:gamma i : inner square outer frame ( checkerboard ) <p> Both color regions should appear equal as soon as Li is equal to Lo . For that purpose you should of course place your eyes perpendicular to the area you look at . <p> Did n't work with Dispcalgui/Argyll before . Nice app , will try whether it works together with my Spyder5Pro on either Win/Mac . How can I interpret the graph . 43971 @qwx453971 <p> The open circles in the graph are the expected gamma values and the @ @ @ @ @ @ @ @ @ @ values , where green is close to expected and red a bit off . This calibration was setup for sRGB , so the expected gamma curve was 2,2 with a fall of in the shadows . 
@@44332541 @4332541/ <p> I am referring to an association - of colorists , by colorists , for colorists . In much the same way as the ASC , ACE , etc . A Colorist Association would be a non-profit honorary , professional society , dedicated to advancing the arts , sciences , and applications of color correction and to improving the welfare of its members by providing professional enrichment and education , fostering community , and promoting industry standards &amp; recognition . <p> I believe that it 's possible that a group like this will happen one day . <p> A Colorist Association would be a non-profit honorary , professional society , dedicated to advancing the arts , sciences , and applications of color correction and to improving the welfare of its members by providing professional enrichment and education , fostering community , and promoting industry standards &amp; recognition . 43971 @qwx453971 <p> It 's actually been approached before . There is already ACE ( the Association of Cinema Editors ) , the ASC ( American Society of Cinematographers ) , the SOC ( Society of Camera Operators ) , the @ @ @ @ @ @ @ @ @ @ ( Motion Picture Sound Editors ) . All of these , unlike SAG or the WGA or the DGA , are not guilds or unions ; they 're strictly honorary and you have to be invited in . There 's always the potential danger of politics and emotion clouding people 's judgement over someone 's qualifications . The VES ( Visual Effects Society ) is voluntary and I think will let pretty much anybody in , but they 're generally most concerned with trying to keep North American work from being outsourced overseas . <p> I seem to recall that Sam Holtz of Action Video in Hollywood tried to start a colorist society in the mid-1990s , but it did not go well . Too many cooks in the kitchen . Colorists as a breed -- particularly in one area like LA -- are very territorial and competitive . The Hollywood Post Alliance is maybe the closest thing to an organization for post people in general , but is not colorist-specific . And even the HPA has kind of gone down in importance over the years . <p> I think @ @ @ @ @ @ @ @ @ @ issue : people are loosely for it , but not enough people will be willing to do the actual work and come up with the cash necessary to actually make it happen . <p> The way I see it ... the question is n't ' if ' - it 's ' when . ' A colorist society will happen one day . I believe it will . 43971 @qwx453971 <p> I hope it could , but there 's always a resistance to the " Not Invented Here " syndrome . LA and NY are notoriously political , and unless we convinced the major post execs , studios , producers , and post supervisors around town to include the designation in credits , I 'm not sure if it 'll make any difference . <p> If you got the ( ahem ) A-list crowd to join -- I would put Stefan Sonnenfeld , Steven Nakamura , Steve Scott , and Peter Doyle at or near the top of that list -- and they made inroads in establishing the initials after their names in credits , then it could happen . But @ @ @ @ @ @ @ @ @ @ always the danger of encountering the " I would n't join any organization that would have somebody like me for a member " syndrome . <p> A friend of mine looking over my shoulder on the net reminded me there 's also the Casting Society of America ( CSA ) and a Set Decorators Association , plus there 's the Stuntmen 's Association . At least in the case of the CSA , there 's a tradition of giving them the initials after their names in credits . <p> There is an interesting trend where some lead colorists have changed their job title to " Lead Finishing Artist " in major film credits , which surprised me . I can see where the line blurs when the job crosses from conforming to VFX to color and deliverables . Me , I 'm glad when they spell my name right on the paycheck . <p> I do n't think many people would disagree with the idea of a society for colorists , but from the stories I 've heard it 's been attempted a number of times , and failed largely @ @ @ @ @ @ @ @ @ @ and emotion ... Too many cooks in the kitchen ... Colorists as a breed being very territorial and competitive ... people are loosely for it , but not enough ... to do the actual work . 43971 @qwx453971 <p> In each case it 's also apparently been very difficult to come to a consensus on the primary objective for the group , as well as how inclusive or exclusive it should be . <p> Many people seem to reference the ASC , but that 's a very exclusive , invite-only organization with extremely high initiation fees and yearly dues . Granted , if you 're an ASC-caliber DP , you can probably afford the fees , but is that the kind of group all of us would actually want ? The " ASC " designation means as much as it does largely because of that exclusivity . If a colorist society were formed in a similar mould , few , if any of us would ever even make the cut . <p> Then you have the other extreme in groups like the Visual Effects Society , as Marc mentions , @ @ @ @ @ @ @ @ @ @ usually formed around common , necessary objectives , like creating health benefits , improving working conditions or promoting the industry in a given region . Everyone is included , everyone is allowed to join , yet because of that , membership grants you the same rights , privileges , and relative esteem of a local gym membership . <p> I 'm sure there are also infinite combinations of the two , but that 's probably the catch-22 of all of this . What we all want ( ASC ) would exclude most of us , and what would be useful ( VES ) is n't exciting or necessary enough yet for most of us to bother with . And whenever we do get motivated enough to take a stab at it , apparently we do n't have enough discipline as a group to come up with a concept and see it through . 
@@44332542 @4332542/ <h> What would you consider the correct usage of a color LUT ? <p> With the release of Osiris by VisionColor , it got me thinking ... I know there are different types of LUTs , used for different purposes ( calibration for example ) . But there has been discussion in a Facebook group recently that I am part of where this is not the " proper " use of a LUT . <p> However , I have read through some of the other posts here on the forum and I 'd have to disagree . I understand that they are n't a replacement for color grading overall , and are n't  a magic " look it 's a great video " solution . However , I still think they have their place . <p> So after reading some things , and taking the Osiris collection as an example , how would you , as colorists , suggest these be applied ? Footage .... then general color correction ( exposure , W/B ) , apply LUT , possibly some additional grading/adjustments afterwards ( power windows , hue/saturation adjustments @ @ @ @ @ @ @ @ @ @ but is that what you would consider the most basic of a workflow for this ? <p> I personally have had some success with setting the image up for the LUT to be applied , applying it , and dialing back some of the saturation on some colors if I deemed to it to be too much . Thoughts ? <p> I am trying to figure out this answer as well . I think it can be used however you want to use it . I find more controlled results using it at the end of a node tree in Davinci . Placing a LUT in the start will create a baked in contrast that is n't always what you are looking for . I 'll post some images from a project I am working on . <p> As you can see there is a significant difference . Left is the LUT at the end of the tree and the right is at the beginning . LUT 's seem to clip data in highlights and shadows . <p> From what I know/understand , they should be used at the end @ @ @ @ @ @ @ @ @ @ been using Premeire . I still do all my corrections beforehand . I seem to get better results . I have n't experienced any of the clipping or crushing though . ( well maybe a little on " M31 " from the Osiris collection , but that could just be attributed to the footage I was using ) . I 'd still be very curious about others ' usage also . I have n't really had an issue with using these . I was just wondering if maybe I could be utilizing them better . And yes , I agree about the usage . If you get a good visual ..... you get a good visual . No need to analyze the technique if the results are pleasing , I suppose <p> I 'm no expert , I 've noticed these " stylized " LUTs clip a little bit as part of the ' look ' , so make sure you do your balancing and primary pass before applying a " stylized " LUT on a separate node . That being said , I do not believe this would the correct @ @ @ @ @ @ @ @ @ @ like the ones created by Juan Melara . Any of the experts want to jump in ? <p> From what I know/understand , they should be used at the end . I am familiar with Resolve , but I have been using Premeire . I still do all my corrections beforehand . I seem to get better results . I have n't experienced any of the clipping or crushing though . ( well maybe a little on " M31 " from the Osiris collection , but that could just be attributed to the footage I was using ) . I 'd still be very curious about others ' usage also . I have n't really had an issue with using these . I was just wondering if maybe I could be utilizing them better . And yes , I agree about the usage . If you get a good visual ..... you get a good visual . No need to analyze the technique if the results are pleasing , I suppose 43971 @qwx453971 <p> If you are doing it in premiere you probably wo n't notice the clipping as much . @ @ @ @ @ @ @ @ @ @ the different color channels and luminance values . Juan Melara does a good job talking about it in a blog post . <p> If you are doing it in premiere you probably wo n't notice the clipping as much . The clipping is a lot more evident in DaVinci in the different color channels and luminance values . Juan Melara does a good job talking about it in a blog post . 43971 @qwx453971 <p> Hmmm , thanks Taylre . I 'll try to locate that blog post . If you know where it is located , I would appreciate if you could link me there . <p> Thanks Frank . I had seen that post before but read through it again . Very thorough , and reassuring . It 's similar to my workflow with some adjustments as I do n't use Resolve . I was just doubting myself as people were insisting that LUTs are n't to be used for that purpose . And I understand that they are n't just presets that you drop on your footage , but I digress . I 've gotten good results @ @ @ @ @ @ @ @ @ @ I please . <p> By the way , I saw the " The One " . Great story line , fantastic grading . <p> For anyone who is interested I uploaded some old footage I had and did some grading on them . If you want to check it out , feel free . If not , that 's ok . My intent was n't to show my work , just ask a legitimate question . 
@@44332543 @4332543/ <h> help for " metallic " skin tones of mad max <p> Hi , I 'm coloring a music video and they asked me if I can make it similar to mad max , expecially for the skitones etc. it seems like a very specigic s-curve + saturation adjustment on the skintones but I would like to know what would be your approach to achieve something similar . I'ts not really HDR I think , it 's mostly a sharpen thing . below are some references <p> I 'm not looking for a magic lut just what woul be your approach . The footage I 'm coloring has been shot with Red epic in the desert so I have sand + sky like madmax . One note , I 'm not really looking for the warmth as much , mostly for the contrast+sharpen effect . Thank you <p> sorry only jpegs the last one is the red original in redlog format . the others are my experiments . the color tone is probably going to be very different in the end . I think for the metallic look is @ @ @ @ @ @ @ @ @ @ any other method I 'm here . Thank you <p> I 'm thinking one difference between the work you are showing and the reference is that in Mad Max the backgrounds ( especially in your reference images ) are very low contrast , and therefore the subject looks more contrasty without needing to crush the actors too much . On your work the sky and the clouds have a very nice contrast , while most of your talent is in the dark shadows-area . This , to my eye , makes it harder to add a feeling of sharpness to the image . So I 'd suggest that you try to decrease the lighting ratio . <p> Also , in your references there is actually very little skin visible . It 's hard to add that sweaty shine when there 's no sweaty skin available . <p> I 'm thinking one difference between the work you are showing and the reference is that in Mad Max the backgrounds ( especially in your reference images ) are very low contrast , and therefore the subject looks more contrasty without needing to @ @ @ @ @ @ @ @ @ @ sky and the clouds have a very nice contrast , while most of your talent is in the dark shadows-area . This , to my eye , makes it harder to add a feeling of sharpness to the image . So I 'd suggest that you try to decrease the lighting ratio . <p> Also , in your references there is actually very little skin visible . It 's hard to add that sweaty shine when there 's no sweaty skin available . 43971 @qwx453971 <p> this is a great idea , about the face , sorry I posted just the one that I had available , But this contrast difference is a good idea for sure . <p> The production stills had a very different look to the theatrical release but if you want to isolate the skintones from the rest of the image then Juan Melara 's workflow is a great starting point if you have n't seen it already . <p> The grade of the Mad Max movie is quite different from the production stills , the stills are quite desaturated , while the movie itself is @ @ @ @ @ @ @ @ @ @ which is the reference ? <p> not to mention in the productions still the increaddably crappy mask that floats out into the sky around the bg .. and mr hero has a halo ... sheeesh .... the movie show no similar crimes against qc ... guess there 's no need to get production stills past qc .... 
@@44332544 @4332544/ <p> But at the end of the day a 10 bit Slog2 file has a finite amount of possible range it can store . I 've seen it written down as 18 in total but not convinced about that . <p> If i take a linear RAW file with , for example , 11 stops and transcode that via the various LUTs out there ( Resolve VFX-Slog2 , ACES , 3rd Party Custom ones , SG etc , . ) . It appears that the range these 11 stops goes into is up to about CV 640 then clips . Now if the Slog is a finite container then it does make sense that my 11 stops would only go so far into that container . <p> To reverse this back into Linear ( i use Nuke ) i have an Slog2 formula that unbends it back to Linear space . But it is n't bending it back properly . ( I can compare a camera Slog2 version with one processed via a RAW pipeline and it 's very different ) <p> Now if i take Slog2 camera originated @ @ @ @ @ @ @ @ @ @ to clipping 1023 . However if Slog2 is supposed to be an 18 stop container then how is this possible with a camera doing 11 stops ? <p> So is there any link to scene referred linear within an Slog2 camera originated file ? Perhaps not , perhaps each camera ( whether it 's 6 stops , 12 stops or 16stops is scaled to use the whole Slog curve ) <p> So how are people dealing with this in grading ? <p> thanks for any pointers ( even if it 's a better place to ask this question , i 'm here because the level of knowledge is very high ) <p> Oh entirely possible ( in terms of making it more complex ) . I need to balance footage in a number of different formats ( different profile curves , Slog , RAW etc , . ) . My approach is to get them all to match in scene linear , within Nuke and then i can match between them a lot better . <p> In terms of generating there have been several approaches . In Resolve with DNG @ @ @ @ @ @ @ @ @ @ Or into ACES ( CinemaDNG IDT ) . Both of these produce very restricted ranges compared to a camera sourced Slog . Other attempts include BMDFilm -&gt; Slog2 and also the cinelog profiles . In all cases the resulting log files scale the range back . There is no clear method in Resolve to get a full range log file out from RAW DNG that i 've found . ( CinemaDNG for example does n't do highlight reconstruction , Rec709/Linear does n't appear truly linear , BMDFilm is an unknown quantity , a derivate of a LogC curve with BM Camera colour compensations in ) <p> It dawned on me that a camera source Slog file may not be scene referred , instead the full range of whatever each camera can do is stored using the full Slog curve . So a theoretical 6 stop camera would use the same 90 to 1023 range that a 12 stop camera would ( if it was scene referred then the 6 stop camera would stop half way up the curve ) . The encoding curve is Slog but both flavours are holding @ @ @ @ @ @ @ @ @ @ case . <p> Sadly not for cinemaDNG files , there are no settings for ISO at all . As i mentioned , cDNG is the wild west of RAW , there are so many caveats and problems with support overall . I 'm keen to here of any alternative methods to debayer them . <p> Now if i take Slog2 camera originated footage then i see full range , all the way to clipping 1023 . However if Slog2 is supposed to be an 18 stop container then how is this possible with a camera doing 11 stops ? So is there any link to scene referred linear within an Slog2 camera originated file ? Perhaps not , perhaps each camera ( whether it 's 6 stops , 12 stops or 16stops is scaled to use the whole Slog curve ) So how are people dealing with this in grading ? 43971 @qwx453971 <p> if by " linear " you mean Rec709 color space , I just pull the XAVC footage in as SLog2 and then come up with a base grade , either using curves or corrections ( or @ @ @ @ @ @ @ @ @ @ on from there . Did they give you any charts to start with ? I find a gray scale chip chart helps a lot , or maybe one of the DSC gray/color charts . <p> I do n't treat SLog any different than RedLogFilm or DPX log images . They are different -- as I 've often said , if you get log film scans from five different post houses , you 'll have five different looks -- but only the results matter . <p> Are you delivering log in the end , Rec709 , or what ? Are you correcting in Nuke or Resolve ? <p> Are you delivering log in the end , Rec709 , or what ? Are you correcting in Nuke or Resolve ? 43971 @qwx453971 <p> Real linear Full on scene referred floating point goodness . Yes , i 'm doing technical grades in Nuke along with comping and vfx work . The technical grades involve getting various formats of footage into the same ball park ( colour and range ) and perhaps shadow and exposure work ( Nuke is much better at tracking than @ @ @ @ @ @ @ @ @ @ shot with the same camera but through 709 profile curve with tweaked knee and toe . Therefore i have to reverse that profile to unfold it back to scene linear - this helps me match . All the colourspace is 709 AFAIK , even the RAW stuff - It 's not SGamut . <p> If i take Slog2 footage into Nuke through an appropriate look up curve then i will see a full range image from 0 to 13/14 . In order to view that i have a viewer process at the end that adds a soft clip to bring the overbrights into range and also in my case a look LUT too . So i can work in full linear ( which for bokeh effects and blur is essential ) and switch on a viewer that will show me closer to the final intent . <p> Same deal with Alexa footage in LogC , Cineon , etc , . <p> My challenge has been to find a route to transcode from Linear RAW into a format that i can deal with , ideally Log . This is a no @ @ @ @ @ @ @ @ @ @ EXR all the footage . I 'd ideally like to be using the cineform 12 bit 444 RGB container . <p> Linear raw generally is already debayered compared to true camera raw . Odessey FS raw has baked white balance and colour space . 709 which both suggest debayering has been done before putting into the cDNG . <p> Have no experience of the Odessey raw or sLog2 . But for the purposes of grading is there any real point in linearizing , unless you are doing some comps and blending . It 's just removing gamma nothing more , good for comping and blending but that 's it . ' Scene Linear ' is relative right ? , not a defacto uniform ' space ' in every regard . Two cameras with differing sensor capability , gamut and DR both providing camera raw will still almost certainly give different renditions of a scene . <p> If you 're using Nuke with OCIO . OCIO is available for Resolve 10 via OFX , TuttleOFX , LUT 's , CTL and math work . Maybe that might help . Again not @ @ @ @ @ @ @ @ @ @ maybe to do with LUTs 0-1 clipping . Perhaps a shaper LUT first , du n no . <p> Yes tried those ( and various versions of ) , there are some serious colour issues with these LUTS , especially in the Reds . Also the recommended workflow is to debayer via CinemaDNG which does not offer highlight reconstruction . They 're more than just doing 2.2 ( although Resolve appears to add a 2.2 anyway ? ) . If i push an artificial RGB ramp through these LUTs i can see what they 're doing . So not 2.2 <p> I 'll look into OCIO in Resolve 10 , i thought it was 11 that had OFX support ? <p> And yes needing the linerisation for vfx and post work , not just grading . <p> The RAW DNGs do not have white balance baked in and it 's not clear what the colourspace is . The camera has to be set to Slog2 but the data in the DNGs is most certainly not Log . I think the general feeling is the colourspace is Rec709 and the RAW @ @ @ @ @ @ @ @ @ @ much Linear . I can poke around the bayered files with RawDigger and mess with the actual values before linearisation and matrixing . <p> If you can , send samples of your images . Maybe I can take a look . I know of exactly what you are looking for , but I am not sure what the best approach will be without doing some testing . In the past I have had to deal with this kind of situation as Comp Sup . <p> Careful with OCIO plugging for Resolve ... It does not have GPU support . You can probably achieve the same results by baking cube luts from Nuke . <p> Yes tried those ( and various versions of ) , there are some serious colour issues with these LUTS , especially in the Reds . Also the recommended workflow is to debayer via CinemaDNG which does not offer highlight reconstruction . They 're more than just doing 2.2 ( although Resolve appears to add a 2.2 anyway ? ) . If i push an artificial RGB ramp through these LUTs i can see what they 're @ @ @ @ @ @ @ @ @ @ the raw files you have are from the Odessey 7Q ? So it 's either Canon 500 raw , FS700 raw or ARRI raw . Do you know which you have . Again sorry I do n't know these formats , just been looking at various external recorders so I 've read about the Odessey that 's all . Highlight reconstruction or Recovery , different things ? Reconstruction Resolve does n't have for any raw format afaik . Highlight Recovery option yes , but if it 's not offered then again it suggests debayer and WB are baked . <p> I 'll look into OCIO in Resolve 10 , i thought it was 11 that had OFX support ? 43971 @qwx453971 <p> 10 and 10 lite . As Abel says perhaps not GPU assisted , I did read recently Resolve used GPU for OFX but that may be in 11 . Only reason I mention LUT 's in OCIO is maybe avoid possible 0 - 1 clipping in Resolve , if it does , but others would be able to confirm that . <p> The RAW DNGs do not have @ @ @ @ @ @ @ @ @ @ the colourspace is . The camera has to be set to Slog2 but the data in the DNGs is most certainly not Log . I think the general feeling is the colourspace is Rec709 and the RAW ( as far as i 've tested ) is pretty much Linear . I can poke around the bayered files with RawDigger and mess with the actual values before linearisation and matrixing . <p> I 'm not wishing to argue because again I 've only read the manual due to prospective purchase , you have the raws in front of you but ... ' Linear raw ' is a specific type of raw which may already be debayered as well as baked WB . I think you 're using the term Linear raw as ' Linear ' is your thing but no real need to qualify raw is linear . Saying Linear raw may suggest to others as well as me not camera raw but one of the other types , the more restricted raw . Or Linear DNG even . <p> From my limited understanding ' real ' camera raw to display @ @ @ @ @ @ @ @ @ @ 709 and I mean 709 colour primaries , then to and fro between Debayer &lt;-&gt; WB Adjustment to get optimum interpolation , WB adjustment being the RGB or RGGB or whatever combination channel multipliers , whichever raw RGB channel gets to sensor saturation level first from exposure settings in camera and temperature of the light in the scene , the remaining RGB channels get multiplied out until ' white ' is achieved ie : white balance for the chosen colour space , then black level is chopped off and the remaining scaled into the bit depth of the container , hence your queries as to why a given bit depth can hold 6 stops and 12 stops , the levels are just scaled to fit the bit depth . In more involved raw development like Lightroom , dcraw or libraw you can decide whether to scale to fill or not . <p> If you were shooting raw into 16bit linear light then there 's plenty of levels available per ' stop ' so in that case you may have your 6 stops only filling a 1/3rd of the 16bit levels @ @ @ @ @ @ @ @ @ @ you can make the choice to scale all into display space , ie : Highlight Recovery , which is n't really recovery at all , just means you can see it all in display space without adjustment to pull down the highs or leave Recovery off and pull down the highs after to see them through the rec709 gamma ' window ' . Highlight Reconstruction on the other hand is where you have literally clipped in the raw data , full well , saturation and want to steal from the non saturated data later in the raw development process with some blending or neutralizing to grey , for example combating pink highlights . <p> Here 's a bit more on FS700 raw from the Odessey 7Q . Matt states WB is baked . I think debayered as well because if you can set WB in the raw what 's the point in debayering , the two work together . <p> I do n't understand though why he 's chosen BMD Film for colour space , it 's a Sony camera not a BMD . I thought BMD Film was for @ @ @ @ @ @ @ @ @ @ camera ' space ' route is n't it ? Proprietary secretive camera science transferred correctly to a known documented colour space for processing and display . <p> I know this is straying from your initial post , you really need someone to come along who 's used these formats , but in the meantime maybe some of this may help with some of your initial queries . <p> To clarify the raw files you have are from the Odessey 7Q ? So it 's either Canon 500 raw , FS700 raw or ARRI raw . Do you know which you have . Again sorry I do n't know these formats , just been looking at various external recorders so I 've read about the Odessey that 's all . Highlight reconstruction or Recovery , different things ? Reconstruction Resolve does n't have for any raw format afaik . Highlight Recovery option yes , but if it 's not offered then again it suggests debayer and WB are baked . 43971 @qwx453971 <p> Thanks for taking an interest . The O7Q has a license to decode the compressed Sony RAW from @ @ @ @ @ @ @ @ @ @ that . Instead they have to transcode it to uncompressed cDNG files . I do n't know exactly what happens in this transcode , i assume that it is a fairly straight route from decompressed Sony signal to DNG . The resulting DNG files are very simple , they are Linear ( have no shaper LUT in them ) , 12 bit and have a simple colour matrix in . <p> I can access the raw data in these files via the command line ( dngvalidate ) and also with apps like RawDigger . <p> So I 'm not wishing to argue because again I 've only read the manual due to prospective purchase , you have the raws in front of you but ... ' Linear raw ' is a specific type of raw which may already be debayered as well as baked WB . I think you 're using the term Linear raw as ' Linear ' is your thing but no real need to qualify raw is linear . Saying Linear raw may suggest to others as well as me not camera raw but one of the @ @ @ @ @ @ @ @ @ @ DNG for example . 43971 @qwx453971 <p> It 's a fantastic recorder and i can recommend it wholeheartedly , most of my queries stem more from the camera . The data is linear in nature and 12 bits is n't very much space . There is a small curve at the knee and toe to give the sense of &gt;12 stops but with RawDigger you can see the actual values being recorded and quantify for yourself the linearity of the files . <p> From my limited understanding ' real ' camera raw to display space process involves , choosing the output colour space say 709 and I mean 709 colour primaries , then to and fro between Debayer &lt;-&gt; WB Adjustment to get optimum interpolation , WB adjustment being the RGB or RGGB or whatever combination channel multipliers , whichever raw RGB channel gets to sensor saturation level first from exposure settings in camera and temperature of the light in the scene , the remaining RGB channels get multiplied out until ' white ' is achieved ie : white balance for the chosen colour space , then black level is @ @ @ @ @ @ @ @ @ @ of the container , hence your queries as to why a given bit depth can hold 6 stops and 12 stops , the levels are just scaled to fit the bit depth . In more involved raw development like Lightroom , dcraw or libraw you can decide whether to scale to fill or not . 43971 @qwx453971 <p> AFAIK the DNG process takes the raw data , applies an optional embedded 1D LUT to turn the raw data into Linear space ( these files do n't have that , whereas the BMC cameras do ) applies black and white levels and scales the result to 16 bit linear whilst the white balance and embedded matrixes turn the camera RGB filter colours into XYZ space . DNG offers a number of various ways in which these colours can be matrixed ( some use a to and fro to work out the best way as you say ) but these DNGs use a very simple matrix for all colour temperatures . My belief is that the colourspace of these cameras are natively Rec709 . <p> If you were shooting raw into 16bit @ @ @ @ @ @ @ @ @ @ ' stop ' so in that case you may have your 6 stops only filling a 1/3rd of the 16bit levels available ? It 's at the point of scaling that you can make the choice to scale all into display space , ie : Highlight Recovery , which is n't really recovery at all , just means you can see it all in display space without adjustment to pull down the highs or leave Recovery off and pull down the highs after to see them through the rec709 gamma ' window ' . Highlight Reconstruction on the other hand is where you have literally clipped the raw channels and want to steal from the non clipped channels and try some blending or neutralizing to grey , for example combating pink 43971 @qwx453971 <p> 16bit linear raw does have more levels available per stop , compared with 12 for sure . The files are scene referred and the Green channel is way more sensitive than the others . Which means when green clips you can recover as much as a stop by using the R and B channels to rebuild @ @ @ @ @ @ @ @ @ @ i think highlight recovery is doing . So in my case in Resolve , if i bring in an Slog2 wedge then it clips a stop earlier than the RAW version with highlight recovery on . Turn that off then the RAW shows the same clipping as Slog . <p> My comparison is between a camera originated Slog2 file vs a linear RAW file . I was wondering whether camera originated Slog2 files are not scene linear but scaled to fit the entire Slog2 curve . If a camera is capable of 12 stops then should it hit white in an ' official ' Slog2 file ? Well in the camera originated files they do , in files generated from Linear RAW they do n't . I 'm wondering whether it 's as simple as the range of an Slog2 file depends on the particular camera generating it . In the case of the FS700 it might be 12/13stops , in an F55 it might be 14/15 stops etc , . ? <p> Here 's a bit more on FS700 raw from the Odessey 7Q . Matt states WB is @ @ @ @ @ @ @ @ @ @ can set WB in the raw what 's the point in debayering , the two work together . 43971 @qwx453971 <p> If i set up a scene , record some frames with the camera at 3200 then record some more at 5600 the results should be different right ? They 're not . And that 's looking at the raw data , pre de-bayer . Of course now you 've got me questioning my sanity as i did those tests a month or so back now ... I 've been round the block so many times on this stuff ... <p> I do n't understand though why he 's chosen BMD Film for colour space , it 's a Sony camera not a BMD . I thought BMD Film was for BMD colour science that 's the whole point of the camera ' space ' route is n't it ? 43971 @qwx453971 <p> Because it sort of looks like it works . But as you say it does n't really and this is my point and same for anyone that 's looking in detail at this . There are no @ @ @ @ @ @ @ @ @ @ I 'm testing out some SpeedGrade stuff hopefully this week that may offer another route . The colour in a BMDFilm processed file ( which internally takes the linear output of a debayer DNG into a log representation ) is pretty good but there is no information on the nature of the BMDFilm secret sauce and appears to be protected by BMD . No idea why . It 's a derivative of LogC , that 's all i could get out of them . <p> I know this is straying from your initial post , you really need someone to come along who 's used these formats , but in the meantime maybe some of this may help with some of your initial queries . 43971 @qwx453971 <p> Honestly ? I do n't think many people are using RAW with this and the 700 . The O7Q offers an in recorder 4K -&gt; 1080p ProRes Slog2 which does an amazing job . I want 4K for VFX work and the ability to denoise at 4K before downsampling . I suspect i 'm sitting on the edge of a very bloody @ @ @ @ @ @ @ @ @ @ from people using 4K RAW in this combination . As i mentioned DNG support is the wild west at the moment ! 
@@44332545 @4332545/ <h> PLAYBACK OUT OF SYNC BUT SRUB IN SYNC <p> I have a documentary sent to me that was cut in FCP 7 and sent through Resolve to give me an AAF with DNxHD footage for my Symphony 7 . I have random clips falling out of sync and a sporadic drag to throughout on playback . If I scrub through in real time , they all play in sync . We also found that when we set in color and split the screen to see with and without color , the without side falls out of sync from the color side . Brainstorming and troubleshooting and hoping someone out there has run into this before . Looks like I have some HDV footage , GOP goodness . 
@@44332546 @4332546/ <h> Group Buy : Special Editions of LightSpace For Resolve . <p> Hi , As request by many , here the special thread for LightSpace For Resolve . Steve Shaw has offered to do a group buy on a Special Editions of LightSpace For Resolve . We need to get a minimum of 25 people committed for purchased this version . Two version will be available as Resolve &amp; Resolve Light . The light edition is limited to the probe i1 Display 3 OEM . The list price are : -1625.00 &amp; -465.00 ( we will get a 50% discount on top ) . The ideas is the get the finesse of this calibration made for resolve . - Export of LUT to Davinci Resolve as Native LUT file . Please send me a private message if you are interested to purchase it . Thanks guys . Fred <p> Hi Steve . I 'm not trying to be negative or to start another LightSpace vs Calman . All I would like to point out , that Calman for Resolve cost $99 ( with the special coupon ) with i1 @ @ @ @ @ @ @ @ @ @ . It is a hell of a deal . While LightSpace Resolve special is a great deal as well , the price of the package with the special discount comes to $388 for LightSpace and i1 Display Pro OEM on your site is $376 for a total of $764 , which is just about double price of Calman package . Granted , users easily can buy i1 Display Pro OEM from the other site . Nevertheless , I think that may be the reason for the lack of interest for this special . Said that , I would like to thank you for being extremely responsive to the users and for continuing to offer a great service to the post community . <p> Calman will always be cheaper - and if it works for you ( or anyone else ) go for it . <p> But actually , the main reason for the lack of interest is that most users that contact us want to be able to generate other LUT formats , and prefer to use a LUT box , or a display with LUT capability . <p> We @ @ @ @ @ @ @ @ @ @ the ' special ' , but virtually all have then purchased the Full LightSpace . We have also sold a number of the Resolve specials as those users had immediate need - and could n't wait for the group buy to complete . <p> Hi Steve . I 'm not trying to be negative or to start another LightSpace vs Calman . All I would like to point out , that Calman for Resolve cost $99 ( with the special coupon ) with i1 Display Pro OEM for $249 for a total of $348 . It is a hell of a deal . 43971 @qwx453971 <p> Hello Jake , where 's that special coupon is located ? I see 299$ as it 's CalMAN for Resolve Software price right now . <p> Hi Steve . I 'm not trying to be negative or to start another LightSpace vs Calman . All I would like to point out , that Calman for Resolve cost $99 ( with the special coupon ) with i1 Display Pro OEM for $249 for a total of $348 . It is a hell of a deal @ @ @ @ @ @ @ @ @ @ well , the price of the package with the special discount comes to $388 for LightSpace and i1 Display Pro OEM on your site is $376 for a total of $764 , which is just about double price of Calman package . Granted , users easily can buy i1 Display Pro OEM from the other site . Nevertheless , I think that may be the reason for the lack of interest for this special . Said that , I would like to thank you for being extremely responsive to the users and for continuing to offer a great service to the post community . 43971 @qwx453971 <p> where exactly can this be bought ? seems like they only sell through resellers , and the code does n't seem to be working anywhere . <p> Hi Steve . I 'm not trying to be negative or to start another LightSpace vs Calman . All I would like to point out , that Calman for Resolve cost $99 ( with the special coupon ) with i1 Display Pro OEM for $249 for a total of $348 . It is a hell of @ @ @ @ @ @ @ @ @ @ deal as well , the price of the package with the special discount comes to $388 for LightSpace and i1 Display Pro OEM on your site is $376 for a total of $764 , which is just about double price of Calman package . Granted , users easily can buy i1 Display Pro OEM from the other site . Nevertheless , I think that may be the reason for the lack of interest for this special . Said that , I would like to thank you for being extremely responsive to the users and for continuing to offer a great service to the post community . 43971 @qwx453971 <p> Hello Jake , a detail that may have missed for this CalMAN for Resolve Licence/Offer is that is locked to only REC.709 - D65 - BT1886 only ! , no other colorspace or gamma target or white point option is available at this CalMAN for Resolve licence and you have to upgrade to a higher license level to get more options . <p> With price , as was mentioned above by Adrian , you do need to take supports costs , @ @ @ @ @ @ @ @ @ @ is unique in that is a one-off payment , with no additional costs - for life . <p> Hello Jake , a detail that may have missed for this CalMAN for Resolve Licence/Offer is that is locked to only REC.709 - D65 - BT1886 only ! , no other colorspace or gamma target or white point option is available at this CalMAN for Resolve licence and you have to upgrade to a higher license level to get more options . <p> Yes , you are correct . But for the overwhelming majority of users Rec-709 is all that they will ever need , if they only work on video delivered projects . Why pay more , if that is all that is needed ? Beside , all we 're talking here is just about the $99 dollars for the license <p> I am blind then , as hi ca n't seem to find where to add the resolve edition to the cart . 43971 @qwx453971 <p> This is weird , ' cause two weeks ago you could buy it directly from their site using the coupon . I just looked @ @ @ @ @ @ @ @ @ @ directly thru them . So you 're not blind , or we both are . 
@@44332548 @4332548/ <h> Why Use a Color Test Card ? - Interview with David Corley of DSCLabs <p> Podcast interview with David Corley CEO of DSCLabs . David tells some great stories about the creation of color test charts and how his company is working with BlackMagic and the new Resolve 11 software . Turns out DSCLabs is working on quite a number of fascinating new projects actually . Tom <p> BTW ... David Corely reached out to me after the interview was posted and asked if I would mind updating the blog post with some personal stories , additional history and acknowlement of his wife 's contributions to the growth and success of the DSCLabs . If you should have a chance to pop back over to the blog I think you 'll find David 's words fascinating and heart warming . LONG ... <p> Yes , a lot of the early LED lighting panels were very spiky and had weird color issues . You could balance them out to a point , but there was always something weird and non-linear about them . Now , they look a lot @ @ @ @ @ @ @ @ @ @ emphisis on skin tones . David is such a wonderful man . A NAB 2012 , he gave me 20 minutes , one on one . I walked away with so many insights for camera set up and post production from that encounter , I 'll always be grateful . It raised my level 10 fold . Just wrapped up a v 10 project , and gearing up for the next one using v11 . I 'll be in Texas , scouting locations mid week , then back home to fully vent Sony F55/Resolve v11 . I carry the 24 chart , and have added Art , s DSC single shot glossy as well as the X Rite Chart to my kit . Not having as yet to test the new Match function out , I 'm wondering if anybody has any experience to share . I 'll post this weekend on my tests . Marc , Do I take , you have used Match for some LED scenes . Very curious as to your thoughts on this . I hate LED , but admit I have added some lite @ @ @ @ @ @ @ @ @ @ . &gt;3K output at 4 amps is hard not to like , but the spikes , even with -green make me cringe on set . <p> Tom , Great interview . Loved his emphisis on skin tones . David is such a wonderful man . A NAB 2012 , he gave me 20 minutes , one on one . I walked away with so many insights for camera set up and post production from that encounter , I 'll always be grateful . It raised my level 10 fold . Just wrapped up a v 10 project , and gearing up for the next one using v11 . I 'll be in Texas , scouting locations mid week , then back home to fully vent Sony F55/Resolve v11 . I carry the 24 chart , and have added Art , s DSC single shot glossy as well as the X Rite Chart to my kit . Not having as yet to test the new Match function out , I 'm wondering if anybody has any experience to share . I 'll post this weekend on my tests . 43971 @qwx453971 @ @ @ @ @ @ @ @ @ @ and various tests with those charts . Where in Texas will you be - any chance you might be in Austin ? Would love to catch up with you . Thank you for the note on the show . Appreciate that ! <p> Do I take , you have used Match for some LED scenes . Very curious as to your thoughts on this . I hate LED , but admit I have added some lite Panels , along with an Eco Punch to my truck . &gt;3K output at 4 amps is hard not to like , but the spikes , even with -green make me cringe on set . 43971 @qwx453971 <p> No , I 've only done it with eyes and hands and scopes . I 'm not convinced an automatic method is a viable solution over human skill , because there 's too many variables . And people still tend to shoot charts badly . If they obey all the rules , it can absolutely work . <p> But as one example : what if the shot starts outside , under a tree , then moves @ @ @ @ @ @ @ @ @ @ a house and into a kitchen ? How 's the chart going to do then ? There 's always going to have to be some human intervention here . Half the time , I wind up having to compensate for poor judgement on the part of the camera operators , who are trying to change exposure and wind up making things worse than if they had just left it alone . <p> Half the time , I wind up having to compensate for poor judgement on the part of the camera operators , who are trying to change exposure and wind up making things worse than if they had just left it alone. /quote <p> For 30 years my primary job has been DP in long form news doc and series type programs , and I stand guilty as charged for changing exposure during those exact scenarios when baking in a 709 signal . Only once , did an editor , not a colorist , call me with the same charge . I 'm sure there were dozens of times . Since shooting SLog/2/3 , I have experienced the benefits @ @ @ @ @ @ @ @ @ @ light cause an unwanted exposure change . During those extreme changes , I 'll try to find a natural edit point , and stop , adjust iris and start a new clip . <p> great but I would like to ask what that 's device he used to adjust for example gain ? Cause in our studio we use only RED cameras so how can I apply this aligment to RED cameras ? Or it will be handled in post ? maybe my questions are wrong but I hope some people can explain it to me Thank you 
@@44332549 @4332549/ <p> When analyzing their resulting shade palette in Rec 709 YCbCr color space you come up with the conclusion that skin color should be aimed at a Hue between &gt;113- and &lt;144- . Average corresponds to flesh line in vectorscope . <p> Saturation depends on Luminance Y very nicely with maximum close to Y = 50% as shown below . It easily fits to a inverse parabola function . Saturation results between 13% and 25% . In addition , Saturation behavior is nearly independent of Hue value . <p> Would be great if you could tell what impression do you have about saturation of natural skin . What 's about its memory color . <p> I 've worked a lot on retouching and color correcting fashion and beauty images , where the skin color is very important . I 've always tweaked it by eye , as the color varies case by case , so no formula or swatches will work . Sure there are ballpark numbers like this , but you should be able to see if it 's right or not , and not rely on numbers @ @ @ @ @ @ @ @ @ @ is that you can extract rule of thumbs which are proved to be valid especially for beginners . And why not start to think about color grading actor faces when they receive their makeup . Further more you can start talking in same language with cosmetic specialists , dermatologists etc . <p> I did integrate above findings now into zoneSCOPE ( will inform when I publish new version ) . This seems to work out really useful to analyse scenes of films stored as pictures for example at http : **28;132;TOOLONG . Most important is then to look at the difference between calculated natural saturation and analysed actual saturation results . Whether you focus on shadows , mids or highlights of graded skins - difference stays in most cases constant . When you analyse scenes you observe comparable distances . Even different people with different skintones ( caucasian/african ) in one picture show similar distances . <p> That means in terms of color correction within a scene skin saturation control can be kept very precise . <p> Might be of interest to have a tool which extracts Saturation vs Luminance @ @ @ @ @ @ @ @ @ @ picture . <p> Here now the implementation into zoneSCOPE for natural skin . Saturation values are filtered and lay between 112- &lt; HUE &lt; 144- . The reference curve in Lum vs Sat graph is a fit derived from these colors ... A simple example for dark and bright skin colors ... <p> This is from Google Images " skin tone " as keyword . That should give a good measure for skin memory colors .... <p> To me , nothing can take the place of eyeballs ... and clients . I still say the scope just tells you when you 're about to get a speeding ticket . You still drive by looking out the window more than anything else . <p> Eyeball first for me . I find myself to choose a skintone that is 1/2 degree less than the one marked in the vectorscope , but is a preference . i will challenge that piece of paper asking " under which illumination/light condition " those hue are valid ? if you have a end of the day golden hour , the saturation , lightness and hue are @ @ @ @ @ @ @ @ @ @ the " perfect skin " hue or should i color coherently with the intended illumination in the set ? <p> Granted , I saw Superman ( the last one ) and having bluish skintone ( a horrible cast to my eye ) did not sell the image . Just because all the " blockbuster " MUST have yellow orange hilight and blue lowlight that does mean that the image you get is pleasing . If the client like it , then is ok ... <p> And , I 'm also curious to check the skintone preference based on where you are in the world : you will find a lot of variation in tint/hue and saturation/cast . <p> Follow your client desire , last movie I 'm working on , I got three great closeup of the main actress and i colored that with the DP , then we went through the movie and COHERENTLY with the set lighting we kept it consistent . Pleasing to see and right within itself . <p> Would be great to see something like this integrated into ScopeBox as well . There were rumours @ @ @ @ @ @ @ @ @ @ have n't seen or heard anything since . It 's certainly an interesting tool if for nothing else than MATCHING skin tones . <p> Quick run through of an analysis of 300 pictures from Humanae **35;162;TOOLONG are ordered first by Luminance and second by HUE.Skin Scope shows Saturation vs Luminance as well as HUE vs Luminance plots . Position of orange indicators are mean values of Saturation and HUE at the Luminance position of Ansel Adams Zones . The hight of orange indicators is the measure of frequency of skin color within its Ansel Adams Zone.Vectorscope changes to white indicators at the borderline settings of &gt; 20% saturation.Waveform Monitor changes either to blue indicators at borderline below 25% of luminance and red indicators above 75% . Screen display of Luma shows position where luminance occurs beyond the borderline . Screen display of Chroma shows orange indicators for skin tone , white indicators as oversaturated positions . Luma borders are neglected areas . <p> Although I would question where did the photos come from ? Who supervised the scans ? What format are the scans in ? How were the scans @ @ @ @ @ @ @ @ @ @ as a defacto standard . It 's like judging somebody 's work from YouTube . <p> Please do n't see above results as given for natural skin tones . I assume that published internet pictures are ( intentionally made as ) not good versions . For me they seem to be in average to much on the pink side . There should be more at HUE &gt; 128- ( Rec. 709 , YCbCr ) Shadows of some Pictures seem also not to be balanced well . 
@@44332550 @4332550/ <h> Anatomy of a DaVinci Resolve Composite <p> I recently pushed DaVinci Resolve further than I have yet . A project came in that needed a bit of compositing , and I tackled in right in DaVinci Resolve . And with a little help from Boris Continuum Complete , it all came together beautifully . <p> Thanks for sharing this with us Mike ! It was really good to observe you working ; it felt like sitting next to you in the suite . I did n't realise that Resolve was such a powerful compositing tool . I would usually insist on doing that kind of work in After Effects or Nuke . <p> Thanks very much for this . I always seem to forget about the alpha output . I recently removed a snowbank from a shot that was supposed to have been summertime and did similar kinds of things though not nearly as finessed . The only problem came when I changed my output from 4K DCI ( fit entire image : 17x9 ) to HD ( scale and crop to retain 17x9 ) The node sizing @ @ @ @ @ @ @ @ @ @ I 'm sure it was me not knowing the proper way to change output sizing . ANYWAY ....... So .... had there been movement on that shot would you have been able to track the baseplate and designate the fix layers as destinations for the track data ? And which PTZ sizing choice would you want to add it to ? Would it make a difference ? <p> The question is ... if the shot was hand held , is there a way to track those things in ? 43971 @qwx453971 <p> Yes , as Peijn pointed to , it is possible . And I actually had to use this technique in this composite . Even though the camera was on a tripod , it was n't locked off . So the shot seems like it 's pretty still , but there was just barely enough drift in it to make the jar of peanut butter and the table seem slightly floating . So I stabilized the backplate and set ' Strength ' to ' 0 ' . Then I copied that stabilization data to the jar and the table @ @ @ @ @ @ @ @ @ @ ' -100 ' . This worked really well to lock them both to the backplate . <p> This will still work pretty well with a bit more of a drift in the shot , even if it 's handheld . But keep in mind , with greater camera movement comes greater perspective change , so it becomes more difficult to place a still image in the shot . 
@@44332551 @4332551/ <p> Like Mamba , the software licensing is broken down by function : Base System , Color Tools , 3D Tools , DCP Export , etc . Getting a fully featured system capable of 4K finishing will put you into Flame Premium/ Baselight territory , but like everything at this level it depends on what you finally get and how much they like you <p> It 's interesting to note that I have n't seen an actual printed pricelist for Baselight or Pablo for years ( if ever ) ; same thing with the old Florida-based daVinci and Pandora . I got the impression you had to contact them verbally and request a specific price quote , and they 'd come up with something at that moment . I would expect as Jason says that this would be the same situation with SGO . <p> At least Mistika is off-shelf hardware , so the real-world prices would be predictable to some degree . <p> I think the Z820 used is actually 32 cores now , and storage is a real variable ( capacity and bandwidth ) . There are @ @ @ @ @ @ @ @ @ @ real-time resolution , tool-sets , hardware panels , etc ... <p> That does make having a ' standard price ' rather difficult . <p> Also , although most systems are provided ' turn-key ' if you have suitable hardware already available , and SGO verify its suitability , I know you can get software only . Obviously support is then limited to the software only , so most users I know go with the turn-key solution , as they get full guarantees of support , etc . <p> So , as Jason says , the actual final price is a variable - you really need to talk to SGO to define your needs and requirements . It also helps they are great individuals to talk to , and I 've always found them very helpful ! 
@@44332552 @4332552/ <h> different version of a dcp <p> I 'm currently making a DCP with 2 different versions of a doc . One english and one dutch version . My question is what is common practice here ; should I create 2 seperate DCP 's and put them on one disk . Or should I create one DCP that has 2 CPL 's ? The first option seems the easiest to me , but I ca n't find out if DCP disks can have multiple folders ( dcp 's ) on ' m or if everything needs to be on the root of the drive . <p> I 'm currently making a DCP with 2 different versions of a doc . One english and one dutch version . My question is what is common practice here ; should I create 2 seperate DCP 's and put them on one disk . Or should I create one DCP that has 2 CPL 's ? The first option seems the easiest to me , but I ca n't find out if DCP disks can have multiple folders ( dcp 's ) on @ @ @ @ @ @ @ @ @ @ root of the drive . 
@@44332553 @4332553/ <h> Future of Flame Premium in a small post-house <p> I founded 6 years ago a small post-house with Flame Premium as main tool for conforming and color-grading ( with Cinetal 42 " ) . I have a talented in-house colorist , very used to Lustre and Smoke for years . I bought a Smac station in order to cooperate with Premium two years ago . Our actual version is 2012 on both Mac or Linux . My 8600 HP is not able to run correctly 2013 nor 2014 and I 'm " forced " to this configuration actually . The investment for a brand new HP and Flame Update is 5 times the price of a complete Resolve station for exemple and it 's impossible for me at the time to spend this money on a very difficult market , and I 'm not sure it will be the case until 2 years . Considering Resolve rooms are growing everywhere around me with small prices , choices for the future are extremely hard . Talent and great skills are one thing , the possible market another . My @ @ @ @ @ @ @ @ @ @ it 's hard for my technician guy for example to miss all the new features and camera formats you can access with the latest versions of smoke or resolve . I do n't want to discuss here about Autodesk prices and policy , but simply ask for your opinion about the future of color grading tools . I have many commercials doubts about staying 2012 Luster for 2 years from now . Can we simply consider color is not a tool but a talent thing ? Is Resolve so bad compared to Lustre ? Many thanks for opinions . <p> Color is not a tool but a talent thing . No one who knows anything would specifically be asking for Lustre over Resolve unless they were very concerned with some workflow advantage and even there I think 2012 would be a hindrance . <p> I know this is unpleasant to hear , but gone are the days where having invested in the gear would be enough to secure a job . In color cost is n't much a differentiator of capability . I mean Resolve is FREE , how do @ @ @ @ @ @ @ @ @ @ , performance and capability , NOT access . <p> The one place in the market you can still sort of sell access is flame , and well see how long that lasts . I would try and talk to autodesk , explain your situation , wait until they 're coming to a close on their sales department and see if you can swing a deal . A z800 or z820 can be had quite reasonably , and if you can get the license reasonably priced you might be able to swing something . <p> That said even autodesks public statements have indicated that Lustre is at the end of it 's life , and being phased out for something new and more integrated . And performance wise even the most recent version of Lustre does n't really compare to what the other platforms are doing . Do you sell your services primarily as a color shop , or an fx shop ? <p> Thank you Juan for sharing your opinion . Color is talent , I 'm definitely ok with that and I was before posting anyway <p> I understood @ @ @ @ @ @ @ @ @ @ market that the big gears was a thing of the past , I 'm ok on that matter . Your statement does not sound unpleasant to my ears . And when a client call me for prices , I ca n't argue anymore about the big differences my tool offer ... <p> In the other hand , I want my talent to be happy with it everyday tool . I 'm not a color grader or a technician , but I perfectly know the great possibilities I could have on macpro with great graphics cards . I can size my station the way I want and I need , that 's not the case with Autodesk . It 's a pain for us to work with new formats , debayer , or even simply outpout a DPX sequence on a drive with our linux station ... So , you can tell me I already have my answer , but it 's a complex decision . <p> When you made such an investment , you simply have the strange feeling you go backward going Resolve . It 's much more a @ @ @ @ @ @ @ @ @ @ tools for 6 years ( me included ) <p> But having access to a larger and simplier world , with almost better performances on about every aspects of our work is definitely a good direction . <p> It 's difficult to discuss with Autodesk in France , we have no real choice about the machine we can order for example ... It 's the big 820 with K6000 or nothing ... <p> We do sell Fx , but we 're NukeX , not Flame . My Flame is sleeping all over the year , we only use some features time to time in Smoke advanced . We use the station for Lustre 90% of the time .... <p> Given you 're using the station mostly for Lustre , then there really is n't much point to putting more money into it , IMO . <p> Resolve still requires decent hardware in order to run well . In the US for a " commercial session " ready Resolve , you 're still looking at $20K+ on hardware , minimum . So there 's that . But do n't think of it @ @ @ @ @ @ @ @ @ @ an INSANE deal on a $600K DaVinci ! <p> Out of interest Lionel , how do you handle online / client sessions currently ? You mention you 're a NukeX shop primarily given you have invested in both a Smoke and Flame station . <p> We 're looking forward and amongst other things Smac has been discussed . It has however also been discussed it 's somewhat of an " lonely island " and that for what I think one key element of the station - conforming - can be done else where probably more efficiently and with more flexibility . But , it seems today Smac and Flame are the few " big iron " systems left . <p> If you have a node-locked license and you change a major component ( CPU , Graphics card , RAID controller , Video I/O ) of your configuration , you need a new license . Floating licenses do n't have that restriction , because they have to be able to move between machines with different hardware . <p> If you can move to a license server , you can source your @ @ @ @ @ @ @ @ @ @ to , say , that of a " huge " Resolve . <p> A question i have been wrestleing with as a DS centric workflow that has to change ... <p> Discreet , Quantel and SGO all offer deals for DS owners , it far , FAR less expensive to buy a DS licence and trade-in on a Smoke , Pablo or Mystika than to buy any of them outright , <p> The deals make Smoke by far the least expensive option , but also the most limited . <p> And Resolve with big panels , Pablo with Nano or Mystika with Tangent all in the same cluster as far as ROI if you already own a DS , but Resolve needs something infront of it for conform / paint / tweak <p> With 68% of the market gradeing on Resolve , a combined around 10% on Pablo / Mystika / DS , and 1% on Luster it seems the ship has already left the station and sailing into the sunset for Luster , so learning something else is not an option , it 's a reality ... just like @ @ @ @ @ @ @ @ @ @ had to when Nuke became the defacto standard <p> And add in the conform capabilities of Premier and Lightworks , and the paint / fix capabilities of Mamba it makes a harder to overlook this choice for a straight conform / tweak pipeline to color and return .. worth looking at i guess , so i will be ... costs next to nothing <p> Really it 's crazy not to have a licence of Resolve + small surface on hand , just to open doors to projects that have started in Resolve and want to finish with a real mon and scopes ( and maybe with a colorist who knows how to use them ) <p> So from here it seems that Premiere / Mamba / Resolve is the target to beat , and someday Resolve might gain a somewhat more useful conform / fix / tweak / paint tool set , i 'd have more hope for that than for AD to get it together and leave anything on the table that diffrentates it 's tool set enough from the above to make the ROI for the owner realstic @ @ @ @ @ @ @ @ @ @ do you handle online / client sessions currently ? You mention you 're a NukeX shop primarily given you have invested in both a Smoke and Flame station . <p> We 're looking forward and amongst other things Smac has been discussed . It has however also been discussed it 's somewhat of an " lonely island " and that for what I think one key element of the station - conforming - can be done else where probably more efficiently and with more flexibility . But , it seems today Smac and Flame are the few " big iron " systems left . 43971 @qwx453971 <p> Smac/Premium are our tools for conforming , grading and finishing . Every material coming from the VFX studio ( Nuke and others ) or the client is assembled on Smac . The bridge beetween Premium and Smac is quite good , I must admit , so no need to render for hours . The timelines could be exchanged , upragded and color corrected in Lustre pretty quicky and can be updated anytime if new material come from Smac . The workflow is solid @ @ @ @ @ @ @ @ @ @ , surely . When it comes to ProRes , R3D , F55 and so on , things are much more frustrating . As Premium and Smac are DPX workflows ( especially on linux ) , you have to debayer or convert everything to go Lustre . In our days , difficult to explain to some clients that everything has to be debayered or converted first . It 's time consuming and a huge constraint on many aspects . Not to mention the dark side of Linux admin in a small company . Anyway , I ca n't say Flame Premiim is a bad tool , far from that . It 's rock solid for 6 years and I was able to do 2K real time with many secondaries in 2008 , that 's was not the case on other systems . I definitely choosed a " Lonely Island " when I started the company and Smac was a great addition in the workflow 3 years ago , especially when Lustre was booked most of the time with clients . That 's the story ... <p> If you have a node-locked @ @ @ @ @ @ @ @ @ @ Graphics card , RAID controller , Video I/O ) of your configuration , you need a new license . Floating licenses do n't have that restriction , because they have to be able to move between machines with different hardware . <p> If you can move to a license server , you can source your own hardware , bringing the cost of Flame ownership down to , say , that of a " huge " Resolve . <p> A question i have been wrestleing with as a DS centric workflow that has to change ... <p> Discreet , Quantel and SGO all offer deals for DS owners , it far , FAR less expensive to buy a DS licence and trade-in on a Smoke , Pablo or Mystika than to buy any of them outright , <p> The deals make Smoke by far the least expensive option , but also the most limited . <p> And Resolve with big panels , Pablo with Nano or Mystika with Tangent all in the same cluster as far as ROI if you already own a DS , but Resolve needs something infront of @ @ @ @ @ @ @ @ @ @ of the market gradeing on Resolve , a combined around 10% on Pablo / Mystika / DS , and 1% on Luster it seems the ship has already left the station and sailing into the sunset for Luster , so learning something else is not an option , it 's a reality ... just like DS artists are dealing with now and like Flame artists had to when Nuke became the defacto standard <p> And add in the conform capabilities of Premier and Lightworks , and the paint / fix capabilities of Mamba it makes a harder to overlook this choice for a straight conform / tweak pipeline to color and return .. worth looking at i guess , so i will be ... costs next to nothing <p> Really it 's crazy not to have a licence of Resolve + small surface on hand , just to open doors to projects that have started in Resolve and want to finish with a real mon and scopes ( and maybe with a colorist who knows how to use them ) <p> So from here it seems that Premiere / Mamba @ @ @ @ @ @ @ @ @ @ Resolve might gain a somewhat more useful conform / fix / tweak / paint tool set , i 'd have more hope for that than for AD to get it together and leave anything on the table that diffrentates it 's tool set enough from the above to make the ROI for the owner realstic in 2016 ... 43971 @qwx453971 <p> I do have a Resolve on a small station for some stuffs . I do n't want to extend the thread too far , but actually , I ca n't run these Resolve on the same Mac as Smac , because 2012 Smoke ca n't run with a blackmagic card , only AJA . If I move to 2013 or 2014 where Davinci and Smoke can exist together , I loose Premium bridge <p> To go ahead , Smoke is a very great conforming tool on many aspects , and really solid when it comes to outpout to HDCAM or SR , which is still the way to go for broadcast in my country . It lakes some features for titling , but nothing we ca n't do in @ @ @ @ @ @ @ @ @ @ Smoke on those aspects and conforming ... <p> My only problem is Lustre actually , I definitely will keep Smoke ... <p> Moving to next generation Flame Premium is actually 100 K$ for the registered user I am ! ! It 's too expensive for my company and I miss some features when it comes to linux supported formats . Anyway , if I had the money , I will do it right away , because it works and I love the software , so my colorist If the job is done , no matter the software , my clients will be happy . As I stated in my early posts , it 's a matter of money and future of tools . <p> But I agree with you , many workflows are possible today , but Autodesk one is great and solid , but really expensive in nowdays market , especially for a small company . <p> I have a client in Prague . It 's a VFX company using Flame Premium as well . They were in exactly the same boat as you are . They were torn @ @ @ @ @ @ @ @ @ @ usual with Flame Premium does n't cut it anymore . So , after a long conversation and an extensive Resolve demo , they made a plunge and installed Windows Resolve station in their office in Prague . So now all their grading is done remotely using Resolve . Frankly , there are many things in Resolve , that still needs to be addressed , but in today 's post market ignoring Resolve is a loosing proposition ... <p> My colorist is not convinced at all by Resolve , but it 's hard for my technician guy for example to miss all the new features and camera formats you can access with the latest versions of smoke or resolve . 43971 @qwx453971 <p> I think there 's often a fear of the unknown when an editor , a VFX artist , a colorist , or a mixer has to throw away much of what they know and adapt to new software . A lot of it works out to translating what you used to know into new menus , new widgets , new features , and even compensate for the lack @ @ @ @ @ @ @ @ @ @ of getting into Resolve , I think it 'd be a minimal expense to get a fully-featured system up and running provided you do n't need the full panels and do n't need 4K delivery . <p> Can we simply consider color is not a tool but a talent thing ? Is Resolve so bad compared to Lustre ? 43971 @qwx453971 <p> Lustre is very VFX-oriented , coming from the whole Flame/Inferno/Smoke legacy , so there 's kind of a " compositing philosophy " based around how it works . My quarrel with it as a colorist is that it ignores the long history of dozens of color-correction devices that came before it . I do n't dispute that it 's very powerful , but the market realities today are that we need to work quickly and efficiently and get the job done in a reasonable amount of time . To me , Resolve is going to be a lot faster , at the expense of compromises in certain features , while giving you some additional features Lustre does n't have . Color is absolutely talent based , and I @ @ @ @ @ @ @ @ @ @ their work at Company 3 or Encore care that the colorists are using Resolve or anything else . " daVinci " is just a buzzword , like " Avid " or " Pro Tools , " one that 's reliable , dependable , and will get the job done . <p> If you 're about to make a decision , my advice would be to wait six weeks until NAB ( April 5th ) and see what happens there . Most likely , everybody will offer new products , new deals , and everything will be shaken up yet again . <p> Lustre is very VFX-oriented , coming from the whole Flame/Inferno/Smoke legacy , so there 's kind of a " compositing philosophy " based around how it works . My quarrel with it as a colorist is that it ignores the long history of dozens of color-correction devices that came before it . I do n't dispute that it 's very powerful , but the market realities today are that we need to work quickly and efficiently and get the job done in a reasonable amount of time . @ @ @ @ @ @ @ @ @ @ , and developed by colorfront . It was never designed around the " compositing philosophy " as it was n't a compositing application . It 's hard to say that it ignores the long history of color-correction when it 's roots are such a MASSIVE part of that history . Among the first DIs EVER done many were done on Colossus . <p> When autodesk ( at the time discreet ) purchased it , they went about integrating it into their pipeline , but it was and is still very much a color , not compositing application . Today it 's behavior is more like that of a plugin than a DI application like baselight or even resolve . <p> I 'd say the general slowness and inefficiency of Lustre is not due to it 's pedigree or design , so much as it 's neglect . It 's clearly the ugly stepchild over at AD , and has n't really seen any changes since 2012 , which itself hardly saw any changes within Lustre . So today as the OP noted , it 's native support , features and @ @ @ @ @ @ @ @ @ @ think there 's often a fear of the unknown when an editor , a VFX artist , a colorist , or a mixer has to throw away much of what they know and adapt to new software . A lot of it works out to translating what you used to know into new menus , new widgets , new features , and even compensate for the lack of features in the new tool . Given the cost of getting into Resolve , I think it 'd be a minimal expense to get a fully-featured system up and running provided you do n't need the full panels and do n't need 4K delivery . 43971 @qwx453971 <p> Yeah , that 's the point in the decision . From my colorist test , Resolve miss importants features he uses a lot in Lustre . I 'm sure there 's other way to do or compensate , but the learning curves are usually long and we need time for that , as always . He also has to be confident with the client so it 's always a pain to do such jump @ @ @ @ @ @ @ @ @ @ " to , it will be made . <p> Lustre is very VFX-oriented , coming from the whole Flame/Inferno/Smoke legacy , so there 's kind of a " compositing philosophy " based around how it works . My quarrel with it as a colorist is that it ignores the long history of dozens of color-correction devices that came before it . I do n't dispute that it 's very powerful , but the market realities today are that we need to work quickly and efficiently and get the job done in a reasonable amount of time . To me , Resolve is going to be a lot faster , at the expense of compromises in certain features , while giving you some additional features Lustre does n't have . Color is absolutely talent based , and I do n't think any of the major studios who do their work at Company 3 or Encore care that the colorists are using Resolve or anything else . " daVinci " is just a buzzword , like " Avid " or " Pro Tools , " one that 's reliable , dependable , @ @ @ @ @ @ @ @ @ @ Lustre is not so VFX oriented in my opinion , it a very well made tool for color . You 're not lost by multiple window or menus which is nice at the end . The smoke/flame side is clearly made for that , in addition to Luster where you ca n't do a single thing for titles for example . <p> If you 're about to make a decision , my advice would be to wait six weeks until NAB ( April 5th ) and see what happens there . Most likely , everybody will offer new products , new deals , and everything will be shaken up yet again . 43971 @qwx453971 <p> Yep , I was waiting for that . But I do n't think AD will announce a different approach concerning the license and price side.I hope I 'm wrong 
@@44332554 @4332554/ <h> The Importance of Colour Grading <p> I 've been asked to do a talk on the importance of Colour Grading in the modern post production environment . Obviously as Colourists we all believe it to be extremely important ( not least because we would be out of a job if it was n't ) , but I wanted to get the opinions of some of the more experienced folks who have been working in post for a long time and have seen the evolution from telecine to DI grading and how they feel the process has shifted to place greater importance on ; shooting for post , colour enhancement rather than correction , creativity , greater degree of flexibility in post . <p> My plan was to simply talk about my experience I entered the industry in 2009 just as RED was really taking off so I 've seen the decline of analog based workflows and the greater degree of responsibility placed on the colourist to create the look/tone in post . I have an understanding of how things were done back in the day but it would @ @ @ @ @ @ @ @ @ @ actually working back then . <p> That would be a 900-page book . We dumped analog workflows in 1994 , if memory serves , going completely to component digital standard-def . The real revolution was going HD in 1999-2001 , then moving away from film circa 2009 . Every step brought new problems and new benefits , but the biggest problem I see is managing expectations from inexperienced clients . That has not changed from the 1980s . <p> Also , one fundamental thing has not changed , even since the invention of photochemical color timing . If you put aside the incredible creative potential that modern digital processes have , you 're still down to a basic premise . <p> Shot-to-shot , scene-to-scene , color must be balanced to provide a sense of time and space continuity . It serves the story and maintains suspension of disbelief . <p> I think just one of the things that has changed is the perception that colour grading is/was limited to productions with bigger budgets , which were usually shot on film . Grading is no longer a service offered only by @ @ @ @ @ @ @ @ @ @ it viable for independent colourists to ply their trade , and I think there is a much broader market for it . Colour grading is no longer thought of as " nice to have if there is money for it " , but rather a necessary part in any post production chain ( even if done by an editor ) - this is especially true as more cameras use log-style shooting methods . If it ai n't graded , it ai n't finished ! 
@@44332555 @4332555/ <h> Confused on usage of Linearize/Linearization ? <p> You 're confused because the word " linearize " is second only to the word " raw " as the most mis-used term in post-production right now . To linearize means to give your footage a Gamma of 1.0 , or to make the Gamma curve flat . An image with a Gamma of 1.0 is n't really useful from a aesthetic standpoint , but is mathematically a great place to work from especially in certain situations like compositing/ VFX . <p> By performing operations in a linear color space , you can prevent certain edge and halo artifacts , such as the fringing that appears when high-contrast , saturated colors are blended together . Many color operations benefit from working in a linear color space , including those operations involved in image resampling , blending between layers with blending modes , motion blur , and anti-aliasing . <p> --- <p> The correct term for " making log footage viewable " by applying a Gamma curve to it is actually " de-log " . One method to de-log footage is to @ @ @ @ @ @ @ @ @ @ the how the original source ( digital camera , film scanner ) created it . This is probably considered the " most correct " way to de-log footage , but people will often use generic Rec709 , P3 , Film Emulation , or other types of LUTs especially if the resulting effect looks pleasing or adds character . <p> You can also de-log footage by simply stretching the contrast out and manually setting a black point , white point , and possibly adjusting the Gamma . A lot of people do this since those adjustments will usually be needed anyway , but doing that can lead to a loss of detail and information that the original LUT would have revealed . <p> The misconception here is because there are 2 " linear " things and they are hit or miss in our industry : Video is perceptually linear , which means to have something that looks on the TV like it was shot by the camera there has to be some processing done to the picture . With a video signal , there 's for various historical and technical reasons @ @ @ @ @ @ @ @ @ @ so the signal is not linear . In CG you generally work in linear scene referred , which means there is no gamma applied to the signal , it 's actually linear , this is how you work in Nuke for example . BUT , since your display expects to have a signal with a gamma , you need to add one just for the display . As you can guess , those concepts are way too complicated for most people to tell them apart , but at the end of the day that brings me business and people come to my color science seminars , so I wo n't complain <p> I 've always wondered what 's the difference between Rec. 709 and video ? DaVinci comes with preinstalled LUTs called Log to Rec. 709 or Log to Video . What 's the difference ? 43971 @qwx453971 <p> I 'm not sure specifically what the " video " LUT in resolve is , but it 's possible that it 's for Rec. 601 ( SD ) . They could be using legacy " video " nomenclature because before Rec. @ @ @ @ @ @ @ @ @ @ speak . <p> Luma coefficients for Rec. 709 and Rec. 601 are different . When converting from R'G'B ' to Y'CbCr you get different values depending on 601/709 . The difference is context dependent but visually small in most cases . In theory one should apply a correct matrix when downconverting tape from HD to SD , but I doubt many facilities do this at all . 
@@44332557 @4332557/ <h> Using " directors cuts " of work on public demoreels/galleries ? <p> What 's the general consensus on a colorist/post facility using a " rejected " version of a graded spot when showing their work off publicly ? <p> I recently graded a TVC where the director and DP ( both highly seasoned professionals with fairly impeccable taste , as far as color goes ) pushed the grade of the spot to a very bold look that we all agreed was great ( well , it was at least " bold " for our rather creatively-conservative local market ) . <p> The look was not pushed just for sake of looking " cool " either . The performance , set design and tone of the script all lended themselves to the kind of adventurous grade that we applied to the spot . <p> Agency and client came in for a screening , everyone liked it and approved it ( although we had to defend our aesthetic decisions at first , we did n't really have any kind of a fight with agency about it ) . The spot @ @ @ @ @ @ @ @ @ @ of the agency partners sees it on a STANDARD DEFINITION TV , and comments that she thought it looked " bad " . <p> So because someone who has no real valid insights into the color other than " i do n't  like it " mentions her opinion to one of the agency partners who originally approved the finished spot , I now find myself having to undo everything we did , and regrade the spot using a boring , neutral , straight out of the camera look . <p> The actual client was fine with the look . It 's light years better than anything else that they 've done for their TV campaigns in the past . The DP--an internationally respected DP whose talents are often personally requested by directors like Terence Malick--thought it looked great . Same goes for the director . <p> So now I 'm wondering , politically speaking , how I should handle the way I showcase this spot on our website and demoreels . Because , color aside , it 's a really good spot that deserves to be highlighted . But neither @ @ @ @ @ @ @ @ @ @ the " dumbed down " color grade on our respective websites . <p> How do you guys handle " creative differences " like this ? I 'm worried that by posting the original grade on our site with a disclaimer caption that this is the " directors version " may send the message to the agency that we think they are absolutely wrong about their decision to undo the grade . And in this day of ubiquitous video embedding/linking via social networks , I 'm also worried that having both versions floating around the web may be confusing and disconcerting for both the client , as well as the general public . <p> Am I overthinking this ? Our market is extremely small ... to the point where if I fart in the middle of a grading session at 10am , every single person from every ad agency and production company in town knows about it by 10:15am . <p> Just curious to know how some of you handle these kinds of " creative differences " ? <p> If its just for your reel and to show off what you @ @ @ @ @ @ @ @ @ @ and most of what is on my reel is a tweak or modification compared to the final airing piece . <p> The only thing that might be a issue is if the piece is airing a lot or happens to be very popular as folks tend to get used to seeing something a certain way and it then becomes blatant that your version is n't the " official " one , but such things are usually not an issue . <p> Just had the same thing happen , which ended in a really watered down grade . The DP is a friend of mine and had brought me the job . So i did a separate session with him to give him something good for his ( and my ) reel . <p> Hey Mel , I work in a very small South Pacific market place also and I feel your pain , <p> But there is some saving grace , and you can turn this to your advantage . This is what I would do , I would take out the agency guy who wanted the change for a @ @ @ @ @ @ @ @ @ @ proud to be apart of the team , etc Then ask if he has any objection to you using the original piece just for your website/reel as it is a better demonstration of pushing a look . Once he 's agreed you have covered your ass , and ca n't ruin the relationship , you might even build a better one with the lunch . 
@@44332558 @4332558/ <h> Kong : Skull island <p> Man I just love what they 've done with this ! Really bold image and I think it works really well . Kinda reminds a bit of Mad Max ... Also the CGI looks nice not overly fake . This project looks like a colorists wet dream <p> The early reviews say this is one of the rare films that bucks the trend of making the audience wait an hour or more to see the creature . You get Kong in your face pretty early on this one . ( And god knows how that will smell . ) <p> Larry Fong guys , he 's a king . He 's used to shooting on film and I think this is his first movie shot digitally . Alexa , anamorphic ( Panavision lenses I 'd say ) . I 've never seen the Alexa look this good ever before , great color timing too . The texture is very pleasant , unilke any other Alexa shot film . <p> Larry Fong guys , he 's a king . He 's used to shooting @ @ @ @ @ @ @ @ @ @ shot digitally . Alexa , anamorphic ( Panavision lenses I 'd say ) . I 've never seen the Alexa look this good ever before , great color timing too . The texture is very pleasant , unlilke any other Alexa shot film . 43971 @qwx453971 <p> Gee , I see an awful lot of nice-looking Alexa projects . In fact , I saw an Alexa horror project the other day where they had crushed the hell out of it and added quite a bit of film grain -- I thought it was 16mm ! <p> Because I usually do n't like digital , I 'm a film guy . There 's really very few movies shot digitally that I think look good . Too often , I find it to be just too clean , sterile and flat . Nothing beats film ( and has the charm &amp; texture of film ) imo . <p> Does anyone know who colored this ? IMDb does n't have that info , but implies it was done at Fotokem . Love me some Larry Fong cinematography--he does amazing work . Kong is @ @ @ @ @ @ @ @ @ @ , too . <p> Larry Fong guys , he 's a king . He 's used to shooting on film and I think this is his first movie shot digitally . Alexa , anamorphic ( Panavision lenses I 'd say ) . I 've never seen the Alexa look this good ever before , great color timing too . The texture is very pleasant , unilke any other Alexa shot film . 
@@44332559 @4332559/ <h> Which uniformity of chromaticity at your grading monitor do you urge ? <p> in an ideal world we would grade on a perfect monitor but in the real world the monitor may falls short to some demands . Uniformity of chromaticity at the screen is one important thing . Which deviations from the center are acceptable ? For instance a deviation of 200 K in CCT from the center to a corner ( I noticed that on a pvm-2451 ) ? <p> EBU tech 3320 exacts : ' The uniformity of chromaticity should be within 2.6 + " u*v* of the measured white at the centre of the screen , for Grade 1 monitors ( which corresponds to the tolerance of -0.002 + " u ' , + " v ' ) ' How much is a difference of 100K in this term ? 
@@44332560 @4332560/ <p> One of my biggest pleasures is to help a filmmaker realize his or her dream . I am grading a documentary and the director had a specific vision that did n't come through in the photography . After talking about the mood that she wanted , I came up with a look that was exactly what she had in mind when she first thought about making the movie . She was so happy to see that she could finally turn her film into what she had originally envisioned that she could n't wipe her big smile off her face . <p> I am working on a different project , a feature film , in a way that is very exciting . The project started with my feedback as a colorist from the pre-production stage , and the director will base the film 's look , including the whole art direction , on the color palette that I came up with after we shot a photography test . So , the director , the DP , and I worked on this look from the start and now the wardrobe @ @ @ @ @ @ @ @ @ @ . It is very rewarding , as a colorist , to have an input from the start , not just at the end . I hope this practice becomes more commonplace . 
@@44332561 @4332561/ <h> Issue with mattes out of Sync <p> Hey guys , maybe someone can she 'd some light on why I might be having this issue . ( DaVinci 11 ) <p> I have some sky replacement shots a client gave me with the black and white luma mattes . I 've connected the mattes into a node and connected the node to an alpha output . All is well . <p> Then , of the 20 shots , I have one shot that seems to be puzzling me . I 'll apply the matte but for some reason the matte wo n't begin at the starting point of the matte , it seems to start in the middle . So I adjust the offset of the matte to get it to match perfectly but then when I move another frame into the shot the matte suddenly jumps back to the middle of the matte clip . <p> I 'm very puzzled/frustrated as to why this is . Anyone have any ideas ? <p> What I have tried : <p> -The matte locked and unlocked . -Adjusting the offset to @ @ @ @ @ @ @ @ @ @ of sync on 2nd frame ) . -Making sure the matte is exactly the same length as the clip . -Ensuring the matte and clips are the same frame rate . -I 've added the matte as a separate matte and as a matte connected to the specific clip . -I 've converted the matte from a jpeg sequence into other formats such as a DPX . <p> 1 import the matte into the media clip with the master clip selected , to lock them together . if this has been done then try the opposite just add as matte. 2 process the matte clip through resolve to ie transcode it to a . MOV or DPX and try again . <p> Also worth double checking FPS and frame counts this would have been my first guess . <p> 1 import the matte into the media clip with the master clip selected , to lock them together . if this has been done then try the opposite just add as matte. 2 process the matte clip through resolve to ie transcode it to a . MOV or DPX and try @ @ @ @ @ @ @ @ @ @ counts this would have been my first guess . <p> Check in 12 . That 's an odd one . Maybe a key frame is resetting the offset ? Does it still happen in a new grade ? On the same media in a new project ? 43971 @qwx453971 <p> Was n't a keyframe . I would sync the shot at any point in the clip with the offset and then the next frame it would always begin over . Will try it in a new project and such . Have n't tried it in 12 but do n't really feel like installing until it 's officially released . <p> I had to kick it out without the matte unfortunately due to needing a turnaround by next morning . I believe they just had their vfx guy re-apply . <p> I have tried both . I 'll be trying it in a new project tomorrow and will delete and re-import the mattes again on the current project . I 'd really like to get to the root of the problem so we will see how it goes ! <p> Still @ @ @ @ @ @ @ @ @ @ the " Loop " option the matte just turns black . <p> Also attempted it in a new project with just this clip with this matte . Same issue . This time is lasted for 12 frames before it randomly jumped to a middle point of the matte . In this project everything was fresh and the only thing applied in the clip was the matte ( no grades or anything ) . <p> When I scrub through the matte in the media tab is does n't have a random jump , it plays back like the clip should . 
@@44332563 @4332563/ <h> How to get MXF out of Resolve into Avid <p> When I was using v11 of Resolve , I used to export my graded timeline as MXF ( single clip ) and then place that file in the famous AvidMediafiles folder , boot the Media Composer , let it scan , open the media tool etc etc . You know the drill . <p> But yesterday was different . I was working on V12 for the first time and noticed a change in the naming . What used to be MXF is now MXF OP ATOM . Fine , I exported MXF OP ATOM , gave the file to the editor and told him to place it in the AvidMediafiles folder , but Avid started giving a lot of errors and it did n't work . It even corrupted other databases . Now the only thing I could think of that was different from what I do on my own setup is that they were using Avid Unity . So , I 'm I correct in understanding that the MXF OP ATOM files can not be placed in @ @ @ @ @ @ @ @ @ @ Next thing I tried was to export an MXF Op1A from Resolve ( DNxHD 1080i 120 ) and I tried to import that in the MC but Avid could n't read the file . Why ? I know Avid can output MXF Op1a files that Resolve can read with out any problems . <p> We ended up using the MXF OP ATOM file , but instead of dumping it on the Unity we placed it on a local drive 's media folder and the MC scanned it/read it correctly . We then consolidated that file from the local storage to the Unity within MC . <p> Op atom is the self contained with audio a la QuickTime right ? Is n't it what cameras that record to dnxhd make ? I think you would treat it like ama or direct link . But the option to make the regular Mxf is still in resolve right ? Not in front of it . <p> Op atom is the self contained with audio a la QuickTime right ? Is n't it what cameras that record to dnxhd make ? I think you @ @ @ @ @ @ @ @ @ @ the option to make the regular Mxf is still in resolve right ? Not in front of it . 43971 @qwx453971 <p> Yes , the original mxf option is there but what got me what that it no longer is used by the AAF preset <p> Op atom is the self contained with audio a la QuickTime right ? Is n't it what cameras that record to dnxhd make ? I think you would treat it like ama or direct link . But the option to make the regular Mxf is still in resolve right ? Not in front of it . 43971 @qwx453971 <p> Op ATOM is for separate video and audio files and it 's what Avid uses in its mediafiles folders . MXF Op1A is the self contained one with audio included a la QT . <p> Generate the OP-Atom files . It works for me , although it had a hiccup on Resolve 12 a couple of times . It could have been an operator error though . <p> When you place these in Avid MediaFiles path and switch focus to Media Composer , it will @ @ @ @ @ @ @ @ @ @ directory . Absence of such files indicates that Media Composer ca n't see or does n't like the files . <p> On a standalone Media Composer MXF files must be in a numerically named directory . You can not use letters in a directory name until after the media has been indexed . In a shared storage environment , the media directories will have names like WORKSTATIONNAME.1 , WORKSTATIONNAME.2 , etc . You should be able to drop the Resolve generated files in a directory like that , delete the database files and have Media Composer index the contents again . <p> In a shared storage environment , the media directories will have names like WORKSTATIONNAME.1 , WORKSTATIONNAME.2 , etc . You should be able to drop the Resolve generated files in a directory like that , delete the database files and have Media Composer index the contents again . 43971 @qwx453971 <p> So that 's exactly the part that did n't work for us . We were able to do it on the ' D : ' drive ( local ) but not the shared storage . 
@@44332564 @4332564/ <h> Should I be working with data levels instead of video levels ? <p> I was looking in the Resolve settings and I saw that I could select data levels which would let me go from 0-1023 on the scopes , allowing me to get darker shadows . Is it worth grading with data levels then using a LUT at the end to get the levels ' legal ' ? <p> I 'm basically trying to get " Extended Range Video 0-109IRE CV64 to CV1023 " as mentioned in this article , but is it worth it ? <p> That is more than likely the case ! I 'll read this now . I 'm not worried about all the work I 've been doing its just me having a fear of missing out if it 's what all the pros are doing edit : <p> What is important to understand up-front is that all displays show black as ' black ' and white as ' white . This means that regardless of the input signal , if it is correctly matched to the display 's expected input , @ @ @ @ @ @ @ @ @ @ if a display has the option of setting the expected input to ' data ' levels , or ' TV legal ' levels , when correctly match to the input signal , black and white will look identical . This is hard to get your head around , but understanding this is critical ! 43971 @qwx453971 <p> Had to read that a few times but I get it now I think . I thought working with video levels would forcefully clip footage that had been shot in the ' extended range ' ( 64-1023 ) and leave me with more clipped highlights . So black ( 0 ) on a monitor with data levels will look the same as black ( 64 ) on a video levels monitor , and same goes for white ? <p> It takes a while to wrap your head around it . Think of it this way : every link in the chain expects a certain thing , and then outputs a certain thing . <p> Some cameras can record either Data or Video levels . On set , the camera , independent of what @ @ @ @ @ @ @ @ @ @ is either Data or Video levels . This can go to a LUT box that is expecting either Data or Video levels . The LUT box can then output either Data or Video levels . To a Monitor that is either expecting Data or Video levels . The Monitor converts the signal to individual pixels that then your eyeballs take and interpret and , THANKFULLY , do n't expect either Data or Video levels , just photons . <p> If you feed Data levels to a Monitor that is expecting Video levels , when the Monitor does it 's whole maths thing , it 'll extend the number range out , and your highlights will be above 100 IRE , and your lows will be below 0 IRE . <p> Thing is , neither Data or Video is " wrong " . But every link in the chain has to be consistent . <p> Yeah I see now . So there 's no point in grading with data levels *if* it 's going to be viewed on devices with legal " video levels " and I 'd end up with @ @ @ @ @ @ @ @ @ @ necessarily . If you 're exporting from Da Vinci , for example , your Timeline can be set up Video , but in the Delivery tab , you have the option of exporting in Data , Video , or " Auto " . If you graded in Video and then export in Data , DaVinci will automatically perform the correct transform for you , so that the Data output file will look the same in a Data viewing chain as the Video timeline looked in your Video viewing chain . <p> edit : no I 'm confused again actually . I remember when I calibrated my TV I was able to see up to 255 on a grey scale and not 235 which is the TV level . Does n't that mean it exceeds the video levels ? 43971 @qwx453971 <p> Your TV may only take in Video levels , but it transforms them to the full range of black-white ( 0-255 ) . <p> Remember , there is no " wrong " choice between Data and Video . So long as whichever you choose , your viewing chain is @ @ @ @ @ @ @ @ @ @ 
@@44332565 @4332565/ <p> My daughter was watching a current music video . It looked ok . Then she started watching another music video . I though to myself " wow this one looks awesome . " Then I realized that unlike the first one it was 4:3 and based on that it must have been over twelve years old . But the color on it was saturated , contrasty , hue-accurate and nothing like the faded dye Instagram look . <p> But what puzzles me the most about the ' Instagram " look is that Kodak and Fuji spent millions trying to avoid this kind of look . It 's what happens when you shoot out-of-date color neg and let poorly processed prints sit in an unsuitable environment for years . <p> So could n't help myself here ; there 's actually some interesting things going on in this look that make it pretty interesting in my book . Using Adobe Color you can quite easily break it down into the dominant hues of the color scheme ( with emphasis on the " dominant " hues ; yes there 's more @ @ @ @ @ @ @ @ @ @ contrast , but we 're just looking at the hues here ) . Here 's what it looks like as a five color scheme : <p> Look at the waveform and notice the " blue/orange " and the " magenta/green " opposites going on here . Instagram or not , there 's some great complimentary tones going on in this look . Take out the magenta in her dress or the blue in his jeans and the picture is n't so interesting chromatically . Also look at how the greens go from a " bluer/cyan " green in the dark areas to a warmer " oranger/green " in the brighter greens . That 's a trait you see in quite a bit of feature film grading nowadays ( and something I will admit to doing all the time myself ... ) . There 's more going on here than meets the eye . I like it ( and am i no way affiliated with this at all ) . <p> So in answer to the original question , how would I describe this color scheme ? No idea . If @ @ @ @ @ @ @ @ @ @ something vague like " Vintage Dark Pastel " or " Orangy Instagram " ... make up your own name , but I think the important thing is to be able to communicate what the dominant tones are , and even more importantly understand WHY it works ( or does n't for that matter ) . <p> My daughter was watching a current music video . It looked ok . Then she started watching another music video . I though to myself " wow this one looks awesome . " Then I realized that unlike the first one it was 4:3 and based on that it must have been over twelve years old . But the color on it was saturated , contrasty , hue-accurate and nothing like the faded dye Instagram look . <p> But what puzzles me the most about the ' Instagram " look is that Kodak and Fuji spent millions trying to avoid this kind of look . It 's what happens when you shoot out-of-date color neg and let poorly processed prints sit in an unsuitable environment for years . 43971 @qwx453971 <p> RE : Igor @ @ @ @ @ @ @ @ @ @ either . I came into grading through stills photography having always enjoyed editing my pictures more than I did taking them ; priding myself on the way my photos looked even if they were n't composed all that well etc . <p> Anyway so it kills me when someone 'll ask for a picture I 've taken of them , that I 've put a lot of time into editing/cropping carefully in Lightroom , only to see it on Instagram or Facebook cropped/filtered to within an inch of its life with the caption " thanks for the great photo Sam ! " because it 's not the picture I took anymore . <p> The worst offenders are the in-built iPhone filters . Some of the Instagram filters are n't too horrid but I think VSCOcam is a much better tool for recreating the film look , I 've actually had some fun recreating some of the VSCO Lightroom presets as a lot of the functions in Lightroom have corresponding tools within Resolve . <p> Kudos for the analysis Chris but I still think the yellow 's too heavy handed ; @ @ @ @ @ @ @ @ @ @ about to come bounding down . Just feels muddy overall . 43971 @qwx453971 <p> To each their own of course : ) We definitely get paid to have our opinions . I just wanted to point out that it did n't look entirely haphazard , there was actually some " interesting " things happening color-wise ( or at least interesting to me ! ) . <p> Look at the waveform and notice the " blue/orange " and the " magenta/green " opposites going on here . Instagram or not , there 's some great complimentary tones going on in this look . 43971 @qwx453971 <p> The only problem is that it looks like crap . I think there 's a way to make something striking and interesting-looking , but avoid the crap-ness . Sometimes , " different " is just different , not better or good . <p> ha . well feel free to tell me how you really feel gentlemen . : ) all good . <p> but in the spirit of trying to stay un-jaded and sticking to ones metaphoric " guns " so to speak , I @ @ @ @ @ @ @ @ @ @ at all ) , and I really would say there 's more going on here than a single filter or a curve manipulation . However , I could be wrong , and if this was someone 's magic bullet colorista filter , then they got lucky , if not kudos for going beyond just a " clean look " and trying something more ( hopefully with the DP 's blessing of course ) . I have clients that have been around and working FAR longer than I , that ask me all the time to " break " images in interesting ways ... this is interesting , to me ( for the reasons I outlined above ) . but c'est la vie , I 'll sail on that boat alone apparently : ) <p> Absolutely , but I think the problem is that in this case the only thing hyper-real about the image is the look . If their surroundings or outfits or anything else were a bit more unconventional then it might work better , but it 's just a normal street with normal people with a very abnormal @ @ @ @ @ @ @ @ @ @ . <p> I 'm with Chris and Ian on this . I dig it . One thing I am trying to explore right now is how to create a cool look without immediately relying on contrast and saturation . I love punchy , contrasty looks , but it 's pretty limiting creatively if that 's all you ever do . <p> It 's not really my cup of tea but I agree with Chris and Ian , it 's different - like it or not . I think it 's easy to discount a look , style , but instead why not take a moment to figure it out . At the very least if you do n't like it figure out why you do n't and how you would do it differently or not . 
@@44332566 @4332566/ <p> Watch it be 100MB per frame . 1 second of 5K is already 100MB ( at 3:1 ) ; if it 's 8K and a double-sized frame ... it 'll be huge . I think the Red owners grossly underestimate how difficult , expensive , and impractical the post workflow will be for VistaVision 8K . What was Alexa 65mm ? 2TB an hour ? Come on . <p> BTW , I 'd like to retract the 8k statement . Actually , no one knows Weapon 's resolution at this point . 43971 @qwx453971 <p> Yep , no official statement . But they said no sensor upgrade . Means it has to be a bigger Dragon sensor so more sensor size =&gt; more pixels . So about 8K is the assumed resolution due to the size difference between Red Dragon 6K and Vista Vision . <p> Interesting ... many lenses already start to vignette on 6k , some even on 5k . Wonder how they 'll resolve this for the size and shape difference going to vistavision ? At least the Alexa 65 has the former 765 system @ @ @ @ @ @ @ @ @ @ lenses already start to vignette on 6k , some even on 5k . Wonder how they 'll resolve this for the size and shape difference going to vistavision ? At least the Alexa 65 has the former 765 system lenses to use right away . 43971 @qwx453971 <p> Coverage of the lens has nothing to do with resolution . It is strictly the area size dependent . For that matter any lenses , that work with Canon 5D , which has the same FF sensor size , will work on Weapon just fine . I believe , Alexa 65 uses slightly updated and rehoused Hasselblad lenses . <p> Coverage of the lens has nothing to do with resolution . It is strictly the area size dependent . For that matter any lenses , that work with Canon 5D , which has the same FF sensor size , will work on Weapon just fine . I believe , Alexa 65 uses slightly updated and rehoused Hasselblad lenses . 43971 @qwx453971 <p> Yes , resolution is ultimately an independent factor , except in the example chart above in this thread , where @ @ @ @ @ @ @ @ @ @ most cinema lenses are/were made to focus on the academy 35mm size , this is where the vignetting comes in , as the chart itself let 's you see how much area beyond the 35mm aperture these RED frame sizes extend . <p> And yes , since vista-vision is essentially the same size as 35mm still film aperture , it should n't be a big deal to use FF 35mm lenses on the W. <p> at the risk of going insane grading this stuff every day , can i just ask , " please , no more k 's ... " just redo your color science , rework your compression , and get rid of the noise on the images you already make ( ie dragon , epic ) . enough with the k 's ... killing me <p> So i 've seen Dragon material that looked amazing and a lot that looks bad and noisy . But sharpness wise i would be hard to tell tell the difference between 4k Dragon and Alexa HD . Is this just a new way for Red not to have its pants pulled down @ @ @ @ @ @ @ @ @ @ oh shit 4k red is soft as shit at native resolution . <p> at the risk of going insane grading this stuff every day , can i just ask , " please , no more k 's ... " just redo your color science , rework your compression , and get rid of the noise on the images you already make ( ie dragon , epic ) . enough with the k 's ... killing me 
@@44332567 @4332567/ <h> Workflow &amp; Color of " Ender 's Game " <p> Michael Cioni of Light Iron Digital just posted a fascinating 19-minute video about the dailies , the on-set color , the editing workflow , data path , color science , and VFX work on the current film Ender 's Game . Quite an interesting presentation , particularly when you take into consideration they were dealing with 75TB of material for a single project . <p> This is a great in-depth look on the workflow for a big budget studio film . I 'm curious why do they have to convert the P3 color space to XYZ space when at the end the projector is projecting at P3 color space ? <p> This is a great in-depth look on the workflow for a big budget studio film . I 'm curious why do they have to convert the P3 color space to XYZ space when at the end the projector is projecting at P3 color space ? 43971 @qwx453971 <p> The colour space is XYZ , P3 is the gamut within that space . I have n't watched the @ @ @ @ @ @ @ @ @ @ , but you 'd usually work in RGB colour space ( regardless of your gamut , which can be P3 , REC709 , etc ... ) and convert it to XYZ for the DCP . <p> I guess I 'm just shocked that films in this day and age are n't finishing in 4K to future-proof. 43971 @qwx453971 <p> It 's not yet possible with VFX-heavy films to do an all-4K pipeline because of speed . If they had tried to do all 900 VFX shots in Ender 's Game in 4K , the movie would come out in January of 2014 ( if they were lucky ) . It 's more a question of time than money . Even Peter Jackson -- a brilliant man who owns his own post house and VFX house -- does n't have the time to finish all his own effects in 4K for The Hobbit . And those are $200 million movies . <p> Pipeline slowdowns are a huge problem in our business . The joke I used to tell for years is , when I worked at Cinesite in LA from 2001-2004 @ @ @ @ @ @ @ @ @ @ waiting either for files to be copied , files to be backed up , files to render , or files to be restored . Less than 2/3 of it was actually spent color correcting . The smart companies now doing DI 's in LA always have a couple of people hiding in the backroom doing all that dirty work nowadays . <p> I 'm not convinced that 4K is the be-all , end-all anyway . I 'm already concerned that the consumer UHD/quasi-4K we 're about to get at home will still be 4:2:0 , and we 'll again be saddled with 8 bit color . My mantra for years has been " more bits -- not more pixels . " I 've heard DPs say a hundred times , " sharpness is not our friend with this actor , " forcing us to pull a defocus key or use NR on a face to hide up skin problems or age lines . 4K is not going to make that actor look better . <p> It 's great that Michael Cioni and the folks at Light Iron are happy to @ @ @ @ @ @ @ @ @ @ resolutions and camera settings ) . Interesting the differences between Ender 's Game 's workflow and the Elysium ACES workflow ... 
@@44332568 @4332568/ <p> nice give away I 've tried it and got very different outcomes with " i1 display pro " and " i1 pro " ( spektralfotometer ) . It seems that it 's not possible to create a correction for the i1 display with this free version ? Or do I miss something ? <p> Thx Steve . I understand the idea However it could may be useful to have some display/probe corrections for popular entry level gear . Of course that would not cover the variances of the items , but would probabably better than no correction ( ? ) . <p> Anyway it looks that I 'm frigging blind I scrolled thru the manual but could not found any hint how to start the correction processs . So could you please point me in the right direction ? <p> I played a little more with it and got some issues : -the i1display pro shows further more very different outcomes compared to the i1 pro , despite to the use of a correction matrix . How close could one expect them ? - got a lot crashs @ @ @ @ @ @ @ @ @ @ button , before disconnecting ) - got instantly crashing ( with click on calibration interface ) on Acer Travelmate ( core duo , 2GB ram , win7 pro , 32bit ) Are there any log files , other hints ? <p> I 've played a bit with this tools and became a little confused . If I understand it rightly , the filters of the colorimeter do n't cover the small bandwith of the wide gamut CCFLs properly and it 's required to correct that based on the readings of spectralfotometer . Maybe I 'm to enthusiastic , but should that notgive the same results in the mid and high levels ? I got these results : <p> i1pro rgb balance <p> Delta E <p> i1 display pro with correction : <p> Which of the results is more trustworthy ? Both measurements were made after a decent warm up ( more than 1h , with both probes placed on the monitor ) and the i1 pro was recalibrated before building the reference for the i1 display . <p> Steve , thanks for your reply . So , you suggest there @ @ @ @ @ @ @ @ @ @ in high-midtones ) ? It 's right , the verification period of the i1pro is expired . I have to send the i1 pro to xrite for recalibration . The i1 display is less than a year old ( btw purchased from lightillusion ) , but I thought there is no verification for this tiny thing ? <p> But I guess the variation in the results from the i1 pro , which are in the bounds of possibility would cause a deviation in absolute terms , but would not effect the errors from the i1 display in relation . But yes that should be fixed first . 
@@44332569 @4332569/ <h> Picking the brain of a DIT <p> I have been doing basic DIT work on set for a while now . I handle the data monkey side of things primarily , create dailies and try my best to help the DP with whatever I can . Still a lot to learn on that side of things . Once we get to post and I am finished editing , I also do the coloring for the film . I am really looking to step my game up though and put together a real DIT cart . I 'm planning to purchase a filmtools senior cart , run my macbook on it with a 27 " thunderbolt display for my GUI monitor and I 'm planning to finance and purchase a FSI CM-171 . This should take care of things on the equipment side for a while I hope . I am wondering if there are any professional DIT 's on this forum that would be willing to let me PM them and ask a few questions . I 'm curious as to how they receive or feed video throughout @ @ @ @ @ @ @ @ @ @ . I 'd also like to find out a little more about what they feel is their responsibility on set . Thanks for any advice . <p> Yeah . I 'm definitely getting that feeling . Unlike most DIT 's I work specifically for a production company and I 'm not out looking for work . Just trying to do a better job for the company I am with . Thanks for the links , every bit helps . <p> Note that these are not cheap . My concern is often for projects on location , where the need to get drivers and transportation over difficult terrain is a tough challenge for anybody with a cart setup . One could argue that maybe it would work better in a truck , but I think there 's differing philosophies on the process . Technicolor , Ascent , eFilm , Bling Digital , and Fotokem ( to name but a few ) each have different systems that work fine . <p> To me , the three essential roles of the DIT are a ) making a good temp picture to guide the @ @ @ @ @ @ @ @ @ @ and efficiently , and c ) protecting the data at all costs . I 'm not a DIT , but the guys I know who are have recommended the following software packages : <p> Plus there 's a ton of free utilities out there like RedCineX-Pro , Wave Agent , and so on . Having great data recovery software is highly recommended , too ; Data Rescue is one of my faves , and it 's saved me on a few occasions in post . <p> Because there are a few different on-set workflows for DITs , having a well thought out cart is important . The cart I would build for a live signal-grading system would be different than one where you offload and correct with Davinci Resolve , Redcine-X , etc . It seems like you have a handle on the ingesting and dailies creation side of things so if you have specific questions about building a system for live grading or signal distribution for multiple cameras , I would be happy to answer . <p> Yes Patrick , I am very interested in live grading . We @ @ @ @ @ @ @ @ @ @ I can get done while on set would greatly help things out . We typically shoot a feature film in 8-12 days , and have to edit , do sound design and color in less than 3 weeks sometimes . Unfortunately there is only 2 of us in post , so this is quite an undertaking . I was very excited to see that Resolve 10 utilizes live grading , and I want to find a way to work it into my workflow so I can get a good jump start on grading as well as give the DP a much better idea of what he will be getting with the final picture . A lot of the time the DP is just looking at raw footage or the exported signal from the Sony F3 with the baked in Rec 709 LUT . We also do n't have the best production monitors on set , which is why I am trying to get the FSI CM-171 . I will give you a quick run down of our current setup and then how I plan to build my cart out . @ @ @ @ @ @ @ @ @ @ The SxS cards only record 8 bit ProRes 422 files , so we send a signal out of the HD/SDI output to a PIX 240 . This allows us to record a 10 bit ProRes 4444 file . The signal then runs out of the PIX to the 1st AC 's TV Logic monitor for focus pulling . I run an image out of the SDI output to the operators TV Logic for framing and then run a signal from the monitor out to a switcher which feeds the A and B monitors for the director and producers to watch . We do all of our editing and coloring off of the ProRes 4444 files and keep the SxS cards as backup just in case . <p> My ideal cart would be assembled as follows . I would receive a feed from each camera that would run into a switcher . This would allow me to switch between A and B camera when sending a signal into Resolve for live grading . I am aware that the CM-171 can accept multiple inputs and can also display side by side images @ @ @ @ @ @ @ @ @ @ feeding an image into Resolve . I am thinking that down the line it makes the most sense for me to take the feed from the camera and then send it to video village from me . Maybe I could throw a LUT box into the mix so the director is still able to at least see a Rec 709 look instead of raw . At this time I do n't really want to spend a ton on a switcher , though as I said , it might be beneficial for me to get something that allows me to loop through the A and B camera while still being able to send a dedicated signal to Resolve . Someone in a previous thread mentioned the Monoprice 4x1 SDI switch . From there I would feed the signal into a Sonnet Echo Express III Expansion Chassis . Since I currently work off of a Late 2011 MacBook Pro , I do n't have the option of PCI-e card expansion . This would give me 3 PCI-e slots that would connect through my thunderbolt port . In one of the slots I @ @ @ @ @ @ @ @ @ @ it is one card , I have spoken with Blackmagic and they insure me that the system will recognize it as two independent cards . Therefore I could feed a signal into Resolve to live grade and then output it to the monitor for viewing . This leaves me with 2 other slots . I currently only have need for one other slot and that would be to expand my data transfer capabilities . I was looking at the Highpoint USB 3.0 expansion card . This would give me 4 USB 3.0 ports . I currently only have USB 2.0 , so this would be a huge boost in transfer speeds . The PIX 240 caddy connects through USB 3 as well as the G-Drive hard drives we purchase . This would allow me to read a card and dump to two drives simultaneously . <p> As mentioned before I would be using a Filmtools Senior cart as my station . This model is the long 48 " version which would give me extra workspace for drives and cards . I also need something collapsable as well to load into @ @ @ @ @ @ @ @ @ @ and an offset nose pin to the U shaped handle on the end of the cart and I can keep the monitor off the top shelf and utilize more of the space . I currently have a APC-1500 Backup battery that gives me 20-30 minutes of power in a crisis situation . I would really like to move to a rackmount version later and install a rack on the bottom shelf of the cart , but the cost just is n't worth it right now . I would also be very interested to hear any opinions on traveling with DC batteries that I could run the cart off if we are shooting outdoors . I know this is a pretty inefficient method , so any suggestions there would be helpful . <p> I am absolutely open to any other suggestions for equipment you think might work better or any reasons you see something not working the way I think it will . I 've already had some ideas shot down in the past , and I am really grateful that people pointed out the flaws in my design before I @ @ @ @ @ @ @ @ @ @ away from pulling the trigger on making the investment to really step up my game , and I am using that time to research and talk out my ideas before wasting any money . Thank you so much for your help . It is very much appreciated . <p> I currently have a APC-1500 Backup battery that gives me 20-30 minutes of power in a crisis situation . I would really like to move to a rackmount version later and install a rack on the bottom shelf of the cart , but the cost just is n't worth it right now . I would also be very interested to hear any opinions on traveling with DC batteries that I could run the cart off if we are shooting outdoors . I know this is a pretty inefficient method , so any suggestions there would be helpful . 43971 @qwx453971 <p> Unless they 're lighting the set with fire torches , I 'd assume there 's a generator somewhere to provide electricity for the grip &amp; lighting crew . I would bet they could give you two or three amps to @ @ @ @ @ @ @ @ @ @ be figured out under " battle " conditions , and stuff like a working sound cart or a working D.I.T. cart only comes together over time . One thing I 'll warn you about the Filmtools/Backstage full-size carts is that they 're very heavy and hard to move up stairs . You need a couple of guys to maneuver the stuff around . Another potential issue is that both hard drive enclosures and desktop computers make a ton of noise on set and are not welcomed by the sound department , since all that noise gets picked up by the mics . ( UPS power backups also tend to put out hum and noise as well . ) <p> If you 're shooting an entire feature film in 8-12 days , my observation is that there 's going to be zero time available to color-correct on set . Shooting 10 script pages a day is very , very challenging , even given a really big crew and lots of amenities . Just copying , renaming , and backing up files will be a lot of work . I just finished @ @ @ @ @ @ @ @ @ @ 21 days , and there was n't time to do anything more than to just get a very basic Rec709 look , enough to say , " OK , close enough ... roll camera ! " With a 60-day or 90-day schedule , shooting 2 pages a day , then there 's lots of time to test and experiment and do all kinds of stuff . <p> I think there is enormous value in having a cart that is quick and self-contained . It sucks to spend 15 minutes after every company move plugging in wires and hard drives , etc . I like to keep things mounted as much as possible , whether that 's standard rack gear or zip ties on the hard drives , etc . I built my own DIT cart road case style , because I 'm also a hobbyist woodworker . That FSI monitor is very nice , but I think usually that level of quality is not needed on set . You are only making rough decisions , enough to fix/identify problems and set a general look . Personally I use an LM-2140 @ @ @ @ @ @ @ @ @ @ plug the Comtek transmitter into the monitor 's headphone output , so all the producers at village always have a good signal . The sound guy likes that , and it 's also a little weight off his shoulder . ( Assuming the camera is getting a wireless audio feed already . ) If you 're live grading the director 's monitor in Resolve , you might have a hard time simultaneously doing other work in Resolve . That 's one benefit to something like Pomfort LiveGrade . Regarding the UPS , noise is somewhat alleviated if you put it in an enclosed rack case . I actually have a dilemma right now with that . I had a 2200VA line interactive unit from APC , that made a very reasonable amount of noise . But after doing more research , I decided to replace it with an online double conversion UPS in order to clean up bad power from generators . Unfortunately my new one from Cyberpower sounds like a hair dryer , and apparently all online units do . I 'm curious whether other people think line interactive @ @ @ @ @ @ @ @ @ @ comes to protecting expensive computer equipment . <p> Marc - it is true that I have little time to do much besides dump , label and sync audio at the moment . My biggest issue though is that since I also have some engineering duties on set , I am getting pulled away in the middle of work to handle small issues on set and I waste a lot of time going back and forth between set and where I am set up . If I am right on set , it will also make it easier for me to switch mags more often and work with smaller batches of files . <p> Elliot - I am aware that the CM-171 is probably overkill for set , but it will also double as my color grading monitor back in the office . We are currently using a Panasonic BT-LH1710 and it 's crap . I also definitely agree about wasting so much time setting up and tearing down with each move . I 'm not too concerned with the weight of the cart . Rarely do we shoot in areas @ @ @ @ @ @ @ @ @ @ If we do , in the worst case I can camp out downstairs , which is generally what I do now . Noise is definitely something that has crossed my mind . If I do temporarily lose power , the hum on the battery backup would kill a scene for sure . I am not looking to be sitting right next to the camera though , just a little closer than where I am currently located . I am usually stuffed away in some back room in the building , or out in a truck if we are shooting outside . I just want to be closer so the DP has a reference without having to travel back and forth between video village to set up his shot . I do n't ever plan to live grade the directors feed . They currently get a feed from the camera with a baked in Rec 709 LUT and it will probably continue that way . Gives them a good enough idea of what is being shot . <p> I agree that working in Resolve to perform live color grading on such @ @ @ @ @ @ @ @ @ @ interface that can grade multiple signals at once with out switching back and forth is preferred . LiveGrade can connect ( VIA USB hub ) up to eight LUT boxes ( Pandora Pluto , HDLink Pro and coming soon , the Fujifilm IS-Mini ) , all controllable with one easy-to-use interface . When you are grading and data managing and running cable and moving your gear every 3 setups ( and the light keeps changing ) , you must simplify . <p> For signal switching and distro to video village , you can use one small 4x1 switcher plus a bunch of AJA or BMD DAs , or better yet a Smart Videohub or KUMO . Fast and easy switching in one small rackmount unit . I can take LOG signals , send them to LUT boxes , send those signals to multiple monitors and capture cards without unplugging and replugging . <p> I agree with you on the monitor . If you are grading - even basic LOG-to-Rec.709 - you need to know you have an accurate monitor that you can trust . The FSI CM-171 is a great @ @ @ @ @ @ @ @ @ @ rented 10-bit panels like Cinetal B230 , TVLogic XVM and Sony BVM OLEDs , but for the day that those are n't available and the production insists on their own 17 " Panasonics , I will buy a CM-171 . <p> You did n't mention scopes . If you ca n't afford a versatile waveform monitor like the Leader 5330 with Anton Bauer battery plate on the back ( you never know when you might be in the back seat of a car and need to monitor the exposure ) , you can start with Scopebox on your laptop and an Ultrastudio Mini Recorder . <p> The UPS is really to save your hide if you lose power while offloading data . On a film set you should have power from the lighting dept. or house power 99% of the time . The noisiest thing on my cart right now is the Truelight SDI LUT box , which has a couple of fan spinning full time . My 8-bay RAID is whisper quiet , and the UPS only makes a racket when the power is pulled . <p> My carts @ @ @ @ @ @ @ @ @ @ rack rail , 2 x 24 " monitors , RAID , UPS , LUT boxes , signal routers , drawers of stuff , plus a rain cover . Once in a while I need to enlist help lifting it up stairs or pushing up an embankment , but everything is self-contained and fast to power-up . There are mobile solutions like DIT Station Rogue4 ( and many have built their own ) , but they often do not have live monitoring and grading capabilities . <p> It sounds like you plan to create a scale down Lilypad-type workstation and give it live grading capability . You already have the data management infrastructure . For live monitoring and grading I would add : <p> After reading some of the articles that Marc posted , I definitely have more questions but also quite a few answers . My biggest question at the moment is monitor size . In one of the articles , Ben Cain mentioned that a 24 " monitor is needed to objectively evaluate an image . The 17 's tend to sharpen things up and make viewing focus not @ @ @ @ @ @ @ @ @ @ due to the short time on set is limited number of takes and we tend to have shots out of focus . I know this is something that just ca n't be fixed in post . FSI is having that great sale on the CM-171 for $800 less . Is the CM-240 really worth the extra $2500 , and when I say that , I know that it is , but at my level of expertise am I really going to do that much better of a job with the 24 " . I know technology moves at an incredible rate , so I am curious what the resale value is on grading monitors . If I get the 17 " and decide in a few years to upgrade to 24 " or 32 " is it going to be worth it ? or should I just go ahead and go with the better monitor now ? <p> I would definitely love to own a Leader 5330 , but I just ca n't justify that cost at the moment . I will definitely check out Scopebox in the meantime . @ @ @ @ @ @ @ @ @ @ a few years to upgrade to 24 " or 32 " is it going to be worth it ? or should I just go ahead and go with the better monitor now ? 43971 @qwx453971 <p> That 's a tough call . Keep in mind the bigger the monitor , the more of a chore it is to carry around . For me , I went with a 17 " because it 's more portable . I think anything bigger than 24 " will be replaced by 4K monitors very soon . So for me , it did n't make sense to invest in an HD model above 17 " because I am expecting the bump up to 4K soon . <p> Is the CM-240 really worth the extra $2500 , and when I say that , I know that it is , but at my level of expertise am I really going to do that much better of a job with the 24 " . 43971 @qwx453971 <p> Take that $2500 and invest in a nice control surface . Learning how to use color wheels will certainly make grading @ @ @ @ @ @ @ @ @ @ you do it , the more you 'll do , and the better you 'll get . I would recommend the Tangent Element . Skip the Wave . There 's a reason you see them selling used all the time . because everyone outgrows it ! The Element panels are well built and have a small footprint , so your precious workstation space is n't wasted . <p> I have a setup similar to this . I played around with a bunch of different brackets and things , but after a bunch of trial-and-error , I settled on a monitor yoke . A monitor yoke give you perfect balance of the monitor , much more stable . I got mine from cinecrew2000 on eBay. 
@@44332570 @4332570/ <h> Geoff Boyle has balls of steel . <p> Hats off to Jeff for organizing such great tests and not to afraid to publish them , warts and all . LONG ... Red users will burn Jeff into stake for claiming LLO Daylight Dragon 's DR only at 12.5 vs Alexa 's 15 . Even Varicam with Sony IDT came out at 14 . The best one , Dragon STH with Tungsten is just 10 ! That should really stir up Red 's hornet 's nest <p> As a Dragon owner I 'll chime in and say that I 'm not surprised the Alexa rated at 1.5-2 stops better . This was well established over a year ago through numerous tests ( even tests of RedUser found this to be the case ) . Basically , the consensus has been that if red claims 14 stops then the Alexa is 15-16 if red claims 12.5 then the Alexa is 13.5-15 , etc ... <p> The one thing that does confuse me though is that Geoff found the Low Light Optimized OLPF to be better in the highlights and the @ @ @ @ @ @ @ @ @ @ . This is basically the exact opposite of how those are supposed to operate . <p> Also , I 'd be curious to know if Geoff used the D.E.B. feature when debayering the R3Ds or even the ADD feature , since it seems like biggest thing holding back the Dragon in his tests was noise . <p> There are a bunch of things about the methodology here that seem a bit off . Big red flag is the idea that a miss assigned IDT is in some way telling . Why stick the sony IDT on a Varicam in the first place ? <p> Also , I 'd be curious to know if Geoff used the D.E.B. feature when debayering the R3Ds or even the ADD feature , since it seems like biggest thing holding back the Dragon in his tests was noise . 43971 @qwx453971 <p> Applying any type of NR ( ADD and DEB are various RED Dragon specific types of NR ) would distort all tests . What NR would you use with the rest of the cameras then ? What would it prove ? I wholeheartedly @ @ @ @ @ @ @ @ @ @ type of NR ( ADD and DEB are various RED Dragon specific types of NR ) would distort all tests . What NR would you use with the rest of the cameras then ? 43971 @qwx453971 <p> Well , if we 're going to start disqualifying certain debayering options , in the name of upholding fairness then where do you draw the line ? Is it unfair if a camera has built in NR ? And I 'm not even suggesting he SHOULD have used DEB , I 'm just wondering if he tried it at all . <p> I honestly do n't know , maybe that ticking one box ( that is freely available in the RED SDK ) can make for cleaner blacks ? Maybe it would screw up other things in the process , that 's why I 'd like to know if he tried it . <p> Additionally , if it 's all about fairness , then why was the Dragon shot at 4.8K rather than 6K ? How is it fair to handicap one camera 's resolution vs another ? <p> Maybe that extra 1.2k @ @ @ @ @ @ @ @ @ @ can all agree that it would have at least some effect on the noise when downsampled down to 4K or 2K . <p> I too love that Geoff has done these tests , and I hope he 'll continue to do them for years to come ! I personally find them immensely valuable to learn more about my camera and it 's strengths and weaknesses vs other cameras that are available . So I 'm not faulting Geoff in the least , I just would like to know more about the choices made given some of the results . <p> Well , if we 're going to start disqualifying certain debayering options , in the name of upholding fairness then where do you draw the line ? Is it unfair if a camera has built in NR ? And I 'm not even suggesting he SHOULD have used DEB , I 'm just wondering if he tried it at all . I honestly do n't know , maybe that ticking one box ( that is freely available in the RED SDK ) can make for cleaner blacks ? Maybe it @ @ @ @ @ @ @ @ @ @ 's why I 'd like to know if he tried it . 43971 @qwx453971 <p> If you were advocating using better debayering algorithm , then sure , that 's a valid question . DEB has nothing to do with debayering algorithm . It is clearly a NR tool . Advocating using NR on one camera and not allowing it on others would skew the whole test . You do n't see it ? ADD is not available on SDK and frankly , it 's not working in real time . DEB is available , but not every software manufacturer chose to implement it , because some have much better NR tools . Do you use NR on all of your material ? Would n't necessity of using DEB with RED material indicate , that it is inherently noisier camera , if it always REQUIRES NR , unlike other cameras ? <p> If you were advocating using better debayering algorithm , then sure , that 's a valid question . But advocating using NR on one camera and not allowing it on others would skew the whole test . You do @ @ @ @ @ @ @ @ @ @ . DEB is available , but not every software manufacturer chose to implement it , because some have much better NR tools . Do you use NR on all of your material ? Would n't necessity of using DEB with RED material indicate , that it is inherently noisy camera , if it REQUIRES NR , unlike other cameras ? 43971 @qwx453971 <p> So what would you do if RED had n't included it as a user option in the SDK and just implemented it as an unchangeable default ? Is RED disqualified from all testing at all then ? <p> The point is that it is a viable option . It exists . Just because other camera do n't offer something similar does n't necessarily mean you have to disqualify it 's usage . <p> So what would you do if RED had n't included it as a user option in the SDK and just implemented it as an unchangeable default ? Is RED disqualified from all testing at all then ? <p> The point is that it is a viable option . It exists . Just because other @ @ @ @ @ @ @ @ @ @ you have to disqualify it 's usage . <p> Should the Sony A7S be disqualified from all camera tests ? 43971 @qwx453971 <p> All cameras have this option . It 's called post Noise Reduction . One more time . You 're advocating using NR with Red camera , but disallowing it with other cameras . It 's quite simple ... BTW , DEB is very limited form of NR . Resolve built-in NR , unless you 're using free Resolve Lite , is much better , than DEB . Nucoda 's Clarity is light years ahead of both . I find it ironic , when first images for Dragon appeared many were taken back as to how noisy it was . So , here comes a fix- DEB . And then , voila , it 's now a " feature " The same can be said for botched roll out of the original OLPF . Now interchangeable OLPFs is the " feature " too ... <p> when first images for Dragon appeared many were taken back as to how noisy it was . So , here comes a fix- @ @ @ @ @ @ @ @ @ @ a " feature " The same can be said for botched roll out of the original OLPF . Now interchangeable OLPFs is the " feature " too ... 43971 @qwx453971 <p> The problem with a patch mentality is that it only leads to more patches . People sometimes question why ARRI and other cameras are comparatively simple . It 's because they get the basics right early on , and therefore do n't have pay the Piper later . If Geoff is attempting to accurately compare cameras , then his cutoff needs to be based around incorporating everything up to , and including the recording of the raw data , but nothing after . To turn those post processing features on would really just be the equivalent of using camera-specific NR in your timeline . <p> I think what Jake 's trying to get at in the comment below is that many of the " features " RED seems to offer are actually patches for questionable fundamentals ... <p> The problem with a patch mentality is that it only leads to more patches . People sometimes question why ARRI and @ @ @ @ @ @ @ @ @ @ get the basics right early on , and therefore do n't have pay the Piper later . If Geoff is attempting to accurately compare cameras , then his cutoff needs to be based around including everything up to , and including the recording of the raw data , but nothing after . To turn those post processing features on would really just be the equivalent of using camera-specific NR in your timeline. 43971 @qwx453971 <p> I get it , I really do . Honestly I think this is one of the fundamental differences between Arri and Red , Arri products just work . <p> But do you guys know for a fact that Arri and Sony are not doing any noise reduction in their debayering process ? <p> I 'm not trying to pick a fight , it 's an honest question , I really do n't know . I have n't ever looked at any of the code behind any of these raw conversions , but maybe you guys have . 
@@44332571 @4332571/ <h> Is there any real way to " restore " /clean up old VHS footage ? <p> I know I may be going down the rabbit hole with this one , but I was just recently given old VHS footage from home movies .... that were converted to a DVD format . After looking at the footage and bringing it into the Adobe suite to work on , I noticed the obvious things , overblown highlights , oversaturation , etc which I was able to adjust . ( although , the more I got it within legal limits , the flatter the overall footage looked ) <p> Are there any other suggestions to clean up this footage more than that ? I 've run into some issues . It definitely is soft . I threw some sharpening on it , but that was a little tricky , as too much makes the footage literally hard to watch . I know that I can not necessarilly add what was n't there , but I was n't sure if anyone had any " tricks " . I was able to add some contrast @ @ @ @ @ @ @ @ @ @ eyes , but man .... those blacks are very easy to crush . <p> I had to do the same thing on a project recently . If you or someone else , can re-capture the VHS tape over component video ( or at least S-VHS ) directly to ProRes/DNxHD you will have a much nicer starting point . The DVD conversion included some fairly intense compression , that you will now have to rip and transcode just so you can work with it , only to render out and possibly re-compress it again ( H.264 ) when you 're done . You will eliminate a number of conversions , and get back a lot of information if you can capture the VHS directly to a file . <p> As far as restoration , a Neat Video pass with some light grading should be all you need . Old footage usually benefits from slightly balancing out some of the dominant color cast , and then setting contrast a bit . It usually has a lot of character , so the goal is n't to make it ' new ' looking , @ @ @ @ @ @ @ @ @ @ distracting as much as possible . <p> Lots of VHS issues should best be fixed in the analog world . Digital " burns " in many of these things . A pass through a TBC and something like a Snell Prefix will address many , many issues . Things like blown highlights may be recoverable much more from the original tape than may appear to be possible . But you lock yourself in , in many ways as soon as you go into the digital realm . You should make sure you 've done as much as you can in the analog world before starting digital work . <p> Thanks for the information gentlemen and thanks for that link Jason . That 's awesome . I 'll ask if those tapes still exist . If not , it will just have to be what it is . <p> Juan , it 's funny that you mentioned recovering the higlights . Even with the digital copy I was given , once I pulled the highlights down past 100IRE , I saw clouds in the sky Did n't  expect that . I @ @ @ @ @ @ @ @ @ @ But anyway , yes , I agree with both of you . There 's only so much you can do with the footage in its current state . <p> You need to get your VHS into another digital format . The other issue you have is just on how well do you want your video restored : from 1% - 100% ? The company I work for transfers 35mm film to 2k . dpx . The final deliverable , after restoration and color , is completed is 1080 . <p> Our restoration artists spend weeks cleaning the films frame by frame . The larger format gives our team more detail ( control ) . They need to do their work before I can do mine , color restoration is the final step . <p> The same process holds true with old videotaped programs . So , you 're really looking at two processes : one is film/video restoration , the other is color restoration . This process requires two completely different software programs in one unified workflow . <p> Even with all the talk about the new Resolve v10 ( which @ @ @ @ @ @ @ @ @ @ one thing really well : color . <p> I only provide this as information about the restoration and color restoration process , and not as an effort to solicit work . Hope you found the information helpful . <p> Very true . The best VHS deck ever made ( IMHO ) was the JVC BR-S822 , which does have a built-in TBC . Turn off the NR and the enhancement , and you can get a very , very stable picture ( even genlocked ) into a capture card . If you have scopes on the capture card , you can use the proc amp within the VCR to adjust the levels a little bit . Once there , the rest boils down to fixing bad dropouts , glitches , hits , doing a little color-correction , and then some NR if necessary . <p> Very true . The best VHS deck ever made ( IMHO ) was the JVC BR-S822 , which does have a built-in TBC . Turn off the NR and the enhancement , and you can get a very , very stable picture ( even genlocked @ @ @ @ @ @ @ @ @ @ on the capture card , you can use the proc amp within the VCR to adjust the levels a little bit . Once there , the rest boils down to fixing bad dropouts , glitches , hits , doing a little color-correction , and then some NR if necessary . 43971 @qwx453971 <p> That was a great deck , indeed . I wonder if there are a few units still around in great operating condition . I gave one away that was in great shape 13 years ago because I had no more use for it . <p> That was a great deck , indeed . I wonder if there are a few units still around in great operating condition . I gave one away that was in great shape 13 years ago because I had no more use for it . 43971 @qwx453971 <p> I held onto one for nostalgia -- my last analog machine in the house . A buddy of mine still has a bunch of 1 " , Betacam , and 3/4 " decks which they use on rare occasions to dub archival footage , mostly @ @ @ @ @ @ @ @ @ @ how to doing that well . <p> What Juan says is an absolute must--run the VHS tapes through a time base corrector first . Without it you will get geometry problems that are probably next to impossible to correct digitally . <p> Imagine a deck of playing cards . Each card is one line of video . If the cards are stacked but in disarray , a TBC will line them all up so the deck is all neat and straight . <p> Another thing , depending on the age of the tapes , the oxide may she 'd from the polyester base . Do n't subject the tapes to too much VCR transport until you 're ready to play them back for digital capture . 
@@44332572 @4332572/ <p> A client supplied me with a LUT he bought somewhere , it is supposed to be 709-&gt;Fuji3510 viewer LUT , i normally drop something like this over a linear ramp to see what it 's doing .. in this case note the toe of each colour channel ; <p> I have no idea who made it , but i am not comfortable working with that particular LUT , most software have packaged LUTs that are at the least not actually destructive to the image ... <p> Take a quick look with the LUT over a linear ramp <p> If i was the producer I 'd ask for my money back on this one .. <p> Do n't know whom of the many LUT providers created this piece of art , but it kinda pains me see anyone actually pay for this <p> I never used it , i check LUTs on a ramp before i use them . that one was really bad , it was labeled " 70923510 " <p> Producer sent it to me with a note that they really liked the look in the offline @ @ @ @ @ @ @ @ @ @ a uncallibrated computer screen set to ultra bright so they could not see any shadows ... <p> Just overlaid it with a ramp with a CC on it and made curves to match it , but without the wacko-toe action do n't think it took me more than 120 seconds <p> diid start the grade with that cc node , and went from there , changed just about everything anyway on that music video <p> I approach all those LUTs for sale and the freebies from the WorldInterWideNetWeb with great caution And it 's very easy to match them in a cc node and then have control of the whole mess <p> I approach all those LUTs for sale and the freebies from the WorldInterWideNetWeb with great caution And it 's very easy to match them in a cc node and then have control of the whole mess 43971 @qwx453971 <p> Dermot is very wise . <p> I have been sorely tempted to reverse engineer a complete set of these so-called " look LUTs " , matching the look as Resolve corrections and just recreating them as corrections inside stills @ @ @ @ @ @ @ @ @ @ sell you a film emulation LUT for -- say -- Kodak 5219 , and yet none of them match , tells you there 's a lot of smoke &amp; mirrors and subjective interpretation here . As far as I know , Kodak invented the 3D LUT and they changed them about every week on me when I worked at Cinesite. 
@@44332573 @4332573/ <h> Color Restoration of Classic Films <p> I am the Senior Colorist on a project to restore 250 classic Spanish language films for U.S. distribution via cable and satellite . The films were produced from the 1930s - 1980s . Many of them are from the Golden Era of Mexican Cinema , and have not been seen by the public in decades . <p> The films are cleaned and scanned in Mexico City to 2K . dpx from either a print or negative and shipped to West Palm Beach on hard drives for restoration . Once they are fully restored , I put them on the bench for some color love . I drive a DaVinci suite with a Flanders Scientific LM-2461 grading monitor . <p> As Colorists it is very easy for us to change the tonality of a film . However , unlike most other color jobs I do my role as Senior on this project is to help restore the color of the director 's vision when the film was first produced . In many cases this is a challenge because the films are pretty much @ @ @ @ @ @ @ @ @ @ get them . This is the result of degradation due to poor storage , or simply the result of film decay . <p> I do a primary to color balance to find out how the colors react in a scene . Sometimes I get lucky and the colors play nice . In the case of La Escondita ( The Hidden Ones ) , produced in 1956 , it was a challenge just to get the colors to behave . <p> In this scene , the color of the woman 's head scarf had changed several times throughout the reel . Many times what I do is rather like solving a puzzle : what color is it really ? Fortunately , I was able to make a distinction , finding two similar scenes where the color of the head scarf stayed the same : cream colored . So I made a judgement call to stay with that color . <p> To date , I have color restored 60 of the 250 films , and they are airing nationally on Verizon cable . The NYTimes has even written about this project : @ @ @ @ @ @ @ @ @ @ do . Let me know if you find this interesting , and I will be happy to post more before and afters . <p> Hi Timothy , It is interesting , and challenging . As for me , I ca n't say that I actively pursued this area of color correction : rather , it found me . I have no complaints because each film teaches me new things about problem solving , over-coming challenges , and movie-making styles . Definitely worth considering if you get the opportunity . <p> Are you using a lot of secondaries , or is it mostly just a primary grade ? <p> The tech really did n't  exist back then to do secondaries with anything other then production/lighting design or filters , so i can imagine that getting too deep into secondary isolation may conflict with the idea of restoring - not totally re-grading - the classic film . <p> sorry to bombard with questions - old film really fascinates me and i do n't  ever get a chance to work with it . <p> Joey , Thank you very much . Getting and @ @ @ @ @ @ @ @ @ @ of my biggest challenges . <p> Re : original camera negs . I never see those . It 's my understanding that the films are mostly prints and negs . Not sure if they are master positives or dupes . I believe that a number of the films are projection prints , which have seen a lot of use . <p> My job is dependent on not just the quality of the film , but also the work of the restoration artists . I need as much film information as possible to pull the colors out . What I do can expose their work , or any mistakes that were made . For instance , if the restoration is n't done correctly problems become more pronounced as soon as I pull the color out . <p> If that happens , the offending frames are exported out and back to restoration for more work . <p> Joey , No problem . Jason Myers and Pat Inhofer both urged me to post some samples here . So you can blame or thank them for this post . I did n't think there @ @ @ @ @ @ @ @ @ @ correction . <p> I take as long to color correct a movie as it needs . I do get into secondaries , but keeping in mind that my job is not to change the tonality of the film . For example , I use secondaries to color match between shots . In many cases I can speed grade my way through a film and get all 9 reels completed in a couple of days . But that depends on the quality of the restoration and the condition of the film . <p> In the case of the ' La Escondita ' ( above ) , I played with some of the scenes to see what the colors would do . It was quickly apparent that the film was going to require extra time and effort . I decided to think about my approach , and turned instead to color correct other films that did n't need as much attention . In between I pulled up the film to try different approaches . This trial and error approach kept the film on my bench for a good six weeks before I @ @ @ @ @ @ @ @ @ @ that quite a large number of the films that I color restore are black and white . Like the color films , the black and white films are equally challenging and equally fun to work on but for different reasons . <p> I can post some black and white samples in another post if there is any interest . <p> That 's an incredible improvement . Thanks for the example , it really helps illustrate what you 're doing . Do most of the films have that heavy color cast ? Have you been able to set-up any power grades that help you fix common problems like that ? Please post some of your black and white before/ afters , too . It would be interesting to hear more about how grading those is compared to the color films . <p> Each film has its own challenge due to the age , and the way the film was stored , and the source material when it was scanned . Some films are in better shape than others , some not so much . ' La Escondita , ' which I @ @ @ @ @ @ @ @ @ @ was not the worst color challenge that I have seen . <p> The color wash in this example is something that I see on a regular basis . However , while the films may share common traits like this , each one requires individual solutions that are unique to the issue at hand . As the Colorist I find that it helps to have a plan , a toolset or procedure to start with , and be able to think on my feet and adjust accordingly . <p> For instance , I find one of the best things I use to help tame the color is DaVinci 's RGB Mixer . I use this very nearly on a regular basis . It 's a regular go-to toolset in my utility belt . I also use the Mid-Dark Lum and Mid-Light Lum as part of the sweet sauce to help give ' life ' and a little punch to old movies , particularly to hero shots . <p> That 's why , although I have made power grades , they do not come close to solving the problem . Unfortunately there @ @ @ @ @ @ @ @ @ @ . <p> Joey D'Anna said : Thats actually why i was asking about original camera negs - i heard that for some of the shots for the jaws restoration they had to go back to them . 43971 @qwx453971 <p> I only know what I saw in the video . And what an honest comment from Spielberg on the condition of his breakout film , " It was pretty crummy , it was pretty bad . " Given that it 's Jaws and was stored in the vault at Universal I was surprised at the condition of the film when they went to restore it . <p> And to follow up on what Jason wrote , that wet gate film scanner looks pretty awesome . <p> Wow fantatic job James . I work in film restoration too , the last movie I do was " Chimes at Midnight " from Orson Welles and it was a fantastic job . I really love the restoration but i have to say we scan all the material with an arriscan but without a wedgegate and archivegate so was hard-ass . Once again congratulations @ @ @ @ @ @ @ @ @ @ like to know all you can tell me about the process of color and scan please , techniques you use and all becouse your work its fantastic ! Congrats james <p> Hi Andres , Thank you very much for your kind words . I am amazed at the reaction this thread has generated , there seems to be more interest in color restoration than I thought . Colorist Bob Sliga emailed me recently to say that he used the before and afters that I have on my website to teach his students at Columbia College in Chicago . <p> I will pass along more about the color restoration process in future threads . <p> Many times what I do is rather like solving a puzzle : what color is it really ? Fortunately , I was able to make a distinction , finding two similar scenes where the color of the head scarf stayed the same : cream colored. 43971 @qwx453971 <p> Amen . Even under a heavy color cast , our eyes are often pretty good at interpreting what , say , white is when the separation is distinct @ @ @ @ @ @ @ @ @ @ scene , where the walls in the background of primary shots were off-white , and the opposing shots were light blue . It was log footage , with a warm afternoon color cast , and as I started balancing , I could n't figure out why they were n't matching easier . Then after comparing some of the shots , I realized the walls were close , but actually different colors . <p> When we went over the initial grade , the director asked why the whites were n't matching , I said , " because it 's not white , it 's blue " , so even he had forgotten ! <p> Thanks for Posting James . Did you have any stills that you could use as reference ? Also it looks like it has a technicolor look to it or is that how Eastmancolor looked . I have done work with 1960 's NASA footage which was quite a rewarding experience seeing it come alive again . <p> When we went over the initial grade , the director asked why the whites were n't matching , I said @ @ @ @ @ @ @ @ @ @ blue " , so even he had forgotten ! 43971 @qwx453971 <p> You are lucky to work with a director like that . The films that I am color restoring are so old that many , if not all , of the directors are no longer living . The challenge I face with each film is trying to honor their vision as closely as I can . <p> I consider that , as Senior Colorist on this project , I am , in effect , a kind of bridge between the director and the viewers . <p> This is so awesome to see . can you give a detailed profile of your hardware ? Also maybe a before/after with what you did with RGB mixer settings . It 's funny , but some of the jobs I do , I practically consider restoration - indie stuff and docs with such heavy color casts , lighting and other production problems , you really have to go all out sometimes to bring the stuff back , even though it was shot on the latest , greatest cameras . Some of the biggest @ @ @ @ @ @ @ @ @ @ and Panavision ! <p> The short answer is no , I do n't have any before and afters with the RGB Mixer settings . There are two reasons : <p> Firstly , the restored source files were archived immediately after I completed the film . To bring them back , load them in , line them up , and so on would take more time than I can spare at present ; I still have nearly 200 films to color restore ! <p> Secondly , I have found that using the RGB Mixer is more than just lining up the levels . I would describe it as being similar to the way a musician ad-libs during a performance : there is a ' feel ' to riding the mixer . <p> Like any good Colorist , I live by my scopes , particularly the parade and vector . i have learned that the recipe for good results using the RGB Mixer consists of this : - one part scopes - one part eye on the grading monitor ( I use an FSI LM-2461 ) - one part feeling the color @ @ @ @ @ @ @ @ @ @ other way . All I know is it works for me. 
@@44332575 @4332575/ <h> What is the benefit of SLI or Crossfire ? <p> I was under the impression that having ( 2 ) GPUs in SLI or Crossfire configuration would benefit playback , render , and export within Premiere Pro , After Effects , and Media Encoder . But after running some comparisons I see no such benefit . Playback is approximately the same as are render times and export times . Am I missing something ? What is the benefit of SLI or Crossfire ? <p> None of that software actually supports SLI/crossfire . You are better off disabling it . <p> Right now Premiere Pro only ever uses one GPU . Media Encoder can use more then one - but wo n't if SLI/crossfire is configured . After Effects uses the GPU for almost nothing - so no amount of GPU is going to help there . <p> SLI/crossfire is really only for games - i do n't  think any pro software uses it . Software that can use more then one GPU ( for example resolve ) usually specify to have it disabled . 
@@44332576 @4332576/ <h> Pack of LUTs made by me <p> I am a french graphic designer and I often work in photo and video projects. I created a collection of LUTs to ease the color grading of my projects . <p> Recently I created a color grading pack with my 40+ original LUTs . <p> They are profiled on high-resolution color matrix and were built to preserve skin tones. I had to work with many different cameras , I spent a lot of time creating these looks , testing on footages from GH4 , A7S , Blackmagic 4K , RED EPIC and more . <p> Hey Benjamin . Just curious . Did you get a permission to use these images to promote your LUTs ? I see no attribution anywhere on your site . The bottom left image is from music video shot a while ago by Matt Hayslett . <p> Yes , you are correct , the credits are at the bottom . But that still leaves you with a little problem of not having a permission to use those images . Phil posted that and other images to demonstrate @ @ @ @ @ @ @ @ @ @ you up from getting a permission to use those images . Phil and Matt may not care much , but Phillip Bloom does . Pretty much all his published stuff carries a copyright ... <p> I understand . I do not really transforms images , I do not reuse them in another context , I would not allow myself to do that . Those images helps me a lot to test and show the result of the LUTs , otherwise I am a bit stuck . If author ask me to delete , then I 'll do it without problems of course <p> Hi Benjamin , I 'd like to learn more about your LUT creating process . What program did you use to make these ; Resolve , Lattice , ColorIO ? Also , the webpage describes that LUT #9990 transforms Video to Log ... ? <p> You know , if people use a fake LUT to create a look , I see no reason why you ca n't take a look and create a fake LUT out of it . I 've been doing this for some time @ @ @ @ @ @ @ @ @ @ to duplicate with curves . I actually have a few versions of it that I use for different levels of density depending on how hard I 'm slamming the image , and whether I need to roll off the whites or preserve detail . The hue/sat curves are a little tricky , particularly with blues ( which is very similar to the way 35mm negative responds ) . <p> Maybe when the self-contained Compound/Locked Node thing is available , I 'll post some to let people play with them . This could become a popular thing -- and I have no problem with this provided there 's no claims comparing the look to a film stock . A look is just a look ; a kiss is just a kiss . <p> About my LUT creation process , I use multiple applications : Adobe SpeedGrade , Lightroom , Photoshop and After Effects . I export Lightroom presets as 3DL and finally I merge and adjust layers with Photoshop by testing on a ten of stills from different cameras . Last step is testing on raw footages via After Effects . @ @ @ @ @ @ @ @ @ @ a linear gradeient and i can see several area 's where curves appear to show math errors , doubling back onto themsleves , and thereby creating artifacts <p> in this case i applied it inside Baselight , but i do expect identical resualts in any system <p> i normaly convert supplied " look " LUTs into grades , so nothing downstream will be clipped or crushed , i do a lot of checking LUT 's over both a linear gradient and a specfic frame generated by Softimage 's " LUT builder " . Like Marc i have some experience looking at LUT 's with an eye to de-constructing them . <p> Can you see the same issues ? Does anyone else see the same issues with 8700 ? <p> Hello , I test my LUTs on high resolution stills and footages with color chart . I try to not degrade the image . <p> I 'm interested to learn about your process , can you tell me more about : " applied it to a linear gradient and i can see several area 's where curves appear to show math @ @ @ @ @ @ @ @ @ @ artifacts " ? <p> i have all my machines rendering currently , when one come free in a few hours i will grab a screen cap of my scopes ... i use a linear gradient full range , 10bit DPX created in a Flame , and apply a LUT to that , and then look at my luma waveform and parade displays . <p> Ryan i 'd be happy to break down my LUT creation process , if you would like .. I use a variety of applications , depending on the LUTs usage . 43971 @qwx453971 <p> Jason <p> I would also be quite interested to learn . As I understand these things are a bit of a highly guarded military secret for colorists , but since I do n't really work in a big facility with their own color science department , I try to learn where I can , so any advice would be highly appreciated <p> i have all my machines rendering currently , when one come free in a few hours i will grab a screen cap of my scopes ... i use a linear gradient @ @ @ @ @ @ @ @ @ @ and apply a LUT to that , and then look at my luma waveform and parade displays . 43971 @qwx453971 <p> Oh , that would be great ! As I create and test my LUTs only on in-situ scenes ( stills and footages ) , I am really interested to " check the quality " of a LUT via your process . 
@@44332577 @4332577/ <h> How do you achieve some of the looks from The Hobbit ? <p> I am currently a student trying to develop my skills and techniques in color correction and grading . A friend gave me his project to work on and experiment with . He said that he really likes the looks from The Hobbit and had that in mind when shooting his short film ( he shot on the 5D Mark III RAW ) . So , my question is , how do I achieve these looks in Davinci Resolve ? <p> Obviously location/set design , costume , camera , lens , lighting , etc. play a huge part in achieving these exact looks , but how would you go about doing this in Resolve ? <p> With the last image with Gandalf ( and a little with the first one with Radagast ) , I can see that there is some teal pushed into the shadows ; if you look at Gandalf robe/cloak , parts are teal and parts are gray ( the original color ) . But if you push teal into the shadows , @ @ @ @ @ @ @ @ @ @ you qualify the darkest shadows/black and desaturate them ? Or do you create another node and push the complimentary color into just the blacks ? Would this work ? Is this done with the primary and log color wheels or curves ? What would the luma curves look like ? <p> I have so many questions and unfortunately , most of this is beyond the scope of any course at my school , so I am seeking some help here . <p> All your questions sound like good ideas to try ! You 're a student , well this seems like the perfect research . I suggest you just make different versions with different techniques and see which one comes closest . I still do this myself sometimes . Footage , lighting , set dressing , camera , lens etc , it 's always different and what worked on one project looks horrible on another . Just try , try and try . <p> Set design and costumes are really carefully planned there plus light setup . I bet they did super extensive testing also . Grading is the last @ @ @ @ @ @ @ @ @ @ lut does that . <p> The Hobbit looks great because they have three colorists working around the clock , a world-class DP , and six months to tweak the images . It also helps to have a very high budget , fantastic art direction , and the imagination of somebody like Peter Jackson . <p> To me , what I see on the Hobbit movies is very good straight-ahead color correction without any bizarre weirdness going on . I know from some insiders that those movies tend to go a little " power window crazy " ( like windows on every eyeball in the scene ) , practically relighting the scene , but I think some of that comes from the nature of large bluescreen shoots . <p> I know I do n't do much in the way of grading any more , but from all the system 's I 've used it is the most amazing and capable as it links far better than any other system the tools from grading , VFX , paint , etc , into one really viable toolset . <p> It is this application @ @ @ @ @ @ @ @ @ @ get some of the very unusual looks they have generated for The Hobbit , as well as many other films ... <p> And with Mamba being available for so little ( free in some cases ) it is easy to play with the exact same tools used by Park Road Post ... <p> Thanks for the responses everyone ! Marc , in regards to your comment about client 's expectations , it seems every student expects their little to no budget short film to look like one of their favorite features once it gets graded . Anyway , I did some testing/experimenting and came up with this ... <p> ... looks nothing like The Hobbit , which is fine because it 's not ever going to . So , my question now is , what do you like or do n't like about these grades ? ( There from different parts of the film so I 'm not trying to match the shots since they take place at different times ) <p> To me , what I see on the Hobbit movies is very good straight-ahead color correction without any @ @ @ @ @ @ @ @ @ @ In many high-end , heavily CGI films it seems like grading is more of a final compositing pass where all of the shots are finally considered together as an aesthetic whole , rather than an attempt push them toward some sort of extreme look . <p> Definitely . In many high-end , heavily CGI films it seems like grading is more of a final compositing pass where all of the shots are finally considered together as an aesthetic whole , rather than an attempt push them toward some sort of extreme look . 43971 @qwx453971 <p> I 'd second that ( to a degree ) . As a bit of an insight into the third Hobbit film , I know that between Weta Digital and Park Road Post ( along with a lot of new product development from SGO ) , there 's been a huge focus on sophisticated EXR based workflows . The EXR format ( in it 's various EXR , SXR and new XRM variants ) can store a huge number of additional mattes and metadata channels , which can be a really liberating tool in the @ @ @ @ @ @ @ @ @ @ almost a blurring of the lines ) between the worlds of VFX and colour . <p> To tie it back to Marc 's comment about the use of selections and windows , working with this new EXR integration can hugely speed up the grading process , and rather than taking away from the creativity of the grade , actually provides a lot more opportunity . The team down in Wellington are able to make ( quite literally ) " point-and-click " selections to grade and even re-light individual objects in the effects deliveries , tweak the colour of CG elements that were rendered with a specific surface texture , or even grade foreground , midground and background individually , without the need to roto elements , all of which comes in addition to standard selections , shapes and windows . <p> And obviously , there 's a hell of a lot of talent and experience in there as well . <p> But , back to your work , Parker . I think it 's a pretty good start . One comment on the second image ( and this is based @ @ @ @ @ @ @ @ @ @ the motion of the shot or the story ) , I love the moody feel of the main action , but for me the right side of the frame with the burnt sky distracts to a degree and almost pulls the eye away from the action . That 's probably my still-photographer side taking over , though . <p> But , back to your work , Parker . I think it 's a pretty good start . One comment on the second image ( and this is based purely on the still , without knowing the context of the motion of the shot or the story ) , I love the moody feel of the main action , but for me the right side of the frame with the burnt sky distracts to a degree and almost pulls the eye away from the action . That 's probably my still-photographer side taking over , though . 43971 @qwx453971 <p> That was an issue I was having . The sky was blown out and got clipped so there 's not a whole lot to work with . So i just tried @ @ @ @ @ @ @ @ @ @ type effect ( blooming ? ) to the highlights . Aside from the sky off to the right , what else is an issue with this attempt ? You said it was a pretty good start , do you have any other suggestions for me to get it to a decent grade ? <p> That was an issue I was having . The sky was blown out and got clipped so there 's not a whole lot to work with . So i just tried to make the best of it and added a glow type effect ( blooming ? ) to the highlights . Aside from the sky off to the right , what else is an issue with this attempt ? You said it was a pretty good start , do you have any other suggestions for me to get it to a decent grade ? 43971 @qwx453971 <p> It 's difficult to say without knowing context . What 's the mood you 're trying to achieve ? What 's the background to the scene ? <p> It 's difficult to say without knowing context . What 's @ @ @ @ @ @ @ @ @ @ the background to the scene ? 43971 @qwx453971 <p> The a short summary of the 3 minutes short is , two soldiers ( enemies ) communicate through the use of a helmet to overcome the language barrier between them . In the second image , the main character is trying to save his friend who had just stepped on a mine . The Director said that he did not want his film to have the typical " war film " look and that he wanted to have it be a fairly neutral ( true to the environment and time of day , w/ a bit of warmth ) and saturated . <p> The hobbit is a concentrated force of Peter Doyle ( savant grader/colour technician ) 6 months to grade 3 hours in a team of 6 very long hour , and last I heard 42 odd layers of grading . Having said that i think it looks amazing , and wish that all films were able to lavish this amount of time on the grade . Oh yeah they also use Mistika and EXR work flow . But EXR @ @ @ @ @ @ @ @ @ @ last time I checked resolves implamentation was rather crude , but can work . This was about two years ago . <p> The hobbit is a concentrated force of Peter Doyle ( savant grader/colour technician ) 6 months to grade 3 hours in a team of 6 very long hour , and last I heard 42 odd layers of grading . 43971 @qwx453971 <p> No more Peter Doyle -- he departed sometime back . David Hollingsworth was the lead colorist on the first film , Trish Cahill on the second ( all women on the second film , as a matter of fact ) . I believe Mr. Doyle is now working at Technicolor/London . <p> No more Peter Doyle -- he departed sometime back . David Hollingsworth was the lead colorist on the first film , Trish Cahill on the second ( all women on the second film , as a matter of fact ) . I believe Mr. Doyle is now working at Technicolor/London. 43971 @qwx453971 <p> To my Understanding , Mr Doyle set the looks and template for all three . Not exactly sure when Dave left @ @ @ @ @ @ @ @ @ @ be true , or where he is these days ? Also Yes you 're correct Trish , Claire and others are working on the last instalment . But I have to admit this is second hand information . So could be completely wrong . <p> About 9:30 into the interview , Peter Doyle talks about building a " matrix " ? that basically said that " cyan can never be greater than the sum of the other channels , in the blacks only , " that keeps clean blacks . How would one achieve this in Resolve ? Or is this sort of effect achieved some other way within Resolve ? 
@@44332578 @4332578/ <h> Exporting out of Davinci with my Final Audio Mix <p> I have exported XML from Premiere Pro into Davinci and I can see all my clips . <p> However I want to import my final audio mix track so I can export my final version out Davinci but I ca n't import an MP3 or WAV file to the " timeline " . Can this be done or do I have to render out of Davinci just the video and then bring the video in back into Premiere to render yet again with the audio ? <p> I feel kind of silly asking this question because I watched Alexis 11 hour training months ago and I tried going back and finding an answer to this question but I ca n't find it without rewatching all 11 hours again . 
@@44332579 @4332579/ <h> Telestream Episode products entering EOL <p> At Telestream , we consider ourselves innovators . And sometimes the hardest decisions we have to make are those where we have to choose between multiple good courses of action . Increasing our focus and innovation in our core businesses of scalable transcoding and workflow automation , streaming , and cloud requires us to exit other product areas such as our desktop encoding products . We apologize for any disruption this causes you , and are committed to delivering continued support through the end of life process. ? <p> ? <p> What are the products involved in the EOL ? ? <p> Episode ? <p> Episode Pro ? <p> Episode Engine ? <p> ? <p> What are the key milestone dates ? ? <p> End of Sale : January 31 , 2017 ? <p> End of Support : January 31 , 2018 ? <p> ? <p> How they can call it innovation by killing one of if not the best desktop encoder on the market I do n't see . There are basically two major options available now : Adobe Media Encoder @ @ @ @ @ @ @ @ @ @ in the past ) . They are offering Engine users a discount to move to Vantage which for that user might make some sense , given a very expensive one . For Desktop users it 's " good bye " basically . <p> Of course , if you 're mostly making H264/AVC files , the best encoder out there right now is x264 , which is free . It 's relatively easy to write batch command line scripts to automate encoding . Fast and high quality , and you can do it through ffmpeg , so you get to take advantage of all that ffmpeg has to offer , too ( including audio process and encoding/decoding lots of different formats ) . On Windows , AVISynth coupled with ffmpeg is ridiculously powerful . both are free . <p> I ca n't say I blame them for pulling the plug on this tool - the market for desktop encoding tools has been declining for years thanks to Compressor , AME and others . <p> I always liked Episode for having a direct connection into Stone+Wire , so i could easily export @ @ @ @ @ @ @ @ @ @ unique and one of my main selling points when I was selling it a couple years back . Plus it had ProRes-encoding on Windows ( server ) . <p> Of course , if you 're mostly making H264/AVC files , the best encoder out there right now is x264 , which is free . It 's relatively easy to write batch command line scripts to automate encoding . Fast and high quality , and you can do it through ffmpeg , so you get to take advantage of all that ffmpeg has to offer , too ( including audio process and encoding/decoding lots of different formats ) . On Windows , AVISynth coupled with ffmpeg is ridiculously powerful . both are free . 43971 @qwx453971 <p> True , but avisynth requires a lot tinkering and has lots of gotcha 's especially when it comes to color spaces . <p> You can actually use ffmpeg in Media Encoder and Premiere by setting up a frame server and piping the video through avisynth ! <p> We 've been using AVISynth for a few years now for all our DVD and Blu-ray mastering @ @ @ @ @ @ @ @ @ @ for in-house use , so there 's no guesswork . We use it for stuff like simple trimming , scaling , even specify which sections of a film should be greyscale vs color , to avoid any color shifting on B/W material when encoded for DVD or Blu-ray . <p> We do n't work with a variety of file types normally - pretty much exclusively ProRes or 10bit Uncompressed 422 Quicktime files when encoding . So keeping things straight has n't been too hard . Probably not as straightforward if you 're working with a mix of input formats though . <p> The good thing with Episode was / is it 's flexibility of almost all known in and output containers and codecs . It 's not perfect but very broad . The main competitor I presume would be Sorensson Squeeze . <p> The ironic thing is Telestream are recommending a CLOUD service as a replacement . Huh ? I 'm conemplating not sending things to our server to save time , sending it out of the house to service X wo n't ever be that fast . Long-form-projects would @ @ @ @ @ @ @ @ @ @ it , so I 'm guessing , but I 'd imagine that the processing work is still done locally , but the product is SAAS , like adobe Creative Cloud . Some stuff gets stored ( smaller web deliverables ) on their cloud service for sharing , probably , but the work is done on your local machine still . <p> It 's really worth looking into Carbon Coder , if it 's still for sale . It 's designed to take just about anything in and spit just about anything out . last time I used it regularly , there was the ability to set up a render farm , batch processing of jobs , watch folders , and within the application you could do a lot of stuff like cropping , ARC , frame rate conversions , etc . A bit of a funky interface until you get used to it , but a reasonably fast and high quality encoder . <p> This is where I wonder how Telestream is thinking . For whom does the cloud-based service make sense really ? It 's pricey and it 's @ @ @ @ @ @ @ @ @ @ <p> If you 're starting with compressed files , as I imagine a lot of people are , it 's not necessarily that bad . We have about 18Mbps upload speed here at the office , and it 's become a fairly regular thing for us to upload 20-30GB files . it only takes a few hours . I 'm not saying it 's an ideal setup , but as bandwidth increases , this is less and less of an issue . I 'm guessing corporate clients , and organizations who are starting from compressed formats ( H.264 , for example ) , are the target here . I do n't think anyone working in DPX or doing long-form work is who they 're interested in. there 's probably a lot more of the former than the latter . <p> I drilled Telestream about this at IBC and basically if you 're a high-end desktop user they do n't have a product for transcoding any more . Quite shocking . <p> - Telestream Cloud is all in the cloud . If your files are 100 GB they have to be @ @ @ @ @ @ @ @ @ @ per minute of footage and format , hence 1 min of h264 720p will cost different than 1 min UHD h265 . <p> - Vantage is their server / enterprise " in house " solution . It 's something of Episode Engine on steroids and then some . Downside is it has to run on a separate Windows Server hence with a few options and some decent hardware you 're taking $15 000 - $20 000 for that solution . 20X of Episode Pro . <p> It really seemed like the people on the Telestream floor understood my pain but that " people above our pay grade thought this was the way forward " . I think they 're hoping Episode customers do jump to their other products . I have a very hard time seeing us as a small post house ( 3-4 people ) investing $20 000 , i.e. $4 000 - 7 000 per person , for something we today get for $1 200 and that we already own . We 'd be investing 3x $1 200 ( current Episode Pro investment ) + $20 000 ( @ @ @ @ @ @ @ @ @ @ - almost $8 000 PER OPERATOR - CRAZY ! <p> That is dumb as hell . Episode has a great pricepoint for what it can actually do . I do n't know about their cloud numbers but that seems really strange way to do transcoding . Also Vantage is neat but the price is insane . Maybe they could do a Vantage " lite " with fewer options if you do n't need to do insane automation and QC . <p> That is dumb as hell . Episode has a great pricepoint for what it can actually do . I do n't know about their cloud numbers but that seems really strange way to do transcoding . Also Vantage is neat but the price is insane . Maybe they could do a Vantage " lite " with fewer options if you do n't need to do insane automation and QC . 43971 @qwx453971 <p> It seems to me that Telestream exiting this business could create an opportunity for another company to come in and offer a similar product at a reasonable price for those who want to do it locally @ @ @ @ @ @ @ @ @ @ me that Telestream exiting this business could create an opportunity for another company to come in and offer a similar product at a reasonable price for those who want to do it locally and not in the cloud . 43971 @qwx453971 <p> I suspect Telestream exited the business because it 's not worth being in . Standalone desktop encoding is presumably thoroughly covered by Adobe Media Encoder and Apple Compressor . I doubt there were that many customers -- present company excluded -- willing to pay " so much " for Episode 's extra capabilities when they already had it in a low/no extra cost add-on to their NLE . <p> I think we 're seeing the somewhat darker side of democratization here . Just because we have a need , does n't mean anyone is obligated to fulfill it . Just like Apple in certain cases , Telestream has run the numbers , weighed the business case , and decided to exit . They wo n't be the last to do this . <p> I think we 're seeing the somewhat darker side of democratization here . Just because @ @ @ @ @ @ @ @ @ @ obligated to fulfill it . Just like Apple in certain cases , Telestream has run the numbers , weighed the business case , and decided to exit . They wo n't be the last to do this . 43971 @qwx453971 <p> Yep . I was just telling a DP/friend of mine about Deluxe leaving the post business in Australia . At some point , they run the numbers and decide the profits just are n't there . <p> But in the case of a piece of software like this , there is room for a small company to come up with a workalike that will produce the same results at an affordable price . Even if they only sold 100 copies a month , that 's more than enough for some companies to survive , particularly if they already have 3 or 4 other products . 
@@44332580 @4332580/ <p> There is always a weak link and no encryption is unbreakable . The best hardware efforts are easily bypassed trough simple social engineering . For example , I know someone who was in possession of a master DCI key for a MAJOR worldwide release . That 's about all I can say <p> I think the DCI encryption has been broken so the studios are sending out pristine digital copies which are likely to be easy targets for pirates . I could be wrong about that but I know HDCP has been . 43971 @qwx453971 <p> HDCP copy protection in Blu-ray discs is not nearly as severe as the copy-protection in the DCI-regulated DCP files . I was told early on at a SMPTE DCI meeting about 7 years ago that they can literally change the copy protection every few seconds within the key supplied . If anything , the DCP system is too fragile in that if the system senses the slightest thing wrong , it stops and refuses to play on the projector . I know of quite a few major screenings where the DCP failed @ @ @ @ @ @ @ @ @ @ a different key , often keys that are time-limited only through a certain date . <p> As far as I know , DCI copy protection is still locked up very tight ... but I understand that you can never underestimate a determined hacker . <p> I think it 's not just about the technical ( or social as Jake mentioned ) question if dcp encryption can be broken yes or no . You will probably always be able to pay for the cinema or download an illegal copy , so the best question is ' why would you pay for the cinema ? ' <p> Things like Dolby Vision and Atmos sound can keep people coming to the cinemas . It 's all about the big screen experience with incredible sound . There will always be a crowd for that . Plus it 's a social expwrience that some people like . <p> Also some visitors with handy cams will render any protection useless in some sense . 43971 @qwx453971 <p> That 's true . I once heard a studio piracy exec give a speech where he said within an @ @ @ @ @ @ @ @ @ @ movie will hit the net in some form -- even if it 's just a guy in the audience with a camcorder . <p> Where they really freak out is when an editorial copy gets out -- which happened with Wolverine and recently with Expendables 3 . When there 's 7 or 8 VFX houses working on a film , plus three or four picture editors , plus sound editors , plus studio execs and marketing people getting copies of various cuts , there could be a lot of loose files floating around . That 's a lot worse than a DCP , because the movie is n't even released yet . <p> That 's true . I once heard a studio piracy exec give a speech where he said within an hour of the first theatrical showing in Asia , the movie will hit the net in some form -- even if it 's just a guy in the audience with a camcorder . <p> Where they really freak out is when an editorial copy gets out -- which happened with Wolverine and recently with Expendables 3 . When @ @ @ @ @ @ @ @ @ @ film , plus three or four picture editors , plus sound editors , plus studio execs and marketing people getting copies of various cuts , there could be a lot of loose files floating around . That 's a lot worse than a DCP , because the movie is n't even released yet . 43971 @qwx453971 <p> tales from the front lines ; <p> 1 ) There was a show ( in 2002 ) that i worked on and the screener made it to the web before the i finished grading it , i was able to trace the IP addy back ( from note on a list serve , think it was 4Chan ) to a desk in the office of the lawyers who were doing the E &amp; O insurance clearances in LA i did that mainly to clear ourselves from producer suspicion <p> 2 ) Shopping in the old HuaHaiLu market in Shanghai around 2006 , got a copy of " All Gone Pete Tong " for 7.5 cents a gift for the one of the producers who is a friend <p> 3 ) Again Shanghai , @ @ @ @ @ @ @ @ @ @ , she was excited to get into the new StarWars episode that was opening in a week , i gave her my review as had seen it a week earlier , again from the old/awesome/now gone HuiHaiLu market <p> 4 ) More recently i graded a music video that was at the end of a Twilight film , the editorial was trying to lock the cut while i was grading/finishing/VFX'n the clip , so they sent me the entire film as a DPX seq finished / graded .. this was three weeks before it was released .. that 's trust as i had the temp mix delivered with the rest of the Avid files a huge security hole that i kept locked down as tight as i could have <p> 5 ) Doing political ads recently , i had 24/7 security in the building to guard the raw footage. and kept all footage on a separate array that was turned off when i was not working on the spots , plus i was asked to pull the IT connection from my machine while working on the spots . <p> 6 @ @ @ @ @ @ @ @ @ @ for Hasbro at the same time in the late 90 's , we had to rent another floor of the building and get a separate infrastructure in place , there was a VERY limited list of folks who had all access there . <p> Yea security is an issue , always has been , but breaking DCDM encryption seems the long way around given the holes in security one the pic is locked and many screeners need to be sent out <p> From my meager understanding of cryptography , *each* KDM has to get cracked separately . It 's like PGP right ? There 's a public and a private key and you need both to play . <p> The weakest points for pre-release screeners &amp; workprints are in production offices , Distribution companies , Blu-Ray Replication facilities &amp; VOD sites . I 've never met anyone at a post facility that would never risk their career ( and the financial wellbeing of their entire company ) for some minuscule amount of glory in an IRC channel . <p> You 'll know DCP is broke if you start seeing 2048x858 @ @ @ @ @ @ @ @ @ @ of that from the internet people I know . <p> 6 ) Working on an animated series for Disney and one for Hasbro at the same time in the late 90 's , we had to rent another floor of the building and get a separate infrastructure in place , there was a VERY limited list of folks who had all access there . 43971 @qwx453971 <p> When I worked on some pieces of Spielberg 's War of the Worlds back at Technicolor a few years ago , the editors had us hard patch everything , removing it from the router , turned off the monitors at the HD tape machine , and they also had me put a piece of black cardboard in the door window that led to the color-correction bay . Dreamworks was ( and is ) extremely picky about security . 
@@44332581 @4332581/ <h> Quote of the day ... <p> Sitting with a great DP , working with something he shot with 435 , S4 's &amp; 5217 ... really nice images ... <p> The Director wanders in , looks at what we are doing , says ; " there 's a v cool look i think we should try ... I 'd really like you to make it look like it was shot on my 7D , that kind of plasticy skin look " <p> Seems to be just like how log made the washed out look popular . People see it enough and that becomes their new point of reference . It really may be just the ebb and flow of culture ; all things must eventually return back toward the mean . <p> We just went through the technological apex of film , and high-end digital originally sold itself largely on high resolution capture . So that might help explain the current Instagram/ LomoKino/ Plastic Bullet craze . Even in music , lo-fi is popular , 8-bit audio is cool , and musicians are exploring sounds from the dirty @ @ @ @ @ @ @ @ @ @ the door for one project with RYB/Itten . If the diector want 's to turn gorgeous film into 7D plastic , then go for it . Get out the gasoline Jimi Hendrix style and torch that shit . Who knows , you might just become famous for it . 
@@44332582 @4332582/ <h> Achieving the look of " Killing them softly " <p> I recently watched the film Killing them softly with brad pitt . And i fell in love with the look of the film . I happened to have the american cinematographer issue that had an article in the film . The DP said that they were seeking an old school look so they shot with anamorphic . They were trying to achieve a look they got with a canon 5d when doing camera tests . The dp said their blacks tended to go towards the grays . How does one achieve a similar look ? I ve been playing with the blacks in my footage but have not gotten close . I also enjoyed the look of THE MASTER . ( That was shot anamorphic too ) Myself i am shooting with the RED ONE and i have seen footage graded to get a close look . <p> here some guys diged up some old soviet time lenses to use in film with red where they wanted to have organic look and it helped a lot . not @ @ @ @ @ @ @ @ @ @ image like Paul pointed out . <p> Some prime lenses like red tend to give sharpish video look if not used softness in mind . Some also use pro-mist in front of the glass to soften stuff up . <p> Anamorphic by its nature gives you different feel to the image as they compress in lens and if you find good one from 60 's then you should feel really organic i bet . 
@@44332583 @4332583/ <h> Color Palette of David Fincher <p> Juan Hernandez , a contributing writer for FincherFanatic.com has created a PDF entitled Paint it Black that includes breakdowns of several of David Fincher 's most famous movies . <p> ---- <p> David Fincher has been labelled all variations of a ' prince of darkness ' . Perhaps rightfully so , as his movies mostly are crafted with a signature color palette of dark tones , mostly green and blue . Beyond aesthetics , why does Fincher shoot his movies this way ? Because it looks cool ? Or is there an additional layer of meaning to the colors in his films ? <p> Fincherfanatic.com is proud and grateful to bring you the second installment of the newly launched Fincher Film School , with compliments to contributor Juan Hernandez . In this episode , Juan is taking a closer look at Fincher 's use of color to support narrative -- a subliminal and often overlooked directorial task . 
@@44332584 @4332584/ <h> Order of equipment upgrades <p> Hi everyone , first would like to say thank you for this fantastic resource . I would also like to apologize for what will likely be an extremely long post . I am new to posting here so if posts of this length are in bad form please let me know and I will try to be more concise in the future . <p> I have been " color grading " for a few years now using After Effects ' built in tool set , Synthetic Aperture , Magic Bullet Looks and Magic Bullet Colorista . I have also recently begun using SpeedGrade . I have yet to be paid solely for color work , but always " correct " and " grade " any freelance pieces I am editing . I thoroughly enjoy the process of grading and would like to begin taking some steps towards focusing more on color moving forward . <p> I am currently at a bit of a crossroads wondering what the next step forward should be . There are three items I feel are important moving forward @ @ @ @ @ @ @ @ @ @ playback of 1080p material with a few nodes of corrections , a control panel , and a color accurate monitor . As time progresses I will be able to afford all three based on revenues from editing/shooting etc , but I would like to be able to progress as much as possible during the time it will take to acquire the various pieces . I was wondering if any of you all had input as to what would be a logical upgrade order . <p> Personally I was thinking that the control panel or the monitor would be the most critical first component . My reasoning is the control panel would allow me to speed up my grading and the extra time gaining familiarity with the panel while I budget for the other pieces would ensure that when everything is together I can work efficiently right away . My thoughts with the monitor ( the 17in option from Flanders Scientific ) are that having a color accurate monitor would allow me to confidently grade with the system I have now , giving the piece of mind that the colors I @ @ @ @ @ @ @ @ @ @ current computer I would not be able to monitor in real time , I would still have to rely on my mouse for all grading and for the time being display would be limited to 8 bits on the monitor because I would need to connect it to the MBP using a mini-display to DVI adapter . One additional benefit of the monitor , however , is that it would do double duty as a field monitor that provides a number of tools that would be of huge benefit to me as a freelance shooter . Lastly , while I would love to have a fast new computer I feel it is the least necessary of the three components for me currently . I would really love to begin working with Resolve , and actually have a full copy that is unfortunately sitting unused due to my computers limitations . The computer would also definitely help with editing , playback , and exporting for the freelance projects that are my main source of revenue , but while the current MBP I 'm using is limited it gets the job done @ @ @ @ @ @ @ @ @ @ Resolve with lightning fast performance is necessary at the moment while I am still working to better myself technically . Ideally a paying coloring job will come in the future and when that is the case I think the computer upgrade will be warranted . Until then I think I can get by with the software grading tools I currently have at my disposal and a few dropped frames . <p> I would love to hear any feedback you all have and appreciate it if you took the time to read all of that . <p> If you 're familiar with tablets , I suggest you get a Wacom Intuos Pro Small or Medium . That can be a good substitute to a panel until you can afford it . I personally find using a Wacom much better and faster than using a mouse - if I can not use a panel . In fact , I used a Wacom for 4 years before I took the step up to a panel . I 'm not saying it 's how it should be done - but it worked out . @ @ @ @ @ @ @ @ @ @ as well , such as Photoshop and After Effects ( rotoscoping will be much less of a pain ! ) . <p> Think of the panel as a ( really ) long term investment . While you 're grading on your current equipment , read up , ask questions and even try first hand how different panels operate and feel . I honestly ca n't say how I would react if I chose one of the panels I now dislike , and later found out " if I had saved up a little more .. " <p> In my opinion ; get the Avid panel . Feels smooth , has a very small footprint on the desk , and is rather cheap . It 's got the features you need . While the economic cost is small , the time spent browsing through pages and memorizing what bank or shift ( there 's a left and right shift ) key opens up of menus and possibilities can slow you down a little . <p> If you wan na get as close to the " real Resolve panel " as possible @ @ @ @ @ @ @ @ @ @ 'd want to save up for the JLCooper Eclipse . It 's by far the panel I 'm the fastest with . Intuitive layout and most operations are a one-click operation . Some times two . <p> Thank you for the suggestions on various panels and the Wacom tablet . I have read about people using the Wacom tablets , but for some reason had n't considered it . The fact that it can pull double duty in other programs is definitely a plus . <p> I picked up an ex demo wacom intuos and I love it . I 've used it for editing in FCP and Avid , graphics work in after effects / Photoshop and in Resolve . There 's a tactility that I find I get with a tablet that I do n't get with a mouse . You also have your programmable keys and wheel which I 'm sure you could map to some useful tasks in resolve . I picked mine up for about -100 and its been a great investment . Its also meant to help you avoid RSI ... Not something I @ @ @ @ @ @ @ @ @ @ . <p> Another thing you could add to your list of " cheap things to get to help me speed up the workflow while saving for a panel " is a small keyboard or keypad , such as the Logitech G13 or similar . You can basically map anything to any key , and you have a total of three pages per key . You can map things such as " add serial node + window " , " track forward " , " copy grade from previous clip " etc . Saves you a great deal of time , and you do n't have to memorize ALT+CMD+SHIFT+something all over the keyboard . <p> Another thing you could add to your list of " cheap things to get to help me speed up the workflow while saving for a panel " is a small keyboard or keypad , such as the Logitech G13 or similar . You can basically map anything to any key , and you have a total of three pages per key . You can map things such as " add serial node + window " , @ @ @ @ @ @ @ @ @ @ clip " etc . Saves you a great deal of time , and you do n't have to memorize ALT+CMD+SHIFT+something all over the keyboard . 43971 @qwx453971 <p> That 's actually a great idea ! Never thought of this , as i 'm quite in the same situation , got the monitor but not the panel , it 's a good advice ! <p> Thank you for the suggestions on various panels and the Wacom tablet . I have read about people using the Wacom tablets , but for some reason had n't considered it . The fact that it can pull double duty in other programs is definitely a plus . 43971 @qwx453971 <p> I know several editors who are using Wacom tablets for editing , and once you get used to it , you can fly through the material . I 've used tablet-like surfaces with certain color-correction panels ( like Baselight ) , but I 'm skeptical of how well it 's supported within Resolve . <p> I know several editors who are using Wacom tablets for editing , and once you get used to it , @ @ @ @ @ @ @ @ @ @ tablet-like surfaces with certain color-correction panels ( like Baselight ) , but I 'm skeptical of how well it 's supported within Resolve . 43971 @qwx453971 <p> I purchased a small Intuos last year before starting on a feature length movie because I knew the first week would be unsupervised and it would force me to get the muscle memory needed to work in front of clients . The first few days were slow and somewhat frustrating but I will NEVER go back to a mouse or a track ball . Using the tablet and the Tangent Element ( full set ) are almost as fast as the $30k panel , which I used for 10 years . I find myself working in a hydrid manner , sometimes using the panel , sometimes dragging a slider in the U/I and sometimes clicking a box and entering a value . Get the tablet and get good at it , you will miss it any time you have to use something else . I like the small one because I can cover most of the area moving my hand only a little @ @ @ @ @ @ @ @ @ @ negative is searching for the stylus if you take it with you on breaks ! <p> I also second the advice to get a REAL PROFESSIONAL monitor , I 've been working in TV for a LONG time and you have to trust your monitor ! <p> +1 to the tablet ( after you have the mon sorted ) - i was gradeing with a tablet on a Cyborg2K a decade ago , before any software would support a surface , did alot of work with nothing but it over the years .. now i keep a large wacom beside my surface , and often have the pen in my right hand while working with the surface ... old habits i guess <p> I love my Intuos 4 ( Better than the 5 IMO ) ! You map custom macros/shortcuts to the panel and have them identified on the corresponding OLEDs . Easy to read in the dark . They can be software specific too , so they can change if you 're bouncing between different software packages all the time . You can do this with other versions of @ @ @ @ @ @ @ @ @ @ 4 is the OLEDs which have been removed on the 5 ( I 'm guessing as a cost savings measure ) . <p> Excellent ! Thank you all for the information and insight . I ended up grabbing a Wacom and am loving it . I 've spent most of my time editing with it so far , but hopefully I can sit down toward the end of this week and spend some time grading with it . 
@@44332585 @4332585/ <p> No offense , but if find those blur and fog effects play themselves a bit to much into the foreground , especially since they do n't contribute any dramatic meaning . Looks like they are just there because you can . <p> It 's the cinematographer that 's responsible for those elements , not David . But I do agree they are a little overdone in this video . It works in a few of the shots , but when they appear so often and in successive shots , it just starts to look forced . <p> If you look at the grading breakdown the raw footage already have the blurs and fogs . I agree with Juan , Bruno is the master for this kind of style , I learn a lot about mood &amp; tone from his photography book . And Bruno never retouch or doing digital manipulation for his photos . <p> I think you did a great job grading a kind of poorly shot piece , I think they were trying to copy a fashion film from a few years back that ( I @ @ @ @ @ @ @ @ @ @ through stuff like glasses and bottles . In that film the objects and nets in front of the lens were all in constant motion and it was shot by a really great cinematographer on 35mm . I just get the feel from this that they stuck one split field in one spot and thought that was enough but it ends up more distracting then interesting . I think the grade breakdown is a great demo to show the work you did to improve it . <p> Thank you very much guys ! I can now see that I 've manage to post this in the Jobs page , I 'm very sorry for that . <p> I think you have some great thoughts about this film and yes , Bruno is amazing with these kind of obstruction stuff but I also went really amazed by Benjamins work on this project . I think some of the footage in the film are exceptional . 
@@44332586 @4332586/ <h> DaVinci Resolve Mini Panels - Review <p> I 'm sure you 've all heard of them and many of you have them , but I thought it would be worthwhile to do a quick overview of the DaVinci Resolve Mini Panels from the perspective of someone who has been using the grand daddy panels for some time . <p> It has been several years since the purchase of DaVinci by Blackmagic , and with it the introduction or re-introduction of what is now called the Advanced Panel . Those large panels contend with the Baselight Blackboard 2 and the Nucoda Precision panels as the flag ship panels for their respective platforms . They 're meant to be hero panels in a hero room . <p> The Mini is marketed at the mixed purpose room . The editor/colorist , who needs a panel a few times a week , or part of the time every day . I would suggest that the model of LONG ... ( hereafter referred to as finish artist ) is a role that the vast majority of colorist now find themselves . <p> The @ @ @ @ @ @ @ @ @ @ and clean design , sparse packaging . The box houses nothing more than the panels , a USB-A to USB-C cable and a welcome note . But what else do you need really ? Power cord is not included , but one of the best features of the panel is it 's ability to be powered over ethernet , allowing a single cable to the device to send both data and power , an almost limitless distance from the computer . Firmware updates , as of now , do require a USB connection however . <p> The build is tight , modern , and undeniably solid . It feels as if it 's a single piece of machined hardware . It has a bit of a retro asthetic , somewhat in line with the 80 's nostalgia styling that is in fashion for some consumer electronics . But the design itself is able to take into consideration the many changes in functionality to the Resolve software . The buttons have a feel not unlike that of the chiclet keys on the new MacBook Pro . A bit " chewy " @ @ @ @ @ @ @ @ @ @ than those of it 's big brother . <p> In comparison to the Advanced Panels , the Mini has dedicated knobs for the Midtone Detail , Color Boost , Contrast , Pivot , Highlights and Shadows . All invaluable functions added since the introduction of the Advanced Panels . Along with controls for Y-only Lift , Gamma and Gain , Saturation and Hue . <p> Like it 's little brother the micro it has a set of toggles above the trackballs ( log , offset , viewer ) . The log key allows quick toggle between Primary(LGG) controls and Log ( Shadow , Mid , High ) controls . Offset turns the far right ball and wheel into an offset balance and master control . Where the center and left wheel become temperature and tint . If you 're properly flagging the colorspace for your sources and timeline this is a mathematically accurate control , and a great way to quickly balance and correct shots . And a viewer key allows a quick jump to full screen mode , a concession to the fact that many people use resolve directly @ @ @ @ @ @ @ @ @ @ of the panels are quick jump buttons for particular controls : raw , primary , motion , curves , qualifier , window , tracker , blur , keyer , sizing ... and the as yet unassigned FX and user buttons ( one can imagine what will go here ) . <p> The raw button will only function when the source material is a raw codec , but the utility here is easy to imagine . Primary is handy for jumping back from somewhere , and if you 're on the edit page and hit the primary key it will jump you to the color page . Motion allows controls for noise reduction functions . Curves ... well you get the idea . <p> This is a good moment to interject something very important , the ball and wheels have an excellent feel , as precise and smooth as those of the larger panels . But the rotary digitizers for the soft controls and dedicated controls are the smoothest and most precise I have ever used . Let me reiterate , I 've tried - in some capacity - all of @ @ @ @ @ @ @ @ @ @ digitizers on the DaVinci Resolve Mini Panels are the best I 've ever encountered . They 're smooth and precise with an impressive degree of finesse , this makes the adjustment of custom curves ( for example ) an absolute delight . On the larger panels one can shift into a curve adjustment mode on the fourth trackball , this is quite handy , but modifies both X and Y position of the mark on the curve . The digitizers on the other hand are locked on the Y axis and thus allow a degree of exactitude that feels delightful . I 'm not a huge curves fan , but I really enjoy using curves on the mini . Additionally once you establish a point on the panels you can use the mouse to move it along the Y axis , and then go back to digitizer for X control , great for very fine work . <p> There is a a pair of page shifting buttons along the top of the left side , that allows you to view and modify various functions with the digitizers . I wo @ @ @ @ @ @ @ @ @ @ , but most controls are there . On the right side are node controls . <p> One particularly handy one is the append node button , which - wherever you might be in the node chain - will add a node after the last node in the chain . Perfect for that last tweak in the review session . Underneath are some memory/still/copy/paste controls and transport controls under that . <p> Everything you need to move and get around is there , minus a few things : storing and applying from memories is missing . There is no way to adjust the position of a still/reference wipe . And the transport controls are limited to play forward , play back and stop . Ideally you 'd have a fast forward , rewind button set as well . On the big panels my most used buttons are the memory stores and the double speed forward keys , and the dedicated save key .. which is also missing . <p> Over all these panels are truly a pleasure to use . Curves and digitizers are the best on the market , IMO @ @ @ @ @ @ @ @ @ @ in the ball park of the Tangent Elements , which these vastly outshine in almost every way except for interoperability , as the Mini only functions with DaVinci Resolve . The memory store situation can be addressed with the addition of a X-Keys console or Logitech G13 ... though the software only allows assignment of the first 8 memories and not the full 26 the Advanced Panels allow . One imagines that Blackmagic could release a side-panel to accompany the Mini with dedicated memory functions , extended soft menus and a t-bar for wipe controls , that could live along side it . A way to maximize the mini , if you will . Panels are a rather subjective item , so it 's worth trying it out for yourself if you can , but if the experience of someone who 's used panels for years , hours on end is anything to go by , these get a very high recommend . For use in Resolve , I do n't think anything comes close to the value these offer . <p> Great wrote up . The mini is tempting @ @ @ @ @ @ @ @ @ @ . <p> I actually got my advanced panels a few weeks before the mini was announced . If the opportunity to buy the advanced gently used had n't come up , the mini would be happily sitting on me desk , with enough room to spare for the new Fairlight desktop audio surface . <p> Maybe it 's just mine , but I do find some of the rotaries not as smooth as I 'd like . <p> One of the main reasons I traded my Tangents plus cash for the advanced Panels was the way the reference still was activated in the Tangent . So many button presses to get the wipe up , adjusted , then back to the sub page of what I wanted to tweak . <p> BMD should have a rotary as wipe adjust , but at least there is a dedicated play still button . <p> One area where the advanced excells is a feature you mentioned in custom curves . The track ball allows allowing the point to be moved in all 4 directions , to me is Killer . I tend to @ @ @ @ @ @ @ @ @ @ to benefit from a control point close to black , to give good blacks but just raise slightly above it for good shadow detail . <p> If the mini had these two features I 'd be tempted to get it . <p> However I ca n't wait to try the improved jog shuttle with 14 on the advanced . <p> One area where the advanced excells is a feature you mentioned in custom curves . The track ball allows allowing the point to be moved in all 4 directions , to me is Killer . I tend to use curves a lot , and often problematic footage seems to benefit from a control point close to black , to give good blacks but just raise slightly above it for good shadow detail . 43971 @qwx453971 <p> In this sort of use , I think the mini 's digitizers are actually better . One can set the 0% control point at 0 and the activate the next control point , move it to that point above the 0 that works for you . And then use the digitizer to dial in @ @ @ @ @ @ @ @ @ @ trackable moves around too much on both axis . I like having control over just one axis at a time for precise curves . <p> And yes it 's also possible to control the custom curve points on the Advanced Panel with the soft knobs . But it 's not nearly as precise , so much less enjoyable . <p> You 'll be happy to know how ever that 14 does add an effects function to the Advanced Panel . Along with updated UI elements . <p> THere 's a few things that are kinda busted for me on the Mini Panel . Just as an example : occasionally , the Play button just does n't work . It might 2-3 hits before the system says , " oh ! You need to play back now ? " And then does so . I du n no why . Tangent Elements and the regular Mac keyboard spacebar are flawless and never fail . Go figure . <p> I have to say , they fixed a whole bunch of Mini Panel problems in Resolve v14 , but I 'm still perplexed @ @ @ @ @ @ @ @ @ @ The Curve controls are brilliant . <p> - The network connector The network cable is connected to the back of the panel and has a green light that blinks . The problem is that this light is reflected in the grading monitor . Solutions ... some black tape covering the light . <p> - Rotary knobs above wheels It 's hard to read the labels for the rotary knobs since they are not backlit . After a while muscle memory kicks in , but the knob you do n't use that often are hard to find in the dark . <p> Whishlist - play 2x , 3x etc ( as mentioned above ) - use the " stop " button as " shift " so you can have secondary functions on the buttons . ( I miss go to first / last frame in current clip ) - some kind of GUI feedback for the rotary knobs when you are not in the correct page of the GUI ( like what is the value of Color Boost ( on panel 2 ) while being on panel 1 , which shows @ @ @ @ @ @ @ @ @ @ copy grades using the panel ( like middle mouse button ) <p> What if you could hold ' play still ' and use left ring to wipe still and middle ring to adjust angle ? 43971 @qwx453971 <p> I would suggest keeping the ring the same and changing the button . Less hand travel or better yet use the trackball and move it left or right ( on the horizontal axis to rotate wipe angle ) . Ring to move the wipe all while holding down the stop button . <p> Do folks use the Y-only controls often ? I was surprised that they are some of the few knobs on the micro . 43971 @qwx453971 <p> Depending on the type of work , I 'm using it somewhere between every shot to 20% of shots . Though sometimes I go about it the long way , by putting a node in LAB space and then disabling channels 2 and 3 . <p> A bit " chewy " for lack of a better word , and less mechanical than those of it 's big brother . 43971 @qwx453971 <p> That @ @ @ @ @ @ @ @ @ @ , it did remind me of the pocket calculator keys a bit . <p> But a bigger miss in my opinion , unless it is there and I was n't able to see it , are static and dynamic keyframe keys . I noticed a next and previous keyframe , which it almost seems like an overlook to include these instead of the " set keyframe " which are probably used more . <p> I 've heard bad things about the build quality of the advanced panel , specifically the dials . In which they do n't age well and supposedly , a couple years of regular use and the material will start flaking/tearing . <p> But a bigger miss in my opinion , unless it is there and I was n't able to see it , are static and dynamic keyframe keys . I noticed a next and previous keyframe , which it almost seems like an overlook to include these instead of the " set keyframe " which are probably used more . 43971 @qwx453971 <p> Yeah , Start Dynamic and End Dynamic are not there . I @ @ @ @ @ @ @ @ @ @ to work around things like this . They did add first clip frame / last clip frame to Resolve 14 , but there has been no update for 12.5.5 yet . It 's possible to work fairly quickly on the Mini Panel , but there are pros and cons like everything else . <p> I 've heard bad things about the build quality of the advanced panel , specifically the dials . In which they do n't age well and supposedly , a couple years of regular use and the material will start flaking/tearing. 43971 @qwx453971 <p> I 've used 3 sets of advanced panels on and off for 5 years or so . So I have some familiarity with how the age . The dials and the material they 're made from has not been a problem at all . The build quality of the advanced panels is excellent . Where they have issues seems to be in the reliability of specific components . The built in keyboard is a common point of failure . They can also be a bit finicky in between power cycles , but nothing @ @ @ @ @ @ @ @ @ @ finish material , it seems to be very similar , from feel and look . But ca n't speak to a definitive answer on that . The mechanics of it are different however . 
@@44332588 @4332588/ <h> Grading on DCI projector of a local cinema <p> Because I 've been getting more requests for grades that are destined for the cinema ( commercials , short films ) , I 'm thinking about expanding the way I work . My workflow now is grading on my macpro in DaVinci in rec709 , previewing it through my Blackmagic Decklink Mini Monitor card on my Flanders Scientific monitor . But because I know some people at a local cinema ( I create DCPs for them ) , I 'm thinking about renting their smallest cinema in their non-peak hours and doing the final grading session directly in P3 on their 2K projector . <p> My question is , is this a good idea , what are the caveats and what hardware would I need to invest in , as I 'm pretty sure my Blackmagic card is n't good enough ( 1080p only ) . <p> When the digital switchover first happened , I found most projectors were pretty well calibrated and ' in the ballpark ' , now ... Not even close . So just grading on @ @ @ @ @ @ @ @ @ @ p3 . I just sold my old projector to a cineplex , I think it 's on Long Island , at least that one must be pretty close . <p> ... My workflow now is grading on my macpro in DaVinci in rec709 , previewing it through my Blackmagic Decklink Mini Monitor card on my Flanders Scientific monitor . But because I know some people at a local cinema ( I create DCPs for them ) , I 'm thinking about renting their smallest cinema in their non-peak hours and doing the final grading session directly in P3 on their 2K projector . ... 43971 @qwx453971 <p> Which do you want : a BT.709 DCP or a P3 DCP ? With your present method you are choosing colors for the BT.709 gamut . You can have those very same colors in the DCP . DCPs are not encoded with P3 primaries but with neutral X'Y'Z ' . DCP projectors are P3 capable . You can forgo the extra colors offered by the P3 gamut by making the DCP without any gamut mapping , and you can have the D65 white @ @ @ @ @ @ @ @ @ @ does exactly that . <p> On the other hand , if you want to make movies that take advantage of the larger P3 gamut I think you should take pains to see what you 're making . You can check your grading on a DCP projector only after you 've made the DCP , but how can you perform the grading that way ? P3 gamut capable monitors are uncommon . I just learned that the excellent Flanders Scientific DM250 monitor misses the P3 gamut . Apparently OLED technology forced them to a green primary less yellowish than P3 wants . <p> Concerning Juan 's concern about out-of-calibration projectors . You can mount your monitor colorimeter directed toward the screen and check the projector calibration yourself . I made a short DCP for testing my friendly local theatre 's . Test results . All was well except the primary blue : way off ! I still do n't know if it was their projector or my meter . Notice that the test covered the BT.709 gamut , not P3 , but they have the same blue primary . <p> Dennis @ @ @ @ @ @ @ @ @ @ sdi signal into the projector . Though this does bring up a side note , some projector models for cinemas do n't have HD-sdi inputs . <p> Woeter , having setup a DCI theater , I do n't think I would ever want to grade in a commercial cinema . But if you 're grading and exhibiting there , and you 're producing the DCP , well you could n't ask for a more closed system . You 're pretty much your own standard there . If you want that content to hold up elsewhere , then you need to verify every piece in the chain . <p> As to what you need that depends on the particulars of the projector and its installation . <p> Juan , if you were dubious about the projector 's calibration for DCP , should n't you be the moreso about its second calibration for this SDI video feed ? In any case Woeter will have to know exactly what it did in terms of gamma and color so he can make a DCP that projects the same . <p> DCP projectors are required @ @ @ @ @ @ @ @ @ @ this by honest bit-depth . I included a banding test with color test at my friendly local theatre and I believe I observed the projector dithering . Will the DCP projector do all its DCP'ish things with an SDI video feed ? <p> I 'll have to check with them which projector they have , but I 'm pretty sure I saw SDI inputs on the side last time I was in the projection booth . And I 've heard them talk about also being able to playback blu-ray , hdcam sr and even digibeta . They ingest the DCP 's I give them on a central server , so I do n't go into the projection booths that often . Could take a while because they are very busy at the moment , preparing for a film festival . This cinema does n't focus on big blockbusters , but rather the arthouse films . <p> The whole point is being able to show the director and DOP how their movie can look on the big screen in the cinema instead of my relative small grading monitor in my grading @ @ @ @ @ @ @ @ @ @ P3 gamut . For now it was good enough to just make a DCP and hope for the best in the cinema , which fortunately has n't been much of a problem . <p> Most grading jobs I do are still for broadcast , that 's why my Decklink mini monitor ( limited to 1080p in rec709 ) was enough for the time being . But when I look for a card to replace the mini monitor , for instance the Decklink 4K extreme , it can output 2K at 24 in 12 bit RGB , is this what I 'm looking for in a card ? Also the specs say this card operates in rec709 , not p3 . Is the specsheet wrong or does it have to be something else than a Decklink to be able to do this from resolve ? <p> Juan , if you were dubious about the projector 's calibration for DCP , should n't you be the moreso about its second calibration for this SDI video feed ? In any case Woeter will have to know exactly what it did in terms of @ @ @ @ @ @ @ @ @ @ projects the same . 43971 @qwx453971 <p> Most dci projectors have a calibration table that can be applied to any input , they also allow defining everything . These are all very practical concerns we have solutions for , but yes you do need to be aware of these things . With regards to bit depth , there is no requirement we feed XYZ in , so RGB-P3 ( correctly configure of course ) is much easier on quantization , a 10-bit pipe is more than capable , ( and there are solutions for even higher bit depth and with it XYZ support ) . Again , 10-bit is standard for SDI and almost universally supported on those devices . It 's almost as if we use DCI projectors in professional grading environments all that time , Dennis . <p> Thanks guys ! Pepijn has sent me a message in Dutch , explaining a bit about what the projector would need to be able to process if the SDI signal comes from a blackmagic card ( RGB instead of XYZ ) , so I think I have my answer . @ @ @ @ @ @ @ @ @ @ and see what 's possible . And if it should be possible , then we 'll plan a test where I maybe also could bring my Flanders to compare ' what I know is right ' to what the projector is doing . Could take a while , but I 'll let you know what happened . 
@@44332589 @4332589/ <p> I moved my resolve disk database to live in dropbox folder . It 's not good for any sort of project sharing , or trying to sync 2 computers that are currently working but it works as an automatic cloud b/u solution . There was a tutorial on it on mixing light a while ago , since I moved to it it 's saved my but a couple times . Stills &amp; Powergrades do n't come through though ... <p> I moved my resolve disk database to live in dropbox folder . It 's not good for any sort of project sharing , or trying to sync 2 computers that are currently working but it works as an automatic cloud b/u solution . There was a tutorial on it on mixing light a while ago , since I moved to it it 's saved my but a couple times . Stills &amp; Powergrades do n't come through though ... 43971 @qwx453971 <p> When you say you moved your resolve disk database to live in the dropbox folder I 'm assuming you mean that you generated a symlink of @ @ @ @ @ @ @ @ @ @ dropbox ? <p> As far as I can see the tutorial on mixing light is only in regards to Postgre Databases not Disk Databases , and involves running a cron job to backup the Postgre database , which is not what i 'm looking to do . <p> No , when you create a new disk database , you can choose the directory . I just happened to choose one that lives in my Dropbox . <p> Added advantage is that you can use dropbox 's versioning as an emergency rollback feature . Maybe it was n't on mixing light , but I was sure was , as I remember Patrick Inhofer showing the how-to on a video . <p> No , when you create a new disk database , you can choose the directory . I just happened to choose one that lives in my Dropbox . <p> Added advantage is that you can use dropbox 's versioning as an emergency rollback feature . Maybe it was n't on mixing light , but I was sure was , as I remember Patrick Inhofer showing the how-to on a video @ @ @ @ @ @ @ @ @ @ n't realize that you can choose the location of the Disk Database I thought it was always in the /Library folder . Nice tip ! Do you find that it congests your network constantly syncing all the hundreds / thousands of tiny files in the Disk Database ? Any problems you have experienced with having the Database in your dropbox folder ? <p> It is syncing all those little files , but I have n't noticed any network clog . Those files are tiny . However , if you have the same dropbox on multiple computers , with DaVinci , DO NOT have them both open the same Disk Database . Have separate dropbox folders with a DiskDB on each comp. 
@@44332590 @4332590/ <h> Freelance rates and time estimates <p> Just wanted to get an idea of how you determine how much time it will take to color grade something . And how you decide what to charge . <p> I have quite a few friends seeking me out for color who have a budget to pay me ( which is fantastic ) but since I 've just started doing some side freelance work I 'm still figuring out how long it will take me and , thus , how much to charge . <p> I 've hear 8 shots/hr is a good speed . I know this all depends on how much work has to be done , in terms of color , but is there a good starting point to gauge time spent ? I 've seen colorist charge anywhere from $80/hour to up in the hundreds/hour . <p> A producer friend of mine who knows my skill level said I should charge about $50/hr since I 'm still building my skills and client base and from what I can see it wo n't need anything crazy ( mostly just @ @ @ @ @ @ @ @ @ @ ) . <p> If it helps the most recent project I 'd be working on is a 12-13 minute short . <p> i 'm also curious about peoples opinion on this mattr , although I know this is a very inidividual choice based on experience , equipment , and content being graded . Just curious for a ballpark that people follow : Maybe one for short films , one for music video ( or other high end color work ) , and one for long form film ? <p> As far as time , somewhere around 5min 's of content/hr ( broadcast doc/reality ) ... this is , of course , is subject to the whole gamut of variables associated with every production ... but it 's a good baseline metric ... for me at any rate . <p> If it 's longform work , I usually estimate 500-600 shots per 8-hour shift if the material is reasonable . Less than half that if it 's ugly and difficult . It 's very typical to do a one-hour dramatic network series episode ( 43 minutes ) in 2 days . Reality @ @ @ @ @ @ @ @ @ @ . <p> I have done entire features in one day , and also entire features in 3 months . One day per reel ( 20 minutes ) is pretty typical with a director and a DP in the room . If they decided to get very picky and make " every frame a Rembrandt , " this could double or triple or quadruple . <p> Some people will bid per project , some will do it by the facility hour , and rates also differ depending on whether you 're furnishing the equipment or whether you 're using an existing facility 's room . There are many variables . <p> I know of colorists in LA who make $10 an hour , and there are several who make well over $10,000 a week . ( And at least 1 who makes $50,000 a week . ) I think $2000/week as a starting rate was pretty widespread a few years ago , but things change . <p> Last I checked , I think IA Local 700 has $80/hour for the union colorist rate ( $3200/week ) . For somebody starting off @ @ @ @ @ @ @ @ @ @ bear . " <p> Tell us more about that 10k - 50k bracket . What makes them stand out in such a way that they can charge that - skill ( obviously ) , expensive gear , good connections with plenty of $$$ ... ? <p> It would seem to me that guys like Nakamura and Sonnenfeld would be a couple of the pretty well-paid guys . 43971 @qwx453971 <p> None of the above , actually matter but would be very helpful in achieving the one thing that does matter . If your going to work/change post house after being freelance/owner operator/employee the only thing that makes you worth the time and effort is your client list . This is your value and how your able to know how much you can charge . If you have no clients and this is your first job I would do it for free as an introduction . They return to you congrats you have your first client , but not all clients are equal . 
@@44332592 @4332592/ <h> Cloud offsite backup <p> What if any are you using for offsite cloud backup of your files ? There seems to be a number of services around and it is very hard to know which is the best option . Does anyone have an opinion or suggestions ? <p> So far I can see the following as possible contenders but would really like some real world user thoughts . <p> I second the use of S3 . For one , it is the actual cloud technology most of the other services use anyway , they just layer convenience on top and then charge for it . Also S3 has a much simpler metered rate card instead of the ever slightly off tiered plans . You pay per GB/month stored and you pay per download network traffic , but not for upload network traffic ( which is good for backups ) . <p> Also , given the size of particularly video files it 's good to have a strategy of what files are backed up locally and what gets backed up in the cloud . It impacts both cost @ @ @ @ @ @ @ @ @ @ tools either do way too much or are not transparent enough . <p> Many of today 's backup applications do have cloud integrations that can use S3 buckets . I 've been using EMC Retrospect which is an enterprise backup software which has small and personal plans . It allows you to define backup policies and automatically schedules jobs . EMC also has more powerful tools to manage incremental backups , making it more transparent in terms of what it 's doing and what you actually have backed up . <p> Generally speaking any backup strategy needs to be thought out in terms of what failure modes you protect against ( hardware failure , system failure , building failure ) as well as your data classes ( undelivered client data , delivered client data , long-term asset library , working files , raw files , etc . ) <p> I use a combination of local and cloud backup . The local backup ( NAS and simple external drives ) backups all long-term data , many terrabytes of it . A subset of this copy quarterly on some external drives that @ @ @ @ @ @ @ @ @ @ . I do backup every undelivered client job via S3 as soon as it comes in the door to protected it against all failure modes . I also backup master renders and key asset library assets to the cloud on an ongoing basis to cover the gap between offsite rotations . And I have a stack of naked drives and SATA adapter that I copy my working folders to after I finish a job , separately from archiving a master render with an intermediate codec , which may be different then what the client asked for . My total cloud storage ranges between 1-2TB and costs me about $20/month . <p> The advantage of having an S3 account is that it can be a single destination for multiple storage options . I use it for backup . I also use it to backup my servers at my ISP that run my websites and a few client websites . And finally I use it as my cloud drive for file delivery to clients . For a while I used Dropbox and Hightail , but always got frustrated by pricing and limitations @ @ @ @ @ @ @ @ @ @ my S3 bucket , create a signed download URL that expires , wrap it with bit.ly for convenience and tracking and send that to the client . There are no size limits , whether you upload a 20MB image or 200GB timeline render . Many ftp clients have S3 integrations , after some playing around I prefer Cyberduck as it generates the signed links right after upload . <p> And with any backup plan , you always have to exercise the restore from time to time , just like a tornado drill . So many backups that exist and are worthless because nobody bothered to test and verify until it 's too late . <p> In full disclosure - I am somewhat partial to Amazon , having worked there for many years and knowing how their data centers are run . And before then I worked at HP in Enterprise data storage and backup hardware like LTO . <p> I can vouch for Backblaze . I 've used them in the past for a couple years and might return to them unless I find something better . Their software is @ @ @ @ @ @ @ @ @ @ experience overall . 
@@44332593 @4332593/ <h> Alter Ego Post / Toronto : color assistant <p> DESCRIPTION Alter Ego is looking for a full-time Colour Assistant to join our team . As a Colour Assistant you will gain valuable hands on knowledge of the commercial industry at Canada 's largest commercial colour and visual effects studio . The Colour Assistant 's primary responsibility is to manage all ancillary aspects of the current and upcoming sessions , enabling the artist to focus solely on the operation of the Colour Suite . This includes preparing the suite for client attended sessions , coordinating meals and ensuring the suite is always presentable . <p> REQUIREMENTS 1-3 years of experience working in a post-production environment . Knowledge of Photoshop and various editing software . Understanding of various file and camera formats i.e. , R3D , Alexa , Sony , Canon &amp; Phantom . Understanding of Baselight would be considered an asset . <p> ROLES Working knowledge and understanding of the post production workflow . Detail oriented and highly organized . You are responsible for the accuracy , quality and technical standards of all deliverables . Have the ability to @ @ @ @ @ @ @ @ @ @ to anticipate the needs of the Colourist . Strong ability to multi-task and meet deadlines while working under tight schedules . Exceptional interpersonal and communication skills ; excellent email and phone etiquette . Willingness to take direction and to collaborate with others . Enjoy working in a team-based environment . Passionate about the work and willingness to learn . 
@@44332594 @4332594/ <h> Gallery Stills are coming up as black instead of actual still frame <p> An issue I am having is when I try and use a gallery still to compare shots . Instead of getting one half of the picture showing the grade and the other half showing the still , I am getting a black picture showing for the still and it is another problem I am just unable to find a fix for . <p> Turns out that the original media is gone and although I can still access the nodes and add the grade , it will not display it in a wipe at all . So I add the grade from the still to the new media and then re save a new still . <p> I already had the same problem as you had , and it happened because I had set the Scratch Disk of DaVinci Resolve to an external hard drive , which was not available anymore when the problem happened . <p> The gallery stills you take are stored at a location defined on General options tab of the project settings @ @ @ @ @ @ @ @ @ @ this option , then the location of the " Working Folders " is gon na be defined by preferences pane in Resolve ( DaVinci Resolve &gt; Preferences ... ) . <p> So , if the directory where Gallery Stills are stored is not available ( ex . A disconnected hard drive ) , then the picture part of the still wo n't be stored ( it will be black ) , but the information of the grade of the still will be stored with the project file . That 's why it were available to you . <p> See these pictures , it may help you finding your way to solve the problem if it happens again . <p> You are correct . My RAID was destroyed and is no longer available . So I had to create a new RAID and start all over again . Then I imported all the footage into the media pool again . <p> The original stills were actually saved onto an SSD drive which I use for OS and applications . So the stills were some how kept with the node information in @ @ @ @ @ @ @ @ @ @ new project and found if I right click/ add correction , it will add the grade to the new node . But like you said , the still came up as a black image when I tried to compare with a wipe . Solution was to grade from the reference media which I had uploaded to vimeo and then save a new still once I was happy with the new grade . <p> This seemed to work fine until I noticed there were some funky shifts in color happening.There are a couple of obvious reasons but I can not put my finger on any one specific thing and have decided to start from the beginning all over again . <p> Example I changed a few settings in the Camera Raw for Color Space , Color Science and Gamma Curve for my Red footage . I was made aware of the different looks and decided to try it out . So obviously , this changed every piece of media and gave me more space to work with . At the same time , it also affected every still I had saved @ @ @ @ @ @ @ @ @ @ , it was sometimes totally different lol <p> So I went in and began grading everything and just adjusting things according to the new adjustments I had introduced . I accidentally did a save as and added the date to the project which basically saved it as a new copy . I logged out and came back later and logged into the latest copy with the latest date and for some strange reason , every shot was now blown out with the entire color being shifted into the highlights ! ! ! ! ! ! ! ! I am unsure why this happened and I did n't change any settings . <p> Today I re opened the original project to find all the grades are perfect and I may not need to do this all over again . No doubt , these are simple issues that I am learning how to deal with over time . <p> I 'm so grateful for the help LGG has been through either searching for key words to issues I have had or posting and getting feed back from guys like you Elieser . @ @ @ @ @ @ @ @ @ @ I had the same issue you describe . Previously I had my Cache and Gallery location set to my 3x RAID 0 HDD set . I struggled to export stills and every time I rebooted DaVinci the stills would disappear and leave me black thumbnails , the same happened for the Gallery itself . I then moved it to my single SSD boot drive , seems to work fine now . Hope that helps . Seems certain RAIDs it does n't like . 
@@44332595 @4332595/ <p> The Embassy is an award-winning studio with expertise in vfx , animation &amp; design for advertising , film and TV . <p> We are currently seeking a very talented artist to help lead projects in our Vancouver vfx studio . This is an exciting new opportunity for those wishing to work with the industry 's best directors , on some of the highest profile projects in the industry . <p> You are a great all-rounder with the ability to lead teams but also complete smaller less complex projects on your own . The duties for this position are equally split between compositing and finishing ( editorial ) , candidates should be proficient in both areas . 
@@44332596 @4332596/ <p> Four Walls are looking for an experienced Baselight Colourist to take the lead role in its Colour Department . The successful candidate will have a proven track record in grading for commercials , pop promos , online content , corporate and on-air promotional material . Job Title . Lead Colourist Job Description . The Colourist will be responsible for conforming and colour grading a variety of different projects from diverse source material using Baselight . They will have the ability to take a project from start to finish , whether in attended or unattended grading sessions . The successful candidate will be an outgoing and proactive individual and with the help of our marketing , production and PR team will pursue introductions to new clients , promoting the Colour Department and company as a whole . Skills required . <p> A thorough knowledge of Baselight grading systems . <p> Extensive experience working with Baselight within an attended commercial environment . <p> A full and in-depth understanding of colour science , additive colouring , subtractive colouring and structural colour . <p> Knowledge of the psychological effect of colours and @ @ @ @ @ @ @ @ @ @ <p> In-depth knowledge of cameras and cinematography ( including camera codecs and the use of LUTs ) along with a thorough understanding of Film and Digital production techniques . <p> A thorough understanding of the post-production process including an understanding of VFX techniques . <p> Thorough understanding of conform process , including the use of both EDLs and AAFs . <p> Good initiative and problem solving skills . <p> Excellent communication skills . <p> Highly organised and able to work effectively under pressure and to strict deadlines . Location . The position is based in our London Office in Wardour Street . Salary . The salary is within the range of -75K--100K pa dependant on experience . Closing date for applications . The closing date for applications is 13th August 2016 <p> Structural color from what I understand is color that is generated by some form ( mainly microscopic ) that affects the light in a certain way to produce its color . Good examples of structural color are things like the iridescence of peacock feathers or butterfly wings . 
@@44332597 @4332597/ <h> Banding in Blu-Ray Compression <p> So I have a film with a very dark , exterior night scene . It rendered out of Resolve nicely , but when the filmmaker had a Blu-Ray created there is noticeable banding in the blacks . Does anyone have a suggestion on how to avoid that ? <p> Thanks for the quick reply . What do you mean by setting the perimeter properly ? For what it 's worth , this Blu was authored out of Compressor . Could that be the culprit ? 43971 @qwx453971 <p> A lot of factors are involved in the final Bluray encode , like source material quality . Compressor is not advanced enough for best quality encode IMO . Professional bluray encoder has far more picture tuning settings to make use of every bit of datarate . <p> Apart from the adding grain method , you can also have a look at the compressor settings , make sure everything is set properly , max out bitrate , 2-pass or multipass etc . <p> For what it 's worth , this Blu was authored out of Compressor @ @ @ @ @ @ @ @ @ @ Yes . They need to use a real encoder . Compressor is garbage . As is Sonic/Rovi 's encoder ( based on MainConcept ) . I do n't think Sony 's is much better , and we had a ton of problems with the CinemaCraft encoder as well when we tested it out . Wound up rolling our own and have authored many commercial discs with it . No banding , no artifacts , no issues in replication . <p> High bit rate helps , but banding can happen even at the highest rates . It 's possible to get top notch encodes from reasonably clean source footage at rates in the 15-18Mbps range with a good AVC encoder , less than half the max bit rate for BD . Proper dithering of 10bit source ( such as ProRes ) , and the ability to add in a little imperceptible noise in footage that 's super smooth ( such as computer animation ) are key . <p> If I remember correctly , 40-45Mbps is the maximum/optimum bitrate for BluRay Disk . Sony 's ' Mastered in 4K ' commercial BDs @ @ @ @ @ @ @ @ @ @ encode my Resolve DPX Image Sequence Masters to BD . Convert has an optimized BD algorithm and I 've never experienced any banding even though the resulting file is 8bit 4:2:0 . Banding is usually associated with heavy compression , poor encoding practices or both . <p> The maximum bit rate for Blu-ray is 40Mbps ( but can momentarily spike a fair bit above that ) . That includes audio and other streams . In the real world , most professional encodes are in the 20-30 range , because uncompressed or DTS-HD MA streams take up a fair bit of space . You also have to account for spikes in the bit rate due to image ( and audio ) complexity , so encoding at that high a rate is asking for trouble . It 's also unnecessary with a good encoder . Frame rate also matters - you get 20% more bits per frame with 24fps material compared to 29.97 because there are that many fewer frames to encode for the target bit rate , for example . Higher frame rate footage tends to need higher bit rates to @ @ @ @ @ @ @ @ @ @ but more often it 's from lack of dithering when converting from 10 bit to 8 bit . It 's rarely a problem with film-originated material because film grain is random enough that it prevents a lot of banding from happening . Once you start getting into really aggressive compression ( say , 8-10Mbps for 1080p material ) , you 'll see compression artifacts that can look like banding . <p> The maximum bit rate for Blu-ray is 40Mbps ( but can momentarily spike a fair bit above that ) . That includes audio and other streams . In the real world , most professional encodes are in the 20-30 range , because uncompressed or DTS-HD MA streams take up a fair bit of space . You also have to account for spikes in the bit rate due to image ( and audio ) complexity , so encoding at that high a rate is asking for trouble . It 's also unnecessary with a good encoder . Frame rate also matters - you get 20% more bits per frame with 24fps material compared to 29.97 because there are that many @ @ @ @ @ @ @ @ @ @ for example . Higher frame rate footage tends to need higher bit rates to compensate . <p> Banding can be a compression artifact , but more often it 's from lack of dithering when converting from 10 bit to 8 bit . It 's rarely a problem with film-originated material because film grain is random enough that it prevents a lot of banding from happening . Once you start getting into really aggressive compression ( say , 8-10Mbps for 1080p material ) , you 'll see compression artifacts that can look like banding . <p> Oh , it can definitely be done ( in AVC ) . Our encoder does an amazing job of encodes in that range . But most encoders , including some of the expensive ones , are terrible below 25mbps . <p> With the right encoder and clean source footage , there 's no reason ( HD , 24p ) Blu-rays could n't be encoded in the 15mbps range as a matter of course . In reality , home theater review sites that publish bit rates in graph form , force the companies that release discs @ @ @ @ @ @ @ @ @ @ getting reviews that the clueless think are bad . Seriously , it 's a thing and it 's driven me crazy for like 15 years . <p> Yes . They need to use a real encoder . Compressor is garbage . As is Sonic/Rovi 's encoder ( based on MainConcept ) . I do n't think Sony 's is much better , and we had a ton of problems with the CinemaCraft encoder as well when we tested it out . Wound up rolling our own and have authored many commercial discs with it . No banding , no artifacts , no issues in replication . <p> The one that finally pushed us to make our own tool for encoding was an issue where excessively grainy film would create a kind of pulsating effect - the grain would go momentarily blurry , then pop back to normal . We tried re-encoding with various deblocking settings tweaked , but could n't completely get rid of that problem . In the end , we built our own software for in-house use based on x264 , which is what is being used these @ @ @ @ @ @ @ @ @ @ . As a test , we re-encoded half a dozen films that had been done on Sonic and Cinemacraft HD encoders , and the results were pretty staggering - our encoder blew them both out of the water and we 've been using it ever since for all blu-ray work we do . I have n't had to do a segment re-encode since 2014 because it 's just right on the first try , every time . <p> Cinemacraft for DVD is still the best out there . It 's a great encoder . But for the money it costs , I do n't think the HD version is worth it . <p> I was just about to say , having a big name encoding tool is not a necessity , x264 and avisynth are freetools that have the capability to produce transparent encodes . It takes a lot of time and effort to master them , but it is worth it in my opinion . Infact many labels do use x264 and publish stunning blurays . Also , When it comes to encoding efficiency is more important than bitrates @ @ @ @ @ @ @ @ @ @ an inefficient high bitrate encode. 
@@44332599 @4332599/ <h> how to achieve these looks in davinci resolve <p> ok. learning from juan melara 's website. i understood a fair amount of how to grade the summer blockbuster way . but i realised it is not so aesthetically pleasing when 99.9% of your cast is black . i tried it and it sucked . i then embarked on more search online and i saw a number of south african clips with black people graded in a different way . i liked the grading ( personal thing ) and i want to emulate such . but i just do n't  know how to achieve this look in davinci resolve . i have therefore posted the clip here . bear in mine the one of the clip has only white in it , the other has just blacks . i want to achieve this look . is there any tutorial out there or can anyone put something quick together on this look and how to achieve it in davinci resolve . <p> nb : i am a film director and i fund my own film , i do n't have @ @ @ @ @ @ @ @ @ @ opinion as a DP is you 're going to achieve 90% of this look on-set with good lighting , creating atmosphere , possibly filters , and in general I would recommend getting a DP who can deliver the look you are after . <p> I would also say that one of the most important parts of the filmmaking process is learning how to raise money for a sufficient budget . Get more money and hire an actual colorist . There are inexpensive people out there , but they do require a reasonable wage . <p> Have to agree with Dennis : the great majority of the look in both of these was likely done in-camera . <p> That said , pay close attention to the depth of field : if it was n't shot that way you can try to fake some of that with blur applied to a power window or windows ( or EXCLUDED from a window ) . As far as the color of your actors go ... skintone is skintone ! If they do n't look right after you 've done a primary correction , try @ @ @ @ @ @ @ @ @ @ an HSL qualifier to narrow it down to just the skintones : you may need both an HSL qualifier AND power windows to do this ) which pulls the colors in the opposite direction of your primary grade : in a sense " un-doing " what the primary grade does . <p> Lastly , keep the blacks neutral ( all three YUV waveforms in your parade more or less line up at the bottom ) and try playing with saturation . <p> Hope it helps ... and seriously : Marc is right . Budget ahead of time for success and you will reap huge gains in better quality work which will in time bring more clients . And some colorists work remotely for very fair rates : the interwebz and services like DropBox make this possible ! <p> I have to say , you can never minimize the importance of good lighting and good lenses . On too many of the projects I get , I 'm basically doing triage trying to salvage terrible photographic problems . When the project is well-lit to begin with , it 's relatively effortless @ @ @ @ @ @ @ @ @ @ and struggling with 11 nodes just trying to stop something from looking horrible . I feel happy when I can take those projects from horrible to mediocre -- and it 's usually a big difference when the client sees it . <p> But expecting world-class results is very hard , since the end result hinges so much on art direction , lighting , lenses , camera exposure , and the skill of the DP. 3/4 of the hard work has already been done before we get the images . <p> Hi Idowu Just to echo everyone else . Having had a quick look at the clips , the lighting has been executed beautifully in these films , lots of diffuse light , just enough fill to ensure the skin does n't get too dark , cool front lights to mimic the TV , soft practicals and possibly a promist on the lens . The most beautiful looking material will always start with a well lit well exposed raw image or neg . A grade can make good material great , great material amazing and bad material passable . <p> Also @ @ @ @ @ @ @ @ @ @ it sound like any discredit to professional colourists - of course they play a very important role ( if not final role ) in creating a films appearance , but like most things garbage in and garbage out , so I just wanted to emphasize that how to image is captured and lit can be very important as a starting point . <p> I 'm one of those DP 's who lights my shadows knowing they can be brought down in post and knowing that it will result in detail being preserved in the shadows . I love that . <p> I 'm one of those DP 's who lights my shadows knowing they can be brought down in post and knowing that it will result in detail being preserved in the shadows . I love that . 43971 @qwx453971 <p> That 's the right way to do it . The more-limited dynamic range of digital cameras kind of demands a more " down the middle " approach than the old days . It really ties one arm behind our backs if the digital image is too crushed or too @ @ @ @ @ @ @ @ @ @ through few days of that , and my brain still hurts from the experience . ) <p> This is a great interview with DP 's Geoff Boyle , Bill Bennet and Rodney Charters and they talk about how no matter how sensitive cameras get that lighting will always play an important role in good cinematography , they also say that with cameras being so sensitive they find they are using negative far more with their productions . ( negative , or black fill incase anyone is not aware what that means is actually removing light from a scene and/or subject by using black fabrcis or black card stocks ) 
@@44332600 @4332600/ <h> DPX files <p> I do n't have too much experience using image sequences and i have a rather basic question . I noticed that the transfer of a 50GB folder filled with DPX files takes quite a lot more time than transferrring a pro res 444 file around that size . I figured this probably has to do with the simple fact that the system is dealing with thousands of little files , and that takes extra time . I was just wondering if there are ways to deal with this , like maybe a dedicated copy tool that speeds up things ? Or just deal with it ? <p> I do n't have too much experience using image sequences and i have a rather basic question . I noticed that the transfer of a 50GB folder filled with DPX files takes quite a lot more time than transferrring a pro res 444 file around that size . I figured this probably has to do with the simple fact that the system is dealing with thousands of little files , and that takes extra time . I was @ @ @ @ @ @ @ @ @ @ , like maybe a dedicated copy tool that speeds up things ? Or just deal with it ? 43971 @qwx453971 <p> Pepijn its a real pain same as moving loads of black magic DNG files around and your absolutely right it 's the all the files <p> i , m not sure if your Mac or PC <p> on a mac i use a variety of rsync which a unix command there is a front end called arsync <p> also i use rapidcopy for the mac which is built on top of rsync that works quite well <p> these seems to work a bit better faster with file copies like this and you can stop it half way restart it things like that <p> i think the PC version is called fastcopy <p> the key thing your after is copy and checksum which i believe all the apps i have listed do <p> Pepijn , if you still have Convert v3 installed on your PC , ' right click ' in the Media Pool where you 'll find a file transfer app with checksum verification . See if it works @ @ @ @ @ @ @ @ @ @ where a bigger stripe size in your RAID array comes in . Something at least over 64K , and even 128K to 256K if possible . Also , with DPX you begin to deal with fragmentation , which can be hard on spinning disk arrays . SSDs handle things much better as they basically have an instantaneous seek time across the entire disk , where spinning disks will really slow down as they get full . <p> One thing some of us have been experimenting with is using a large , standard HD array as source , and then caching to a smaller SSD array for playback . For instance in Nucoda , you can point most of your file paths to your main array , and then your cache location ( S ) to a set of 4 to 8 SSDs . You can keep the bulk of your media on your large array , cache to SSD , and then manage the deletion of cache files once you 're done using the Clean Up options under Library . <p> Until we 've got our 40GbE network fully deployed @ @ @ @ @ @ @ @ @ @ over a gigabit network . There are two advantages to doing this - we have a ready archive we can back up to tape fairly easily , and having a single file makes the file transfer over SMB smoother . But it takes time to make the tar archive , so it 's usually an operation that we before leaving for the day . I 'll often log into the machine from home to see if it 's done , and then initiate the file copy before going to bed . That way it 's where it needs to be in the morning . Of course , I 'm talking about file sets in the 1TB rage , so everything is slower ... <p> Hi Pepijn , it really depends on your OS and how you 're RAIDing . Basically , a high transfer speed , low-latency raid is capable of copying these files pretty much as fast as it can be transferred . If you have a high transfer high latency raid it gets a lot worse quick . Several things help , the biggest of all is enabling @ @ @ @ @ @ @ @ @ @ flushing ( if you 're on windows , that is ) . But if you have a software raid that gets a little risky especially if you do n't have a UPS . <p> Personally I take that chance , using the on-board RAID that 's built into the SuperMicro which has no BBU option and I do n't have a UPS , but all my original media is on a NAS so the chance of data loss is small . Still , it 's not best practice . If I re-enable Windows cache flushing it gets slow again . <p> Lastly , in my experience win 10 is a lot faster with sequences than win 7 , not sure about 8.1 . <p> If you 're on Mac , most of this info is useless , but you 'll still notice a difference if you 're on a hardware raid vs a software raid . Generally read speeds should be pretty high , writes are just slow . I have n't noticed ( much ) slow down copying DPX over LAN vs Quicktime on either platform . 
@@44332601 @4332601/ <h> Back to Baselight Editions after a year away ... <p> For a number of reasons , the rise of Premiere being one of them , I 've been away from BLE for about a year now . In the meantime I see there 's been a number of improvements . <p> A job has come up , that does have BLE licensed . <p> It 's tight turnaround , shot balancing for the most part and I will use the Symphony colour corrector for speed ( relational grading ) and BLE for secondary and power window work . <p> But ... <p> Is there a way now of doing relational grading within BLE ? <p> By that I mean I want to apply the same grade to all instances of that master clip . This saves a lot of time in A/B cam work , especially where pick ups are from different day . It 's more than having scratchpad 1 for camera A and scratchpad 2 for camera B , because of the pickups ( Cam A may have been shot on Day 1 and Day 4 @ @ @ @ @ @ @ @ @ @ broken into parts ( so Shoot Day 1 Cam A for Scene 1 may also pop up 10 minutes later .. ) <p> I am going to have a look at the release notes over the last year to see what 's come up , but I think you lot will have the answer quicker ... <p> For a number of reasons , the rise of Premiere being one of them , I 've been away from BLE for about a year now . In the meantime I see there 's been a number of improvements . <p> A job has come up , that does have BLE licensed . <p> It 's tight turnaround , shot balancing for the most part and I will use the Symphony colour corrector for speed ( relational grading ) and BLE for secondary and power window work . <p> But ... <p> Is there a way now of doing relational grading within BLE ? <p> By that I mean I want to apply the same grade to all instances of that master clip . This saves a lot of time in A/B cam @ @ @ @ @ @ @ @ @ @ . It 's more than having scratchpad 1 for camera A and scratchpad 2 for camera B , because of the pickups ( Cam A may have been shot on Day 1 and Day 4 and shots are intercut ) and because scenes are often broken into parts ( so Shoot Day 1 Cam A for Scene 1 may also pop up 10 minutes later .. ) <p> I am going to have a look at the release notes over the last year to see what 's come up , but I think you lot will have the answer quicker ... <p> Thanks 43971 @qwx453971 <p> Hi Trevor , <p> You wo n't be able to do it in the version of Baselight for Avid you have , but our upcoming release ( beta very soon ) contains much , much stronger copy &amp; paste tools , including the ability to paste to all shots in the timeline , and to all shots with a matching tape name or clip name . <p> DM if you 've not been a member of previous Baselight for Avid betas and want to @ @ @ @ @ @ @ @ @ @ a theory , so I would have to test it and work out the detail , but I suspect there may be a way of using a Baselight Lens to apply the same grade to all instances of the same clip . That way you could tweak an individual instance of the clip , and if you decided you wanted to push that tweak to all the other instances , you would re-save the BLG file for that clip , overwriting the old one . <p> our upcoming release ( beta very soon ) contains much , much stronger copy &amp; paste tools , including the ability to paste to all shots in the timeline , and to all shots with a matching tape name or clip name . <p> This is just a theory , so I would have to test it and work out the detail , but I suspect there may be a way of using a Baselight Lens to apply the same grade to all instances of the same clip . That way you could tweak an individual instance of the clip , and if you @ @ @ @ @ @ @ @ @ @ other instances , you would re-save the BLG file for that clip , overwriting the old one . <p> Sure . I 'm just thinking that ( a ) the lens approach might work now , without waiting for the new beta , and ( b ) is n't what I 'm describing fundamentally what lens is for , except you are pushing new grades from within Editions , instead of from a separate full Baselight ? <p> Sure . I 'm just thinking that ( a ) the lens approach might work now , without waiting for the new beta , and ( b ) is n't what I 'm describing fundamentally what lens is for , except you are pushing new grades from within Editions , instead of from a separate full Baselight ? 43971 @qwx453971 <p> Alas , the BLG Lens in BL for Avid does n't have the ability to match by clip name only , so if any of the other uses of the clip do n't overlap the timecode range of the BLG , it would n't work . <p> By the way , @ @ @ @ @ @ @ @ @ @ way to use it in this context . <p> With a bit more time we could do grades of master clips whilst editorial is underway - then use Lens to view the BLGs created . Pretty elegant solution , could involve Daylight even . But time/money/available equipment/available personnel gets in the way of that . <p> Plus , as Martin states , the Lens is read only ... and everything needs a tweak in context . <p> I think the copy/paste future looks rosy for this . Sadly too late for the current job ( turnaround too tight to risk a beta , as reliable as that beta is likely to be ) . 
@@44332603 @4332603/ <p> Just tried it today on a music vid on 2 " matched " dragons . Did n't fix shit . The only saving grace was that the noise pattern was not of the " fixed " variety so DVO Clarity on the Log footage , then grade , then DVO Grain ( DVNR AGR4 ) and DVO Sharpening downstream saved the shots . I am SO done with these stinking cameras ! We tried to rent 2 matched Epics but there were n't any available . Got 2 Dragon packages with lenses for 1/3 the price since nobody wants them . I guess Fincher got the only 8 good ones . <p> kinda what i 'm seeing ... was just wondering if it was broken or sumpt'n .... noisy Dragon , wacky / ugly noise-artifacts in the shadows on skin tones .... Deb does absolutly nothing as far as i can see .. on -or- off it 's the same issue , Deb is no help what . so . ever . <p> if you follow that lord of the rings size thread all the way thru it @ @ @ @ @ @ @ @ @ @ much use on what DEB is ...... never seen REDuser thread do that before ..... that get 2 smiles <p> as for the discussion about RED cams themselves i have some great tips about them and how get the best results but i , ll put them on another thread at some point a lot of the issue people have with red is they are not maintained correctly <p> Anyway DEB in practice does work exactly as the initial statement implies <p> to removed the " red specks " that effect midtones , dark and shadow " <p> if your interested next week when i , m back in the office i , ll post a R3D you can download load and you can actually see it working .... if i can find it ..... most shots you won , t see any thing but some it is quite useful <p> if your interested next week when i , m back in the office i , ll post a R3D you can download load and you can actually see it working .... if i can find it ..... most shots @ @ @ @ @ @ @ @ @ @ is quite useful 43971 @qwx453971 <p> yea , i 'd like to see it do something , anything ... please send a linkkie on , i 'll grab it .... <p> So , the red introduce artifacts in the image and then put a tool to try to get rid of it . 43971 @qwx453971 <p> This is not the first instance of Red camera screwing something up , then coming back with a " solution " to the problem they created themselves in the first place and then trumpeting this as a new " feature " . Dragon 's OLPF debacle comes immediately to mind ... ADD is another ... <p> Ok not very scientific . One might even call it anecdotal . But here are a couple pics of dragon with and without DEB . Very dark shot . ISO pushed to 1280 . Actual iPhone pics of my Sony Oled in blue gun ( sorry , monochrome ) and it 's definitely doing something to reduce noise and reconstruct image . <p> well it looks like its doing a lot . OP question was if it does @ @ @ @ @ @ @ @ @ @ a shot and the tool is there , then ... In practice I 'm not too acquainted with it making or breaking my day but YMMV <p> Ok not very scientific . One might even call it anecdotal . But here are a couple pics of dragon with and without DEB . Very dark shot . ISO pushed to 1280 . Actual iPhone pics of my Sony Oled in blue gun ( sorry , monochrome ) and it 's definitely doing something to reduce noise and reconstruct image . 43971 @qwx453971 <p> thanx ! ! ! ! <p> not gon na help on what i have heading my way , but hey it does do something .... just not the something i wanted ... <p> well it looks like its doing a lot . OP question was if it does anything at all . and if you need to save a shot and the tool is there , then ... In practice I 'm not too acquainted with it making or breaking my day but YMMV 43971 @qwx453971 <p> It clearly does n't do " a lot " as Dermot @ @ @ @ @ @ @ @ @ @ as I already asked , do we need a CAMERA SPECIFIC noise reduction tool as opposed to a standard noise reduction , like Neat Video or built in Resolve NR that happens to work with all cameras ? <p> I 'm just railing against Red camera approach to everything they do in post . It 's like for them the rest of the industry does n't even exist . RR and RRX comes to mind . I ca n't imagine Sony , ARRI , Panasonic , BMD requiring a $7000 card in order to make workflow efficient . For the longest time Red refused to allow GPU debayer , even after forcing other software manufacturers to abandon their GPU debayer efforts . MTI , Assimilate and scores of other demonstrated years ago GPU debayer and Red shut them all down though intimidation . Then a year or two ago they finally allowed GPU debayer and everyone on Reduser were apoplectic with joy , praising Red on this momentous achievement So , I asked on that thread why it took them so long and Jarred called me a douchebag So , @ @ @ @ @ @ @ @ @ @ seriously we all just have to pray and get Alexa jobs . Usually when Red jobs come my way , shadow noise is the least of my problems . it generally means every step in the production from start to finished is compromised . when was the last time anyone had an Alexa job that was half assed ? 
@@44332604 @4332604/ <p> Only the Advanced ( $30k ) version can do ProRes out . Resolve Studio for Linux ( the **35;199;TOOLONG ) ca n't . <p> The main advantages would be : <p> 1 ) Ability to use more GPUs ( though I 'm not sure if the Studio version has the same limits as on Windows ) 2 ) potentially faster than Windows , but this may require tweaking 3 ) easier to integrate with other Linux systems , not so much with Windows/Mac ( unless you 're into that kind of thing ) 4 ) it 's not Windows ( again , if you 're into that kind of thing ) <p> I just purchased an advanced dongle for our studio . We 're going to use it with a set of DaVinci 2k panels . The initial tests will be a dual-boot setup on our existing Windows PC ( ASRock X99 , w 5930K ) . We get good performance out of this setup on Windows so I 'm really curious to see the relative difference on the same hardware . <p> The downside : if you @ @ @ @ @ @ @ @ @ @ Windows . <p> I do n't think there will be much of a performance gain by using Linux . <p> I think a Davinci Linux version would really shine if they would include a frame server option . With a frameserver you can setup piping and parallel processing and those operations tend to be more efficient on Linux compared to Windows . For instance with piping and parallel processing you could spread out encoding ( and rendering blocks of frames ) to as many machines as you want . <p> I still have a shrink wrapped box of Windows version 1.0 . I have used Windows for decades . Every version . I did that because I had little choice . Programs I needed were not available on Linux . Now they are . <p> Windows 10 is a different paradigm . Microsoft essentially owns your computer , and if your computer is how you make a living , Microsoft owns you . <p> Linux is a state of mind . It is elegant , powerful , secure , free from agreements , free from registration , and free from @ @ @ @ @ @ @ @ @ @ in a vice . <p> I have two dual boot Linux/Windows systems now . From strictly a performance point of view , of coarse , any purpose built OS will always tramp an OS written for general use , that tries to cater to the lowest common denominator . You can obviously turn all those " enhancements " on Windows off . In regards to the philosophical differences of who got how 's head in a vice , you can always turn Windows updates completely off and then Windows insistence on constant updates becomes a non factor . The biggest issue I have with Linux vs Windows is the industrial strength , that you get with use of Linux . Once properly configured it is bulletproof and it just works . The biggest Linux drawback is a distinct possibility of every installation turning into a huge chore . Getting proper dependencies installed from proper repositories is not for a faint of heart . Fstab , networking , reading and writing NTFS and HFS and now AFS also needs to be addressed . Also , a lot of software is just @ @ @ @ @ @ @ @ @ @ to an individual what they are going to choose . Personally , I prefer Linux , by a mile . 
@@44332605 @4332605/ <h> Telecine History <p> My old pal Rob Lingelback of the Telecine Internet Group has agreed to host a bunch of old 1980s color-correction brochures that capture a moment in time when serious color-correction first started happening . They include literature on the Rank TOPSY and Amigo color-correction computers ( among the first serious mass-market devices for color-correction that could store to disk ) , the original Rank-Cintel Flying Spot Scanner , and a bunch of other crap . <p> and here 's an example of what passed for " state-of-the-art " film transfer in 1982 : <p> I 've been threatening to write a " history of color correction " article for some time now ; maybe I 'll eventually get off my duff and actually do it . It 's sobering to think how far we 've come in 30 years -- from a time where you were lucky to store just one reel ( 20 minutes ) of simple color corrections directly from film , to being able to do very complex secondaries , to using hard drives as image sources , to doing all the @ @ @ @ @ @ @ @ @ @ single computer . <p> Our joke at Complete Post in LA ( courtesy of Sparkle , our lead colorist at the time ) was " nothing could be worse-a , than working on an URSA . " Problematic machine . They were nice compared to the old IIIC 's , but I was never happier than when the Spirit 2K and 4Ks came in . The pictures off those were lightyears better than the previous scanners , in my opinion , plus I think they handled film a lot better . <p> The machine pictured above cost about $200,000 in 1982 money ... and that was before you added a color-correction system ! <p> For anybody under 35 : this is back in the day when we had to actually load up actual film and physically let the reels turn and pull images off of that in real time , rather than deal with files . Once I started doing file-based work in 2002 or so , I thought , " I 'm done with film . " It was a huge chore in those days when you 'd get @ @ @ @ @ @ @ @ @ @ would say , " could we check the second shot on the reel ? I just had an idea on how to change it . " And we 'd sit there for 2 solid minutes waiting for the reel to rewind . Pulling up the shot instantly from files spoils you very fast . <p> I just loved the fact i worked in place with four ursas working 24 hours a day except for Friday afternoon , they almost always broke down on a Friday afternoon . Extended lunch break in the pub while the engineers fixed them , but always late getting out on a Friday night . But could a spirit really be considered a telecine ? Even Phillips called it a datacine . <p> I just loved the fact i worked in place with four ursas working 24 hours a day except for Friday afternoon , they almost always broke down on a Friday afternoon . 43971 @qwx453971 <p> Doh , we had five ! ( Plus a Turbo 'd Mk III , which was awful . ) Sometimes , they all worked at one time -- @ @ @ @ @ @ @ @ @ @ of having to auto-align before doing a Super 35mm job -- I have n't thought of that in 15 years at least . I made little cardboard shields to go on top of the glass cover , so that if somebody turned the overhead lights on in the room , the spill would n't get into the light path and ruin the alignment . Man , those were the days , eh ? <p> We still called the Spirit a telecine , even in the days when our company was owned by Thomson . The name for the department finally changed to " Color " once Complete moved across the street to the Technicolor building ; " Telecine " seemed a little quaint by that point . <p> At Modern we easily had double of that and they were used 24 hours a day . Very rarely we had down time . I know , I was one of the engineers keeping them going . 43971 @qwx453971 <p> Ah , I was at Modern when it first started , from 1980-1984 , in both buildings they had in Hollywood , @ @ @ @ @ @ @ @ @ @ Melrose . I had some terrific friends back then , though only a handful are still working in the business today . Roger Berger was the original tape op for the facility and is still there at Modern as a top finishing editor and conforming artist , a great guy . <p> Modern 's Moshe Barkat knows a lot about how challenging the early days of telecine were . I can recall we had one of the original Amigos with secondaries , and we had to disconnect the secondaries because of the ongoing " Rainbow Patent " lawsuit going on from Armand Sarabia of CCC . It took a year or two for that to be settled , since he claimed ownership of any system that changed hue or saturation of color in analog form . Eventually , digital systems came out that bypassed the patent , which made it possible to perfect more advanced systems ( and not get stifled in court ) . 
@@44332606 @4332606/ <h> Invisible hard drive in Finder <p> I have a weird issue . All of a sudden a hard drive became a ghost . With my limited knowledge of what to call this , thus various Google answers , I have n't figured out what to do to make it show up in the sidebar . <p> Luckily at least I 've made a shortcut for a folder on that drive , so I can access it somehow , but it 's very annoying that it does n't show up in Save dialogs and such . <p> I have tried repairing rights and all that , but did n't do anything . I also checked the preferences of Finder , where you can check whether to show hard drives , CDs , disk images and network . The image below represents what 's shown when I check that box . Unchecking and checking it over again makes no difference . The " Hoveng Media Portable " is still a ghost . <p> Do n't let the hard drive name fool you , it was previously used as an external @ @ @ @ @ @ @ @ @ @ installed internally . It have functioned and been visible as normal just until a couple of days ago when I pulled my hair and screamed " AAAAH ! ! " thinking it was dead . 
@@44332607 @4332607/ <h> Clipster , What for ? <p> Well this is a question many friends told me since I work with clipster , I try to explain them why really a Clipster diserve it , so here i go : Clipster is a very specific tool , DVS said is " the heart of your DI " , and i said , is the heart and the head . If you have the lucky of having a Clipster you have to understand that is a machine capable of many many things , let 's discuss this : <p> - Clipster is a machine wich is capable of moving 4K in real time , not so impressed ! ! well what if i told you that you are capable of doing some kind , repeat , some kind of analog device , BTC DIG , HD CAM , HD CAM SR ... without a rendering and without a AJA , Blackmagic , ETC . The HD-SDI output of clipster is completely adaptable to your need , so if have 4K media and do you want a BTC DIG ( PAL or NTSC @ @ @ @ @ @ @ @ @ @ Awesome ! <p> - With Clipster you ca n't export for example a QT NONE of a movie in 2K in about 40/48fps , you are going to have a QT NONE of your entire movie in a awesome time , let 's say i want mu QT in a PAL resolution , 40/48 fps velocity too . The hardware of Clipster is absolutely great , it has a lot of cards connect each other that aloud you have this velocity on render times , Do n't  fotget to mark the HARDWARE FINALIZING SUPPORT in finalizing window to have this . <p> - In the conform is where clipster gets the A+ , a entire new interface appears to show you many options about all kind of things , you can edit the EDL entirely , since the TC until the CLIP or FILE names wherever you want , it makes the work of the online editor easier . <p> - You can digitalize too 444 RGB , ( you need dual link for that ) <p> - You can grade a material , pan and scan it , and finishing in @ @ @ @ @ @ @ @ @ @ RED Epic 4K HDR to ProRes at 170fps , A full days worth of rushes encoded in around 2 hours . Headed straight for the offline the same day i received em . Creating DCP-s is straight forward and works a treat too . <p> I was transcoding RED Epic 4K HDR to ProRes at 170fps , A full days worth of rushes encoded in around 2 hours . 43971 @qwx453971 <p> Combined with the killer DCP creation , this is practically worth the price of admission alone . DVS OEMs the Rocket for RED , but the most we 've ever gotten out of a RR is about 30-36 FPS going to DPX . So , I 'm guessing Clipster must include an even more advanced version of the Atomix ( newer DSP , Chipset , PCI-E 2.0 , etc ) than RED is selling . <p> I think Light Iron uses 3-4 of them for R3D transcoding , and they 've mentioned up to 200fps in articles I 've read . I 'm helping to " untangle " an indie feature conform , I ca n't tell you @ @ @ @ @ @ @ @ @ @ But , the cost of a Clipster would eat up a fourth of their budget alone . lol . <p> good news about clipster. would somebody be interested to try MIST DCI mastering , it would be a pleasure to get your feedback . Our software runs on win7 64bit with NVIDIA . To have a fair comparison in speed to clipster you would need to use our turnkey but the software evaluation would give you an idea how it works.MIST HOST : **25;236;TOOLONG : mistPASS : m15T37 send your mac address to **25;263;TOOLONG and I will send you a temp license 
@@44332608 @4332608/ <p> The display " ZRD-1 " is made up of scalable , seamless tiles , each tile is 320x360 resolution and 20ppi , a 6x3 tile configuration yields a 1800w , 110inch 1920x1080 display . It supports HDR with 1000nit peak white and has a contrast ratio that rivals OLED ( they are both self emitting with no backlight ) . Colour gamut is quoted " 140% sRGB " , along with 120fps refresh rate . <p> According to my calculations , you 'll need around 11x7=77 of 360x320 tiles for quad HD , which will cost $616k . If for HD you 'll need 18 tiles , then that one will coast $144k 43971 @qwx453971 <p> I remember one rear projection TV in the 90s that cost $120k and the price included remodeling the room to fit it . If Sony 's new tech becomes successful , prices will definitely plummet . In the meantime , I 'll order two - one for my living room and one for a client monitor . If they made it waterproof I 'd use the tiles to line up the inside @ @ @ @ @ @ @ @ @ @ . 
@@44332609 @4332609/ <p> Steve you 're aware there is a free version of Resolve with which you could easily test yourself to verify . Some sample LUTs and images exhibiting the problem would be handy . I 'm happy to spend some time figuring this one out , but a vague description of " noise " in a lut is n't much to go on . <p> Steve you 're aware there is a free version of Resolve with which you could easily test yourself to verify . Some sample LUTs and images exhibiting the problem would be handy . I 'm happy to spend some time figuring this one out , but a vague description of " noise " in a lut is n't much to go on . 43971 @qwx453971 <p> I agree . I also generally find in noise situations that the user is trying to slam more contrast into a scene to overcome an exposure issue . The noise actually comes from forcing black into a scene that has no blacks . Curves and/or exposure changes are often a better way to alter this , whether or @ @ @ @ @ @ @ @ @ @ dubious on using a LUT for a look , beyond an actual transforming LUT moving into a different colorspace . RAW to Log , Raw to Rec709 ... those are fine . Powergrades ... sure . Any colorist worth his or her salt needs dozens of Powergrades to pull out of their pocket for everyday situations . But Powergrades are not LUTs , and I think it 's bogus to use a LUT as a Powergrade . <p> I tend to doubt some of these programmers have ever even shot film , let alone know the intimate characteristics of what the film emulsions really did or did not do . I worked every day with the Kodak color scientists who designed the actual Cineon LUTs for real film color space , and even they would admit ( under duress ) that some of what they did boiled down to trial &amp; error and guesswork . The math alone does not always work , and a lot depends on the characteristics of the input file -- and the original cinematography . Heck , nobody can agree on what " Log " @ @ @ @ @ @ @ @ @ @ out there . Which is real ? Which is best ? It 's all very subjective . <p> I have spoken to the customer in question and they say they will be adding info to this thread ... They also provided more info to me via e-mail : <p> The issue relates specifically to the Rec.709 option that is built into Resolve and accessible via the raw settings . Choosing this setting will result in noise when used with one of our Rec.709 to Rec.709 LUTs . This issue does not appear when the same LUTs are applied in other software . A workaround is to use the conversion LUTs that are included with Resolve , rather than the built-in Rec.709 setting in the raw controls . 43971 @qwx453971 <p> The LUTs work perfectly within LightSpace CMS using the in-built LUT Viewer , so the issue is indeed within Resolve - not the LUTs LightSpace generated . <p> I have spoken to the customer in question and they say they will be adding info to this thread ... They also provided more info to me via e-mail : <p> The @ @ @ @ @ @ @ @ @ @ Viewer , so the issue is indeed within Resolve - not the LUTs LightSpace generated . <p> Steve 43971 @qwx453971 <p> What 's the source ? I 'm guessing maybe R3D or cDNG ... sounds like it is n't noise to the lut , but noise in the debayer . Need much more info Steve . Think this thread is a bit alarmist without any weight yet . Sounds like an interesting problem , but we 're not getting a lot ( or really any ) info . <p> Hi there , I 'm the customer Steve was referring to . As Juan surmised , we 're seeing this in cDNG and . r3d where Resolve 's Rec.709 option is chosen in the raw settings . We ca n't reproduce the issue in Premiere or AE . The issue also does not appear when you use the Rec.709 LUTs that ship with Resolve ( either " v1 " or " v2 " ) in place of the Rec.709 raw setting . So our thought was that something weird was going on with the debayer , but we do n't know @ @ @ @ @ @ @ @ @ @ be unrelated to the LUTs we 're using . Instead , it seems to be introduced by Resolve when you choose Rec.709 in Resolve 's raw settings . This noise was then exaggerated when we applied a LUT . Here 's a detail of a . dng frame , as displayed in Resolve , FCP X , and Premiere . ( No LUTs were applied to any of these . ) <p> No , I was n't kidding , any gamma other than 1 is by definition a LOG gamma . Now if you talking about different types LOG curves , like Log-c , Cineon etc , then it 's different atory . That 's why it will be nice to finally get rid of " creative " LUTs altogether and start using transforms , like in ACES ... 
@@44332610 @4332610/ <h> Reddit " Ask Me Anything " with " Digital Colorist " Marina Amaral <p> I ran across this thread at Reddit , and thought it worth sharing here : <p> IamA professional digital colorist . I restore and colorize black and white photographs , working with several museums , TV stations , publishers , and institutions from all over the world . AMA ! - read here <p> While Marina is not a colorist in the sense we use it here , I believe it might be of interest to some . As noted , her work is with the digital colorization of historical photos- something I have to say I 'm not a huge fan of . With that caveat , consume at your own risk ! <p> That said , I am quite interested in her tools , techniques and approaches to doing a lot of tasks we share in common . There is also some discussion of automatic/AI-assisted colorization , which has been discussed here of late . At 22 , she is quite a young voice in the field , and already accomplished and @ @ @ @ @ @ @ @ @ @ There 's a little bit of **30;290;TOOLONG attitude that she is going to be a " game changer , " and she does n't exactly go out of the way to acknowledge the work of others . Still , there 's plenty I found to admire in her admittedly short career . 
@@44332611 @4332611/ <p> Please show before/after images , with a description of the workflow used . <p> The best use of the LUTs will receive a discount voucher for purchase of LightSpace CMS and an i1 Display Pro OEM probe ! ( If the best use of the Free Look LUTs is from a user that already has LightSpace CMS an alternative will be offered ... ) There can be more than one ' winner ' , and we will make choices every few months ... 
@@44332612 @4332612/ <h> Action Camera Stabilization Software outputs ProRes on a PC <p> I 'm afraid I was ' away the day they taught stabilization software ' . After 35 years behind a heavy camera and an even heavier tripod , I have never seen the need . However , I recently came across some stabilization software for the PC which is marketed to action camera enthusiasts . On closer inspection , I noticed the app outputs ProRes 422 and as I shoot native ProRes HQ 422 , I was intrigued to see if it would read and write ProRes without any obvious issues . Happily , it does . <p> The app is a German product called Mercalli v4.0 SAL+ from proDAD and costs between US$300 and $200 , depending on where you buy it . There is a free , watermarked trail so I installed it on my Resolve PC workstation . The product says it will use your GPU if you have one installed but there was no indication that it was talking to my GTX 970 . However , it certainly harnessed the full power of my @ @ @ @ @ @ @ @ @ @ rendered outputs which were acceptably fast . <p> I tested the app with some native 200Mbps ProRes 422 which I had shot recently in Taiwan . It was a telephoto shot of a Bullet Train crossing a huge viaduct in the distance . The camera is stationary , ( on a heavy fluid tripod ) focused on the centre arch of the bridge until the train enters the telephoto view . The camera then pans to track the train left , pulls back slightly and holds steady until the train passes out of view . It is a superb shot spoiled only by some slight jitter caused traffic passing the camera position as well as a not too steady start to the camera pan which is at the telephoto setting . ( 500mm ) <p> As you know , CMOS sensors show some issues with tele shots too so I experimented with Mercalli 's various setting until I found a combination which totally removed the jitter and smoothed out the pan without any obvious picture ' quality ' issues . I chose the ' medium ' rendering option which resulted @ @ @ @ @ @ @ @ @ @ original . On completion , I put the original ProRes and Mercalli 's rendered ProRes on a Resolve time line and played them out on my calibrated 10bit SDI monitor . The re-rendered image looked excellent , the sound was ' as recorded ' ( 16bit PCM setting ) and did not appear to suffer any obvious colour shifts on the vectorscope . Mercalli did not pass through the native ProRes file 's timecode or reel number but I can easily perform a re-wrap with ClipToolz to restore these . <p> In it 's most basic form , Mercalli is a transcoder , designed to process heavily compressed 8bit 4:2:0 action camera footage along with some pretty sophisticated filters and CMOS sensor analysis and solutions but the fact it appears to use FFMBC to output ProRes on a PC , put it in the ' worth testing ' category . Now before I run off and spend my hard earned to remove the watermark , has anybody else tested this app or can you recommend a similar product which will achieve the same results on a PC ? Thanks in @ @ @ @ @ @ @ @ @ @ for Adobe is of no use to us as we use Lightworks and Resolve . The advantage of Mercalli SAL ( Stand Alone Application ) is that it will perform a batch transcode of pre-selected parts from multiple shots in one pass , outputting to ProRes 422 which we simply re-wrap to include a unique timecode and reel number . This blends perfectly in the edit with our other ProRes camera source material . 
@@44332613 @4332613/ <h> Scopebox/Resolve clamping signal <p> Hi There , I ' m setting up scopebox for monitoring the signal from davinci resolve . the chain is : output from decklink sdi 4k in the main computer with resolve to fsi cm250 and pass trough to a second computer in a decklink sdi 4k input for scopebox . I 've got this issue : while watching at the signal produced by an offline clip on the second computer ( movie source ) , scopebox shows out of boundary values ( below 0 and above 100 ire ) , and this is fine . <p> On the contrary , if if watch the very same signal in live source mode ( from the main computer ) , scopebox shows out of boundary values as they were clipped within 0-100 ire range , and I can not see the values that lies below and above 0-100 range . so if i crash or i clip the signal in resolve i can not see out 0 and 100 ire There is some strange setting in resolve that clamp the signal ? In the blackmagic @ @ @ @ @ @ @ @ @ @ 's for your help . <p> That is expected behaviour . The way Resolve works by default it clamps the output to the 0-100 IRE range . Your deliverable should not have image data outside the 0-100 range , so this is not normally a problem . <p> It is possible to configure Resolve so you can get an out of range signal on the SDI output , but it means taking manual control of the range mapping . I would not recommend that approach unless you really understand what you are doing . <p> thank you for your answer . I understand . So there is no way to monitoring out the entire signal in an external scope without this problem ? if i want to see how much the signal is gone out of boundaries what can i do ? and if this is a Resolve feature i imagine that with ultrascope , or any other videoscope the problem is the same , right ? <p> The situation would be the same with any scope , as what you are seeing is the signal Resolve is outputting . @ @ @ @ @ @ @ @ @ @ out of range signals on the SDI output by using different specially created LUTs for the viewer , SDI output ( and possibly internal scopes , depending on what you want to see there ) to scale the signal . Or you can work with un-scaled signals throughout , but I would not recommend this approach . <p> But why do you want to output out of range signals ? Some cameras produce them in the source , but you would normally want to bring these into the 0-100 IRE range as part of your grade . <p> There was a similar discussion on the Blackmagic forum a couple of years ago . I posted a LUT which could be used to do the required scaling . Resolve at the time could not do separate LUTs for display , scopes and SDI , so the situation is better now than what I describe in the post . <p> To see the entire range of your signal you should set resolve to data levels . But as Nick above , you will always want to legalize it 0 100 as part @ @ @ @ @ @ @ @ @ @ setting data levels ( at input and output ) is that the grading maths does n't work as expected . You want a gamma adjustment to " pin " black and white , but it won't. -7% and 109% get pinned . Similarly lift and gain pivot about -7% and 109% instead of 0% and 100% . You end up having to compensate for this in your grades . <p> There was a similar discussion on the Blackmagic forum a couple of years ago . I posted a LUT which could be used to do the required scaling . Resolve at the time could not do separate LUTs for display , scopes and SDI , so the situation is better now than what I describe in the post . 43971 @qwx453971 <p> I just tested , and my LUT can be used to scale the values so you work internally in the 0-100% range ( so that a gamma adjustment pins black and white , etc. ) but your SDI output includes super-whites and sub-blacks . However the ability to apply different LUTs to different outputs is not as flexible @ @ @ @ @ @ @ @ @ @ the SDI output , they also become visible on the UI monitor , as that ends up showing the image scaled to video range . Unless you put the same LUT on all three , a clamped version of the LUT gets applied to the SDI output , which defeats the point . <p> I would say ideally you would want to leave your SDI output clamped as it is by default , and apply my LUT only to the internal scope , so you could see what image data your grade was pushing out of range . Logically this should work , but it does n't ! 
@@44332614 @4332614/ <h> Dealing with the FPA Harding Test <p> I 'm not sure if this is a " mastering " question per say , but I do have a quality control issue : handing of the so called HARDING TEST . Now , I understand what the test is made for in that we want content that does n't provoke seizures to an audience prone to epileptic attacks . I however have a very hard time dealing with this test as it 's very new to us here in Sweden and I really do not understand exactly what parameters it goes on . <p> On a given image sequence I might have 2-3 frames landing on a 2.3 value on the scale where 0.5 is above failure limit . The question is - how does one deal with this ? Another question is if people have any experience in how " harsh " these tests are . With that I 'm wonderings if a channel sees 1 frame of failure ( of the course of X seconds or minutes of material ) - will they fail the spot or do @ @ @ @ @ @ @ @ @ @ this quite subjective test ? <p> I asked an editor pal of mine how they dealt with it on a show of ours , and he shrugged and said they had to do a pull-up to eliminate the flashed frames . I think they will allow X number of flashed frames , but there is a limit . The BBC specs are here : <p> According to the BBC Worldwide Content Delivery Book , they have a provision where program producers can request that the network just put a disclaimer at the head warning of the potential for photo-sensitive epilepsy . <p> I 'd like to see what they 'd do for a movie like Andromeda Strain , where a character goes into epileptic shock during flashing red alarm lights in the laboratory ( it 's actually part of the plot ) . <p> Erik , We 've had the Harding test as a requirement here in the UK for many years . As far as the parameters it tests , these can be broken down into three areas ... <p> 1 ) Pattern Analysis 2 ) Luminance Flash Analysis @ @ @ @ @ @ @ @ @ @ doubt discovered , these ' failures ' can be triggered by seemingly innocuous things , such as a white van traveling past the screen , or a metal fence , or even a cut ! <p> Looking at the HardingFPA site , there 's not a lot of detailed info easily available , but I do have a file which goes into some detail . It 's a fairly old document , so some of the info may be out of date , but I think the basic testing principles still apply . I ca n't upload the whole document as it 's too large , but hopefully pages 8 and 9 will help . <p> As to the fixes , this depends on the specific problem , but things like reducing the luminance of the offending items ( or clamping the luminance peaks to something like 60% ) , applying a little blur/defocus to problem patterns and reducing saturation on reds are all the sort of techniques used to get that elusive ' Pass ' certificate , which is what the broadcaster wants . <p> The issue I 'm @ @ @ @ @ @ @ @ @ @ to left . Gives anywhere from 1-2 frames of issue up towards 10 frames . I have zero clue how to control it but I 'm battling with this F-IGN test . <p> If it is high frequency Twitter that is causing it you could try upping the anti aliasing of the render or blurring anything repetitive , but definitely lower saturation . If it is that 90% of the screen changes from high to low contrast in to few frames you may need to reconsider the framing . There is a free pc app somewhere to test . <p> According to the BBC Worldwide Content Delivery Book , they have a provision where program producers can request that the network just put a disclaimer at the head warning of the potential for photo-sensitive epilepsy. 43971 @qwx453971 <p> I had a show broadcast on BBC Four in the UK that was accepted PSE fails because it was about butterflies and the butterflies themselves were causing PSE fails . But this week on another show in UK for different broadcaster that flat out refused productions request even though it was news @ @ @ @ @ @ @ @ @ @ As to the fixes , this depends on the specific problem , but things like reducing the luminance of the offending items ( or clamping the luminance peaks to something like 60% ) , applying a little blur/defocus to problem patterns and reducing saturation on reds are all the sort of techniques used to get that elusive ' Pass ' certificate , which is what the broadcaster wants . 43971 @qwx453971 <p> Other things you can try is adding a vignette to reduce the uniformity of the luminance change over the whole screen , slowing down the clip or sometimes what I do on a shot if its flash photography is find the brightest frame and remove it . 
@@44332615 @4332615/ <p> Now , in Resolve , I can apply these three LUTs in consecutive nodes to the scene linear footage in order and everything looks perfectly correct . However , if I generate a new 3DLUT in Resolve that has all three combined into a single LUT for use in FFMPEG automation , all the highlights are getting clamped ( in Resolve , Nuke and FFMPEG ) . So , I try to use the three separately and get these results : <p> - FFMPEG : The first lin2SLog3 LUT clamps the highlights , so the log-to-709 LUT ca n't map the bright values back to 100% , they are closer to 70% and have a clipped look to them . <p> - NUKE : When applying the three individual LUTs in consecutive VectorField nodes , I 'm getting crazy clipping and shifting of colors in the bright areas ( such as fire or specular highlights ) . However , if I use the lin2slog formula as an internal LUT curve in Nuke instead of the 3D LUT file , I get perfect results ( that match Resolve when @ @ @ @ @ @ @ @ @ @ then use GenerateLUT to convert this function to a 3DLUT and try to use that in Nuke , I end up getting identical results to the original lin2SLog3 LUT I tried using . <p> So clearly , Nuke and Resolve have different ways of mapping the float values to integer values while using a LUT , and Nuke has to do it mathematically while Resolve seems to be able to figure it out with a 3D LUT . The problem is , FFMPEG does n't seem to be able to take the function as a filter , and is n't working with the 3D LUT . Is there any other way to properly convert the linear EXR to the proper SLOG colorspace with maybe a 1D LUT or something that would work in FFMPEG ? I do n't really know much about the actual workings of the math with LUTs , so I 'm running out of ideas . <p> I 've tried almost every combination imaginable generating LUTs on lutCalc as well , to no avail . Any help would be greatly appreciated , thanks ! <p> Also side @ @ @ @ @ @ @ @ @ @ a new curve to the LUT menu , I can get it to show up in the drop-downs of the READ nodes , but I ca n't get it to show up in the drop-downs of the colorspace node , which is what I need to properly use the custom curve as a conversion . Currently , I found it will show up in the vectorield node colorspace in/out drop-downs , so I just load a NULL LUT and use those drop-downs to perform the mathematical conversion . <p> Definitely when in Linear . exr Land . Most LUTs ( Resolve . cube files especially ) only work on the Integer range . By that I mean 0-1024 in 10bit . It 's easy to see - just open a . cube in a text editor and you can see the limitation at the top . <p> A cube , by definition , has a limit . And when you hit it the results can be ... unfortunate . <p> Use mathematical transforms instead . They 're often less processing power for superior results . Usually reversible almost 100% losslessly @ @ @ @ @ @ @ @ @ @ by using scopes . To the human eye its perfect , at least for the 1st 10 times that you transform &amp; reverse . <p> You can do this in Resolve using the new Resolve FX in v12.5 ( I prefer working that way rather than using the ' Working Space ' option in RCM . <p> If you ca n't find the Transform that you need then you can use the new custom . DCTL option in 12.5 . Basically it 's a custom Transform version of RCM , rather than being limited to a custom LUT . <p> If you need someone to create a . DCTL for you then talk to Nick Shaw . I 've never seen bad work from him and things like . DCTL maths is his bread &amp; butter . <p> That 's what I gathered ( even though resolve somehow still uses the LUT correctly if it 's not combined with the display LUT ) . My main problem is getting our footage back to SLog for dailies . Resolve is n't an option since I ca n't automate it on the @ @ @ @ @ @ @ @ @ @ . Is there any way to create a transform using the mathematical expression for FFMPEG ? Or any farm-compatible tool for that matter ? <p> Regarding ffmpeg , did you try using a super high rez lut like 64*64 ( max dimension ffmpeg handles atm ) ? Also if you are looking for an alternative automation route and you seem to have success with nuke , you could set up a script in nuke for your conversion , define parameters and call nuke from the commandline passing these parameters . Nuke manuals give enough examples to get it up and running easily . Used this route in an earlier vesion of my dcp automation , but ffmpeg route with lut was way faster then nuke ( less overhead ) , but Nuke is ultimately flexible and gives you endless options to automate anything you want and farm compatible . <p> Sam , I appreciate the advice , but it 's not really applicable to our situation . As I stated before , Resolve is n't an option to implement into our workflow . When we have dozens upon dozens of @ @ @ @ @ @ @ @ @ @ someone sitting on a resolve machine doing a bunch of manual labor ( even if it 's just loading shots in and setting up renders ) . My problem is not for color grading , it 's for processing large numbers of VFX shots for weeks on end to send to the client as work is being done . <p> When the artists render their Nuke scripts over the farm , they submit their shots to a Rush script which sends the EXR through the farm pipeline which currently uses FFMPEG . Our pipeline requires absolutely no human interaction , hence the use of LUTs right now . Each shot automatically picks up its corresponding LUT from the LUT path and applies it during the transcode in FFMPEG ( which has a small range of acceptable LUTs and to my knowledge . cc files are n't one of them ) . <p> Also - all the CDL values have been pre-set by the LAB and are sent to us in the EDL . The CDLs were made in SLog3 space , so going the LogC route wo n't work either @ @ @ @ @ @ @ @ @ @ is if there 's a tool that can be added into our automation pipeline to get the Linear files back to SLog3 , because the rest of the pipeline works great . <p> Glenn - I have tried using 64*64 LUTs , and 16 bit 1D LUTs . I tried just about every option and combination I could think of on the LUTcalc website . Nothing seemed to have changed . I 'm almost certain it 's because the values are going up to 10 , and the 3DLUT in FFMPEG is just clamping it at 1 . I even tried playing with the value scaling set to 1-11.0 in LUTcalc , and it ended up just producing even weirder results . <p> I do like the idea of automating some Nuke scripts and have thought about this before . My only hesitation is like you mentioned , the overhead . Nuke is n't exactly efficient in raw speed of transcoding , and is it stands , our farm gets hit pretty hard on a day-to-day basis so every second I can save in the processing , gives the artists @ @ @ @ @ @ @ @ @ @ . I 'll have to put it through some tests though in the meantime until I find another solution . I also thought of RVIO , which I believe can take a mathematical expression , but we 'd have to order a whole bunch more licenses which I 'm trying to avoid if at all possible . <p> I do like the idea of automating some Nuke scripts and have thought about this before . My only hesitation is like you mentioned , the overhead . Nuke is n't exactly efficient in raw speed of transcoding , and is it stands , our farm gets hit pretty hard on a day-to-day basis so every second I can save in the processing , gives the artists that much more time to push their main renders through . I 'll have to put it through some tests though in the meantime until I find another solution . I also thought of RVIO , which I believe can take a mathematical expression , but we 'd have to order a whole bunch more licenses which I 'm trying to avoid if at all @ @ @ @ @ @ @ @ @ @ based on non-exr material so could move from Nuke back to ffmpeg . Guessyou need to see the actual performance numbers and decide if it is a worcable solution until you find some other trick . I was thinking , did you try piping your ffmpeg thru imagemagick using its convert tool with the -color-matrix flag ? Hardly any stuff on google for it but seen it used in some difficult cases . Here the man on -color-matrix flag LONG ... If your farm allows to configure the full command string for ffmpeg you could play with other tools that can hanfdle matrix conversion in a pipe . I found an example useage online for an xyz to rgb conversion of a single mxf frame using the convert tool of the imagemagick suite : <p> Your difficulty is that you are starting from unclamped float linear EXR . Simple cube LUTs do not work with linear input , and by default clip outside 0-1 . You need to add a 1D shaper ( Lin to S-Log3 would probably be best in your case ) to your LUT . But I @ @ @ @ @ @ @ @ @ @ Instead of trying to combine the LUTs , you may be able to use a 1D LUT followed by a 3D LUT in ffmpeg . <p> I was thinking , did you try piping your ffmpeg thru imagemagick using its convert tool with the -color-matrix flag ? Hardly any stuff on google for it but seen it used in some difficult cases . 43971 @qwx453971 <p> Glenn , I have not yet tried Image Magick , although it is in my list of tools to look into . Our farm script is custom made and it just assembles FFMPEG commands , so we can really throw at it anything that 's installed and configured on the render nodes . This is a good lead for me to take a look into . <p> Your difficulty is that you are starting from unclamped float linear EXR . Simple cube LUTs do not work with linear input , and by default clip outside 0-1 . You need to add a 1D shaper ( Lin to S-Log3 would probably be best in your case ) to your LUT . But I do n't @ @ @ @ @ @ @ @ @ @ trying to combine the LUTs , you may be able to use a 1D LUT followed by a 3D LUT in ffmpeg. 43971 @qwx453971 <p> Nick , this is what I was thinking of trying , although I 'm also unsure if FFMPEG handles a 1D shaper LUT , but I can certainly try . The main issue is I 'm not sure how to create a shaper LUT -are there shaper LUTs out there from lin-SLog3 that I could use as a starting point ? <p> Perhaps it 's possible to mathematically make these conversions in a python program , although I imagine that would be a pretty big developmental endeavor . Could be worth it as EXR is becoming the de-facto for VFX , it 's a process we 're likely going to need to employ fairly often . Otherwise , we can try to find a new tool that handles expressions for gamma adjustments . 
@@44332616 @4332616/ <h> Alexa Mini Pink Tint <p> Am I crazy or has anyone ever noticed a very strong pink tint ( and a lot of reds in the shadows ) when grading Alexa Mini Footage ( I do not have the Amira LUT on , but that again is a good indicator of how pink the footage can become ) <p> Ok thank you guys ! I guess I 'm juste surprised at how much red there are in the shadows ... Maybe it 's because most of the shots are a bit under ? Anyways , had to use a Lum vs Sat on most of the timeline on red to get rid of it ... <p> Ok thank you guys ! I guess I 'm juste surprised at how much red there are in the shadows ... Maybe it 's because most of the shots are a bit under ? Anyways , had to use a Lum vs Sat on most of the timeline on red to get rid of it ... 43971 @qwx453971 <p> Were you on set ? it looks like the camera was white balanced @ @ @ @ @ @ @ @ @ @ the interior was likely lit with tungsten lamp with coloured shade , causing it to appear very warm and reddish . <p> Yes , I was on set , directing actually . I do n't think there was any source inside and if it was , it was either HMI or Joker so for sure everything was daylight . Maybe it 's just a question of personal preferences ! <p> Was there something big and red in the room ? looks like spill and I see a red-ish pillow on the bottom right ? Looks pretty normal other that slight red cast on the wall . This is just contrast adjust no pushing colors . <p> Were you on set ? it looks like the camera was white balanced to the exterior daylight coming from the windows , and the interior was likely lit with tungsten lamp with coloured shade , causing it to appear very warm and reddish . 
@@44332617 @4332617/ <p> I think that 's just marketing BS . As a colourist you could probably eventually arrive at the same look with enough work . <p> they 're tools , LUTs are used to EITHER match a specific target such as a colour space or camera picture style , or for creative effect . When doing colour grading you will most likely find yourself using both . You should n't expect a LUT will do all the work for you , as a colourist you still have to put some effort into it . LUTs are starting point , more often than not . <p> Not necessarily at all , there are various ways of producing LUTs , some are created far more scientifically than artistically , using measured values from film stocks unless I 'm mistaken . It 's ultimately " just " data . But you would be wise to consider a great LUT as a starting point for your grade , before you do some actual work . Again , LUTs are used for technical matching for cameras/films etc , and for creative reasons " how @ @ @ @ @ @ @ @ @ @ , for me , LUTs ( if I use them at all ) are usually a " Finishing " touch . You 've usually got to put a lot of work into a project , correcting and balancing shot to shot &amp; scene to scene before applying the LUT . Most " Look " LUTs have a " sweet spot " where they want the image data to reside in order to get the promise " look " . Additionally , LUTs tend to clip highlights &amp; lowlights , so if you use them early in your process , you 'll be throwing away data that you will likely want later . <p> Hmm , my mistake it was text not a video . And either an edit was made to that text or I was dreaming about LUT marketing last night . 43971 @qwx453971 <p> I do n't think you were dreaming , something akin was said in this thread , of SpeedLooks : " Our smooth evenskin tones ca n't be matched and are not achievable through regular grading methods " <p> I 've had features that had @ @ @ @ @ @ @ @ @ @ being discarded entirely . Most often the problem with the original grade was it was done with poor monitoring , but I would say the second most common problem is poor implementation of a technical or creative LUT , or a bad LUT itself . If you use creative LUTs without understanding 100% what the LUT is doing or without the monitoring environment to know what you see it what you get , the tail can start wagging the dog ( so to speak ) and suddenly you 're chasing the LUT around , making adjustments to suit what it expects , unable to make tweaks to it , and at the same time , you 're not really learning how to achieve the look yourself , or thinking through if that 's the best option . At least , that 's what I 'm told from the producers and DPs who went through those sessions . <p> Is there truth to this - " Our smooth evenskin tones ca n't be matched and are not achievable through regular grading methods " ? 43971 @qwx453971 <p> No , it 's @ @ @ @ @ @ @ @ @ @ luts can or ca n't do and how to work with skins . <p> What those shops are talking about with their LUTs is more like custom color science . There is some truth in the value of that , but the lut there is a means to end , the real value is in the research and development that goes in to the color science . <p> LUTs can be more precise than feeling your way to a look . They 're a tool . but as described in this thread and by many other preaching their use , a missused tool . <p> I rarely use them , as I do n't like not knowing *exactly* what I 've done to a shot . <p> There are some nodes I use in exactly the same way so often I feel I should turn them into LUTs , but I have n't yet , as I like seeing them , and being about to tweak them in my node tree . <p> As a colorist , I often feel like a LUT is a node you ca n't tweak @ @ @ @ @ @ @ @ @ @ on set monitoring , where you rarely have perfect condition to grade , but applying a LUT can help everyone understand what the look is supposed to be . Especially for clients who do n't know how to look at raw footage . <p> The biggest problem with so called " creative " LUTs is that they are essentially black boxes- you feed an image into them and out comes out something else . But you have no idea what they were smoking , when they created that look . Call me strange , but I prefer to know what is going on under the hood . Anyone can create a " look " , save it as a LUT in Resolve and call it a " Our smooth evenskin tones ca n't be matched and are not achievable through regular grading methods " LUT . The only time I 'd use these " creative " LUTs are for grading music videos . Directors on those projects usually have no idea on what they are trying to achieve until they plop in the chair next to me . Usually I @ @ @ @ @ @ @ @ @ @ visual ideas they have in mind for their project . You 're lucky , if they provide you with a couple of reference videos , but even then , they manage to select videos with very large budgets asking for the similar look , that they shot on 5D with a couple of lights and with $300 grading budget . Anyway , having a few of those " creative " LUTs can help finding a quick and inexpensive solution in those situations . But that 's about all what they are good for . <p> I 'm trying to get a better understanding of LUTs . Are LUTs not just , basically , grades compressed into a universal file format ? ( . cube for example ) ? 43971 @qwx453971 <p> Not exactly . LUTs predate digital color correction as we know it and have been used in display technology for a while . They are two different beasts . A LUT is a numerical table that explicitly defines translation of one set of code values to another set of code values . <p> A grade is parametric , defined @ @ @ @ @ @ @ @ @ @ ) , before " look LUTs " came around , a LUT was just a tool for translating images from one color space to another . <p> Is there some magic to a LUT that ca n't be achieved without it when trying to match it using just color grading tools ? Thanks ! 43971 @qwx453971 <p> Yes . Color space conversion is tough to pull off without LUTs . In theory , you could create a LUT that 's plain impossible to approximate mathematically . When you speak of look LUTs , I do n't see much advantage they have over creating the same look using color corrector 's tools , other than they are canned effects that can be deployed as quickly as any premade filter . <p> When LUTS were first created there was n't any creativity or artistry involved . They were just simply to make the picture coming through a computer monitor behave the same as the image coming out of the film projector . A digital response to mimic the chemical reaction that occurred when printing the film . <p> Then , it was colorists @ @ @ @ @ @ @ @ @ @ spin on that . So really , Luts are the recipes created by colorists for other colorists to get the same taste from their dish If that 's what they want . Sometimes from scratch tastes better , sometimes the dish tastes just fine from the recipe . You just need to watch out for the frozen microwaveable version . <p> I agree 100% with Igor above that the real job of a LUT is to convert from one color space to another , and that 's something that 's mathematically complex enough that you do need to map that out for best results . A color correction is just a color correction , and in most programs you have the ability to store looks to try under different circumstances , but it 's just a pre-made look ... some of which are very handy to have and will save you time . Believe me , by the time I 'm up to reel 5 in a movie , I have 20 presets I 'm grabbing from the previous 4 reels just to get through the footage quickly . Same @ @ @ @ @ @ @ @ @ @ n't dispute that there are some amazingly cool effects available as plug-ins through OFX like GenArts Sapphire or Boris FX , and that 's a look you ca n't create any other way . But ... the difference is they do n't market them as LUTs -- they 're simply looks or effects . <p> First , it is true that grading can produce results LUTs can NEVER match ! This is because a LUT is a global effect , and can not alter different areas of an image differently , as grading through shapes can for example . Also , for a LUT to accurately match a complex grade , and be as smooth in its result , would require a very , very large LUT - far bigger than the present biggest sizes used . It should be fairly obvious why this is - but , basically , the space between the points in a LUT are not ' controlled ' , so rely on the way the system using the LUT interpolates them - and all systems are not equal in this respect . Grading allows direct @ @ @ @ @ @ @ @ @ @ creative LUTs can never do this at the size we presently use . For example , if a LUT with too few ' points ' is used to attempt to represent a complex grade you will get banding ... LUTs work best with ' smooth ' and relatively ' slow ' changes . <p> Second , a lot of the ' good ' film emulation LUTs use our tools and/or services for their generation : <p> It is the original film profiling process that ensure a good emulation match . <p> However , such LUTs do expect a given input stimulus , so only work very specific formats , that are exposed , etc. , exactly as the LUT expects . Obviously a good colourist can grade the footage ' under ' the LUT to achieve such a state of affairs . It is this ' grading under a creative/look LUT ' that is often is the best approach to using such LUTs - rather than seeing any given LUT as a magic bullet . <p> Technical LUTs , for colour space changes for example , are very specific , @ @ @ @ @ @ @ @ @ @ be incorrect , as is unfortunately seen with far too many conversion LUTs that are built into various post systems . <p> And then calibration LUTs are specific to the display they are generated for . While they are not always loaded directly into the given display , and may indeed be held at the output of the creative system they are intrinsically linked to the display , and must never be burnt into the footage being worked on . <p> The only time the last sentence is not strictly correct is when you deliberately burn a calibration LUT into footage so it looks correct when played back on a given display that ca n't be calibrated directly - say the director 's laptop - after first profiling said display and generating the LUT from the profile .... <p> That said , so does a preset grade ... If you put a hero grade created on LOG material onto LIN stuff .. its going to be crazy . 43971 @qwx453971 <p> What I think can work very well is just to create a dozen " looks " as preset grades @ @ @ @ @ @ @ @ @ @ a finished grade for each shot and see if it works . If it 's something simple like a vignette or a desat look , then it might be reasonably close . <p> With the exception of dailies ... I do n't  see the point of creative LUTs .. just give me the grade files . 43971 @qwx453971 <p> Even with dailies , I think the DP wants to see something reasonable in a straightforward Rec709 image . That 's unless they specifically say , " hey , I want all this stuff to be desaturated about 40% and leaning towards cyan with slightly cyan blacks . " That you 're going to basically have to do by hand , but I suspect four or five preset looks could handle it . <p> I get that there 's not a lot of time for dailies , especially if you 're crushed under 3 hours of footage that has to be cranked out in a set amount of time , but chances are an overall look per scene will get close . There are those DPs who create specific on-set LUTs @ @ @ @ @ @ @ @ @ @ can work fine ... since it 's a project-specific LUT . The only danger there is if the on-set monitor is n't calibrated . <p> Is there some magic to a LUT that ca n't be achieved without it when trying to match it using just color grading tools ? <p> Thanks ! 43971 @qwx453971 <p> I can answer to this one . ( this is really watered down to the ground ... ) <p> Theoretically you can do with color grading anything that a LUT can do , actually it is IMPOSSIBLE to match it perfectly , practically you can get close within a tolerance . <p> Let 's start with something simple . To give you the Idea of what this is , think about a Rubik Cube , is a 3x3x3 cubes/4x4x4 vertices. in every point a vertices exist , there there is a value of RGB . Well , now think about having that cube made of soft rubber/plaster : you grab in your hand and you modify it . The landing vertices now have a different RGB-&gt; those are the value that the LUT represent . In @ @ @ @ @ @ @ @ @ @ from the original cube to the distort one . <p> You will notice also that if you look only at the original vertices , there are a lot of details of your cube that are not represented correctly , so you want to increase the density of the original cube : slice the pie more . <p> 8x8x8 ( 9x9x9 ) is the next step , but it is still course . The most common format of LUT you can think of is the one with a 16x16x16 cubes or 17x17x17 vertices . The next step is 32x32x32 ( 33x33x33v ) wildly used . 64x64x64 ( 65x65x65 ) is also used and so on . <p> Now , if you think of a grade , a primary grade , you 're closer to a 3x3x3 cube , so if you want to mimic what the little different between all those vertices you have to do a qualifier for each of the individual vertices you want to manipulate and change it where it should land . <p> So , in short . You CAN theoretical make one correction with a tide @ @ @ @ @ @ @ @ @ @ want to mimic and modify accordingly , this mean for a 17x17x17 is 4193 corrections . AND you still are not 100% there ( might be 99% : key versus thetraedrical lut interpolation ) <p> You can get away wil a lot less correction if you accept tolerances ... It depend what the LUT does : if it is a grade done in resolve you can probably get it very close , if it is a real film emulation lut ... not so much . 
@@44332618 @4332618/ <h> Tangent Element Rings looking old and peeled <p> Hello guys , I bought a complete Tangent Element Panels set back in 2014 , I have been using this for almost everyday since then , but after 3 years of use the rings of the panels are not looking so good . In fact all the most used rings look old and peeled . I sincerely think it is due to the contact of the nails with the rings . <p> Maybe I am not doing the spinning movement as gently as I should do with my fingers , or maybe someone else is facing the same issue . <p> I would like to know if someone else has experienced the same issue and if it was easy to get a replacement or something like that from the manufacturer . <p> I just opened a support ticket in the Tangent web support page . I hope they can help me . <p> It is not a really serious problem but it does affect the original soft touch of the rings and is not cosmetically good for sessions with clients @ @ @ @ @ @ @ @ @ @ rings . I supplied 30 new rings to one of my clients ( big production house ) last year and more to other various post houses . If you need new ones , feel free to send me a pm . <p> However , I 'm working on a little side project regarding the rings and might have an update on that in the coming month , if you can wait that long . <p> i play fingerstyle celtic guitar for fun , and have the left hand nails super short , but my right hand nails are acrylic , stickout about 1/8 " and are rock hard , i use a thumbpick so my thumbnails are also short ... i 've drilled holes through my mbp 's key 's several times with the right hand , but the nails have not caused any issues with Artist color , transport or XKeys with long days of constant use 
@@44332620 @4332620/ <h> remote and local grades merrygoround <p> My preferred workflow using Resolve is to output full length clips by switching to the master timeline on delivery . Works like a charm .... I love it over exporting edited clips . Makes me sleep better at night . <p> HOWEVER - <p> When you create a 5th version grade on a remote clip , I notice the grade does not get applied to the version on the master timeline . It only gets applied if you keep the remote grade to version one . I have to say , this just screwed me on a grading job , I got into 4 and 5 version grades to make client happy and then ended up having to copy my tracking , nodes , etc .. manually back to version 1 of the remote grade so my master timeline would have the right grade when I did the final export of full length clips . <p> Am I missing something here , am I using this feature wrong and incorrectly ? <p> Go to specific clip in master timeline and change the @ @ @ @ @ @ @ @ @ @ separate folders and then delete ones that you do n't need and of course check your renders . <p> I think the best way to go is to create one new nested timeline made up of all the other timelines . And I check them to make sure there 's no extraneous crap ( slates , countdowns , test signals , etc. ) for the final render . For feature delivery , there are possible alternative workflows provided you have assistants and other people doing versioning . For episodic TV , it 's a question of how much time is available and what the turnaround is . For shortform ( commercials , music videos , etc. ) , it 's whatever works . <p> I tend to not use the master timeline as much as I did in years past , but it 's nice to have alternative ways of working . <p> Nikola , yes I noticed I could switch to master and match the version number but it seemed tedious for me and I did n't really trust myself not to miss anything . 180 edits in a @ @ @ @ @ @ @ @ @ @ to get drunk now that this project is over . <p> Am I missing something here , am I using this feature wrong and incorrectly ? 43971 @qwx453971 <p> I think you 're creating more work for yourself .. <p> Are you relinking your NLE to the Resolve renders instead of doing an actual roundtrip ? What if you have two shots that reference that same piece of source media that need different grades ? Are you keyframing your master timeline to accommodate for that ? <p> I agree with Mark that the Master Timeline can be super useful , but it 's less so to me as well these days .. <p> Are you relinking your NLE to the Resolve renders instead of doing an actual roundtrip ? What if you have two shots that reference that same piece of source media that need different grades ? Are you keyframing your master timeline to accommodate for that ? 43971 @qwx453971 <p> I give the full clips renders back to the editor , he relinks them back in his NLE , and this keeps it easy for him so his @ @ @ @ @ @ @ @ @ @ remain in tact . <p> Yep - keyframes between points that reference the same source media which required different grades . Using master timeline / remote grades also means the same clip will get the same grade automatically in the event they do n't need a different grade which is usually often . Then the only thing to be keyframed are power windows / skin qualifiers , etc. 
@@44332621 @4332621/ <h> SONY OLED TRIMASTER still green tint after calibration <p> Got myself a Sony OLED PVM-A250 monitor a couple of months ago , read some users complained about a green tint and I could confirm it too when I got mine . <p> Thought it could be fixed with calibration , but after calibrating it with a X-Rite i1PRO 2 ( and Sony Automatic White Balance software ) the monitor still have a green tint ? ! ? ! I calibrated to both x.313 y.329 and then x.307 y.318 ( the later one worked better ) but still not good enough .. Brought some test images into Resolve and balanced the highlights to exact white with the waveform monitor , and they look pure white on my ( calibrated ) computer monitor , macbook screen , iphone , ipad ... but greenish on my SONY monitor ! ! <p> When we were using HD CRTs at Complete Post ( $40,000 Sony BVM-32E 's ) , our engineers used Philips color analyzers , Minolta photometers , and the Sony probe ... and they were all different . We wound up @ @ @ @ @ @ @ @ @ @ when even the measuring devices do n't quite agree with each other . They were ballparkish , but not really . <p> Steve Shaw of Light Illusion has strong opinions on the Sony broadcast OLEDs ... <p> Got myself a Sony OLED PVM-A250 monitor a couple of months ago , read some users complained about a green tint and I could confirm it too when I got mine . <p> Thought it could be fixed with calibration , but after calibrating it with a X-Rite i1PRO 2 ( and Sony Automatic White Balance software ) the monitor still have a green tint ? ! ? ! I calibrated to both x.313 y.329 and then x.307 y.318 ( the later one worked better ) but still not good enough .. Brought some test images into Resolve and balanced the highlights to exact white with the waveform monitor , and they look pure white on my ( calibrated ) computer monitor , macbook screen , iphone , ipad ... but greenish on my SONY monitor ! ! <p> What do I do ? This is driving me nuts ... 43971 @qwx453971 <p> @ @ @ @ @ @ @ @ @ @ too blue ( cool native whitepoint ) . Thus the Sony looks yellow/green in comparison . .307 .318 is correct Judd voss offset white point for the oled . And the Sony A250 is a very sweet display . You wo n't find better for the price . <p> Unlike these RGB triad designs , most modern LED backlight solutions involve placing a border ( or in some case clusters ) of white ' LEDs behind or at the side of the LCD matrix , often near the edges and using a diffuser to spread the light across the screen . Despite being called white ' LEDs they actually emit a blue light which passes through a yellow phosphor to give a more neutral white and provide the red and green components of the image . Early iterations of the technology ( those circa 2009-10 ) tended to suffer from an obvious and uncorrectable blue bias . As manufacturers became more familiar with the technology and were able to tweak the backlights , phosphor coatings and the LCD panels this tint became more workable . Despite these advances many WLED @ @ @ @ @ @ @ @ @ @ when it comes to the spectrum of light they produce . The graphic below represents the relative intensity of light at various wavelengths for a typical ' modern WLED backlight . <p> Typical WLED spectrum You can see a distinct peak of spectral energy in the blue ' region , specifically at 450nm ( light considered pure blue ' ) . This comes from the blue diode of the backlight which is typically composed of InGaN ( indium gallium nitride ) . A much weaker spectral response of less than a third the intensity can be observed between 500nm and 700nm , corresponding to the yellow ' light of the typical scintillator phosphor coating ; YAG ( yttrium aluminium garnet ) . In combination the InGaN and YAG components of the backlight produce white ' light with a native colour temperature ( white point ) determined by the ratio of InGaN to YAG . <p> This light is filtered through the red , green and blue subpixels of the monitor to produce a wide range of colours and allow further refinement of the white point . After filtering a considerable @ @ @ @ @ @ @ @ @ @ lost ; the filter ' is far from perfect and the initial spectral imbalance of the backlight is still an underlying issue . Provided the filters are working as intended ( i.e. the monitor is properly calibrated ) your typical WLED-backlit monitor will be able to make good use of the strong pure blue ' spectral component to produce strong pure blue ' colours . The red and green components ( originating from the yellow light of the YAG phosphor coating ) are relatively weak . These gaps in spectral energy and relative lack of intensity for wavelengths other than 450nm restrict the colour gamut of a typical LED-backlit monitor to roughly the sRGB colour space . The colour gamut shown below compares the Dell U2412M 's colour gamut ( red triangle ) with the sRGB colour space ( green triangle ) . Although the U2412M is now quite dated , this sort of colour gamut is rather typical for current models with 1920 x 1080 ( Full HD ) resolutions in particular . <p> Sony have actually changed their ' suggested ' Judd Offset values two ( or is @ @ @ @ @ @ @ @ @ @ a Judd offset just does n't work . And the reason for that , as stated on the website page Florian linked to , is now believed by many display manufacturers to be due almost exclusively due to the amount of spectral energy emitted in the blue region below 460nm by OLEDs . <p> Normal LCD displays , with CCFL or LED backlights , actually calibrate almost perfectly to the numbers with a Spectro , or Colourimeter with a well matched offset matrix . <p> If so calibrated , they will be very accurate , and not too blue when displaying a grey scale/white , and can be used to perfectly set the correct offset for an OLED - including the Sony displays . <p> The offset value thus defined will not match the Sony suggested Judd offset , but will generate a very accurate OLED calibration. 
@@44332622 @4332622/ <h> wrong transform properties after FCPX XML Import <p> After importing an XML from FCPX 10.3 into Resolve 12.5 , I have this problem where about 50% of the clips have their PTZR parameters not translating properly . <p> I had to postpone my delivery because of it and it is a real PITA . <p> I analyzed every clip of my music video to check when it happens and when it does n't . I was not able to see any pattern . It can happen on any type clip ( regular , compound , multicam ) , on any parameter ( but mostly position ) , on clips that were originally part of a secondary storyline or not , ... <p> Even more surprising : sometimes in Resolve there are keyframes appearing even though nothing has been changed at all on the clip in FCPX ! ! ! ! <p> Another weird thing : sometimes for a clip , the framing is respected but the position values in Resolve seem to be about half ( but not exactly ) of the values in FCPX . At first @ @ @ @ @ @ @ @ @ @ a different resolution in FCPX &amp; Resolve , but no they are the same . And the clips are matching it too . Then I thought maybe it 's because they are zoomed but I have it on clips that have been zoomed , and also on clips with no zoom . <p> Additional info : In FCPX , all of my spatial conform is set to the default : fit . In Resolve 's inspector too : Retime and Scaling &gt; Scaling &gt; Fit ( default ) <p> As workarounds I thought of the following but none seems ideal ... <p> Workaround 1 : having the whole project exported clip by clip from FCPX to bake in all transformations . But since 80% of the transitions are cross dissolve , this is not going to work easily . <p> Workaround 2 : I select all clips in my Resolve timeline and then turn off the transformations in the inspector to grade the original clips . Problem : then we will need to replace each and every clip in FCPX with the graded ones manually . <p> Premiere to Resolve @ @ @ @ @ @ @ @ @ @ but that 's caused by incorrect scaling settings in either Premiere , Resolve , or both . 43971 @qwx453971 <p> hi jussi , what would be the workaround for that kind of problem ? i frequently encounter the same kind of problem as ron fya ( premiere essentially but also fcp X ) and allmost only on music video , wrong sizing but also clips off by a few frame . several hours of conforming for a five minutes video is absolutly maddening ... i 've taken some time to analyse the premiere projects but could'nt find any clue about the issue . <p> what would be the workaround for that kind of problem ? i frequently encounter the same kind of problem as ron fya ( premiere essentially but also fcp X ) and allmost only on music video . a few hours of conforming for a five minutes video is absolutly maddening ... <p> hi jussi , what would be the workaround for that kind of problem ? i frequently encounter the same kind of problem as ron fya ( premiere essentially but also fcp X ) and @ @ @ @ @ @ @ @ @ @ clips off by a few frame . several hours of conforming for a five minutes video is absolutly maddening ... i 've taken some time to analyse the premiere projects but could'nt find any clue about the issue . 43971 @qwx453971 <p> The main issue is the auto-scaling : <p> In Premiere never use " Scale to frame size " ( I think it 's on by default in the preferences ) . instead use " set to frame size " , so the scale value will change in the clip effects ( and will be in the XML ) . <p> And with Resolve use " center crop with no resizing " as the input preset , so Resolve will not do auto-scaling , but uses the scaling from the XML. 
@@44332623 @4332623/ <h> Film Studio Looking to Partner with Post Production Company <p> Another interesting business proposition on Craigslist . This one is a little more succinct than some of the others I 've seen . Also , as much as there is a lot of risk doing anything like this , there is a part of me that wonders if these sorts of deals will actually become more commonplace in 5-10 years . <p> We just opened a Film Studio ( we are not doing our own Distribution yet but heading that way ) and we have a slate of 3 films scheduled for the next 12 months . A horror , a thriller and a comedy . This first slate of three films are micro-budget at $110,000 each . <p> Each movie has been carefully drafted , character driven material , with a powerful message for humanity underneath each genre . We are committed in leaving a legacy to humanity , not just shooting movies and having fun ( which that is an important part of it as well ) . <p> We will be shooting the films for @ @ @ @ @ @ @ @ @ @ $40,000 will be coming from a partnership with a Post House . <p> We are looking for a Post House ( which has editing , sound , composing , coloring , VFX and deliverables ) to come as investor for these services ( valued as an investment of $40,000 ) , once the movie does its money back ( the $110,000 ) , the Post Company will get its $40,000 plus 20% ( ROI ) plus 18 points on the Movie LLC for perpetuity . <p> We do n't have to do the three films together but start with the first one , the Horror , and go from there . <p> I know this is a non-conventional way to do things , and we are committed in building relationships , in shooting this slate of 3 films and move on to our next slate of 6 with bigger budgets . <p> We are located at one of the major Studios in Melrose , Hollywood , if you are interested in hearing more , please email me and we can set up a meeting and go from there . <p> @ @ @ @ @ @ @ @ @ @ while now , so far i 'm not aware of any 100% investment deals being struck other than MoW 's where the return is pretty solid and timeframe known , so the only bet is limited to distb outside the main territory . <p> I know of someone who offered camera and post-services on about 5 films for backend points etc similar to what 's being offered here on indie budget films between $250k-1mil films with name actors . He never received a penny back . Because , the distributor always takes any/all profits and washes it away in " Marketing " and packaging costs and simply because most indie films never make money or take a VERY long time to make small money . Not to mention $40,000 is nothing for editorial , sound finishing , VFX and final color + deliverables . Hate to be the guy that says do n't do it , but do n't do it . <p> Looks like a huge risk to me . I 'm reminded of what happened to Digital Domain when they " invested " in certain films by agreeing @ @ @ @ @ @ @ @ @ @ then when the films ' schedules got pushed , they lost lots of money in having to keep people on staff during the hiatus . This is a losing battle . <p> The other problem I see is that post is actually four things : editorial , VFX , color , and sound . You might be able to find ( say ) an editor willing to cut their price in half in return for 5% of the gross , but not sound . And I do n't know of a colorist who would do this kind of work willingly . <p> I think a better approach for the company would be simply to raise more money and have a realistic budget where the crew can make a living , vs. living on a promise and a prayer that may never come true . <p> I 'm looking for someone to invest in my personal transportation . I need a Bugatti Veyron , but all I have now is US$ 10K . If I make enough money in the next couple of years , I 'll pay back the remaining @ @ @ @ @ @ @ @ @ @ access to the car for 10 full days every year . Any takers ? ... <p> Seriously , in my thirty something years in the business I have yet to meet a new client with low budgets that promises to pay me top rates in the future , if I work for peanuts now , that does n't run to the next high end facility as soon they get higher budgets . Once one agrees to work for cheap , one becomes the cheap guy , no matter how good the work performed . Now , it 's a different story if I understand this and decide that the project is worth investing my time on for one reason or another , anyway . But expecting to make more money or getting paid more in future projects in exchange for cheap labor is something that I 've yet to see happen . Additionally , my experience in these situations is that when one works for next to nothing , the amount of work they expect is usually insane . <p> I agree with Marc that if they expect top quality @ @ @ @ @ @ @ @ @ @ more money . After all , if they call themselves a " Film Studio " , they should definitely be able to do better than raise US$ 70K per film . <p> This is being done quite a lot , and has been for a while . Molinaire in the UK will quite often be a producer of films it works on and takes it 's fee from any " profit " this method was used on movies like " Moon " and " The King 's Speech " . This works only some of the time because as we all know the level of creative accounting in the movie biz means no movies ever make any money , from Avatar to a low budget indie . Plus as mentioned before $40k is rock bottom for post on a movie . I would not touch this with a bargepole . <p> This seems to be a trend world over at the moment . Its a sad day when post houses have to pay to get the work . Also means smaller outfits like mine find it hard to compete . @ @ @ @ @ @ @ @ @ @ certain local outfit have invested 30k into several productions . Personally I know they are going to try and charge them for every coffe and change they make , to claw back the money . In the end everyone will walk away unhappy . So I 'm taking a seat and playing the long game . <p> This seems to be a trend world over at the moment . Its a sad day when post houses have to pay to get the work . 43971 @qwx453971 <p> I can recall a time when certain LA post houses would lobby heavily to get a big studio or network contract , up to and including a bribe to the person making the decision . It was n't always cash -- sometimes " favors , " vacations , and expensive gifts ( without receipts ) were offered . This worked great until that person left the studio . <p> Often , great equipment , great people , and competitive prices alone were not enough . Sometimes , it boiled down to nepotism or human connections : the work comes in because somebody 's @ @ @ @ @ @ @ @ @ @ is the top exec in charge . Sadly , it 's not an even playing field out there . <p> I can recall a time when certain LA post houses would lobby heavily to get a big studio or network contract , up to and including a bribe to the person making the decision . It was n't always cash -- sometimes " favors , " vacations , and expensive gifts ( without receipts ) were offered . This worked great until that person left the studio . <p> Often , great equipment , great people , and competitive prices alone were not enough . Sometimes , it boiled down to nepotism or human connections : the work comes in because somebody 's brother-in-law is employed there , or somebody 's best friend is the top exec in charge . Sadly , it 's not an even playing field out there . 43971 @qwx453971 <p> as I am sure you know , you 've just described a large portion of Earth . 
@@44332625 @4332625/ <h> Sharing/Moving a project between two systems <p> I 'm on Davinci Resolve 9 and want to be able to move a project ( project file and all coloring/work done ) between computers . <p> i did this once before , I think by copying the project file and the user database , then pasting it into the database on the other computer . However , is there a more streamlined way like from in resolve where you can export the project file and the looks/work done and open it up on another system ? <p> There is , I just had to do it on a few projects once I got my new computer system , its very easy . <p> Go to your old system , fully open up your project then go to File &gt; Export Project . Tick all the boxes and save anywhere you want . It basically creates a contained file with all relevant material ( these files can be quite big , one for me was 80MB ) . Copy this DRP file onto your new computer . <p> Open up your @ @ @ @ @ @ @ @ @ @ Manager make a new project timeline ( create a folder first if you want , such as Music Videos or such for organisation ) . Right click inside it and select import and navigate to your exported project file save . It will then basically copy and paste all material to your DaVinci 's default scratch disk ( unless you moved it prior ) . Alternatively you can open a new fresh project timeline then File &gt; Import Project . <p> Once done that open up the project and then go to media page to refresh the location of the media as you will have moved it also probably to your new system unless its on a network or something . 
@@44332626 @4332626/ <h> Neutral Lighting For Grade Suite <p> We are setting up a low end grading suite at work , and i 'm working on sorting the environment of the room out . <p> I 'm looking at getting D65/6500k led strip lighting and putting it on a dimmer to control it , do you guys have any idea about the restrictions i should be aware of ? I.e - i know it needs to be very low foot lamberts to not affect the monitor viewing . <p> I 'm planning to have a lamp for clients at the back of the room and an led strip backlighting the monitor on the wall , but i will block the front of it so there is no visible light only soft lighting . <p> It 's important that the light is full-spectrum , whereas LED is usually has a very spiky spectral response . Even though it may measure as D65.Usually CCFL tubes are used with a very flat spectral distribution , ideally upwards from 95 CRI . <p> SMPTE suggests light levels of the bias light around 10% of peak level of the @ @ @ @ @ @ @ @ @ @ to set-up around a room wherever you need light . I have two behind my client display on the wall , one behind my desk for GUI/ Ref , and one in an opaque/ frosted white lamp for fill light around the bay so my clients do n't sit in total darkness . <p> Somethings to watch out for : -Standard CFL bulbs do n't work with dimmers . They will dim a small amount and then go out completely . You can buy dim-able D65 CFLs , but the cheap ones ( $12-15 ) will often flicker or buzz . If you are willing to pay about $30 per bulb , you can find a D65 that dims nicely like a tungsten . <p> -The CRI index on true reference lighting like the Ideal-Lume is usually up over 90 . I 've used the Ideal-Lume lamps , and while nice , they are costly to use everywhere you like . To me , they also have a slight green color cast I find un-appealing . The CRI on the CFLs like the one above is usually somewhere in the @ @ @ @ @ @ @ @ @ @ reading , especially in alternative lighting forums for things like photo imaging , fine art , printing , bio-science , etc , and since we are using these lamps as bias lighting , and not as a reference illuminant for say , validating a Picasso , we can get away with a lower CRI . <p> Lighting is subject to the rules of good , fast , and cheap like everything else , but if you need to get it done on a budget , grab a few CFLs and see how they work . If you find you need something else , you can always throw them in your bag for when you 're out freelancing . <p> There is a cheap LED product on Amazon that sounds great , but it 's not . LONG ... Although they claim " professionally calibrated 6500K " , I metered it closer to 8500K , with a great deal of red/green shift from end to end . The response I got from Antec customer support is that none of their units are calibrated or tested with color meters , and they @ @ @ @ @ @ @ @ @ @ up sticking it on my living room TV because it 's better than nothing and it 's powered by the TV 's USB port . I wish I had the legal skills to go after them for false advertising ... <p> I 've been in contact with Verivide who have 98 CRI D65 tubes . They have sales points in the UK too , if you contact them at sales@verivide.com they 'll point you to a sales point . It 's not going to be as cheap as a regular tube , but substantially less expensive than the Ideal-Lume kits . <p> Hey guys thanks for the info . On somewhat the same topic . I 'm in the process of putting together a multi use finishing suite as well and in conduction with lighting what is the consensus on paint color for the walls ? Is a middle grey recommended or something darker ? Any specifics you guys can share ? <p> Behind your monitor the SMPTE recommendation is 18% grey ( middle grey ) lit to ca. 10% of peak white level of the monitor ( with D65 @ @ @ @ @ @ @ @ @ @ necessarily have to be grey , although you may want to avoid strong colours , and grey ca n't hurt . If you also want to use the room for DI grading with a projector , darker is better . Black would be best but dark grey would also be excellent . <p> I 've recently been speaking with Paul at Verivide and not only are we going to be getting some D65 tubes from him but he has also hooked us up with a 5 litre tub of their N5 grey emulsion paint . Just painted our new suite with it and it looks great . They do n't advertise it in that sort of quantity on their website ( the paint is usually intended for colour critical viewing booths for textiles etc ) but they can supply it if you ask them . He even brought it down to us personally as he was in London anyway , so we saved on shipping costs ! So if there is anyone setting up a suite in the UK I strongly recommend giving Verivide a call . I showed him @ @ @ @ @ @ @ @ @ @ he said he 'd check it out so that they can be better clued up on what the colourists need . <p> I have n't purchased them yet , Paul 's working out a quote based on making a custom housing with a dimmer switch ... I think from looking at his prices though that the individual tubes on their own were pretty inexpensive , I think a 600mm one was around -20 , but do n't quote me on that ! I 'd recommend just giving them a call , they 're very helpful . <p> FYI , you can buy similarly excellent tubes from JustNormlicht and GTI . I do n't know their UK addresses but I 'm sure the internet does . All three manufacturers produce the best flouorescent D50 and D65 tubes you can buy . And yes , they are all a bit pricey - but worth it . <p> Oops ! Hit Return too early . I was going to say that I 'm moving rapidly away from fluorescent bulbs towards LED strips . You can get " 6500K " strips with a pretty @ @ @ @ @ @ @ @ @ @ price . But in fact , as a monitor white point reference , the light source 's CRI is n't so important as its correlated color temperature . So I just ordered some " temperature-tunable " LEDs . Will be interesting to see if they let me match the ambient lights by eye to a monitor that 's been calibrated to D65 with a decent spectro . <p> I 've setup my grading room with the proper fluorescent 65K lights long time ago . But sometimes I wonder why we use 65K lighting if our audience is almost always watching their TV under tungsten instead of daylight . There must be a good reason ... 
@@44332628 @4332628/ <h> AMERICAN GODS on Starz <p> Hokey smokes ... it 's only May , but I think we have a nominee for the best show of the year right here ! <p> AMERICAN GODS is an amazing , strange , bizarre fantasy series the likes of which you have never seen . Almost indescribable , many layers of story and characters and the best damned dialogue I 've heard in a long time . Brilliant writing based on Neil Gaiman 's fantasy novel from about 15 years ago . Never a dull moment . <p> Stunning photography , great performances , and like nothing else on television . Very , very rated R. Watch it and prepare to have your mind blown . ( And congrats to Dave Hussey of CO3 on the color -- the show has some of the most stunning and original looks around . ) <p> If anybody is looking for inspiration on new and different kinds of color and mood and contrast , look no further . It 's a really beautiful show -- another one that changes aspect ratio depending on when and @ @ @ @ @ @ @ @ @ @ year 1000 and Viking explorers to modern times and prisons and deserts and airports and rivers and cities to weird artificial sci-fi worlds dominated by robots &amp; machines -- just in the first hour -- has got my attention . <p> You should take a look how they even make creative use of the letterbox at the end of the vikings scene . Georeous ! 43971 @qwx453971 <p> Yeah , that seems to be a thing now : Legion also went back and forth from 2.40 to 1.78 and back again , depending on what " world " they were in at the moment . I seem to recall the fantasy worlds were widescreen but the " real " worlds were normal HD . American Gods is all over the place , but by god , it 's very entertaining and never boring , which is high on my list of what I want to see in a TV series . <p> It reminds me a bit of the show Hannibal , so Bryan Fuller probably has a big influence on the look . It 's too much in my @ @ @ @ @ @ @ @ @ @ applaud anyone willing to push the boat out and risk new things even if the results are n't always agreeable . <p> My girlfriend insisted we subscribe to Starz just because this show was coming out soon ( she 's read the book ) . I love how saturated , sharp , and contrasty it is . I was wondering what other ' critical eyes ' would think . I 'm way more a fan of this sort of look than of the washed-out almost Log look that was popular for a while I could n't stand the Samsung phone commercials from a few years ago . <p> I like a lot of aspects of the show , as a whole - like that post is way over done in general . But I do n't like the overdone/over the top midtone contrast - I just do n't like the look that it brings . Also I 'm not quite sure about the CGI being obviously fake . Like the story structure though . <p> I like a lot of aspects of the show , as a whole - like @ @ @ @ @ @ @ @ @ @ I do n't like the overdone/over the top midtone contrast - I just do n't like the look that it brings . Also I 'm not quite sure about the CGI being obviously fake . Like the story structure though . Do you guys do n't mind the midtone contrast ? 43971 @qwx453971 <p> It looks fine to me . I do n't have any checkmarks where I say " a shot must look like THIS " in order to look good or bad . I could n't give a crap about midtone contrast -- I just look at the whole picture and judge it in context . On that basis , to me it 's a beautiful-looking show . I do n't dispute that it looks different from everything else on television . They made some bold creative choices , but there 's nothing wrong with that . <p> I like a lot of aspects of the show , as a whole - like that post is way over done in general . But I do n't like the overdone/over the top midtone contrast - I just do n't @ @ @ @ @ @ @ @ @ @ not quite sure about the CGI being obviously fake . Like the story structure though . <p> Do you guys do n't mind the midtone contrast ? 43971 @qwx453971 <p> Yes and no . I 'm not a fan of that much contrast but I do appreciate a show that you can identify by its look . It takes some balls to go that extreme . 
@@44332629 @4332629/ <h> GUI Monitor - Size/PPi/Distance sweet spot ? <p> It seems like I 'm stuck in the past when it comes to GUI Monitors - all of my monitors being " standard definition - 1920x1200 " ( but who knows what 's really standard nowadays - for many of You 1920x1200 is probably ancient technology ) <p> I 'm long overdue for an upgrade . <p> Moving aside intenet PPI calculators and guidelines for viewing distances . From Your experience what would be the the sweet spot between screen size and resolution for viewing " Davinci Resolve-like " GUI ? Taking into consideration that you are doing it for the better part of the day/week/year . <p> For example - typical calculator will tel me that optimal viewing distance for 27 " QHD display 2560x1440 is about 0.79m ( 2.6ft ) But how does this translate to GUis with small digits in them , loads of small icons , etc etc . Would i even see anything on it ? I know that for Grading this in not as big of a deal - because You mainly look at @ @ @ @ @ @ @ @ @ @ also use Software like Premiere or Fusion for that matter , witch means lots of Gui eyeballing . <p> I just got the 32 " 4k Asus PA329Q . And it 's the nicest GUI monitor I 've ever used ( by a fair margin ) . However I 've got mine scaled down to 2560x1440 because it 's almost impossible to read text anyway at the screen 's native 3840x2160 . <p> I have n't tried Resolve , Premiere or FCPX at 4k yet , but find them the easiest to use they 've ever been , running at 2560x1440 at 32 " - for the first time I feel like I have comfortably enough space to work with and see everything I need . <p> I just got the 32 " 4k Asus PA329Q . And it 's the nicest GUI monitor I 've ever used ( by a fair margin ) . However I 've got mine scaled down to 2560x1440 because it 's almost impossible to read text anyway at the screen 's native 3840x2160. 43971 @qwx453971 <p> Did You buy this monitor just for Gui @ @ @ @ @ @ @ @ @ @ do some other stuff ? I guess what I 'm asking is - is it worth buying 4K if You have to scale it down anyway ? Since I 'm on Windows here I have to admit what is good for resolve GPU wise is good for gaming also - and sometimes I take advantage of that , but i need to buy at least two large Gui monitors . and 4k i considerably more expensive . <p> 2560 ( or 2550 ) x 1440 works very well for a single-screen Resolve GUI , at least to me . The lettering is not too small , though I do have to pop up the text size a notch for web browsers. 43971 @qwx453971 <p> Mark how far are you sitting from You monitor and how big it is ? . Right now I 'm about 1meter away from my 24 " HD Monitors and sometimes i actually have to lean forward to see more clearly : / . This of course starts to happen more often after couple of hours , when my eyes start to get tired . <p> Did You buy @ @ @ @ @ @ @ @ @ @ game on it ? or do some other stuff ? I guess what I 'm asking is - is it worth buying 4K if You have to scale it down anyway ? Since I 'm on Windows here I have to admit what is good for resolve GPU wise is good for gaming also - and sometimes I take advantage of that , but i need to buy at least two large Gui monitors . and 4k i considerably more expensive . 43971 @qwx453971 <p> I do n't game or watch movies or anything on it , so it 's just for GUI purposes really . With the added advantage that it can do 100% SRGB/Rec709 , and most of P3 ( not that I think I 'll ever use that mode ) . My main consideration was simply getting a screen that 's at least as nice to look at as my 27 " iMac ( and ideally had a little more screen real estate ) . <p> From the research I did before making my choice ( and it took a frightful amount of reading , watching and spreadsheets - @ @ @ @ @ @ @ @ @ @ rates and the Asus tops out at 60Hz , so I do n't think it 'd be ideal for that . <p> I sit about 70cm back from mine , and that feels pretty comfortable . <p> As to the value of 4k , the fact that I 'm scaling mine down would suggest to me that it really is n't necessary ( at least for screens up to 32 " in diameter ) . 
@@44332630 @4332630/ <p> An exciting opportunity exists to join the Physical and Post Technology team as a Senior Media Systems Engineer . Our engineering team is expanding and we are looking for talented engineers to support Industrial Light &amp; Magic and Lucasfilm technology based at the Presidio campus in San Francisco . <p> The Senior Media Systems Engineer is responsible for front line maintenance of all department related computer and A/V equipment . The engineer is also required to assist the director with the planning , designing , procurement and implementation of media technology systems . Experience with digital cinema projection is a requirement . <p> - Installation , maintenance and troubleshooting of equipment associated with various technical and operational departments throughout ILM and LUCASFILM - Frontline maintenance and troubleshooting of all physical and post production related equipment including professional and consumer grade AV equipment , Avid editorial systems , digital cinema projectors , monitors , video teleconference systems , AV control systems , motion capture equipment , video cameras , media streaming appliances , video/audio routers , theater control systems etc. - Maintenance of media server hardware and associated software @ @ @ @ @ @ @ @ @ @ - Create and maintain documentation using various tools including Jira , Confluence , Microsoft Office , CAD , Visio etc. - Perform hardware , firmware and software upgrades - Provide basic technical training to staff as required - Ensure assigned projects are delivered on time , within budget and to the agreed standard of quality and functionality - Efficient workflow and pipeline design for post-production services - Liaise with IT engineers to maximize system performance and reliability - Perform programming database modifications to video and KV M routing systems - Prepare high-quality written reports and presentations for management - Create and update control system databases for various automation systems - Participate in off-hours maintenance , upgrades and installation for systems when required - Participate in On Call ' rotation - Work to ensure best security practices are met and maintained - Conduct research and evaluate new technology and trends to assess their impact on current and future workflows 
@@44332631 @4332631/ <h> Black levels reading <p> I have a lot of monitors and I usually calibrate them once every two months or so and when you compare them one next to the other everything looks fine except the black levels . Obviously technologies differ and some monitors look way different than others but still I was wondering if there was a way to read black levels faster . <p> Like some sort of a simple patten generator of 5 patches of black installed in each machine and a i1 display pro hooked to an apple mini ... and you could go walking around like a doctor with a stethoscope checking how " healthy " the black levels are . Ok , I just let my imagination fly but I hope you get me . <p> You can quickly check black levels with a SMPTE test chart , like the one in the toolbox generator in Resolve . There are a couple of thin black bars down the bottom , the darkest of which is sub-black and should n't be visible . <p> Lift the monitor brightness until you see it @ @ @ @ @ @ @ @ @ @ You 'll need to do this with the monitor in the actual room &amp; lighting setup where they 're used . <p> You can quickly check black levels with a SMPTE test chart , like the one in the toolbox generator in Resolve . There are a couple of thin black bars down the bottom , the darkest of which is sub-black and should n't be visible . <p> Lift the monitor brightness until you see it , then dial it down until it just disappears . You 'll need to do this with the monitor in the actual room &amp; lighting setup where they 're used . 43971 @qwx453971 <p> That 's a nice idea for leveling your UI display . <p> But I think you should n't touch your initial calibration of your grading monitor at all before measuring your true black levels . And then a real calibration with a 3D LUT would serve you better then eyeballing ... <p> True , but if you 're already checking the calibration of the monitors every month or two like Julian said , it 's a quick way to @ @ @ @ @ @ @ @ @ @ You 're only adjusting the brightness dial , so if you remember it 's parked at , e.g. ' 50 ' , and set it back , you do n't have to throw off the calibration . <p> We 've had clients &amp; their minions before on-set or in post who 've never heard of calibration . Their first instinct is to swagger over to a monitor when no one 's looking and start playing with brightness &amp; contrast controls until it looks ' nice ' . These people are a pain . 
@@44332632 @4332632/ <h> help me decide <p> i 'm using an Apple cinema display right now calibrated with DataColor for small colouring work I get , but I 'm looking at getting a screen that offers decent colour and is n't so glossy , but I also ca n't afford a Flanders or Eizo at the moment . Some of my spare money is going into a surface panel and Santa is n't coming to town for a while . <p> From these monitors , on paper the ASUS actually looks pretty decent as far as colour goes . The Dell is 5K and double the price but I would be fine with HD for now . <p> Anyone have any direct experience with any or all of these ? Frankly they all look pretty good to me . Any others I should be looking at in 27-30 " range ? <p> I have tried the high-resolution route , including owning the LG for a while , and found there are just too many apps that are n't optimized for it . The additional real estate is nice , but controls @ @ @ @ @ @ @ @ @ @ just too difficult to work . 2560x1440 is a nice compromise between space and resolution , so that leaves the ASUS and NEC . Of those two , the NEC is going to give you the nicest end result . The ASUS many be nice as well , but I doubt it 's color management is as complete as the NEC , so that would probably make it your second option . <p> I would seriously consider getting a decklink mini monitor , an eecolor lut box and an lg 1080p OLED TV and talk to light illusion about calibration tools . Keep your cinema display for GUI . Lut box is $300 and can use free Argyll to calibrate if you ca n't stretch to light illusion just yet . 40 " 1080p OLED is around $1000 . And much more impressive . There are specials on for light illusion packages though if you want a better probe , which you do ! I think the eecolor is an interesting option for GUI monitor as well as you could profile a standard NEC for less money than the spectraview @ @ @ @ @ @ @ @ @ @ display output . <p> Is there no 10-bit available with El Capitan and Davinci , even though El Capitan is suppose to provide 10-bit capability ? 43971 @qwx453971 <p> For purposes of color correction , it makes no difference what the OS and GUI display are or are not capable of , 10-bit or not . What you need for color correction is a TV video signal on a calibrated TV , not a computer signal on a computer monitor . <p> Devices like the BMD Mini Monitor and others convert the signal out of your color corrector program into a TV video signal ( such as ATSC or PAL ) encoded as Y'CbCr with timecode and all that jazz , that you then send into a calibrated monitor that is capable of receiving a TV signal in all the variations that a TV signal could be in . Typical computer monitor can process only computer signals , not TV signals . <p> If you were to hook up a TV to the HDMI port on your computer , it would show up as a second display , extending your @ @ @ @ @ @ @ @ @ @ of your graphics card , manipulated in divers ways by the OS color management . <p> When you hook up a TV via a Mini Recorder or somesuch , the TV will not be seen by the computer as another desktop monitor . In fact , nothing will appear on the TV at all until an application like an NLE or color corrector sends out a TV signal to it . This is sent out as a TV signal through the monitoring device , not the graphics card outputs and not influenced by OS color management . <p> So whether or not El Capitan is 10-bit is irrelevant for video color correction . It 's relevant only for computer **31;322;TOOLONG . <p> I am a TV . I assume this content is video levels . I stretch it out to full range . I have been calibrated so you ca n't see below 16 or above about 250 depending on how forgiving you are . I usually get things wrong and interpret signals incorrectly . <p> I am a piece of video content . I look good on television . @ @ @ @ @ @ @ @ @ @ might look good on a computer , but probably I will look washed out and weird . <p> One day this madness will be history . If we only work together to end it . <p> That is just bad calibration - it is not ' colour squash ' . With correct calibration the images would look exactly as they should . The ' only ' potential issue is banding/blocking due to on--screen image granularity being too poor . And that is down to compression and low bit-depth . <p> No . It is good calibration. and it is correct interpretation of video and data levels . It is just most content is not viewed on a television device , so probably displays full range images that will be incorrect as long as we continue to grade everything in video levels . <p> No . It is good calibration. and it is correct interpretation of video and data levels . It is just most content is not viewed on a television device , so probably displays full range images that will be incorrect as long as we continue to grade @ @ @ @ @ @ @ @ @ @ not found that to be the case . I 'll occasionally watch content on an sRGB computer monitor , and then on a calibrated Rec709 video monitor , and neither is so far out that the image does n't look reasonable . I suspect you 're seeing an extreme problem that has another cause . <p> No Dell display can be directly calibrated with LightSpace - they do not have the necessary capabilities built into the display . ( NEC displays with 3D LUT capability do , and can be directly calibrated with LightSpace . ) <p> But ANY display can be ' calibrated ' with LightSpace CMS if you use an external LUT box , or the Output LUT capability in most grading systems , such as Resolve , Mistika , etc. 
@@44332633 @4332633/ <h> Imitation news footage <p> Something I 've started noticing in films is that when they insert a fake news broadcast the footage is still shot using a cine camera . Here 's an example from Gone Girl ( taken from bluscreens.net ) of what I mean ; <p> Because it so obviously looks out of place ( especially when it 's on an ancient CRT monitor in the corner of a hospital or something ) ; why are scenes like this still lit/shot so cinematically , or why do n't they just shoot with a news camera in the first place ? <p> Another example from the opening scene of Edge of Tomorrow ; the shots of Brendan Gleeson look so out of place with the archive footage they 've used elsewhere in the scene . <p> I know the vast majority of the audience are n't going to notice but it just seems to me that when so much time/money is spent trying to create as immersive an environment as possible something like this so often looks out of place . Curious if anyone else has noticed @ @ @ @ @ @ @ @ @ @ on a project and how you approached it . <p> I have worked on films ( Swing Vote was one within the last few years ) where the production actually hired an ENG TV crew to come in and light the news set and actually use real news cameras . In that case , they did shoot it in HD 1080i , but they made sure it was flatter and had some visible scan lines . <p> David Fincher is an extremely technical , hands-on director who knows lenses and photography about as well as anybody I 've ever seen . I 'd say if he wanted his fake Gone Girl news footage to look that way , he did it deliberately . One can always argue that a movie is " heightened " reality , so it does n't necessarily have to be 100% identical to the real thing . <p> Gone Girl made about $370M worldwide and got great reviews , so I suspect Mr. Fincher does n't care what anybody thinks of the fake news footage in the film . I did n't like the film overall @ @ @ @ @ @ @ @ @ @ of view , it 's technically a brilliant movie in many ways . The amount of post repositions in the film are astronomical . <p> For U.S. productions shooting with a typical newsroom camera would entail 1080 59.94i which does not scan rate convert easily to 24p . The resulting artifacts would be more of a distraction to the viewer than what distracts you in the above example . <p> Even before digital acquisition when TV content was filmed off real TVs they would scan rate convert fake TV for on-set playback using a variety of methods in order to avoid visible refresh issues . In most cases it involved slowing down refresh and genlocking TVs and VTRs to the same sync generator feeding the camera . Such systems were really 24p playback systems before HD . So , in a sense , fake TV content was nearly always using the same frame rate as the film camera . <p> I have worked on films ( Swing Vote was one within the last few years ) where the production actually hired an ENG TV crew to come in and light @ @ @ @ @ @ @ @ @ @ In that case , they did shoot it in HD 1080i , but they made sure it was flatter and had some visible scan lines . <p> David Fincher is an extremely technical , hands-on director who knows lenses and photography about as well as anybody I 've ever seen . I 'd say if he wanted his fake Gone Girl news footage to look that way , he did it deliberately . One can always argue that a movie is " heightened " reality , so it does n't necessarily have to be 100% identical to the real thing . <p> Gone Girl made about $370M worldwide and got great reviews , so I suspect Mr. Fincher does n't care what anybody thinks of the fake news footage in the film . I did n't like the film overall , but I think from a photographic and VFX point of view , it 's technically a brilliant movie in many ways . The amount of post repositions in the film are astronomical . 43971 @qwx453971 <p> Your Swing Vote example is how I 'd have done it . It @ @ @ @ @ @ @ @ @ @ try make a cine camera look like news footage when you can just shoot with a news camera in the first place . But I had n't considered the frame rate issue Igor . We had a news show in college which we ran as a senior year class so I 'm just used to seeing the small chip look in a news scenario . <p> However it should also be said that over here we have n't glamourised the news and how we broadcast it as much as I think the American networks have . This - - is how the talking head style of interview looks over here maybe the examples I cited are closer to reality than I 'd realised ; we do n't have those kind of **30;355;TOOLONG shows over here . <p> Anyway I 'm a big Fincher fan and thought Gone Girl was great ; it did n't affect my opinion of the film but the immersion was broken when it happened and I remembered I was watching a movie , albeit momentarily . I read/watched all the interviews with him and Jeff Cronenweth @ @ @ @ @ @ @ @ @ @ all ; idk if the whole splitting shots and using performances from different takes had been done before but I remember reading about it first with them and I think I read in an interview with John Seale that they did a similar thing on Fury Road which surprised me because I ca n't remember many shots where the camera was static enough for them to be have been able to do that . <p> Maybe Mr Vertovec will appear as he did in the Apple Watch thread and clear up whether it 'd ever been a discussion . <p> On any of the shows I 've finished , and in most cases , it comes down to M.O.N.E.Y . Crew , talent and " Cine " cam are already paid for , framerate issues in the US also play into it . Have " TV 'd up " footage that was used as screen replacement in some cases to help sell the illusion . <p> For U.S. productions shooting with a typical newsroom camera would entail 1080 59.94i which does not scan rate convert easily to 24p . The @ @ @ @ @ @ @ @ @ @ viewer than what distracts you in the above example . 43971 @qwx453971 <p> My memory is they wound up processing the whole sequence through a Snell &amp; Wilcox Alchemist to remove the extra fields and to undo the motion artifacts . I seem to recall the end results held together fairly well , and it was only about 90 seconds out of the entire movie ( shot on film ) . <p> My memory is they wound up processing the whole sequence through a Snell &amp; Wilcox Alchemist to remove the extra fields and to undo the motion artifacts . I seem to recall the end results held together fairly well , and it was only about 90 seconds out of the entire movie ( shot on film ) . 43971 @qwx453971 <p> we have Snell &amp; Wilcox Alchemist here its pretty great that just the kind of problem it excel at <p> as for inserting TV in to films i still think Paul Verhoeven nailed it by taking the whole screen as the TV and sort of lowering the quality a bit and dress up the footage with bad @ @ @ @ @ @ @ @ @ @ as a way to advance the narrative with channel 9 mediabreak or put the brakes on and give the viewer and moment to pull in the story but cutting to a commercial , there is an interview with him some where were he linkens it to a mondrain line painting with black links between the color ... nice <p> it also apparently confuse the the LA times film critic who thought the projectionist had put the the wrong reel on <p> any way just remember with channel 9 's media break " its bad news with a smile " <p> Something I 've started noticing in films is that when they insert a fake news broadcast the footage is still shot using a cine camera . Here 's an example from Gone Girl ( taken from bluscreens.net ) of what I mean ; <p> Because it so obviously looks out of place ( especially when it 's on an ancient CRT monitor in the corner of a hospital or something ) ; why are scenes like this still lit/shot so cinematically , or why do n't they just shoot with @ @ @ @ @ @ @ @ @ @ example from the opening scene of Edge of Tomorrow ; the shots of Brendan Gleeson look so out of place with the archive footage they 've used elsewhere in the scene . <p> I know the vast majority of the audience are n't going to notice but it just seems to me that when so much time/money is spent trying to create as immersive an environment as possible something like this so often looks out of place . Curious if anyone else has noticed this and/or if you 've encountered a situation like this on a project and how you approached it . 43971 @qwx453971 <p> My understanding , and I may be wrong , was that the " national network " had money so they had nicer cameras ( maybe not REDs , but nicer ) . I know the local St. Louis stuff was shot with a legit local ENG crew . If you go back and watch Nicks TV interview or the news stuff with Amy at the end , hopefully you can tell it 's a real news camera ( just shooting at 24P ) . @ @ @ @ @ @ @ @ @ @ crew even helped light nicks interview too , but I may be wrong . <p> My understanding , and I may be wrong , was that the " national network " had money so they had nicer cameras ( maybe not REDs , but nicer ) . I know the local St. Louis stuff was shot with a legit local ENG crew . If you go back and watch Nicks TV interview or the news stuff with Amy at the end , hopefully you can tell it 's a real news camera ( just shooting at 24P ) . In the name of authenticity , I think the local crew even helped light nicks interview too , but I may be wrong . 43971 @qwx453971 <p> Hey Ian thanks for the insight . I know it was super nit-picky but it 's always something I notice whenever it crops up . Again , our news shows over here do n't have as much money behind them so maybe things actually are starting to look like that in the States . The BBC 's got rid of studio cameramen altogether which @ @ @ @ @ @ @ @ @ @ a directing/vision mixing error ) are happening . <p> I only chose that frame because I was going through screenshot websites and Gone Girl was the only film on bluscreens.net that had what I was talking about ; did n't mean to it out because I thought the film was great ( I actually just finished up a short where the shot of Amy on her side looking at the camera was the director 's reference image for the look of the film ) . That shot of Brendan Gleeson in Edge of Tomorrow 's much more jarring and more what I was talking about . <p> This is what I think works well , they just filmed a bunch of our regular newscasters in their regular studios and put it in the film . <p> Hey Ian thanks for the insight . I know it was super nit-picky but it 's always something I notice whenever it crops up . Again , our news shows over here do n't have as much money behind them so maybe things actually are starting to look like that in the States . @ @ @ @ @ @ @ @ @ @ means things like this ( even though this looks like a directing/vision mixing error ) are happening . 43971 @qwx453971 <p> Watch Sky News or CNN International . The shots you 're referring to are exactly what is seen every day on CNN , Headline News , MSNBC , Fox News , and many others , both in the US and wherever the international feeds of these services are available . <p> Sam , what was it about the news footage that made you feel it did n't fit in ? I can see it 's maybe got a slightly softer key light than you 'd expect and it obviously would have had to be made 24p somehow . And of course it 's inherited the overall grade of the scene . <p> I remember noticing the news footage in Robocop - I was learning about comb filter decoders at the time and I noticed the artifacts on the SD news item blown up for the cinema ! <p> By the way , BBC news has had remote cameras since the early 80s ( I used to operate them back @ @ @ @ @ @ @ @ @ @ frame was a bad example to use because it 's done very well and I 'm in no position ( at all ) to tell Ian or any of the Fincher team how to do their job ; that was just the only example I could find . <p> By the way , BBC news has had remote cameras since the early 80s ( I used to operate them back in the day ! ) 43971 @qwx453971 <p> Ha no way . I 've been behind the camera in a news studio too ; albeit on a much smaller scale . <p> But I guess it 's the shallow depth of field too ; I 've always thought of news as being shot on smaller chip cameras in an f/8-and-be-there kinda way . In that Edge of Tomorrow clip ; the cut from the BBC presenter to Tom Cruise at 1:08 ; the shot of Cruise just seems very different with the soft lighting and bokeh-y background , especially if you compare it to the interview with Glenn Greenwald I linked to earlier . But then maybe the idea is @ @ @ @ @ @ @ @ @ @ with all this talk of higher production values . <p> Michael a lot of news is n't even broadcast in HD over here ; the BBC only started transmitting their News channel in HD last year I think but all local programming is still shown in SD , if you 're watching BBC1 HD at 22:25 when the local news comes on you get a test card saying programming will resume at 22:35 and you have to switch over to BBC1 SD instead . All the US networks ' channels are SD over here - just took this now to demonstrate - channels in yellow are HD , white are SD . And I have n't watched Sky News in years because of stuff like this , so maybe I just have a completely different perception of how news programming looks and this whole thread 's baseless . I had n't considered the whole frame rate thing and that 'll definitely weigh into it as well . 
@@44332634 @4332634/ <h> Alexa footage very noisy <p> Let me apologize if this has been covered , or if I am doing something fundamentally wrong . We just shot some Alexa footage in ProRes4444 at ISO800 daylight . On set everything looked beautiful , now I get the sLog , and everything is very grainy . I read that I need to use the sLog to Rec709 LUT from the Arri website , which I did , but that does n't  seem to help the noise . Using Resolve 10 Lite . My options are NeatVideo , seems to work fine , or buying full version of Resolve and use the noise tool , but I ca n't belief that I would have so much noise in daytime footage . I never had this problem with RED footage . We shot with 2 Alexa camera bodies , and both have the same noise , so I assume its me <p> Well depends what the DP was going for but looking at that dpx under scopes it would appear to me that the shot is fairly under-exposed . If this is a day @ @ @ @ @ @ @ @ @ @ waveform/ histogram . If this was shot on raw you would have a lot more wiggle room but as it 's been baked in with prores you will have to work with what you have got . A denoiser would be recommended like you have already tried . The DP should have been paying more attention to the scopes I guess . <p> And trust me a RED would be just as bad in this situation , in fact probably worse as the noise is less organic and wobbles about in the blue channel . <p> You need to be sure you got the right Lut from Arri . There are so many options like extended to extended or legal , normalized or not and so on . Check back with the DoP what was recorded . Especially legal or extended is important for you . All other options are for post processing ... <p> So the image I send you is the sLog straight out of the camera , if I do n't  add any gain to it , should I still have that much noise ? I would @ @ @ @ @ @ @ @ @ @ the skin on the legs to be that noisy . I understand that once I add my lock by crushing the blacks that a lot of that noise will get crushed and hidden , but I was n't planning on crushing my ( I will admit what seems to be underexposed ) skin by that much . So here is a file from what I shot on a Epic a few month ago , also under exposed . I converted it from RAW to sLog , and it seems a lot cleaner . <h> Attached Files : <p> As Oliver already said , the shot looks a bit under-exposed . A LUT or another contrast adjustment will then naturally increase the perceived noise . Also , while ISO 800 is stated to be the sweet spot for an even dynamic range , it is not the one and only " correct " value to shoot with ! You can definitely decrease the ISO for more shadow detail and less noise . Here 's a good article about the Alexa 's ISO behavior : LONG ... <p> So the image I send @ @ @ @ @ @ @ @ @ @ if I do n't  add any gain to it , should I still have that much noise ? I would not expect the brown side panels of the car and the skin on the legs to be that noisy . I understand that once I add my lock by crushing the blacks that a lot of that noise will get crushed and hidden , but I was n't planning on crushing my ( I will admit what seems to be underexposed ) skin by that much . So here is a file from what I shot on a Epic a few month ago , also under exposed . I converted it from RAW to sLog , and it seems a lot cleaner . <p> Well there is as much noise in that shot and some visible blue banding BUT once corrected it is less noticeable because the shot is at night . So perceivable noise is less because everything around it can be dark . Your day time shot , not so much unfortunately . Also with the RED you get the bonus of being able to deliver in 2k ( @ @ @ @ @ @ @ @ @ @ 5k whatever which really helps with the noise issue . With the Alexa you have to pay more attention to things like exposure as your noise will be more apparent if there is a screw up . <p> With the Alexa footage there is n't any LUT that will fix the exposure and contrast issue . The inside of the car is being light by diffused light ( light bouncing from the road outside , the inside ceiling roof of car etc etc ) . This all adds up to make a low contrast , under exposed image . <p> The best that you can do is correct it to where you want it and attempt to manage the noise from there on . In my opinion anyway . <p> I recently did some comparisons with alexa footage shot pure red the idea being , to use one channel as the black and white source . I noticed working from the logC footage was more noisy than applying the lut then working on it . Not sure if this helps in this case but there seems to be a lot @ @ @ @ @ @ @ @ @ @ raw " as a get out of jail . IE its now our fault. ! ! <p> Our problem was , if I applied the standard Arri Rec709 LUT , we got an image that was about 50% darker than what the client wanted . I sometimes wind up creating what I call a " pseudo-LUT " to take us about half of the way there , but I 'm convinced most of it is just underexposure . It can be worse with Red footage , particularly night material . <p> I think it 's not unlike the old situation in film : DPs are nervous that they 're going to be second-guessed in post and the director or producer will take control of the session and make things real bright . If they deliberately shoot it super-dark , that ca n't happen ( without a lot of pain ) . <p> My advice has always been to trust us to keep it dark , and give us a nice thick negative -- that is , a balanced histogram -- but what I 've seen in the last year or @ @ @ @ @ @ @ @ @ @ The results with 8-bit cameras are even worse . At least with Alexa and Epic we have some wiggle room to coax it up another stop or two , but just barely . <p> problem is the cam manufacturers are telling customers that 800 ISO is optimum for the camera . Well that 's not always true but it is so ingrained in everyone 's mind , even guys who really know better . Sometimes you got ta be at 320 and get more light somehow . Go to Home Depot and get some floods , anything ! Or deal with a bunch of nice organic grain .... <p> problem is the cam manufacturers are telling customers that 800 ISO is optimum for the camera . Well that 's not always true ... 43971 @qwx453971 <p> Yeah , there 's a lot of conversations about this on the various CML discussions . Geoff Boyle and the other regulars are unconvinced that either the Alexa or the Epic are optimal at 800 ISO . My gut feeling is that you have to overexpose at least 1/2 a stop and then constantly @ @ @ @ @ @ @ @ @ @ to see how the Dragon chip will change this , and if Dragon really works at 2000 . <p> I have shot a lot of Alexa at ISO 800 and it is not noisy . Something else is going on here , probably underexposure for a reason other than the ISO setting . <p> I also think it 's a good practice to use one of the Alexa LUTs as a starting point for grading Alexa LOGc footage . The LUT corrects Alexa color response in a way that a primary color correction can not . <p> And lastly , as a DP , I 'm not sure I want my colorist to tell me how to expose my movie . That 's my job . I am supposed to know what I 'm doing <p> And I generally expose RED camera at ISO 500 ... but that 's just me , and not always . <p> I do allot of work with the Cineflex Elite ( Alexa-M ) in the helicopter and it is almost always 800iso at 180 shutter with an ND ( often an 1.2 ) for @ @ @ @ @ @ @ @ @ @ that someone did n't forget to pull and ND out of the matte box ? <p> I also think it 's a good practice to use one of the Alexa LUTs as a starting point for grading Alexa LOGc footage . The LUT corrects Alexa color response in a way that a primary color correction can not . 43971 @qwx453971 <p> I would respectfully disagree , only in that the custom curves and a little saturation in Resolve actually can reproduce the official Alexa Rec709 LUT fairly closely . My issue is I 've seen material on occasion where the factory LUT does too much damage to the image , and I need something that only goes about halfway in terms of blacks and gamma . I 've worked on projects in the last couple of years that made me wonder how they 're monitoring on the set , because there 's no way they could 've been looking at the factory LUT and thought their images looked right . <p> And lastly , as a DP , I 'm not sure I want my colorist to tell me how @ @ @ @ @ @ @ @ @ @ I am supposed to know what I 'm doing . 43971 @qwx453971 <p> I agree wholeheartedly . But at the same time , I assume you want a colorist who will tell you the objective truth about your footage . Sadly , the histograms do n't lie . If the material comes in slammed all the way over to the left side , and the director wants it bright , you 're going to wind up with very noisy footage . I always try to fight on the DP 's side , but ultimately , we 're both employed by the director . 
@@44332635 @4332635/ <p> When you first set up your project , you need to go into the camera raw settings tab , select Red like you see in the picture , but use these settings instead : <p> Decode Using : Project Color Space : REDcolor3 Gamma Curve : REDLogFilm <p> Also , playback of R3D files is very processor intensive . You wo n't be able to achieve real-time ( 24fps ) playback without a very strong computer . Generally speaking , it takes a Mac Pro 12-Core 2.4Ghz or better for Half Res Good ( very decent looking ) , and say an 8-Core 2.26GHz or better for Quarter Res ( somewhat pixelated ) . <p> Yes , but its a little expensive . You 'd need to grab a RED Rocket and use it in something like a Magma 3T Thunderbolt expansion chassis . Its cool that you have internal SSDs , but this is a CPU issue , and not so much a storage one . <p> The easier , more common way to handle RED-acquired media in a situation like yours , is to transcode it @ @ @ @ @ @ @ @ @ @ settings you 'd like use on your project . Once its transcoded you wo n't be able to change them after the fact , but you 'll be able to work with transcoded media a lot easier , and you 'll have the color space and gamma you 're looking for . <p> Not necessarily . If you use ProRes 4444 or DNxHD 175x you can easily just finish in one of those and be done with it . For instance , while ARRIRAW is fantastic looking , the vast majority of TV shot on ARRI Alexa gets written directly to ProRes 4444 and they never look back . <p> Grading directly from the R3Ds offers a certain amount of flexibility , but if you 'd like to do that you 're probably going to want to look into a RED Rocket or at least a powerful desktop at some point . <p> Hi Jason , thanks for that . I 'm not using RED material enough yet to warrant the price tag on the rocket card . <p> I 'm pretty happy working with RED natively in resolve and @ @ @ @ @ @ @ @ @ @ or at all for that matter ! ) , just working on stills pretty much . I 'm am going to be investing in a new Mac Pro though asap . <p> One other problem I have with the AAF to avid workflow is that although it remembers the resize effect basic parameters and keyframes it does n't seem to pull in the easing on the keyframes which is a bit of a nightmare since I ca n't play back the RED files in realtime to recreate them properly in resolve . <p> While you 're in Resolve , go to your RED project settings , and drop the decode quality to a low value , like 1/8th . The image will become a bit pixelated , but playback should get closer to real-time , so you can recreate any motion effects that did n't come across . Once you 're done re-applying the effects , you can set your decode back to 1/4 or 1/2 . <p> Also , that data should stay intact through your round-trip . So another idea would be to grade the shots as they @ @ @ @ @ @ @ @ @ @ AAF back in Media Composer . 
@@44332636 @4332636/ <h> Self training ; in Davinci Resolve <p> I 've been working in Avid DS for about 2 years now . With that being put down by Avid ; it looks like we 're going to move towards Davinci Resolve . At the same time we might be starting work on a big local drama which would be a big deal for me to get my hands on . I 've been wanting to make the move from online to Coloring for a while and this could be a great opportunity . <p> My question is what are the best online training resources for Resolve or just color grading in general ? I 've found a number of subscription based options , but I 'd like to get some insight from people who have tried them . Are they worth it ? Which ones are best ? <p> 1 . learn the tool as much as you can . Ripple training is good for start and then Help is always good friend or try private ( individual ) training with colorist 2. start grading and be organised ( very @ @ @ @ @ @ @ @ @ @ improve your sensitivity to color , looks etc <p> Although I 'm primarily self-taught as a colourist ( coming from a fine-art painting &gt; film &gt; vfx/animation background ) this is not the fastest way of getting up to speed . Besides the ' button-pushing ' colourgrading is much more about developing your general approach - and although I think Taoofcolor addresses that to some extent ( disclaimer : have n't seen the actual course material ) I think it would be extremely useful to observe some colorists at work in their natural ' habitat ' . Just by sitting besides a colourist for a day and seeing how they approach a project is immensely useful . I do n't know where you 're based but it 's worth reaching out to colourists in the neighbourhood . <p> Currently there are 137 posts tagged under colour grading and 77 tagged to DaVinci Resolve . This represents tons of great ( free ! ) tutorials , resources and information for colorists of all calibre 's , gathered from all corners of the web . <p> Hope you find it useful , @ @ @ @ @ @ @ @ @ @ Thank you all for your suggestions and advice . I 'm currently taking Ripple training and have plans to start subscribing to MixingLight once IO have the funds . Appreciate the book recommendation as well , I 'm reading that exact book right now ( although Alexis is JUST finishing up a new edition ) <p> So its been a few monthsm I 've taken quite a bit of fantastic training and its been insanely helpful , but I 'd like so see what you recommend from here . I 've taken the Dead Mans Lake Grade Along from Tao of Color ( thanks Pat , Excellent course ) , Ripple Training from Alexis , have the CC handbook 2nd ed , and have considered the looks book as a good reference . I am what you could I guess I call a " weekend colorist " , or someone who works in the entertainment but not in editing or color " 9-5 " , but it 's become a passion of mine and something that I wish I could turn into a full time career . <p> Being financially @ @ @ @ @ @ @ @ @ @ lot of advice via podcasts and from many of you , I refuse to drop everything &amp; go freelance ( nor do I feel I am ready skill and expertise wise ) . I plan to continue to grade short web series , short films , and other smaller projects as I have time to improve my skills and continue to find my style , but I was wondering if you had any suggestions to help me push myself up skill wise ? I 've not taken the FXPHD class Resolve 10 , which it very useful but will I just be repeating a lot of what I got in my previous " classes " ? <p> Glad you found the blog useful - sign up for the free digest to never miss a post . <p> My thought is that you 're on the right track with planning to grade as much stuff as you can - the more experience you rack up , the better you 'll get . Plus then when you meet a difficult challenge you 'll have to figure it out for yourself . <p> That @ @ @ @ @ @ @ @ @ @ the time , freedom , opportunity , inclination . But the former is probably much easier to sort out than the latter ! <p> Thanks Jonny ! It 's like my second Tao of Color newsletter ! Thanks for the response . I 'd love to shadow some colorists , but with my normal job working 9-5 , it 's really hard to find someone who would be working at that time . That 's why I 'm finding Tao of Color grade along , Mixing Light , and other video training so useful ! I 'm still getting a " pro " advice on my own schedule , which is getting harder as I get farther away from learning buttons and more into learning the art and style of the craft . <p> Thanks Marc ! That was pretty much exactly my ? . Is Warren 's training more about how to use Resolve or does he get into learning of the craft of grading a bit ? I have no problem picking up his tutorials if I 'll learn something more besides what I 've already learned about @ @ @ @ @ @ @ @ @ @ seen any of it yet and could provide an opinion . Thanks everyone for the advice <p> Thanks Marc ! That was pretty much exactly my ? . Is Warren 's training more about how to use Resolve or does he get into learning of the craft of grading a bit ? I have no problem picking up his tutorials if I 'll learn something more besides what I 've already learned about Resolve , but I wanted to see if anyone had seen any of it yet and could provide an opinion . Thanks everyone for the advice 43971 @qwx453971 <p> It 's more about Resolve . Fxphd also has their ' craft of grading ' titles and that 's probably what you 're looking for . <p> The best way how to learn is to sit and watch experienced colorist what he is doing . I attended private training last weekend and I learnt far more than in any tutorials out there cause nobody teaches you craft but app . They are good when you start to learn tool but ... My advice : try to find someone @ @ @ @ @ @ @ @ @ @ ! <p> Thanks Marc ! That was pretty much exactly my ? . Is Warren 's training more about how to use Resolve or does he get into learning of the craft of grading a bit ? 43971 @qwx453971 <p> It 's more about the tools , not in the philosophy . I go with the " 10,000 hours to become an expert " line of thinking , where you just have to put in the hours and expect that the first year or so of color-correction you do will be crap . There is n't a thing I did in my first 10 years of color correction that I would n't like to redo today . And that was 20 years ago . <p> Thanks you all for your advice and suggestions ! It 's very much appreciated . I 'll be doing as much grading as I can to gain experience , I will be attempt to find a colorist would n't mind me shadowing them a bit ( and until then , I 'll be using Tao of Color 's Grade Along to get that " next @ @ @ @ @ @ @ @ @ @ another one . ) I 'll def check out that craft of grading class from FXPHD , that sounds more along the lines of what I 'm looking for . <p> Check out the Grade Along at the Tao of Color : http : **26;387;TOOLONG It does include a good primer to get you up and running with Resolve , but the main benefit was " grade along " aspect of the training and the client interaction with BloodyCuts . <p> Alexis Ripple training for Resolve is a great overview of the tools in the app and how to access them , I have been playing Colorist for about eight years now and the first year was a painful teething process in finding out people 's expectations of what they wanted their film to look like by looking at footage . I got yelled at a bit and i did n't like getting yelled at so i got better and I think I have now come to have a subtle feel for what it is someone wants and the back and forth with people I am working with is now @ @ @ @ @ @ @ @ @ @ and suggest things or push stuff in directions which will make them even happier . 
@@44332637 @4332637/ <p> If you read the IDMS and Samsung documents on volume measurement , they specify the use of just 8 measurements - RGBCMY , and B&amp;W . From those 8 measurements the exterior volume of the colour space is calculated . <p> In reality , that is not volumetric calibration , it is just displaying the primary and secondary peak colour measurements , extrapolated to B&amp;W , and shown as a volume plot . There is no actual volumetric data in there at all . ( Even if the total 103 values are actually measured , not extrapolated , you still have no volumetric data - just the the edges of the colour volume ... ) <p> To have volumetric data requires the use of interior volume measurements , as you get when profiling a display with a large cube based patch set . From such profile data you can then generate true 3D calibration LUTs that will correct any volumetric errors - for SDR and/or HDR . <p> It really is sad to see such marketing masking the realities behind it <p> This is nothing more than a @ @ @ @ @ @ @ @ @ @ it any way accurate , or of any use , without actual volumetric measurements . <p> If you read the IDMS and Samsung documents on volume measurement , they specify the use of just 8 measurements - RGBCMY , and B&amp;W . From those 8 measurements the exterior volume of the colour space is calculated . <p> In reality , that is not volumetric calibration , it is just displaying the primary and secondary peak colour measurements , extrapolated to B&amp;W , and shown as a volume plot . There is no actual volumetric data in there at all . ( Even if the total 103 values are actually measured , not extrapolated , you still have no volumetric data - just the the edges of the colour volume ... ) <p> To have volumetric data requires the use of interior volume measurements , as you get when profiling a display with a large cube based patch set . From such profile data you can then generate true 3D calibration LUTs that will correct any volumetric errors - for SDR and/or HDR . <p> It really is sad to see @ @ @ @ @ @ @ @ @ @ nothing more than a 3-dimensional graph of a display 's gamut , but not it any way accurate , or of any use , without actual volumetric measurements . <p> Steve 43971 @qwx453971 <p> You have no clue how we have implemented Color Volume measurement , and you only need to measure the shell of a color volume to calculate what the volume in side of it is . Color volume is a display metrology metric , not a calibration process . <p> That really done make the PR seem even more about nothing but marketing , as the suggestion throughout the piece is that it is in some way linked to calibration , which as you agree it is not . <p> And the Samsung documentation on this defines exactly how the volume measurement is performed . <p> That 's what I have commented on . Nothing more , nothing less . <p> Displaying colour volume has been available for ever . And can be done with a 2-dimensional graph . This graph shows how most LCD displays ( such as my Samsung SDR home TV ) have gamut/saturation @ @ @ @ @ @ @ @ @ @ surfaced which further confirm Samsung 's " marketing bollox " <p> I am not going to share the numbers that Samsung touted in the demo because unfortunately the demo was somewhat misleading ... Samsung took advantage of the fact that when an OLED panel reproduces a full-white or a very , very bright image its overall brightness drops ( like all self-emitting technologies , including the true QLED that Samsung is working towards ) . 
@@44332638 @4332638/ <h> Data Levels to Video Levels in Premiere Pro <p> Hello fellow LGG peeps , I received colored media but scaled to Data levels . I can convert it to video levels in resolve but I do n't really want to spend that time rendering . Does anyone know if there is a way to scale data to video levels on a clip in Premiere ? <p> Hello fellow LGG peeps , I received colored media but scaled to Data levels . I can convert it to video levels in resolve but I do n't really want to spend that time rendering . Does anyone know if there is a way to scale data to video levels on a clip in Premiere ? <p> thanks , J 43971 @qwx453971 <p> PP internally scales both data and legal video levels between 0 to 100 . Data levels uses the full range while non legal video levels can be extended to negative and above 100 values . Because PP uses this abstract representation you really do not need to do anything special . <p> PP for most CODECs will export ( @ @ @ @ @ @ @ @ @ @ Exporting to data levels is only allowed for certain codecs ( H.264 is not one of them ) , in those cases you would want to be sure all levels ( data or video ) of your clips are legal because out of range values will be clipped . <p> PP internally scales both data and legal video levels between 0 to 100 . Data levels uses the full range while non legal video levels can be extended to negative and above 100 values . Because PP uses this abstract representation you really do not need to do anything special . <p> PP for most CODECs will export ( and convert if necessary ) to video levels automatically . Exporting to data levels is only allowed for certain codecs ( H.264 is not one of them ) , in those cases you would want to be sure all levels ( data or video ) of your clips are legal because out of range values will be clipped . 43971 @qwx453971 <p> The thing is that it depends on the codec , for example in the case of the DnxHD/DnxHR quicktime @ @ @ @ @ @ @ @ @ @ correctly , for CC 2014.2 you need to add a 16-235 levels adjustments , the cc 2015.1 is the same as the CC 2014 and the CC 2015.2 sees the legal range correctly but clips the Data range . To be honest I really ca n't understand why Adobe ca n't fix these issues , it feels like they trying to force us to use Prores or MXF . <p> Answering to the initial question : as Cary Knoop said you do n't need to do anything special if the program can read your files correctly . 
@@44332639 @4332639/ <p> The image crop will have more of an effect on image quality than the very subtle differences ( and problems , except on Weapon ) of a Dragon sensor . 43971 @qwx453971 <p> If this was a camera site , then yes , no biggie . But this being a colorist site , I think it is important to know about the camera , that many will have to grade and edit . It is important to keep in mind , that this camera , like Weapon , will be able to record Prores at the same time as RAW . But it will be only 2K ( I find that very strange in light of Red always championing 4k ) and Prores will only be up to 422HQ . So , without transcoding , there will be no 4k Prores or 444 or XQ flavors . Now , THAT is important to keep in mind . <p> Their big competition at that price range is Blackmagic , and I think the Ursa Mini looks very good from a lot of angles . But I can see a @ @ @ @ @ @ @ @ @ @ " B " camera . Still 4K , same dynamic range , same color science . <p> Seems to me that red is in the same place as apple . Main ideologist sort of gone and now they run allover it . <p> BM still has better color and more stable platform . <p> We will see what future brings . I bet arri and bm are also not sleeping or sitting on their hands . 43971 @qwx453971 <p> It 's nothing like the Apple situation . Jim Jannard is not dead . He is very much alive and very much involved , even though he has chosen to no longer be the public face of the company . Make no mistake , it 's still very much his company and his vision <p> I bet it is a crop , but officially it 's not known at this time . Correction . I just saw a conformation on that . It is a cropped Dragon sensor . 43971 @qwx453971 <p> mmm .... that is no good . noise , artifacts etc .. looks quite worse on a cropped sensor @ @ @ @ @ @ @ @ @ @ have a new 4K custom sensor made , but , was n't it 10K USD the dragon sensor only upgrade ? and now for 5kUSD they sell it with a camera body ? seems weird .. no ? g <p> How big is RAVEN 's Dragon Sensor ? - RAVEN 's sensor size is 20.48x10.8mm with an image circle of 23.2mm. - RAVEN 's format size is larger than Micro Four Thirds in width and just a hair narrower than Academy 35mm and APS-C. 
@@44332640 @4332640/ <h> Nucoda pricing <p> Jake and I spent a couple hours with Digital Vision on Monday morning . We had a good conversation with Greg and Patrick , and learned a lot about what they are trying to accomplish with their new pricing . <p> They are focusing on two products now , Nucoda Look and Nucoda . Both have the same toolset , but Nucoda Look is limited to one layer of corrections and is the version they are now offering for -1,650 / $ 2,695 . It does not include any DVO tools . <p> The next option is Nucoda itself . It includes the full toolset with multiple layers of corrections and comes with the Classic DVO tools which includes Brickwall , Aperture Sharpen , Grain , and De-Grain and is US$6900 . You can also buy additional DVO tool sets like Enhance , Restore , Convert , and Camera . <p> The recommended video I/O hardware is a DVS Atomix LT which is -2,800 , but they also plan to support AJA in the future . The Nvidia Quadro SDI board is also an option @ @ @ @ @ @ @ @ @ @ other Quadro cards like the Q6000/5000/4000 will also work . <p> For real-time playback and good caching performance you 'll need a fast SAS or Fibre RAID array . Their basic recommendations went like this : <p> 8-Drive RAID5 : HD 12-Drive RAID5 : 2K 24-Drive RAID5 : 4K <p> CPU-wise they generally specify an HP z820 or Dell T7600 workstation , although we even saw one of their systems running on a Silverdraft Demon at the show . Many of the tools rely on fast CPU performance , so I imagine emphasizing a higher CPU clock is preferable over a larger number of cores . <p> All of their products come with one year of software updates , but beyond that you 'll need to buy support which ranges from software-only starting at -2,000 to full hardware , phone , and email support like you would get with a Flame or Baselight . <p> I hope this helps clarify a few things . While talking with Greg and Patrick , the recurring theme in our conversation was how to strike a sustainable balance between price and volume . Simply @ @ @ @ @ @ @ @ @ @ I think they are listening carefully to input from customers on pricing that makes sense for both parties . <p> Price seems very fair given the productivity of the toolset and the reasonable cost of hardware ... <p> For me Aja and MC support would make it a no brainer as i could keep DS running on the same boxes <p> As it stands i will have to either wait a bit -or- decommission a DS for a bit , or run the DS in GL mode , outputting through the Nvidia 's DisplayPort to a mon ... making it pretty useless for versioning ... <p> They are focusing on two products now , Nucoda Look and Nucoda . Both have the same toolset , but Nucoda Look is limited to one layer of corrections and is the version they are now offering for -1,650 / $ 2,695 . It does not include any DVO tools . <p> The next option is Nucoda itself . It includes the full toolset with multiple layers of corrections and comes with the Classic DVO tools which includes Brickwall , Aperture Sharpen , Grain @ @ @ @ @ @ @ @ @ @ buy additional DVO tool sets like Enhance , Restore , Convert , and Camera . <p> All of their products come with one year of software updates , but beyond that you 'll need to buy support which ranges from software-only starting at -2,000 to full hardware , phone , and email support like you would get with a Flame or Baselight. 43971 @qwx453971 <p> No thanks . <p> I knew there had to be a catch . Unless the toolset is such that one usually and easily does a complete grade in one layer , I do n't see how anyone would want to limit themselves , which makes Nucoda Look DOA in my eyes . Oh , and is it limited to only PCs as well ? Paid support and upgrades ? I think for most people , Resolve has soured the idea of paid support contracts and upgrades , not to mention single platform application . <p> Maybe Nucoda should do a Lite version like Resolve , just to get people and newcomers interested ? They could simply disable certain features in render so as to protect @ @ @ @ @ @ @ @ @ @ in a ecosystem they tend to stick with it if it 's good . <p> I bet most people who started grading in the past 4-5 years , started with a free version of Resolve , learned the program , and have no issues purchasing it later . Hell , I would pay a lot more than $1,000 for it . <p> We really do work in a niche market , there are just so few of us , and it amazes me that there are enough people/post houses out there to support this glut of software companies . Look at editors and vfx artists , so many more of them , yet fewer applications , what gives ? <p> Really interested to see how such companies pan out in the coming years . <p> It runs beautifully under the Bootcamp . So , if you have a Mac no need to buy another computer . And it writes Prores , unlike some other free app on Windows we know But yeah , Nucoda Look in todays market of free tools is a complete non starter . You absolutely do @ @ @ @ @ @ @ @ @ @ of users do n't need support for Resolve . It is as easy to install and troubleshoot as Resolve . Digital Vision needs to come up with a reasonably priced program for just Nucoda updates without support . <p> First off , Jake and jason , thank you for a great write up , it was great to meet you guys . <p> We did a lot of work on the Tangent panels , glad you liked them .... as for the comments from JD , I thought I would add my personal comments as a user ( I used to do " real " work once long ago . - so Digital Vision hat off ) <p> I knew there had to be a catch . Unless the toolset is such that one usually and easily does a complete grade in one layer , I do n't see how anyone would want to limit themselves , which makes Nucoda Look DOA in my eyes . 43971 @qwx453971 <p> Look is designed to be a conform / export / prep and rendering system , many users use it to check @ @ @ @ @ @ @ @ @ @ Oh , and is it limited to only PCs as well ? Paid support and upgrades ? I think for most people , Resolve has soured the idea of paid support contracts and upgrades , not to mention single platform application . 43971 @qwx453971 <p> Support costs man power ... when a client needs a code fix , and it is a truly messy bug , we get it done , you can pick up the phone and call us , and speak to anyone in the company , also , development costs money , innovation costs money , I amazes me that people still has this idea that software writes and debugs itself ... as for PC Mac debate , I 'm the wrong guy to ask , I do n't have an Iphone or Ipad or Ipod ( you get the picture ) - buy the new MAc , wait for the next model to upgrade your graphics cards ... could n't be bothered . <p> Maybe Nucoda should do a Lite version like Resolve , just to get people and newcomers interested ? They could simply disable @ @ @ @ @ @ @ @ @ @ end tools . Once you get people invested in a ecosystem they tend to stick with it if it 's good . <p> I bet most people who started grading in the past 4-5 years , started with a free version of Resolve , learned the program , and have no issues purchasing it later . Hell , I would pay a lot more than $1,000 for it . <p> We really do work in a niche market , there are just so few of us , and it amazes me that there are enough people/post houses out there to support this glut of software companies . Look at editors and vfx artists , so many more of them , yet fewer applications , what gives ? 43971 @qwx453971 <p> You are not in a niche market anymore Dorothy ... everyone is now a colourist , just like everyone is an editor , and an FX guy , ironically , I said to a journalist at NAB , audio was the first to go in the whole process as far as pricing goes , many years ago , however , @ @ @ @ @ @ @ @ @ @ that shows up people who just took it up because the tools are cheap . A bad colourist calls is an edgy look , a bad editor calls it progressive editing .... and bad sound guy .... does n't work again , even if he 's willing to work for cheap , bad audio is just bad audio . <p> Really interested to see how such companies pan out in the coming years . 43971 @qwx453971 <p> For all our sakes ( and my job , and many others ) I hope that Digital Vision , Filmlight , Assimilate and others are still around to provide innovation and choice , free is n't always cheap , there is nothing worse than not being able to make a choice , even if you do n't want to ... at least you have an option . <p> I would certainly like to make up my own mind , even if my decision means I do n't get something for free . ( sorry ) <p> ( Digital Vision hat back on ) <p> Thank you to everyone for the discussions , time @ @ @ @ @ @ @ @ @ @ but I am absolutely fried .... <p> It runs beautifully under the Bootcamp . So , if you have a Mac no need to buy another computer . And it writes Prores , unlike some other free app on Windows we know But yeah , Nucoda Look in todays market of free tools is a complete non starter . You absolutely do not need support for Nucoda in the same way majority of users do n't need support for Resolve . It is as easy to install and troubleshoot as Resolve . Digital Vision needs to come up with a reasonably priced program for just Nucoda updates without support . 43971 @qwx453971 <p> Jake , I believe there is an option for that already . Will post details when i can . <p> Support costs man power ... when a client needs a code fix , and it is a truly messy bug , we get it done , you can pick up the phone and call us , and speak to anyone in the company , also , development costs money , innovation costs money , I amazes me @ @ @ @ @ @ @ @ @ @ debugs itself ... as for PC Mac debate , I 'm the wrong guy to ask , I do n't have an Iphone or Ipad or Ipod ( you get the picture ) - buy the new MAc , wait for the next model to upgrade your graphics cards ... could n't be bothered . 43971 @qwx453971 <p> Yeah , I get it . My comments were simply observational . <p> Do n't know exactly what you meant by your mac comment . I like Macs and PCs . I like to have the option to chose between Macs/PC . I 'm not interested in the graphics cards arms race though . A good graphics card should last two-three years , if not longer . Titans have been out over a year and still going strong . I 'm sure they 'll still be a very viable option a year from now , just as the D700s will be . <p> You are not in a niche market anymore Dorothy ... everyone is now a colourist , just like everyone is an editor , and an FX guy , ironically @ @ @ @ @ @ @ @ @ @ was the first to go in the whole process as far as pricing goes , many years ago , however , I is one of the the few disciplines out there that shows up people who just took it up because the tools are cheap . A bad colourist calls is an edgy look , a bad editor calls it progressive editing .... and bad sound guy .... does n't work again , even if he 's willing to work for cheap , bad audio is just bad audio . 43971 @qwx453971 <p> I disagree . I still think it 's a niche market . I still have to explain to people outside the post industry what it is that I do . <p> The point I was trying to make is that the " everyone " you refer to are not serious about color and most likely wo n't invest in the hardware or software element of it , let alone learning it well . So that is what I find so surprising about so many expensive color apps . I do n't think there are a lot of @ @ @ @ @ @ @ @ @ @ ( and my job , and many others ) I hope that Digital Vision , Filmlight , Assimilate and others are still around to provide innovation and choice , free is n't always cheap , there is nothing worse than not being able to make a choice , even if you do n't want to ... at least you have an option . 43971 @qwx453971 <p> Yeah , but when these systems cost $30k and above , that does n't leave much of a choice to freelancers . Lower priced apps make a very compelling case since they 're just as innovative and powerful , and supply good support and listen to their customer 's feature requests as Resolve and Speedgrade have shown . <p> Software or hardware is like any other commodity : we search for the highest quality/performance at the lowest price . I really do n't see want is so compelling about Scratch , Nucoda , Baselight and Smoke ... Mystika and Pablo are another story , but then again , they 're so much more than color correctors . <p> So like I said , I @ @ @ @ @ @ @ @ @ @ of color tools out there . Everyone in post has felt the tightening budgets , it 's only natural that it trickles down to vendors of hardware and software too . In the end , it 's lower prices and choice that are good for the end user and clients . The days of selling access to expensive heavy iron systems is quickly coming to an end . <p> Yeah , but when these systems cost $30k and above , that does n't leave much of a choice to freelancers . Lower priced apps make a very compelling case since they 're just as innovative and powerful , and supply good support and listen to their customer 's feature requests as Resolve and Speedgrade have shown . <p> Software or hardware is like any other commodity : we search for the highest quality/performance at the lowest price . I really do n't see want is so compelling about Scratch , Nucoda , Baselight and Smoke ... Mystika and Pablo are another story , but then again , they 're so much more than color correctors . <p> So like I @ @ @ @ @ @ @ @ @ @ to this abundance of color tools out there . Everyone in post has felt the tightening budgets , it 's only natural that it trickles down to vendors of hardware and software too . In the end , it 's lower prices and choice that are good for the end user and clients . The days of selling access to expensive heavy iron systems is quickly coming to an end . 43971 @qwx453971 <p> I see Nucoda and Baselight having speed and significant workflow advantages over Resolve , unless Resolve has the big panel I see Smoke having massively significant workflow advantages over Resolve and PP , but zero useful colour tools for long form I am having a hard time seeing what Scratch brings to the party anymore <p> I also am having an interesting time looking at my next few years , i have a number of films to finish at 2K , and a number of TV movies that finish at 709 lined up , and a core toolset that has been EOL 'd and needs to be replaced fairly quickly ... as soon as the first @ @ @ @ @ @ @ @ @ @ ... i finish and grade in the same room , so a big panel is not an optimal choice in terms of a working environment <p> the NS combo would be about the same price point as Nucoda with killer paint / fix / title tools , and both are about a third less than Resolve 's price given the crap mapping on the small panels forcing the need to buy the big panel ... <p> And add to that Disk Arrays / Dolby mon / scopes / audio mon / LTO / CMS as a basic start to the room , that lot is about 50K before any machine and software is in the room <p> None of these are 1k , or free , and there 's no chance of delivering to distb 's and networks with a 1K/free system the bond will bounce you right out the door ... fortunately producer 's are willing to budget realistically knowing that QC rejections are more costly than doing it correctly in the first place ... <p> So a ROI in the neighbourhood of 75 - 90K is the target @ @ @ @ @ @ @ @ @ @ think Resolve gives you enough of an advantage to be able to bill another 15% -or- to save 15% of your time and have 15% more work to fill the suite , then Resolve makes sense , otherwise it looks like the most expensive toolset of the lot , and NS by far the most powerful <p> d <p> ps .. speedgrade is so far out of the race it does n't even get a thought , and the development seems to be aiming it towards becoming a paint by numbers app , it 's getting dumber not smarter for now , so i guess they are listening , but listening to the 10 's of thousands of weekend warriors and PP users , not working colourists ... <p> And would belive BMD is listening when they map a small surface even half assed ... <p> If Nucoda comes up with just updates program without the support , you probably looking at a lot less , than $2k per year . If you looking at using your system a lot , present mapping on Resolve on any of the supported @ @ @ @ @ @ @ @ @ @ not very productive . On the other hand , the world is your oyster with panels mapping of Elements on Nucoda . It was actually designed by someone who deeply understands color grading and not by an engineering and marketing committee at BM . My understanding NS is not really good at real time performance . It 's basically NUKE with the timeline bolted on and we know NUKE is not very fast on the render , because it does n't need to the way it is normally used . But for grading that is a different story . Right now it is way too early to even contemplate NS for serious long form grading . We do n't even know anything about the conforming on NS . <p> If Nucoda comes up with just updates program without the support , you probably looking at a lot less , than $2k per year . If you looking at using your system a lot , present mapping on Resolve on any of the supported panels , other than BM panel , is miserable and not very productive . On the other @ @ @ @ @ @ @ @ @ @ of Elements on Nucoda . It was actually designed by someone who deeply understands color grading and not by an engineering and marketing committee at BM . My understanding NS is not really good at real time performance . It 's basically NUKE with the timeline bolted on and we know NUKE is not very fast on the render , because it does n't need to the way it is normally used . But for grading that is a different story . Right now it is way too early to even contemplate NS for serious long form grading . We do n't even know anything about the conforming on NS. 43971 @qwx453971 <p> I agree far to early to write it off or add it in .. where Nucoda is a known and stable platform , a sledgehammer - <p> I 've been scarred by early Hiero meltdowns , but do have 5 years of Nuke under by belt ... Hiero apparently is now stable , but a few years ago it was a crash-a-thon , so hopefully lessons have been learned <p> For me Nucoda seems to be the @ @ @ @ @ @ @ @ @ @ the colour surface .... it would drop straight in to my existing machines <p> They were pretty reassuring about the upcoming AJA support . Patrick mentioned , that MC Color support is still there , so will have to see if it still works . As far as I remember from Fuse days , it was pretty good and bidirectional . Frankly , I 'm not against moving to Elements . The main reason why I decided to use MC Color , I was able to work as fast as using Elements for a third of a price . But I do n't mind spending the money , if my grading experience and productivity increases . To me the biggest question mark is the absence of XML support . Personally , the XML based workflow is probably 75% of my business . <p> If you looking at using your system a lot , present mapping on Resolve on any of the supported panels , other than BM panel , is miserable and not very productive . On the other hand , the world is your oyster with panels mapping of @ @ @ @ @ @ @ @ @ @ who deeply understands color grading and not by an engineering and marketing committee at BM. 43971 @qwx453971 <p> I really do n't mind the current Element panel mapping . What Speedgrade did with Element is amazing , so I can understand why it 's such an important consideration for some , but definitely not all . I still use keyboard shortcuts a lot . <p> Also , +1 on XML support being like 75% of my current work too . XML support is crucial and frankly I do n't understand how any color corrector or conform app can lack this fundamental feature . <p> So a ROI in the neighbourhood of 75 - 90K is the target for ROI - not " free " <p> ps .. speedgrade is so far out of the race it does n't even get a thought , and the development seems to be aiming it towards becoming a paint by numbers app , it 's getting dumber not smarter for now , so i guess they are listening , but listening to the 10 's of thousands of weekend warriors and PP users , @ @ @ @ @ @ @ @ @ @ listening when they map a small surface even half assed ... 43971 @qwx453971 <p> I think your needs are representative of many staff colorist , whereas mine are that of freelancers . Horses for courses . <p> I would n't want the Resolve panels . Too big , too many buttons , too expensive . So I would immediately deduct $30k from your budget mark up , add the Elements which would bring Resolve to $15K . Also , I 'd deduct another $3K for machine because you really do n't need a $10K box for Resolve . You can build a beast of a PC for $7K or just get the 6/8core new mac pro , which is a little killer itself . <p> At that point , Resovle system comes in at half of Nucoda , or really anything else for that matter . <p> Yes , Speedgrade is n't ready for prime time yet , as I have said several times , but it 's getting there increasingly fast . Give it another year , two at most . I have high expectations of it . <p> @ @ @ @ @ @ @ @ @ @ , but they got a business to run too . I think the rule with BMD is hands off our hardware , and rightly so . They have a pretty sick business model that continues to impress . So long as they keep up with the plethora of updates each year , I can live without perfect panel mapping . Besides , the Elements have been out close to two years . I 'm sure another panel is in the works for 2015 whether it be by Tangent , BMD or others . <p> Somewhat different with my clients , almost all are MC edits , and deliver AAF 's and AFE 's , the only things i see that are XML 's are student films i do pro-bono , and a few commercials <p> I used to use the duck to get XML 's into AAF 's and washed those though a MC to make a AAF for DS , recently i 've been using Resolve to open the XML , and then sending the AAF out to DS for conform / grading / finishing . <p> Nucoda reads @ @ @ @ @ @ @ @ @ @ who has both , conform &amp; grade in Nucoda , finish in DS , DS relinks to the Nucoda renders from the AAF , and runs them RT <p> I would think Nucoda would not have any issues conforming a XML washed through Resolve to AAF if DS can use it cleanly ... <p> I think your needs are representative of many staff colorist , whereas mine are that of freelancers . Horses for courses . 43971 @qwx453971 <p> I am a freelancer , one who speclises in indie features , MoW 's and Canadian national TVC 's , do own my own gear tho , one system in an agency , one system at a producer 's office , and one system at home , and i often work in facilities with huge machine rooms when that is a better choice for the project , in most cases i bring the project to the house and manage it , once i a while i have a producer who has deal in place at a facility ask for me to freelance there rather than work with the house colourist ... @ @ @ @ @ @ @ @ @ @ - ) <p> I do n't do pipeline , reality or episodic TV , and nothing for the web either .. i would need far more resources than i have now , and i just do n't want to go back to running a large facility again .. <p> And how would you grade with RGBLLG ? and yes i really like the precision i can get with those controls , and yes they are not mapped anywhere on the elements panel in Resolve , or any panel but the big one . <p> I do n't want the desktop real-estate taken up by the big panel , love the Pablo Nano and Baselight Slate panel 's ... they are context sensitive and have a reasonable footprint , but with the mapping in Nucoda i should be able to get to much the same place with Elements and still have room for my giant wacom ; - ) <p> Besides , the Elements have been out close to two years . I 'm sure another panel is in the works for 2015 whether it be by Tangent , BMD or @ @ @ @ @ @ @ @ @ @ another 12 films out the door , and they all will be graded with RGBLLG ... likely in Nucoda , even if i get only a year out of it , it will pay it 's self off @ 7K if i can re-use my existing hardware , and then see who 's on first by NAB 2015 ... d 
@@44332641 @4332641/ <p> My 2011 MBP running older OS X chugs , crashes freezes running Facebook and sometimes with YouTube even after complete reinstall of OS X ... whereas any of my newer macMini 's in my studio and mac pros rip through anything without a single issue . <p> Could be time for a new machine if you ca n't replace your video card ... which is likely the culprit . Lots of lawsuits against apple for issues around video chips getting too hot and going flakey and crashing machines or just acting real slow . Some authorized mac dealers can " re-ball " the chips on the video portion of the board if you 're really attached to an older mac . <p> Joe , is it relatively easy for you to do a fresh install ? I know it 's pain in ass and Mac should n't need it but there 's some chance it fixes it . Also consider removing any extra browser add-ons that might have been inadvertently installed . Macs are great at only installing what you want however or authorize but still worth reviewing . 
@@44332642 @4332642/ <p> It 's the same rubbery surface as my Lenovo laptop which I clean with eyeglass lens cleaner . I mix the lens cleaner in a little spray bottle as maybe 90 percent distilled water , 9 percent rubbing alcohol and a drop of dish soap . The ring covers are held down by magnets . You can pull them off . <p> I usually just use cloth with water on it - depends on the degree of the stains of course . But usually , a piece of damped cloth is enough to get it clean . In case of your rings being scratched ( as the soft coating on them rubs off after some time - especially the gain-ring ) , you can get replacement rings for those ( sold 30 of those alone to a bigger client here in Munich a while ago ) . <p> It 's the same rubbery surface as my Lenovo laptop which I clean with eyeglass lens cleaner . I mix the lens cleaner in a little spray bottle as maybe 90 percent distilled water , 9 percent rubbing alcohol and @ @ @ @ @ @ @ @ @ @ held down by magnets . You can pull them off . 43971 @qwx453971 <p> My problem on my panels is getting the blood , sweat , and tears off them . There 's a lot of those at the end of every session . 
@@44332643 @4332643/ <h> High Dynamic Range ( HDR ) and the elephant in the cinema <p> Hello all , I made a new video about HDR in Cinema . Some may find it interesting .. ( Note , disclosure I only mention my facility in terms of any promotion aspect of the video , but really , I hope this widens the understanding of HDR in cinema use case . ) <p> In this video I discuss the differences and issues with High Dynamic Range ( HDR ) images in Cinema . Why you do n't hear much about it , and why .. <p> I then show a quick demonstration of the main cause for the limitations of High Dynamic Range ( HDR ) in a reflective screen implementation ( Ie as in Cinema ) . <p> The problem James describes , of diffusely reflective things before the screen sending light back to the screen and so reducing the white:black contast , is classically solved by using a grey screen instead of a white one . It 's the method used when projecting in white-walled conference rooms and galleries . @ @ @ @ @ @ @ @ @ @ intraframe contrast is an impressive 5000:1 in the empty black theatre with the white screen , but due to reflective stuff in the room it drops to 500:1 . Replace the white screen with a light grey screen having 1/2 the reflectance . Now the intraframe contrast is 910:1 . Or better replace the white screen with dark grey screen having 1/10 the reflectance . Now the intraframe contrast is 2634:1 . Dare I suggest using a black screen ? <p> The contrast is salvagable , but at the cost of low screen luminance . In conference rooms and galleries this is gotten around by using extremely bright projectors . The problem with theatrical HDR projection is that there are n't brighter projectors . The screens are so large that the brightest projectors can only manage 100 nits from white . James ' explanation for this was incorrect . Much higher screen luminance simply requires too much power . The heat loads on the lenses and the whole image forming system are too great . <p> * * * <p> A final thought about the 100 nit limitation . This @ @ @ @ @ @ @ @ @ @ 0.1 nits , rod vision becomes dominant over cone vision and colors are much affected . Either this spells doom for theatrical HDR 's attempt to imitate reality , or else it means good business for colorists who must fake the darker parts of images of colorful daytime scenes for a screen where they will be seen mesopically . It means quite different grading for theatrical HDR and home HDR ( which does not have the 100 nit limitation ) -- more business . <p> The problem James describes , of diffusely reflective things before the screen sending light back to the screen and so reducing the white:black contast , is classically solved by using a grey screen instead of a white one . It 's the method used when projecting in white-walled conference rooms and galleries . <p> For an HDR example , suppose the projected DCI intraframe contrast is an impressive 5000:1 in the empty black theatre with the white screen , but due to reflective stuff in the room it drops to 500:1 . Replace the white screen with a light grey screen having 1/2 the reflectance @ @ @ @ @ @ @ @ @ @ replace the white screen with dark grey screen having 1/10 the reflectance . Now the intraframe contrast is 2634:1 . Dare I suggest using a black screen ? <p> The contrast is salvagable , but at the cost of low screen luminance . In conference rooms and galleries this is gotten around by using extremely bright projectors . The problem with theatrical HDR projection is that there are n't brighter projectors . The screens are so large that the brightest projectors can only manage 100 nits from white . James ' explanation for this was incorrect . Much higher screen luminance simply requires too much power . The heat loads on the lenses and the whole image forming system are too great . <p> * * * <p> A final thought about the 100 nit limitation . This allows little contrast range before entering mesopic vision . By 0.1 nits , rod vision becomes dominant over cone vision and colors are much affected . Either this spells doom for theatrical HDR 's attempt to imitate reality , or else it means good business for colorists who must fake the darker @ @ @ @ @ @ @ @ @ @ where they will be seen mesopically . It means quite different grading for theatrical HDR and home HDR ( which does not have the 100 nit limitation ) -- more business . <p> DC 43971 @qwx453971 <p> But would n't the grey screen produce less bright whites than a white screen seeing the same light unless it had some kind of directionally biased highly reflective coating ? All it would do , without a special coating , is the same thing as an ND over the projector lens , making everything darker . <p> And do n't these grey screens with reflective coatings have viewing angle issues ? I see the viewing angle problems all the time with projections in museums . These grey screens bounce back light very directly , in a single direction ( light does not scatter like on a usual white screen but behaves more like a ping pong or pool ball when it hits them ) and scattered light coming from various side angles is partially absorbed . The big problem is this create a very narrow sweet spot where this trick can do its @ @ @ @ @ @ @ @ @ @ are screen uniformity issues from many seats in the house . <p> If what you suggest actually worked without issue it would be cheating physics . <p> Grey is n't a color , it 's a less reflective white . It 's only grey relative to the white . It 's not like an ND in that it is n't transmissive , it 's absorbent . A " grey screen " we usually just call a low gain screen , and is a normal and effective way to increase contrast performance . Problem is that it does reduce total brightness . As Dennis suggested , the catch is actually that you need to up the projector 's output if you want to maintain brightness , and this in turn means brighter lamps , driven harder . Not a big deal on a relatively small screen over driven by a high end projector . Read : DI rooms and VERY high end home theaters . But , and this is the real catch , it does n't reflect the environment that would ever be seen in distribution . Theater owners tend @ @ @ @ @ @ @ @ @ @ ( not low gain ) screens , in order to increase the life span of the bulbs . Not to meantion at large screen sizes is just plain unattainable . In other words , we could do it in DI rooms . They already do it in home theaters . But it 'll never happen in large venues with current tech . And it 's not violating any laws of physics . <p> ... without a special coating , is the same thing as an ND over the projector lens , making everything darker . 43971 @qwx453971 <p> No because an ND over the projector lens does nothing to stop stray light and room bounce light from contaminating the " blacks " on the more reflective screen , effectively raising the blacks and reducing the contrast . OTOH , if you throw more light at the screen and reduce its reflectiveness , you can maintain the same intended brightness but reduce the influence of the stray light contamination of the blacks . If that makes more sense . <p> But it 'll never happen in large venues with current tech @ @ @ @ @ @ @ @ @ @ . 43971 @qwx453971 <p> I agree with you Juan ( as I generally do ) , but I would note that the 4K Imax/Christie Laser projector at the Chinese Theater in Hollywood is claimed to do 100 nits , and it 's a 90-foot screen . But : I have heard unofficially they have almost a million dollars ' worth of stacked projectors creating the images , and I think it was a 2-month build out to add new cooling to the projection booth , plus replacing lenses and so on . <p> Maybe a better way of saying is , it 's never gon na happen unless the theaters spend ridiculous money . But it is real HDR and it looked terrific at the SMPTE demo I saw six months ago . Ditto with Dolby Vision across the street at the El Capitan . <p> I agree with you Juan ( as I generally do ) , but I would note that the 4K Imax/Christie Laser projector at the Chinese Theater in Hollywood is claimed to do 100 nits , and it 's a 90-foot screen . But : @ @ @ @ @ @ @ @ @ @ ' worth of stacked projectors creating the images , and I think it was a 2-month build out to add new cooling to the projection booth , plus replacing lenses and so on . <p> Maybe a better way of saying is , it 's never gon na happen unless the theaters spend ridiculous money . But it is real HDR and it looked terrific at the SMPTE demo I saw six months ago . Ditto with Dolby Vision across the street at the El Capitan. 43971 @qwx453971 <p> But it is n't using a low gain screen . And I 'm still putting large scale laser projection in the future tech ( as opposed to the current tech : xenon ) category . No doubt that 's happening , but not at your local Cineplex , yet . And certainly not with current technology , <p> Grey is n't a color , it 's a less reflective white . It 's only grey relative to the white . It 's not like an ND in that it is n't transmissive , it 's absorbent . A " grey screen @ @ @ @ @ @ @ @ @ @ and is a normal and effective way to increase contrast performance . Problem is that it does reduce total brightness . As Dennis suggested , the catch is actually that you need to up the projector 's output if you want to maintain brightness , and this in turn means brighter lamps , driven harder . Not a big deal on a relatively small screen over driven by a high end projector . Read : DI rooms and VERY high end home theaters . But , and this is the real catch , it does n't reflect the environment that would ever be seen in distribution . Theater owners tend to actively underpower their projector lamps , and use high-gain ( not low gain ) screens , in order to increase the life span of the bulbs . Not to meantion at large screen sizes is just plain unattainable . In other words , we could do it in DI rooms . They already do it in home theaters . But it 'll never happen in large venues with current tech . And it 's not violating any laws of @ @ @ @ @ @ @ @ @ @ lens does nothing to stop stray light and room bounce light from contaminating the " blacks " on the more reflective screen , effectively raising the blacks and reducing the contrast . OTOH , if you throw more light at the screen and reduce its reflectiveness , you can maintain the same intended brightness but reduce the influence of the stray light contamination of the blacks . If that makes more sense . 43971 @qwx453971 <p> Yes , but only if that stray light has another source creating it - like say the normal other lights in a museum- because if it 's just the projected image creating the light then everything scales up and you are back to square one . With only the projected image as the light source in a sealed room , if you are throwing more light at the screen , then you are throwing more light into the imperfect blacks as well , and raising them both by the same amount . <p> I was talking about directional high gain grey screens specifically , which are a trick for increasing contrast within a narrow @ @ @ @ @ @ @ @ @ @ suite down the block here in Montreal which uses exactly that , and it works fine for the 4 very carefully placed seats in their otherwise empty theatre . That screen refuses to bounce back light to those seats that is not coming at it from the correct angle - so the projected image is relatively much brighter than stray light bouncing back from people in the room or the walls . If you bring down whites via grey , and blacks via grey , with just a normal grey screen , then you are bringing down both ends together and contrast remain the same . I used ND as a comparison- but I could have ( should have ) said it 's the same as closing down the projector iris . That too would bring down whites and blacks by the same proportion , making the contrast remain identical . <p> because if it 's just the projected image creating the light then everything scales up and you are back to square one . With only the projected image as the light source in a sealed room , if @ @ @ @ @ @ @ @ @ @ you are throwing more light into the imperfect blacks as well , and raising them both by the same amount . 43971 @qwx453971 <p> Not quite . The light traveling through the air from the projector to the screen is n't the direct source of the light polluting the blacks of the screen , it 's the refracted light coming of the screen , hitting the surrounds and bouncing back and around , and onto the screen . The light coming from the projector , you 'll remember is tightly focused and directed to the screen , the light bouncing off the screen is going everywhere . That 's the light pollution , the stuff bouncing back off the screen , or rather most of the light pollution . And the amount of that light stays the same , as some of the light is now being absorbed by the screen , instead of reflected by it to equal the same amount of reflected light as before , in spite of the increase light from the projector . Now same ( or similar ) number of photons are flying around @ @ @ @ @ @ @ @ @ @ , fewer of those photons are once again being reflected back out , therefore less are polluting the blacks . Raising the effective contrast . <p> Yes , OK , a grey screen can reduce the value of " multiple bounces " each time sucking up a bit before sending that light back to bounce further times . I see your logic . <p> However black levels are usually problematic mostly because of the limitations of the projector itself , and for this a grey screen does not help contrast . <p> In a dark room , with black walls , a grey screen can not get contrast to be better than the perfect-world maximum capabilities of the projector . That is what I meant by " cheating physics " . A grey screen can only help reduce the negative effects of bounced light , because it taxes each bounce by a percentage , each time , by absorption . A white screen absorbs less , taxes less , so the light keeps on bouncing . <p> Also , in the " museum set-up " a very bright projector is @ @ @ @ @ @ @ @ @ @ light level are relatively much higher than other light sources , so on that dark wall , the projector 's light is much more noticeable relative to other sources in the room . <p> Directional screens also help differentiate what is coming from the projector from what is " other light " , either bounced or from unwanted nearby light sources . <p> The use of low gain screen has been considered in creation of this video . I have read quite a few SMPTE documents and other recent reports regarding a new push into SMPTE looking into new standards for contrast in cinema .. I actually made a comment on why they did not chose to do tests on a low gain screen for the tests used in the screation of the report .. As Dennis does have a point .. But as others have mentioned .. the use of low gain screen simply does not enter the minds of the exhibition industry . One large issue you must take on board is that cinema is a business . As supersonic commercial planes ( concord ) demonstrated going supersonic @ @ @ @ @ @ @ @ @ @ using low gain screens . ( Tho this could change due to developments in laser coming down the track ) Using a low gain screens means a far more expensive projector to purchase and operate . " That is not commercial " . As such , its not a feasible outcome for the exhibition industry to adopt . At the same time , saying the projectors are not bright enough is also wrong . Sure they are bright . Its all a matter of how big a screen you want and the destruction of contrast due to all the issues with projection . Projector issues- ( Dust build up , use of Iris , etc. ) and reflected light as demonstrated in my video . Dolby is 108nit to be exact . IMAX 75nits . I chose 100 as a generalised point .. They would have done a huge amount of testing in this area .. I am sure they went for as bright as possible . Especially Dolby , as that is there thing . ( Ie over engineer it .. its why we like Dolby in general ) @ @ @ @ @ @ @ @ @ @ screens . there are always trade offs and IMAX and specifically Dolby have done the millions of dollars of investment to come up with these numbers . I am sure the 108nits is very much taking all these issues into account . <p> But as the video points out .. The elephant in the room is that its nothing like the push into Consumer Electronics HDR levels and how they are very different animals .. And yes , Trim grade passes , **29;415;TOOLONG . The hope of one master to rule them all .. does not seem to be working out that well right now .. Something 's got to give . Man is that a big elephant in the room ... <p> On the " Mesopic vision is a combination of photopic vision and scotopic vision in low but not quite dark lighting situations " From my understanding , typical 48nit content is already at a level that keeps our eyes predominantly in a photopic region . Going brighter will only help with this . <p> And finally , on Dolby Cinema . Yes the comments are right .. @ @ @ @ @ @ @ @ @ @ At a cost .. My mates in digital cinema projection development say its a million dollar projector . They say its not a new idea its just an idea no one could commercialise so has never been created . Hands of to Dolby for doing this .. ( Remember , a Dolby cinema is a revenue share of the theatre . you can not buy one .. That is how it can " work " as it does not fit into the same cinema exhibition commercial model . Its more like an IMAX model . But I am excited they have done it as it now gives the rest of the world a target for the next generation of projected image quality .. Thanks Dolby .. <p> I thank Juan for taking up the job of explaining why use of a grey screen improves the projection contrast in the case described . I spent months explaining this to people in an online forum for home theatre fans nine years ago , such as here and here . It 's tricky . <p> Use of laser light does not promise huge @ @ @ @ @ @ @ @ @ @ nm , and 630 nm lasers ( required for Rec 2020 ) requires 6200 watts oflight to achieve a 1000 nit D65 white on an IMAX screen ( assumed a perfect white diffuser ) . The three wavelengths are pretty well balanced at 1840 , 1900 , 2460 watts , respectively . Such powerful lasers are room-filling things used for machining . I do n't know if those wattages are achievable with lasers making those three wavelengths . <p> A colorist forum is not the place to do optical engineering . I think the theatrical HDR 's 100 nit white poses a colorproblem. 50 nit cinema works fine . Vision adapts , so after a few minutes the 50 nit white looks very light . With cinema there is little picture below 0.1 nits , where rods dominate in vision , and those dark parts look OK because colorfulness is n't expected there . But theatrical HDR is shooting for a huge contrast range while hardly increasing the white luminance . So theatrical HDR is requiring colorfulness at luminance levels where color vision functions poorly . Adaptation ca n't surmount @ @ @ @ @ @ @ @ @ @ at Siggraph a few years back , of a system of projecting an image onto an e-ink style surface , so that the screen surface itself could display a corresponding image , allowing it to effectively alter the gain on a per-pixel basis . Provided a surprisingly stunning HDR image , a different feel than a self-illuminated HDR display . <p> I saw an HDR demo at Siggraph a few years back , of a system of projecting an image onto an e-ink style surface , so that the screen surface itself could display a corresponding image , allowing it to effectively alter the gain on a per-pixel basis . Provided a surprisingly stunning HDR image , a different feel than a self-illuminated HDR display . <p> It was about 12 " wide and did n't have motion picture capabilities . 43971 @qwx453971 <p> This ? <p> In Dolby 's viewer-preference study , an NEC 2K digital-cinema projector fired onto a 21 " monochrome LCD panel , resulting in a very high-contrast display that was also capable of very high peak brightness . ( Courtesy of Dolby Laboratories ) <p> @ @ @ @ @ @ @ @ @ @ did n't pass through the e-ink surface , it was projected onto it . Like displaying a high contrast black-and-white image on an old-fashioned Amazon Kindle , then projecting a complimenting color image right onto the kindle ... so front projection . I 've been googling but ca n't find images of the demo set-up ... It was n't well suited for cinema projection , but did demonstrate how relying on a screen with a uniform gain is a limitation for projected HDR . <p> I thank Juan for taking up the job of explaining why use of a grey screen improves the projection contrast in the case described . I spent months explaining this to people in an online forum for home theatre fans nine years ago , such as here and here . It 's tricky . <p> Use of laser light does not promise huge luminances on huge screens . Using 467 nm , 532 nm , and 630 nm lasers ( required for Rec 2020 ) requires 6200 watts oflight to achieve a 1000 nit D65 white on an IMAX screen ( assumed a perfect white @ @ @ @ @ @ @ @ @ @ at 1840 , 1900 , 2460 watts , respectively . Such powerful lasers are room-filling things used for machining . I do n't know if those wattages are achievable with lasers making those three wavelengths . <p> A colorist forum is not the place to do optical engineering . I think the theatrical HDR 's 100 nit white poses a colorproblem. 50 nit cinema works fine . Vision adapts , so after a few minutes the 50 nit white looks very light . With cinema there is little picture below 0.1 nits , where rods dominate in vision , and those dark parts look OK because colorfulness is n't expected there . But theatrical HDR is shooting for a huge contrast range while hardly increasing the white luminance . So theatrical HDR is requiring colorfulness at luminance levels where color vision functions poorly . Adaptation ca n't surmount those physiological limitations . <p> DC 43971 @qwx453971 <p> Yes Juan is truly gifted at making things clear . <p> What you bring up about low light vision is fascinating . Watching an OLED screen in a dark room with the OLED @ @ @ @ @ @ @ @ @ @ wondered if it was " just me " . <p> To some degree I worry that all advances focus more on numbers and marketing games than physiological **27;446;TOOLONG 4K is another example . From the vast majority of reasonable seating positions , for the vast majority of humans , it is not actually distinguishable from 1080p/2K . <p> No , that one is cool though - the light did n't pass through the e-ink surface , it was projected onto it . Like displaying a high contrast black-and-white image on an old-fashioned Amazon Kindle , then projecting a complimenting color image right onto the kindle ... so front projection . I 've been googling but ca n't find images of the demo set-up ... It was n't well suited for cinema projection , but did demonstrate how relying on a screen with a uniform gain is a limitation for projected HDR . <p> But it is n't using a low gain screen . And I 'm still putting large scale laser projection in the future tech ( as opposed to the current tech : xenon ) category . No doubt @ @ @ @ @ @ @ @ @ @ , yet . And certainly not with current technology , 43971 @qwx453971 <p> Well , you can technically buy a Christie 6P projector now . It 's just beaucoup bucks . Both the Chinese TCL Imax theater and the El Capitan -- surprisingly -- were not using curved high-gain screens , though both are set up for Laser 3D . <p> It 's an interesting technology , and as I said back in April , I was surprised that the blacks were as solid as they were and the color looked perfectly fine to me . I 'd be curious to see what a light valve vs. laser comparison would be , and if laser is superior in all ways . I also have n't seen a price list on what a real-world laser projector costs , and whether it 's going to have any effect on the world of post . <p> And finally , on Dolby Cinema . Yes the comments are right .. They have build a truly marvelous projector , but .. At a cost .. My mates in digital cinema projection development say its a @ @ @ @ @ @ @ @ @ @ idea its just an idea no one could commercialise so has never been created . Hands of to Dolby for doing this .. ( Remember , a Dolby cinema is a revenue share of the theatre . you can not buy one . 43971 @qwx453971 <p> Small correction : I was told that this is still a Christie 6P . The Dolby Cinema idea is more of a " package " with sound , calibration , specific standards , basically a 2015 version of THX . As far as I know , Dolby is not making or selling a laser projector , but I would assume there are a handful of models out there that are certified for Dolby Cinema . <p> What I think Dolby Cinema basically is is their version of " Premium Large Format " theaters , or a more affordable version of Imax . My opinion is that many theater owners are looking at nicer chairs , bigger screens , bigger speakers , brighter pictures , 3D and so on as a way of charging another $5 per ticket . Normally I 'm skeptical about this @ @ @ @ @ @ @ @ @ @ convinced me it looked terrific . Whether it 's worth $20 per ticket is another question . The presentation is very slick and well done : <p> Dolby does make their LCD mastering monitor . I get conflicting info on the Dolby HDR monitor depending on who I talk to . Some say it 's only out for evaluation , some say it 's only being leased , and others say it is actually available and being sold now . What the price is , I du n no . I also do n't know the details on whether there 's a 4K version yet . 
@@44332644 @4332644/ <h> Graded on wrong Timeline framerate <p> I graded a project with a timeline set to 24fps but the footage was 23.976 . All of my exports have a playback problem . Is there a workaround or fix for this without starting a new project and regrading ? <p> An alternative to color tracing would be to grab a still set from the 24p session and bring it in to your 23.976 session and paste the grades from the stills . I do n't think auto color trace works between timelines of different frame rates , all though I could be wrong . Manual color trace should work though . <p> Traditionally , once you set and saved the project framerate in Resolve that was it . A lot of us have learned this lesson . This may have changed in Resolve 11 or 12 , but I have n't tested it . Since then , when I 'm in Resolve I have setting 23.976 so burned into my cortex , I 've have n't tried it at 24 for probably three years . <p> Traditionally , once you @ @ @ @ @ @ @ @ @ @ it . A lot of us have learned this lesson . This may have changed in Resolve 11 or 12 , but I have n't tested it . Since then , when I 'm in Resolve I have setting 23.976 so burned into my cortex , I 've have n't tried it at 24 for probably three years . <p> I 've had Color Trace fall down too many times . Now I just manually copy/paste grades using the Memories or Grab Still . Took me 30 minutes on a 90 minute show , so tedious , yes , but not the end of the world . <p> Remember though , that all tracked windows will need to be re-tracked , regardless of which method you use . <p> I 've had Color Trace fall down too many times . Now I just manually copy/paste grades using the Memories or Grab Still . Took me 30 minutes on a 90 minute show , so tedious , yes , but not the end of the world . 43971 @qwx453971 <p> I 've used both , but starting after the first month @ @ @ @ @ @ @ @ @ @ ColorTrace has been pretty reliable . I generally have to use it in manual mode , and I do n't like the interface ( sliding around the timelines is a drag ) , but it has n't crashed on me in many weeks . I just used it a few days ago and it was flawless . Carrying along the tracking , keyframes , and versions is of huge importance . <p> I have done it with stills before , but that 's a much worse drag . It takes a lot longer than 30 minutes if you have to change 100 shots and retrack them all -- on top of checking any new shots that have been dropped in . <p> I graded a project with a timeline set to 24fps but the footage was 23.976 . All of my exports have a playback problem . Is there a workaround or fix for this without starting a new project and regrading ? 43971 @qwx453971 <p> Yes there is a workaround . Highlight everything on you timeline and hit apple c or copy in the top menu . Next delete @ @ @ @ @ @ @ @ @ @ change from 24fps to 23.98 ( which will now be allowed since we do n't have a timeline ) . Create a new timeline and paste all your shots back into the timeline . Regards , Paul 
@@44332645 @4332645/ <h> Prep Resolve timeline for protools guys <p> I am about to finish the first cut of a movie shot in Ecuador . I do edit in Resolve12 . We run into some problem with the audio department . I did sycronize the whole movie inside Compound clips. 1st track is the camera audio with is a mix of all microfones used . The second track is a 5.1 track with includes all mic tracks supposedly separated . Going from Resolve to Premiere via XLM and from there exporting a AAF to Protools does n't let 's the audio guy open up all the separated audio track from the tascam . What is doe wrong or can be done ? ? ? We tried everything . Exporting all available options resolve gives me for audio . <p> Getting audio from Resolve to ProTools is very buggy and has cost a lot of people hair . <p> I have found the following procedure to work most of the time : <p> - In the timeline open the audio toolbar , track mixer . Force multi-channel track ( stereo , surround ) @ @ @ @ @ @ @ @ @ @ in each track . You need to decouple the stereo pan feature . It should say 1,2 instead of the default 1+2. - Use the default ProTools export in the delivery page ( no need to trip through Premiere ) , but adjust the number of channels to match the timeline ( 2 or 5 ) instead of the default 1 . <p> In my experience the AAF files created in this manner will open properly in ProTools . <p> Just did it myself a few days ago with Resolve 12.5.5 and ProTools 12 . <p> Ok I will trie that . The audio guy says something like that the audio tracks from the tascam are kind of bound together and that why he ca n't open them up as individual tracks . Maybe its what you say about decouple the stereo pan feature . I am not sure if I messed it up by bringing in 1 combined tascam track into the compound clip instead of its individual tracks ? ? ? <p> This allows you to just copy over the original audio files , then bring in the @ @ @ @ @ @ @ @ @ @ of that . The trick is in getting all the correct timecode clips in the audio session . It can be done and it works well , but it 's not cheap . <p> Ok I will trie that . The audio guy says something like that the audio tracks from the tascam are kind of bound together and that why he ca n't open them up as individual tracks . Maybe its what you say about decouple the stereo pan feature . I am not sure if I messed it up by bringing in 1 combined tascam track into the compound clip instead of its individual tracks ? ? ? 43971 @qwx453971 <p> Sorry , was traveling . Yes , the issue has to do with stereo tracks in Resolve . So if the Tascam recorder imports as stereo ( or 5.1 ) that can trigger this . Though we should bend the software not the audio track layout , since there 's a reason to couple them . <p> When you read the default ProTools export there seem to be a lot of audio files missing , which @ @ @ @ @ @ @ @ @ @ Resolve has the stereo panning control and when enabled somehow the export fumbles it . By decoupling it and exporting each track of a pair without panning data , then the files show up properly . <p> On another forum , somebody said that they were able to select the Avid round-trip AAF export , then turn off video and just export audio-only with handles , and this worked for Pro Tools . 43971 @qwx453971 <p> I can confirm this . I just tried it on a recent project . Some caveats though - in the AAF export you can indeed turn off video export . You can also select various container/codec options in audio , however only MXF exports seem to work . I tried WAV and failed to import properly . I can also confirm that it does properly export handles though you have to set those in the video tab , which is non-intuitive when you just turned video off . <p> One advantage of this route is , that the export is faster since it does n't have to render the video in case you do @ @ @ @ @ @ @ @ @ @ video track that is usable in ProTools. 
@@44332646 @4332646/ <h> What makes Baselight different ? <p> Very good interview with Peter Doyle . It should really been titled differently because it really shows what a baselight system is really good at , integration . Taking advantage of the difference of the deliverables and hold the whole workflow with post , fx included , under control . <p> For people looking for more of the same ; Filmlight have a couple of similar interviews with Maxine Gervais about her work on American Sniper and Sully , BMD have a few with CO3 LA guys on the Resolve product page and Autodesk has an excerpt from a NAB demo by Steve Scott on the Revenant . <p> I agree , Peter Doyle does amazing work , and in a " cost no object " situation , I would definitely consider a top-of-the-line Baselight X for a facility . It 's interesting to note that Doyle choose to work solely with just a keyboard and mouse , but I have to say Baselight 's Blackboard 2 panel is probably the best purpose-built colorist control surface in the world . <p> 1 @ @ @ @ @ @ @ @ @ @ TK panel , not a Blackboard. 2 . That picture is not Peter 's theater ( it is a room at Dolby 's facility in Soho ) . <p> Mark is incorrect . Peter works with a simple trackball panel , keyboard , and mouse . What he does n't use is the Blackboard , in part because he feels it 's too big and busy . He also does n't use the trackball panel nearly as much as most colorists because he tends to use other approaches to color manipulation . <p> Yeah , that surprised me , too -- it looks to me like there are some custom macro panels and later shots where " somebody " is using a Blackboard . I have no idea how he works on an actual project vs. just a demo . <p> I would say you could do just fine with just a trackball panel and keyboard as long as you had lots and lots of time to work on each shot , or had a hall-full of assistants to pick up overflow work , tracking , and matching . Printer @ @ @ @ @ @ @ @ @ @ in the right color space . At the level Mr. Doyle is working , I 'd say only the results and the client 's satisfaction matter . <p> I enjoyed his interview very much , though I noticed he provides very few clues as to actual work methods beyond a general philosophy . <p> What I find interesting is that he 's doing a lot of stuff with custom transforms , but it does n't seem to be very efficient as he 's only doing 1 movie a year , when other " manual " colorists put out 10-20 movies a year <p> Often , when you have a workflow for major studio features the ' lead colourist ' does little of the actual work . They tend to set looks and colour concepts , and leave the actual grading to ' jobbing ' colourists . ( And to be honest , on a lot of the major features I 've been involved with over the years , the real final ' look ' is often down to the ' jobbing ' colourists - not the lead colourist . Not @ @ @ @ @ @ @ @ @ @ seen over the years . ) <p> Often , when you have a workflow for major studio features the ' lead colourist ' does little of the actual work . They tend to set looks and colour concepts , and leave the actual grading to ' jobbing ' colourists . ( And to be honest , on a lot of the major features I 've been involved with over the years , the real final ' look ' is often down to the ' jobbing ' colourists - not the lead colourist . Not being negative here , just commenting on what I 've seen over the years . ) <p> Steve 43971 @qwx453971 <p> Certainly seemed that way on the hobbit movies with Peter , and 6 jobbing colourists for 6 months . Oh to have that time and budget . <p> He 's kind of the R&amp;D color dpto of sorts But seriously , I really enjoyed this because it 's exactly not about the actual color grading work but it 's more on the conceptual philosophy of a lot of works , the umbrella . Also about @ @ @ @ @ @ @ @ @ @ this issue on a feature that the colourist , on the competing plataform , " upgraded " the crossdissolves to the FX house . The FX could n't deliver the crossdissolve like the offline intended , they had a particular optical look , because the color work was still being developed . As usual , the schedule was very tight and they were in different cities . If this was done on BL , we could have relied on the BL/Metadata/plugin for Nuke , and actually deliver the cross dissolve as intended . This is a tiny detail of how the whole BL integration works . <p> What I find interesting is that he 's doing a lot of stuff with custom transforms , but it does n't seem to be very efficient as he 's only doing 1 movie a year , when other " manual " colorists put out 10-20 movies a year 43971 @qwx453971 <p> Peter does a lot more than " one movie a year . " I do n't know where you got that idea . <p> All of the supervising colorists I know are @ @ @ @ @ @ @ @ @ @ capable of putting in hours that would make a lot of people reconsider grading as a career altogether . Sounds like some of you are hanging around with the wrong crowd . <p> Peter Doyle is a monster ... I remember I listened to a podcast with him once and I could only understand 30% of what he was talking about . 43971 @qwx453971 <p> His background in both photography and visual effects make him a very unique artist . He uses a lot of color science knowledge , as well as a collaboration with the color science department , to formulate his approach to look creation on every project he does . I do n't really know any other colorist quite like him . <p> All of the supervising colorists I know are the hardest working people in the building . They 're capable of putting in hours that would make a lot of people reconsider grading as a career altogether . Sounds like some of you are hanging around with the wrong crowd . 43971 @qwx453971 <p> Quite true . You would be very hard pressed to find @ @ @ @ @ @ @ @ @ @ that Steve Scott , Peter Doyle , or Stefan Sonnenfeld do ( just to mention a few ) . <p> Quite true . You would be very hard pressed to find people in this industry who are putting in the hours that Steve Scott , Peter Doyle , or Stefan Sonnenfeld do ( just to mention a few ) . 43971 @qwx453971 <p> I would also cite some of the episodic people like Sparkle at Technicolor . A lot of these guys routinely do 80-90 hours a week , every week , with or without several assistants . It 's their name that goes on the project when it hits viewers ' screens , and I think each of them works very hard to make sure the final results measure up to that . 
@@44332647 @4332647/ <p> I would say the rates should be similar to those for assistant editors . In a union gig , that 's $30/hour or $300 for a 10-hour day , $1500/week . I have seen assistant colorists at non-union companies in LA make anywhere from $800/week to $2000+ a week depending on conditions and hours . And , of course , experience . Having a great assistant makes a huge difference to any session . <p> Marc , why is it that non-union companies can pay more to assistants than the union companies ? Is that a common trend ? Also , I 'm from Chicago , do you know anything about the wages for assistant colorists out here ? <p> Marc knows more about unions than I do , and I believe he 's mentioned before at some point , that those are usually minimum rates , not necessarily maximum rates . The actual rate you make depends on a lot of factors beyond even union vs non-union . For instance , if you 're a freelancer you generally get paid a bit more since your employment is @ @ @ @ @ @ @ @ @ @ . If you 're staff , you might get a lower hourly rate , but you have the security of full-time employment and benefits . <p> Also , the term " assistant " covers a lot of territory . Some assistants might only do fundamental tasks like suite set-up , conforms , and render-outs , while others are very capable and able to step in anywhere from setting base grades or spreading looks , to doing roto , light vfx , or compositing . <p> So , your " wage range " is dependent on all of those things , multiplied by the kind of shows you 're doing ( web/indie ---&gt; major network shows or features ) , divided by how much of a life you need . Hopefully that comes to an amount that you find " fair " , but in the end it 's really up to you , as much as it is your employer . <p> Marc , why is it that non-union companies can pay more to assistants than the union companies ? Is that a common trend ? Also , I 'm @ @ @ @ @ @ @ @ @ @ for assistant colorists out here ? 43971 @qwx453971 <p> I think it 's a tough market out there . I see a lot of facilities that are kind of stretched thin , forcing assistants to work longer hours to handle more projects in a longer shift . You may be doing the work of 1.25 or 1.5 people , but only getting paid as one person . <p> As Jason above says , a union company would only establish the minimums -- higher pay would be something to negotiate . If they really want somebody , they can always through in perks : more vacation time , high pay , sometimes a bonus . There are definitely places where hard-core assistants are actually doing colorist work around the clock ( technically supervised by the lead colorist ) , so it could well be more responsibility than just setting up timelines , importing media , updating lists , replacing shots , or rendering out finals . <p> I know of nothing specific on Chicago rates . I have known colorists in Chicago , and I think it 's fair to say @ @ @ @ @ @ @ @ @ @ staffs in NY and LA . Freelance is more of a cloudy area . 
@@44332648 @4332648/ <h> MC round trip issue <p> I 'm working on a series that I 've been grading in Resolve , exporting AAF and importing back into MC7 . It 's been fine so far . This latest episode , When I import the AAF to MC , I get this error : Exception : **32;475;TOOLONG , range:473 . Any ideas ? <p> I 'm thinking this may be because some original clips may have had less handles then I told Resolve to render with . I hope . Re Rendering with 0 handles to see if this helps . <p> I ran into issues round tripping avid to resolve and back . I ended up rendering out with a specific frame handle from resolve , then duplicating the original sequence in avid and decomping with matching handles to my resolve media . Then relink the decompressed seq in avid to the media from resolve . This is a summarized basic breakdown of what I did . It worked perfectly , and if you need more detailed workflow info , let me know . <p> I ran into issues round @ @ @ @ @ @ @ @ @ @ rendering out with a specific frame handle from resolve , then duplicating the original sequence in avid and decomping with matching handles to my resolve media . Then relink the decompressed seq in avid to the media from resolve . This is a summarized basic breakdown of what I did . It worked perfectly , and if you need more detailed workflow info , let me know . 43971 @qwx453971 <p> This did n't work , unfortunately . It must be something with this project because , even if I export a 10 second AAF from MC , grade in Resolve , and round trip AAF back to MC , I get the same issue . I was able to render out a DNxHD flattened QT . Not ideal but got me 90% there . <p> I did n't use the aaf . I just copied rendered DNxHD files from Resolve project to a new numbered folder in Avid media folder , let Avid build data base , then told Avid to relink seq to that media . I found using the aaf from resolve to be quite buggy , @ @ @ @ @ @ @ @ @ @ I did n't use the aaf . I just copied rendered DNxHD files from Resolve project to a new numbered folder in Avid media folder , let Avid build data base , then told Avid to relink seq to that media . I found using the aaf from resolve to be quite buggy , so I just do n't even use it . 43971 @qwx453971 <p> I tried your method of relinking from the original sequence ( made a copy , decomposed with 0 handles , rendered from Resolve with 0 handles ) . Nothing relinked . I 've noticed that even if you choose 0 handles in Resolve , it usually adds a few frames . I tried decomposing with 1 , 2 and 3 frame handles as well and nothing worked . <p> Hi Ken , I run into this sometimes coming back from Resolve . It 's most likely a motion effect that 's been incorrectly translated by Resolve . What I do in AVID is set an in to out on the first half of the show and play in-out on that region . If I @ @ @ @ @ @ @ @ @ @ second half . Keep whittling it down and you 'll find the problem clip or clips . Often they appear in the Avid timeline with no name or info . Then just fix the clip by cutting in the master clip and reapplying the speed effect . You may have to do is a few times on different clips . I always render with handles in case this comes up . Santiago <p> Hi Santiago , I 've had what you describe in the past and have gone through the seqeunce in Avid as you 've mentioned and removed dissolves ( usually dissolves to filler ) . This case , however , the AAF from Resolve gives me an error during import , not playback , and does not finish the import .. I ca n't ' get the sequence back to Avid other than exporting a flattened QT out of Resolve . I 'm hoping it is just related to this one project . <p> Hi Ken , I have n't seen that particular behavior before . Were these clips AMA or DNX media ? Did you have mixed frame @ @ @ @ @ @ @ @ @ @ mixture of 23.976 and 29.97 DNX media and have n't had that particular problem . Did you have any issues with playback in Resolve by any chance ? A few times I noticed that Resolve did n't recognize some spanned DNX MXF files and brought in the first part of a clip and then went black for the back half . I had to manually import the second MXF file from the AVID MediaFiles folder , and cut it into the Resolve timeline. 
@@44332649 @4332649/ <h> Speedgrade vs Resolve <p> I know there is a similar topic as this , but I have a more concrete claim I would like to hear your opinions on . <p> I have worked in Autodesk Smoke the past 6 months and started to love its grading tools , actually even more than Resolve 's . Why ? <p> Because in Smoke you have control over Lift , Gamma and Gain , exactly like Resolve . But also , you can dive into and adjust RGB channels , Lift , Gamma and Gain in specific areas like shadows , midtones , and highlights . This is really great if you would like to do changes in only the highest or darkest parts of the **27;509;TOOLONG for example , which you can not do in Resolve . <p> Although , I know the workflow is n't the best in Smoke compared to Resolve , like adding nodes and secondary corrections etc . <p> But then I jumped into Speedgrade to see if you could get more control over specific parts of the image than in Resolve , and actually @ @ @ @ @ @ @ @ @ @ control over the Lift , Gamma and Gain , but you can also dive into those settings in only the shadows , midtones and highlights , which is awesome . And you also have a pretty good workflow when you want to add nodes ( which in Speedgrade would be layers ) . <p> So , long story short . Because of the extra tools in Speedgrade which makes you have control over more specific parts of the image than you have in Resolve , is there anything that Resolve has that still makes it a better software for grading ? I ca n't come up with anything ... <p> I 'm sorry if my swedish version of english makes this text impossible to understand . <p> I 'll say that saying one tool is better than another is a bit difficult now a days . IMO they all have their strengths and weaknesses , and any modern color system will give you all the tools you need to get work done . Some have an easier time performing certain functions , or lend themselves to a particular type of @ @ @ @ @ @ @ @ @ @ way you want to grade . To me qualifiers offer that very fine control in the way I like . <p> Because in Smoke you have control over Lift , Gamma and Gain , exactly like Resolve . But also , you can dive into and adjust RGB channels , Lift , Gamma and Gain in specific areas like shadows , midtones , and highlights . This is really great if you would like to do changes in only the highest or darkest parts of the **27;538;TOOLONG for example , which you can not do in Resolve . . <p> I have n't ever encountered any issues doing fine tuning work in the shadows , midtones , highlights within each color channel using the curve controls in Resolve . Also , Resolve still has the best tracking around . Having said that , the beauty of today 's grading climate is the sheer numbers of great grading applications available each with their own strengths and ways of working . For me Resolve is still tops for its toolset and affordability . I liked Speedgrades layers as it reminded me a @ @ @ @ @ @ @ @ @ @ also found the interface clunky as well as the SDI monitoring limitation that Hillary mentioned . It 's an NVIDIA SDI out and that 's it . If you have a lot of man hours in Smoke I 'd stay with that since you have a finishing system ready to go and can charge a hefty day rate for good Smoke work . <p> Log controls in Resolve will get you there , but so will a Luma Key , or even Curves . You can control whatever part of the image you like , and it 's all in real-time . <p> I like the way the Smoke CC/CW handles an image , but really wish they would just combine the two . On their own they each feel somewhat dated and incomplete . If you compare it to Lustre below , you 'll see what I mean ; film or video controls , curves , 12 secondaries , windows , and tracking all in one pane . If they would just add that and some grade management to Smoke it would become a great color tool . <p> @ @ @ @ @ @ @ @ @ @ a lack of broadcast monitoring on the Mac . I think once that hits and people can actually see what they 're doing , many people will take a hard look at it again . <p> I think the ( hopefully ) incoming tighter integration of SpeedGrade and Premiere Pro will make it a default grading tool for many users currently not involved in color correction , or resorting to Colorista or MB Looks . Prepare for the spike in popularity with the next release of Adobe software , most likely around NAB ( previews ) , and then May ( actual release ) . I 'm certain the problem with monitoring will be resolved , along with several nagging issues that the current version of SpeedGrade has . <p> Not to completely drag this thread off topic , but with regard to Smoke ... I could n't agree more . Nobody wants Lustre in SMac 2014 more than me , but I 'm thinking it 's probably more realistic , and probably better for everyone , if they worked out a solid media/timeline exchange with Resolve . As much @ @ @ @ @ @ @ @ @ @ just do n't see the realtime capabilities of SMac being adequate for or equivalent to what we all now expect from a legit grading tool . <p> Thank you all for your comments and thoughts . Really helpful to hear what you all have to say ! <p> I do n't completely agree that you can get the same results with the curves in DaVinci . Curves is a great tool , no doubts , but it 's also pretty easy to break the image and destroy some color edges if you make too many points and separate the curve into many pieces . This is especially something that it easy to do in DaVinci 's curves , I do n't know what kind of technique they 're based on but they are not giving the same results as the curves in AE and Premiere . It both looks and feel different to play with DaVinci 's curves , but of course they are not useless in anyway , although I think it 's easier to destroy the colors with many points in those curves than in AE , Premiere @ @ @ @ @ @ @ @ @ @ such a big upside that it 's making it the number one . 
@@44332650 @4332650/ <h> Heavy color cast <p> I 'm an editor who would like to eventually move into color correction . I know my way around Apple Color and am now dipping my toes into Resolve Lite . <p> A reoccurring situation at my current gig is footage with a very heavy yellow color cast . I have a solution for REDRAW footage that includes using the color temp and tint to get the footage pretty close to neutral . However a job came in from a 5D last week and I ca n't seem to find a good solution for removing the color cast in Resolve . <p> Source video : <p> RGB Parade : <p> My attempt : <p> What I 've done so far is to use a qualifier to grab everything yellow and desaturate it . Then in another node use the three way to try and make the result neutral . I 'm not happy with how it 's turned out . I 'm finding it very hard to balance the image . The chairs are especially unnatural and I have no idea how this solution @ @ @ @ @ @ @ @ @ @ If this was a Steven Soderbergh film , he 'd want you to leave the yellow cast alone . All kidding aside though , I actually think you did a really decent job . With the R3Ds you have control , but with the H.264s you 're dealing with 8-bit 4:2:0 encoding so you just do n't have a ton of room to swing it around . If you are dis-satisfied with the results that could be a large part of the reason why . <p> You are lucky , in this shot at least , that you have a predominantly white environment and that the yellow cast is consistent across the entire frame . So , this could largely be taken care of in a base grade . Before you even bother qualifying colors , see if heavily pulling down the red and green in fairly equal amounts , and pushing blue helps . You can also try pulling the saturation down a small amount as well . You will have a neutral image when the ranges for Red , Green , and Blue are all similar in your @ @ @ @ @ @ @ @ @ @ Resolve , another way to quickly get it close is to press Option+A , and Resolve will attempt to balance the shot for you . <p> Qualifying colors and desaturating would be a last resort kind of thing for me on such a wide swath . I would try the rgb mixer and see where that gets you trying to white balance . Then try additional traditional color balance . Hue vs hue or hue vs sat is another tool ( make a wide selection or it will Solarize pretty quick ) Then sat vs luminance to desat highlites . Then you could qualify the faces to bring them back .... But yeah it does n't look too bad in your example from here . But close up it might look a bit crunchy . <p> Normally you 'd do RGB offset for this ... But clearly you 've got heavy crushing in the highs on the red channel and low on blue ( green seems ok , which is actually a good thing ) so I 'd start with dropping the red offset . Raising the blue ... then @ @ @ @ @ @ @ @ @ @ you do this , do n't want them completely out of whack . Then once you got that as close as possible . Start playing with secondaries . You do n't need to qualify yellow , could just do a hue-sat curve adjustment . But they should be fairly mild adjustments . <p> Better question to ask is what went wrong here ? Seems like maybe a crazy profile ? <p> Better question to ask is what went wrong here ? Seems like maybe a crazy profile ? 43971 @qwx453971 <p> I ca n't be sure , but i think nothing went wrong . Looks like it was shot in a cleanroom - they light them using only yellow light so it wont mess with the photolithography . Kind of like safelights in a darkroom . <p> It actually looks like that in there . ( If it really is a cleanroom - could be wrong about this - i 'm just assuming because of the suits ) <p> So now it becomes a creative decision - do you make it look " right " by artificially balancing it ? @ @ @ @ @ @ @ @ @ @ a bit more to make it dramatic ? <p> Then I 'd say leave it , maybe pull saturation back some and pull back the clipping if you can . When you try and take stuff too far from where it is it starts looking unnatural . Color / image purity and all that <p> I mean like i said - i could be wrong , i havent seen the rest of the show or the context - but in the before shot look at the computer screen , and the hallway past the door in the background - they do n't  have the yellow cast . <p> Better question to ask is what went wrong here ? Seems like maybe a crazy profile ? 43971 @qwx453971 <p> This was shot in a microchip fab , so the yellow is a result of the actual light in the room . I could leave the color cast , but that 's just not the precedent that has been set around here . Any fab footage is corrected so that the white ' bunny suits ' look white . <p> I like @ @ @ @ @ @ @ @ @ @ wafer fab rooms before and this is the actual color of the lights in the room . Personally I like the drama of the yellow lights . I think you did a pretty good job if this is the footage you have to deal with . It 's great to learn on footage like this but do n't kill yourself if it 's not perfect . With footage like this , only us nutjobs will notice such things anyway . Play around a little , but sometimes you have to know when to move on . If you do n't seem to make much progress with the other 's suggestions I would say stick with this and drop the Blues in the Blacks a little and maybe add some contrast from the log window . Also if the poor guy is stuck with the 5D have him balance off a white card in the room and shoot Technicolor style . Or I could be completely wrong . <p> Another way to make the blacks a bit more natural might be to use the lum vs sat curve to drop their @ @ @ @ @ @ @ @ @ @ lift onto what you have - and i think it could look pretty good . <p> I had a similar shot to do once . Shot in a ' restroom ' with white wall tiles . Everything looked green though . I tried all sorts of different things but I could n't push the blue far enough to balance the shot . I finally cracked it when I used the curves ( this is on Pablo ) to pull in the green channel into the blue channel . Can you do that in Resolve ? There 's a channel mixer ? I actually think you 've done a pretty good job and you may not get 5D footage looking any better . <p> I had a similar shot to do once . Shot in a ' restroom ' with white wall tiles . Everything looked green though . I tried all sorts of different things but I could n't push the blue far enough to balance the shot . I finally cracked it when I used the curves ( this is on Pablo ) to pull in the green channel @ @ @ @ @ @ @ @ @ @ Resolve ? There 's a channel mixer ? I actually think you 've done a pretty good job and you may not get 5D footage looking any better . 
@@44332652 @4332652/ <h> AME Discreet audio vs Mono <p> If I have an 8 channel file that I 'm encoding in AME , in this case , ProRes source being encoded to XDCam. 8 Channels of audio . I 've selected 8 channels in the preset but then it gives me the Mono or Discreet checkbox . What is the difference ? <p> Be careful and double check everything . Last year 's version of AME was swapping C and Lfe , and occasionally duplicating one of the tracks . Needed to use friggin compressor to deliver a few films . I think Gavin is correct about the mono vs. discreet . The places you need to be careful are when you choose any of the preset 6+2 's or any quicktime compatible 5.1 setup . Good luck . <p> Both instances of either Mono or Discreet showed the same results . It QT player info window , it showed the file had audio but did not break out the tracks as it usually does . When AMAing into MC to check , both files came in with proper channels . @ @ @ @ @ @ @ @ @ @ Discreet showed the same results . It QT player info window , it showed the file had audio but did not break out the tracks as it usually does . When AMAing into MC to check , both files came in with proper channels . Still confused . 43971 @qwx453971 <p> i Have to say I have found AME quite tricky with audio also premier <p> and adam has mention one of the issue we hit in the past .. to the point we abandoned it and we i don , t use it for any thing complex with audio <p> I have been using FCPX for any thing like this as its easy to maintain the track layouts , as you bring stuff thru it and also if its going to DCP its a snip to render out the individual channels and have them come out labelled correctly or bring them out as a muti track .. still stay labelled correctly <p> as there are two line ups in a time line for 5.1 , normally we would use top down L , R , C , LFE , @ @ @ @ @ @ @ @ @ @ that are stacked L , C , R , Ls , Rs , LFE <p> so you need to keep your wits about you as what 's going where .... AME did , nt help Ha ! <p> any way <p> i think they have up dated resolve so you can do multi channel multi track ..... which was a major issue in the past <p> and i do know you need to be able to do the discrete thing for some reason for delivery ... but for the life of me i Can , t remember i bet one of our QC guys could tell me in second 
@@44332653 @4332653/ <h> Adjusting luminance only , per-channel <p> I swear I knew how to do this a few days ago , but it fell out of my head . I 'm trying to manipulate only the luminance of , say , the Red channel in RGB , without affecting the color . Like a r-only curve set to Luminosity Mode in Potatoshop . <p> In Photoshop , make a dupe of the BG and set its blending mode to Luminosity . Then adjust the Red curve . No color change . Tonal change only . It 's an occasionally-useful way to manipulate contrast . ( It 's also super easy to posterize your image . ) <p> What Bart is saying is correct . The only thing I can think of that you might be describing is modulating the levels ( and therefore the percentages ... 0.299R + 0.587G + 0.114B ) in the RGB color mixer while it 's set to Monochrome Only with Preserve Luminance off . That only gives you a black and white image though , so if your result needs to be color , you @ @ @ @ @ @ @ @ @ @ Multiply to mix it back in with your original . Since you 're multiplying two nodes , you 'll need to boost Saturation on your original to get your color back . <p> I do n't know how to technically do the last step in Resolve , Multiply is only going to darken the image , while overlay will add contrast on top of the original . Normally you 'd want to replace the L channel in Lab color space , and Resolve 's Combiner node only works in RGB as far as I know . <p> The problem is , I do n't think there is a functional equivalent to a " luminance " blend mode in Resolve , but multiply in the node tree above works pretty well . The full range is there , from totally crushing the image to blowing it out , all using the RGB mixer . <p> This might be worth a go . In node 02 above , turn saturation down to 0 , and in node 04 in the Primary Bars section turn the Y bar of Gain all the way @ @ @ @ @ @ @ @ @ @ Layer node select Add . That way you separate colour form luminance , and you can use the splitter combiner nodes preceding the desaturation node to target the luminance of the individual channels . <p> This might be worth a go . In node 02 above , turn saturation down to 0 , and in node 04 in the Primary Bars section turn the Y bar of Gain all the way down ( leaving just colour information ) . In the Layer node select Add . That way you separate colour form luminance , and you can use the splitter combiner nodes preceding the desaturation node to target the luminance of the individual channels . 43971 @qwx453971 <p> This works perfectly . You can even switch viewing of each Luma node on and off while you work using the Highlight On/Off . Thank you . Here 's the Resolve 12 node tree for it . 
@@44332654 @4332654/ <h> Any other poor souls using PFClean ? <p> We bought this last year because it did one type of fix incredibly well , but it 's been an unmitigated disaster otherwise . I feel like it 's the 1990s and I 'm using Mac OS 9 software : random crashes , corrupted project files , weeks of lost work , etc . The developers have basically told me ( when we crash the software ) " Do n't do that . " I 've also pointed out to them that it 's impossible to complete a single reel of restoration on a feature film without maxing out the system 's memory ( we had to upgrade from 32GB to 64 to work around this ) , and was told it 's a low priority . <p> Dealing with their support folks has been an exercise in futility . It takes 24-36 hours to get a response , if you get one at all . there does n't seem to be any sense of urgency , but there 's plenty of time to fiddle with the user interface in @ @ @ @ @ @ @ @ @ @ is such a house of cards . <p> We can now reliably crash the system , hard , in 4-5 different ways . Do they care ? It really does n't seem so . <p> In any case , if you want to commiserate , I 'm here and I feel your pain . <p> We switched from MTI actually , which I 've used for over a decade in various incarnations . We 'll probably wind up going back to that as a primary tool , because it 's a great tool for lots of things , and one of the biggest workflow advantages is that it 's doing work directly on the files you import , as you apply the fixes , rather than requiring a render , or generating terabytes of temporary cache files . It 's super efficient . With PFClean , a 1TB 4k scan ( one reel of a 35mm feature ) requires at least 3TB of disk space : 1TB for your source , 1TB+ for your cache files , and 1TB for your final render ) . <p> But MTI is not @ @ @ @ @ @ @ @ @ @ the specific issue we were dealing with ( fingerprints on film that were only in the blue channel , for four or five frames on either side of every single splice ) just became too time consuming . PFClean was able to address that problem in seconds . <p> I 've looked at Diamant before and found it to be kind of funky , but perhaps it has improved . We 're also considering getting Algosoft , which I used when it was first released ( or possibly in beta - it was a while ago ) , and the interface left a lot to be desired : no manual tools at all , and a UI only a Windows 95 programmer would love . But it was a pretty cool automated tool , and is on our list as a potential standalone dustbusting tool . <p> I realize the answer here is to have multiple tools and to play to their strengths , but with restoration work , I do n't want to have to manage multiple versions of projects ( automated pass done in one tool , and @ @ @ @ @ @ @ @ @ @ backing out of changes or making changes becomes much more difficult that way . With PFClean , the fixes are done in an effects stack , so you can easily back out of one item in the stack at any time , or make adjustments or whatever . However , it 's so poorly implemented that if you just look at the software the wrong way , it 'll crash ... <p> We have a PFClean at work as well , and we have one operator who 's taken to it and really likes it . I have not heard him complain of crashes , but I know he has had to have it rebooted a few times . It seems , like grading , each restoration system has a few things it 's excellent at , and a few others it totally misses . Apparently none of them cover it all . <p> Since you 're already very familiar MTI , have you ever given Phoenix a try ? <p> Since you 're already very familiar MTI , have you ever given Phoenix a try ? 43971 @qwx453971 <p> @ @ @ @ @ @ @ @ @ @ sat through some demos at NAB and I 'm impressed , but it was always priced way out of our range . I 've been too busy to call back the sales guy to get current pricing though , and they do n't seem to want to email me a quote . I 'll have to get on that soon . <p> Like I said , we have found that PFClean has some really strong points . I do feel like they 're spending a lot of time on user interface stuff though , and not addressing underlying problems . Someone on the PFClean google group has suggested that we try it on a mac . I 'd be more open to that if I was sure it 'd be more stable , but tech support has told me I wo n't really see a difference . Is it better on a mac ? I do n't know . I 'd prefer to keep it on Windows or even Linux , because we can keep the machine in the rack ( so it does n't take up floorspace ) , @ @ @ @ @ @ @ @ @ @ I 've got no interest in the new macpro 's , because it 's a terrible design that locks you into what you buy , or into buying overpriced Thunderbolt interfaces for otherwise cheap connections to basic stuff . <p> Some of the issues , like using up all the memory in the system , are there by design according to their tech support folks . They claim they have to " strike a balance " between memory usage and performance . That 's fine . If the system requires 128GB of RAM to work correctly , and I knew that up front , I 'd have built that . But they say on their site that you need 8GB of RAM and a 1GB GPU . We now have 64GB and a 6GB 980ti in there , and we 're still having tons of problems . <p> When we scan a film , we do it by reel , and that 's fine - we 'd bring each reel into PFClean and restore them separately . but sometimes we get complete films as files , and with MTI we @ @ @ @ @ @ @ @ @ @ it . With PFClean , they 're telling us we need to break up our 20 minute reels into smaller pieces , to avoid memory issues . I mean , that right there is just bad design . <p> We used a Digital Vision 2K DVNR which is a realtime signal based cleaner thingy . It did a good job with automatically cleaning dust and scratches . It obviously ca n't do any bigger fixes like cleaning splices or splotches or broken frames so it was a semiautomatic workflow . The Phoenix uses the same tools and if I was really for realzies making a bunch of restoration work I 'd probably get that . If I had the money for it , that is . <p> Our workflow with MTI was always to do everything manually , no automatic cleanup at all . That made sense with MTI , because its auto-pass tools are n't very good . But it 's fantastic with manual cleanup , making some really seamless fixes with almost no effort . <p> PFClean has some really great auto-dirt tools , and we 're making @ @ @ @ @ @ @ @ @ @ is a real mess . But those tools seem to be responsible for the excessive memory use so we 're stuck . I like this hybrid workflow , where we run an auto-pass first and then hit the whole film manually with manual fixes . It 's a lot quicker than what we 've always done , and with the quality of fixes we get from PFClean , it 's a good solution -- when it works , which is n't often . <p> So if Phoenix can do something similar , I 'm open to that . I 'd much rather have everything in one system than having to bounce stuff around , because I like being able to revisit a project in 5-6 years and know exactly what we did . Invariably , we need to do that . If it 's been through half a dozen pieces of software , it 's much harder to track what we did to what . <p> When we scan a film , we do it by reel , and that 's fine - we 'd bring each reel into PFClean @ @ @ @ @ @ @ @ @ @ films as files , and with MTI we could load that whole thing in and just run with it . With PFClean , they 're telling us we need to break up our 20 minute reels into smaller pieces , to avoid memory issues . I mean , that right there is just bad design . 43971 @qwx453971 <p> I bet some of the memory issues would go away if you just broke it down to 20-minute reels , then put them all back together at the end . You 're never going to be able to process more than 20 minutes of material in a day anyway , assuming there 's lots of pos &amp; neg dirt plus splices plus warps plus vertical bumps plus rips . To me , this is the best answer . <p> So we are working in 20 minute chunks ( most of the time ) , and even that 's too much for PFClean in some cases . We sometimes work with complete features and MTI handled that just fine , but with anything we scan ourselves , it 's always done by @ @ @ @ @ @ @ @ @ @ is to break the 20 minute reels into even smaller chunks . The problem with this is that a single film goes from having 5 separate projects to having 10-20-30 separate projects , to deal with all the little pieces . It becomes a logistical nightmare , and it clogs up the UI . It 's workable if we 're only doing one film at a time , but we usually have 3-5 restoration jobs going on at once , all with different deadlines and schedules . Some are long-term , lower priority jobs that we work on as we have free time , others are more urgent . <p> What the Pixel Farm people are telling us is to break the 20 minute reels into even smaller chunks . The problem with this is that a single film goes from having 5 separate projects to having 10-20-30 separate projects , to deal with all the little pieces . 43971 @qwx453971 <p> Oh , well , then that 's totally stupid . Have you called these guys in England and yelled at them ? This is preposterously wrong . I @ @ @ @ @ @ @ @ @ @ ( most of whom are now laid off , sadly ) and they typically worked in 20-minute chunks and did OK . I know a company using Pixel Farm on the East coast ; I 'll contact them and see what they 're using and if it works . <p> The general attitude from The Pixel Farm seems to be that they have our money and they 're not really interested in making this work . Look , it may very well be a configuration issue on our end , but all evidence ( and that includes anecdotal evidence from other users ) is that the software is built on a very shaky foundation and this kind of problem is the norm . <p> I think it 's telling that the bug report form ( which is built into the application ) includes a section where you can detail a workaround , if there is one . When you look at some of the open bugs on their support site , many of which have been there for a long time , they do point out workarounds . But to @ @ @ @ @ @ @ @ @ @ the bug exists , but just telling you how to avoid it , instead of fixing the underlying issue . Several of these bugs are crashes , and are listed as lower priority than I would assign a crash . Also , there are only maybe 50 bugs shown ( which I find hard to believe - I 've worked on much less complex software projects that had hundreds of open bugs at any given time ) , and most have n't been addressed in the 6 months we 've owned the software . This , despite weekly builds being available for end users to download . <p> I worked in the software world for a long time , so I know how to track down and report a bug . Anything that causes a crash should be an automatic Medium or High priority bug at minimum ( I 'd say High to Critical ) , but they seem to think if there 's a way to avoid it , it can be put off . I think there 's a systemic problem with the way The Pixel Farm does @ @ @ @ @ @ @ @ @ @ support . Also , I 'm fairly certain they 're a small company so odds are the programmers are the tech support and QA people , and that 's not usually a good thing . <p> Seems one solution might be to revert back to the 2013 version ( assuming our license allows that ) . That version came out before they started messing with the UI and people seem happier with it . Of course , none of the projects we 've done in the past 6 months would be openable if that 's the case , because all that work was all done in PFClean 2015 . <p> Reading about all these trials &amp; tribulations I really think you should try Phoenix . Their support staff is solid and have a helpful attitude , a far cry from what you are describing . There are some complex restoration problems the software ca n't solve because they are more in the area of advanced compositing but for 98% of your tasks it does them very well and conveniently . Auto-dirt , auto-scratch , auto-flicker , auto-steady are reliable and @ @ @ @ @ @ @ @ @ @ tool allows you to work quickly . Good UI and very stable . It does need to render all auto-effects for real-time playback and depending on how many tools you stack you can have 16 cores on a Xeon workstation at 100% for the night . Then in the morning you review , make corrections and do the manual work on top of that . A structured workflow pays off . <p> So at this point , I 've spent the holiday weekend working on getting a project exported . I was able to figure out a series of workarounds on my own with zero help from tech support until today . And the help I got boiled down to this : <p> " I 'm afraid at this point in time we cant offer any advise on getting to the root cause of the problem , all we can offer is trips/tricks ... Until we get to and address the real root cause of your issues you 'll probably continue to run into problems I 'm afraid and all we will be able to offer in the short-term is @ @ @ @ @ @ @ @ @ @ " figure it out yourself and then we 'll look at it . " No suggestions are made as to a starting point , just a blow-off , pretty much . At least , that 's how I read this . <p> How does one deal with this ? It 's not like we 're talking a $250 piece of software here . For a small or even mid-sized shop , $10k is a substantial outlay ( not to mention we 've lost at least 5x that over the past few months in re-doing work due to file corruption , etc ) . <p> Obviously we 're looking for alternative software here , but others seem to be able to use PFClean successfully ( though many are on mac and Linux ) . We 're toying with installing linux on that machine and seeing if it 's more stable there , but that 's going to require a fair bit of time to set up and we ca n't do it mid-stream when we 've got 3 different restoration projects going at once . So who knows when I 'll @ @ @ @ @ @ @ @ @ @ being open about this . I think we all need to hold vendors feet to the fire if they make a product that does n't preform as it should . The margins are so tight today that " bad " purchases are n't easily absorbed . <p> I 'm now being told that they will revisit this *after* they get their 2016 release out the door , but that in order to do it , they need to reproduce the problems . That means I guess we 'll have to send them the entire 2TB project/cache file plus 1TB of source media for one of the reels , in the hopes that they can see the instability I 'm seeing . At least that 's something , I guess . <p> It 's just not a priority because they have come up with this kind of ridiculous annual version cycle , rather than just releasing a version when it 's fully ready . By most accounts , from users who have been on PFClean for a while , the 2014 and 2015 versions are kind of unfinished software , and @ @ @ @ @ @ @ @ @ @ it 's been through one drastic UI change in the media management . Documentation is out of date with the current software , showing UI elements that no longer exist , etc . <p> I spent the weekend in the office and managed to get this current project exported out , and now it 's back in Resolve to finish it off . But it should n't have been nearly this complicated . Part of the promise of PFClean is that it allows you to do a lot of that stuff in one application ( exporting to various resolutions/frame rates , aspect ratios , etc ) . But I just do n't trust it enough to mess with that at this point . <p> Hi , folks . I accidentally wondered in through Google search . I myself am not in the film restoration industry , but the subject matter is a kin interest of mine . I found this thread so fascinating that I read all of it . <p> Any progress on that front ? I have just posted in a thread about the config for PF Clean @ @ @ @ @ @ @ @ @ @ good but it 's a lot of money if it is not able to deliver 43971 @qwx453971 <p> Hi Julian - sorry , I missed this back in November somehow . <p> We are still using PFClean 2015 with extensive workarounds. for 4k projects , we break 20 minute reels into 3-4 chunks , depending on what we need to do . If the job involves lots of automatic dustbusting , the memory spikes rapidly , and because of the way PFClean is designed , the time it takes to switch , close or open a project can run up to 20-30 minutes . Seriously . So with smaller chunks , we can work a bit faster . We also upgraded the memory in that machine to 96GB , and that has helped quite a bit with the crashing . We were really hitting the wall with even 64GB. and breaking up a 20 minute reel into 4 pieces means we have to have 20-24 separate projects for a single feature film , one for each chunk of each reel . <p> We 're currently at a bit of a @ @ @ @ @ @ @ @ @ @ can set up other machines on the network to act as render nodes at no additional charge . With our new 40Gb network , this is a really nice way to quickly render out a long project . When it works it does a nice job . But it 's slow . <p> In the time since I started this thread , they 've uploaded a comprehensive article on building a PC for the system , and surprise surprise , the specs are substantially beefier than they had originally told us when we spent $3500 building a PC for it a couple years ago . We 're building a machine similar to what they spec , for Resolve . I may throw PFClean on that PC to see if it 's better , before we put it into service as a Resolve PC . The spec for that is similar to what Phoenix requires as well , so if we end up building a second machine like that , then at least we 'd be ready for Phoenix if we decide to do that . <p> There are a couple @ @ @ @ @ @ @ @ @ @ avoid using the auto dirt tools , which seem to be huge part of the problem as far as crashes . If that 's reliable , it may be a viable workaround , though we 're going to have to sink another $1600 into this software , which is a tough pill to swallow given all the issues we 've had . <p> The demo I saw of Phoenix at NAB last year was really impressive and it 's at the top of our list if we switch . But there 's a substantial cost to doing that , in the software , hardware and the time to learn a completely new system and then train an operator on it . So as much as I ca n't stand PFClean and the way The Pixel Farm operates , that is the path of least resistance , assuming the hardware upgrade fixes the problems . 
@@44332655 @4332655/ <p> It 's been balanced so they probably paid some one to do that but .... My money is that the client and director fell in love with the look of the raw footage in the edit , and ca n't handle a change . I 've had to deal with this sort of thing before I usually explain they have sunglasses on ..... they think that 's good because that how they have been seeing it , if they remove the sunglasses yeah it will feel weird for a bit but then the world becomes real once again . <p> It 's been balanced so they probably paid some one to do that but .... My money is that the client and director fell in love with the look of the raw footage in the edit , and ca n't handle a change . 43971 @qwx453971 <p> There was some talk about this on the Cinematographer 's Mailing List awhile back , and one theory was that the director &amp; producer got non-LUTted dailies by mistake , it went all the way through the edit , and they could @ @ @ @ @ @ @ @ @ @ theory is that they wanted a look " different from everything else on television " ( i.e. , the stuff with Rec709 contrast ) . 
@@44332656 @4332656/ <h> How do you like to be credited ? <p> So I was looking at IMDB to see what most Colorists get credited as ... I thought it would be " colorist " but I 'm seeing a lot of people credited as Digital Intermediate Colorist ... so what 's the standard these days ? <p> In my book , colourist is the broad term for what we do , and it implies that you were responsible for colourgrading the master version of the project . <p> You can narrow it down to dailies colourist for instance if you were doing dailies but not the master grade . <p> The digital intermediate process implies going back out to film from digitally graded data . I think for a while people felt it important to clearly communicate on IMDB that there was digital data grading involved ( as opposed to tape-to-tape or lab printing ) . <p> These days that 's kind of a given , so I 'd stick with colourist . You do n't see the cinematographer adding ' digital ' to his credits . <p> The major @ @ @ @ @ @ @ @ @ @ Colorist , Colorist Assist , and Dailies Colorist , but I 've seen Digital Intermediate Colorist , Digital Colorist , Digital Color Timer , Digital Film Colorist , and a number of others . <p> What 's funny is that the credits may not be consistent even within the same film . Check out the color-related credits for Godzilla : <p> 1 . supervising digital colorist - where there is a team of colorists and the supervising one takes the lead with the DOP. 2. digital colorist - where you are the sole lead colorist , or a colorist as part of a team . 3. digital dailies colorist - a colorist working as part of a team with the sole responsibility of primary balancing using a fixed toolset defined by the supervising digital colorist . <p> The major titles seem to have settled out into Supervising Colorist , Colorist , Colorist Assist , and Dailies Colorist , but I 've seen Digital Intermediate Colorist , Digital Colorist , Digital Color Timer , Digital Film Colorist , and a number of others . <p> What 's funny is that the @ @ @ @ @ @ @ @ @ @ . Check out the color-related credits for Godzilla : <p> Yes , there is still a lab color timer to tweak the IP and IN when used for generating prints . It 's a smaller part of the business , but you need a guy there to at least ensure consistency for the parts of the world that still need 35mm prints . <p> Jason Myers said : <p> What 's funny is that the credits may not be consistent even within the same film . Check out the color-related credits for Godzilla ... 43971 @qwx453971 <p> On a massive project like that done over a period of 4-5 months , there 's tons of minions in the back room working on it . I know of projects ( not necessarily this one ) where the guy ultimately given the " supervisory " credit barely worked on the film more than a week . Steve Scott is an exception and he 's a pretty hands-on guy . I think most of the inconsistencies you 're seeing are how the credits are reported to IMDB -- not what was shown in @ @ @ @ @ @ @ @ @ @ sequences is often a separate process , and the final color gets tweaked the week the project gets shipped . ( It 's a real grind when there 's four or five VFX facilities all working on the same project , some out of the country , and the files all get slugged into the final conform at the last minute . ) <p> I would rather see a list of colorists credited instead of just a lump credit that said " Post Production by Blah-Blah Facility , " which is the reality for lots and lots of TV series these days . 
@@44332657 @4332657/ <h> Best BD-R media <p> Ive been making blurays using Taiyo yuden water shield discs since these were widely regarded as the best media for DVD-R authoring . However I recently had a disc that would n't play on a clients old outdated bluray player . Some research shows that there are two types of BD-R discs , HTL and LTH and many people regard the HTL discs are more reliable and compatible with older players . <p> So the question is , have other people for this to be true ? What HTL media is the best ? Verbatim seems to make both HTL and LTH media and people seem to have good experiences with the HTL . <p> Thoughts ? <p> p.s . I am quite confident that this is unrelated to the authoring process but FWIW , I typically encode in media encoder as 2 pass VBR MPEG2 use the Minnitonka Dolby plugin to create 5.1 audio , and author the discs with Encore , burning them in a Microboards duplicator . <p> p.s . I am quite confident that this is unrelated to the authoring @ @ @ @ @ @ @ @ @ @ @qwx453971 <p> I would recheck your assumptions , I 've seen many more compatibility issues related to encore than issues related to media on bd . Assuming it 's a disc with an authored menu , I 'd bet that encore was related to the issue . <p> Juan perhaps but in this case these are menu-less blu-rays , as plain vanilla as can be . Will test the same player with different media and with a different authoring solution , but I 'm betting that its LTH BD-R media for now . <p> You could try to follow this very serious impartial study made by a lab of the French ministry of culture . I do , and use Sony ; Verbatim LTH is presented as the worst performing media of the study . 43971 @qwx453971 <p> Thanks Michael , I did actually see that study , but I was n't able to find any specific part #s in there , which made it less useful , since it looks to me like Sony and Panasonic which are the top rated discs have a large variety of BD-R discs @ @ @ @ @ @ @ @ @ @ , and do you know if they are HTL ? <p> Thanks Michael . <p> -Nat p.s . The french study is also highly concerned with longevity which is not a concern for me , I am only concerned with immediate compatibility . <p> The main problem with all this is that all the vendors seem to like to farm out manufacture of their discs ... " Sony " discs may be made by mitsubishi electric in japan or Ritek in taiwan , or elsewhere in india ... Some " verbatim " discs are made by panasonic ( ranked the highest in the french study ) ... Oyieee what a nightmare . <p> After some research , the product I 'd for the " good " Sony HTL discs is Sony-NN3-002 . Will try and find a big batch of these . <p> I appreciate everyone 's feedback , but in this particular case , I 'm pretty certain that its the BD-R media and the fact that this specific media was LTH that 's at fault . The disc in question plays on lots of players without issue , its just @ @ @ @ @ @ @ @ @ @ shows a cross sample of players , and if they have issues with LTH discs . As you can see , quite a few of them seem to ( about 1/3 of them ) . Its worth noting that almost all of these issues can be resolved with a firmware update , but you ca n't tell a client to do that . I 'd rather use HTL media . Before this I was unaware of the difference between LTH and HTL media . <p> So yeah , if anyone has recommendations on specific HTL media and where one can buy it let me know ! Finding the sony media specified in that French study ( SONY-NN3-002 ) seems to be easier said than done . <p> It might be easier to buy the client a $50 Blu-ray player from Walmart and send them the disc after you checked it on that exact player . I keep three different Blu-ray players hooked up in my home system all the time on the assumption that if a disc wo n't play on one , it 'll play on one of the @ @ @ @ @ @ @ @ @ @ ) <p> Part of the problem seems to be that some vendors such as verbatim and sony offer both HTL and LTH discs , and even occasionally have the same part #s manufactured at different factories with differing levels of quality . <p> I 'm going with the Falcon " smart guard " ( glossy finish similar to water shield ) discs . They are HTL discs , and seem to be the highly regarded by those who obsess over this stuff at http : //club.myce.com/f33/ <p> Also , FWIW in a pinch the verbatim part #97339 is a HTL disc and is cheap and readily available ( in stock at B&amp;H ) and as an HTL disc has better reliability than an LTH disc like the taiyo yuden JVC discs . Hope this info is useful to some others . 
@@44332658 @4332658/ <h> Consolidated/Copy not working ? <p> I 've just finished a job which I 'd been doing , linking directly to the client 's drives so I wanted to Consolidate everything to my drive before returning his drives ( probably should have sorted this before I started but ? ) . I did a Consolidate of the RED files in Resolve 11 and I 've since installed Resolve 12b . I tried to relink to the consolidated files but they all seem to be slipped so that , for example , the in point is wrong and the end has no media . I 've redone the while Media Manage&gt;copy ( Copying clips used in the Timeline ) and repeated the whole thing - the Copy never finished , it always fails at the last stage , before creating a new project to link to - and I got pretty much the same result when I manually relinked to the new files . Reconform to Selected Bin(s) mostly crashes it too.Also , if I manually Force Conform on an individual clip , it loses the grade - all nodes @ @ @ @ @ @ @ @ @ @ a shorter timeline but I ca n't get this to work now . Any thoughts ? <p> For a situation where the files have been trimmed and moved to another drive and the relinking problem persists , you can use the " change source " option with the previously imported footage in the Media tab and point the clip drive paths to the new destination . That usually works without messing up your other project settings . <p> I 've had to do that when I 've received meta from an editorial system where the source files were arri raw image sequences and the range was part of the reel name used offline ( oops ) . If I trimmed and tried to relink to the trimmed media , I 'd never get the AAF to work as I could never re-create the original range in the reel names of the trimmed files . <p> Change Source makes Resolve just assume that the range of clips is the same at the new path and you can get on with the more creative aspects of the finishing . <p> I have @ @ @ @ @ @ @ @ @ @ the Media Management feature and it " appears " to work . Important safety tip : be sure to check the Relink to New Files option . I have n't yet figured out how to do this manually after the copying process . <p> Yeah , it always crashed before completing the Re-link to New Project . But when I look at the files it 's successfully made they 're not the correct range for the edit . It 's hard to tell exactly but the action seems to be slipped and the end portion of each clip is offline . Weird . <p> I threw a couple projects at the new media management , and I definitely do n't always come out the other end perfectly ( or at all sometimes ) ... too early for me to say what the exact issues were , as some were really weird projects , and I did n't do extensive testing . <p> Yes , it needs a bit of work I think . When it throws up an error you can only see the first part of the path of @ @ @ @ @ @ @ @ @ @ the new project size started getting bigger when I was trying smaller timelines . When it worked it worked brilliantly and there was nothing else needed to relink to the new files , they were all there and linked . <p> Actually , the penny just dropped ... The estimate is for the size of the current project after you 've trimmed the media which is why when I experimented with smaller timelines , the estimated size went up because less of the project was being trimmed . That 's not how I expected it to work , I 'd thought it would create a new project ( FCP 7 Media Manager style ) but it does n't . I 'm sure it 's all there in the manual ! 
@@44332659 @4332659/ <h> Is it camera or grading ..... <p> I frequently have on the music channels at home and notice most videos seem to have a kind of hazy finish ..... Not like smoke machines ..... Just hazy .... Is this produced by some of the top end cameras or is it done in post ? <p> The low con hazy look is the current trend . Its an " artistic " choice that does not necessarily have to do with the type of camera used . I get asked to do it a lot , I always try to talk my clients out of it because although it might look " fresh " today who knows how well it will age . I think part of the reason it has become so popular is because cameras that shoot in a log or raw format have become more accessible , and the directors are falling in love with the flat look during the edit process . <p> Yeah , " inside llewyn davis " had a fantastic soft glow look that I really dug , but I was waiting in @ @ @ @ @ @ @ @ @ @ reality trash was on that was run through the ' super soft look " filter and it did n't improve the quality of the content any 4 Sure . <p> I frequently have on the music channels at home and notice most videos seem to have a kind of hazy finish ..... Not like smoke machines ..... Just hazy .... Is this produced by some of the top end cameras or is it done in post ? 43971 @qwx453971 <p> I think it can be both . ProMist filters have a certain look to them , and I 'm a big fan of Tiffen GlimmerGlass , which has a look all its own . And there are definitely plug-ins that take it even further , as well as defocusing highlights . And it 's rare I 've been on a music video set that did n't have a fog machine somewhere . <p> To this day , I have to laugh every time I see a serious TV drama and there 's clearly smoke moving around in the background , because they could n't wait for it to settle . @ @ @ @ @ @ @ @ @ @ ! " This stuff is so heavy-handed nowadays , such a cliche . <p> I frequently have on the music channels at home and notice most videos seem to have a kind of hazy finish ..... Not like smoke machines ..... Just hazy .... Is this produced by some of the top end cameras or is it done in post ? 43971 @qwx453971 <p> Can be both , but sometimes it has to be " prepared " on shooting to have space for grading . You can do almost all you want in post-production , but not with any kind of raw material as it can look very artificial . <p> Funny because I have seen two exemple of what you talk about recently , in very different material , one in a classical photo shoot and the other with tuned post-production work . <p> That 2013 music video try to reproduce visual look of the 80 's dance club and videos , due to the " synth-funk " style of the music . Neon , color , light , framework : no matter you like or not the music @ @ @ @ @ @ @ @ @ @ 
@@44332660 @4332660/ <h> Green artifacts appearing in transcodes from Pomfort Silverstack Lab . <p> I just finished a feature as a DIT on which I used Silverstack Lab ( version 6.0.2 ) for the first time . I used to do a ShotPut/Resolve combo . <p> I worked the second half of the feature and another DIT worked the first half . I just got a message from post that some of both of our editorial transcodes have green artifacts in them ( the artifacts flash for a frame or two ) . Sometimes it 's green rectangles ( I 've attached an image ) and sometime 's it 's a green flash that covers the entire screen . <p> The other DIT and myself both used the same Silverstack license , project file , and transcode preset - but we used different computers , both iMacs . <p> I have a Late 2015 5k Retina 25-inch iMac with a 4 GHZ Intel Core i7 Processor , 16 GBs of Ram , and an AMD Radeon R9 M390 2048mb processor . I 'm on 10.11.6 El Capitan . I do n't @ @ @ @ @ @ @ @ @ @ <p> We were transcoding 3.2K ProRes camera originals from an Arri Amira and an Alexa Mini to ProRes 422LT 1920x1080 editorial proxies . Source is set to Debayer and Default ( GPU ) . <p> I 'm supposed to start a new 6-week feature on Tuesday and now I 'm not feeling comfortable to use Silverstack Lab again . <p> Anyone know what the problem could be on those transcodes ? Silverstack lab is still only a few months old . Am I safer reverting back to Silverstack 5 from my offloading/dailies transcoding on this next project ? <p> I had similar issue recently - green artifacts - but the reason had nothing to do with export or tranascoding . It turned out the problem was sending files with MyAirbridge . Airbridge introduced those artifacts while uploading . The solution was to zip the file before sending . <p> Maybe this wo n't help - but wanted to to throw it out there just i case . <p> Hey Matt , make sure to email Pomfort 's support . I found a few bugs in LiveGrade and emailed them 3 weeks @ @ @ @ @ @ @ @ @ @ days that fixed the issues for me and got me through the end of the feature I was working on without further incident . With Silverstack Lab just coming out , I did NOT expect them to have the time to offer me a useable solution . Thankfully , I was wrong . <p> One further piece of information that only might help : what kind of drive system were you writing to ? About 6 years ago , I was working in post , and we had just bought a new file-server system that was shared by around 25 workstations . Long story short , it turned out that the company that sold it to us KNEW there was a bug that if you rendered ProRes directly to the server , you 'd occasionally get glitched frames ( not 100% like what your'e seeing , but not 100% different either . We were seeing squares or rectangles of single colors . Not necessarily green , and I do n't think ever an entire frame ) . The company in question was also told explicitly that our workflow involved rendering @ @ @ @ @ @ @ @ @ @ came to a head , that was the angriest I 've ever seen my boss . Point is , if you were writing to some sort of server , there 's a chance , albeit an extremely slim one , that something in that chain is causing your issue . <p> I have n't yet tested out Silverstack Lab , so that 's the best help I can offer . <p> Thanks for the info . I sent a note to Pomfort support and I got a response this morning that said they were looking into it . As far as drives , we were using 24 TB G Shuttle XL drives formatted to Raid 5 . We had three of those drives , but I would initially just send the transcodes to one drive and then copy them to the other two once finished . <p> Just because my next feature starts tomorrow , I 'm going to only use Silverstack Lab for offloading footage and I 'm going back to Resolve for the transcoding . To see these green artifacts you have to watch the transcodes in real @ @ @ @ @ @ @ @ @ @ and if you shuttle through the footage at even 2x speed you ca n't seem them . With two Epic Dragons shooting 6K and two sets of transcodes on this next show - one for post , one for dailies viewing - I wo n't have hours each day to watch the transcodes in real time to make sure there is n't still an issue . <p> Yeah , that 's the unfortunate thing about new software . As awesome as it could be in the future Pomfort has a great track record , there are always bugs on release . <p> Scratch is currently my tool of choice for dailies on set . Steeper learning curve than Resolve , but I find it streamlines a few things better than Resolve when it comes to dailies . Express Dailies is a bit too costly for the jobs I get that need dailies on-set . As mentioned , I have n't yet tried Silverstack Lab , nor have I touched Codex Production Suite . Kinda weird how many more options there are now for dailies than there were a few years @ @ @ @ @ @ @ @ @ @ issue came up . As an offloading tool and dailies transcoding tool it was fantastic . The color grading is limited - but it was sufficient for this last movie I was working on where I was n't doing any secondary corrections for the dailies . Honestly , I probably would n't have used a program this early in its release normally ( I 'm still hanging onto DaVinci 12.5 ) , but in this instance I was taking over for the last week of a three week shoot and the Silverstack Lab workflow was already in place so I stuck with it . <p> Hey guys , just a brief update on this from Pomfort : We 're still investigating this . Unfortunately even with iMacs of same specs we could n't reproduce the issue in our lab after hours of transcoding " and we also had no other reports so far , where could learn more about the specifics of that problem . So currently it 's hard for us to track down the reason for the green squares , but we still in touch with Matt for more analysis on his machine(s). 
@@44332662 @4332662/ <h> GPUs vs OS - finding the sweet spot <p> I know there are already a ton of posts about OS and GPU configurations , Mac migration , etc etc . But I have a specific goal in mind which warrants it 's own thread . <p> Goals In my current build of 2x Titan-Blacks , I quickly run out of VRAM when doing tracking and noise reduction in UHD timelines . I also ca n't sustain real-time playback for long . So if I 'm going to invest in new GPUs I 'm going to want to best manage that investment . Since MacOS ca n't see GTX10XX cards I do n't particularly want to pay a premium for old hardware . That means an OS migration . So between Windows or Linux , what 's the best combination of performance vs cost with multiple GPUs ? <p> Using Davinci Resolve primarily . I also use Avid and Premiere for Online/Packaging . <p> Q 1 - How many GPUs can MacOS , Windows , and CentOS effectively see and utilize ? <p> Mac - ? CentOS - 8 @ @ @ @ @ @ @ @ @ @ At what point would there be diminishing returns with multiple GPUs ? <p> Would love to hear from multi-GPU enthusiasts what their experience has been . I have two Cubix X-panders in my possession so I could put 8 x GTX1080s in my system , but would there even be a point to that ? <p> Q 3 - Managing PCI lanes <p> My understanding of the Cubix is that it takes an x16 lane and creates 4x x4 lanes . This may have been good in the Quadro 4000 days but today is a PCI bottleneck like that an issue when working with higher end cards ? <p> I 've been reading on another forum that SuperMicro users have 4x x16 lanes and do all this internally . I imagine that this would be more effective than an external solution ? <p> Q 4 - Scale-ability If I went CentOS and invest in a certain build , how hard is it to regularly update the components ? For example every 18 months I get new GTX cards , will I run into a laundry list of drivers issues ? @ @ @ @ @ @ @ @ @ @ I 'm going hat-in-hand to my employer asking for a system , I want to make sure the idea is sound and not have a glaring oversight in my build proposal . <p> If you 're planning to use Avid and Adobe , then CentOS is out of the question . Yes , you can do a dual boot , but then you might as well stay in Windows . It sounds like you 're not really familiar with Linux administration and for someone like that Windows is a much better solution . <p> As already mentioned , there 's been lots of discussion on this ( and other ) forums about what we 're going to need in terms of workstations , to cope with future demands . I 'll add my personal recommendations , which are ... <p> Forget Mac OS , or Linux - with the applications you want to run , a high-end PC workstation is your best option . <p> External PCIE extender boxes are fine for boosting laptop performance , but a good motherboard with the necessary PCI lanes/slots will always out-perform them . @ @ @ @ @ @ @ @ @ @ on copper cables with cheap connectors is challenging , with cable capacitance and cross-talk potentially reducing the maximum throughput . A well designed motherboard will be keep these problems to a minimum to better cope with present/future demands from high performance cards . <p> The old adage of ' you 're only as fast as your weakest link ' still hold true . There 's no point in filling your workstation with multiple GPUs if the rest of the system can not keep pace . Your CPU and memory are still crucial to your overall throughput and performance , so I would look to spending the money you would have spent on a third GPU on fast memory and CPUs with as many cores as you can afford , and consider overclocking with liquid cooling . Overclocking was once regarded as something only gaming geeks resorted to , but it 's become much more mainstream in recent years , and I 've talked to quite a few fellow professional who have gone for modest overclocking without any stability problems . <p> It goes without saying that SSD drives are the @ @ @ @ @ @ @ @ @ @ personally stay away from RAID 5 and just use RAID 0 for the fastest performance . If you 're worried about data loss , keep a copy of your data on separate NAS storage . It 's just as quick restoring your media from NAS storage as it is rebuilding a RAID 5 array , and it 's less prone to failure . <p> I often say ' we did n't get into the industry to make Quicktimes all day " , yet some days that is all I do I am not a sysadmin , I do n't have Linux experience , I do n't even know any Linux terminal commands or basic folder tree layout etc . <p> Seems like a good course of action would be : -Windows 10 Pro 64-bit -2 or 3 GTX1080s ( or 1080Ti ... ) -M.2 bootdrive -Thunderbolt PCI card -we already have a SAN so no real need for a local cache raid , aside from maybe another M.2 drive or a SSD -We have a BlackMagic HyperDeck 4k so I can still make ProRes masters on Windows -I plan on @ @ @ @ @ @ @ @ @ @ still output ProRes that way too -I 've always liked HP turnkeys , but Supermicro has more BTO options and is recommended as a Resolve turnkey in the user manual . <p> I 'm OK to skip the Quadro route in favor of a GTX. 10-bit GUI would be a nice value add , but I have a SDI monitor to evaluate on . I do n't need a 3 year warranty as I 'll probably be upgrading the GTX by then . <p> Now I just need to decide on a suitable processor , but there are plenty of threads on that . <p> I would love to just sit in a room with 4 units of every current GPU and benchmark for a week straight and report it all back here . If anyone owns a VAR and needs someone to test give me a shout ! Who knows maybe 8 x GTX1060s is the magic combination of price vs performance . <p> I would love to just sit in a room with 4 units of every current GPU and benchmark for a week straight and report it @ @ @ @ @ @ @ @ @ @ needs someone to test give me a shout ! Who knows maybe 8 x GTX1060s is the magic combination of price vs performance . 43971 @qwx453971 <p> Oh , I would love to have an 8 x GPU machine . But the expansion chassis are not cheap ... <p> I have worked on several Supermicro SuperServer 7048GR-TR systems ( X10DRG-Q motherboard ) and can highly recommend them . You can add up to 4 GPUs and I would suggest investing your money mainly for the best graphic card you can afford . You also need two Xeon CPUs to get sufficient PCIe throughput , eg two Xeon E5-2690 v4 ( 14 Cores @ 2.60GHz ) or Xeon E5-2697 V4 ( 18 Cores @ 2.3 GHz ) , and enough RAM . <p> To help you choose your GPU options I add below my benchmark results from the Standard Candle test . Please note , performance does not scale linear if you add additional GPUs . So I would start with the best graphic card you can afford . <p> Ooooo ... I 'm very curious about the cost/performance difference between @ @ @ @ @ @ @ @ @ @ " to tossing the Trashcan Mac out the window . 43971 @qwx453971 <p> I 'm in the exact same boat with two trashcan's ( which will probably cut over to sound only work , but even that is moving over to windows more then I realized ... i.e. all vr sound needs vive and the sound guy has to go back and forth between a mac and windows to actually hear his mix ) . Another thing is I find a ton of the software now a days on frames that are 4k and 8k , single threads somewhere , so I think I 'm going to get one of those 4.8 ghz 4 core machines with one of the new 1080i 's , and that will be like 3x faster as my 12 core d700 trashcan . I was looking at at getting another dual cpu megamachine ( i.e. two cpu 's and 2 gpu 's ) ... but the bang per buck drops massively , but I 'm pretty sure 5ghz 4 core 1080ti will beat a 3 ghz 20 core with two 1080ti for a huge percentage @ @ @ @ @ @ @ @ @ @ getting the boxx laptop with the 1080 , or the boxx 2 with a 1080ti . Another kind of wierd thing , is I think I 'm cutting all the laptops over to 802.11ac with the AC being in some cases actually faster the 1 g ethernet . My main current problem with windows laptop users is getting to the 10g san , this is n't a problem with the mac laptops but the thunderbolt to 10g convert is huge and expensive . <p> Why not just go with this Supermicro , so you have 8 pcie 3.0 slots for plenty of graphic cards ? 43971 @qwx453971 <p> With my space limitations and the noise issues , that ai n't gon na work . I could live with a z840 and 2 1080ti 's . Or maybe a custom ADK machine . There 's lots of options -- it 's a question of time , budget , and whether I can afford to shut down for 7-10 days to make the changeover . When you 're in the middle of projects , there 's a lot to juggle . <p> @ @ @ @ @ @ @ @ @ @ ( which will probably cut over to sound only work , but even that is moving over to windows more then I realized ... i.e. all vr sound needs vive and the sound guy has to go back and forth between a mac and windows to actually hear his mix ) . Another thing is I find a ton of the software now a days on frames that are 4k and 8k , single threads somewhere , so I think I 'm going to get one of those 4.8 ghz 4 core machines with one of the new 1080i 's , and that will be like 3x faster as my 12 core d700 trashcan . I was looking at at getting another dual cpu megamachine ( i.e. two cpu 's and 2 gpu 's ) ... but the bang per buck drops massively , but I 'm pretty sure 5ghz 4 core 1080ti will beat a 3 ghz 20 core with two 1080ti for a huge percentage of my jobs . I am real close on either getting the boxx laptop with the 1080 , or the boxx 2 with @ @ @ @ @ @ @ @ @ @ I think I 'm cutting all the laptops over to 802.11ac with the AC being in some cases actually faster the 1 g ethernet . My main current problem with windows laptop users is getting to the 10g san , this is n't a problem with the mac laptops but the thunderbolt to 10g convert is huge and expensive . 43971 @qwx453971 <p> I think an optimal machine for a lot of jobs today would be something like a dual 4-core 4.5 Ghz machine ( or single CPU 8-core ) but keeping very high per core speed , but also allowing at least 128 GB of RAM and giving more PCIe-lanes than the normal core-series has . <p> i just got a machine up and running at a client 's facility , being used on a project for the next year or so , went with a refurb 'd z820 / 2x 2690v1 / 128g / k4200 + 1080 ( need a Q/K for Avid ) and a 8 slot SAS array with WDRedPro 's ... it 's keeping up just fine on UHD ArriMini ProRez , and VFX shots @ @ @ @ @ @ @ @ @ @ , machine , cards , array , everything came to a bit under 3,500K usd ... would close to the same price with 2 x 1080Ti under the hood if no K/Q works for you <p> maybe worth revisiting the decision to invest in the resources to debayer the same shot over and over and over and over again for no gain and great expence .... <p> With my space limitations and the noise issues , that ai n't gon na work . I could live with a z840 and 2 1080ti 's . Or maybe a custom ADK machine . There 's lots of options -- it 's a question of time , budget , and whether I can afford to shut down for 7-10 days to make the changeover . When you 're in the middle of projects , there 's a lot to juggle . 43971 @qwx453971 <p> I switched to a custom-made PC ( Corsair Obsidian 900D chassis ) based on SuperMicro X10DAX dual-Xeon motherboard with dual GTX1080 . The CPUs and GPUs are water-cooled . Very quiet . A LOT more power than the Trashcan @ @ @ @ @ @ @ @ @ @ 
@@44332663 @4332663/ <h> Color halo with Red Scarlet-w footage <p> Hi I 'm not sure if this post goes here but since is a more camera specific problem i thought this is a good place . I had a shooting yesterday with the scarlet-w we did 4k,5k and slow motion in some of the footage we found this color haloing ( note the yellow one is slowmotion and the other is not , so not sure if its a pull down issue ) . Any idea how I can get rid of this in Resolve ? <p> You can trim R3D footage down to a few frames in both Resolve and RedCine-X , and could then upload it someplace that we could download it ( your own web server , DropBox , etc . ) . <p> For the very reason that you can do this , there is no " pull-down " issue . The camera is n't shooting interlaced . Every frame is a frame . <p> Is the lower image shot at a lower resolution ? What was your frame rate , what was your compression ratio , @ @ @ @ @ @ @ @ @ @ , based on the JPGs posted , to me it looks like a combination of spillage and chromatic aberration , not anything specific to the camera sensor . On the top image , on the man 's right , there 's a liiiiiiittle bit of red , and on the left there 's a liiiiiittle bit of blue . The wavelengths of light bend a little bit stronger or weaker creating this aberration . It 's more noticeable in the bottom image , which I bet was a lower resolution aquisition ( which means that the same percentage of aberration takes up a larger portion of the recorded frame , since Red cameras do not downscale , they record a smaller portion of the image sensor ) . <p> How far from the background was your subject ? Was the ground under them also the same color as the background ? In a perfect world , if you 're shooting greenscreen , you 'd want the background to be a mile away . Really . It feels weird on set , but you want as much space as possible between @ @ @ @ @ @ @ @ @ @ of limiting spill . If the ground is also that color , unless you 're seeing the ground , you 're only making your life in post harder , because the color is going to bounce up on the subject . <p> I could be wrong about my assessment , but given the limited details I know , these two things are my best guess . <p> First of all thanks for your interest both Nick and Cary . I will try upload tomorrow . <p> Due to budget reasons the background colors where really close ( I advice against this because I know that is better put them far ) . The ground was different , just the curve but the subjects where not on top of it . The first shot was shot in 5k and the second one is a 240fps slowmotion 2k hd the lenses used where Milvus 35 and 50mm . I have to say I did n't have no power on choosing since the filmmaker did the deal before hiring me I only had power to ask for a Kino in exchange of part @ @ @ @ @ @ @ @ @ @ ) Anyway I think the spill will be because they were too close from the background , I thought about putting them far because the problems with the shadows , I knew about the spill but I thought this be like a tint on the subject not that halo . <p> 5K versus 2K is a HUGE difference . Any sort of halation or chromatic aberration will be increased by a physical factor of 2.5x in both dimensions , which means it 's FAR more visual . <p> Reds let you trade of some things for others . In terms of high speed , you 're trading sensor area for more frames per second . What you 're seeing is just how your lenses and your camera work . You can somewhat limit this by using more light and lowering the ISO ( Red rates their cameras at 800 ISO , but everything I 've seen looks better at 640 , 400 , or 320 ) , but your lenses only resolve so well and so far . <p> 5K versus 2K is a HUGE difference . Any sort of @ @ @ @ @ @ @ @ @ @ factor of 2.5x in both dimensions , which means it 's FAR more visual . <p> Reds let you trade of some things for others . In terms of high speed , you 're trading sensor area for more frames per second . What you 're seeing is just how your lenses and your camera work . You can somewhat limit this by using more light and lowering the ISO ( Red rates their cameras at 800 ISO , but everything I 've seen looks better at 640 , 400 , or 320 ) , but your lenses only resolve so well and so far . 43971 @qwx453971 <p> I shooted at 1600ISO in slowmo to compensate my lack of light . The rental service sell me the Milvus like they were really good but I was dissapointed with the results ( anyway It was not my choose ) . Anyway is interesting all your comments I will learn for the next time . <p> High ISO exposures are a bad idea for situations like this . At more light , you 'll get a much less-noisy image and better @ @ @ @ @ @ @ @ @ @ people away from any kind of colored cyc just to avoid contamination from bounce and spill . This is a particular problem with colors that are a little close to skin tone ( like yellow and orange ) . I would say 10 ' or 15 ' is a minimum for this kind of thing -- even more if it 's green screen or blue screen . <p> High ISO exposures are a bad idea for situations like this . At more light , you 'll get a much less-noisy image and better accuracy overall . <p> It also helps to pull the people away from any kind of colored cyc just to avoid contamination from bounce and spill . This is a particular problem with colors that are a little close to skin tone ( like yellow and orange ) . I would say 10 ' or 15 ' is a minimum for this kind of thing -- even more if it 's green screen or blue screen . 43971 @qwx453971 <p> Yep I was totally aware , I mean I do n't like to shoot higher than 800 @ @ @ @ @ @ @ @ @ @ of lighting in the slowmo . I think my mistake was I totally put the spill only happens on blue green chroma setups , anyway i could n't do anything since the main problem was the backgrounds were reallly little . I already did the correction of this project ( the main reason I accepted to DoP it ) and at the end i could solve everything in 4 hours ( It had to be delivered this friday and the XML arrived Thursday at 5pm . <p> As a fun fact the other day I did second camera unit in the new Levis commercial , the main camera was the Red Weapon and the DoP shooted at 3200iso , i think it was at 8k. 
@@44332664 @4332664/ <h> SSD reliability in the real world : Google 's experience . <p> Since a lot of us are using SSDs as boot disks and media drives , here is an interesting article about the failure rates Google has seen after running thousands of SSDs in their data centers over the past 6 years . <p> Some interesting points : <p> SSD age , not usage , affects reliability . High-end SLC drives are no more reliable than MLC drives . Ignore Uncorrectable Bit Error Rate ( UBER ) specs . A meaningless number . Good news : Raw Bit Error Rate ( RBER ) increases slower than expected from wearout and is not correlated with UBER or other failures . Bad news : SSDs fail at a lower rate than disks , but UBER rate is higher ( see below for what this means ) . Bad blocks in new SSDs are common , and drives with a large number of bad blocks are much more likely to lose hundreds of other blocks , most likely due to die or chip failure . 30-80 percent of SSDs develop @ @ @ @ @ @ @ @ @ @ least one bad chip in the first four years of deployment . 
@@44332665 @4332665/ <h> Low Light Strategies <p> Wondering about the best grading strategies when light is low ( and a dark look is desired ) and there 's some minor noise from ISO boosting ( which , in my case , is a Sony FS100 , one of the most sensitive chips for its heyday . ) I am denoising with NeatVideo in advance , but after some great-looking grades , by the time the footage gets translated to quicktime for delivery to Youtube , there is additional blocking and banding in continuous tone areas which I have to fix back in BL with keying , blurring , and grain . Maybe the blocking is a quicktime issue regardless , but I assume that the issue is worsened by low light condition . The images coming from the camera only use the bottom 50% of the camera 's gamut . <p> Wondering if anyone has any suggestions to streamline and improve the results of the denoising/grading process in such situations . Wondering about : <p> 1 ) in the camera : up the noise in the signal with the camera 's @ @ @ @ @ @ @ @ @ @ full camera gamut with more noise from the camera than a narrow gamut and less noise . ( This is in part a camera / neat video question but thoughts are welcome . ) <p> 2 ) in Baselight for AVID : Increase the signal 's range to broaden the use of the full gamut as a first layer in Baselight and then bring the levels back down with the final baselight layer to give BL the full gamut with which to do its work between them . <p> Excessive noise-reduction can actually cause macro-blocking in some case . Try several approaches and see what looks better on YouTube . <p> My personal opinion is that a little noise here and there is not bad -- it 's keeping the noise consistent that 's important . <p> If you watch shows and films that are often set it night , you 'd be surprised at how thoroughly they 're lit . Walking Dead is a prime example : there 's pools of light all over the place , and the actors are generally backlit and they always have an eyelight @ @ @ @ @ @ @ @ @ @ see your point . It 's indeed the reduction in high frequency noise that has both drawn attention to and increased the low frequency noise . Barely discernable in Baselight , but obvious after export . In the case of this footage , going back into Neat Video and then increasing the low frequency noise reduction helped a bit . Keying out the offending areas , blurring out the remaining low freq blocking in them and then adding grain to break up banding from the resulting smoothing got rid of the issue , but it was a hassle ! <p> What I am wondering about is - when they are low - if expanding the dynamic range ( either in the camera or in Baselight 's top layer , or both ) , to give Baselight more to work with for the other grading tasks , will improve results all round . Not just for noise but for banding and the general fidelity of all operators . It wo n't have more data to work with , but it once the data 's expanded the programme will be working with @ @ @ @ @ @ @ @ @ @ Adding ( blurred ) noise , especially in the blacks discourages the codec from compressing too much . " I hope I 'm safe in assuming that 's affirmation of the blurring of noise I 've been having some success with . <p> " For YouTube uploads I always ' dither ' before I upload . " Can I ask where you accomplish this ? I ca n't find the operation in Baselight or AVID MC manuals . ( Have n't used it per se since my Amiga days ... ) I believe I am accomplishing the same ends with the Grain operator in BL as i would with dithering , though I understand the difference between the two . <p> " For YouTube or Vimeo I would recommend always to upload 4k/UHD " So you render to that format from the NLE then upload in that format . I assume then that the intended streaming format ( HD , in my case ) can be chosen from Youtube at the time of upload ( have n't seen that option array . ) <p> You could render the noise real @ @ @ @ @ @ @ @ @ @ the original video . Depending on the footage and the noise you could experiment with Linear Dodge ( Add ) , Linear Light , Exclusion for the blacks only , or even Pin Light if you center and contrast reduce the noise around mid grey . <p> Both Vimeo and Youtube render an uploaded video in various formats , it depends on the bandwidth of the viewer which one is selected unless the viewer specifically selects a resolution . 
@@44332667 @4332667/ <h> Color management on set <p> Here it is rather common to have rather combined set of monitors in usage like let 's say TVLogic or Sony Oled for director and some Small HD ones on camera . <p> Usually they do not match much in color even if you try to calibrate them , specially Small HD ones . <p> No super dedicated DIT usually on set ( usually non just someone do loading ) and all color is all over the place . <p> What i started to think of was how important at all is all that ? <p> Let 's say exposure can be measured with spot meter and contrast ratios calculated by that to get precision . Light color temp i would imagine also measured by Sekonic C-700 and leave all other decisions by that and not evaluate things with random displays . <p> So how much do others think color accuracy in that stage matters ? <p> I would say it is doable but monitors should be selected more precisely colors in mind if that would be the goal and not just slapping some random @ @ @ @ @ @ @ @ @ @ heard back from rental house that they went to laptop display in the end to evaluate colors whichs seems something i would never do myself . <p> From my perspective as a DP : 1 . Color accuracy is not so important on set for my work , but proper gamma and contrast are . It helps me to keep the lighting consistent . I have n't pulled out my color meter in a few years , but do use traditional light meters all the time . 2 . When non technical executives , producers , and clients are on set , color accuracy and beautiful monitors become much more important to keep the level of confidence high . <p> You will never get all the monitors the same for one reason : ACs have different needs than anyone else on set . <p> ACs learn quickly to crank the contrast on their monitors , and typically use some sort of edge-defining feature built in to whatever monitor it is . And they turn down the saturation . This is because their job is focus . Period . While the @ @ @ @ @ @ @ @ @ @ the focus is right . Nothing else matters . The changes I stated above accentuate the definition of the image in such a way that it makes it easier to judge sharp and not sharp . <p> So the AC 's monitors ( sometimes just SmallHDs on the camera , but often a 17 " on a stand nearby ) will not match the other monitors on set . <p> Part of my job is to funnel the DP 's eyes to my calibrated monitors more than any other monitor . A lot of DPs these days are operators too , so their eye is in the EVF during the take . So there 's only so much I can do there . <p> Oh , and also , people are CONSTANTLY adjusting the " Brightness " on the monitors in front of them . I check Video Village a few times a day to try to get them into whack , but it 's a never ending struggle . I can sometimes get help from a digital loader , or VTR , or VTR 's utility , but that @ @ @ @ @ @ @ @ @ @ people against the producers , hair + makeup , friends and family of the cast , and , yup , the director . <p> Oh , and also , people are CONSTANTLY adjusting the " Brightness " on the monitors in front of them . I check Video Village a few times a day to try to get them into whack , but it 's a never ending struggle . 43971 @qwx453971 <p> We need a special mode that provides a mild electrical shock to anybody setting the knobs out of center-detent . <p> As mentioned I use a spot meter to judge contrast and dynamic range , although this is often me squinting ! hey , I 've done it a long time now ... There is always one monitor that I have set up and it has tape over its controls with a note not to alter anything . <p> Oh and no mild shock , we need thousands of volts ! ! make their hair stand on end . <p> Everything else is whatever happens . <p> Oh and as far as CT goes , I now @ @ @ @ @ @ @ @ @ @ I do that then it tends to be easier to match in post . <p> Oh and as far as CT goes , I now auto white at the start of a scene . If I do that then it tends to be easier to match in post . 43971 @qwx453971 <p> I agree with that approach . In my opinion it is better to base the white balance on the actual camera you are shooting with ( in combination with the lens and filters ) rather than dial in a numerical colour temperature from a meter . 
@@44332668 @4332668/ <h> Flickering/Strobing issue on slow pan shot <p> I 'm having an issue with a video I 've graded and that I 'm mastering , and am a bit stuck as to how to fix it . <p> Basically , the video is a slow pan across a table , from above that was shot on Red . While playing back , the high/contrast part of the image are strobing quite a lot , which is fairly displeasing for the eye . It does n't seem to be as pronounced on every screen , and it looks a lot better when played back in FCP to an external monitor through an I/O , but this is destined to the web . <p> I was told it comes from the rolling shutter and the way r3d files process higher luminosity , and I 'm sure it comes a lot from the image being fairly sharp , but would anyone have a solution to fix it ? We 've tried deflickering ( not sure through which method , sorry , since I did n't take care of it ) , and @ @ @ @ @ @ @ @ @ @ to help . <p> Here 's a link to a bit of the problem shot : <p> It 's mostly visible when played back at 1080p and full screen . And it 's not better on the ProRes master . <p> The shutter angle was set to 180 . It was shot at 50fps for 25fps playback though , so yeah it s due to shooting problems . It 's my job to fix it now though <p> I 'm not sure how I can add frame blending when there are no frames to blend it with ? Do you mean Motion Blur ? I tried some directional blur , since the movement is constant , motion blur should have a pretty similar effect , but it did n't help at all . I need to move the project to AE CC and try that new motion blur effect , maybe . <p> I think it 's not just RED , it 's any digital camera without a hard shutter , ie ; Alexa Studio or F65 <p> I have delt with this by starting with using Foundry 's adaptive @ @ @ @ @ @ @ @ @ @ cases running the footage at 99% in a opticalflow time remaper , and then one more pass at 101% to get the frames back , hard to do on food tho due to the edge violations everywhere ... i 've even made a pile of clean plates and comped the shot back together with a 3D camera move to match the in camera move ... <p> The first section of the shot ( which is n't in this bit that I posted ) has been twixtored to 150% speed and it does n't look better , so I 'm afraid that it 's not going to fix my problem . I had tried the built-in timewarp AE effect which I believe is now based on the Kronos plugin , but it 's not better . <p> I 'm going to have to deliver this soon , so I might have to live with this problem , but I 'm curious to know if there is a fix ( both in camera and in post ) . <p> I think shooting a lot slower may be the key here . Have @ @ @ @ @ @ @ @ @ @ of a job now and can not shoot on 4 days time . I remember some old DP told in the chat somewhere : " try to shoot sky with stars and pan it , it is a nightmare " <p> So i believe it is well know craft issue even from some film cameras . <p> I have lately seen this here also on few films i have worked on . <p> So my 2 ways i would test now would film physically more slow pan or shoot some 200 fps . So one you would speed up in post the other slow down . Generally you would have more " frames " to play with . <p> I may be wrong here big time but this is why interlaced tv is smooth and 24-25p may be jerky in my mind and they have " half " the frames interlaced has . <p> I 've seen this on films ( opening ttitle from the movie Sahara for instance ) . If you pan too fast at too low a frame rate that 's what you get . Try the @ @ @ @ @ @ @ @ @ @ ? <p> I 've seen this on films ( opening ttitle from the movie Sahara for instance ) . If you pan too fast at too low a frame rate that 's what you get . Try the Force Motion Blur in After Effects , it might help. ? 43971 @qwx453971 <p> Actually , having just watched your clip on a bigger screen , are you talking about a sort of stutter every half second , or is that just YouTube ? <p> Actually , having just watched your clip on a bigger screen , are you talking about a sort of stutter every half second , or is that just YouTube ? 43971 @qwx453971 <p> Yeah I have two different problems here but with similar effect . The strobing of the high contrast parts , and the stutter . The strobing is present everywhere , no matter what application or computer I use , but the stutter is only visible when I watch it in Quicktime ( or YouTube ) . The file plays fine ( if I forget about the strobing ) in FCP/Premiere , and on Windows @ @ @ @ @ @ @ @ @ @ I have n't been able to check myself ) . 
@@44332669 @4332669/ <p> Hi and welcome to The nuke world . There are many tip in nuke for color grading , The best option isla working with The grade node forma channels , for matching i think you should work with grade node blackpoint and whitepoint , if you choose that and in The viewer use ctrl shift alt you have The values directly of The shot . You have match color of furnace to+ , if you need more info please send me an inbox and i will send you what you want <p> I 've tried the grade node a few times and sometimes I get great results , other times ( like now ) not so much . The grade I am working on at the moment is an animation that 's being put in to an already graded background , and I have to fit it in so it does n't look totally out of place . Using the grade node only created weird and bright colors , so I scrapped that idea . I 'll keep trying and find a way around it . <p> Hi @ @ @ @ @ @ @ @ @ @ tip in nuke for color grading , The best option isla working with The grade node forma channels , for matching i think you should work with grade node blackpoint and whitepoint , if you choose that and in The viewer use ctrl shift alt you have The values directly of The shot . You have match color of furnace to+ , if you need more info please send me an inbox and i will send you what you want 
@@44332670 @4332670/ <h> EDL Conform Issue - Resolve <p> I 'm conforming a 30min timeline . In the EDL , Clip name points to the avid named scene names ( V14A-2B ) . Source file shows the correct file name ( A019C011 ... ) . When I try to use the EDL in resolve , with any of the " assist using reel names from ... " it does not find any media , only showing those scene named clips all missing . If I uncheck " assist using reel names from ... " and then load the EDL , it finds most of the media , but I get a ton of reel conflicts and some of those do n't have the right clip in the options to choose from . As a test , manually replaced the clip name with the source name on the EDL in text editor and it works perfectly , but , there 's 500 shots and will take forever . Any suggestions on how to fix this ? Tried various settings in Avid EDL manager without success . Any other applications I should be @ @ @ @ @ @ @ @ @ @ are the camera source files ? How are the files named ? What are their timecode numbers ? <p> If somebody changed the source clips to the scene name in the edit , you 're screwed . The only answer I see is to just roll your sleeves up and manually conform the entire thing . Make sure you bill the editor for your time . And be glad it 's only 500 shots -- I 've seen cases where it was a lot more . If each shot takes you a minute , it 's 500 minutes = 8.3 hours , and that 's not the end of the world . <p> If the offline material was good enough quality , you could just have the editor render out a flattened file and color-correct that . This will immediately eliminate the conform problem -- provided it 's of high quality . <p> What format are the camera source files ? How are the files named ? What are their timecode numbers ? <p> If somebody changed the source clips to the scene name in the edit , you 're @ @ @ @ @ @ @ @ @ @ roll your sleeves up and manually conform the entire thing . Make sure you bill the editor for your time . And be glad it 's only 500 shots -- I 've seen cases where it was a lot more . If each shot takes you a minute , it 's 500 minutes = 8.3 hours , and that 's not the end of the world . <p> If the offline material was good enough quality , you could just have the editor render out a flattened file and color-correct that . This will immediately eliminate the conform problem -- provided it 's of high quality . 43971 @qwx453971 <p> Source files are Alexa Prores 4444 . Timecode matched the EDL , and source clip filename matched the EDL , it was just the clip name in EDL referenced the scene number . First approach that worked , was to manually copy and paste the source name to the clip name for every shot in the EDL . That was taking too long . What finally worked , was in Avid EDL Manager , was able to disable comments and @ @ @ @ @ @ @ @ @ @ A019C011 ) and import that into Resolve . That , mixed with " Assist using source clip file name " got me 60% there , and the rest was either selecting the right clip in the reel conflict window or a couple force conforms . Took about 3hrs . Whew ! I do wish there was a feature in Resolve to use selective data from EDLs . <p> Source files are Alexa Prores 4444 . Timecode matched the EDL , and source clip filename matched the EDL , it was just the clip name in EDL referenced the scene number . 43971 @qwx453971 <p> If this is Avid , I 've generally used AAF 's and those have been flawless in conforming in Resolve . I always ask for an EDL ( and an XML if I can get it ) just as a last gasp , and I use the EDL just in the event I have to find a missing shot by eye so I know what the file name is . The truncated source file name is generally enough for Resolve to figure it out . <p> @ @ @ @ @ @ @ @ @ @ two ago , and the only issues I had were with flipped Steadicam shots and a few speed changes , since those are not carried in the lists ( at least not these ) . But I caught those with no problem when checking with the reference video . <p> What I wish clients would do is always provide a reference video with visible source file name , source timecode , then project timecode , so we could manually whack at the thing if we had to . Sometimes , the only real answer is manual fixes , and I would say if you solved the problem in 3 hours , you got off easy . <p> I have not used Avid EDL Manager , but that 's something I really ought a learn how to use . 
@@44332671 @4332671/ <h> New shape vs. new curve ? <p> " New Curve " seems to add a new ( I do n't want to say Shape ) ' outline ' inside the same Master control ( Blue Box ) . It also creates a void where the two curves overlap . It can be used ( if fully contained ) to create a hole in a shape . <p> " New Shape " seems to create a new Master control ( Blue Box ) for each shape created , it can have its Layer ( front/back ) position adjusted and the combining options include Union , Subtract &amp; Stamp ( still need an explanation on " Stamp " ) . <p> " New Shape " seems the more robust of the two options . If you are rotoing a body , hand , face , etc. , it seems mandatory . <p> Is " New Curve " there for legacy reasons , or yet another example of BL 's penchant for providing more than " one way to a skin a cat " ? <p> Buller ? No one ? @ @ @ @ @ @ @ @ @ @ would take quite some time . So , if you use an add new curve , you will get an intersect of the shapes . In case of adding the new shape , you will get a classic sum of both shapes . For obvious reasons , it is impossible to have an intersect with an edge , so you 'll see , that that shape is not even available as an added curve , only as a new shape . When you add an outside shape , then the behavior is different again and now you 'd see , that intersect button becomes active . There you can combine up to four shapes with boolean formulas and there you can also invert individual shapes as well as the combined shape . So , you can do it the easy way , which I personally prefer , or you can do it the very sophisticated way . Good luck ! <p> In my understanding and operating is a curve a part of a shape . If I need boolean operations between a matte ( means localized selection ) and other @ @ @ @ @ @ @ @ @ @ particular shape ( or matte ) . The matte merge operator does boolean merges between mattes ( not curves if they a part of the same shape ( matte ) ) . <p> Like Knut says , shapes are composed of curves . The way I think of it is a shape layer contains one or more curve elements . So for instance , if you wanted to sharpen a person 's eyes , you would use a single " shape " , composed of two " curves " , one for each eye . These two curves , while not necessarily overlapping , would make up a single shape that is trackable , transformable , and could include boolean operations based on how the curves intersect . <p> Filmlight has a good tutorial that goes through basic shape operations , and covers a lot of what you can do , including several key commands which are really helpful . <p> So for instance , if you wanted to sharpen a person 's eyes , you would use a single " shape " , composed of two " curves " @ @ @ @ @ @ @ @ @ @ while not necessarily overlapping , would make up a single shape that is trackable , transformable , and could include boolean operations based on how the curves intersect . 43971 @qwx453971 <p> Except for eyes sharpening shape I use a shape consisting not of two curves , but of two oval shapes , tied together with a pivot between them . This way two shapes act as a single shape , that can be tracked , while both shapes pivoting around the central pivot point and can be adjusted together with one adjustment control . 
@@44332672 @4332672/ <p> We 've been making optical discs ( mostly features , for commercial release ) since 2000 . The bottom dropped out of this market long ago . We decided not to bother with UHD discs because the cost to buy the authoring tools is prohibitively high for the number of companies interested in releasing on physical media . The major studios will do it , but in my opinion , this format is pretty much DOA because it excludes indies , and is n't enough of an improvement over other offerings to make it worthwhile . <p> DVD took off because it was a huge improvement over VHS in many ways ( not just picture quality ) . It was reasonably easy to make a disc and it was affordable even for indie filmmakers and one-time creators of discs . We built a pretty good business on that . Blu-ray was an incremental improvement , burdened by the greed of the patent holders behind it , shutting out indies for several years , and then making concessions to lower costs way too late . With UHD authoring software @ @ @ @ @ @ @ @ @ @ back in the same boat . <p> Personally , I have n't owned a DVD player at home in years . Never bothered with BD . We stream everything now , and I do n't miss physical media one bit . Good riddance , I say . <p> We 've been making optical discs ( mostly features , for commercial release ) since 2000 . The bottom dropped out of this market long ago . We decided not to bother with UHD discs because the cost to buy the authoring tools is prohibitively high for the number of companies interested in releasing on physical media . 43971 @qwx453971 <p> I do n't there is a recordable UHD blu-ray in the market yet , last time I checked there was n't any , and I tried to see a UHD blu-ray demoed on some resellers but they 're always sold out , or they do n't put it on the floor due to high demand . <p> With a decent home theatre setup , the UHD Bluray experience can easily surpass most of the cinema chains ... 43971 @qwx453971 <p> I @ @ @ @ @ @ @ @ @ @ dead . Nobody except for an increasingly small niche market actually wants to deal with it when streaming is as easy as it is . For most people , the HD or 4k you get from Netflix , Amazon , etc , is plenty good . Of course it 's not the same as what you 'd get on a disc , but few people care about that level of difference in quality . And as codecs improve and internet speeds get faster , physical media is just going to become more and more marginalized . <p> When I get a uhd TV some day I hope these discs will still be around . Nice not pounding the internet for a perfect uhd home experience . I currently watch Netflix via their Blu-rays AND screaming . It 's nice to have both . <p> Physical media is so dead that pretty much every store in Finland is selling blurays and DVDs . Here 's the thing : Not all countries have good streaming options for stuff . For example the Finnish Netflix is goddamn terrible and most of the other @ @ @ @ @ @ @ @ @ @ extremely disappointing outside the US , or at least in Australia . That said in this country the main substitute has been piracy as opposed to disk sales . <p> I used to work at a DVD rental store and there was a sudden and measureable dropoff from what it used to be . Blurays definitely never took off to the same extent . While I think Bluray will never reach the hay day of peak DVD , hopefully there will be a resurgence in the market for high quality media . <p> Send them a VHS and have done with it . Most of my DVD collection is unwatchable now due to kids and bad manufacturing . The VHS will look better on the shelf anyway . 43971 @qwx453971 <p> I ripped the important DVDs to hard drive years ago , and then replaced the important ones with Blu-ray versions . And now we 're in the process of ripping all the Blu-rays to hard drive . We 've completely protected even in the event of total annihilation ! <p> Here in Australia , net speed is nowhere fast @ @ @ @ @ @ @ @ @ @ located close to the CBD and enjoy expensive fibre to the premises ) so DVD is still the way most people watch a movie on their TV . I 've had a big 4K TV for at least two years now and UHD BDs are just starting to appear in the big chain stores with one or two UHD Players available priced between $400 and $800 dollars each . The enthusiasts will buy them and I 'm of the age where I enjoy owning my own disks so I 'll probably upgrade early next year , when prices start to drop . I think ' The Revenant ' is one of the few UHD disks currently available and sells retail for about $50 . <p> We have one or two 4K streaming service available such as Netflix available for an expensive subscription model but to the best of my knowledge , they do not allow high resolution downloads for later viewing . For 50% of the rural population west of the big cities , slower than ' dial up ' internet is the norm . Most people under 30 watch @ @ @ @ @ @ @ @ @ @ claim 4G cell phone coverage to 90% of the population but that would be less than 20% of the Continental land mass. 
@@44332673 @4332673/ <h> Applying Grain Matte to highs/mids only - Resolve <p> This is a novice question ... and I 'm sure it has been answered here before . But I want to apply a grain . mov as a matte to the qualified highlights and midtones only of a clip . I thought I could add the matte to the top part of my layer node with a qualifier in the bottom , but that was n't working . Can anyone point me to the best way to do this ? Thanks so much ! <p> Interested in this question as well . Not applying grain to highs/mids only - this is not how real film grain behaves . But trying to simulate how real grain varies , according to the different densities in the image . <p> True - this is hard to accomplish , since every film stock behaves differently . The possibilities are infinite . An application like LiveGrain is highly customizable ( and highly expensive ) , giving you control over every aspect of how grain appears . But ( as mentioned in other threads in @ @ @ @ @ @ @ @ @ @ as one respected colorist said - the results are virtually undistinguishable from applying a grain plate as a matte with the proper key/curve , so it does n't simply appear as a blanket of grain . <p> My question is this precisely : keeping it simple but natural-looking , how are people applying grain plates as a matte and how do they key it to make it look natural and an integral part of the picture ( as if it was film grain , not digitally-applied grain ) ? In a nuttshell , how does grain behave according to the different densities in the frame ? Is it none in the deeper blacks , less in the shadows , more in the midtones , then again less in the higher values and none in the highest highlights ? Like a bell shape/curve ? <p> Is Film Grain Resolve FX real film grain or synthetic film grain ? Also - is it applied naturally ? ( based on the different densities of the image ? ) 43971 @qwx453971 <p> I would say the grain is synthetic but based on the look @ @ @ @ @ @ @ @ @ @ flavorings used in mass-market food : it tastes just like chicken , only it 's not chicken . <p> " Applied naturally " is in the eye of the beholder . I worked with film for more than 30 years , and grain is a slippery process . It also changes on every single shot , and hinges on light , exposure , film stock , and the developer . There 's a lot of things that can make it change . I 'd say it 's very subjective and you have to use good judgement and experience to decide where and how much to apply it . The highs/mids/lows adjustment will add another wild card to the equation . <p> I generally tend to want to apply more grain than my clients do , because I 'd rather have the effect be visible than be very subtle . But the client is the ultimate judge on what they want , and it 's a very subjective area . I always felt that with real film , the lightest part of the negative generally had the most grain , because @ @ @ @ @ @ @ @ @ @ up . I tend to go with Cinegrain because I think it looks the best to me -- I tend to grab the 500T ( 5219 ) grain , partly because it 's my favorite emulsion -- but it depends on the project . For real crazy old stuff , we have used 16mm grain before , but I have yet to use the Super 8 . <p> Since the ResolveFX grain is so much more convenient , I might tend to use that more often , particularly with caching , if it helps the scene and I was in a hurry . But I 've used Film Convert grain , Gorilla Grain , Sapphire Grain , all kinds of stuff as tests in the past , and generally decided that the Cinegrain had a better overall feel . I would use the latter if there was enough time available in the schedule . <p> When you mention " the lightest part of the negative " - are you referring to the darkest parts of the positive ? I think you are referring to the highlights instead ? If so @ @ @ @ @ @ @ @ @ @ <p> When you mention " the lightest part of the negative " - are you referring to the darkest parts of the positive ? I think you are referring to the highlights instead ? If so , do you put most of the grain there ? 43971 @qwx453971 <p> The highlights -- most of the grain in film winds up in the highlights , which is the darkest part of the negative . My take would be that highlights would get more grain than blacks . But ... there 's no rule that says you ca n't be creative and try something different . <p> The highlights -- most of the grain in film winds up in the highlights , which is the darkest part of the negative . My take would be that highlights would get more grain than blacks . But ... there 's no rule that says you ca n't be creative and try something different . 43971 @qwx453971 <p> Hi Marc ! <p> You are right . <p> I 've read a few scientific papers on the distribution of grain as it relates to the various @ @ @ @ @ @ @ @ @ @ , but to oversimplify it , the grain " curve " appears more or less like what you see in the attached picture . Like you say , there is visibly more grain in the right side ( towards the brighter values ) then in the left side ( towards the darker values ) . <p> But more precisely , there is none in the deepest darks , then it increases a bit in the shadows , it becomes really visible in the midtones and especially towards the shoulder part of the image ( where we have most of the grain ) , then it decreases again as we approach pure white - but , surprisingly , it does n't disappear completely in the whitest whites . <p> Still , it would be nice if we could have the option of replicating a similar curve in Resolve ( in a very simple way , using only one-two controls ) when we apply the grain plate as an external matte and use blend . <p> One could go crazy trying ( probably in vain ) to replicate exactly how grain behaves @ @ @ @ @ @ @ @ @ @ go - especially for features . Applying grain as a matte and using blending BUT being able to apply such a curve as far as the visibility of the grain is concerned might be the ticket . <h> Attached Files : <p> I 've read a few scientific papers on the distribution of grain as it relates to the various densities in the frame . It 's a huge subject , but to oversimplify it , the grain " curve " appears more or less like what you see in the attached picture . Like you say , there is visibly more grain in the right side ( towards the brighter values ) then in the left side ( towards the darker values ) . 43971 @qwx453971 <p> Yes , that 's my gut feeling just based on too many years of transferring bad negative . I do have to chuckle that , after decades of trying to get grain out of the picture , now we 're trying all kinds of different methods to put grain back in the picture . <p> I do n't think it 's worth @ @ @ @ @ @ @ @ @ @ to apply grain , but I could see using ResolveFX 's control to lean more towards highlight grain than ( say ) lowlight grain . I would also bet that Film Convert , Sapphire , Boris FX , and the other plug-in companies will react with similar features on their grain plug-ins. 
@@44332674 @4332674/ <p> In Vertigo , Hitchcock 's work with color design is exceptional . Both from the costume design and set decoration point of view ... It goes hand in hand with the story and supports dramaturgy of the movie . Really inspiring . <p> Thanks for posting this wonderful resource Barbara . The stills galleries are stunning and fascinating . When you say " a calibrated camera set up in HDR " , can you tell a little more about what **35;567;TOOLONG you used . <p> I 'm very grateful for your kind comments about my work , thank you very much . <p> @ John Claude : <p> Since last summer ( 2015 ) I am using a Canon EOS 5Ds R with 50 MP resolution . The whole chain is calibrated with an IT-8 backlight target in Adobe RGB . When I 'm taking the photographs I make sure that the exposure is correct by checking it with the histogram . Apart from cropping the images I do n't do any processing/color grading . <p> For web publishing I apply a little bit of unsharp masking and @ @ @ @ @ @ @ @ @ @ scientist Franziska Frey in her paper about the reproduction of paintings . 
@@44332675 @4332675/ <h> Changing solid color Q <p> I 'm grading a premiere project in Resolve v12 . XML came in and grade is fine . There are a bunch of dips to white . Many came across fine but some came across as dips to black ( labeled as dip to color but black ) and others are supposed to be white slugs but I have a clip called Solid Color but it is black . I ca n't seem to change the black to white . I can edit a new one in but would like to just change the color of what is there . <p> ken dumb question ... do the the colour dips actually show up as a resolve transition so can you control them in the effects inspector tab on the edit page 43971 @qwx453971 <p> Actually , I checked again . in the inspector , it says Solid Color and there 's a slider next to Color with a value of 0 but I ca n't change that . It 's not that big of a deal as there are only a few and @ @ @ @ @ @ @ @ @ @ <p> Actually , I checked again . in the inspector , it says Solid Color and there 's a slider next to Color with a value of 0 but I ca n't change that . It 's not that big of a deal as there are only a few and I 'm round tripping back anyway but just curious . 43971 @qwx453971 <p> This is a longstanding bug that has existed since they introduced this auto generation of solids via the XML workflow . <p> When I saw it the color of the solid came in with a weird value such as " null " or something similar and would n't allow editing . I reported this bug but its never been fixed . <p> Please report to BMD , if more people report these little bugs its more likely they will get fixed . Each little bug like this is not a big deal on its own but cumulatively they can be burdensome . Death by a thousand papercuts <p> This is a longstanding bug that has existed since they introduced this auto generation of solids via the XML @ @ @ @ @ @ @ @ @ @ solid came in with a weird value such as " null " or something similar and would n't allow editing . I reported this bug but its never been fixed . 43971 @qwx453971 <p> The maddening thing to me is that if you create a timeline in Resolve with several generator , text , and solid color backgrounds , then export this timeline as an XML , then create a new timeline in Resolve with this XML , the generator , text , and color backgrounds will not be right coming back in . Usually the text is lower-third ( instead of regular text ) , the color backgrounds are missing or wrong , and the generators are wrong . I 'm assuming this generation of XML ca n't interpret what these " non-clip " clips are . <p> Create a compound clip of white in the color generator , and create dissolves to white on a second layer . Or just do it in color correction , timing it to the original edit using an offline reference . 43971 @qwx453971 <p> There were only a few so I just @ @ @ @ @ @ @ @ @ @ it . Is there a way to load a solid color into the source monitor ? I found that I needed to drag it directly to the timeline . <p> This is a longstanding bug that has existed since they introduced this auto generation of solids via the XML workflow . <p> When I saw it the color of the solid came in with a weird value such as " null " or something similar and would n't allow editing . I reported this bug but its never been fixed . <p> Please report to BMD , if more people report these little bugs its more likely they will get fixed . Each little bug like this is not a big deal on its own but cumulatively they can be burdensome . Death by a thousand papercuts <p> Especially if BMD wants to continue building Resolve as a complete finishing solution . <p> There were only a few so I just edited in a solid color from Resolve and that was it . Is there a way to load a solid color into the source monitor ? I found that @ @ @ @ @ @ @ @ @ @ @qwx453971 <p> No , just the timeline . The exception would be if you actually created a physical compound clip , which would then behave just like any clip with timecode -- it can be cut , pasted , superimposed , corrected , reframed , etc. 
@@44332676 @4332676/ <h> Adobe Premiere CC DCP export . anyone tried it ? <p> Premiere CC DCP export ... I was wondering if anyone here has tried this . Not that I 'm planning in doing it ( using easydcp here ) but i was curious about the results ( export speed , gamma/lut interpretation , artifacts , frame rate , aspect ratio , etc ) . I would test it myself if i had Premiere CC at hand . I assume it should be a cool way to quickly have a viewable file for festival application for instance ... but not so much for master delivery ( ? ) ... <p> I have n't created a DCP directly out of Premiere , but I did create several out of Adobe Media Encoder when the feature first came out . I believe both are based on Quvis Wraptor , with the option to upgrade to a more feature complete version if you find you need more than basic DCP encoding . <p> Overall , I would say it worked fine , but the quality was n't as nice as the output @ @ @ @ @ @ @ @ @ @ For some reason , even though I specified 250Mb/s , the bitrate never made it anywhere near that high . I think it peaked at around 80Mb/s . This made the file sizes considerably smaller than the others , which might seem nice , but to me felt more like unused potential . No matter what I tried I could not increase the bitrate any more than that , and you could see the image suffer for it in direct comparisons . FinalDCP and DCP-O-Matic both created much larger files , and likewise much higher average bitrates , usually between 150Mb/s - 200Mbs . <p> Of the three , FinalDCP created the nicest looking image , with DCP-O-Matic just slightly behind , and Media Encoder being a more distant third . I brought all three versions to our QC screening , and the director was quite happy with FinalDCP and DCP-O-Matic , but only watched the Adobe encode for about 5 minutes before we turned it off and moved on . <p> I do not create DCPs regularly , and things may have improved dramatically over the last year or @ @ @ @ @ @ @ @ @ @ looking option , beyond Premiere or AME , DCP-O-Matic is surprisingly good for very little cost and effort . <p> I 've used it for projection internally but would n't trust it for festival submission , as I 've had some problems with them with some older projectors . <p> Biggest plus I would say is the small file size and convenience out of premiere . Biggest negatives are that the images seem a little too dark to me , and that it ca n't output a 7.1 mix unless you use the stand alone version . The space they seem to be competing against is against free dcp creators with a significantly better render speed , less so against more professional packages due to the lack of flexibility . <p> I made a DCP using AME ( you 're recommended to send from Premiere to AME , not use the Wraptor setting directly in Premiere ) and it looked really good in the cinema and was very easy to make . Interesting to read the other comments here , because I did n't compare it to anything else and @ @ @ @ @ @ @ @ @ @ . And I naively believed it would set the bit rate at 250Mb/s as set ! <p> Premiere CC DCP export ... I was wondering if anyone here has tried this . Not that I 'm planning in doing it ( using easydcp here ) but i was curious about the results ( export speed , gamma/lut interpretation , artifacts , frame rate , aspect ratio , etc ) . I would test it myself if i had Premiere CC at hand . I assume it should be a cool way to quickly have a viewable file for festival application for instance ... but not so much for master delivery ( ? ) ... 43971 @qwx453971 <p> Every thing has pretty much has been said about it quality of AME and prem above , however there is one small thing that premier does with DCP , s that is really really handy ( it has been mentioned on the forum before ) <p> you can put a mastered unencrypted DCP in a premier time line and play it back .... ( not as colour check or anything ) but just @ @ @ @ @ @ @ @ @ @ audio assignments on 5.1 <p> now don , t laugh .... but i have had 3 so far this year where people have exported discreets and mis labelled the channels you can sort of see it in easy DCP player if you have it ( i always check whole thing obvs ) but if your one of those pesky indie kids home brewing your DCP possible you won , t know till your in the theatre going for a test screening <p> premier can give you that extra bit of confidence before your head explodes with the joys of formating a EXT2/3 drive getting the right inode size and setting your permissions <p> When I asked the guys at Quvis if it would respect an RGB 444 source , they told me that Premiere Pro should pass on the info about the source so it would . The fact that none of these controls is accessible is n't very encouraging though . <p> Out of interest , what was wrong with DCP-o-matic 's image ? Colour errors ? Some other artifact ? 43971 @qwx453971 <p> Nothing was wrong with it @ @ @ @ @ @ @ @ @ @ FinalDCP , the image was slightly softer and a little less saturated . Just a small amount , but all of us noticed it . Fwiw , the DCP-O-Matic encode was what we used for the screening , as the FinalDCP encode had audio sync errors that we were never able resolve , while the DCP-O-Matic audio was spot on . I think that 's pretty admirable for a free product . <p> Nothing was wrong with it , it was quite nice looking , but compared to FinalDCP , the image was slightly softer and a little less saturated . Just a small amount , but all of us noticed it . Fwiw , the DCP-O-Matic encode was what we used for the screening , as the FinalDCP encode had audio sync errors that we were never able resolve , while the DCP-O-Matic audio was spot on . I think that 's pretty admirable for a free product . 43971 @qwx453971 <p> Was there any difference in render speed between finaldcp and dcp-o-matic ? <p> I just took a look at dcp-o-matic and must say it 's quite impressive @ @ @ @ @ @ @ @ @ @ soon and compare against FinalDCP to see if I can spot the differences as well in the images . <p> I 'm totally new to DCP creation and just ran a test quicktime through dcp-o-matic . After rendering I opened the resultant MXF in Premiere , and I expected it to look greener and brighter ( or something ) on my Flanders monitor ( rec709 ) and it mostly just looked the same . <p> I 'm not sure if Premiere is doing some color managing that I 'm not aware of . The file properties dialog showed that it knew it was a 12-bit XYZ file . <p> After doing some googling , I can only say that internet forums ( not this one ) discussing what DCP packages work best are awash with negativity and accusations , go figure . Apparently , they 're all terrible and will get you rejected by cinemas ! <p> Exporting still frames from Premiere from JPEG2000 MXFs ... And I 'm sure this is a completely naive statement--and I 'm missing plenty of information--but I do n't see why you would spend @ @ @ @ @ @ @ @ @ @ googling , I can only say that internet forums ( not this one ) discussing what DCP packages work best are awash with negativity and accusations , go figure . Apparently , they 're all terrible and will get you rejected by cinemas ! 43971 @qwx453971 <p> Yes , that 's right , all DCP creation software is unusable and nobody is making any DCPs 
@@44332677 @4332677/ <h> GoPro " Native " white balance <p> The latest GoPros have a white balance setting called " Native " , which the manual says is : <p> Minimally color-corrected file from the Native image sensor that allows for more precise adjustments in post-production <p> It appears to be around 6500K plus some magenta . GoPro Studio software has a color matrix to correct this , or of course you could just grade it manually . But if the scene is lit with tungsten , I would expect better results by setting 3200K in camera , so you 're not making such a large correction in post with the low bitrate H.264 . Has anybody done any testing of this ? <p> If possible , avoid the GoPros . The autogain issues make color-correction difficult when the camera moves from very dark to very bright images . I 'm a much bigger fan of a Canon 5D or a Blackmagic Pocket Camera run in pure manual mode with better lenses . <p> Agreed . I work a lot with property videos . So the producers and real estate agents @ @ @ @ @ @ @ @ @ @ " 4k gopro drone . " It 's some of the toughest stuff to match , and the exposure changes every second . Ca n't wait to see what the micro blackmagic camera can do . But if you have to use them , like I do , then I 've found it better to actually adjust the color temp manually , and disable any auto functions . <p> I think for a lot of gopro use , there is no " time " to fiddle with the white balance settings : you can only do it with the smartphone app and many times you 'll have a bunch of gopros . So " native " is better than auto wb , as all the " native " cameras will match . <p> I have n't tested gopro with " native " vs correct wb , but to have the correct wb set in camera is the standard procedure with pretty much all cameras , unless shooting in RAW . <p> Agreed . I work a lot with property videos . So the producers and real estate agents throw their @ @ @ @ @ @ @ @ @ @ gopro drone . " It 's some of the toughest stuff to match , and the exposure changes every second . 43971 @qwx453971 <p> The exposure changes are maddening . Color Temperature you can overcome to a point , but it 's nice to start out with something reasonable . <p> The black versions have an iso lock which might help . I urge people using gopros to shoot using protune enabled with everything lowered ( sat , sharpness etc ) . 43971 @qwx453971 <p> Jesus , I try to discourage people from using the GoPro period and tell them to use the Blackmagic Pocket Camera instead . It 's $995 , can use real lenses , and is n't that much bigger : 12.5 ounces , 5 " x 1.5 " x 2.5 " , and it 'll shoot ProRes or CinemaDNG Raw . <p> I just had to deal with some drone footage shot on some cockamamie little camera , and everybody forgets that the 8-bit stuff and nasty clipping kills you far more than any benefits gained from the 4K resolution . <p> Jesus , I try @ @ @ @ @ @ @ @ @ @ them to use the Blackmagic Pocket Camera instead . It 's $995 , can use real lenses , and is n't that much bigger : 12.5 ounces , 5 " x 1.5 " x 2.5 " , and it 'll shoot ProRes or CinemaDNG Raw . <p> just had to deal with some drone footage shot on some cockamamie little camera , and everybody forgets that the 8-bit stuff and nasty clipping kills you far more than any benefits gained from the 4K resolution . 43971 @qwx453971 <p> I agree 100% but sometimes jobs come in with gopro footage and I 'm not one to say " not gon na do it " . It just advise I 'm degrading other higher quality format to meet a middle ground . Aerial stuff seems to always be gopro or heavily compressed 8b h264 for many projects , so I got ta deal most times . Hoping to see more micro cinema in the future . <p> I had to match 6K Red , 4K Blackmagic and the inbuilt DJI Phantom camera with auto-balance one once . Fun times . What do @ @ @ @ @ @ @ @ @ @ 43971 @qwx453971 <p> Have n't used one yet , but the concept looks fantastic . Still shoots ProRes 422HQ or CinemaDNG Raw HD . And it 's actually 10-bit . A camera like this would stomp the crap out of a GoPro in terms of picture quality , 4K or no . <p> I agree 100% but sometimes jobs come in with gopro footage and I 'm not one to say " not gon na do it " . It just advise I 'm degrading other higher quality format to meet a middle ground . 43971 @qwx453971 <p> Same here . If they bring in GoPro material , they immediately see on screen that it looks terrible and it becomes a process of triage : fixing what 's possible , minimizing the horrible , and matching the footage before and after . <p> While I never say " not gon na do it " ( assuming the check is good ) , I do mutter " madre de dios " under my breath quite a bit . <p> I never said anything about auto WB . The question is between @ @ @ @ @ @ @ @ @ @ answer is nobody really knows because most people who cares about the image quality ( aka LGG members ) avoid the camera altogether . <p> How many of you are unlocking the Protune file to access the higher bit depth &amp; colour space ? Also , I always have my Hero 4 Black set at minus .5 stop in exposure compensation to help the highlights from overexposing . That helps a lot . Cheers Pete . <p> How many of you are unlocking the Protune file to access the higher bit depth &amp; colour space ? 43971 @qwx453971 <p> There is no " key " to unlocking GoPro footage . ProTune increases the bit rate , not the bit depth . It 's still highly compressed 8-bit H.264 , it just has lower contrast and saturation , and enables some manual settings . 
@@44332678 @4332678/ <h> Clarity/ Noise reduction first or last ? <p> I 've been experimenting with order-of-operations in Nucoda , especially with DVOs like Clarity , ( De ) Grain , and Sharpen . In the past with Resolve , I always added noise reduction as a final node , but in Nucoda it seems more common to add NR operations as Input Effects . <p> I also used to grade without NR and only add as much as I needed at the very end depending on how the overall exposure in each shot turned out . I 'm considering using that workflow again ; grade without NR and then add Clarity or Grain reduction as a final FX layer , and wanted to see if anyone else had tried that and if you liked the results better ? <p> Frequently I 'll use Clarity as an InputFX but I have used it before as part of the image texture at the end of a grade . It can be more frustrating for grade tweaks as I grade in a very structured way that means I would always have to recache @ @ @ @ @ @ @ @ @ @ mind you ... <p> I often use Sharpen , RGB Regrain , Chroma and others at the end . On a series I 've just completed I had to lift a particular part of the image a lot so did the shape on the Base layer and then used Matte : Previous to just do Clarity for that shape . I was then able to cache that layer in the router and still make my grade adjustments as normal - my look is usually on layer 10 or so and my balance and shapes before that . <p> Jason , I 've always applied my noise reduction at the end of the chain in resolve as well . Unless , for example , I 've pulled a sky key and am pushing a gradient pretty hard which begins to add noise to the sky in which case I 'll add noise reduction just to that particular sky key area of the image . In my very brief experience with Nucoda , it appears more likely that you would add these into the input before your correction layers as you suggest @ @ @ @ @ @ @ @ @ @ on selection areas of your image down the layer chain if necessary like the situation with a sky as I mentioned earlier . Not sure there 's a right way or wrong way to do it but applying clarity to the input of the grade might make it easier to pull keys downstream . <p> I think the main reason that DVO Clarity is more often an InputFX is due to the quality of the NR . Highly non-destructive compared with other NR tools out there . <p> I think it 's all circumstantial as you say . As for pulling cleaner keys , I would n't use NR for that but different colourspaces such as YUV . Looking forward to the 3D Keyer announced for Resolve as the one in the Nucoda has been my primary keying tool for the past 5 years ! <p> The idea that NR quality makes a difference with regard to where you can place it in the chain is really interesting . Clarity is fantastic , not just in that it looks good , but also in how it never seems to do @ @ @ @ @ @ @ @ @ @ for ( De ) Grain ? I know it incorporates very high-end temporal NR , but would it be reasonable to say it 's more " conventional " as opposed to Clarity , and therefore probably better to include as a final layer ? That might also make for a nice strategy for sweeping up anything that Clarity could n't get . <p> Adding Clarity in the Input FX is a great workflow and I find it really cleans up the image in such an amazing way , not to mention helping with pulling better keys and smoothing fine skin detail making the image look so pristine so effortlessly . <p> I do n't think anything out there can come even close to Nucoda 's Clarity . Not Neat and certainly not Resolves TNR <p> The first time you use it , you feel like you 've been cheated out of a better result on all of your prior projects . That being said , it 's not an overly dramatic effect , it 's just that wherever you apply it , it suddenly feels like the DP shot a @ @ @ @ @ @ @ @ @ @ Clarity is pretty great . And the way that the " Contrast " and " Bright " controls are geared are pretty awesome . I also feel like when I spin the wheels the color represents the way it does on a Lustre . It 's difficult to articulate with the limited time I had with it but I feel like the color response was more in tune with what I expect when I rotate the balls . I feel like I had to work a little harder in Resolve or Scratch to achieve the same results . I 'm not knocking Resolve or any of the other tools here , but Nucoda controls and FX tools really are nice . After a brief go with it I am seriously considering the purchase . On the topic , is there anyone that does training for Nucoda ? I feel like I would need a day to ask questions relatable to how Resolve works for some things that are n't quite as obvious or intuitive to feel like I could make the switch pretty seamlessly . <p> Mike you are right @ @ @ @ @ @ @ @ @ @ from brightness contrast with pivot and softness to Brightness regions to the unusual and awesome HLS tool which are so far above Resolves basic LGG tools . Not only is the gearing better and infinitely adjustable through the tangent mapper and through Nucoda itself but the color tools themselves just get you a better cleaner looking picture so much easier and faster . <p> Regarding training , DV offers it and the seasoned Nucoda colorists are very helpful too . Adam , Jake and Jack to name just a couple of guys . Patrick from DV is a great resource and a super nice guy . I think if you 're looking for a better grading platform Nucoda is the way to go ! <p> ICA offers Nucoda training , but frankly , I feel just getting your hands on it and just playing with it is a better use of time . Especially now , that you can request a temp license and just install it and run it . It 's just that easy . I even ran it for a while on a Mac under the Bootcamp @ @ @ @ @ @ @ @ @ @ it as an InputFX . It is so intelligent , that I were never able to see any kind of loss of resolution . If you closely examine the noise/grain structure of the image after applying Clarity , the image somehow acquires this smooth and orderly grain . It 's difficult to explain , but it just instantly cleans up the image , even if you thought it was clean to begin with . The best part , it 's completely automatic with zero need for interaction . I just apply it to the whole timeline and go to have coffee . If you then need to apply more NR , you still can . Some users report great results after applying two instances of Clarity . Also , as a side note , you can apply Clarity by using Loki Automation . You can render a shot to a watch folder and Nucoda will apply chosen effect in the background and will render using given codec , like Prores XQ 444 back to the desired folder . So , even with other software DVO tools are still available . @ @ @ @ @ @ @ @ @ @ DV the possibilities are endless ... <p> On the topic , is there anyone that does training for Nucoda ? I feel like I would need a day to ask questions relatable to how Resolve works for some things that are n't quite as obvious or intuitive to feel like I could make the switch pretty seamlessly . 43971 @qwx453971 <p> If you can get down to LA , I know Digital Vision has helped a number of people at their office in Hollywood . Beyond that , there 's always options like Skype or Teamviewer with one of us . In all honesty though , Jake is right . You can really get a long way on your own just experimenting . Pick a few tools to focus on initially , then learn conform and export . That should take about a day and will allow you to complete a project . Then as you have time , experiment with and add on any tools you discover that you like . The ones I use most are Brightness/ Contrast , Balance , and Color Curves ( which are n't @ @ @ @ @ @ @ @ @ @ LGG ) . <p> Clarity is pretty great . And the way that the " Contrast " and " Bright " controls are geared are pretty awesome . I also feel like when I spin the wheels the color represents the way it does on a Lustre . 43971 @qwx453971 <p> Brightness/ Contrast , and Color Curves behave and feel a lot like Lustre 's Log and Linear mode ( respectively ) . However , you can also use them both at the same time , whereas in Lustre you have to choose one or the other . You also have a lot of control over your image processing chain , so like we were discussing earlier , you can insert color tools , as well as DVOs and CMS ( color management ) almost anywhere you like . You can keep things really simple , then as soon as you 're ready , you can do some very sophisticated things with LUTs , matrices , transforms , etc . <p> Jack , for instance , is actually really inspiring in the way he combines tools together so that they @ @ @ @ @ @ @ @ @ @ interesting frameworks of tools focused around speed and repeatability . As for me , I try to stay super minimal by doing as much as I can in a primary grade using the Base layer , and then only adding secondaries when I absolutely have to . If I use more than three or four color layers that 's a lot . That being said each layer can contain up to 15-16 different color tools so there can be a lot going on in each layer , but the bottom line is you can be as complex or as basic as you like . <p> A two hour session focussing on answering user and potential user questions as well as giving an overview of my look development strategies , grade management and I 'm happy to do a fairly brief tour of the software for those that do n't know it . <p> It 'll be fully interactive so in theory you can either ask questions by talking or simply via the text chat . <p> I wo n't be able to record it due to some of the content @ @ @ @ @ @ @ @ @ @ there to experience it ! <p> As far as Clarity is concerned , it was designed to be relatively gentle with grain , we do n't want to eradicate it ( especially from film at a restoration scenario ) - sometimes especially for restoration , it is common to remove grain and then add it back at the end , part of the reason we have refined the DVO RegrainRGB tool to be more flexible . <p> As for where to use it ... depending on the source , and what you need to do it , placing it in the Input FX layer will very nicely clean up a digital source for easier manipulation and very good results , else , you may find that the grade itself may increase the noise in the image , and you can add it later . <p> A two hour session focussing on answering user and potential user questions as well as giving an overview of my look development strategies , grade management and I 'm happy to do a fairly brief tour of the software for those that do n't know @ @ @ @ @ @ @ @ @ @ theory you can either ask questions by talking or simply via the text chat . <p> I wo n't be able to record it due to some of the content I 'll be using so you 'll have to be there to experience it ! <p> Jack , that sounds great . Maybe we can help provide you with some footage that we can get permission to use so this session can be recorded ? It would be really helpful to have this be accessible at a later date . 
@@44332679 @4332679/ <h> XAVC artefacts <p> First of all I apologize for not posting any still frames here of the issue I 'm about to describe , but I think anyone that has experienced the same will recognize it . <p> Thing is I received an Avid DNxHD 120 MXF file exported from the Media Composer full of compression artefacts , especially in the RED channel . The original footage was XAVC HD coming from a Sony F55 , shot in Slog2 . I could n't believe how bad it looked so I tried to import ( or AMA link ) the camera files myself in the Avid but I immediately saw the same ' pink blocks ' dancing on the screen again . Especially when using the build-in Sony Slog2 lut it looked even worse . However when I looked at the exact same camera files in Resolve the artefacts were not visible anymore . <p> Has anyone seen this before ? I was a little bit shocked by how bad it looked . <p> I 've just finished a political campaign that used a couple of F55s . I @ @ @ @ @ @ @ @ @ @ the footage . XAVC HD had tons of compression artifacts . I had to use masks inside Resolve to improve green screen backgrounds just to help the CG department achieve decent keys . Shooting in RAW resulted in a different problem - extremely noisy footage . The first outdoor shots we got , shot under Rio 's very bright sun , looked as if they were shot with 18 db of gain ( or ISO 6400 ) . I did some research and found out that many DPs have to overexpose by several stops in order to get rid of the noise . So I did some tests with our DP and we discovered that it does indeed help a lot . <p> If you press the blue only button on your monitor you 'll be shocked by the size of the XAVC HD compression artifacts . <p> I was so shocked with the camera 's performance that I thought we had a faulty unit . But the second camera was just as bad . I wonder if the camera operators were setting them wrong . But there are lots @ @ @ @ @ @ @ @ @ @ , I 've heard that a lot of F55 owners are putting their cameras up for sale . <p> Since I ca n't yet accept that all F55s deliver such poor images , does anyone here have different experiences ? <p> I did a 55 show last year ... SUPER low light LONG ... and everything looked great ! Did another a few months ago and I will say there NOTICEABLY more noise . Not unfixable ... But I was a bit shocked . Could there have been a weird firmware upgrade somewhere in the middle that did it ? <p> To everyone who replied ; I 'm not just talking about the F55 footage , I 'm specifically talking about the Avid Plugin and what happens once you use it . The footage did not show those artefacts when directly imorted in Resolve . <p> To everyone who replied ; I 'm not just talking about the F55 footage , I 'm specifically talking about the Avid Plugin and what happens once you use it . The footage did not show those artifacts when directly imported in Resolve . @ @ @ @ @ @ @ @ @ @ Slog2 Sony lut applied . Watch the sky and see the pink stuff I was mentioning earlier in this thread . What kind of problem could this be ? Second is the same frame , with the same lut exported from resolve . <p> First a still exported from the Avid with a Slog2 Sony lut applied . Watch the sky and see the pink stuff I was mentioning earlier in this thread . What kind of problem could this be ? Second is the same frame , with the same lut exported from resolve . View attachment 2372View attachment 2373 43971 @qwx453971 <p> There 's a lot more banding on the Avid image as well . It seems to me that Resolve 's internal floating point processing is actually fixing some of the XAVC HD compression issues . Avid is probably working in 8 bit . <p> I did a 55 show last year ... SUPER low light LONG ... and everything looked great ! Did another a few months ago and I will say there NOTICEABLY more noise . Not unfixable ... But I was a bit shocked @ @ @ @ @ @ @ @ @ @ in the middle that did it ? 43971 @qwx453971 <p> This makes a lot of sense . A bad firmware could have caused the noise problems . I had heard nice things bout the F55 before and I was puzzled when I saw the extremely noisy footage . <p> Avid MC has a little video quality switch on the bottom of the timeline that needs to be set to ' 10 bit ' in order to not see all the artifacts in AMA link mode . <p> If you decide to transcode the xavc files you MUST transcode to a 10 bit DNxHD codec or you 'll bake in the same artifacts that were visible when viewing in 8 bit mode ( it 's not called 8 bit , it 's simply ' green ' ) . <p> Now for monitoring your new transcoded DNxHD 10 bit media you will still need to switch to 10 bit mode or you 'll still be watching a blocky mess . <p> I still think Avid should default to the highest quality , especially when results between 8 and 10 bit mode are @ @ @ @ @ @ @ @ @ @ how to view AMA by default , and likewise how to transcode ? I feel like in the AMA setting you can tell it best quality for both as default , then at that point any issues would be a result of your transcoding codec and not how you are viewing ... since the viewer allows for better playback performance ... <p> Ca n't you tell Avid how to view AMA by default , and likewise how to transcode ? I feel like in the AMA setting you can tell it best quality for both as default , then at that point any issues would be a result of your transcoding codec and not how you are viewing ... since the viewer allows for better playback performance ... 43971 @qwx453971 <p> All AMA settings were set to Highest Quality , but I think those settings refer to what file the AMA plugin will look at . For example , if a card coming out of there camera contains both HI and LO res versions of the media you can tell the AMA plugin which one to pick/ look at . @ @ @ @ @ @ @ @ @ @ had I can only conclude that it has to do around the difference between 8 bit and 10 bit in both viewing ( little green box in timeline ) and transcoding . If you transcode to DNxHD 120 ( 8bit ) the files will be full of harsh compression artifacts but if you choose to transcode to DNxHD 185x ( 10 bit ) it looks clean . One big problem now is that when you are using Avid Media Composer without any Video IO card ( Decklink , AJA , Avid Nitris etc etc ) you will NOT get the 10 bit viewing option , so you will never be able to see the footage free of these horrible artifacts and make the right judgements . And we all know there 's a trend in NLE workstation to not have proper monitoring and the necesary video IO anymore . <p> What I still find strange is that I 've been using Media Composer for decades in 8 bit viewing mode and did n't come across such ugly artifacts before , so my idea is now that this is n't just @ @ @ @ @ @ @ @ @ @ but also a very bad implementation of transcoding to 8 bit by the Sony PDZK-MA2 plugin . <p> I saw the artifacts in After Effects and in Final Cut Pro X. I believe that there 's more to it than just the 8 bit monitoring switch . In Resolve the artifacts are visible , too . Just active Blue Only on your monitor and you 'll see them . 
@@44332680 @4332680/ <h> DVO Tools Come to the Rescue <p> I just wanted to share my experience with everyone as to a problem , and solution , to a project I worked on recently . <p> A client of mine had a DIT issue where their raw rushes had been shot as frames alternatively to two different cards on an external recorder and unfortunately the part time DIT mis-diagnosed the situation as duplicate recordings and consequently only backed up half of each of the rushes ... Every other frame ! <p> You could normally get around this by using the camera proxies , that were recorded , however in this situation the delivery was 4K UHDTV , and was to be QC 'd the hell out of by the eventual end client 's lab . To make it clear , quality of the deliverable was of the highest priority . <p> To get around the ' problem ' I used Bulk Rename Utility on Windows to put the alternative frames back into sequential order . At this point I loaded in the footage into Digital Vision Nucoda and calculated the first @ @ @ @ @ @ @ @ @ @ . <p> I then used DVO Twister to retime the footage from 25fps to 50fps , essentially changing the playback speed to 50% using motion compensation to fill in every other frame . What I like about DVO Twister was the amount of options for motion compensation to help sort out issues with fast movement and blurring . <p> Considering the material contained everything from plaid shirts to highly detailed textures , all with movement , I was amazed at how well the software worked first time , and ultimately how well the problem shots improved by tweaking the settings . <p> Finally for any of the more detailed issues , I used DVO Dropout + Fix to either paint the whole frame or paint individual sections of the image . This works by using the previous and next frames to create an interpolated fill . <p> My client was thrilled and naturally it passed it 's grueling 4K tech review in the lab . To be fair , when we QC 'd it ourselves , we were struggling to work out which were the original frames and which were @ @ @ @ @ @ @ @ @ @ every other frame could go to two different cards ! " 43971 @qwx453971 <p> They might have used an external recorder like the Convergent Design Gemini . The GEMINI 4:4:4 recorder has two Slots for 1.8 " Solid State Drives ( SSD ) , 256GB / 512GB sizes . Depending on the resolution and frame rate , it splits the recorded media on the two drives if the data bandwidth is to big for just one . You then need to use the GEMINI CLIP MERGER software to reconstruct your media . 
@@44332681 @4332681/ <h> Kinetta scanner <p> I 've got some old 16mm print from the 50s that looks to have some warping and shrinking , it 's bouncing around on the capstan and gate of my spirit . I 'm looking for some leads on any facilities that might have one of these scanners to see if I can get a better scan . I 've emailed kinetta but have n't received a response so I thought I would ask the collective . I went looking for a telecine/scanning sub-forum but could n't find one , sign of the times I guess . <p> I forwarded the original query directly to Jeff Kreines , inventor of the Kinetta scanner . He was unaware of this forum and promptly joined . He then tried to reply to the thread but got a message saying he has insufficient posting privileges . Can one of the mods look into this for him ? Thanks . <p> I forwarded the original query directly to Jeff Kreines , inventor of the Kinetta scanner . He was unaware of this forum and promptly joined . He then @ @ @ @ @ @ @ @ @ @ saying he has insufficient posting privileges . Can one of the mods look into this for him ? Thanks . <p> Hello , everyone . Glad to discover a new forum devoted to grading , scanning , and restoration . <p> The Kinetta Archival Scanner can handle extremely shrunken and damaged ( including vinegar syndrome ) film . Of course , some film will need digital stabilization and other fixes post-scanning , but as long as you can capture each entire frame you can then stabilize it without any loss of image , which is a problem with scanners and telecines that do n't overscan . <p> if you go to the Kinetta website ( kinetta.com ) and go to the scans page , there is a scan of some very damaged early 1950 's Kodachrome with optical track of Walter Bach , inventor of the Auricon , demonstrating his new Pro-1200 camera poolside in the Hollywood Hills . The film had a lot of torn and missing perts , was severely cupped , and had VS . I ended up scanning it from tail to head , with no @ @ @ @ @ @ @ @ @ @ scan without having usable perfs . It was stabilized with one pass through Final Cut X and a clean-up pass through Resolve 9 . Audio extracted with AEO-Light . <p> There is n't currently a Kinetta in Australia ( we 're working on it ) but there are machines in 4 other continents . There is a new facility opening in NYC in July 2013 that would probably be your best bet for scanning this . Email me for particulars . <p> This is on CNN August 1st -- watch it in HD if you can . I scanned all of the Super-8 footage ( about 70% of the film ) at 3.3K . Mostly from original , but some from bad S8 contact prints , which will be obvious . I did n't grade it . But it looks quite good on a big big screen ( like Lincoln Center or MoMA ) . 
@@44332682 @4332682/ <h> Tape capture &amp; layback <p> Is anyone still doing this on a semi-regular basis ? What are you using ? We 've had very bad luck with AJA Control Room , Blackmagic Media Express , Resolve , and Premiere . Most people I ask either have Venice/Vantage systems that are way more than we need or still just use FCP7 , which has its own problems ... <p> Is anyone still doing this on a semi-regular basis ? What are you using ? We 've had very bad luck with AJA Control Room , Blackmagic Media Express , Resolve , and Premiere . Most people I ask either have Venice/Vantage systems that are way more than we need or still just use FCP7 , which has its own problems ... 43971 @qwx453971 <p> If you already have an Aja or Blackmagic card , you could try our MediaNXS capture product . It has crash and batch VTR control recording for Aja , BlueFish444 and BlackMagic cards . If you want to test it out , I 'd be happy to get you a trial key . <p> We @ @ @ @ @ @ @ @ @ @ SR deck still operational and pile of other stuff digi beta/beta decks ..... and some of those white ones ... umatic or something its like an AV museum in one of the corners of our MCR <p> the guys use AJA control room and they do n't have many issues ... i think they do fire up FCP7 every now and again but that 's just out of pure nostalgia <p> and one of them will say ... " do you remember when ........ " and get all misty eyed but what 's your problem with the software <p> edit : we have one of these as well if you ever need to make to a 60 , scfi movie <p> We do it with Media Composers , but I 've done capture and output from Resolve and Flame as well . Resolve seems less reliable for VTR I/O . I ca n't say for sure because Most of the tape oriented jobs go through Media Composer which is 100% solid . <p> FCP7 : Drops frames and timecode often becomes corrupted enough to be unfixable no matter how many times @ @ @ @ @ @ @ @ @ @ Media Express : Captures ProRes 444 with an absurdly high bitrate , wo n't open clips with 5.1 audio AJA Control Room/Premiere Pro CC : One of these was dropping frames and one of them offset TC by 1 frame on the capture ... ca n't recall which Resolve : Complains there 's no video *input* when you 're trying to *output* to tape <p> Some folks in addition to the responses here have suggested Avid in the last day or so , so I 'm excited to give that a try ; - ) <p> AJA Control Room/Premiere Pro CC : One of these was dropping frames and one of them offset TC by 1 frame on the capture ... ca n't recall which 43971 @qwx453971 <p> I have n't used Premiere in forever , and though we have Kona3 board , it 's been on the shelf for years since we 're primarily BMD . But I would imagine that Premiere has something similar to FCP 's Capture Presets , where you can specify an offset to compensate for this . Because different devices can cause some latency @ @ @ @ @ @ @ @ @ @ <p> When we used our old Teranex ( pre-BMD model ) , the lag in processing was something like 6 frames . So if you wanted to run something through that you had to use an FCP capture preset that corrected this . <p> FCP7 : Drops frames and timecode often becomes corrupted enough to be unfixable no matter how many times a clip is re-exported ( even through Premiere/Resolve ) Blackmagic Media Express : Captures ProRes 444 with an absurdly high bitrate , wo n't open clips with 5.1 audio AJA Control Room/Premiere Pro CC : One of these was dropping frames and one of them offset TC by 1 frame on the capture ... ca n't recall which Resolve : Complains there 's no video *input* when you 're trying to *output* to tape <p> Some folks in addition to the responses here have suggested Avid in the last day or so , so I 'm excited to give that a try ; - ) <p> Thanks for the responses , all ! 43971 @qwx453971 <p> i , m just going to mention this were are you capturing @ @ @ @ @ @ @ @ @ @ of the back off our capture macs and all that goes on those raids is the captured footage ... once it all done and check it then get shifted on to our SAN thing ... aka editshare <p> and the raids get cleaned out quite often so they do n't degrade <p> otherwise i remember we used to get dropped frames on capture .. things like that 
@@44332683 @4332683/ <h> ACES artifacts <p> Hello , I am getting some weird green color shifts in small areas of some images . I am using color temp control in Resolve and at certain color temperatures these green areas appear . Also strange is that it appears only on FSI monitor not on GUI computer monitors . Any help greatly appreciated . Michael Resolve 12.5.3 ACEScc 1.0.2 Windows 10 footage is from Red Dragon 6k <h> Attached Files : <p> It 's hard to judge accurately from a photo , but I would say those are not green artefacts , but rather they are monochromatic , but appear green due to the saturated magenta surrounding them . Does it appear that way on any SDI monitor ? Is it possible that there is an error in the calibration LUT in that monitor , which desaturates strong magentas ? Or is the SDI range of the Resolve output mismatched to the input range of the monitor ? <p> It 's hard to judge accurately from a photo , but I would say those are not green artefacts , but rather they are @ @ @ @ @ @ @ @ @ @ surrounding them . Does it appear that way on any SDI monitor ? Is it possible that there is an error in the calibration LUT in that monitor , which desaturates strong magentas ? Or is the SDI range of the Resolve output mismatched to the input range of the monitor ? 43971 @qwx453971 <p> I made a mask with a piece of paper ( hole ) and look at it .. they looks green alright . <p> I made a mask with a piece of paper ( hole ) and look at it .. they looks green alright . 43971 @qwx453971 <p> Zooming in and colour picking I 'm reading very low saturation , with a slightly cyan cast if anything . But I think the white balance of the camera photographing the screen is likely to be affecting it significantly , so it 's hard to say . But the artefact certainly looks like an abrupt desaturation . <p> Nick , You are correct . It is the calibration LUT that is causing the error . Thank you for helping me get to the bottom of this . @ @ @ @ @ @ @ @ @ @ image in a sequence with 100 other shots with very similar grades . Michael 
@@44332684 @4332684/ <p> I would bet Apple shut them down . Their patent is pretty tight , and their legal department is fierce and unrelenting on matters of patent infringement . Somebody ( I think Forbes ) claimed Apple files more than $100 million worth of patent-infringement cases every year worldwide , and wins quite a few of them . <p> I would bet Apple shut them down . Their patent is pretty tight , and their legal department is fierce and unrelenting on matters of patent infringement . Somebody ( I think Forbes ) claimed Apple files more than $100 million worth of patent-infringement cases every year worldwide , and wins quite a few of them . 43971 @qwx453971 <p> I 'm inclined to think the same way . Camera manufacturers should switch from ProRes to something else as robust as ProRes ( IMO ) . <p> I 'm inclined to think the same way . Camera manufacturers should switch from ProRes to something else as robust as ProRes ( IMO ) . 43971 @qwx453971 <p> I actually think ProRes is perfectly usable for cameras , and Apple readily will @ @ @ @ @ @ @ @ @ @ The problem is their operating system bias against Windows . AE , Premiere , Resolve , and quite a few other Windows software products ca n't export to ProRes . If I were Apple , I 'd make it a $99 plug-in or something , representing their lost sales in the OS and hardware , and lock it to the hardware . I 'd pay $99 ( maybe even $199 ) for authorized , foolproof ProRes on Windows . <p> I actually think ProRes is perfectly usable for cameras , and Apple readily will license manufacturers to record in ProRes on hardware products . The problem is their operating system bias against Windows . AE , Premiere , Resolve , and quite a few other Windows software products ca n't export to ProRes . If I were Apple , I 'd make it a $99 plug-in or something , representing their lost sales in the OS and hardware , and lock it to the hardware . I 'd pay $99 ( maybe even $199 ) for authorized , foolproof ProRes on Windows . 43971 @qwx453971 <p> What s also interesting @ @ @ @ @ @ @ @ @ @ an app for windows , but some ' niche ' systems are . <p> I use HDCinematics Convert v4 but Footage Studio ( also ffmpeg based ) claims to be able to ' spread ' 8bit source material out across the 10bit spectrum during the transcode which sounds interesting . I 've not tried this path but many I know swear it works better than a straight 8bit source to 10bit transcode . <p> with scratch you can and turn the license on and off when you need it its $79 you just need to give them a week notice not to be re billed 43971 @qwx453971 <p> Or just do your monthly payments with PayPal , in which case there is no automatic rebilling . This is what we do - when we need to renew it we do , when we do n't we do n't . Sometimes there are a few weeks where we do n't need it , so we 're not renewing . Scratch is a pretty great tool for this , and it can do multiple outputs simultaneously from the same Construct ( so @ @ @ @ @ @ @ @ @ @ of a 2k file , they 're rendered simultaneously rather than sequentially ) . The UI takes quite a bit of getting used to , but it 's a solid tool and makes reliably good ProRes files . <p> Or just do your monthly payments with PayPal , in which case there is no automatic rebilling . This is what we do - when we need to renew it we do , when we do n't we do n't . Sometimes there are a few weeks where we do n't need it , so we 're not renewing . Scratch is a pretty great tool for this , and it can do multiple outputs simultaneously from the same Construct ( so if you want to make an SD and HD version of a 2k file , they 're rendered simultaneously rather than sequentially ) . The UI takes quite a bit of getting used to , but it 's a solid tool and makes reliably good ProRes files . 43971 @qwx453971 <p> Indeed , and it 's much faster than ffmpeg . <p> Just as a side-note on the pricing @ @ @ @ @ @ @ @ @ @ 79$ - keep in mind , that it is indeed a *subscription* , so you -need- to cancel it , if you do n't want it anymore at some point . The annual license is 650$ and the permanent license ( which I ca n't really recommend anybody to buy , as it rarely makes sense anymore ) is 3k. 
@@44332686 @4332686/ <h> Autodesk " forces " subscription <p> I got a letter this mourning , and I saw the update to the website ( LONG ... ) ... basically an 20% increase on maintenance or an option of a forced license turn over . The 60% price break for the subscription does n't make sense for the first three years ( since I all ready have the perpetual license the subscription obviously should n't cost the same ) , and the price break is not fixed after 3 years so net net , it 's a loss for me . <p> I have one full suite of the autodesk line ( was hugely expensive at the time ) and another workstation license for maya - I 'm just going to freeze the shop at 2017 versions and plan on slowly dropping autodesk products from 2018 through 2020 . In the Cinematic VR space I just do n't have a lot of financial margin 's to deal with this kind of stuff . I btw have had to freeze my foundry products at 2017 . On the Avid side I did @ @ @ @ @ @ @ @ @ @ give up my perpetual licenses ( same with adobe ) . Autodesk has been very responsive to my needs in the past , it 's a bummer moving off of them . Some of the more pertenant lines from the FAQ : ... 1.3 Will these changes affect all Autodesk customers ? These changes will affect customers with Autodesk products on maintenance plans , but will not affect subscriptions to Autodesk products . ... to me this basically means all old accounts2.6 Can I continue to use my perpetual license software after switching my maintenance plan to a subscription ? Upon the commencement of your subscription date , you will no longer be able to continue using your perpetual license as the option to switch to subscription at this significant discount is conditional upon trading in your perpetual license/s on a maintenance plan for a new subscription . .... to me this basically means I loose the ability to use the software when I stop subscription4.1 What previous versions will I be able to use when I switch my subscription ? Previous Version Rights Eligibility List to ensure that you @ @ @ @ @ @ @ @ @ @ to switch to subscription . ---this link shows versions in 2015 still available , previous to that most things are not runable <p> 2.6 Can I continue to use my perpetual license software after switching my maintenance plan to a subscription ? Upon the commencement of your subscription date , you will no longer be able to continue using your perpetual license as the option to switch to subscription at this significant discount is conditional upon trading in your perpetual license/s on a maintenance plan for a new subscription . .... to me this basically means I loose the ability to use the software when I stop subscription <p> Did I get this totally wrong ? 43971 @qwx453971 <p> I think you did misunderstood this one . My read is this . If you have a perpetual license , you will be able to continue using it , but if you do n't sign up for subscription , later you will never be able to pay up for missing years and get back on the subscription wagon . That also means , that you ca n't sell it to someone @ @ @ @ @ @ @ @ @ @ So , once you 're off the wagon , you ca n't get back on ... <p> I think you did misunderstood this one . My read is this . If you have a perpetual license , you will be able to continue using it , but if you do n't sign up for subscription , later you will never be able to pay up for missing years and get back on the subscription wagon . That also means , that you ca n't sell it to someone , who 'd like to have the latest version . So , once you 're off the wagon , you ca n't get back on ... 43971 @qwx453971 <p> Hmm ... so I think this is my options then ( for my current perpetual licenses that I am paying maintenance on ) : <p> Option 1 - Pay 20% more on maintenance license per year ( this is DOA , this is basically forcing cutting over to subscription ) Option 2 - Stop paying maintenance license and freeze my versions at 2017 ( what I think I will be doing ) Option @ @ @ @ @ @ @ @ @ @ license Option 4 - try to sell license now ( but that 's hopeless since I 'm in mid post production , plus I need software access to redo shots on old versions for at least a few years ) 
@@44332687 @4332687/ <h> A YUV vs . RGB question <p> I am not sure whether I should post this here or in the Telecine section .. <p> Back when I started as an assistant to the telecine department , I often overheard the colorists state that there were certain colors or looks that could n't be reproduced on our system ( Shadow/Spirit and Pogle ) because it worked in YUV whereas Davinci was RGB . It might have been the other way around , and it would probably be more accurate to call it YCrCb , but the main claim remains . <p> However , fast forward ten years , I now have a better understanding of color science , I am questioning that statement . Is n't the actual color volume as described in Rec.709 the same , regardless of whether it is represented in RGB or YCrCb encoding ? Is there such a thing as " colors that ca n't be displayed in YCrCb/RGB " ? If I remember correctly , the colorist was referring to certain yellow hues as well as the cyans " taking a sharp turn @ @ @ @ @ @ @ @ @ @ <p> Not just digital . There are quite a lot of Y'CbCr combinations which map to RGB colours outside the unit cube , whether you are working analogue or digital . My test pattern generator has a function which shows this by making the RGB values turn red when they hit the outside of the gamut , as you move the Y'CbCr LONG ... <p> Back when I started as an assistant to the telecine department , I often overheard the colorists state that there were certain colors or looks that could n't be reproduced on our system ( Shadow/Spirit and Pogle ) because it worked in YUV whereas Davinci was RGB . It might have been the other way around , and it would probably be more accurate to call it YCrCb , but the main claim remains . 43971 @qwx453971 <p> We generally told our telecine clients in the 1980s the two toughest colors for us were " gold " ( if that was a color ) and brown . We could kind of tip-toe up what those colors represented , but it never looked on a monitor @ @ @ @ @ @ @ @ @ @ But it was n't a YUV/RGB thing . I think it was more of a display problem with the monitors . <p> I think we can get closer today with modern display technology in Rec709 colorspace , but they 're still challenging , at least to me . There 's a famous story about Barbra Streisand demanding a " golden look " for Yentl in the 1980s , and I recall the colorist on that job telling me his solution was to make everything " bat piss yellow . " I think different people have a different concept of what a " golden look " is in digital video . <p> It 's been my experience that " gold is green " rather than " gold is red " " going by client preferences . It seems counterintuitive . <p> I have n't been asked for " bat piss " , much to my dismay . 43971 @qwx453971 <p> My introduction to this was when National Geographic started doing a lot of televison . Their art department sent us the graphics bible . Turned out official National Geographic " gold @ @ @ @ @ @ @ @ @ @ days .... <p> Rec.709 ( or actually BT.709 ) has a well defined color gamut whether that is represented by RGB , YUV or whatever format is tangential to that . 43971 @qwx453971 <p> The whole Rec.709 gamut can be represented by both RGB and Y'CbCr . But a significant part of the Y'CbCr code space is outside the RGB gamut . As luma approaches black or white , the range of Cb and Cr that map to in gamut RGB gets smaller , reaching zero at black and white . Since black and white are achromatic , any non zero ( or 512 with digital offset ) chroma value is meaningless . <p> Sorry , my question was just worded in an exceptionally bad way . I do understand that both represent the BT.709 gamut . In a very simplified way , if we imagine the BT.709 volume as diamond shaped , the YCrCb code values are more cylindrical or cubic as you can have the same saturations all across the luminance axis ? <p> This is an oversimplification , but imagine the RGB space as the cube with @ @ @ @ @ @ @ @ @ @ Overlaying the Y'CbCr cube on that , the Y ' axis is the diagonal axis of the RGB cube , from the black corner to the white corner . In fact also because Y ' puts black at 64 and white at 940 , the Y ' = 1023 face and the Y ' =0 faces of the Y'CbCr cube sit slightly beyond the points of the RGB cube . Imagine how much of the Y'CbCr cube sits outside the RGB cube , particularly near the black and while corners of the RGB cube . <p> In fact it is more complex , as the achromatic diagonal of the RGB cube is not linearly related to Y ' . Y ' is a weighted linear sum of RGB , and the length of the diagonal is a root of the sum of the squares ( remember Pythagoras from school ? ) so the reality is distorted compared to my simplified example . 
@@44332688 @4332688/ <h> make broadcast safe/legal in AVID or Resolve <p> Hello , so here-s my question : Some of the values in my timeline are still illegal . In a perfect world I would try a shot by shot fix but that-s just not doable in the time I have . I could use the resolve broadcast safe-but not sure how " safe " i should go ( -20/-120 ... -10/-110 ? ) The final playout will be done through Avid and not by us inhouse . I could just rely on Avid-s legalizer or activate resolves broadcast safe-what would be the better choice in your opinion ? <p> I-m afraid I don-t have any influence on what legalizer would be used in Avid as it-s not inhouse . I assume they will simply use the built in . But maybe i can find out . But i-m sure they don-t use any hardware legalizer <p> Hello , so here-s my question : Some of the values in my timeline are still illegal . In a perfect world I would try a shot by shot fix but that-s just not @ @ @ @ @ @ @ @ @ @ the resolve broadcast safe-but not sure how " safe " i should go ( -20/-120 ... -10/-110 ? ) The final playout will be done through Avid and not by us inhouse . I could just rely on Avid-s legalizer or activate resolves broadcast safe-what would be the better choice in your opinion ? 43971 @qwx453971 <p> I would always keep luma between 0-100% , chroma can be over and under just a bit ( like up to 20% ) . But making sure you stay within gamut is just as important for color . <p> If Rec 709 in camera footage is outside of legal ( if it does it usually comes in the form of 16-255 ) you could use a timeline node to compress the highs as a starting point . Dynamic log shots obviously are almost always beyond legal when trying to map onto Rec 709 or some other format so you need manual intervention . <p> Not sure about your time argument , assuming you color correct each shot getting it within range should take no more than a few seconds . <p> Do n't @ @ @ @ @ @ @ @ @ @ show up on a Waveform monitor or a Vectorscope -- for that you 'll need a Lightning display or something like it . As Robbie Carman shows in the illustration below , Scopebox 's Channel Plot GxB and GxR can give you a similar display and show you when you have an RGB gamut problem : <p> On the rare occasions I 've had something kicked back , usually it was a very brief chroma excursion that did it . These are not always fixable with just clips alone , and if you did clip them , it 'd kill the whole signal . I 'd rather go in and just bring down the chroma in that one shot manually , rather than really lean on the entire show or the entire sequence . <p> One way I wish Scopebox would improve is to provide a " record overshoots " function that actually created a file with timecode that showed whenever a given show playing back had any QC issues like excessive chroma or excessive luminance or gamut issues . Note there are still QC issues that can get you @ @ @ @ @ @ @ @ @ @ which they feel could cause epileptic siezures ) , or Safe Title problems , or a bad edit , and none of these can be fixed with scopes . <p> thanks for all the input , luma is fine 99.9% of the timeline I would say . I was worried about the chroma overshoots . We-re currently finishing 13x10 minutes 3d animation , a lot of shots overall . the look is very vibrant with intense colors on the characters and enviroments . <p> I have the vidcheck trial for another 14 days.It tells me that i have mostly problems in the darker areas - for example <p> RGB level limits : 16-235 - 5% The above limits were violated for 23 frames within the alert duration . The worst violation was 1.04% of screen area at 00:08:26:24 . <p> Chroma level limits : Y : 16-235 ( tolerances : -1% and +3% ) , UV : 16-240 The above limits were violated for 60 frames within the alert duration . The worst violation was 1.03% of screen area at 00:02:26:08 . <p> when i use broadcast safe in resolve @ @ @ @ @ @ @ @ @ @ , less when set to -10/-110 or no change at all with -20/-120 . I was just wondering if avids legalizer could handle those chroma issues better than resolves- broadcast safe or if it doesn-t matter which legalizer we would apply . <p> It 's worth checking with your broadcaster as to the actual specifications in any technical documents they publish . Many of these specs allow for bit of tolerance , especially with small transient overshoots , which are difficult to completely eradicate . For instance , here in the UK , the BBC specs are as follows ... <p> 2.3.2 Tolerance of out of gamut signals In practice it is difficult to avoid generating signals slightly outside this range , and it is considered reasonable to allow a small tolerance , which has been defined as follows under EBU Rec103 : RGB components must be between -5 % and 105% ( -35 and 735mV ) and Luminance ( Y ) must be between -1% and 103% ( -7mV and 721mV ) Slight transient overshoots and undershoots may be filtered out before measuring , and an error will only @ @ @ @ @ @ @ @ @ @ least 1% of picture area . Many monitoring devices are designed to detect errors to this specification . <p> alright , seems with the help of vidcheck it-s pretty fast and simple to isolate the offending shots.reduced a bit the most offending saturaion value in these cases and voila-no more errors on the first episode just a shame vidcheck is so expensive . I have to say , it seems almost impossible to me to find these errors by eye or scope . i guess I would need a loooot more experience <p> scope box could , nt see ( marc record over shoots would be perfect ) eyeheight legalize plugin would , nt get it either try as we might with resolve and soft clip and ( the results looked odd ) color finesse soft clip legalizer ......... no FCPX broadcast legal etc ... useless <p> we were under bit of time pressure for delivery it was probably just a few frames ... but it really had to spot <p> good to know - the machine with my scopebox has fcp7 installed . Hope i won-t need to go @ @ @ @ @ @ @ @ @ @ go back to fix noisy renders , spurious pixel , smearing artefacts , visible masks and what not : / Glad that it-s not for the BBC 
@@44332689 @4332689/ <h> Calibration on a budget <p> Ok , so as the subject heading suggests , I 'm on a budget and need to get at least a ball park level of calibration . In a perfect world I would purchase an Ezio or Flanders ref monitor but alas , this is something i ca n't do at this moment . <p> So , would I be better off throwing some money at a decent calibration probe ( X-Rite i1 ) with a lower end monitor or spend my budget on a better monitor ? <p> I know neither is an ideal solution for reliable colour accuracy , but any advice/recommendations greatly appreciated . <p> Get the i1d3 , learn how to use ArgyllCMS and DispcalGUI and use it on your monitor . You might be surprised how good ( or not good ) you can get your monitor to behave . <p> When you 're too cash strapped for the good stuff , you have to learn to develop a confidence in what you can do for yourself without fancy gear . When you can say " I know @ @ @ @ @ @ @ @ @ @ , " Then it 's easier to deal with . The high end clients who would scoff at your crappy gear are n't going to hire you anyway because of your lack of experience ( which I assume because you 're asking this question ) . If they are novices and scoffing at your gear , then they are likely not worth working with . Most of the clients I had when starting out ( and many today ) just wanted their picture to look better than it did before , and they were happy . Nobody is going to sue you for gamut excursions on a web video . <p> Thanks chaps - have worked in a post house for too long . Tech guys have taken care of all the calibration and I have just graded . Life in the freelance world requires a few areas of upskilling . Much appreciated .. 
@@44332690 @4332690/ <h> RED grading in Resolve Live <p> Going to do some live grading soon with Resolve live with a RED camera . ( not sure yet wich one , not helium though ) . Need to produce 4 LUTS , for 4 different episodes . Should be simple looks with primaries adjustement . Looks will probably be uploaded to an HD link behind the monitor during the shoot . Planning on exporting . cube LUTS directly from resolve . <p> I 'll have a simple setup : Macbook pro with BM Ultrastudio Mini recorder for in signal , and mini monitor output to an FSI monitor . <p> Never used Resolve Live and maybe some of you guys have already used this setup . Any advice on some settings worth knowing ? Except matching Timeline Color Space with the settings of the camera ( REDcolor4/REDgamma4 for exemple ) . is there any other things I should be aware with Resolve Live <p> On my system , there is an input lag of at least one second , this can trip the director up if they are directing from the @ @ @ @ @ @ @ @ @ @ frames will probably be dropped from time to time , so do n't count on using the output to record dailies . <p> You used to need to have the camera timecode running in free run mode at all times to get a picture , I do n't know if that is still a requirement . <p> Your computer will be working very hard . If you are running from a laptop , it might overheat . <p> You might honestly have a better experience with Pomfort LiveGrade , and the HDLink only basic version is fairly cheap , $99 for two months . <p> Sure , the HDLink is n't fully color accurate . Unless the monitor will be in a tent , neither is on set monitoring in general because of visual influence from the environment . There is a big thread on the LightSpace forums somewhere that goes into detail of it 's problems if you are interested . <p> As Nick says above , on set monitoring is far from an ideal controlled environment . While the HDLink may have some issues , the chances of @ @ @ @ @ @ @ @ @ @ primaries " is pretty small in my opinion . I ( and many others ) have used HDLinks on set on big feature films without issue . It is only a rough preview of the look at that stage , after all . <p> If you are saving the looks as DRX files , as well as LUTs , for finishing in Resolve the you obviously need to work with a Resolve . Otherwise you may also want to look at the public beta of FilmLight 's Prelight as well . <p> . While the HDLink may have some issues , the chances of their being noticeable on set with " simple looks with primaries " is pretty small in my opinion . I ( and many others ) have used HDLinks on set on big feature films without issue . It is only a rough preview of the look at that stage , after all . 43971 @qwx453971 <p> Yeah I was aware of the color issues with the HDlink , but since this is just for a " general idea " of a look to be shown for @ @ @ @ @ @ @ @ @ @ . I doubt that the production would want to rent a BoxIO or even buy it for the shoot , as no DIT will be on set during the shoot . We do these looks during camera test . <p> Yes , the HDLink Pro is not colour accurate . If you are using a FSI monitor of the BM , CM or DM series , why not load your LUTs directly into the monitor ? A great feature . 43971 @qwx453971 <p> Great idea ! Was completely forgetting this feature . I 'll ask the production what monitor they will be using , and try to propose this option for the actual shoot . <p> As for Resolve Live , I went for this option to not have to get a license of Livegrade for half a day , but I know that for a long shoot , and for a better DIT job , this would be the way to go . I 'll just try not to overheat my laptop with resolve Live <p> As for Resolve Live , I went for this option to not have @ @ @ @ @ @ @ @ @ @ , but I know that for a long shoot , and for a better DIT job , this would be the way to go . I 'll just try not to overheat my laptop with resolve Live 43971 @qwx453971 <p> The basic version of Prelight will be free on release . The public beta has the full functionality now at no cost . I thoroughly recommend people try it out . <p> Prelight can use a live signal from e.g. A Mini Recorder , and output via e.g. a Mini Monitor , but will have some latency . Or it can drive a LUT box . Think of it as LiveGrade combined with Resolve Live . <p> I 'm have been a Prelight beta tester for almost two months . The current version of the application , ( 4.4m1 ) does n't support BMD Mini Recorder nor the Mini Monitor . In a report I sent to FilmLight 's Support Team on the subject , January 9th , James Milne wrote that hopefully the next build of Prelight is going to support these two devices . Meanwhile , as an @ @ @ @ @ @ @ @ @ @ Io4K , IoXT can be used . The UltraStudio Express is not a full-duplex , so it only supports input and any one time , not both . I was told . <p> Please take into account that the at the moment of this writing BMD does n't has firmware support for the HDLink Pro in MacOS 11 nor Sierra . In order to use the HDLink Pro the user needs to stay in MacOS 10.10.5 I bought one three weeks ago and returned to B&amp;H in NYC one weeks after . Currently , I 'm using the FSI BoxI/O with LightGrade 3 and Prelight . <p> I 'm have been a Prelight beta tester for almost two months . The current version of the application , ( 4.4m1 ) does n't support BMD Mini Recorder nor the Mini Monitor . In a report I sent to FilmLight 's Support Team on the subject , January 9th , James Milne wrote that hopefully the next build of Prelight is going to support these two devices . 43971 @qwx453971 <p> Sorry . I guess James must have removed support while he @ @ @ @ @ @ @ @ @ @ Mini Monitor and Mini Recorder with earlier betas . I have n't had time to test the more recent builds of Prelight . <p> Sorry . I guess James must have removed support while he worked on improving the performance . I certainly used a Mini Monitor and Mini Recorder with earlier betas . I have n't had time to test the more recent builds of Prelight. 43971 @qwx453971 <p> Nick , thanks for your response . James said that registered beta testers of Prelight are going to receive a notification by the time the support for the BMD Mini Recorder and Monitor is ready . 
@@44332691 @4332691/ <p> Very cool . It will be interesting to see how this evolves . There does n't appear to be anything uploaded yet . If that 's the case it might be good to upload a few samples so its more evident how your site will look and work . <p> LUTs ( at least for film/video ) are really dependent on a known input in order to be useful . In video , for instance , colorspace , gamma , and often , peak light level would be needed for someone other than the creator to make use of it . <p> There are also other standards that might be good to specify , like whether you 'll accept anything other than . cube format ( .3dl , . lut , . cms ) , and minimum number of points for a 3D lut ( 17 , 33 , 65 , etc ) . In many cases the post-production world is moving toward color transforms ( . ctf ) so while you may not be able to support them now , that might be good to consider for @ @ @ @ @ @ @ @ @ @ . One of the problems on this product is discoverability of what is available . Actually there some lut samples already uploaded . You can find them at the second part of the main screen . Or you can find some other samples on my profile page https : **36;604;TOOLONG . <p> Currently there is no support of different color spaces , gammas etc . I consider adding custom curves on import or export according to what user selected . I would recommend using tags to separate different LUTs and provide correct samples with flatcolors . But I need to think more about this topic . <p> Currently app could import 3d lut in cube , 3dl or HALD format with any reasonable size . It could be some issues with big luts as we try to apply luts to pictures in your browser real-time . <p> Thank you for pointing at . ctf . It looks carefully designed . Is it getting traction in industry ? 
@@44332692 @4332692/ <p> someone knows a OS software to fastly scroll into a list of LUT and screening the result ? <p> We got 4Go of LUTs but explore them with Davinci is a pain . 43971 @qwx453971 <p> I agree . I would love something like that . Especially if it showed you basic lut info and an RGB curve as you scrolled through each one . <p> To me the best bet would be for Lattice to add this functionality , as it already has everything else , it just needs to offer more of a gallery view , rather than simply opening each lut in its own window . <p> Vey well played Walter . Pretend , that you do n't use LUTs , when in reality we all know , that by using those secret LUTs all top Hollywood colorists achieve the blockbuster looks . Let 's face it , without those special LUTs , none of the top colorists would last a day <p> Vey well played Walter . Pretend , that you do n't use LUTs , when in reality we all know , that @ @ @ @ @ @ @ @ @ @ the blockbuster looks . Let 's face it , without those special LUTs , none of the top colorists would last a day 43971 @qwx453971 <p> Shhhhh ! ! ! ! ! All the stills guys already think it 's the holy grail secret haha . Time for the next generation of instagram colorists . <p> i must admit , i like luts . Not for applying them ( with some rare exceptions ) but for quickly scanning all kinds of looks on the actual footage to see what works , get inspired and then just grade in that direction . Took me only an hour or so on a rainy day to put all luts on categorised powerbins an by selecting several at the same time , activate split screen with selected stills mode you have a full carousel on you external monitor . So not sure yet what this new magic solutiion wil bring that is not already there but let 's wait and see . <p> i must admit , i like luts . Not for applying them ( with some rare exceptions ) but for quickly scanning @ @ @ @ @ @ @ @ @ @ what works , get inspired and then just grade in that direction . Took me only an hour or so on a rainy day to put all luts on categorised powerbins an by selecting several at the same time , activate split screen with selected stills mode you have a full carousel on you external monitor . So not sure yet what this new magic solutiion wil bring that is not already there but let 's wait and see . 43971 @qwx453971 <p> I did the same and it 's really paying of . I love those powerbins . I have now also started to create them per tv show so if i did something in a different show that i would love to try it 's already there , ready to ' click ' . Another great plus of powerbins is when you double click a still it does automatically ' append grade ' which is great for look management . <p> btw talking about powergrade albums , do you know any way to sort/reorder them . Ca n't find anything anywhere . Stuck in creation order seems like it @ @ @ @ @ @ @ @ @ @ , maybe i just missed the obvious . <p> edit : looking at db sofar its seems hardcoded . I could change the order by rearranging the DBindex field values in table Gallery:GyViewSMUser and works for me , but obviously this should be possible from the gui for non retarded folks like me that go hack their db . <p> btw talking about powergrade albums , do you know any way to sort/reorder them . Ca n't find anything anywhere . Stuck in creation order seems like it . Before i go and hack directly in the database , maybe i just missed the obvious . <p> edit : looking at db sofar its seems hardcoded . I could change the order by rearranging the DBindex field values in table Gallery:GyViewSMUser and works for me , but obviously this should be possible from the gui for non retarded folks like me that go hack their db. 43971 @qwx453971 <p> Yes , agreed , a simple order sort to name/date/shot number would be great . Being less retarded and slightly less smart than you I just drag them in another bin @ @ @ @ @ @ @ @ @ @ someone knows a OS software to fastly scroll into a list of LUT and screening the result ? 43971 @qwx453971 <p> I use BMD Fusion . Just a mouse moving of video and cube file to the program window . Unfortunately , there is no way to simple walking for all luts in a folder . Every lut requires mouse dragging and one click to confirmation . But Fusion allows very easy ways to compare different results in very different ways , so it is a plus . 
@@44332693 @4332693/ <p> Technicolor Vancouver is currently seeking applicants for a fulltime Dailies Colorist to service episodic television series and feature film projects . Candidates must have a minimum of 5 years ' experience in a similar role whether onset/near set and/or in a facility environment . The Colorist must be organized and detail oriented and able to multi-task in order to manage workload and meet deadlines . <p> Reporting to the Director of Operations , the colourist is responsible for hands-on operation of dailies colour correction suites and related facilities to conduct the efficient processing of dailies media . The position requires a high degree of technical skill and artistic judgement . In addition , the colourist serves as a key front-line representative of the company and frequently deals directly with clients in challenging situations ; accordingly , a professional demeanour and diplomacy are also important . Duties include : <p> Operate the colour correction suites and related technical facilities ( workstations , routing switcher , etc ) as assigned by the scheduler and project coordinators to ensure efficient and accurate completion of assigned tasks . <p> Responsible for all @ @ @ @ @ @ @ @ @ @ , audio syncing , color correction/lut/look application , through to rendering and delivery of material . <p> Liaise with clients as necessary before and after sessions to ensure customer satisfaction with the work done . <p> Personally ensure the technical accuracy and quality of all work produced in the transfer session . <p> Be responsible for pre- and post " session tasks including materials preparation , session paperwork including project bibles and work orders , etc . <p> Related duties as assigned by supervisor.We thank all candidates in advance for applying but only those selected will be contacted for an interview . <p> There 's a show that needs someone to fill in next week , but you will need to have the chops ... <p> for dailes colorist the likely list would include Technicolor Deluxe 24Frames Finalie SkyLab <p> there 's a few more smaller places like mine , but none of us are likely to be taking on shows that call for a dailies colorist <p> Stop by sometime , i 'm on Granville Island ... 43971 @qwx453971 <p> Hi , <p> Thank you so much for the @ @ @ @ @ @ @ @ @ @ as a DiT . However , besides having over 8 years of experience as a VFX lighting/compositing artist , I worked as a full-time colorist/offline editor on TV shows in Sydney and freelanced on TVCs , docos , shorts etc . <p> I would definitely be interested in small companies that are doing color grading in Van . I would love to stop by and chat and greatly appreciate an advice ! 
@@44332694 @4332694/ <h> Reading color values from image into text file <p> For LUT development I 've been using the ancient Shake 's PixelAnalyzer() to analyze many color patches from image sequences . I would then export a text file from it which my program can read and parse all the relevant numbers into float vectors . <p> With one of the last OSX updates I finally could n't get Shake to run anymore . Because of this , and because the whole process was hack-y to begin with , I 'm looking for an alternative . Namely a tool to read pixel values at given X , Y from TIFF or DPX or EXR and store it in a text file . A C++ library would be the next best thing . <p> Would it help to define X , Y locations form a screenshot where RGB values where added into a table when you press a button . Hence you could import different file formats into an editor or retouching program which could automatically scale it ( it would be able to use always relative position ) . How @ @ @ @ @ @ @ @ @ @ <p> It would be hence a group of color pickers with fixed positions ( pre-chosen by mouse ) and output per button-click ( or even a timer ) into a txt-file . <p> I would expect that it can do all the things you describe , but how quickly to develop is another question . <p> RGB values into a table can be done fairly quickly - the " Jitter Matrix " can parse and write values into a plain text file in any syntax you like . Can also make the image sample area definable and graphically controllable ( or hardware controllable ) . <p> I 'd be interested to prototype something for you , but free time is always the problem ! <p> Did develop this little tool - memPICKER . You are able to track screen changes for up to 50 individual points ( it 's shown here for 6 points ) . Maybe I 'm ready to publish that at the weekend for free ( Mac and Win ) . Do n't know how it works on a 10bit screen ... <p> Data can be stored @ @ @ @ @ @ @ @ @ @ easy for import in Excel etc. for further processing . <p> Would appreciate some feedback if it works well on your ( Win or Mac ) platform . <p> memPICKER is a pixel picker tool useful for color correction , color grading and color analysis . memPICKER is able to track screen changes with up to 50 individual points and save them into a data file . Download memPICKER as Donationware for OSX and Windows platforms . 
@@44332695 @4332695/ <h> Premiere to Resolve Resizing and Workflow Headache <p> First of all , I searched a lot this and several websites . Sorry I have to make another topic about it . <p> I 've been color correcting a project with multi camera/frame rates and it 's been a long run . After all my efforts and learnings I am now cornered by the workflow on two different levels . It 's been very frustrating and I did a lot of re-dos . Now I really need some advise , as all suggested workflows do n't seem to work and my time is running out . <p> Hope it 's not too confusing , as I have two different problems . My workflow consists into editing the proxy in premiere , grading in resolve and reimporting graded media into premiere for effects , titles , etc .. <p> First I imported the media into Resolve , changed all frame rates to 23.976 and exported the proxy . The sizing option is set to Center Crop With no Rescale so I could match the sizing information coming back from Premiere @ @ @ @ @ @ @ @ @ @ and resized. 4k clips were rescaled to around 50% , cropping a little the side borders . 5D RAW 47.952 clips were rescaled by 120% and then stretched by 1.67% . Everything was perfect on a 1920x1080 23.976 fps timeline . <p> When I imported the XML into Resolve , as said in the LOG.txt when exported by premiere , the rescale information was OK , but not the stretching on the 47.976 footage neither the horizontal flip . I tried two different ways , Scale with " Uniform Scale " unchecked , so I could stretch it and with " Transform &gt; Scale Height " . Both ways did n't work , so I manually stretched into resolve Edit page . <p> He made several cross dissolves with specific custom alignments . When I import the XML into Resolve , the alignments are lost as he only understands duration . They are also lost when I reimport it on Premiere , I do n't know why the XML is not preserved . <p> The solution would be exporting Masters and relinking into Premiere , but I did different gradings @ @ @ @ @ @ @ @ @ @ my grading . <p> When I manually relink exported media ( not the masters ) into the original Premiere timeline , it works perfectly with the ' ins and outs ' , but it 's again a big problem when the same clip is used more than once . This would be solved if I manually unlink and relink the file , but it would take too long as the project is very big . Also , when doing this way , my resizing into Resolve adds up to the resizing by premiere and I would have to carefully take off the effects so it remains OK . This would also take too long . I could research a sizing only flat pass on resolve deliver , but this would not solve my relink problem into the original timeline . <p> There seems not to be a way to manually copy all the cross dissolves from original timeline into the imported one , as they are all different from each other . <p> Have n't tried it with mixed frame rate ( so you may want to do a quick @ @ @ @ @ @ @ @ @ @ do n't mind losing your raw controls you can use render and replace to get unique file names for each cut . <p> I unlink all the audio and video tracks , render and replace all the clips with appropriate handles , then export out an XML . Import into resolve , conform and grade to taste then export individual clips at source resolution , without sizing information , with same clip names , handles and file type . Then either make a backup of the render and replace folder and replace the original folder with the graded files so it auto links , or just offline all the files in premiere and relink . <p> I 've been having similar issues with a project recently . I have a choice of using all the sizing data from the edit through the XML - some of which is useful but lots of it is n't useful because Premiere Pro sets the Scale to 50% with 4k clips on the HD timeline - or tell Resolve to ignore the edit sizing and scale everything to fit . The latter seems the @ @ @ @ @ @ @ @ @ @ try to find a best-of-both-worlds setting if I could <p> some of which is useful but lots of it is n't useful because Premiere set Scale to 50% with 4k clips on the HD timeline 43971 @qwx453971 <p> In your case , " Center Crop With no Rescale " would solve the 50% scale problem , as it is how Premiere interpret all sizing into its timeline . <p> The problem with fit , in my case , is that not everything is set to fit . Some footage is 51% , 52% instead of 50% . Also I have some digital zoom-in from premiere , I guess sizing information is needed for that . Even if I could use fit , the image would stretch twice when going back to premiere , using the exported footage by the XML from resolve on the original premiere timeline . Anyway this option does n't seem good as I can not link two different cuts from the same clip . I could use the " replace footage with " , but this would take too long to get into the correct timing @ @ @ @ @ @ @ @ @ @ media offline and online again has to be one by one , because when I use auto relink it comes all back to original link but the one I chose first . <p> I 'm guessing I 'll have to export a XML from Resolve and copy/paste each transition , one by one ... Seems to be the best way , as the above would take too long ( maybe someone has a good solution for it ) and exporting the masters would mess up any two different corrections from two cuts of the same clip . <p> I 'm guessing I 'll have to export a XML from Resolve and copy/paste each transition , one by one ... Seems to be the best way , as the above would take too long ( maybe someone has a good solution for it ) and exporting the masters would mess up any two different corrections from two cuts of the same clip . 43971 @qwx453971 <p> That 's basically true . Fractional sizes and fractional speed changes result in even more unexpected problems . I had a strange foreign commercial a @ @ @ @ @ @ @ @ @ @ from Hell " thread ) where the director/editor decided to make every speed an unusual change , like 22.592fps or 27.939fps , and it was very dicey . Pretty much every single cut had to be eyematched , which was no fun . That was a case where five : 30 second commercials took 6 hours to conform and maybe 3 hours to grade . <p> But I also had an Avid project not too long ago where all the resizes -- even dynamic resizes with moves -- came through fine , and the speed changes were very close . There were issues where Resolve optical flow speed changes resulted in a different first frame or different last frame than the client 's reference , but I warned them that 's just the way it is in varispeed , where frame blending changes depending on whether it 's FCP , Avid , Premiere , Resolve , or ( god forbid ) videotape . <p> But I also had an Avid project not too long ago where all the resizes -- even dynamic resizes with moves -- came through fine , @ @ @ @ @ @ @ @ @ @ issues where Resolve optical flow speed changes resulted in a different first frame or different last frame than the client 's reference , but I warned them that 's just the way it is in varispeed , where frame blending changes depending on whether it 's FCP , Avid , Premiere , Resolve , or ( god forbid ) videotape. 43971 @qwx453971 <p> I have been running into this exact same issue . It was particularly a problem because it was a music festival and it made the first frame a light flash that was n't there before . <p> To the general topic , this has been something I have been struggling a lot with . My solution has been to just buckle down and do it the old fashion way . I found that trying a bunch of different ways to make the process faster , I ended up spending more time that I would if I just exported a reference video from Premiere and went through the entire timeline an conformed it by hand . It ends up being the only way to have 100% confidence in @ @ @ @ @ @ @ @ @ @ than checking with a client verified reference before grading and before render . I have the benefit of getting the majority of my work through a close partnership with a DP and Editor , so there are certain things that DPs and editors would normally do prior to color that they expect to carry through that the guys I work with wait to do in a session with me . <p> What has worked really well for me recently is that I do a very detailed conform in Resolve using a reference and then any additional changes that need to be made are either 1 ) done in a new dedicated track that is brought in as XML , laid over the existing timeline as a nested sequence where it starts , and then decomposed in place or 2 ) Shifts of timing are notated with markers and a reference video is used to recreate the changes in Resolve . <p> While it seems like more work , doing it the long and hard way ends up being much less stressful . Part of the reason many colorists never use @ @ @ @ @ @ @ @ @ @ changing a clip without realizing it . <p> While it seems like more work , doing it the long and hard way ends up being much less stressful . Part of the reason many colorists never use linked grades , so they never have to worry about changing a clip without realizing it . 43971 @qwx453971 <p> That 's my experience as well . Having a reference video is an absolute must for conforming , too . <p> Wanted to give an update to this thread , with my final solution and a discovery on XML translations between Premiere and Resolve : <p> First , exporting a XML from Resolve back to Premiere was actually the best way to achieve workflow success . My issue with cross dissolve was actually a misunderstanding of the tool . When importing cross dissolve from Premiere , with a different alignment than the center , Resolve changes the length of the cut so the alignment is back center and still matches the visual dissolve made in Premiere . So , instead of making alignment into the cross dissolve , he put that alignment into @ @ @ @ @ @ @ @ @ @ was such a relief . <p> My only issue was into one clip , with two cross dissolves touching each other , filling the whole clip , that Premiere oddly did n't recognise the handles into the cut ( they were grayed out as it did n't had more frames back of forth , but it actually did into the same imported clip from the bin ) , so I had to get the imported clip in the bin , substitute the timeline one and match again the offline reference clip . <p> So , at the end , without all the learning , frustrations and rework , I had to to reconform some issues on Resolve ( only achieved by looking at an approved offline reference clip , as pointed by Marc and Nicholas too ) , redo some things that were not translated into XML as size and horizontal flips ( very important to look at XML Report that Premiere makes from XML Export ) and even less back to Premiere . <p> That way , If I had to make any change on the edit , I @ @ @ @ @ @ @ @ @ @ with 120 handles , and if I had to make any change into color I 'd do into Resolve and export the clip with the same number of handles as before if I wanted to maintain the sync on Premiere , and replace " physically " the footage into the folder ( Not from premiere , he does n't know I replaced the footage ) . <p> I 'll still try to look what could I do to easier things into resolve conform ( some odd mismatched timings from few footages ) and back into Premiere as said above , but that 's really just preciousism . I guess it 'll remain as part of our mysterious nature . 
@@44332696 @4332696/ <h> Resolve Control Surface red light <p> does any of you know of a way to dim down the keyboard lights on the control surface ? or even better , is it possible to change the color of the light from red to anything else ? it really messes up with your eyes after a couple of hours . <p> we recently bought the full davinci resolve with control surface and it came without a manual . also , even though i ordered a mac pro for resolve , the sliding keyboard on the control surface is for windows . when i asked , the dealer said that this is the only one available world wide . <p> In Project Settings , under Control Panel , you have the option to set numerical values to LCD brightness , Red , Green and Blue light , you can run any combination on those to create the color you want . I also created a project called " Lights Out " where I set those values to 0 so I do n't have it on all night . There 's no @ @ @ @ @ @ @ @ @ @ I open that project and lights goes out . 
@@44332697 @4332697/ <h> Nucoda Tangent Mapping adapted slightly for Resolve users . <p> So for those of you interested in trying out Nucoda here is a mapping that basically maintains most of DV 's default mapping , enough so that their guide still makes sense while at the same time bringing more of great color tools up front and onto the other BT panel . This mapping is meant for a 5 panel layout as per their original mapping . Having the additional BT Panel is really useful and worth getting as it allows you to have Events/Memories navigation and other functions available while still navigating the timeline / adding layers etc etc . <p> The layout is as follows , from left to right <p> KB , BT2 , TK , BT1 , MF <p> If you do n't have a second BT panel you should still be able to play around as the Color tools in this mapping are on BT1 and therefore should show up . Another option is to get the free trial of the Tangent ElementVS for iPad and Android to add a virtual second BT @ @ @ @ @ @ @ @ @ @ As mentioned I have left most of the other items as is but this should help you get your feet wet and remember you can custom map what ever you want . Not to mention having the ability to adjust the sensitivity of every control and map Keyboard Shortcuts onto the panel . The Tangents and Nucoda also have 2 way communication so you can see the values changing on the elements screens . <p> There are still a few important mapping omissions and several things that need to get sorted out that will hopefully be addressed by Digital Vision soon . <p> So in order to get started add the TangentActions.prefs file ( see link below ) along with the Mapping File ( Revised Default.XML ) ( Before doing be sure to change the name of the existing file so it does n't get overwritten just as a backup ) C : **27;642;TOOLONG <p> Once you have set up the Tangent Hub and the panels , start Nucoda and enter a project . <p> Load up the Tangent Mapper , go to File , Map Management , Import and @ @ @ @ @ @ @ @ @ @ , then click Select . <p> You should now be all good to go . <p> All remapping is done in the mapper app . If you want to change entire pages and ripple them across all modes ( I.e regardless of the current tool being actively selected ) there is currently no way of doing that ( I.e no copy and paste or duplicating of entire banks ( as they are called ) ) and be mindful of " All banks " drop down as this does overwrite pages . If you want to ripple mapping as mentioned above there is a workaround by editing the XML . I can walk you through it if you want to go this route . Once you understand the structure and what to do it 's pretty easy until Tangent can implement more batch style workflow and we can skip the XML editing or brute force method of remapping multiple pages . Nonetheless as you will see it 's incredibly liberating to finally be able to map your panels any which way you like , whenever you like ! <p> The attached @ @ @ @ @ @ @ @ @ @ the Keyboard Shortcut Pdf ( which has a lot of nifty shortcuts ) and DV 's Tangent guide . <p> Just gave it a spin and it 's more intuitive than the stock Nucoda tangent map for me anyway . It 's nice to be able to click and just see base grade etc . I recommend anyone trying out Nucoda to give this a mapping a go and use the Tangent mapper to modify and maybe post your map . It would definitely help if the mapper had a batch style copy paste function . I 'm not familiar enough with the mapper to conceptualize how this would work , but maybe Tangent Inc could chime in here . But this is what Resolve users have been griping about for years , and Nucoda gives it to you - full custom mapping of a big and powerful color toolset . <p> @Paul : There is no " batch style copy paste function " . As Harris has pointed out , you can edit the XML file by hand to do this , so there is a work-around . We @ @ @ @ @ @ @ @ @ @ design for the Mapper It 's not a trivial thing to add , and to add it as an intuitive feature is even more tricky . We are currently in the middle of some update work on the Mapper to make it more intuitive for setting up mapping key presses to non-native apps , so perhaps once this is released and settled we could go back and look into this . If we do add this feature to the Mapper , it will take some time to do so do n't hold your breath 
@@44332698 @4332698/ <p> This is a bummer , but really , as UHD/4K becomes the norm this will really only apply to Hackintoshes with modern internals . Having more than three big AMD or Nvidia cards wo n't help if you ca n't push the data across the logic board . I 've been doing some 4K experiments with a 12 x 3.47GHz z800 with 48GB of 1333MHz DDR3 and it just struggles . It will be easy to extend the useful life of the machine using a 2:1 proxy workflow , but no amount of GPUs is going to make actual 4K work any faster or easier on older machines . <p> Using Resolve on a Mac with the HD finish is considered now to be a standard grading workflow . A couple of good GPUs with a modern multicore CPU can handle pretty much anything you throw at it at HD resolution . Now with 4k on a horizon with possible addition of HDR and HFR , working from 5-6K material will bring the whole " brute force " philosophy using Mac Pro into a spotlight . And not @ @ @ @ @ @ @ @ @ @ solution , as it is much more open and it is not limited to a single hardware manufacturer . I 'm sorry , but with this latest news of OS X limiting number of GPUs even with AMD cards now , Hackintosh clearly shows the futility of that approach as well . The era of 4+1 GPUs on a Mac is gone forever . Anyway , as many know , I always championed a more elegant approach- an anti-brute force , if you will , where smart background caching with help of occasional on-the-fly proxies may be a more practical way of pushing 4k images in real time through the grading system . Sadly , Resolve 's cache is not the background type nor is it smart . It is very much work in progress not ready for the primetime . And because of that I honestly ca n't see how the current MacPro architecture could satisfy the upcoming bandwidth needs of the real time 4k workflow , other than work in HD and just render 4k at the end . But that is not really working in 4k is @ @ @ @ @ @ @ @ @ @ looks better , but at the price of no Prores . So , realistically , that leaves only the Linux as a realistic 4k platform for the masses ... Or is it ? <p> On the other hand , Windows hardware solution looks better , but at the price of no Prores . So , realistically , that leaves only the Linux as a realistic 4k platform for the masses ... Or is it ? 43971 @qwx453971 <p> No ProRes unless you switch to Scratch I know , you were referring to Resolve ... I 'm seriously considering Nucoda and my almost retired Z800 box . My 2x R680 's and both 8 bay RAID 's could be migrated back over to Windows and viola . But the question becomes where do I want to spend money , on the software or on the computer ? I 'm leaning towards new software but need a solid demo to see if its a good fit . <p> No ProRes unless you switch to Scratch I know , you were referring to Resolve ... I 'm seriously considering Nucoda and my almost @ @ @ @ @ @ @ @ @ @ 8 bay RAID 's could be migrated back over to Windows and viola . But the question becomes where do I want to spend money , on the software or on the computer ? I 'm leaning towards new software but need a solid demo to see if its a good fit . 43971 @qwx453971 <p> Mike , if your ever around Hollywood , and I 'm not busy grading , I 'd be glad to demo it and let you kick the tires . <p> No ProRes unless you switch to Scratch I know , you were referring to Resolve ... I 'm seriously considering Nucoda and my almost retired Z800 box . My 2x R680 's and both 8 bay RAID 's could be migrated back over to Windows and viola . But the question becomes where do I want to spend money , on the software or on the computer ? I 'm leaning towards new software but need a solid demo to see if its a good fit . 43971 @qwx453971 <p> We 're planning to have another LA Nucoda get together very soon . Just @ @ @ @ @ @ @ @ @ @ not mistaken , this is Elements panel number four for me . But this time I 'm fully committed to use it with Nucoda and even with Resolve , but only if I have to And yes , I was alluding to Nucoda on Windows , not Scratch My set up is the equivalent to your ( my MacPro is hardware equivalent to your Z800 ) and you still can continue running resolve on the same computer . AJA and BMD cards work great together on the same computer . Actually , I can run both resolve and Nucoda at the same time feeding two different displays ! ! ! <p> Read these words : you do n't need the latest OS just because it 's the latest OS . Do n't update . 43971 @qwx453971 <p> Good point , and the reality is that 's incredibly common on Windows and Linux , too . Other than OS X ( 10.8.5 ) , I use Windows 7 SP1 which was released in 2010 , and CentOS 5.3 which was released in 2009 . Just like Vista or Windows 8 , @ @ @ @ @ @ @ @ @ @ do n't use it . <p> Although that is my own experience as well , unfortunately Apple has a way to force the OS upgrade onto the user , be it the inability to run the current FCP X or the latest AMD hardware . Or they could , at some point , decide to limit Prores XQ only to the latest OS . Apple is brutal when it comes to the latest OS adaption and prevention of OS segmentation . <p> Good point , and the reality is that 's incredibly common on Windows and Linux , too . Other than OS X ( 10.8.5 ) , I use Windows 7 SP1 which was released in 2010 , and CentOS 5.3 which was released in 2009 . Just like Vista or Windows 8 , if a new OS does n't work for you , do n't use it . 43971 @qwx453971 <p> I am kind of a Windows 7 fanboy , if there is such a thing . It 's always been pretty rock solid stable , and never changes . 
@@44332699 @4332699/ <h> Full Container DCP ? <p> Fairly simple question - Is full container 2K a standard DCP frame size , or is using the full container uncommon ? I 've found a lot of conflicting information on this question , but a lot of people seem to agree that Flat and Scope comprise the majority of DCPs . I will be coloring a documentary with a variety of HD , 2K and 4K footage with the intention of creating a DCP , and I 'm trying to plan for as many of these specs as early as I can in the process . <p> Yeah , I would choose flat or scope for the DCP , to be on the safe side . You can probably get away with full container , but as Juan said , it 's not a common format . It 's also not that much different from the flat dimensions anyway . <p> The kids at the multiplex scratch their head and toss the instruction sheet out . Footlamberts ? Is that something you eat ? When the last Hobbit came out I was @ @ @ @ @ @ @ @ @ @ was about to watch was in high frame rate . They had no idea what I was talking about . It 's a pity because just a little bit of training would go a long way . <p> The kids at the multiplex scratch their head and toss the instruction sheet out . Footlamberts ? Is that something you eat ? When the last Hobbit came out I was asking the theater staff to make sure the presentation I was about to watch was in high frame rate . They had no idea what I was talking about . It 's a pity because just a little bit of training would go a long way . <p> Note that Grand Budapest Hotel is a rare movie going out in three different aspect ratios within one film . For most filmmakers , I 'd say that 's an incredibly indulgent gimmick and silly proposition , prone to a lot of potential malfunction . There are always exceptions ; George Lucas ( producer of American Graffiti 2 ) and Wes Anderson are among them . Brainstorm did it as well , 30 @ @ @ @ @ @ @ @ @ @ ( 1.85 for most of the film , 2.40 for the " brainstorm helmet " scenes ) . 
@@44332700 @4332700/ <p> Thanks both of you . The key worked , but didn-t look very good , just more grey . I should have another more dramatic sky instead ( there-s thunder going on in the soundtrack ) . Do you know anywhere I could download something free without timelapse ? <p> Thanks both of you . The key worked , but didn-t look very good , just more grey . I should have another more dramatic sky instead ( there-s thunder going on in the soundtrack ) . Do you know anywhere I could download something free without timelapse ? <p> Put to bright one on top of the darker one . Key/mask the sky and create ' alpha output ' in the node viewer , then connect the alpha channel to it . 43971 @qwx453971 <p> Pepijn , <p> I tried a bit of combination of what you and Marc said . Here 's a still of what I came up with . The main problem I see is that it 's very difficult in Resolve to finesse matte edges . I used blur and In/Out ratio , @ @ @ @ @ @ @ @ @ @ the towers . I can change the ringing from white to black , lol , but I ca n't seem to be able actually remove it . <p> Sky replacement is always an option . At some point , there are limitations in pulling a luminance key and replacing the background with something drastically darker , because the seams will tend to show . A good VFX artist could pull this off , but the tools are more limited in Resolve ( especially with a JPG image ) : <p> A moving shot would be tough . A lockdown is possible to a point . Alexis covers this on pp. 809-811 in the Resolve v12.5 manual , and similar techniques work in all major color software . In VFX compositing software , you 'd have a lot more options . 
@@44332701 @4332701/ <h> Upscaling and edge detection in Baselight <p> Is there a way to add some form of a highs filter or edge detection to add a bit of subtle sharping in Baselight ? <p> I have some footage ( quite a bit ) that 's 1920x1080p 10-bit that 's being used in a UHD programme . Obviously i know you ca n't really do too much if the detail is n't there to begin with , In Mistika we built a nice subtle edge detection that just helped add bit of sharpness in places of fine detail ; hair , eyes etc . So i was wondering if anyone knew the best route to do something similar ( or better ) in Baselight ? <p> Hello , there are at least two sharpening tools in baselight . The normal sharpen tool in the " insert " menu , which have a variety of controls functions ( Referenz manual page 569 ) and the " old " sharpen tool with less controls . If you resize the image , there is the possibility to choose between a number of resize algorithems in @ @ @ @ @ @ @ @ @ @ manual page 117 ) <p> Sharpening works great if you downsample a video but it is not so good if you upsample one . Most sharpening algorithms use edge " enhancement " by increasing contrast around edges . <p> A better alternative is to use a warp sharpening technique . Warp sharpening makes sharp things sharper by thinning them instead of increasing the contrast . Warpsharp functions are available in Avisynth/Vapoursynth and also in some plugins . <p> In Mistika we built a nice subtle edge detection that just helped add bit of sharpness in places of fine detail ; hair , eyes etc . So i was wondering if anyone knew the best route to do something similar ( or better ) in Baselight ? 43971 @qwx453971 <p> What you 're describing is a high pass sharpen , so this thread may give you some ideas : <p> I have no idea if you can do this is base light but just a thought if your pretty noise free in the image split out the L channel in LAB colour if you can switch to LAB ...... and just @ @ @ @ @ @ @ @ @ @ need to sharpen it a tiny bit other wise it a very strong effect <p> Thanks Jason &amp; Gavin , are matchbox shaders avaible to use in Baselight 4.4 or just in the upcoming Version 5 ? I 'm not sure you can switch to LAB - that 's something that i 'm not very familiar with if i 'm honest . I 'll do some reading ! <p> You can use Lab in Baselight by using the " ColourSpace " operator . Insert one and you 'll have the option for " RLab " in the drop-down . You then add another one which converts back to the working space . In between the two operators , you are using Lab. 
@@44332702 @4332702/ <h> only export xml from the ; fincal cut pro xml rountrip preset ? <p> When i use this preset in the render page , it render all my clips with a unique new name then in the process create an xml . Is there a way to export only the xml from it ? It got accidentally deleted , and i do n't want to retranscode all the media only for a . xml file thankts <p> The option to export from the timeline is n't working as the clips wo n't have the new name made by the export preset , thanks ! <p> Ooft . I was about to suggest exporting from the edit page , but there 's that . <p> I 'm not in front of Resolve right now , but if I were you I 'd try doing a retranscode and canceling it after a few minutes . If the XML is generated before any transcoding takes place , then that might be your saving grace . I 'm actually not sure at what point Resolve is programmed to do so , but @ @ @ @ @ @ @ @ @ @ to regenerate a proper . xml file from Resolve , unless you do a full render again . But you can set it to a codec like ProRes Proxy , so it wo n't request you a lot of disk space , and you can also set the Enable Flat Pass option to Always On . What it will do is to turn off all your grades and render much faster . <p> When it 's done , you can delete all the . mov files and have your fully working . xml file . <p> My current work around is to re-import the fcpxml of the rendered colored files as a new timeline and then exporting an XML , which I use to go back to Premiere . I do n't  need to render again and the XML refers to the colored files , as they were imported via the fcpxml . <p> Had this issue on a campaign with 32 spots that was cut in Avid but getting onlined in Premiere ( do n't ask ) .. we just gave up on round tripping instead of trying to @ @ @ @ @ @ @ @ @ @ <p> Had this issue on a campaign with 32 spots that was cut in Avid but getting onlined in Premiere ( do n't ask ) .. we just gave up on round tripping instead of trying to deal with the hassle of reimporting/reexporting all those timelines . <p> But you can set it to a codec like ProRes Proxy , so it wo n't request you a lot of disk space , and you can also set the Enable Flat Pass option to Always On . What it will do is to turn off all your grades and render much faster . When it 's done , you can delete all the . mov files and have your fully working . xml file . 43971 @qwx453971 <p> I did that once , and I also set the resolution to the very minimum , about 12x12 pixels or so ... Then I took a video of my rendering fps , and showed off to my colleagues : See that ? My home-build . <p> Jeremie , are you saying that if you export an XML from the Edit page , that @ @ @ @ @ @ @ @ @ @ The XML export from the Edit page should always link to the latest renders from the Deliver page , even if they have unique names ( unless the clips are unrendered , in which case it links to the original source files ) . <p> I never use the XML automatically generated from the render process when round-tripping . Instead , after rendering , I export an XML manually from the Edit page ( I do this so I can save it in the correct location in my file structure and using the correct file naming convention ) . This XML always links correctly to my rendered files with unique names . The only time it does not work is if Resolve crashed during the render process ( after rendering the files but before exporting its automatic XML ) . If it did crash at that time , then Resolve does not know it already rendered those files and does not know their location/name . <p> I never use the XML automatically generated from the render process when round-tripping . Instead , after rendering , I export an XML manually @ @ @ @ @ @ @ @ @ @ can save it in the correct location in my file structure and using the correct file naming convention ) . This XML always links correctly to my rendered files with unique names . The only time it does not work is if Resolve crashed during the render process ( after rendering the files but before exporting its automatic XML ) . If it did crash at that time , then Resolve does not know it already rendered those files and does not know their location/name. 43971 @qwx453971 <p> Side note . I 've had a lot of problems with roundtripping when Onlining to a new Drive . When I copy my renders to a new drive and import the xml into FCPX the clips show up as black even though the renders are fine . Anyone else have this issue ? I contacted BM about it and they could n't give me an answer . <p> If that is the case , how do you get an XML to export that is compatible with Premiere ( since the preset only export FCPX XML 's ) and links to the newly @ @ @ @ @ @ @ @ @ @ have its own xml file format . But it can read and write fcp7 xml files . That being said , Premiere always translates its own metadata , effects , animations and other data to something equivalent in FCP7 xml format , and vice versa . I 've being experiencing motion translation bugs in CC 2014 as well as 2015 . Basically any modification you do to the motion tab on Premiere wo n't translate good to Resolve , neither back to Premiere . <p> Thanks for all the inputs , i did n't know that if i render , then if i export from the timeline it ll keep the new filenames The low rez is also a good solution thanks ! 43971 @qwx453971 <p> If you render your graded clips and then quit Resolve , when you open Resolve again you wo n't be able to export a proper xml file . So remember , whenever you want to export a working xml file from the edit page you should do it right after rendering your graded shots and before quiting Resolve . <p> Is anyone else getting @ @ @ @ @ @ @ @ @ @ Premiere in CC2014 CC2015 ? Several times the XML loaded up and the clips were there , but half way through each clip it cut out , showing a checkered clip . Strange thing is the actual rendered CC media was complete . Other issues are XML just wo n't be read in Premiere , as it throws an error saying it ca n't understand it . <p> Seems I 've been having nothing but " lots of fun " with round tripping recently w Premiere , mainly on the return . Anyone else ? Trying to narrow down if its Resolve or Premiere or both . <p> Is anyone else getting some strange issues when trying to round trip back to Premiere in CC2014 CC2015 ? Several times the XML loaded up and the clips were there , but half way through each clip it cut out , showing a checkered clip . Strange thing is the actual rendered CC media was complete . Other issues are XML just wo n't be read in Premiere , as it throws an error saying it ca n't understand it . <p> @ @ @ @ @ @ @ @ @ @ fun " with round tripping recently w Premiere , mainly on the return . Anyone else ? Trying to narrow down if its Resolve or Premiere or both . 43971 @qwx453971 <p> It 's not reading the FCPX xml or the 1.4 xml ? IT 's not going to read the xml you spit out from the round tripping pre-set . <p> I seem to get problems where I have to re-link the media of a handful of clips even though they are all rendered to the same folder ... <p> It 's not reading the FCPX xml or the 1.4 xml ? IT 's not going to read the xml you spit out from the round tripping pre-set . <p> I seem to get problems where I have to re-link the media of a handful of clips even though they are all rendered to the same folder ... 43971 @qwx453971 <p> Its not reading a v5 . XML file . I know about the v4 v v5 issue , but it previous worked to some extent . For now , I end up bringing all my CC files back @ @ @ @ @ @ @ @ @ @ , and then exporting an XML from the edit tab for the " new " CC timeline . <p> Its not reading a v5 . XML file . I know about the v4 v v5 issue , but it previous worked to some extent . For now , I end up bringing all my CC files back into resolve , importing the fcpxml exported w the render , and then exporting an XML from the edit tab for the " new " CC timeline. 43971 @qwx453971 <p> Yep . It changed in the last update I believe . Resolve should have a Roundtrip preset for Premiere ... It is a main editing platform . Hopefully v12. 
@@44332703 @4332703/ <h> Color diference in sony monitors entry level - HELP <p> Hi folks I have a small video production company here in brazil , ans we are focused in brodcast tv ads . <p> As main monitor we use a 27 " dell 2713hm and we are very happy with their quality and resolution , but we need a reference monitor in rec 709 via ultrastudio miniMonitor thunderbolt . We invested in 2 entry levels sony : LMD 2110w and LMD 1750w . <p> None of this monitors have SDI HD , so we have to use HDMI on 2110w And DVI ( trough hmdi to DVI cable ) in 1750w . <p> We put a powered hdmi split in HDMI output of ultrastudio minimonitor , just to test both monitors at the same time . <p> The problem is : each monitor shows a completly different result , even with factory settings ! But is very diferent and we just do n't  know how to do , and in what monitor trust . <p> The 1750w have in your menu , the option to set rec709 colorspace . @ @ @ @ @ @ @ @ @ @ seems the luminance and brigthness os very lower than 2110w . <p> The 2110w doest have in menu the option to set rec709 colorspace . It shows an image very more brighter , with more luminance and a little more contrast . <p> As those 2 models is entry level , the menus does n't  have a lot of settings to change , But being from the same brand , using the same input signal ( hdmi from ultrastudio minimonitor ) We just does n't  understand how they show images so diferent at the same time . <p> Without getting into detail about the types of monitor etc the basic starting point here is calibration . <p> Very few monitors leave the factories calibrated , and even the ones that do should be tested for accuracy to begin with . <p> There are lots of threads here about calibration and there are lots of hardware and software solutions available so definitely try a search of the forum . Personally , and I am an owner of the software as a disclosure , I find the information on the LightSpace website @ @ @ @ @ @ @ @ @ @ of information throughout the forum as well . <p> Jack is absolutely right . I was hired last year as a colorist for a three month project and I had to work at the client 's facility . They purchased two brand new Sony PVMA250 OLED monitors and they were very different from each other before calibration . And these are supposed to be reference level monitors . <p> i would be wary of using the DVI input of the 1750w as it is probably expecting an RGB full range ( computer ) signal . I 'll bet you need to get one of the option input cards for it to be used as a video monitor in yuv/legal range . the LMD 2110w you would set to D65 and then adjust the brightness/contrast , gamma , and gain/bias controls to set white point using a probe and calibration software like Calman. does n't look like it has selectable color space , but likely the hdmi/d65 is also 709 primaries . you absolutely have to calibrate it to be anywhere close to correct . <p> I had a similar problem @ @ @ @ @ @ @ @ @ @ the HDMI signal showed very badly crushed blacks on my calibrated Rec709 monitor . This may have been the HDMI input on the monitor but to test an alternative , I bought a BMD HDLink box to convert the Decklink 's SDI output to Display Port ( or DVI ) ( no LUTs installed ) and the result was almost perfect . <p> Eventually , I installed a 6G Decklink 4K SDI I/O card and ran the SDI output to the HDLink box for conversion to DVI and now I have a very high quality 10bit HD or UHD link to my monitor 's calibrated Rec709 input . <p> Jack is absolutely right . I was hired last year as a colorist for a three month project and I had to work at the client 's facility . They purchased two brand new Sony PVMA250 OLED monitors and they were very different from each other before calibration . And these are supposed to be reference level monitors . 43971 @qwx453971 <p> Hi Paulo ! I 'm in Rio too . Our company is located on CEO corporate offices in barra da @ @ @ @ @ @ @ @ @ @ would be a plesure <p> Without getting into detail about the types of monitor etc the basic starting point here is calibration . <p> Very few monitors leave the factories calibrated , and even the ones that do should be tested for accuracy to begin with . <p> There are lots of threads here about calibration and there are lots of hardware and software solutions available so definitely try a search of the forum . Personally , and I am an owner of the software as a disclosure , I find the information on the LightSpace website very useful to get started , but there is loads of information throughout the forum as well . 43971 @qwx453971 <p> Hi Jack ! Thanks for your time and support . I still have some questions . Would I need some kind of hardware to calibrate , or I can try to calibrate just using software . This software would create a profile of color menagment to my reference monitors ? I should set manualy settings in each monitor ? <p> It 's why I usually recommend that people get a Flanders Scientific @ @ @ @ @ @ @ @ @ @ a fair amount of research , and Flanders let 's you send them back and forth for calibration . However , knowing Brazilian import tax laws , that may be complicated or cost prohibitive for you . 
@@44332704 @4332704/ <p> 1 . Why do we get two scan rates , vertical and horizontal ? 2 . Can the Eizo for instance display a 25p signal with a 25Hz scan rate ? Meaning it would draw only 25 pictures a second instead of 50 ? Is this something that is preferred - watching 25p in 25Hz ? 3 . What about the backligh ? It probably has it 's own flicker rate ... <p> People who actually know what they are talking about : feel free to correct me if I 'm wrong . <p> The idea of refresh rates comes from CRTs where the frequencies controlled the vertical and horizontal " movement " of the electron beam to scan the phosphors . This was important when analog broadcast was developed as the signal it received OTA was used to directly drive the electromagnets used to deflect the electron beam across the screen . It started in the upper left , made a zigzag pattern down to the bottom right corner ( this is when the actual picture was drawn ) , then went all the way back to @ @ @ @ @ @ @ @ @ @ . The vertical rate was the deciding factor in how many frames were shown each second as deflecting the beam vertically from one corner to the other took way longer than the initial horizontal movement* . Basically , the refresh rates told the beam where to be , the other display signals told the beam how much energy to apply to the spot of phosphor . <p> Digital displays are a bit different in that there is no raster to scan . Instead the refresh rate you input ( the vertical one ) controls the amount of images it will display in a second , or more accurately , how many updates it will receive in a second . The big problem with current 4K monitors is that there is no easy way to send more than a 30hz signal due to bandwidth , so the resultant image looks rather choppy vs. 60hz . TVs are similar but usually have an internal refresh rate that they scale incoming signals to . For example : my PDP can have either 24 or 60hz input . Because it 's a phosphor display @ @ @ @ @ @ @ @ @ @ per second to avoid flicker , so a straight 24hz wo n't do . Instead , it displays it at 72hz since it is a multiple and you get glitch free motion . 60hz will display as straight 60hz , and if I set it in the menu 24hz will display as 60hz with pulldown . LCD TVs usually use either a 60hz or 120hz internal rate that every input is scaled to . LCD monitors are somewhat more flexible as evidenced by the timing ranges of your monitor . <p> What does this mean in regards to your questions ? If you 're viewing 25hz material with no need for a GUI to be represented on the screen , a 25hz input signal should work on the EIZO ( if you were using a Decklink , you 'd pick 25P HD signal as your output ) . If you use an interface on that screen it will look very jerky and will be almost unusable , therefore you 'd want to run it at 50hz or 75hz as then you 'll still have perfect motion , but your GUI @ @ @ @ @ @ @ @ @ @ feeding the monitor from a videocard output ) . When I was using a Trinitron monitor for 24p motion graphics work , I 'd run the refresh rate at 72hz in HD , that way I had perfect scaling with no odd motion artifacts , a low amount of flicker , and smooth GUI . <p> Concerning the backlight : usually backlights are controlled via PWM to facilitate changing the backlight brightness . If the backlight is the strobing sort to improve motion resolution , it would match to the internal refresh rate . <p> *Fun fact : the time the CRT took to paint the picture was so short that the initial brightness several thousand nit brightness of the phosphor looks to be around 80 to 100 to our eyes . This is the reason why motion on a CRT is so smooth compared to current display tech . <p> Hi Synnove , thx for the explanation . And yes , it 's for use with a decklink , 25p output . Regarding the scan rate , is the monitor synced with the signal or does it scan the @ @ @ @ @ @ @ @ @ @ monitor , if in 25Hz scan rate , know when to change the frame or is it totally independent ? Another thing that 's unclear to me is , how long is one frame portrayed ? Is it 1/25s if in 25Hz ? And I presume that if watching 25p video it 's better to have 25Hz signal , so you get a more realistic view of the motion ? But for broadcast this will get chopped up to 50i ... <p> Monitors do not usually modify the refresh rate of the incoming signal ( though LCDs can be picky about what they 'll sync to ) and will usually alert you if the refresh rate is out of its supported sync range . Because the monitor 's refresh period is driven by the incoming signal the monitor will display a new frame whenever it receives one . Yes I believe it 's 1/25th of a second . <p> It is preferable to have the refresh rate match the frame rate of the video , so yes 25hz setting in resolve would be ideal ( 50i is still usually made @ @ @ @ @ @ @ @ @ @ a lot of US broadcast material is from 24p sources but transcoded to 60i ) . That being said , if your project demands motion accurate imaging in your displays ( not usually of importance for color work ) , you 'd be better off with a non LCD technology . If instead it 's a matter of getting the display to sync to the signal resolve is outputting then I believe that monitor should work , though it 's always a good idea to contact the maker if you 're not sure . 
@@44332705 @4332705/ <h> Any Plans For Baselight Premiere Edition <p> It would seem the greatest limitation of Base light edition is really the limitations of the NLEs as relates to camera raw and image sequence ability . To that end a Baselight edition on Premiere CC would seem an incredible combination . <p> I guess the combination of Speedgrade and Premiere still not quite the foothold of Avid or FCP as an NLE of choice are factors but I would be curious if this is in the cards of the future . <p> I guess the combination of Speedgrade and Premiere still not quite the foothold of Avid or FCP as an NLE of choice are factors but I would be curious if this is in the cards of the future . 43971 @qwx453971 <p> It 's not completely about market share or acceptance . I remember hearing that Premiere does n't allow access to clip data in the same way FCP and Avid do , and I think overcoming that , combined with the potential market are what might be making it a lower priority . I know Filmlight has @ @ @ @ @ @ @ @ @ @ maybe one of them can elaborate . <p> It would seem the greatest limitation of Base light edition is really the limitations of the NLEs as relates to camera raw and image sequence ability . To that end a Baselight edition on Premiere CC would seem an incredible combination . <p> I guess the combination of Speedgrade and Premiere still not quite the foothold of Avid or FCP as an NLE of choice are factors but I would be curious if this is in the cards of the future . 43971 @qwx453971 <p> Hi Chris , <p> We 're certainly keen to do a version for Premiere , we are just in a position of having to ship Baselight for Nuke ( and updated versions of other plugins with all the new colour science stuff ) before we can do anything new . Once that is done ( BL for Nuke is going in to beta really soon ) , we will evaluate the next platforms and Premiere is certainly high up our list . <p> We 're certainly keen to do a version for Premiere , we are just @ @ @ @ @ @ @ @ @ @ ( and updated versions of other plugins with all the new colour science stuff ) before we can do anything new . Once that is done ( BL for Nuke is going in to beta really soon ) , we will evaluate the next platforms and Premiere is certainly high up our list . <p> -Martin ( Baselight Dev ) 43971 @qwx453971 <p> Martin , Thanks so much for the fast reply . Keep up the great work . We sure could of used Baselight on Avid recently on a recent long form project . The AAF ( or XML for that matter ) round tripping between NLE 's and a certain other color corrector have a lot of moving parts and when a wheel comes of it really comes off . Looking forward to demo-ing a Baselight Edition . I 'm going to be lazy and ask a question rather than look for it - is there a link to any user manuals for BL Editions for Avid and FCP ? <p> Baselight on a Mac is a marketing decision and not a technical one . Baselight on OSX @ @ @ @ @ @ @ @ @ @ 6 years ago ) as a limited time option in order to get better familiar with the program and to practice and it is available only for the owners of Baselight . <p> I 've got a not completely related question that 's not worth opening a new thread for : <p> Anyone using BL for FCP ? I 've got a regular TV job that I have to grade in FCP at the moment ( because they do n't want to lock things off ) , all with 3-way corrector and other built-in tools , and after 40 episodes it 's starting to do my head in . <p> I 'm toying with the idea of asking them to " invest " in the plugin , which would make its money back in about 2 episodes given the time I 'd save . <p> Is the plugin still supported ? And any way of transferring the licence to another platform later down the track ? ( The client might switch to Avid next season ) <p> I 've got a not completely related question that 's not worth opening @ @ @ @ @ @ @ @ @ @ FCP ? I 've got a regular TV job that I have to grade in FCP at the moment ( because they do n't want to lock things off ) , all with 3-way corrector and other built-in tools , and after 40 episodes it 's starting to do my head in . <p> I 'm toying with the idea of asking them to " invest " in the plugin , which would make its money back in about 2 episodes given the time I 'd save . <p> Is the plugin still supported ? And any way of transferring the licence to another platform later down the track ? ( The client might switch to Avid next season ) 43971 @qwx453971 <p> Hi Julien , <p> To answer your questions : <p> 1 . We have a number of customers who are still using FCP7 and the plugin every day . <p> 2 . It is still supported and will be for as long as we can still actually run FCP7 on at least one of our development machines . <p> 3 . Yes . All of the Editions @ @ @ @ @ @ @ @ @ @ machine . So , if you buy BL for FCP and activate it , the license file generated will automatically activate BL for Avid ( and BL for Nuke when it is out of beta ) . <p> 1 . We have a number of customers who are still using FCP7 and the plugin every day . <p> 2 . It is still supported and will be for as long as we can still actually run FCP7 on at least one of our development machines . <p> 3 . Yes . All of the Editions will activate any of the other editions on the same machine . So , if you buy BL for FCP and activate it , the license file generated will automatically activate BL for Avid ( and BL for Nuke when it is out of beta ) . <p> -Martin ( one of the developers ) 43971 @qwx453971 <p> Julien also asked if Tangent Element is supported . So which panels are supported ? 
@@44332706 @4332706/ <h> Scopebox and Blanking ... <p> I 've recently had to deal with some blanking issues as noted by QC on a 2.4 HD Letterboxed show I 've been working on .... Beyond the fact that this was a very frustrating issues , was initially off by 3 lines on top and bottom , the reported values were not at a scale I was familiar with . <p> To avoid the testing phase of correcting the matter , since math alone proved to be insufficient , are you able to monitor blanking in Scopebox ? I 've looked through the manual and the software and ca n't seem to find anything ... but I figured it would n't hurt to ask ... <p> If Scopebox ca n't display , are you able to evaluate using the Resolve scopes ? Again , I looked at the scopes , but was unable to find any setting that appears to do this ... <p> ( I know QC Central has a spec for 2.40 , but I 'm not sure what it is . ) Having reliable scopes with specific displays for @ @ @ @ @ @ @ @ @ @ generally have n't had anything kicked back for a line or two , but 3 lines is pushing it . Generally the best way to go is to get the specs from the client first , then give the files a once-over before delivery so you can be sure you 've dotted the I 's and crossed the T 's . I worry far more about audio these days than picture ; those ITU loudness specs can be murder . <p> When we have tape delivery , I stay on top of the people who are doing the layoffs from files to HDCam-SR and make sure I see a report from them that all the tapes are correct , particularly on blanking , formatting , track assignments , levels , and clipping . <p> It definitely would have been a much more straight forward process if it had been in pixels ... but , as I found out after much research , they were reporting lines based on a scope set to 1/2 scale 1160 HD ( active lines 91-490 ) ... which made finding the right frame size more @ @ @ @ @ @ @ @ @ @ take a look at QC Central . This was the first time I 've done a QC with a client as opposed to being an employee of the shop getting the QC ... From a peace of mind stand point I can see a big benefit to getting you own QC ... It would feel pretty good to hand the report over with the master too . <p> You can do an easy poor-man 's check by saving your full resolution image and checking the aspect ratio in Photoshop . For a letterbox image you could count the black lines above and below the picture or count the active raster lines . <p> Keep in mind that 2.40 is really 2.39 and that the convention dictates vertical line count must be even , even if this yields a slightly incorrect aspect ratio . <p> That 's what I thought . So I did the math after it was kicked back the 1st time and came up with 1920x800 . Exactly 2.4:1 . But that was wrong . Turned out to be 1920x798 , which if I had been paying attention @ @ @ @ @ @ @ @ @ @ subtracting 91 from 490 , getting 399 and doubling ... But confusion over what the values represented led me to miss that key bit of information . <p> Keep in mind that 2.40 is really 2.39 and that the convention dictates vertical line count must be even , even if this yields a slightly incorrect aspect ratio . 43971 @qwx453971 <p> The studios we delivered to at Technicolor never had a problem with just straight 2.40 . We never gave them 2.39 , at least in my experience . We did occasionally have arguments about 2.35 vs. 2.40 , and I tried to make the point that 2.40 makes more sense simply because it eliminated more splice lines at the bottom of the frame in film . With digital , it 's dealer 's choice : you can make up whatever aspect ratio you want , as long as QC will accept it and the client will sign the work order and pay the bill . <p> I agree with digital the actual aspect ratio can be completely arbitrary . But one should keep in mind the whims of QC @ @ @ @ @ @ @ @ @ @ on 2.35 vs. 2.40 vs. 2.39 history . <p> I agree with digital the actual aspect ratio can be completely arbitrary . But one should keep in mind the whims of QC operators too . There is a very good Wikipedia article on 2.35 vs. 2.40 vs. 2.39 history . 43971 @qwx453971 <p> Yep , I wrote some of that article . I would go totally with whatever the distributor or studio specifically requested , get it in writing , and I 'd make sure there 's a " paper trail " so that in case of any surprises , the post company ( or colorist ) can always produce correspondence where the choice of aspect ratio was approved by the client prior to delivery . <p> This can get problematic , particularly with features shot in 1.78 HD but released in 1.85 , so the question becomes " do we matte it 21 more lines top and bottom , or do we let it go full-frame ? " I also warn clients shooting 2.40 that there will always be distributors who specifically want full-frame 1.78 in addition to a @ @ @ @ @ @ @ @ @ @ anamorphic . Ultimately though , the choice is the client 's , and our job is to make them happy . 
@@44332707 @4332707/ <h> Sony TriMaster Black Levels <p> I have n't had a chance to see all three of these together in person before , but this image really puts it into context . What blows me away is how milky the CRT is when everyone still uses them as the ideal black reference in discussions . I wonder if people are a little utopic in their memories about how good they were ? <p> I have n't had a chance to see all three of these together in person before , but this image really puts it into context . What blows me away is how milky the CRT is when everyone still uses them as the ideal black reference in discussions . I wonder if people are a little utopic in their memories about how good they were ? 43971 @qwx453971 <p> The appeal of CRT is n't about absolute black level . It 's about shadow detail . No LCD I 've seen is able to represent as much information in the shadows as a CRT . But the OLEDs have surpassed in that regard . <p> what @ @ @ @ @ @ @ @ @ @ reflecting off the glass front of CRT that both the OLED and the LCD do not have ? 43971 @qwx453971 <p> Clearly there is a lot of glare in both situations and some dude is taking a picture with flash in the first picture . I 've seen all three of these monitors in my own room and the BVM is significantly higher than the oled but not as bright as the LCD . And the Sony oleds also pick up significant glare . <p> PVM OLED is a very basic model with only 8-bit monitoring . Also you have to remember , that OLED panels have very brief time of life ( about 2-2,5 years ) . BVM-E costs $26 000 AND it 's only 24.5 inch , compared to Dolby 's 42 . <p> EDIT : PVM is 10-bit , BVM series is 12-bit , as is Dolby . Apologies for my mistake . <p> PVM OLED is a very basic model with only 8-bit monitoring . Also you have to remember , that OLED panels have very brief time of life ( about 2-2,5 years ) . @ @ @ @ @ @ @ @ @ @ , compared to Dolby 's 42. 43971 @qwx453971 <p> PVM oled is a 10-bit panel &amp; processing . And powered on time does n't count against its life if the signal is black . There 's truly nothing wrong with the PVM other than off axis issues &amp; limited colorspace settings vs e or f . <p> I too have seen the Dolby reference monitor , and while it 's undoubtably the best LCD display money can buy , it 's still LCD . I 've seen them side-by-side and I would far rather grade using a BVM-E250 than a Dolby . But that 's just personal preference , I like the way the oleds represent shadow detail and color much more than even the Dolby . <p> of course but $5k for the pvm OLED and $35k for the dolby ... although I did hear that there were some NAB b stock of the dolby available in the mid $20k range ! 43971 @qwx453971 <p> dolby is 39K , ex demo has been sold for 27.5K ... in my opinion is the bets monitor ... only issue for that @ @ @ @ @ @ @ @ @ @ spending that much on an HD might not be the safest choice , unless you need it today for an important project that need P3 color work ... <p> 12 bit processing might actually make image quite better : i have 10 bit plasma and 10 bit trimester ( l230 ) quite many times in particular soft images i see banding on plasma and there is no banding on BVM ( not sure if is due to different technology LCD vs Plasma ) but might be due to 12 bit internal processing of BVM .... <p> 12 bit processing might actually make image quite better : i have 10 bit plasma and 10 bit trimester ( l230 ) quite many times in particular soft images i see banding on plasma and there is no banding on BVM ( not sure if is due to different technology LCD vs Plasma ) but might be due to 12 bit internal processing of BVM .... 43971 @qwx453971 <p> I would attribute that to tech . I do n't see any banding on my PVMs vs BVM. 12bit is adventagous for wide gamut color @ @ @ @ @ @ @ @ @ @ ever lined up a Sony oled if you havecwhatvprobe did you use and to what XYZ coordinates . Cheers I heard there are massive problems aligning and flicker . Ps CRT has always been good in the blacks when in a dark room also I would n't put it past Sony raising the black level to make the Oled sell more <p> Pop question anyone ever lined up a Sony oled if you havecwhatvprobe did you use and to what XYZ coordinates . Cheers I heard there are massive problems aligning and flicker . Ps CRT has always been good in the blacks when in a dark room also I would n't put it past Sony raising the black level to make the Oled sell more 43971 @qwx453971 <p> Yes , I 've calibrated PVM &amp; BVM-E/F oleds . Wasnt a problem just had to adjust the sync timing on probe . Issue with flicker was regarding 23.976 PsF content &amp; only on the PVM . Easy work around was to feed true P signal . In any case firmware update modified to function like BVM , new firmware is @ @ @ @ @ @ @ @ @ @ of charge . I still have one with original firmware as flicker never bothered me . <p> Sorry , for all the questions but how did you do the adjustment is it in the Xrite software ? It 's just that I had kind of written them off and invested in another DreamColour and Elements Panel instead would be great to know for next time . Cheers . 
@@44332708 @4332708/ <h> Noise Reduction - Neat Video <p> I think most of the problems with the new cameras like HDSLR , Alexa , RED are the noise . <p> All that night shots , with a lot of grain result a problem for most of us . Personally noise do n't  botter me a lot , but many people are incredible anoing with the noise of the camera , I remember when you shoot in 35mm and you can remove the grain and now people can put grain in Digital Cameras , awesome ! <p> Well for those who hate the noise i want to how one of the most popular noise reduction ( DNR ) filter of the market , Neat Video . <p> Neat Video is a video filter designed to reduce visible noise and grain in video footage produced by digital video cameras , camcorders , TV-tuners , digitizers of film or analog video . <p> I use it all the time in AE &amp; I think it 's pretty great . The results can be ass-saving-ly magical or they can be just ok - it really @ @ @ @ @ @ @ @ @ @ to the footage , and how much time &amp; effort you 're willing to put into noise reduction . It 's not really made to be a fire-and-forget NR plugin - you got ta know a little bit about it for it to look good . And you can pretty much forget about any kind of realtime playback with this effect active on a clip - at least that 's been my experience . <p> As I said , I use it in AE , but I 'd love to be able to try it out in Scratch , if I were ever to get that opportunity . <p> While I keep thinking of how to make the grading more relaxed in format dslr h264 in special with low lights <p> As you can see this possibility , edit in your favorite NLE directly h264 ( I have no problems with CS5.5 or 6 ) upon completion of editing , apply the filter to the entire timeline or clips that need and make a transcoding to ProRes 10bit , after applying the effect that would be used 10bit ? if @ @ @ @ @ @ @ @ @ @ where there is noise to resemble the most of which have at hand . what if we give more space for the filter to work ? ? ? <p> There is also going to be DarkEnergy plugin for AE . I heard that they are the best for restoration and denoising . But I have NeatVideo one week and I did n't  have enough time to play with , just tried one shot and from noise I get skin maps . Tomorrow I have one day to denoise whole DSLR feature . Do n't  you know some straight tutorial with how fine tune and denoise 5D , it would help me a lot coz grading 5D was realy hard . Hope it will sit on grades after . 
@@44332709 @4332709/ <p> Have you looked at the AVID Artist Color ? Used you can get one for $600 range or higher . This is my first panel and the one I think I will be using for a long long time . What I like about it is that it is still small enough to fit your keyboard in front of it and it does n't hog the entire desk space in front of you . It also has a ton of preset buttons which will get you adjusting nodes , etc .. faster once you learn them . <p> The Color ca n't control Lumetri but you can map the buttons to do anything you could want and the Transport can be mapped much as it can in Resolve . I do n't Colour in Premiere except in very rare circumstances ( usually as a favour ) but do like having the Transport mapped for any editing ( usually roundtrip related ) . The Color is fantastic to use with BLE in Avid , Scratch and some of its limitiations in Resolve can be worked around with Macros as @ @ @ @ @ @ @ @ @ @ that the Page 2 controls are n't and ca n't be mapped to it . I do n't have a Midi Twister or anything fancy , I just have buttons mapped to put the mouse in the right place and then drag . For the cost the Color has got to be the best value but depending on your work the Micro could be useful , it just has nothing beyond LGG and the Page1/2 controls . Perhaps combined with an x-keys you could work reasonably quickly ... but only in Resolve . <p> Have you looked at the AVID Artist Color ? Used you can get one for $600 range or higher . This is my first panel and the one I think I will be using for a long long time . What I like about it is that it is still small enough to fit your keyboard in front of it and it does n't hog the entire desk space in front of you . It also has a ton of preset buttons which will get you adjusting nodes , etc .. faster once you learn them @ @ @ @ @ @ @ @ @ @ for the suggestion Dennis ! I 've seen this panel a few times on display at B&amp;H but honest I 've never really looked into it until now . The Avid Artist Color seems like the a great panel however according to a few people in other forums , it seems to have a bit of connectivity issues with the more recent versions of Davinci Resolve . I am not sure how true that is but it seems to be a common issues with quite a few people . Also it would be nice to have a panel that work in premiere pro on the rare occasions when i make a few adjustments in my roundtrip with Resolve . I will continue to do some research on the Avid Artist Color and see what i find . <p> I think that the wave 2 might be great but who know when that is gon na be released and at what price point . To be honest i would love to have the Tangent element however that does n't fit into my budget at the moment . <p> Thanks for all @ @ @ @ @ @ @ @ @ @ appreciated ! <p> Now it will really depend how you grade . If you only work on lgg then any of those panels work but when you need cam raw options on panel and offset i would think of Mini panel not Micro . For example in my work now all plays around offsets i.e. printer lights thus really important to have them . <p> I have n't ever had any connection issues with my AVID Artist Color . I did change my surface controller to static IP instead of getting it from DHCP server and not sure if that somehow helps what other people say they experience ? <p> Anyway good luck with what ever you decide . Getting a panel was the best thing I ever did . <p> I have n't ever had any connection issues with my AVID Artist Color . I did change my surface controller to static IP instead of getting it from DHCP server and not sure if that somehow helps what other people say they experience ? <p> Anyway good luck with what ever you decide . Getting a panel was the @ @ @ @ @ @ @ @ @ @ Dennis Could you go over your setup for the Avid Artist Color ? I think this is the direction that i am leaning towards at the moment , thanks . <p> EuControl connectivity is a total non-issue , at least it should be if you have enough brain cells to run Resolve in the first place ... i kinda roll my eyes at that one .. it 's dead simple , just RTFM ... <p> i 'm not a fan of the " rings " in Wave2 , and it 's a bit deep for my prefered working style with a big Wacom infront of the surface , everything else looks pretty sweet tho 43971 @qwx453971 <p> I do agree with you the wave 2 does look cheep . Also the good thing is that I Studied Computer Network engineering in college so i do n't that i should have a lot of issues creating a static IP Address and so on . <p> The panel is not well mapped in Resolve , so the extra functions in the menus are not as useful as one might think , and @ @ @ @ @ @ @ @ @ @ different pages . Also some of the color functions that the micro panel has , are not mapped at all , like color boost , midtone detail , highlights , shadows . And of course no offset ball on the Wave . <p> So I 'd pick the micro panel over Wave . Having a fixed layout of all the color functions is more important to me than not-so-good controls for keys/windows . <p> Might be worth considering that Avid is dropping support for their panel at the end of 2017. 43971 @qwx453971 <p> Only the v1 ( Euphonix branded ) ones . The Avid branded panels will continue to be supported . Considering that all Euphonix branded " Mc Color " panels must be older than 5 years ( Avid aquired Euphonix in 2010 , I think - but I do n't know how long exactly Euphonix produced their panels with their branding ) , that 's really not that big of a deal , imho . <p> Only the v1 ( Euphonix branded ) ones . The Avid branded panels will continue to be supported . Considering that @ @ @ @ @ @ @ @ @ @ older than 5 years ( Avid aquired Euphonix in 2010 , I think - but I do n't know how long exactly Euphonix produced their panels with their branding ) , that 's really not that big of a deal , imho. 
@@44332710 @4332710/ <p> I have had the same issue in Adobe Premiere . But after a while it switched back again after i restarted the software a few times . Sounds like a bug in the driver . 43971 @qwx453971 <p> That was my hunch too . I tried a bunch of different desktop video drivers and they were either giving me the same channels results or worse still even if I swapped 3 and 4 to try and force it the same original result . <p> BMD uses SDI mapping on all outputs even HDMI . If you 're monitoring audio via HDMI the only solution is an AJA SDI to HDMI converter as it automatically does the swap of LFE and C channels . BMD converter does not . <p> I have submitted a support ticket to have them include channel mapping in the drivers , but thus far to no avail . <p> This is good info to know , thank you ! I am monitoring via HDMI on the Decklink card . So I 'm clear are you saying that audio receivers expect L , R , @ @ @ @ @ @ @ @ @ @ my the Kona card was getting it right is because it swaps the C and LFE before it hits the receiver ? <p> I 'm I were to get an AJA SDI to hdmi adapter would I be able to monitor 5.1 audio over SDI ? <p> Agreed manual mapping in the desktop video app would be ideal . <p> I 've posted this same issue on the BMD site hopefully it will get noticed . <p> I ran into this same problem when I was building my bay back in 2015 . I got a hold of a BM engineer and basically got a that 's good to know but it is not anywhere in our roadmap to address . For some reason most blackmagic products output HDMI in L , R , LFE , C , LS , RS . This is also a problem for all of their converter boxes . If you get an AJA converter box it swaps the channels properly . <p> Also I got a tip from the SpectraCal people back in the day ( though not sure if this is still accurate @ @ @ @ @ @ @ @ @ @ give a truer video signal especially in regards to white/superwhite and black/superblack . So IMO it is a double win to grab one of those AJA boxes ( well minus the price ) . <p> It seems to me that all they have to do is to provide a software setup panel in Desktop Video for the audio output mapping scheme , and just make sure L , C , R , Ls , Rs , and LFE all go to the right place . This is totally doable . The alternative would be to just manually swap the cables once it comes out of the amp and send it to the correct speaker . <p> There are HDMI Audio De-Embedders out there that would give you this flexibility , and they 're not that expensive . The HD-SDI versions are more money and potentially higher quality . I do n't dispute that it 's a pain in the ass to do , but if this problem comes up daily , then I think this workaround would be the way to go . <p> It seems to me that all @ @ @ @ @ @ @ @ @ @ panel in Desktop Video for the audio output mapping scheme , and just make sure L , C , R , Ls , Rs , and LFE all go to the right place . This is totally doable . The alternative would be to just manually swap the cables once it comes out of the amp and send it to the correct speaker . <p> There are HDMI Audio De-Embedders out there that would give you this flexibility , and they 're not that expensive . The HD-SDI versions are more money and potentially higher quality . I do n't dispute that it 's a pain in the ass to do , but if this problem comes up daily , then I think this workaround would be the way to go . 43971 @qwx453971 <p> It may not be immediately obvious , but this functionality is already present in the Fairlight page of DaVinci 14 , although it is not yet operational : <p> Open Fairlight/Buss Format Click on Main , and then in the Format column select 5.1 and click OK <p> Now open Fairlight/Patch Input/Output In the Audio @ @ @ @ @ @ @ @ @ @ Destination select Audio Outputs ( You will only see two default outputs ( L&amp;R ) because surround is not yet implemented ) <p> Now select any source on the left and any destination on the right and click Patch . That 's how you build your surround monitor setup . <p> As a primarily audio person I am immediately familiar with the Fairlight audio workflow which is far superior to any other video program because it was built by audio people and not by computer programmers . <p> Fairlight page will hopefully include an audio delay option so that we could use separate audio card and not have to deal with this HDMI/SDI audio workaround PITA . <p> Audio patching minus an audio delay is already possible in the edit page . It can be done via the audio mixer . <p> The bigger issue is for applications that only use BMD cards for video and audio output and do n't allow audio patching . <p> What I have n't tested is wether or not the audio swapping in the edit page gets rendered into the file . I 'm @ @ @ @ @ @ @ @ @ @ for viewing . <p> The audio de-embedder is an interesting idea but I do n't see how it will helpful as it does n't give you control of re-patching the audio out . <p> The only real options I see are to either get a Aja Hi5 converter or run audio through a USB audio interface . All of these workarounds to fix a shortcoming on the decklink cards . I guess in the end Blackmagic cards and Aja cards are n't that different in price . <p> The bigger issue is for applications that only use BMD cards for video and audio output and do n't have a way of allowing audio patching. 43971 @qwx453971 <p> I agree , this is a kludge . The problem of audio assignments goes back to the tape days , particularly with HDCam-SR , which had potentially 12 separate digital audio tracks . It seemed like every studio and network had different standards , so some were LCR+LsRs+LFE , some were LRC+LsRs+LFE , while others wanted LtRt , then the surrounds , and still others wanted LtRt , then an M&amp;E mono , @ @ @ @ @ @ @ @ @ @ <p> Monitoring , we just solved in the control room with patching and routing . Deliverables are harder . <p> Audio patching minus an audio delay is already possible in the edit page . It can be done via the audio mixer . <p> The bigger issue is for applications that only use BMD cards for video and audio output and do n't allow audio patching . <p> What I have n't tested is wether or not the audio swapping in the edit page gets rendered into the file . I 'm guessing it does and that it should only be done for viewing . <p> The audio de-embedder is an interesting idea but I do n't see how it will helpful as it does n't give you control of re-patching the audio out . <p> The only real options I see are to either get a Aja Hi5 converter or run audio through a USB audio interface . All of these workarounds to fix a shortcoming on the decklink cards . I guess in the end Blackmagic cards and Aja cards are n't that different in price . 43971 @ @ @ @ @ @ @ @ @ @ patching on the edit page of Resolve 12.5 only allows to patch source tracks to stereo or mono buses which are directly connected to outputs for monitoring . What you set here is rendered into the file , as you are correctly presuming . <p> This is very different from Fairlight audio which adds another stage of patching between internal buses and physical outputs . There you can route individual mix channels to correct speakers and this will not affect the rendering . <p> On Fairlight page you can already create multiple internal buses like 5.1 SMPTE or Film , LCRS etc. which are saved with the project . Once we can also save user Patch In/Out presets , no speaker patching will be required if a different project format is loaded . <p> An external audio interface is not an option here because of ( variable ) delay between audio and video . I think for the moment the best solution would be a multichannel analog audio deembedder and a cheap analog half normalized patchbay . This would also work with other applications using BMD card . 
@@44332711 @4332711/ <h> Epic Dragon Noise ( Skin Tone vs Low Light OLPF ) <p> I 'm a bit of a newb to color grading . I 'm on Resolve 12.5 Studio and looking at my epic 6k files with the difference of OLPF . I just swapped the Skin Tone for the Low Light OLPF . I 'm curious people 's thoughts of the amount of noise between the 2 . I 'm pretty new to working with dragon footage . <p> I tend to see a bit of improvement on the Low Light but not a huge amount . I 'm at ISO 1000 on these r3d files . The skin tone had no recent Black Calibration , but the Low light does . Also ( and I know this is big ) , I 'm at 17:1 compression . Trying to see what I can get out of the sensor at 6K 100fps on my media cards and that 's the lowest they can go . <p> Any info or thoughts would be great . I 'm just trying to figure out if its a bit too noisy or @ @ @ @ @ @ @ @ @ @ at 100fps ) and I 'm heavily de-noising the footage . Trying to figure out if there might be something wrong with my sensor or if this is as good as it gets on dragon sensor . <p> I like a lot of things about the Red system , but the Dragon sensor did n't reduce noise nearly as much as people claimed it would . <p> The Get Down was shot on Red ( if you have n't seen it yet , holy crap I recommend the hell out of it ) and the DP shot almost everything at 320 ISO . Shoot Red at 320 or 400 ISO . It looks fantastic if you treat it at that speed . And if you 're shooting at a low Kelvin , like 3200 , you really do n't want to strarve the camera of light ; the blue channel gets noisey to hell and back and makes everything look blah . <p> Alexa footage is also a bit noisey in the lows , but somehow , the noise is n't as jarring . It 's more film grainey , @ @ @ @ @ @ @ @ @ @ Also , the blackshade works independent of OLPF . It 's based on the temperature of the sensor , and how that affects the photo ---&gt; electrical signal conversion . <p> Also also , 17:1 is a hell of a compression . God help you if you 're trying to shoot splashing water or rustling leaves . <p> The skintone olpf blocks quite a bit of light hitting the sensor , I think about 1.5 stops compared to the LLO . The camera is compensating for that by offsetting the ISO values , so 800 ISO with skintone is really around 2000 ISO , when compared to the LLO . <p> Thanks for the feedback guys . I do hear a very mixed bag about the Dragon Sensor . Project wise , I do a lot of documentary type shoots where I ca n't always control the light . I was on a Sony F5 which is crisp and clean at 2000iso , but I just hated the " broadcast " feel of the sensor . I was hoping I 'd be able to push the dragon more into that @ @ @ @ @ @ @ @ @ @ . <p> I did find this thread where Patrick describes the dragon @ 2000iso + Skin Tone Filter as he describes " GOLD " . wow . I 'd love to look under the hood and see what he 's doing to the footage to clean it up because that combo for me has been pretty dang rubbish ! <p> Thanks for the feedback guys . I do hear a very mixed bag about the Dragon Sensor . Project wise , I do a lot of documentary type shoots where I ca n't always control the light . I was on a Sony F5 which is crisp and clean at 2000iso , but I just hated the " broadcast " feel of the sensor . I was hoping I 'd be able to push the dragon more into that range , but mostly I hear that it wont work . <p> I did find this thread where Patrick describes the dragon @ 2000iso + Skin Tone Filter as he describes " GOLD " . wow . I 'd love to look under the hood and see what he 's doing @ @ @ @ @ @ @ @ @ @ for me has been pretty dang rubbish ! <p> Also also , 17:1 is a hell of a compression . God help you if you 're trying to shoot splashing water or rustling leaves . 43971 @qwx453971 <p> REDCODE is an I-frame compression scheme . It is the amount of high frequency detail in an image which makes it hard to compress . It makes no difference if things are static or in motion . If anything , motion will potentially introduce motion blur , which reduces high frequency detail . <p> I have heard a number of people say that motion such as rustling leaves can cause problems for REDCODE at high compression ratios . It 's long GOP codecs like h.264 which fall apart with rippling water and leaves . 17:1 REDCODE will either compress an image of a tree well or not , depending on the detail . Motion is irrelevant . <p> REDCODE is an I-frame compression scheme . It is the amount of high frequency detail in an image which makes it hard to compress . It makes no difference if things are static or @ @ @ @ @ @ @ @ @ @ motion blur , which reduces high frequency detail . <p> I have heard a number of people say that motion such as rustling leaves can cause problems for REDCODE at high compression ratios . It 's long GOP codecs like h.264 which fall apart with rippling water and leaves . 17:1 REDCODE will either compress an image of a tree well or not , depending on the detail . Motion is irrelevant . 43971 @qwx453971 <p> Yes , but the individual frames for the situations I indicate have a lot of small intricate details , and are common enough shots that it 's likely to be one of the first places where someone would notice the limits of the compression . <p> Yes , but the individual frames for the situations I indicate have a lot of small intricate details , and are common enough shots that it 's likely to be one of the first places where someone would notice the limits of the compression . 43971 @qwx453971 <p> I realise that , but then it is sufficient to say " water " and " leaves " . Adding @ @ @ @ @ @ @ @ @ @ be taken to suggest that the motion is significant . <p> I heard the same examples used in a REDUCATION presentation , and I know people came away from that thinking incorrectly that motion was a factor . <p> I know I 've heard the water/leaves discussion as well . I think where things break down is , as a DP , how do I tell a producer or director " sorry that has leaves in it , I ca n't shoot it . The sensor cant handle it .... Sorry I cant capture those waves etc .... " They 'll look at me like I 'm nuts . Then they 'll ask , " how come David Fincher , Roger Deakins , and any number of digital productions can ? " <p> I know I 've heard the water/leaves discussion as well . I think where things break down is , as a DP , how do I tell a producer or director " sorry that has leaves in it , I ca n't shoot it . The sensor cant handle it .... Sorry I cant capture those waves @ @ @ @ @ @ @ @ @ @ 'm nuts . Then they 'll ask , " how come David Fincher , Roger Deakins , and any number of digital productions can ? " 43971 @qwx453971 <p> well they shot at 3:1 , 5:1 not at 17:1 .. that 's a very different place to be ( plus deakins is mr Alexa ) <p> i don , t know why you would want to shoot at 17:1 .. normally you want sit in the 5:1 to 8:1 zone for broadcast above that ....... kenny logins <p> i think the hobbit was 3:1 ........... 17:1 is going to give you all sort of fun in post <p> Those are also people that understand the technical limitations AND have the budget and time to get the gear in place to circumvent a lot of the disadvantages . If they 're shooting a compression ratio that high , I doubt they 're shooting 800 or 1000 ISO or higher . The combination of Noise and Compression Artifacts is not pleasant . <p> Also , they lower the resolution . " Hey , for this one setup we need to shoot 100FPS . @ @ @ @ @ @ @ @ @ @ <p> In other words , if you 're saying " I want to shoot in low light , AND at a high compression ratio , AND at a high framerate , AND at the highest resolution , BUT I do n't want it to look bad " well , that 's not really fair to the camera . You 're stretching it in ALL of the possible directions , and refusing to give any ground to help it . And then wondering why it 's not performing magic for you . <p> " But I ca n't turn to my producer on the day and say we have to sacrifice one parameter to stretch another . " Right . Which is why you go over this stuff in PrePro . " If we shoot at 100FPS , we need to go down in resolution . " The issue is n't that you have to do so , the issue is that it feels like a surprise . So do n't make it a surprise . Have the ammo to say " We discussed this a month ago , and you @ @ @ @ @ @ @ @ @ @ 're shooting this shot . " <p> slow mo is a different thing altogether and it 's all matter of trade off as Nick puts it really well in his post above <p> if your shooting on a dragon and you want to do 6k you need to invest in the mini mags we kit each camera with 4 x 512 for a day shoot that will just about give you 50 mins safe recording at 6k HD 5:1 @ 25p <p> we do n't often need to recycle cards but they are off load the moment they come out the camera so we can if we need to <p> that 's for normal speed stuff <p> for slow mo that is a discussion before the shoot with testing and explaining but we normally end up on 75fps 6k @ 12:1 which is mid ground between too much compression and reasonable slow mo <p> or we can come down in frame size go 4k and get 120 fps at 8:1 etc etc <p> so it all a prep conversation and managing expectation so every one know what to expect ... and @ @ @ @ @ @ @ @ @ @ and post ... make sure i do n't crap on the post boys by giving them rubbish to work with <p> as that could be me on another job <p> there is not a real need for 3:1 unless for doing any thing effects heavy .... i have done green screen with it but .. i 'm not convinced i need to 
@@44332712 @4332712/ <h> HDR vs SDR <p> Is HDR basically SDR + some extreme whites as was perhaps suggested in some other topic or does it require a completely different way of balancing the tonal ranges over the available spectrum ? E.g. is it an afterthought above SDR grading or much more than that ? <p> My understanding is that mapping a video onto SDR is always a compromise , we must somehow project the full dynamic range onto a limited display range . Enter HDR and we have more options , still a compromise but less of it . <p> Should there eventually be a standard " Graded in HDR " and from that we make an SDR version if required or is SDR the starting point and make " Enhanced for HDR " versions ? <p> What seems appealing about the BBC 's Hybrid Log Gamma is that it ( to my understanding ) is basically giving you the highlight roll off that we do for rec709 , always mapped to the correct max brightness . So if the display can do 1000nit it rolls off to 1000 and @ @ @ @ @ @ @ @ @ @ roll off as that 's it 's max . We usually have to roll many stops of highlight latitude into the top part of our rec709 signal and HLG will do this as well , with the same information that would end up as highlights on an HDR display . <p> What 's unappealing to me about HDR10 is that it just clips ( at this stage anyway ) any values above what the screen can do . The Dolby metadata is obviously the most creatively ideal because it let 's you decide those details but unfortunately cheapness usually wins out . <p> I think the actual benefit of HDR is n't that you can have bright whites but that you can have bright , saturated colours . Having even a 400 nit flame that still has colour is going to be so much more emotionally stimulating than it clipping to white at 100 nits or it just being brighter white . <p> I 'm not sure exactly what the nature of HDR10s clipping is but I imagine a sunset that is supposed to have saturated bright areas would be rather @ @ @ @ @ @ @ @ @ @ <p> Reread OP : in an ideal world we would be able to grade the HDR first and have that gracefully degrade to 100nits/rec709 for backwards compatibility , which HLG and Dolby both allow . The Dolby system is nice in that it would be more like doing a trim pass rather than trusting the encoding but if we eventually do standardise on HDR then something automatic would be good for those with 2005 plasmas . <p> What seems appealing about the BBC 's Hybrid Log Gamma is that it ( to my understanding ) is basically giving you the highlight roll off that we do for rec709 , always mapped to the correct max brightness . So if the display can do 1000nit it rolls off to 1000 and of it can do 5000 there should n't be any roll off as that 's it 's max . We usually have to roll many stops of highlight latitude into the top part of our rec709 signal and HLG will do this as well , with the same information that would end up as highlights on an HDR display . @ @ @ @ @ @ @ @ @ @ it just clips ( at this stage anyway ) any values above what the screen can do . The Dolby metadata is obviously the most creatively ideal because it let 's you decide those details but unfortunately cheapness usually wins out . <p> I think the actual benefit of HDR is n't that you can have bright whites but that you can have bright , saturated colours . Having even a 400 nit flame that still has colour is going to be so much more emotionally stimulating than it clipping to white at 100 nits or it just being brighter white . <p> I 'm not sure exactly what the nature of HDR10s clipping is but I imagine a sunset that is supposed to have saturated bright areas would be rather underwhelming partially clipped to ( very bright ) white . <p> Reread OP : in an ideal world we would be able to grade the HDR first and have that gracefully degrade to 100nits/rec709 for backwards compatibility , which HLG and Dolby both allow . The Dolby system is nice in that it would be more like doing a trim @ @ @ @ @ @ @ @ @ @ do standardise on HDR then something automatic would be good for those with 2005 plasmas. 43971 @qwx453971 <p> TV manufacturers can decide what to do with values above the peak luminance of the screen , some hard clip ( Pana ) , some roll-off ( LG OLED. 
@@44332713 @4332713/ <h> Media Player that can chase LTC Timecode <p> Does anyone know of a media player like BMD Media Express or AJA Control Room , that can sync to , and chase incoming LTC timecode ? We need to display a Quicktime of the client 's offline reference video on a plasma during grading sessions . <p> We usually use a Clipster in VTR mode set to chase LTC , but it would be nice not to tie up an expensive box like that with such a simple task . <p> Wondering if anyone knows of other options that would work for something like this . <p> We used to use Rave2k for something similar but the company that made it went out of business and it 's no longer available . Our system ended up getting repurposed into something else . It was not a cheap setup ( until they briefly open-sourced it , which is when we built our system ) , and it required a fairly powerful Linux box with a Kona3 board installed . But it worked and worked well . <p> Other options might @ @ @ @ @ @ @ @ @ @ I think some versions of the Avid can be controlled from an external source , so that might work . <p> So a simple TC window burn into the file is not suitable ? You need to be able to display a real time TC window on the client monitor whilst the edit/grade spools back and forth ? <p> PS : If all you need is a simple timecode window burn into any flavour of H.264/H.265 from most other Pro codecs , then Convert v3 will do that in your choice of several different fonts and sizes : http : **34;671;TOOLONG <p> Does anyone know of a media player like BMD Media Express or AJA Control Room , that can sync to , and chase incoming LTC timecode ? We need to display a Quicktime of the client 's offline reference video on a plasma during grading sessions . <p> This used to be a standard accessory for Pro Tools sessions where they needed video playback to chase the timecode of the Pro Tools system during a sound mix . There are offsets available so you can trim it into @ @ @ @ @ @ @ @ @ @ these in years , but the website is still up and the company seems to be in business . 
@@44332715 @4332715/ <h> Freelance : Cancellation fee <p> What do you guys do when a client cancels or reschedules 1-3 days before a session ? ( in this case the client is a post facility ) It can be frustrating because you may have been having to cancel other jobs to keep that day on the books . <p> Do you charge a cancel or rescheduling fee ? If so how much ? a percentage ? <p> I know in the case of production , DPs and sound people frequently want 50% if cancellation is 48 hours prior to the shoot , and 100% if cancellation is less than 24 hours prior . In the case of facilities , I think a lot depends on the relationship of the facility with the client . If Disney cancels 12 hours prior to a session , I do n't think most facilities would squawk ; if Joe Blow Productions cancelled , then it 'd be a problem . <p> I put up a little bit of a fight unless it 's just a rescheduling , like they had a problem with the edit @ @ @ @ @ @ @ @ @ @ hours or something . <p> I just had a client reschedule three ( ! ) times . Then they cancelled today with less than 24hrs . They 're actually insisting I charge them a kill fee . I 've never done it before but have to agree with them on this one . <p> Rick i don , t know if your a company or a lone colourist ( walking the earth like Caine from Kung fu fixing color wrongs were you find them ) <p> But The thing is make sure your paper work is in order , once they book you , fire them a booking confirmation e:mail or something official ish on a PDF that states your T+C of cancellation and payment term , do n't  go nuts on small print just make it so it really obvious what is going on and make it a matter of course in the process <p> obviously that 's in broad strokes as the law varies around the world but communicating the frame work of your employment with a client is key before you even sit down at the control surfaces @ @ @ @ @ @ @ @ @ @ this <p> The last thing any body likes is having some kind of charge drop on them at the last minute they did , nt know about .... and they are possibly slightly stress as there edit has exploded due to the fact they have upgrade to adobe cc2015 half way thru the project ...... and yet again it 's not the magic that Adobe promised <p> your milage may vary a lot with this process as we all know most of this is down to personal relationship ETC ......... but if its new Joe Blow productions , start the relationship formally and if things don , t work out later on down the line you all know were you stand ... hope that helps <p> and i think Walters rates are very fair but you need to scale according to your own rate <p> It 'd be nice to come up with a very simple Deal Memo for colorists , just to outline what we expect to get paid for a project , the number of hours spent , when payment(s) have to be made , our liability ( @ @ @ @ @ @ @ @ @ @ for credits , who 's responsible for materials , delivery formats , and so on . I 'd be curious to see what other people have been using in terms of a very loose contract . <p> It 'd be nice to come up with a very simple Deal Memo for colorists , just to outline what we expect to get paid for a project , the number of hours spent , when payment(s) have to be made , our liability ( or lack of ) , due dates , the need for credits , who 's responsible for materials , delivery formats , and so on . I 'd be curious to see what other people have been using in terms of a very loose contract . <p> It 'd be nice to come up with a very simple Deal Memo for colorists , just to outline what we expect to get paid for a project , the number of hours spent , when payment(s) have to be made , our liability ( or lack of ) , due dates , the need for credits , who 's responsible @ @ @ @ @ @ @ @ @ @ I 'd be curious to see what other people have been using in terms of a very loose contract . <p> Complete Post had an interesting one : they just copied the form that CFI used for 30 years . Basically , it said if we screwed anything up , the client was S.O.L . But in truth , Complete almost always would redo jobs for free if it was our fault . Our joke was , " our biggest client is CP Redo. " 
@@44332716 @4332716/ <h> Easy DCP , Final DCP or something else ? <p> We 're getting enough requests now for DCP creation that it seems like the right time to pull the trigger on this . One option is the EasyDCP plugin for Resolve , which seems to be the least expensive way to go . On the other hand , since we 're upgrading the Resolve hardware soon , there will be a reasonably quick PC available as a standalone machine , and I 've been following FinalDCP for a while , thinking of setting up that machine for just DCP work . <p> Requirements : *2k and 4k DCP creation *2k and 4k DCP playback for QC *No need for encryption right now *Reliability . I like software I can get set up to work the way I want , and then not have to worry about . <p> Most of what we do will be pretty straightforward : indie films , feature films , no sequencing inside the DCP , just straight playback of one contiguous movie . <p> Pros and cons of each ? Any limitations on @ @ @ @ @ @ @ @ @ @ Other software we should be looking at ? <p> I think the biggest difference you get with finaldcp is that it 's way more flexible and has more options . Multiple cpl 's , just as an example , are not possible with the easydcp plugin . Finaldcp also supports subtitling now , I believe . And you can buy rendernodes and spread the workload over multiple machines in a network , if you want/need that . My average renderspeed on a 2k dcp is about 24 fps , with a 6 core i7 3930k . <p> If we have it on a standalone machine , it 'll be an i7 5930k , with 64GB of RAM and probably a GTX 980ti . If we run on the new Resolve ( which I 'd like to avoid , just to not tie up that machine more than necessary , dual Xeon 14-core E5-2660v4 with 64GB RAM and dual GTX1080 GPUs . <p> Just one thing I run into at a facility I frequent , they have the slEasyDCP plugin . The license is apparently really hard to transfer and if @ @ @ @ @ @ @ @ @ @ probably locking it to a machine you primarily want to use as a grading computer . In their case they ca n't make DCP 's when someone is grading and they ca n't prep sessions when rendering DCPs . Apart from the ' coolness ' of being able to render straight from Resolve and making longer render batches , I do n't really truly see the plus side . <p> Being a bit cheap and coming from an IT background I usually work with either OpenDCP or DCP-O-Matic . Granted they 're a bit slower , but they do n't have the insane maintenance costs the other ones have and DCP-O-Matic really seems to be maturing . What I 'm missing is a good player that can do KDM 's so I might have to bend over and buy either the EasyDCP , FinalDCP or NeoDCP player . <p> Also , something that can save a lot of time , finaldcp has the ability to change just the audio ( or just some other component ) of a dcp without having to re-render everything . Things like that happen more often @ @ @ @ @ @ @ @ @ @ swiss army knife . What I really do n't like about Finaldcp is the pricing model . The purchase is quite high and the development quite slow , so when after two years you finally see some new and usefull features in the latest release , your contract has expired and you need to pay again . <p> Yeah I noticed the FinalDCP suite has some really good features but those high annual fees just do n't make sense to me . The software is pricey AND the maintenance is pricey . It does one thing ( well , by your reports ) and it 's very important but it seems a bit off . <p> Repacking without re-rendering is absolutely important and relatively easy to do when doing it manually ( even if you 're replacing part of the image sequence ) . So the fact that you can hardly do anything from the DaVinci plugin makes that just not an option IMHO . <p> If we have it on a standalone machine , it 'll be an i7 5930k , with 64GB of RAM and probably a GTX @ @ @ @ @ @ @ @ @ @ which I 'd like to avoid , just to not tie up that machine more than necessary , dual Xeon 14-core E5-2660v4 with 64GB RAM and dual GTX1080 GPUs . <p> Does FinalDCP play nice with DPX sequences ? 43971 @qwx453971 <p> perry i have been using the stand alone easy DCP creator for about 3 years now and does the job pretty well , the new version has CUDA acceleration for encoding on the windows .. i have been using the mac version on a old mac 5.1 with 2 x 6 core 2.4 and its pretty much real time ish <p> we use it for features/promos/ and a lot of TVC ... i just found it really easy to set up and use , you need to read and write from separate fast SSD / or raids to get it running fulling speed <p> with the plugin version especially for TVC , S is just a bit annoying for all the annotated text you need to add in for some cinema advertising systems <p> on the standalone version you can just type it in on the plugin you @ @ @ @ @ @ @ @ @ @ the stand alone ( but you used to get a complimentary plugin lic as well so long as resolve was on the same machine ) <p> The Resolve plugin + the standalone version is a solid combination . Like Pepjin said , you will need to revise a DCP , and you ca n't do that easily with only the Resolve plugin , but I think you 'd be fine with the Resolve plugin + open source tools ( if you 're comfortable with them ) . <p> We initially bought Final DCP , had issues ( that are now fixed ) , so we bought easyDCP in the heat of battle and have n't looked back . <p> For speed , the easyDCP plugin gets close to 2x real time on my box , so not a big bottleneck for us . We have other Resolve stations , so no issues not being able keep working . <p> You can drop a DCP into a Premiere timeline to play it back .. Way cheaper/easier than any other dcp player options . <p> If you 're linux savvy , look @ @ @ @ @ @ @ @ @ @ Wow I just tried that Premiere thing . I had no idea , it works perfectly ! Saves unpacking just to verify minor issues . It does n't really read the CPL but I 've been using dcpinspect for integrity checks for a while now . <p> Wow I just tried that Premiere thing . I had no idea , it works perfectly ! Saves unpacking just to verify minor issues . It does n't really read the CPL but I 've been using dcpinspect for integrity checks for a while now . 43971 @qwx453971 <p> Yep , it 's not a substitute by any means for a lot of qc tasks , but beats the heck out of other ways to check a DCP for picture/audio issues . <p> I 'm using EasyDCP . As cheesy as the name sounds it 's a very straightforward process . I do like that the developers for EasyDCP actually do a lot of work for DCI - so in terms of specifications they are spot on . <p> I 'm not actually using EasyDCP for jpeg2000 compression these days . I export @ @ @ @ @ @ @ @ @ @ I need to tweak something I can just select the frame range and overwrite those frames and repack in EasyDCP . <p> I 'm sure it is. but we have all our important machines on a 40GbE network . So having the DCP creation tool on a separate machine means not having to tie up the Resolve system while we work on something else . There are usually 3-4 projects in progress around here , so a couple hours of render time for a feature film means a couple hours the Resolve machine ca n't be used for color ... <p> I am surprised no one uses Doremi , now Dolby CineAsset . It is effective as EasyDCP and allows you to pull apart , and put back together a DCP . Build Multi CPL , and Supplement versions , KDMs , QC tools , Player with Blackmagic card support . And , its created by the same development team as the bulk of the DCI-players in the world are . DCP2000/IMS1000/IMS2000 etc ... So it 's incredibly reliable in terms of making certified DCPs . Also supports AUX-track , @ @ @ @ @ @ @ @ @ @ Kakadu J2K codec , so I expect it of equivalent speed ... <p> Just on that , Those still using OpenJPEG tools , in my experience , the more modern and continually developed encoders , like **26;707;TOOLONG do better quality per bit . So keep that in mind . <p> I use it in my studio ( www.finishingroom.com.au ) , but we also run EVERYTHING we master through a real DCI projector and IMS2000 for QC .. Make sure the Graded master matches the DCP master . Also when doing the HI/VI and OCAP and CCAP . <p> But then again , I have been working with Kakadu and trying to get Chamberlain/Resolve to implement the Kakadu J2K encoder . My personal preference is to master directly out of Resolve to J2K ( All colour accountability based on that render ) Then from there use your DCP creation tool of choice , EasyDCP , CeinAsset etc , to construct the DCP with all the Audio tracks , Atmos , Subtitles , Captions etc .. <p> Do me a favour , Email Peter and ask him where the Kakadu J2K encoder @ @ @ @ @ @ @ @ @ @ have to drop it in . <p> Finally , I want to also remind everyone that this kind of workflow will be very common in the coming years as IMF , based on DCP , becomes the defacto standard for deliverables . It is now we need to consider this development and the " better " workflows to take considering this . <p> Ie considering the developments and capabilities of IMF , and the task of " building " the IMF deliverable with all the potential capabilities , I expect an " IMF Studio " type product line to develop outside of the simple render to MediaFile from Premier/Resolve/whatever . Best we render to J2K images that will then go into the ' IMF Studio ' software of choice . That'ss going to be an interesting battleground over the next few years .. 
@@44332717 @4332717/ <h> Noise Reduction - Best Practices ? <p> Hi all ! I 've been a lurker for the most part here and have found this community to be a great resource ! <p> I 've been curious , what are the best practices for noise reduction ? I 'm currently grading an F5 S-log2 project that 's underexposed in parts , so I 'm lifting mid-tones and revealing a lot of unpleasant noise , even with bring blacks to a normal level . <p> In the past with DSLR footage ( shot flat/neutral profiles ) , I 've done noise reduction and sharpening prior to coloring , since I figure that 's when I have most of the image information still . In general , does this hold true across most scenarios ? Or are there situations where I might grade first then do noise reduction ? <p> I 'm using Da Vinci Resolve Lite 11 , so any noise reduction I 've been doing has been done in After Effects with Neat Video or Red Giant Denoiser . <p> NR and grading sort of need to go hand @ @ @ @ @ @ @ @ @ @ do an NR on uncorrected then raise it , it might reveal artifacts that you could n't see before . On the flip , NR after the grade might prevent you from pushing the image as far as you should for fear of having too much noise . <p> I have seen both sides of this argument made . I 've always said that sometimes , I squash and stretch the contrast so badly in color correction , the end result is going to be very inconsistent if they use NR on the original files . I think in general it 's a better idea to use Juan 's idea of using NR hand-in-hand with the color correction , and then maybe do a mild overall if it warrants it . <p> To tell you the truth , I 've been watching some major American network TV shows lately that clearly are not using any NR . I think they 're just accepting that some shots are noisy and grainy and that 's just the way it is . If it 's a super-slick , lush commercial , then I @ @ @ @ @ @ @ @ @ @ a grim , dark drama , then I think you can let the grainy shots go if they 're reasonably consistent within the scene . <p> With regards to noise reduction , what Juan and Marc said makes good sense . <p> The problem with noise is that it reacts very badly to being pushed in the grade , working with tools outside the grading environment makes it harder sometimes . <p> Depending on the amount of affected material maybe do a few passes with different NR characteristics so you can easily experiment in Resolve with grades on different material . Maybe even use keys or Rotos to reveal different parts of the image . <p> If you are using Spatial and Temporal processing watch for artefacts , noise is preferable to bad artifacting . <p> As with most things i find what works best for some shots is n't effective on others . I use neat video on Mistika so swapping it before or after the grade is quick and i find that its almost artifact free , just really slow to render in UHD . Sometimes working with @ @ @ @ @ @ @ @ @ @ selectively sharpen during your grade as long as you 're careful . I have n't played with what 's available on Baselight yet but will be needing to on a upcoming project <p> Before . You will have more to clean up if you do it at the end , which means potentially more artefacts , more degradation , etc . Also , you will have cleaner keys if you do it at the begining . I use neat video in Resolve . <p> Do a test yourself . Grade and then add at the begining and at the end . Then compare . <p> I 've gone both routes depending on the project . NR can be a slight " project " in its self . <p> On some projects I 'll process all footage prior to grade . This I normally do in After Effects . Thai would be noise reduction and simpler clean-ups . Beauty-work I tend to leave for after the grade . <p> It 's also imparritive to consider the whole chain . NR before grade means I prefer to add a little noise after the @ @ @ @ @ @ @ @ @ @ processed look . However , applying this on the source log files means perhaps 0.5% noise tops compared to the 2-4% one could add on a graded picture . <p> ... I have n't played with what 's available on Baselight yet but will be needing to on a upcoming project 43971 @qwx453971 <p> Hey Mark <p> One on the new additions to Baselight v5.0 is a Denoise tool . <p> If you prefer the NeatVideo 's results ( and have already paid for a NeatVideo license ) then Baselight v5.0 comes with improved NeatVideo support - including GPU processing and improvements to speed of use &amp; rendering . <p> If you prefer the NeatVideo 's results ( and have already paid for a NeatVideo license ) then Baselight v5.0 comes with improved NeatVideo support - including GPU processing and improvements to speed of use &amp; rendering . Another reason to look forward to v5.0 43971 @qwx453971 <p> That 's good to know , when i asked Filmlight about support for Neat in 5.0 the other week they were n't able to to give me a definitive , although with new @ @ @ @ @ @ @ @ @ @ feature that was still being worked out , but i 'm definitely eagerly awaiting 5.0 ! <p> Oh , and I should have included a TIP in there as well for good measure . <p> I recommend what Mark mentioned - I Denoise early &amp; Sharpen late ( in the grade process order ) . <p> If your material is LOG though then the Denoise tools may not work very effectively due to the difficulty in recognising detail in such a low contrast image . In this case I sandwich the Denoise tool between a reversible transform . REDGamma4 is quite good for this as it adds a lot of contrast but does n't clip . <p> You do n't have to undo the REDGamma4 transform of course . This is just something that I set off to render the day before the grade so that I have a ' clean ' version if I need it . As Mark said - Denoise effects can take a lot power . Like with Medium Blurs they have to evaluate every pixel on every frame - which is a lot of work . Especially at UHD res 
@@44332718 @4332718/ <h> HDR and the Human Eye <p> With the various discussion of HDR ongoing , I 've been meaning to ask a rather simple question . <p> The Human eye is very good at dealing dynamically with high contrast range scenes - say , looking around within a dark room , to then looking out the window onto a bright exterior scene . <p> This is around a 1,000,000:1 dynamic contrast ratio , or about 20 stops . <p> But , the static contrast ratio of the human eye is far poorer - often quoted at about 1000:1 , or about 10 stops . <p> This obviously has an impact on any HDR image viewed on a display that is well within the visual angle of the human eye , as any high brightness areas within the scene will effectively ' kill ' the eye 's ability to make out detail in the shadows . <p> This also goes some way to explaining why HDR display often feel as if they ' hurt ' to watch , as any scene that goes way beyond the eye 's static dynamic @ @ @ @ @ @ @ @ @ @ <p> So the question is , how will HDR really work on relatively small displays that are all will within the visual angle of the human eye ? <p> So the question is , how will HDR really work on relatively small displays that are all will within the visual angle of the human eye ? 43971 @qwx453971 <p> I 'm particularly concerned about the 4000-nit displays , which I think a ) could be painful to watch , b ) could have all kinds of issues with power consumption and brightness consistency , and c ) I question as to how the increase in brightness will affect display lifespan . I think the 1000-nit displays are doable . <p> The 100-nit theatrical projection displays are watchable and , though they 're initially very bright when you first see them , you get used to it in about 5-10 minutes . There was nothing in the 2 features I saw in HDR that caused any discomfort -- though I was very aware of the beams of light over my head in the theater , and normally I do n't notice @ @ @ @ @ @ @ @ @ @ it " to such an extent that you are not aware of it being any different from standard projection ? I have not had an opportunity to see HDR projection , so ca n't say . 43971 @qwx453971 <p> Yeah , 100 nits on a 50-60 ' screen is not that bad in a pitch black room . You do initially notice it , but it 's not at retina-searing levels . It certainly was n't fatiguing , but I have inside word from at least two people who have seen 4000-nit displays that it was much , much too tiring to look at for more than an hour at a time , and they needed frequent breaks to get through sessions . I do n't know enough about the specifics to say how HDR was being used , and I concede that if you only used it for highlights , the HDR display would n't necessarily look gigantically different from a traditional Rec709 display . <p> Yeah , 100 nits on a 50-60 ' screen is not that bad in a pitch black room . You do initially notice @ @ @ @ @ @ @ @ @ @ It certainly was n't fatiguing , but I have inside word from at least two people who have seen 4000-nit displays that it was much , much too tiring to look at for more than an hour at a time , and they needed frequent breaks to get through sessions . I do n't know enough about the specifics to say how HDR was being used , and I concede that if you only used it for highlights , the HDR display would n't necessarily look gigantically different from a traditional Rec709 display . 43971 @qwx453971 <p> I 'd totally agree that 4000nits is hard for the eyes . Sadly the one day I had a chance to sit in a grading at a 4000 nits monitor I additionally had some headache which made the experience worse . And the material was on purpose graded to not only use the HDR range for highlights but had some skin tones at 1500 nits or so in a very bright scene . This demo sequence was only 10 minutes which was watchable when we did a review of the whole sequence after @ @ @ @ @ @ @ @ @ @ 10 minutes in one go . <p> I 'd like to - again - cite a sentence ( freely cited from my memory ) said by one of the presenters of the ILM/Pixar HDR talk at IBC this year : " With great brightness comes great responsibility . " If the thousands of nits room is mostly used for some specular highlights it works pretty well . The more scientific test approach we had with the goal " Test and show what is doable even if it may not be usable for film " was a different thing . <p> While at IBC I also visited a demo screening at the Dolby Cinema in Hilversum . Sadly they only showed a 5 minute sequence of Tomorrowland in HDR Dolby Cinema mastering before showing a regular graded 3D movie - I mean who likes 3D anyways ? But the 5 minute demo was totally awesome and showed some of the potential with a night time scene which was n't total dark and flat but with the switch to Tomorrowland blow you away with a bright and contrasty world as a visual @ @ @ @ @ @ @ @ @ @ resulting contrasts the Dolby Cinema kills it . <p> This is around a 1,000,000:1 dynamic contrast ratio , or about 20 stops . But , the static contrast ratio of the human eye is far poorer - often quoted at about 100:1 , or about 6.5 stops . 43971 @qwx453971 <p> I still question that . I can look out a window in dark room and keep my focus locked in one area outside , but in my peripheral vision I can still clearly make out detail inside the room . As in , it 's not underexposed or as if i 'm only seeing 6.5 stops . Or go the other way and focus on inside while keeping the window within your field of view , the outside window does n't blow out or lose detail . Perhaps it has to do with the eyes never being completely still with micro movement so what we practically see even with " fixed focus " is still actually the dynamic range contrast of closer to 20 stops ? Maybe the static ratio IS 6.5 stops , but we NEVER perceive that @ @ @ @ @ @ @ @ @ @ 's even relevant . <p> In essence , it 's my experience I CAN maintain static focus on a high dynamic range scene and still perceive a wide range . More than any camera I 've used . <p> I still question that . I can look out a window in dark room and keep my focus locked in one area outside , but in my peripheral vision I can still clearly make out detail inside the room . As in , it 's not underexposed or as if i 'm only seeing 6.5 stops . Or go the other way and focus on inside while keeping the window within your field of view , the outside window does n't blow out or lose detail . 43971 @qwx453971 <p> My take is that you 're both right . I think the actual contrast range of the human eye is less than we think it is , but the eye/brain connection is so strong , the brain is able to correct for the exposure problems in such a way that you ca n't tell . <p> Wolfcrow ( if you can @ @ @ @ @ @ @ @ @ @ a contrast range of 20 stops , which to me is probably closer to real life : <p> I think it 's a little apples and oranges . Our human vision is so much different from any screen and camera/sensor technology . <p> Our brain does things to help us recognise and understand what our eyes see . It completes missing parts like dynamic range our eyes ca n't adapt to at once ... <p> Latest screen/sensor technology now allows to replicate real world conditions in the cinema and homes soon . This shifts the need to decide what is important more and more away from the actual shooting to post production and/ or the viewer . <p> As a camera person , I always liked that decisions are made on set to help the viewer understand and support the story . I fear that these decisions now can made later in the process of filmmaking , there will be to many cooks in the kitchen . <p> Do n't get me wrong . I think HDR as a screen technology is very welcome for me generally . But I @ @ @ @ @ @ @ @ @ @ because it helps the story . <p> Our eyes need support on the screen . At least up to a certain point . Personally , I like blown highlights in windows where there is no important information and dark shadows where our eyes can hardly find any information as much as I like HDR screen technology for the beautiness it can deliver . <p> Do n't get me wrong . I think HDR as a screen technology is very welcome for me generally . But I think people will use it because it exists and not because it helps the story . 43971 @qwx453971 <p> I was very skeptical about HDR until I saw two theatrical features using it within a 2-week period and liked what I saw . I do n't think it 's a gimmick on the order of 3D , and I think any movie can use HDR and still tell the story just fine . I 'm totally against the idea of doing any movie in 3D , particularly intimate human dramas , romances , comedies , documentaries , and so on . But HDR works @ @ @ @ @ @ @ @ @ @ need support on the screen . At least up to a certain point . Personally , I like blown highlights in windows where there is no important information and dark shadows where our eyes can hardly find any information as much as I like HDR screen technology for the beautiness it can deliver . 43971 @qwx453971 <p> I generally try to talk the client out of blown highlights if I can , simply because that 's not what I see with my eyes in real life or on film negative printed to film prints . There 's always a skosh of detail in the highlights , even when it 's 5-6 stops over the foreground . But at the same time , I have no problem giving a DP blown-out highlights if they insist on it , or if there 's a good reason to do so . I 've blurred highlights or slammed them when the window gives away a location problem ( like seeing a 2015 skyline in a period film ) , and there 's no time to replace the window with VFX . <p> HDR is @ @ @ @ @ @ @ @ @ @ color-correction . One thing on which I do n't agree with Dolby is the idea that a single master could work for three different kinds of HDR plus regular Rec709 . I think there 's far too much tweakiness going on where the client is going to want to control this dynamic range very carefully , and I do n't think metadata or a LUT alone can constrain it . I do think it would be a short trim pass ( maybe one day ) , similar to what we have to do with D-Cinema vs. home video . <p> The issue is the size of the display/scene . Using peripheral vision is not the same as having a display that is fully within your direct angle of immediate vision . That is what separates the human eye Dynamic CR from Static . <p> Another thought is likening HDR to depth of field . Why do I need to potentially be able to see all the DR in detail , when the action the story is focused on is within an isolated segment of the potential CR ? It is @ @ @ @ @ @ @ @ @ @ field - where are the ' cues ' as to where to look ? <p> I 'm not saying one is right/wrong . Just pointing out the difference to the viewer 's perception . <p> I still question that . I can look out a window in dark room and keep my focus locked in one area outside , but in my peripheral vision I can still clearly make out detail inside the room . As in , it 's not underexposed or as if i 'm only seeing 6.5 stops . Or go the other way and focus on inside while keeping the window within your field of view , the outside window does n't blow out or lose detail . Perhaps it has to do with the eyes never being completely still with micro movement so what we practically see even with " fixed focus " is still actually the dynamic range contrast of closer to 20 stops ? Maybe the static ratio IS 6.5 stops , but we NEVER perceive that in real world terms so I 'm still skeptical it 's even relevant . In essence @ @ @ @ @ @ @ @ @ @ on a high dynamic range scene and still perceive a wide range . More than any camera I 've used . 43971 @qwx453971 <p> More directly , I can look out a window and see a colorful scene and then redirect my view to the room interior , illuminated 1/100 as much , and see its colorful scene . Peripheral vision , which is hardly colored , is not involved . Instead the view is deliberately redirected , by perhaps 45 degrees , which means that both views can appear on one HDR screen . A usual photographic image would be either greatly overexposed out the window or greatly underexposed inised the room . An HDR image can have both well exposed . <p> The pre-HDR cinematographer can select which direction -- out the window or inside the room -- to expose for . Thus , as with shallow depth-of-field , and camera movement itself , pre-HDR cinema directs the viewer 's attention ( often unnaturally ) . HDR cinema can offer both directions on the screen simultaneously . The viewer 's attention can wander back and forth . This @ @ @ @ @ @ @ @ @ @ better or for worse . <p> In an earlier strand I commented that the 100 nit projection brightness limit for HDR runs afoul of a human visual limitation , specifically of rod domination in the darker parts of the display . Thomas 's example of the window scene and inside scene is a perfect illustration of this . The outdoor window scene can be represented colorfully with a display range of 1 nit to 100 nits . The real world might have had correspondingly 30 nits to 3000 nits , but we 've adapted and can hardly tell the difference from reality . So far so good for HDR . The non-shadowy scene inside the room has in reality 0.3 nits to 30 nits , that is , 1/100 the levels of the outdoor scene , as we 're supposing . But the HDR projection will display this as 0.01 nit to 1 nit on the screen . I think that these HDR cinema colors , seen in the 0.01 nit to 1 nit range , can not correspond to the real colors seen in the 0.3 nits to 30 @ @ @ @ @ @ @ @ @ @ 's out the window it fails visually with what 's in the room . This because natural visual mechanisms will cause the 0.01 nit to 0.1 nit parts of the inside picture to be viewed somewhat off the fovea , and rod intrusion will affect the color . <p> I take 0.1 nits as the crossover point between rod discrimination and cone discrimation based on Hecht ( 1924 ) . Colorists in here can do a simple experiment to confirm this problem of HDR projection being too dim for its dynamic range . Find a color print of an interior where you judge some colors to have been given a bit too much saturation . Now view that print in a room where it occupies the 0.01 nit to 1 nit range . ( Find a corner where there 's 3 lux illumination . ) I think you will judge it very differently in that light . <p> I was able to spend a considerable amount of time with a 2000 Nit HDR display last week ... My personal conclusion is basically as I mention above - it is extremely @ @ @ @ @ @ @ @ @ @ . <p> What is interesting is that the experience was worse at what are relatively ' normal ' viewing distances - around 3 meters - while being a lot closer was less painful . This seems to be exactly as expected based on the static contrast ratio of the human eye , and the fact that when nearer the display the human eye 's dynamic contrast ratio can be utilised . <p> Steve needs to say how large the display was and what the surround luminance was . <p> It 's all about angles . The example of the dimly lit room having a window that shows a 100x brighter scene is pertinent . The glance must shift by many degrees in order to take in the 100:1 contrast in the one region rather than the other . CIE TC 1-93 ( Calculation of self-luminous neutral scale ) is examining these perceptions . Does a bright region at x- from a dim region contaminate the latter optically -- by flare or scatter in the eye -- or is the effect at the brain level ? It 's not known . @ @ @ @ @ @ @ @ @ @ , I just meant the area that was n't the tiny spot focused on . I 'm talking within 5 degrees or less . My eyes in a fixed position along the edge of window in a dark room . I obviously ca n't focus on both inside and outside , but they are within a tiny area of my angle of view and both are viewable in terms of dynamic range . It 's always seemed to me to be a cones/rods thing ( like a dual gain readout on the Alexa ) , so even within a small focal point my eyes will use a combo of rods and/or cones to sort out the dynamic range - then on top of they eyes will dynamically adjust with the IRIS opening and closing , just its fairly slow to react . <p> Now , that has nothing to do with if I agree that HDR is n't appealing to look at or not . <p> The approach described in this paper allows the signal to be rendered on any display ( OLED , LCD , local backlight dimming , @ @ @ @ @ @ @ @ @ @ intent , without the need for metadata and without needing to re-grade for different displays . That is , the hybrid log-gamma approach defines a signal that is independent of the display . 43971 @qwx453971 <p> The White Paper , with its " hybrid log-gamma " OETF is based on a cheat ( or else a confusion ) . The OETF switches from a log curve in is upper exposure portion to a gamma curve in its lower exposure portion because " the threshold for visibility of banding becomes higher as the image gets darker " . Here the paper refers to the DeVries-Rose Law . That law says that at low luminances the JND is proportional to the square root of the luminance . The Weber-Fechner law , basis of log encoding , says that the JND is proportional to the luminance . But what counts as " low luminance " , i.e. , where , evenapproximately , an encoding should elide from log to gamma , depends on absolute luminance . Likewise the Hecht ( 1924 ) curve shows the deviation from the Weber-Fechner Law for absolute , @ @ @ @ @ @ @ @ @ @ of the hybrid log-gamma OETF is very much dependent on the **30;735;TOOLONG . <p> Compare the curve in Figure 1 of Hecht 1924 with the Schreiber curve in Figure 3 of the White Paper . The left ( low luminance ) halves of the curves are alike . Schreiber , like a good engineer , has flattened the right half of his curve into the perfect Weber-Fechner Law . Hecht ( 1924 ) shows it having a brief flat and then rising to the right , as it must , considering the physiology . <p> The red curve in White Paper Figure 3 shows the gradation ( Weber fraction ) achieved for a 0.01-to-2000 nit display when using the OETF and EOTF recommended by the White Paper . ( Note that the system gamma is 1.5 , so this represents just 11.7 stops of exposure . ) The display is free from banding because it lies below the Schreiber curve . If this had been a theatrical HDR projection , with just 100 nit max , the red curve would be slid far to the left . That 's fine @ @ @ @ @ @ @ @ @ @ what if the White Paper 's technique were used for a display outputting more than 2000 nits ? Then the red curve must be slid to the right and it violates the Schreiber curve : the displayed image is prone to banding . Here is where the absoluteness of the Schreiber curve ( or Hecht demand , as I 'd been calling it ) meets the pretence of pure relativity of the " hybrid log-gamma " OETF 's . Hybrid log-gamma makes it to 2000 nit display by the skin of its teeth ( and using full range 10-bit , not the legal-range used on page 2 ) . Thus absolute display luminance matters for video encoding of luminance . <p> Absolute display luminance also matters for color when it becomes so low that color perception is affected by rod-intrusion . This wo n't happen for HDR television but will happen for theatrically projected large screen HDR . <p> Thus the construction of the hybrid log-gamma OETF is very much dependent on the **30;767;TOOLONG . DC 43971 @qwx453971 <p> Hi Dennis , I believe you and Steve are actually saying @ @ @ @ @ @ @ @ @ @ be to rephrase the sentence to state : " Thus the construction of the hybrid log-gamma OETF is very much dependent on the absolute luminance of a display . " That is what make it relative so to speak as it is relative to any given display 's luminance as opposed to ST2084 's use of an absolute 10,000cd/m2 . If we say in THE display it could be misunderstood to mean one ' idealized ' display al-a-ST2084 . <p> I know this seems like a semantic difference at best , but it is important because Steve 's use of the words relative ( BBC/NHK ) and absolute ( ST2084 ) are fully consistent to how they are used in both the SMPTE and BBC documents in question . If we choose to ignore that very consistent use of language in the two most prominent HDR proposals we risk creating unnecessary confusion . <p> Page 6 of the somewhat older WHP 283 from the BBC is highly informative in this respect and worth a read as it deals specifically with this notion of relative vs. absolute in the way I @ @ @ @ @ @ @ @ @ @ to in his post . <p> Let 's not get bogged down in semantics . There 's a substantive problem with the BBC hybrid log-gamma encoding . <p> E ' =0.5*sqrt(E) when E is in 0,1 E ' **39;799;TOOLONG when E is in ( 1,12 43971 @qwx453971 <p> The problem is that video shot with this encoding and displayed in dark surround ( with overall gamma = 1.5 ) with a device showing more than 2000 nits runs afoul of BBC 's own anti-banding criterion : that the Weber fraction be less than the Schreiber value . This is evident in BBC Figure 3 where the red curve for a 2000 nit display almost touches the Schreiber curve . The Schreiber curve summarizes the luminance discriminations of the eye . It 's given as a fixed fact . The red curve is for one particular display situation . A display capable of 10000 nits also viewed with overall gamma 1.5 would have a different red curve . Its red curve would be like the one shown in Figure 3 except displaced well to the right , crashing through the Schreiber @ @ @ @ @ @ @ @ @ @ absolute " or " relative " . <p> I do n't know if the 2000 nit limitation due to the BBC encoding is a practical disadvantage , but it was shoddy of them not to mention it , and dull of them not to explain how the nit level of the display figures into the construction of their OETF . Since SMPTE designed an EOTF for 10000 nit display , someone wants more than 2000 nits . <p> It 's really in the EOTF , not in the OETF , that anti-banding criteria come into play . When a signal is already encoded , digitized , its steps either are or are not fine enough that the corresponding steps in output luminance are indiscriminable . I have n't studied the SMPTE EOTF , but it is notcrazy that they made it for a definite 10000 nit display . For high luminance display , the EOTF must be done right or you run out of bits ( steps ) . <p> Figure 1 from Hecht ( 1924 ) shows the psychophysical facts somewhat better than the Schreiber curve does . Notice @ @ @ @ @ @ @ @ @ @ at around log(luminance)=2.5 . The small rise from log(luminance)=3 to log(luminance)=4 is actually a factor of 2 -- the coding demand at 10000 nits is just half as great as at 1000 nits . A smart EOTF for very high luminance display could exploit this . Did SMPTE 's ? <p> BBC , in its televisionish way , gives an OETF . Then it practically prescribes the consruction of an EOTF in order to reach display ( or there would be no red curve in its Figure 3 ) . I prefer a strictly EOTF approach that leaves the videomaker fancy free to make his work while sure how it will be displayed . ( DCI is an example of this approach . ) <p> BBC 's construction of the EOTF is fascinating as it detours back through the inverse OETF to the scene and there establishes a kind of Y from the R , G , B , while presuming a final color space , and then applies a simple gamma to just the Y , " thereby maximising backward compatibility " ( ! ) , and then reconstructs @ @ @ @ @ @ @ @ @ @ color from luminance and contrast is not how modern colorists are used to working , but has some use . 
@@44332719 @4332719/ <h> DispcalGUI now works with Resolve <p> So , I was just playing with some new probes and I stumbled into LONG ... LUT creation workflow for Resolve/ Apparently DispcalGUI now works with Resolve in the same way the CalMan and Lightspace do . Of coarse I had to try it and sure enough , it does work ! DispcalGUI uses the CalMan setting in Resolve . And it even works on a Mac , so there is no longer need for the Windows emulation or the second Windows computer to run the calibration software . <p> Been also playing around with it and could n't get reliable results . My problem was that each run ( with i1d3 probe ) some of the patches were n't measured well and that threw off the whole profile as a result . The other reason I found it unreliable has to do more with myself as a user and the challenging high amount of options available . Maybe I 'm not tech savy enough to understand them all but there sure are many of them . And the feeling of ' @ @ @ @ @ @ @ @ @ @ combine well with something as precise as display calibration . <p> Obviously this is not for faint of heart . It can take some time to figure out basics of operation . But for someone just starting out and willing to spend some time studying it , this certainly is better , than having no calibration at all because they ca n't afford the calibration software . I do n't think anyone is hardly complaining , when free software does something flaky . After all it 's free <p> Argyll does n't ( necessarily ) have it 's own " color science " as everything they do is ICC based , but long story short I 've actually seen reports from peeps that showed good results ( compared to many that struggle ) . These users unfortunately never ran large validation patch sets , so it 's hard to tell what is really going on ... <p> But just a heads up : the amount of work to get anywhere near good results is quite a bit ( at least it was for these folks ) ... and again , @ @ @ @ @ @ @ @ @ @ calibration thoroughly to actually know how good or bad your screen performs now ... these quick check sets give you an indication at best . <p> ICC is what it is and has limitations - not saying it can not be used for " calibrations " - I would not use it ( other than for Notebooks ) , but that decision is up to you . The point was that Argyll does not have it 's own color science - it is ICC based . <p> Calman , Chromapure , Lightspace all have their own color science and that is one thing that you get when you pay for these premium solutions . <p> Being a free program I think Argyll is a great gift from Graeme . <p> Folks who have actually evaluated all available solutions know that there can be a huge difference between results of these apps ... useful to share here on a Pro forum with colorists looking to calibrate their Pro equipment for broadcast / film application . <p> as in : <p> just because ur dE numbers look good does not ( necessarily @ @ @ @ @ @ @ @ @ @ important to understand ) <p> Mike , just because you say , that ICC is not good enough for anything other than notebooks is just your opinion . Just as Graeme expressed his Not interested in getting into a pissing match Mike . You 're happy with your calibration tools , more power to you . I 'm happy with mine ... The difference being , I 'm not selling anything and I have nothing to gain financially from any of the calibration tools discussed and so is Graeme <p> Following your logic , I guess it is wise for us to disregard your posts regarding color grading as clearly you are advertising your services as a colorist .... ? <p> Nobody has anything against Argyll ( u seem to be on that trip ) - and stop hiding behind Gill , he ca n't change what are FACTS . I personally would use Argyll before I use Calman for 3D LUT creation , that 's for sure . But it would be wise to understand differences in approaches and results . <p> Running a display profile does n't mean @ @ @ @ @ @ @ @ @ @ downloading Resolve does not make u a colorist <p> That 's funny , I guess by your logic Mike , I advertise my color grading services to the other colorists . Excellent What I 'm trying to say , for some reason , people selling calibration software and services are hell bent on demeaning any competing software . Why is that Mike ? When you say " Nobody has anything against Argyll " how do you reconcile that by immediately following it by " ICC is what it is and has limitations - not saying it can not be used for " calibrations " - I would not use it ( other than for Notebooks ) " ? You go out of your way to diminish all important and free work done by the open source community . You even write " calibrations " in parentheses , as using Argyll is some kind of joke . And if that was n't enough , you even managed to crap on CalMan too in the previous post ! Who 's left ? Oh , I see ... I find this very sad @ @ @ @ @ @ @ @ @ @ all of these solutions - I can say whatever I want . You out of all people should understand , you keep yapping about deficiencies in Resolve and Speedgrade over and over and over ... <p> fourth , straight up : u have very limited experience in display calibration . Very easy to see from your posts that you are arguing w/o knowing what you 're talking about . <p> Some education : <p> 3D LUTs are the best way to calibrate ANY screen . <p> But we can not use use 3D LUTs on Notebooks ( there is specialized sw , I 'm not gon na go into that ) , ergo we have to use ICC profiles which have disadvantages . That is why I said I use ICC profiles only for notebooks as we are forced to do so - clearly , u seem to have no clue about any of this . Everybody who has spent 2 mins reading about this knows this . Jake , oh boy . <p> For ICC creation , I 've personally tested in depth : Argyll CMS , basICColor , @ @ @ @ @ @ @ @ @ @ + Spaceman blows everything out of the water when it comes to ICC - hands down . It is expensive , but it delivers the very best results . Naturally , I have reports for all of these solutions , tested with the same meter same setup etc etc etc .... Jake , contrary to you I know what I 'm talking about . <p> On any other system/screen , we will use LUT boxes or built-in LUT storage to use 3D LUTs . <p> Now , there are various 3D LUT solutions ( LSCMAr ) . And yes Jake , like some other peeps here on this board and elsewhere I 've tested them all - in depth . I 've also used CP and HCFR ... Can you share your actual experience in this field ? <p> I want the best calibration results , ergo I want the best 3D LUT possible . Remember , Jake : this is for broadcast &amp; film . So a tiny bit more crucial . <p> Long story short : Light Illusion specializes in 3D LUTs . That 's all they do @ @ @ @ @ @ @ @ @ @ color engine that delivers great results in all of my calibrations . Better than the other solutions . And they got the biggest clients world wide in the entertainment industry . <p> Argyll on the other hand specializes in ICC profiles . Until a couple years ago , when Gill was asked to jump on the evolving Home Theater train and delivered 3D LUTs for madVR and then the eeColor box .... RING A BELL ? <p> Consequently they just recently added other 3D LUT formats . All based on ICC - there is no actual color science . That was the whole point . Not a problem , it 's a free program and nobody is forced to use it - AND/OR - as I said : some folks have reported good/great results . Are you actually reading and understanding any of this ? Because you keep replying that we all hate Argyll ... <p> And for the thousand time , I 'd use Argyll 's 3D LUTs over Calman 3D LUTs ( with their " color science " ) as the results are really not that good . @ @ @ @ @ @ @ @ @ @ source community or somebody trying to sell u something ... u 're like a fish on land yapping all kind of weird stuff that makes no sense at all . <p> Naturally , I do not care what you use for display calibration . If you 're happy with Argyll ( and the results that you do n't care to share ) then I 'm very happy for you and I can only wish you the best of luck . <p> Following your logic , I guess it is wise for us to disregard your posts regarding color grading as clearly you are advertising your services as a colorist .... ? 43971 @qwx453971 <p> Hey , no need to get personal about this . Jake is entitled to have his opinion on what display calibration works for him . I know for a fact that Technicolor is using Calman calibration throughout all their facilities in North America ( with Klein K-10A 's , as far as I know ) , so that works for them . <p> I 'm personally glad there 's multiple choices out there , and clearly @ @ @ @ @ @ @ @ @ @ about 10 years ago when certain companies insisted that only their magic LUTs would work with film-outs . Clearly , that was wrong because within a couple of years everybody was able to output digital to film with essentially identical results . The equipment and software turned out to not be as important as the know-how of the people using it . There 's many ways to run a race . <p> Nothing personal about what you quoted , if you actually read the thread you can easily see that this was in reply of him accusing peeps to only post here to sell sw . That is why it said " Following your logic " ... <p> Yes , know-how is key , but in 3D LUT cal there 's nothing you can do once the 3D LUT algorithm starts ... u 're just along for the ride and the result is app dependent . Obviously 6pt cal is different , although there can be so many problems with that as well - app dependent . <p> The real key is to know and understand how to evaluate the @ @ @ @ @ @ @ @ @ @ actually know what you 're dealing with in pre-cal and what you end up with in post-cal . If you do n't know that , then you 're putting blind faith into the sw , and I personally can not recommend that . That goes for any solution out there . <p> Argyll has a built-in tool that allows you to create patch sets . It uses an OFPS approach , which IMO is better than the standard grid sets , but I prefer to create sets in HSV/B with full control of positioning , not relying on the OFPS algorithm in Lab ( IIRC ) . It 's a personal choice , this way I know exactly what I 'm getting : which brt/sat levels and which hues etc . <p> If you do n't wan na use the Argyll patch set builder , simply use a standard grid sequence , like a 103 . <p> Minimum I 'd recommend is 1,000 validation points . Naturally , the more the better . Obviously it would be nice to include sample points that were not profiled so u can see @ @ @ @ @ @ @ @ @ @ the points that were profiled , as that is the easy offset fix for the color engine that creates the LUT offsets . <p> As u can see these custom patch val sets can be quite complex , keeping in mind what u profiled , the actual fixed LUT anchor/correction points and what points u would like to validate in general ( brt level/sat level/hues ) - if u wish to be that detailed . But u can start with a general " easy " set . <p> After that validation dE report - no matter what it tells u ( even if all the dE values look good ) - u 'd still need to do visual eval ( using reference ( ! ) images , such a dedicated skin tones , memory colors , Greyscale , gamut ref/banding tests etc ...... and that will separate minor league from major league ) and then , even if all apps ( LSCMAr ) performed in the same acceptable realm , u 'd still see a difference in color depth , clarity , color popping and perceived contrast etc in ur @ @ @ @ @ @ @ @ @ @ no ways of measuring the latter attributes currently , so no real scientific comparison possible . <p> &gt; when using the built-in cal Resolve LUT , keep in mind that you 're relying on Resolve to apply all of your grading AND then the calibration/view 3D LUT ... Resolve had problems with LUTs in the past - I 'm NOT saying it currently has - but that could be another thing that could throw results off ... <p> that is why dedicated LUT boxes or built-in LUT memory is preferred , and even with that signal chain , some devices introduce LUT distortion - so u only wan na buy/use the " clean " ones ... <p> So I 've been messing around with Dispcalgui a little ( with some help of Tilman Holzhauer ) and I have made some progres . In my case ( i1D3 &amp; FSI CM240 ) it was important to add a delay of 250 msec to the measurements to avoid errors . More info on that can be found here . <p> I used an i1 pro to create offsets for i1D3 and I @ @ @ @ @ @ @ @ @ @ progress . <p> Here 's before the calibration and profiling : <p> And here 's after : <p> One thing that 's cool for FSI owners is that the 3D lut coming out of Dispcalgui ( yourlut.cube ) can be quite easily converted into a display calibration lut ( **29;840;TOOLONG ) you can upload to the monitor . How I did it was I downloaded the standard cfe luts from FSI , which are 17x17x17 luts . I then copy pasted everything BUT the header information from the dispcalgui cube lut into the fsi user1.cfe lut . In other words , you have to keep the header info the same for the monitor to recognize and use it correctly . 
@@44332720 @4332720/ <h> What exactly is a DCDM ? <p> This thread picks up where an earlier thread " interop DCP for archive " left off . <p> Juan Salvo was so right . The archive demands a DCDM . Their webpage requesting an Interop DCP is in error . <p> The DCI specification ( version 1.2 ) goes partway to defining a DCDM . The picture part is an image sequence -- a TIFF file for every frame , with image characteristics similar to the DCP 's , except no compression and no padding , and a definite scheme of file naming . However the DCI spec also says things like : <p> Metadata within the DCDM provides a method to synchronize image , audio and subtitles . <p> These structures are mapped into data file formats that make up the DCDM . This master set of files can then be given a quality control check to verify items like synchronization and that the composition is complete . This requires the DCDM files to be played back directly to the final devices ( e.g. , projector and soundsystem ) in @ @ @ @ @ @ @ @ @ @ Image information and parameters , required to successfully interchange the DCDM Image Structure , shall be provided to the mechanism that will ingest the DCDM . <p> Metadata is carried in the image data file format to indicate the frame rate <p> Files within the DCDM set are required to carry information to provide for frame-based synchronization between each file . 43971 @qwx453971 <p> If some device is going to ingest and play the DCDM , that device will be very particular about how this metadata is included , bit-by-bit , in the files . Unless the TIFF file specification supplies the missing information , the DCI spec is just a sketch of what a DCDM will be like : not definitive . <p> Yet people do make and use DCDMs . To what specification are they making them ? What devices are ingesting them ? If this get into proprietary specification , does n't this weaken the archival motive ? <p> I 'd like to make the DCDM myself . I can make the TIFF images , with the correct X'Y'Z ' values , but the file headers will @ @ @ @ @ @ @ @ @ @ be the next hurdle . ) <p> In truth , I think it makes more sense not to try to make a DCP yourself , but instead hire a company at a modest fee and have them do it . The reality is , you have to do a QC on the file using a projector to know how it 's going to translate in a theatrical environment , to make sure the gamma settings and color space are correct , the sound is in sync , and all that stuff . Prices in LA for indie projects are typically $5-$10 per minute , so a 90-minute film would be well under $1000 , plus a charge to spot-check the film in a DCI-capable screening room . <p> To me , after the film has spent a considerable amount of money paying the actors , shooting the feature , editing the whole thing , mixing it , and color-correcting the project , it would seem foolish to want to avoid spending another grand or so to do it right . Unless you 're making DCPs every month or so , @ @ @ @ @ @ @ @ @ @ operation to buy DCP-creation software . <p> Marc:the EDCF guide to digital cinema mastering you linked to repeats the incomplete definition of the DCDM from the DCI v.1.2 specification . It does not address the matters of metadata format in the TIFF files that my whole post is concerned with . <p> As for who should make the DCP , which your whole post is concerned with : whoever does the color correction and therefore understands how his video is played and his monitor is calibrated , should also be able to make a good DCP . When those factors are n't understood , even the best commercial DCP maker ca n't be sure to make a DCP that puts colors on the screen as your colorist saw them . The difference between video monitoring and DCP projection should only be in the blacks and the colorist should be aware of this LONG ... <p> As for who should make the DCP , which your whole post is concerned with : whoever does the color correction and therefore understands how his video is played and his monitor is calibrated , @ @ @ @ @ @ @ @ @ @ @qwx453971 <p> I do n't think there 's an A-list colorist in LA who makes their own DCPs for current Hollywood films . They work for facilities that have departments that take care of that stuff , then they check the final deliverables to make sure they 're OK . If I worried about every single little file of something I delivered , I 'd never get any sleep . <p> At some point , you have to rely on other people -- preferably people better and far more qualified than you are -- to do the job correctly . But that does n't mean you ca n't check their work . To me , the colorist is only responsible for providing a finished master , and from there on we just cross our fingers that what finally gets in theaters or aired on people 's monitors is correct . <p> And as Walter notes above , without a calibrated DCI projector , it 's all a crapshoot . You 're never going to really know how it looks , regardless of the metadata , file naming , formatting , @ @ @ @ @ @ @ @ @ @ what I thought was a very reasoned , affordable , and practical real-world solution . And I 'm speaking as a guy who actually has delivered about 20 projects for DCP in the last 2 years . <p> A-list colorists do n't even make their own coffee . It 's not a fair comparison to most projects people are creating dcp 's for . 43971 @qwx453971 <p> True , but speaking on behalf of all the B-list and C-list colorists in town , my involvement with DCPs ends when I deliver them a decent ProRes 444 or DPX file to work with . Seriously , I can pick up the phone and get a 2-hour DCP done in 6 hours for under a grand . If an indie filmmaker ca n't afford that , my opinion is that they 're in the wrong business . And again , no matter what , you 're going to be confronted with the problem of not being able to QC the file in XYZ space if you try to make it yourself . <p> I can think of ten companies in LA that @ @ @ @ @ @ @ @ @ @ them will do work for anybody in the world if you ship them a drive . And there 's another couple of hundred DCP firms on the net that will do it in other countries . <p> This strand has wandered off topic . I asked a detailed question about the specification of the DCDM files . Can anyone answer that question ? <p> To Marc : if the colorists " work for facilities that have departments that " make DCPs then there is the needed information coordination , so the colorist can expect to see colors on the screen as he ( or she ) saw them . The problem I indicated was based on your first -- " hire a company " -- post where the two functions were not obviously coordinated . <p> To Marc : I luckily have access to a first class DCP projection theatre . However , it is also possible to read the MXF file of the DCP and obtain the X'Y'Z ' data , and from this to know what XYZ will be projected by the perfect DCI projector . <p> DCP @ @ @ @ @ @ @ @ @ @ short films , documentaries , trailers , dancefilms etc etc . All with their own kind of budget . Most DCP 's that I , as a colorist , create are being presented in a festival and they always just want one un-encrypted copy and sometimes a backup copy . QC always takes place in the theatre , few days ahead , which is , in my opinion , the ultimate place cause that 's were it has to play . <p> Some people I 've worked with have put so much work and love into their film ( and often their own savings ) that it would be an insult to tell them they 're in the ' wrong business ' , when their budget is limited . <p> And besides all this I think the colorist is the best person to judge the DCP cause it 's his or her images . <p> I do totally agree with marc on this if you have made a feature or what ever and its going out to the theatres for the first time get someone who really knows what they @ @ @ @ @ @ @ @ @ @ a dept in house get them to do it , it 's not that expensive and let 's face it possibly the most important part of the whole process as the DCP is what the public see ... so it should be right <p> However <p> There is a bizarre twilight DCP world that i seem to inhabit as i seem to be the only person in the my building that know what XYZ is and have working knowledge of UNIX <p> I get clients who have Films or even TV programs which never made it to the cinema or will never end up there , so the project has be locked and graded to legal BT709 best version will be Pro res 422 or very very rarely 444 either stereo or 5.1 <p> there is no way back that 's all they have got <p> they want a DCP of this for a festival one off or just as a screener to show there friends or investors/end of shoot thing ... and what better way to end a project in a theatre then have drinks afterwards etc <p> ( BTW some @ @ @ @ @ @ @ @ @ @ of the films i have done you know from Netflix/itunes but they never made to theatrical over here in UK ) <p> from BT 709 legal to XYZ as long as you have the correct Lut it will be 100% perfect every time and i have done over 70 last year one failed on as issue with the drive not the image , there is a bit of tech stuff to get the drive right and maybe a speed change/slight up scale involved but once you have that sorted your good to go <p> this all become more of a technical file conversion and not any thing creative . But its something to do while my Main work trash can Mac heats my office as it Debayers red footage ... Toastie <p> i think its handy to know how to do and once you get a good work flow and few unix script written its 99% bullet proof <p> but taking a final master of a feature to theatre after grading for the very first time is not something I would attempt and i know what i , m doing @ @ @ @ @ @ @ @ @ @ <p> EDIT and to Answear the question what is a DCDM ..... anything you like so long as it can be taken into a DCP system and a DCP can be created from it <p> so it could be XYZ tiff @ 24 in x amount of reels with a folder for audio + subtitles <p> so really it its the pre / un-compressed version of a DCP <p> note i have seen Open gate DCDM which mean no cropping has been applied to the image so the sound equipment is still in shot so that would be 2048 x 1080 open gate master with a cropping chart on there for either scope or for flat or what ever they want the DCP guys to make it into <p> so quite it open to interpretation but it is the actual Master of the movie not as some people think that the DCP is master <p> ... to Answear the question what is a DCDM ..... anything you like so long as it can be taken into a DCP system and a DCP can be created from it <p> so it @ @ @ @ @ @ @ @ @ @ reels with a folder for audio + subtitles <p> so really it its the pre / un-compressed version of a DCP <p> note i have seen Open gate DCDM which mean no cropping has been applied to the image so the sound equipment is still in shot so that would be 2048 x 1080 open gate master with a cropping chart on there for either scope or for flat or what ever they want the DCP guys to make it into <p> so quite it open to interpretation but it is the actual Master of the movie not as some people think that the DCP is master 43971 @qwx453971 <p> According to the DCI spec the DCDM is not " anything you like so long as ... " and " ... it could be XYZ tiff " and " quite open to interpretation " . The DCDM image very definitely must be X'Y'Z ' 12-bit ( padded 16-pit ) tiffs ( spec paragraph 3.2.2.2 ) . <p> I know how to make the tiffs . My concern is with what metadata might be necessary in the . tif files ( @ @ @ @ @ @ @ @ @ @ comes from the DCI spec making statments like this ( from para . 3.1.1 . ) : <p> A DCDM ... can be sent directly to a playback system for quality control tasks 43971 @qwx453971 <p> This implies that the DCDM " playback system " is be able to read the , really trivial , metadata stating the projection speed , etc . <p> An archive has requested a DCDM and I doubt that they will be satified with any folder of 120,000 X'Y'Z ' . tif files and 6 . wav files . The archive has a DCDM playback system -- I do n't know which kind . If there has been no standardization among DCDM playback systems -- and the DCI spec indicates none -- then what metadata the archive 's playback system requires will have to be specially stuffed into the . tif file headers . <p> If no one knows of any standardization , I will ask the archive for samples of . tif filles from their usable DCDMs and study their headers . <p> Why are n't you using one of the tried and true @ @ @ @ @ @ @ @ @ @ <p> Sure an EasyDCP license is n't exactly cheap , but it pays for itself after a couple DCP 's . And there are other cheaper plugins/options out there that are ok , if slow . ( CuteDCP , among others ) <p> Any work you take on , you have to be able to check it , and that requires renting a DCP room with a server . However , newer rooms have more forgiving servers in terms of drive formats . For my money , the annoying part of DCP 's *isn't* the creation of it . It 's that it is n't a single file but a folder preventing from an easy online file transfer &amp; that the standard asks for a completely obsolete drive format , ( because some early servers are n't being supported with updates ) ... <p> To Joseph : this thread is about making a DCDM not a DCP . If EasyDCP Creator can makes a DCDM , its manual does n't say so . Some softwares go straight from the DSM , or from an image sequence of non-X'Y'Z ' tiffs @ @ @ @ @ @ @ @ @ @ sequence of X'Y'Z ' tiffs needed for the DCDM . My problem with making a DCDM is that I can find no precise enough definition of the thing . According to DCI , the DCDM 's . tif files require some metadata entries that are not normally in . tif files . <p> I agree with your second annoyance with DCP 's . Perhaps start a new threat on that . <p> According to the DCI spec the DCDM is not " anything you like so long as ... " and " ... it could be XYZ tiff " and " quite open to interpretation " . The DCDM image very definitely must be X'Y'Z ' 12-bit ( padded 16-pit ) tiffs ( spec paragraph 3.2.2.2 ) . 43971 @qwx453971 <p> Just to explain the DCDM should be TIFF 's as per the DCI spec but i have drives marked DCDM with DPX 's on them , don , t ask me why it 's up to the studio how they shift there films around <p> plus for indie films they skip dcdm and go straight from DSM to DCP @ @ @ @ @ @ @ @ @ @ concern is with what metadata might be necessary in the . tif files ( as explained in my first post ) . The concern comes from the DCI spec making statements like this ( from para . 3.1.1 . ) : <p> This implies that the DCDM " playback system " is be able to read the , really trivial , metadata stating the projection speed , etc . <p> An archive has requested a DCDM and I doubt that they will be satified with any folder of 120,000 X'Y'Z ' . tif files and 6 . wav files . The archive has a DCDM playback system -- I do n't know which kind . If there has been no standardization among DCDM playback systems -- and the DCI spec indicates none -- then what metadata the archive 's playback system requires will have to be specially stuffed into the . tif file headers. 43971 @qwx453971 <p> here is a quick break down a pretty Generic DCDM <p> a hard drive with all power leads and cables the drive is labelled " your big movie " @ 24 DCDM 5.1 @ @ @ @ @ @ @ @ @ @ the info about what 's on the drive <p> on the drive <p> all the tiffs , or what ever they have done in reels folders ..... or in one big one or may two if it wasn , t made on Clipster <p> an audio folder <p> with the uncompressed wav 's as discreet channels ( labeled ) .... they have a bit meta data sometimes <p> may be a stereo folder as well maybe not <p> some times another folder with a read me and a cropping chart <p> as its a transport format it not super fixed as to what to supply so long as it the master version you want to be made in to a DCP and the other End under stands what they can do with it .. clearly ... i mean there is no " DCDM police " unlike the " Chroma Police " who will bust down your suite door if you over do it on saturation on a grade .. they are real seriously ! ! ! <p> ... i know what the DCI spec say but believe me but the reality @ @ @ @ @ @ @ @ @ @ client what they want or are expecting ..... but go by the spec first obv 's <p> To gavin nugent : When you play back your DCDM in Resolve how does it know the frame rate ? Does it look at the label you 've stuck on your hard drive saying ' @24 ... ' ? Or do you type ' 24 ' into your description of the timeline properties ? According to DCI : <p> Metadata is carried in the DCDM image data file format to indicate the frame rate . 43971 @qwx453971 <p> It 's silly of DCI , but it 's in their specification . So are you playing back a DCDM or just an image sequence that happens to be in a hard drive with a label that says ' DCDM ' ? DCI gets even sillier when they require : <p> Metadata within the DCDM provides a method to synchronize image , audio and subtitles . ... Files within the DCDM set are required to carry information to provide for frame-based synchronization between each file . 43971 @qwx453971 <p> DCI is already requiring the file @ @ @ @ @ @ @ @ @ @ mean metadata here , but who knows ? Thus the title of this strand : " What exactly is a DCDM ? " <p> Among digital labs ' DCDM ' might be loosely defined -- competent digital technicians can deal with that . But imagine a film archive . Imagine a German film archive , where standardization is holy . Imagine a German film archivest who has learned to press a button and view a DCDM . Can that archivest tell you how to make the DCDM ? Or must every digital lab in the country scramble about finding out what button the archivest has learned to push so they can supply film producers with DCDMs that satisfy the archive . That 's my situation . <p> It 's smart that the archive chose DCDM over DCP because the JPEG2000 compression in the DCP is pretty severe , especially when single frames may need to be examined in an archive . TIFF is also more for-the-ages than JPEG2000 in the sense that it hardly needs decoding . Likewise , what archive wants the extra layer of MXF around the object @ @ @ @ @ @ @ @ @ @ excepting the mentioned nasty metadata details , the DCDM is perfectly determinate . The " color space " of the DSM can always map into the X'Y'Z ' color space . A few lines of code . The DCP however is not determinate because the bitrates of the X ' , Y ' , Z ' components of JPEG2000 are not . There are just some bounds . Also even for a given bitrate , different JPEG2000 encoders can produce different images . Thus , if one thinks of a DCDM as a step toward making the DCP , which it often is , it is a purely mechanical step and the finesse is in the compression step . 
@@44332721 @4332721/ <h> OFX list ... <p> It 's no surprise we are all excited that Resolve 10 will have OFX ... So in my excitement I 've come up with a list of plugins that have an OFX version . I 'm sure more and more developers will be jumping on this opportunity to sell their products , so this list will get longer ! I think I hit the main ones , but please add any others you know of , this will be a good resource . <p> Also , here is an interestingblog articleI found ... It 's in indie developer group who create different plugins for VFX ( creating normals , faux AO ... and more ) . This is n't very useful for us yet ... but it 's food for thought . <p> Unfortunately being an OFX plug and working inside Resolve 10 are two different things . <p> The only plugs we KNOW work or will work at this point are the Genarts Sapphire plugs . Many others will need to be adjusted to work inside Resolve . Some may not be able @ @ @ @ @ @ @ @ @ @ developers out there furiously coding in the hopes of producing plugs specifically for resolve that do n't have any OFX plugs right now . <p> I use Neat , Beauty Box , and Monsters with Scratch . The generic OFX versions simply work in Scratch with no porting or specialized code . If Blackmagic would like plugin support for Resolve to go as swimmingly as it has for other hosts , then they should work real hard to make porting a non-issue . <p> I use Neat , Beauty Box , and Monsters with Scratch . The generic OFX versions simply work in Scratch with no porting or specialized code . If Blackmagic would like plugin support for Resolve to go as swimmingly as it has for other hosts , then they should work real hard to make porting a non-issue. 43971 @qwx453971 <p> What I 've been hearing from scratch users for some time is that there is an immense amount of instability that 's come with the less optimized OFX plugs . Is that not the case ? <p> We 're derailing the point of this thread , @ @ @ @ @ @ @ @ @ @ now become a bit of a downer . <p> That being said ... <p> If there 's any sort of " immense " problem with using plugins in Scratch , it 's not with stability , it 's with the GPU hit you take . Scratch is limited to a single GPU , and the plugins I mentioned do not work in real-time even with a GTX 680 . I am hoping that dedicating 2-3 large GPUs for Resolve will help , but anyone ( i.e. most people ) using plugins with one GPU will probably find themselves quickly debating the cost/ benefit when their system is brought to it 's knees with a single mouse-click . <p> As for OFX in general ; support it or do n't . If there 's some sort of custom branch required for each host , what 's the point of having a standard ? If developers require us to buy " Resolve versions " of plugins we already own , then that expense , combined with adding multiple GPUs , along with the Cubix or large PC case needed to run them @ @ @ @ @ @ @ @ @ @ with . <p> As for OFX in general ; support it or do n't . If there 's some sort of custom branch required for each host , what 's the point of having a standard ? If developers require us to buy " Resolve versions " of plugins we already own , then that expense , combined with adding multiple GPUs , along with the Cubix or large PC case needed to run them , just makes the whole thing a non-starter to begin with . 43971 @qwx453971 <p> This is a very good point , though as the OFX concept itself is quite broad there arise inconsistencies between hosts . No developer has ever been hurt by optimizing their code a bit . There is however undoubtably a trade off between openness in the form of supporting 3rd parties and stability . It 's a compromise I 'm mostly willing to make , but I 'll certainly exercise some caution on which OFX plugs I 'll install . We shall see soon enough . <p> Any chance any of these fancy OpenFX plug-ins can de-interlace too ? That @ @ @ @ @ @ @ @ @ @ in one operation . It 's for a DCP . <p> A bit of searching gave me this , which seems to be an OpenFX plugin for Sony Vegas . There 's a link to a Mac port , but it requires someone to compile it . I have no knowledge of such . Anyone up for the task ? <p> I can confirm FilmConvert work 100% . I was just playing around with the beta . My concern is how it will affect the responsiveness of the application once several nodes are added w some adjustments , or even several LUTs with heavy changes . <p> Juan , any reason why you think Neat will be difficult to adapt to v10 ? That 's really my #1 wish , as that will allow me ( as a current lite user ) to have some noise reduction capabilities . I have a feeling a lot of people will start using the new built-in reduction more often so I 'm not sure if its as much of a concern for the regular users . <p> I e-mailed the guy who 's @ @ @ @ @ @ @ @ @ @ OpenFX . Within a couple of hours I got a compiled version working in Resolve . This is HUGE ! All though slow , it 's working . <p> Juan , any reason why you think Neat will be difficult to adapt to v10 ? 43971 @qwx453971 <p> Yes , because of the frame buffer required . Not to mention that it would absolutely kill performance . Buying a Resolve license would be worthwhile even for the noise reduction alone . It works really well and is very fast . <p> I e-mailed the guy who 's made the Mac OS port of Yadif deinterlacing plugin for OpenFX . Within a couple of hours I got a compiled version working in Resolve . This is HUGE ! All though slow , it 's working . 43971 @qwx453971 <p> It 's not so easy to do , but the plugin would either need to be multithreaded or otherwise parallelize onto the GPU to improve performance . I think a lot of these types of plugs would be good in a session to show the client once on a parked frame then @ @ @ @ @ @ @ @ @ @ the node back on for render . <p> Hi Everyone . I 'm new around here . I put together an article about OFX yesterday for Toolfarm that I thought may be of some interest . If you have any verified information that is missing , I 'd be very glad to update it . <p> had a go getting the Yadif deinterlacing plugin for OpenFX working on windows found to get it to work you take the link listed above for vegas and rename the . dll to . ofx then put folder in correct location in C : Program FilesCommon FilesOFX <p> as noted elsewhere on the forum I found it gave shot with a 1px white line at bottom of the screen which means it needed to be scaled in resolve , not sure if that was due to fields shifting up and down as part of deinterlace filter 
@@44332722 @4332722/ <h> Baselight Noise Reduction <p> I used to use Baselight a lot , but have been doing more jobs in Resolve recently . I 've really started to find the noise reduction capabilities useful in Resolve - especially when working with underexposed areas of Slog footage . No more am I just hoping things will come good in a session in a Phoenix post-grade ! <p> I 've got a Baselight job coming up again soon and wondered if the noise reduction has caught up with what Resolve is offering ? I ca n't see any mention of it on the Filmlight website. 
@@44332723 @4332723/ <h> Antler DCTL files for ACES <p> I have created a set of utility DCTL files which perform transforms between DaVinci ACES , ACEScc and ACES ( linear ACES2065-1 ) . These allow you to set your Resolve project to one ACES format , but apply particular operations in a specific colour space , either for compatibility with other systems ( e.g. applying CDL transforms in linear ACES ) or because a particular operation works better in a particular colour space ( e.g. sharpening may cause artefacts in ACEScc ) . <p> I have not added these to my web store , but the full set can be purchased for $50 by contacting me directly on nickatantlerpostdotcom <p> That is actually remarkably useful . The new additions to RCM in v12.5 is great , but the arbitrary separation of ACES transforms from every other transform is a right pain in the arse . Why not have ' ACES ' in the Gamut section of RCM ? I know that it wo n't be ' ACES correct ' as RCM would be performing the transform in 1 , where as @ @ @ @ @ @ @ @ @ @ minimum for ACES . <p> I 've already come across 2 examples where having to select either ' ACES ' or ' RCM ' has bitten me - once for CanonLog2 material ( no option for that in RCM but the IDT is in ACES ) . It was a mixed format project where everything was better as RCM except for the C300 mkII material . I ended up just transcoding it to LogC using ACES and then switching back to RCM in the end . <p> The other example was where VFX were delivered as Linear ACES . Only the VFX were ACES , the rest of the project was Arri and RED , and I shifted the RED to LogC to make the match grade easier . I ended up using FusionConnect for those shots ( I did n't want to lose the mattes embedded in the . EXRs in that case ) . <p> DCTL is the logical solution - although I have no doubt that my 1st five scripts will fail ( as did my 1st attempts at GLSL scripting ) . I would have happily @ @ @ @ @ @ @ @ @ @ day watching a progress bar when I could have been working . <p> Anyone else find that they constantly come across ' Unnecessary Limitations ' in Resolve ? In the ACES/RCM example I would n't have bothered with such a rigid separation between the 2 methods - I would have just had 1 ' Transforms ' effect with a drop-down menu to with ' ACES ' and ' RCM ' options . I understand that ' true ACES ' requires the adherence of a set of rules , without which you risk losing the best thing about ACES ( the ability to change the ODT from one display setting to another and have a better than average chance that the same grade on both displays might actually match ) , and that 's fine . But there needs to be way to take the training wheels off if you know what you 're doing ( which theoretically we all do ) . Both Baselight &amp; Mistika allow for this , and it 's invaluable in the real world - both Truelight/UniColour are almost always more robust and non-destructive than the @ @ @ @ @ @ @ @ @ @ but if I 'm working ' ACES ' on either Baselight or Mistika I will almost always move everything to ACES Linear and then sandwich a LONG ... transform stack in there . RRT/ODT after that . All of the good bits about ACES ( including working Linear , like a grownup ) , all my grading tools all continue to work perfectly , and I can still change ODTs per deliverable which is a hell of a time-saver . <p> Fusion allows for flexibility like this ( compositing systems like Nuke and Mistika usually do ) so for a few VFX shots the FusionConnect option actually works well if you have the time/power to render 3x as much media as I would need in Baselight/Misitka ( I have n't tried this in Nucoda but I 'm told that DV have added something similar ) . Hopefully Blackmagic will err on the side of copying Fusion effects over to Resolve , rather than Resolve effects into Fusion . <p> Anyone else find that they constantly come across ' Unnecessary Limitations ' in Resolve ? 43971 @qwx453971 <p> Always felt like @ @ @ @ @ @ @ @ @ @ wrong i understand that people still do exceptionally good work in it . But it always seemed puzzling to me why a colourist would pick resolve if he had a choice of any other system . 
@@44332724 @4332724/ <h> ROC vs Non-RAID HBAs <p> Storage is currently my suite 's bottleneck . I presently use an external RAID-5 USB 3.0 enclosure , and maintain embarrassing speeds . I 've managed to get by without needing to upgrade it because I work mostly with R3Ds , which rely on my CPU much more than a wide bus . <p> I 've been studying storage for the past several days and am trying to wrap my head around so many questions ; some of them critical , some of them trivial ... Please bear with me ! I 've done my best to answer them on my own time , but here are the ones that have me stumped : <p> 1 . I notice that there are a couple of places a dedicated RAID processor can reside , and was wondering if there are any crucial differences in having the ROC on the HBA , in the actual external enclosure ( eg , ARC-5028T ) ? If no , would the latter enclosure work with a non-ROC HBA just perform the same ? <p> 2 . Given that @ @ @ @ @ @ @ @ @ @ and all that jazz , would a HBA with ROC even be necessary in the context of RAID 0 work arrays ? If no , why not just make an internal SATA III RAID using the OS controller or on the Intel Rapid Storage Technology chipset ? <p> 3 . While SATA drives can be mounted onto an SAS controller no problem , what exactly are we missing out on outside of slower RPM 's and lower amounts of disk cache ? I had read somewhere that SAS drives are full duplex , allowing the array to be read and written from at the same time ? Would this be useful for DPX projects ? <p> 4 . Currently , LSI offers 12Gb/s HBAs . Would it make sense to buy a 12Gb/s card for future proofing , despite the low chance of full bus saturation while using 6Gb/s SATA drives ? <p> 5 . What is the deal with SATA Express ? I understand that it is a revision of SATA that uses CPU lanes , but I ca n't seem to find any mention of any SATA Express @ @ @ @ @ @ @ @ @ @ regular SATA drives right now . <p> 4 . What are the ramifications of using SSDs in a RAID config ? I hear that SSDs would not be able to TRIM or properly utilise their garbage collection protocols in the absence of AHCI . <p> 5 . What are your thoughts on PCIe SSDs in the future ? Obviously there are n't that many on the market right now , but is there a chance that we can start seeing DAS in the form of PCIe SSDs ? <p> I have used the built in Sata 6G with 4 drives in Raid-0 configured in software ( Win-7 ) with good results . That would be your least expensive option for internal storage and 4x 2Tb or 4x 4Tb drives will get you to 5-600Mb/s <p> I have two machines with Rocket Raid 4520 Raid controllers ( About $300 for the card ) with 8x 2Tb drives in Raid-5 ( one Resolve and one Film Scanner ) which get to about 8-900Mb/s with redundancy . <p> Regular 2-4Tb 7200rpm Sata drives are pretty fast for the $$ going to SSDs gets @ @ @ @ @ @ @ @ @ @ that they are necessary unless you have the GPU and CPU performance to push the disk envelope . <p> On two of my Resolve machines I have dual array 's for source and render which really speeds things up . <p> Sata express is not relevant to your question of working storage . It might be some day , when we can buy 15TB SSDs for a few hundred bucks . <p> I bet within 5 years most ssd boot drives will be M.2 or sata express though . <p> Also , with sata express and M.2 I believe you are using up PCI-E lanes . So if you only have 32 to begin with , and want to run 2 GPUs plus a RAID card plus your mini monitor ... Well , that ai n't gon na happen . <p> Thanks for the Linus link , Eric . Always a good reference , even though he has a unbearable high pitched voice Here 's an extra trivia question ; what actually happens if you exceed the amount of CPU lanes that your CPU supports ? <p> One of your @ @ @ @ @ @ @ @ @ @ that happens , I bet BIOS will start to downgrade bus speeds . For example , if you had 2 GPUs running at 16x , it may take one of them down to 8X . Or if your RAID card is running at 8x , it 'll downgrade it to 4x . <p> If you know the model of your motherboard it will tell you what pci configurations/speeds it supports . You ca n't exceed the number of available lanes , in most cases the motherboard will downgrade connection 's speeds . However a motherboard with a plx chip or some sort of pci expander can increase the number of lanes . Case in point I know the the x99-e ws has a plx chip onboard that allows up to 64 pci lanes . It can run four pci connections at x16 x16 x16 x16 . If you use more slots , it will slow down slot speeds to x16 x8 x8 x8 x8 x8 x8 . <p> PCIe SSDs are not going to be your solution for the foreseeable future . They 're best use case is in VFX @ @ @ @ @ @ @ @ @ @ shots at a time , and want all of your textures/models/normal maps/etc. accessible LIGHTNING quick . These cards simply do n't have enough storage to work well as a colorist storage bank . <p> The most cost-effective route is a RAID with very quick ( high RPM ) , very small , drives . Small as in storage capacity . Why small ? Because you want to keep your seek times down . It takes longer for a needle to find a file on a 4TB drive than on a 1TB drive , all else being equal . Of course , if you want to keep your storage minimal , that means trimming your projects as much as possible and only loading onto your storage bank relevant footage . <p> I 'm not sure what the current wisdom is on SSDs in RAIDs . Last I really looked into it , SSDs in RAIDs would die relatively quickly because RAIDing tends to eat up the write-cycles of each block . But that was almost 2 years ago , so I 'm not sure what has advanced since then . With @ @ @ @ @ @ @ @ @ @ there is no head that has to scan over a disk , rather , every file is accessible at the same moment . 
@@44332725 @4332725/ <h> Avoid crushing black clothes <p> Something I can across recently I had some shots where the clothes people are wearing are darker than the shadows . If I place black to keep detail in the clothes it produces a flat image . If I put the shadows to get a nice contrast that the client wanted the clothes crush out and it looks like people are just wearing sold black . <p> I did some masking to bring more detail back to the clothes but was difficult to not noticeably make the clothes look low conrast compared to the rest of the images . <p> Curious if others have come across this before and how they worked around it ? <p> If you cant separate your luma ranges perhaps you can create your contrast in another way , adding deeper blacks to another part of the image , add a power window that suggests a light source ( sun , street lamp , window etc ) that let 's some of the details shine though in parts of the clothing . <p> I think this is a lighting problem @ @ @ @ @ @ @ @ @ @ clothes into consideration and give people wearing dark clothes in a dark room some backlight . Without that , they 're just going to sink into the background sludge . <p> I had a similar situation in a commercial , where policemen were wearing polyester uniform . IR contamination turned black uniform into magenta . Keying it with garbage masks took care of it , but it did took some doing to get rid of it . Again , that 's a DP issue . Problem like that can easily be taken care of on set by making sure you 're not stacking whole bunch of NDs and using proper IR filter . <p> I think this is a lighting problem . The DP needs to take the color of the clothes into consideration and give people wearing dark clothes in a dark room some backlight . Without that , they 're just going to sink into the background sludge . <p> They had this down to a science on the old X-Files TV show . 43971 @qwx453971 <p> Yeah this is what I thought would happen , either the @ @ @ @ @ @ @ @ @ @ for the dark clothes . When I mask it out its tricky I find not to get a noticeable contrast difference between the clothes and the picture as a whole <p> Just had the opposite problem on a feature scene tonight : outside shot , faces look fine , only the lead actor is wearing a white silk shirt with about 90% pure reflectivity . Apparently the crew had not been informed that bright white outfits are reserved for religious figures and period pieces , not modern romantic comedies . Gray would have been much preferred . <p> Or Mark , they know how to light and use a flag or net to take down the outfit . Anyways modern digital cinema cameras and film can handle white clothes . 43971 @qwx453971 <p> Apparently the Red Epic Dragon ca n't for sure . I used several articulated tracking windows to bring his white shirt down to something normal , acting as an invisible movable flag , but I 'm still not thrilled with the skin tones on certain shots . But if you at least have access to the raw @ @ @ @ @ @ @ @ @ @ incandescent stuff shot at 3200 degres and balance them towards something half-approching whites . Very challenging job . 
@@44332726 @4332726/ <h> Sony OLED calibration for TV and theaters <p> I have a sony 1710 OLED that I use for grading on set . I have a few questions . I understand that Sony has a white paper about what x/y values for white point to set when calibrating with an i1 display pro because using standard x/y values do n't give correct D6500 results . I ca n't find the white paper when I google . Can anyone find a link for me ? <p> I also understand that when shooting for broadcast the OLED can be deceiving because of their contrast . Is there a recommended cd/m for black levels when grading for TV broadcast ? I know it is probably preference , but I 'd like to get an idea of what others are doing . Any thoughts ? And then for grading for theatrical release , is the deep black of an OLED a good place , or should there be some black lift added to the monitor for grading for theatrical ? <p> Also I do a lot of on set work . Most of @ @ @ @ @ @ @ @ @ @ . Sometimes not . Is there a good middle ground white level cd/m recommendation that is generally good for these different on set viewing environments ? <p> And finally , I understand the complexity of gamma levels in different viewing environments . I 've read a TON about this . 2.2 , 2.4 , 2.6 . I got all that . But since my viewing environment changes so much , I 'm looking to get recommendations from anyone with similar experience . Should I switch between 2.2 and 2.4 depending on if I 'm in a bright environment vs. tent/dark room ? Or should I just stick to one or the other . I just want to be ballpark decent with the results that I get so that when my grading LUTs are brought in by the colorist in a proper suite it 'll generally look how it did on set . <p> Again , I do n't need a spiel about gamma , viewing environments , that I need a proper probe , or how terrible OLEDs are for grading . I understand that at some point I 'll @ @ @ @ @ @ @ @ @ @ is what I have at present . I 'm just looking for some quick pointers from anyone who can speak to my situation and let me know what you do . <p> I have a sony 1710 OLED that I use for grading on set . I have a few questions . I understand that Sony has a white paper about what x/y values for white point to set when calibrating with an i1 display pro because using standard x/y values do n't give correct D6500 results . I ca n't find the white paper when I google . Can anyone find a link for me ? 43971 @qwx453971 <p> Hi Thatcher , <p> I ca n't find it online , but I had it to my files so I have just uploaded it for you . <p> A thought from a rank amateur ( though isf trained ) . One step of calibration that uses the eye and not probes is setting the black and white levels and gamma because this is effected by perception and surrounding contrast ( and is much more sensitive ) . So if 2.4 @ @ @ @ @ @ @ @ @ @ a lighter environment in order to get the same perceived contrast , but you are unsure of how light your environment is , perhaps checking the effect on a pluge should determine what levels you are able to distinguish . This is often the only difference between an isf night and an isf day calibration as far as I can tell . If there is a more detailed check other than can you see 10,11,12,13,14,15,16 and 243,244,245,246,247 then I would like to know . Are there higher than 10bit pluges to check you can see greater differentiated black levels ? <p> Alternatively use the probe to measure ambient light and if it is low enough to set to 2.4 / 2.6 do so . <p> I generally say if you can see , use 2.2 . If it is pretty gloomy use 2.4. if you do n't know who is sitting next to you use 2.6 . <p> They seem to be saying that by using these settings makes them perceptibly more similar to other display technologies . So grading rooms closer match edit suites etc. which was a major @ @ @ @ @ @ @ @ @ @ CRTs , they recreated an old problem , which was actually the final reason many people finally stopped grading on CRTs ... <p> As a side note , I did just realize that my OLED bias control only allows for lowering RGB levels , not raising them . On Sony OLED , what 's the best control for lifting blacks without doing any funny business to the rest of the image . Obviously lifting the blacks will shift everything . I just do n't want it to shift other levels in the wrong way . <p> Use a LUT rather than a knob ? Are you using Calman for Resolve or sinilar to tweak the accuracy with a LUT ? There is a lot to be said for leaving the monitor happy in its native setting and Lutting it back in ... is n't there . I once again hope I am not diluting LGG with my amateurism . <p> Yeah , I 'm always in favor of a LUT , a solid , calibrated thing . Unfortunately that currently only works for my Scratch workflow . However when I @ @ @ @ @ @ @ @ @ @ n't have a LUT box in line presently . I just need to get a monitor that supports LUTs . The Sony PVM OLEDs do n't . <p> Yeah , I 'm always in favor of a LUT , a solid , calibrated thing . Unfortunately that currently only works for my Scratch workflow . However when I 'm monitoring a live image from a camera I do n't have a LUT box in line presently . I just need to get a monitor that supports LUTs . The Sony PVM OLEDs don't. 43971 @qwx453971 <p> I have talked to several people that are getting great results by putting A Blackmagic HDlink Pro LUT box in front of the Sony OLED 's . <p> I do use a HDLink when I need to do live grading for temporary creative looks . However when it comes to calibrating for color accuracy I hear it has a shift or is inaccurate . Steve Shaw has an article about this I believe . Of course even if I 'm using it for creative looks , putting it in the pipeline throws hyper-critical color @ @ @ @ @ @ @ @ @ @ it up , I wonder if I profile the OLED monitor with the HDLink in the pipeline , does it fix the problem with the HDLink , or is HDLink 's problem in it 's implementation of the LUT itself rather than a general shift in image . <p> Yes , It 's a bad idea , Steve Shaw has reported that problem from 2012 and it 's still unsolved . <p> The problem is that when a LUT is loaded into the HDlink Pro that makes it badly distorted , causing the output to be inaccurate . <p> This makes the HDlink Pro box unusable for any LUT application for critical color applications . <p> There is a Black Clipping and color channel distortion that can be visible by an eye also when the cLUT is active . <p> I have created a complete list for 3D LUT Box / Video Processors Specs comparison that are available to the consumer/professional market here . ( but I have not included the input/outputs specs for SDI/HD-SDI/3D-SDI connections. ) 
@@44332727 @4332727/ <h> Achieving this look in Vikings ( History Channel ) <p> I am a big fan of the color work on Vikings and I was wondering how to achieve this look . ( Episode from Season 2 called Boneless ) Is there a term/name for it that I can do further research on ? I 'm relatively new to this.http : //imgur.com/a/Ct4Vw There 's some interesting placement of blurs and areas of glossy white . I usually do n't see this kind of thing on TV ( save maybe Hannibal ) . <p> Often , looks like this happen through great cinematography , not just good color correction . In particular , filters like Tiffen Glimmerglass yield an effect that would be hard to do in just post ( even with OFX plug-ins and stuff like that ) . The trick is , the DP has to be willing to commit 100% to baking that effect in during the shoot . <p> Your shot to me just looks like a lot of desat and defocus glows , possibly with keys and power windows . <p> Jacob , if you @ @ @ @ @ @ @ @ @ @ to take shots like these into your software of choice , use some demo footage that would be somewhat simar in lighting scenario is day exterior , overcast conditions etc and mimic the color palette as best of your ability . You may end up creating a rediculous amount of correction layers or nodes where the colorist may have only used a few in reality but that 's okay . Start there , then once you 've achieved something similar go back again and try to do the same thing with less layers nodes and see if you can do it . Achieving the look is not the hard part , maintaining it is and controlling it throughout the entire sequence is . The faster and less convoluted you can get there , the better off you 'll be . And , It does n't hurt to have great cinematography to start with 
@@44332728 @4332728/ <h> Baselight Plus <p> Baselight PLUS provides a new integrated colour grading option for uncompromised grading . It 's a complete package , with options for editors and a year 's Software and Advanced Hardware support at an unprecedented price point . <p> At the heart of the package is the Baselight 4.4 software , capable of 4K real-time grading and including all of the remarkable creative power which has made Baselight the number one choice for colourists around the world . The software is bundled with the high performance Supermicro tower , including 24TB of internal RAID storage and 2TB of SSD for lightning fast non-linear access . Control is through the intuitive Slate panel , which uses the same technological advances as on the Blackboard 2 control surface " all in a compact form . <p> Also included in the bundle are two licences for Baselight Editions , which reside inside Avid ( Mac or PC ) or Final Cut Pro editors . This is a huge workflow boost , as Baselight Editions support round-tripping with full Baselight systems , meaning that no rendering time is needed @ @ @ @ @ @ @ @ @ @ can be tweaked and adjusted in the edit suite , or round-tripped non-destructively to the colour room . <p> That 's actually pretty damn competitive with what would be a comparable full-panel Resolve setup . Did they just preempt the BL1 by throwing in a 24 TB RAID &amp; the Slate at essentially the same price ? If not , what 's the diff between this and BL1 ? <p> Did they just preempt the BL1 by throwing in a 24 TB RAID &amp; the Slate at essentially the same price ? If not , what 's the diff between this and BL1 ? 43971 @qwx453971 <p> I think you 've pretty much got it . In casual discussions at NAB it was hinted you could have a BL1 ( " Deskside " Tower w/ 2TB SSD , but no storage ) for around US$50k . Add in a Slate for another $10-12k depending on your deal . So that leaves a decent amount of room to add a good RAID controller and eight , 3TB drives for the interal array . Plus you get the v4.4 software which has @ @ @ @ @ @ @ @ @ @ very fair price for a legit , thoroughbred grading system you can build a business on . <p> It could be helpful for Filmlight to streamline their marketing a bit though as the variations ... BL One , BL Access , BL Plus , BL Editions , BL Two , BL Eight ... might be getting a little confusing . 
@@44332731 @4332731/ <h> Premiere timeline - zebra striped disabled clips <p> I 'm looking for your expertise on this one . Been grinding my head at this problem for a few hours now , ca n't figure out what 's going on . <p> I 've been working on this sequence for a couple of weeks on and off , no problems what so ever . It 's miniDV captured in Premiere Pro CC , and edited straight away . It 's saved on an external hard drive . Unfortunately the Mac I used to start editing is no longer available due to a hardware failure , so I took it to my own Mac . <p> The problem is when I open the project . Everything is linked and fine , as expected . I can see all media files are online in the media browser . However , the timeline is a mess ! Many , almost all , clips have gone " zebra " . They produce no sound nor picture . What the ! ? <p> I 've tried forcing some of the clips OFFLINE , restart @ @ @ @ @ @ @ @ @ @ the media browser and source window , but still no joy on the timeline . <p> Did you update Premiere Pro CC in the meantime ? I had two problems with messed up timelines after updating , and at least one of these included " zebra " clips . I discussed the topics on the Adobe forums . I do n't know , if it is related to your problem , but just in case : <p> That 's it - probably . They " optimized " the way Premiere Pro is interpreting certain container formats like MXF . In the first thread you 'll find a link to a work around , which helped in my case : Messed up timeline after update <p> Whatever the project is , I always make a dedicated folder for the project , and within that folder , all render files , source media etc. goes . So the render files were on that external drive . <p> As I mentioned earlier , the problem was fixed by unchecking " Align timecode " . That did the trick . Not that I understand @ @ @ @ @ @ @ @ @ @ installed on my Mac Pro vs. the iMac the project started on . 
@@44332732 @4332732/ <p> Dolby Laboratories is the global leader in technologies that are essential elements in the best entertainment experiences . Founded in 1965 and best known for high-quality audio and surround sound , Dolby innovations enrich entertainment at the movies , at home , or on the go . <p> Responsibilities : - Operate color correction equipment in supervised color-grading sessions aimed at creating and finishing content for use in testing , demonstration and marketing of Dolby products . . - Modify color and make editorial changes to content as requested by internal or external clients . - Offer advice on color , film and video delivery formats and other technical issues . - Provide quality control review sessions for film elements and sequences . - Work with stereographers on 3D alignment , convergence and color correction issues . - Interact with film laboratory and other 3rd party vendors . <p> My name is Robert S. Douglas and I am a freelance Colorist and Editor with over 20 years experience in color correction , online/offline editing , and visual effects including 3D and 2D feature films , commercials , music @ @ @ @ @ @ @ @ @ @ with Assimilate Scratch , all of Final Cut Studio including Apple Color . I am experienced with media origination from Film , RED , Viper , and High Definition Video Cameras and their associated workflow . You can enter into your browser for my online Colorist Demo reel , , for my Editorial Demo Reel , and for selected online credits go to http : **30;871;TOOLONG . I am reliable and motivated , work well under pressure with or without clients .. Yours sincerely , <p> I use a variety of editing and color correction applications that include Assimilate Scratch , Apple Color and Final Cut Studio , Avid Symphony , Autodesk Smoke , , for final conform and color correction of 2k , 4K , high definition , and standard definition video and film projects . I often work with footage scanned from 35mm and 16mm film as well as Thomson Viper digital cinema and the Red digital cinema camera systems . I also work with Synthetic Aperture Color Finesse , DVD Studio Pro , Compressor , Sountrack Pro 2 , Adobe After Effects , Adobe Photoshop , Apple @ @ @ @ @ @ @ @ @ @ OS and Windows XP . <p> 4/2005 " 11/2006Employed by The Camera House as Post Production Supervisor . Duties included input in creation of the post production department , and operation of all commercial and feature film post production including dallies creation , editing , data archiving , color correction , and master delivery . While at The Camera House I provided input to all post production hardware purchases and designed the post production workflow for Viper film stream digital cinema as well as film originated projects . <p> 7/2002 " 4/2005Laser Pacific Media Corporation and Sunset Digital employed as Freelance Online Editor . Duties include editing and assembling , color correction of episodic television and feature films , film trailers , promotions , and commercials , as well as titling , color correction , and optical special effects . Programs are mastered for network delivery or finished as a step in the postproduction process . 
@@44332733 @4332733/ <h> Baselight STUDENT - Software Not Found render error <p> When I attempt to render in Baselight STUDENT , I 'm getting a " software not found " error . I 've attempted to troubleshoot using the Technical Guide and the MacBook Pro guide , but do n't see any references to what might be causing this . <p> Out of curiosity I just checked and my software is installed at **34;903;TOOLONG <p> Do n't know why the syntax in the warning is not the same ( lowercase b and ' - ' instead of ' / ' ) 43971 @qwx453971 <p> That 's a really good catch . I do not have a /usr/fl directory . I logged in as root and attempted to create a path , but it looks like there 's a feature introduced in El Capitan called System Integrity Protection that prevents you from making directories in /usr . Apparently you can defeat it by booting into recovery mode and running " csrutil disable " in terminal . What 's interesting though is I have Flame on Mac installed , and Autodesk was able @ @ @ @ @ @ @ @ @ @ which like " fl " , needs to be available for Flame to even run . <p> So , disabling SIP , and re-running the BLS installer may fix the issue . I 'll give it a try as soon as I have a moment . <p> That 's a really good catch . I do not have a /usr/fl directory . I logged in as root and attempted to create a path , but it looks like there 's a feature introduced in El Capitan called System Integrity Protection that prevents you from making directories in /usr . Apparently you can defeat it by booting into recovery mode and running " csrutil disable " in terminal . What 's interesting though is I have Flame on Mac installed , and Autodesk was able to add a " discreet " directory to /usr , which like " fl " , needs to be available for Flame to even run . <p> So , disabling SIP , and re-running the BLS installer may fix the issue . I 'll give it a try as soon as I have a moment . @ @ @ @ @ @ @ @ @ @ a symbolic link to the real directory which resized in an approved location . Not sure how BL means to work , but running OS X without SIP is problematic . <p> Baselight should work the same way . Before El Capitan /usr/fl used to be a real directory . Now it should just be a symbolic link to /Library/Application Support/FilmLight . On my system it is , but I already had various Baselight installs on there before I installed the Student version , so I ca n't compare . Also , I am not running El Capitan . <p> NOTE : This is based on my brief initial testing , and not confirmed by FilmLight , so test for yourself . <p> It seems that the x264 encoder used by BaselightSTUDENT expects legal range data . The settings shown in Jason 's screen grab at the top of this thread use the VideoFull ( Rec.1886 : 2.4 Gamma / Rec.709 ) colour space , so according to my testing would give incorrect results . <p> When rendering , you may wish to try either rendering to the " Rec.1886 @ @ @ @ @ @ @ @ @ @ revealed by holding down shift when selecting from any colour space drop-down ) or adding a " Full to Legal Scale " in the Video LUT drop-down ( which you need to first make visible ) . <p> That 's no problem . I will keep you updated with our progress on this matter . If anybody has any issues please do get in contact with us at student@filmlight.co.uk . These issues can then be tracked and monitored through our systems and by our development team . Please be aware that we are experiencing high volumes of requests at this time and we will get back to you as soon as we can . <p> We now have a new build available , Version 4.4m1.9362 . The render issues should now be resolved . Baselight STUDENT should automatically update but if for any reason it has n't you can use the link below to download the latest build . <p> I 'm trying to install Baselight Student on a different/new machine but now I get this fesendauth error when I try to launch . Any clue how to get around @ @ @ @ @ @ @ @ @ @ total beginner there ... <p> This sounds like a database issue - please include your version of macOS , a screen shot of the error , and we will get to the bottom of it . We may need to send you some commands to run to help diagnose the error . <p> If baselightSTUDENT does not detect a running version of postgres it will install its own ( 9.3.4 ) . <p> It sounds as if it detected a postgres version installed , but could not get access to it . <p> This sounds like a database issue - please include your version of macOS , a screen shot of the error , and we will get to the bottom of it . We may need to send you some commands to run to help diagnose the error . <p> If baselightSTUDENT does not detect a running version of postgres it will install its own ( 9.3.4 ) . <p> It sounds as if it detected a postgres version installed , but could not get access to it . <p> This sounds like a database issue - please include @ @ @ @ @ @ @ @ @ @ error , and we will get to the bottom of it . We may need to send you some commands to run to help diagnose the error . <p> If baselightSTUDENT does not detect a running version of postgres it will install its own ( 9.3.4 ) . <p> It sounds as if it detected a postgres version installed , but could not get access to it . 43971 @qwx453971 <p> Hi Ozlem , <p> After uninstalling both Baselight and Postgres , and re-installing Baselight , I got it to work . Baselight installed its own version of Postgres , like you suggested , so thanks . <p> I had to manually remove an empty Postgres folder from the Library in order for the Baselight installer to install Postgres , so that 's goo dto know if you 're having troubles . 
@@44332734 @4332734/ <p> Jef , although I do have the DVO set I do n't have any experience with the specific issue you are looking at solving . The DVO Tools are exceptional and there are many many of them . I believe the closest to them come to being rentable is by buying Loki and purchasing just the tools you need . That said I 'm not sure how that works interface wise . I would suggest you reach out to DV for a trial license actually use the tools and see if they work for you . <p> Jef , although I do have the DVO set I do n't have any experience with the specific issue you are looking at solving . The DVO Tools are exceptional and there are many many of them . I believe the closest to them come to being rentable is by buying Loki and purchasing just the tools you need . That said I 'm not sure how that works interface wise . I would suggest you reach out to DV for a trial license actually use the tools and see if @ @ @ @ @ @ @ @ @ @ restored some beta sp clips with Scanline dropouts ( 4-6 pixels high and going from far left to far right ) and stuff I ca n't name : heavy yellow and magenta blocks on hard contrast edges , kind of similar to chromatic aberration . DVO chroma and DVO dropout fixed it magically ! I 'm out of the suite the next days so I think it 's too late for a test until Monday ? 
@@44332735 @4332735/ <h> Alpha Channel When Exporting Individual Clips <p> I have just outputted an a project ( with XML ) from Resolve , and all of the shots that I used alpha channels - ie. sky replacement , are n't working when I bring the XML back into Premiere . If I export those shots with " Export Alpha " checked in the Resolve delivery page , Premiere chokes on this footage " MXF Decode Error in " G : Render ... " And if it 's unchecked the sky replacements obviously are n't visible in Premiere . ( I am outputting DNxHR 444 from Resolve ) <p> What is the standard for this kind of thing ? When exporting as individual clips , do people typically output alpha work and the multiple video tracks , or do they flatten V1 and V2 , etc , into one clip somehow ? I would really rather have the editor see a single clip on their timeline for any alpha work I have done - is there is an easy way to do that ? <p> The easiest workaround I can think @ @ @ @ @ @ @ @ @ @ shots . Is it possible to just do all the finishing in Resolve ? 43971 @qwx453971 <p> Thanks , Mark . That 's exactly what I ended up doing . Was n't sure if there was a standard procedure for this type of thing . Got a little more complicated when I had to manually add 24 frame handles to the all of the flattened shots . The editor was working in Premiere , so unfortunately finishing in Resolve was n't an option this time . 
@@44332736 @4332736/ <p> I 've got used to using a trackball instead of a mouse . I quite like it as I do n't have to move my hand in all directions . The only negative point is that I ca n't scroll in the image of resolve in the GUI . <p> just curious - how many people on this forum use a Wacom over a mouse and why ? So far , I have not seen anybody using a Wacom with Resolve , but maybe that 's just me ? <p> If not a tablet , who would prefer a trackball instead of a mouse ? <p> Thanks ! Mazze 43971 @qwx453971 <p> When I used to edit ( avid/fcp7 ) I could n't survive working more then 2 days without tablet . It would give me rsi related issues . <p> However , on Resolve , I work with a mouse always and have not encountered those problems , even after many long sessions . I think the control panel/keyboard/mouse combo is versatile enough to avoid issues that come from small repeatative movements . <p> From a user @ @ @ @ @ @ @ @ @ @ miss the ease of the scroll wheel , which has a lot of function in the UI . Also , some modifiers are layed out in such a way that you physically run out of tablet real estate when modifying them ( temp control bottom left comes to mind ) . <p> I use a tablet in Resolve . At first it was because of some jobs that require lots of rotoing . And eventually I feel easier to locate GUI controls , node graphs with a tablet and kept it ever since . Because you just move the pen to that particular point on the tablet , without first have to locate your mouse cursor , then start moving the mouse . <p> Also I wish I could do some of the things that require scrolling ( like zooming in the viewer ) with the tablet like it works in Nuke . ( Pressing alt key with middle mouse button ) 43971 @qwx453971 <p> i have that maped to both of thes slider on my intous3 , works really well to zoom in /out on viewer / timeline .. @ @ @ @ @ @ @ @ @ @ ialso have a macro that sends the cursor to either the viewer or timeline mapped to the transport 's softkeys , that has many uses beyond telling the wacom what to zoom <p> on teh wacom i also have ctl/atl/shift mapped to the left hand side buttons , ctl+c/ctl+v/ctl+b mapped to the right hnad side buttons for copying/pasteing nodes between shots , razoring a shot and marking groups of shots etc etc ... i really have gotten to the point that the kb is only needed for typeing in file names <p> Baselight have done alot of work on making the software wacom freindly , with an onscreen widget taht has very fine control , and DS always was built around wacom tablets from day 1 <p> my desktop as of this morning ; wacom front and center ... i 'm working in a comerical package in DS today , yesterday i was working in BLE all day , and last week it was mainly Resolve ... the tablet is where it so for a reason , maybe if i worked in nothing but Resolve maybe i 'd change it @ @ @ @ @ @ @ @ @ @ and XKeys , and when in BLE i use the small numberic pad for recalling mem 's and i have a bunch of kb shortcuts maped to transport - but ignore the twister / XKeys , Baselight 's mapping is very good already needs nothing really <p> but i always have the wacom pen in my hands no matter what the software <p> I use a large Wacom on my main system and a small Wacom with my laptop . <p> Tablets are easier on the arm/hand , you get to jump directly to the desired screen location , more precise when drawing masks , etc . <p> Main problem with large Wacom : Many clients have no idea what a tablet is and place cell phones and elbows on it . When I explain that it is actually my work surface and I need it all , they look at me very surprised . <p> Pen has not left my right hand since I started grading on Baselight nearly 10 years ago . I mainly grade Resolve now as a freelancer , but then pen remains . Tablet in @ @ @ @ @ @ @ @ @ @ if not behind panels ( when using Elements ) . Seems to be a good balance for both hands . 
@@44332737 @4332737/ <h> Newbie monitor advice <p> I 've been coloring for a few years . Not really professionally but I 've gotten decent at it . I use an Alienware 17 laptop for all my editing since I travel a lot of the year , I have to be able to work on the go . I got sick of the 17inch screen of my laptop while coloring for hours on end working at home so I looked up affordable monitors for coloring and purchased the Dell UP2516D . It 's only a $420 monitor so I was n't expecting too much . But it did boast great colors etc right out the box . I set it up and color looks awful on this thing hahah . I have it set up next to my laptop , and I my eyes might just be used to the Alienware screen from coloring on it for a few years now , but colors look way better on it . The new Dell monitor looks insanely contrasty and super saturated , especially shadows they are really really crushed . Maybe this is @ @ @ @ @ @ @ @ @ @ took a picture of the screens side by side you can check out below . You can see the drastic difference in the two . I do n't know much about calibration and what not , again I 'm not a professional but I spend a lot of time coloring and want to get better . But hopefully someone can give me some advice on what I need to do to get this monitor looking alright . Cause right now it looks awful to my eye ! Thanks ! <p> I hate to be the bearer of bad news but without proper calibration , neither monitor is probably accurate . You are just used to looking at one uncalibrated monitor for so long that another uncalibrated monitor is taking you by surprise . <p> The first step in color correction , no matter which other tools you use , is to have a properly calibrated monitor . Otherwise you are operating blind . You might be able to get within the ballpark using scopes ( and I hope you are using scopes ) but otherwise you are just guessing . @ @ @ @ @ @ @ @ @ @ worth of advice on selecting a proper grading monitor and how to calibrate it . <p> I hate to be the bearer of bad news but without proper calibration , neither monitor is probably accurate . You are just used to looking at one uncalibrated monitor for so long that another uncalibrated monitor is taking you by surprise . <p> The first step in color correction , no matter which other tools you use , is to have a properly calibrated monitor . Otherwise you are operating blind . You might be able to get within the ballpark using scopes ( and I hope you are using scopes ) but otherwise you are just guessing . <p> Please search this forum for the thousands of words worth of advice on selecting a proper grading monitor and how to calibrate it . 43971 @qwx453971 <p> That 's what I suspected ! Thanks for the advice . This is unfortunately the most expensive monitor I could afford . And I 've been able to do pretty good work on my Alienware screen thus far in the past 2 years so I 'd @ @ @ @ @ @ @ @ @ @ get it at least half way decent for grading ? I 'll start looking up calibration . I know there 's a lot of expensive devices that are used for this , if you have any suggestions on how to do this without spending money that would be awesome . Blew everything I had on this monitor and I do n't make much currently ! <p> That 's what I suspected ! Thanks for the advice . This is unfortunately the most expensive monitor I could afford . And I 've been able to do pretty good work on my Alienware screen thus far in the past 2 years so I 'd like to think I can calibrate this new monitor to get it at least half way decent for grading ? 43971 @qwx453971 <p> Steve Shaw at Light Illusion has a good piece on his company 's website titled " Why Calibrate ? " <p> There have also been quite a few discussions here on LGG about the necessity of calibration , and also why low-cost computer displays are generally inadequate for that task . You need a color-managed @ @ @ @ @ @ @ @ @ @ 're color-correcting . It is possible to calibrate computer displays to a point , but I would n't trust them for grading . <p> But even if you find a preset it will be far away from halfway accurate . You 're still not watching a standardized representation of your work which means you will do it differently on different devices . <p> I 'd try to rent at least a colorimeter like the i1 Display Pro and try to calibrate with DisplayCal ( freeware ) . Then take the resulting LUT and put it in Resolve in the Video Monitor Output LUT option . And this wo n't work without a video I/O device like the Blackmagic Design Mini Monitor . <p> Having a standardized and accurate representation of your footage is the first and most important step in setting up a grading place . <p> But even if you find a preset it will be far away from halfway accurate . You 're still not watching a standardized representation of your work which means you will do it differently on different devices . <p> I 'd try to @ @ @ @ @ @ @ @ @ @ and try to calibrate with DisplayCal ( freeware ) . Then take the resulting LUT and put it in Resolve in the Video Monitor Output LUT option . And this wo n't work without a video I/O device like the Blackmagic Design Mini Monitor . <p> Having a standardized and accurate representation of your footage is the first and most important step in setting up a grading place . 43971 @qwx453971 <p> Thanks for the input guys ! I 've quickly learned how important accurate representation is . I 'm just not at the level where I think I can afford perfect calibration . I do n't color for big feature films or massive clients either . Most of my stuff ends up online and clients have always been happy with the coloring I 've done on my laptop screen . So I think I need to settle with having the best calibration I can get on this mediocre monitor . And the time will come where I 'll be able to afford a better monitor and tools . <p> For now let me know what you think would help @ @ @ @ @ @ @ @ @ @ adjustments like rec709 , gamma and color temp .. Where do I make these adjustments ? The monitor itself does n't have these adjustments apart from rec709 and brightness . Are you talking about using some software ? <p> If I calibrate the monitor with that would I be set or at least on the right path ? Let me know your thoughts . And thanks again guys ! Really appreciate the advice . This is all really complicated to someone like me . <p> Thanks for the advice . I 'm going to try to get my hands on an i1 display pro and attempt calibrating this thing . <p> What settings should I have on the actual monitor itself ? And dose the i1 display pro come with it 's own calibrating software or can I just use displayCAL for free is that good enough ? I certainly ca n't afford anything on light illusions website. 
@@44332738 @4332738/ <h> Passing QC Questions <p> Hey all , I 've picked up a feature that another colorist could n't get to pass QC , there were other issues as well , but I 'm fixing the color . Largely the issues that I 'm seeing have to do with inconsistent grading . It was very , very rough with a lot of odd stuff going on , ca n't really explain a number of choices that I saw . I have n't had much in the way of issues passing QC before , and the producer and director wants it to pass on the first try because these QC 's are n't free . <p> Only two notes really pertain to color : - " Ex. of raised black levels ( 40mV ) throughout program ( appears to be intentional creative choice ) " - " Ex. of uneven color timing and very flat looking picture with little contrast ( intermittent throughout program ) " This one 's actually written twice at different timecodes . <p> Questions - Is it common that QC people keep a super close eye @ @ @ @ @ @ @ @ @ @ ? Or was that just to point out that it was maybe not illegal but perhaps unappealing ? <p> What are some things that get caught up in QC a lot that I should be on the lookout for ? I do n't want any " gotchas " that maybe were n't written since this was in a very incomplete state when they tried to get it to pass the first time . <p> I would write a very detailed QC of your own , when you submit , to help it pass first time . That way if the person QC'ing has any issue then they can check your note and see that there is a reason or logic as to why it 's that way . <p> I would write a very detailed QC of your own , when you submit , to help it pass first time . That way if the person QC'ing has any issue then they can check your note and see that there is a reason or logic as to why it 's that way . 43971 @qwx453971 <p> That 's what I @ @ @ @ @ @ @ @ @ @ that warns the QC person of specific content issues that were deliberate or have issues that the producers are aware of that will not be changed . ( I euphemistically call this my " excuse list , " but it 's basically a " pre-QC List " so they know when a glitch is not really a glitch -- stuff like that . ) <p> Inconsistent blacks and weird levels like that sound like just bad color decisions to me , but it all depends on context . <p> Two words : " Artistic Intent " . The best advice I can give is that if your project has a look that veers from standard contrast and/or tonality make sure to include what is referred to as a QC alert sheet . Print out a list of example timecodes , and descriptions . something like " example : elevated black levels " followed by " Artistic Intent " . You wo n't get rejected for it , but it 'll will still be listed as a note on the QC report , ( not as a rejection ) . Now @ @ @ @ @ @ @ @ @ @ as easily signed off on . ... Unless you can actually validate it as a filmmaker choice . It 'll be up to them then to justify it at that point . <p> I 've never heard of something not passing QC based on creative choices . If the producers signed off on the color before it went to QC it is what it is . As for QC I thought it 's Just technical inconsistencies . Like flash frames or interlacing on a progressive timeline . Interesting . <p> Questions - Is it common that QC people keep a super close eye on the black levels as to note a specific measurement ? Or was that just to point out that it was maybe not illegal but perhaps unappealing ? <p> What are some things that get caught up in QC a lot that I should be on the lookout for ? I do n't want any " gotchas " that maybe were n't written since this was in a very incomplete state when they tried to get it to pass the first time . 43971 @qwx453971 <p> The @ @ @ @ @ @ @ @ @ @ their client . Although elevated black level is not really a problem in the technical sense . For example you could have a dark grey solid Bg graphic that would both have " elevated black " and would certainly be a creative choice . But lifted black level in live action material is more often than not a sign of poor craftsmanship . That 's why they are giving you an example . And , that 's just one example of probably many occurrences in the program . <p> Specifically color wise , they may react to heavy clipping . They could also flag illegal Y'CbCr and/or RGB gamuts . <p> I 've never heard of something not passing QC based on creative choices . 43971 @qwx453971 <p> I have had to fight for that before , particularly with Canal+Plus in France and Network Ten in Australia . <p> The only QC stuff I generally agree on are out-of-gamut issues , excessive levels , distortion , glitches , black frames , titles out of Safe Action , motion artifacts , and stuff like that . Color decisions are purely artistic @ @ @ @ @ @ @ @ @ @ it 's specifically in one direction . <p> I have seen some weird things get through QC : just last night I was watching The Making of Star Trek : Beyond on Blu-ray , and all the film clips had blacks up to about 25 , but the ( new ) interview clips had blacks at zero and looked fine . My guess is that whoever put the clips together had a video/data range problem and nobody noticed it . <p> I have seen some weird things get through QC : just last night I was watching The Making of Star Trek : Beyond on Blu-ray , and all the film clips had blacks up to about 25 , but the ( new ) interview clips had blacks at zero and looked fine . My guess is that whoever put the clips together had a video/data range problem and nobody noticed it . 43971 @qwx453971 <p> Or no one paid to give a shit . Famous last words . " JUST GET IT OUT ! ! ! " <p> I have had to fight for that before , particularly with @ @ @ @ @ @ @ @ @ @ The only QC stuff I generally agree on are out-of-gamut issues , excessive levels , distortion , glitches , black frames , titles out of Safe Action , motion artifacts , and stuff like that . Color decisions are purely artistic , and you ca n't argue that , particularly if it 's specifically in one direction . 43971 @qwx453971 <p> The only ' creative ' choice that I 'd worry about for QC is the grade being very dark . ( I might have flagged things like that at Network Ten back in my QC days , sorry if it was you , Marc ) <p> I have had to fight for that before , particularly with Canal+Plus in France and Network Ten in Australia . I have seen some weird things get through QC : just last night I was watching The Making of Star Trek : Beyond on Blu-ray , and all the film clips had blacks up to about 25 , but the ( new ) interview clips had blacks at zero and looked fine . My guess is that whoever put the clips together had @ @ @ @ @ @ @ @ @ @ @qwx453971 <p> Or it was intentional to separate them somehow on creative level . <p> I do a documentary now and director asked if i can add some of the digital noise that material gives on digital cam when under exposed making it more indy and doc type bad looking . <p> But lifted black level in live action material is more often than not a sign of poor craftsmanship . That 's why they are giving you an example . And , that 's just one example of probably many occurrences in the program . 43971 @qwx453971 <p> Not sure I 'd agree with you there , but even if true that 's still admitting that there are creative choices that dictate lifted blacks ( the not ) . Inconsistent black levels yes , likely a sign of poor craftsmanship ... <p> The only ' creative ' choice that I 'd worry about for QC is the grade being very dark . ( I might have flagged things like that at Network Ten back in my QC days , sorry if it was you , Marc ) 43971 @qwx453971 @ @ @ @ @ @ @ @ @ @ the air nowadays that are tons darker and murkier than anything we got away with in the standard-def 1990s . If anything , I occasionally have skirmishes and " creative discussions " with clients , urging them to bring the highlights up a little bit in dark scenes . But in truth , a lot of dramatic shows that have a lot of night scenes have lots of fill ; check out the Marvel Daredevil and Luke Cage shows on Netflix , and even the night scenes have a reasonable level and are lit very well . <p> I think Canal+Plus was worse than Network 10 ! <p> The wackiest QC story I can think of was I once had a major horror feature rejected by HBO in the early 1990s . My boss warned me the master was coming back and I should take a look at it . We had done the job a year before , so I barely remembered seeing it . The master came in , we put it up , and the first thing we saw was a slate I had dropped in that @ @ @ @ @ @ @ @ @ @ in big red letters . Unbeknownst to us , the client took a quickie transfer I had done in a day , which was supposedly just for clips and TV spots , and just shipped it intact to HBO as a final transfer . We had a good laugh on that one , and were able to compel the client to allow us to spend five days on a " real " transfer for final color ... and that one had zero QC issues . And they paid for it , too . 
@@44332739 @4332739/ <h> Projector for Screening Room <p> I 'd like to start by saying that I realize I 'm asking for a solution to an impossible problem . My needs for this space are mutually exclusive , so I know I 'm not going to get everything . So what I 'm looking for is your ideas about the best set of compromises . From everything I 've heard , this is the place to go for expert opinions ! <p> I run a VFX house which has up until recently relied exclusively on plasma displays for monitoring and client review . I 'm trying to add a projection setup . Here are the constraints . <p> Given space considerations , there 's no other option than to combine the screening room and my office . There will be enough space for both ( desk at back , 2-3 rows of seating , 10-15ft screen ) , but probably no room for a separate AC closet for the projector , so the projector will need to be relatively cool and quiet , so I 'm leaning towards a laser phosphor @ @ @ @ @ @ @ @ @ @ the space can be made pitch black on demand , probably 80% of the time there will be a substantial amount of ambient light . For that reason , I 'm considering either using this screen : LONG ... or doing a dual-screen setup where there is a retractable traditional white screen in front of a permanently mounted black screen . For client reviews , if possible , it would be great to have a setup which is fully DCI compliant . Not strictly necessary , as we usually hand off to a color house for final finishing , but useful for picky clients . <p> Projector native resolution needs to be at least 1920x1080. 2k would be better . 4k would be nice , but not necessary at present , and it looks like the cost/quality curve precludes getting a 4k projector . <p> As you can imagine we 're going to be very cost-sensitive , so the more economical the better . I do n't think I want to spend more than 30k on a projector . <p> What do you think ? Any specific projector recommendations ? @ @ @ @ @ @ @ @ @ @ so much in advance ! <p> How does this projector perform in terms of color fidelity ? It seems brand new , right ? I ca n't find a single qualitative review online . Any other recommendations ? For example something that sacrifices some brightness for better color/contrast ? <p> How does this projector perform in terms of color fidelity ? It seems brand new , right ? I ca n't find a single qualitative review online . Any other recommendations ? For example something that sacrifices some brightness for better color/contrast ? 43971 @qwx453971 <p> Projectors always need to be calibrated for professional use and Panasonic ones offer extensive calibration controls . <p> Color is standard Rec.709 . Laser already has the best contrast right now if you want something even better you are looking at 3-chip laser projectors or RGB laser lights which are way out of your price range . <p> I think in a room that size , you 'd be much better off with a large OLED display , like a 65 " . You really need a separate room to cool the projector down @ @ @ @ @ @ @ @ @ @ with trying to work in a small room with a projector is that the fan noise will make you crazy . I 've tried it and it 's not pleasant . <p> I do think a screening room and a work room kind of have different considerations , and a lot boils down to throw distance and ceiling height . You can make it work , but it 's not cheap and requires some outside engineering skills to pull it off . <p> Hey Marc , thanks for your thoughts . Do you think the noise/heat problems will persist even with the new generation of laser projectors which I 'm lead to believe are substantially quieter and cooler than traditional lamp-based projectors ? <p> Hey Marc , thanks for your thoughts . Do you think the noise/heat problems will persist even with the new generation of laser projectors which I 'm lead to believe are substantially quieter and cooler than traditional lamp-based projectors ? 43971 @qwx453971 <p> I 'd have to read the specs , but I would look at published noise figures , power requirements , and throw distances @ @ @ @ @ @ @ @ @ @ to put a ventilated baffle around the projector and it was manageable . Ceiling height is still an issue . If I get a chance , I 'll post some tech papers I 've collected over the years on building DI theaters. 
@@44332740 @4332740/ <p> do you think they might allow Tangent to access their base code to implement that and defacto outsource the panel ? <p> always hopefull , but i 'm thinking only a third party dev like Tangent would create a panel that could work with Resolve , Nucoda and Baselight <p> then the Tangent folks would have to re-think the sub-optimal ergonomic 's of Elements with knobs squeezed together and placed far from the center of the panel <p> add a fourth ring/ball to Elements , and sell us a Kb/Bt combo of 8 Kb + 8Bt underneath to go either side of the Tk , they could be close enough if mapping was massivly improved , be it by BMD or by Tangent , or best yet , allowed to be user mapped like Nucoda does <p> hopeful , but not holding my breath .... Slate is so very elegant , Elements is so very inelegant , polar opposites really , with Color somewhere in the middle . but with knobs near the rings/balls it 's leaning towards the elegant side of the equation <p> Do you think Blackmagic @ @ @ @ @ @ @ @ @ @ as well as Resolve ? Maybe around 5k US and about half the size of the advanced panel ? 43971 @qwx453971 <p> Hey , Steve . I do n't think there 's a chance of that -- I think the market is n't profitable enough . <p> I would recommend the Tangent Elements as being the best compromise between everything else on the market , then the full-size daVinci Panels . I think Elements + a small macro keypad like an XKeys 24 can be extremely efficient and productive . I can move along fairly fast with this kind of setup : I 've done six indie features in the past 3 months , budgeting about 40 hours per job . One took closer to 80 , but that was a client-driven decision . Without Elements , I think they easily would 've taken 15%-20% longer , or I 'd have to do a crappier job in the same number of hours . <p> What I would like to see is for BMD to do a " daVinci Control Surface II " project where they essentially built the same panel @ @ @ @ @ @ @ @ @ @ surface . The closer they could get to a Baselight Blackboard 2 , the better . <p> I would also like to see a smaller Davinci panel but also mapped for editing functions , since I 'm using Resolve as a NLE , and quite loving it . Though It would have to be considerably better than the Element or JL Cooper . <p> I would also like to see a smaller Davinci panel but also mapped for editing functions , since I 'm using Resolve as a NLE , and quite loving it . Though It would have to be considerably better than the Element or JL Cooper . <p> In truth , I 'm nearsighted enough that it 's an effort to see 3-point type on the keys , but at least I know what letter I 'm hitting when I 'm in a pitch-dark room . It 's actually pretty slick and works well . <p> Can anyone tell me or point me to a thread for Elements and Resolve ? I went to the Tangent web page to see if there was an update for the @ @ @ @ @ @ @ @ @ @ Resolve is not a listed application ? Seems like the association keeps getting farther apart . <p> I especially like the idea of having encoders on the Tk . If that came true , there would be hardly a reason to choose the Avid panel over the Tk - especially considering that a Tk still has upgrade options . This plus a mixed button/encoders Element would really add more comfort to the Element series . Maybe make those the " Pro " series ; - ) . <p> seems like there would not be a large investment in R&amp;D on Tangent 's part to make a " pro " surface happen , mostly seems like the costs would be in mfg , and tooling <p> the encoders above the rings are a huge plus for using Artist surface daily , as are the prograbale buttons <p> the major thing that bugs me with Elements is the layout of the knobs , far away , long reach , too close to each other , etc etc .. haveing fewer knobs above buttons makes it much easier to reach the knob one @ @ @ @ @ @ @ @ @ @ accedently <p> And buttons that can opt out of Resolve 's control , like the atist panel does , there is not a single button on Artist that uses default mapping in Resolve on my system anymore . The mapping of the buttons on Elements are for the most part easly replaced with mapping KB shortcuts or macro 's , like Artist there 's little reason to be stuck with the default mapping , well unless it ca n't be changed , but maybe the lack of change could change .... <p> As in blaming BMD for the lack of Prores support on Windows , suggesting Tangent do this huge investment in R&amp;D and manufacturing makes little sense . Of coarse Tangent would LOVE to sell you a more expensive version of Elements . So , let 's say they did just that and delivered this awesome new Pro panel . Then what ? Unless BMD agrees to support it , then it 's an unmitigated financial disaster for Tangent . Does anyone believe BMD would be interested in supporting a relatively inexpensive option to their $30K panel ? On @ @ @ @ @ @ @ @ @ @ buttons and knobs combined with the ability to assign shortcuts , screen coordinates and menus- a-la Quickeys or Keyboard Maestro would make this an interesting option . <p> So , let 's say they did just that and delivered this awesome new Pro panel . Then what ? Unless BMD agrees to support it , then it 's an unmitigated financial disaster for Tangent . Does anyone believe BMD would be interested in supporting a relatively inexpensive option to their $30K panel ? 43971 @qwx453971 <p> I bet it would n't be a stretch to create a " workalike " panel that duplicated every single button and function of the large DaVinci Control Surface and put it in a smaller package . But it 's fair to say there 's proprietary firmware in the panel that would allow BMD to take legal action against anybody who tried it . <p> What I 'm not sure of is whether BMD would agree to license somebody to make a workalike panel and pay them a royalty . I honestly do n't think they make a big profit on the panel . Bear @ @ @ @ @ @ @ @ @ @ daVinci 2K customers , was about $100,000 ten years ago . I would bet that $30K per system barely pays for Linux development and manufacturing the panel . <p> BMD has said there are " over a million " Resolve users around the world , but that 's never been broken down by OS . And I suspect that number came from the number of downloads , vs. the number of actual users . <p> I honestly do n't think they make a big profit on the panel . Bear in mind that same panel in ethernet , sold to daVinci 2K customers , was about $100,000 ten years ago . I would bet that $30K per system barely pays for Linux development and manufacturing the panel . 43971 @qwx453971 <p> I would be shocked , if the BMD panel cost more than $5k to produce . And if it is , shame on them . The only expensive parts on that panel are the trackball encoders and knobs encoders , which when bought in volume ca n't be that expensive . FilmLight 's Slate is way more advanced and @ @ @ @ @ @ @ @ @ @ less than half the price of BMD panel . So , yes , there is a HUGE mark up on that panel . 
@@44332741 @4332741/ <h> DCDM packaging software and/or CPL XML template <p> My company 's recently been asked to deliver a DCDM of a film to our licensor , and while I understand the basics of DCDM creation ( bucket full of specifically-named X'Y'Z ' TIFFs and WAVs ) , I 'm not very clear on what software options are available for creating the acutal DCDM package or at least the CPL XML file . None of the more affordable DCP creators ( we use EasyDCP 's resolve plugin ) seem to create DCDMs . <p> Are there any DCDM creation options besides Rohde &amp; Schwarz DVS Fuze ? <p> Or is the CPL something that can be created manually if you have the right template ? <p> " Yes " to all the above questions . DCDM is technically not a package . It 's just a directory full of TIFF sequential files in XYZ color space . DCP is the package that contains MXF wrapped media encoded from the DCDM along with other supplementary files and CPL which is a text file describing how all these assets combine together . @ @ @ @ @ @ @ @ @ @ manually , except the encoding part , it would be very impractical to do so . Resolve has a fee based DCP option . There is also an online service where you upload your assets to a Dropbox , DCP is encoded and packaged in the cloud and you download it from another Dropbox . I forgot the name of this product . There are several other options as well but they may not be applicable in your case . <p> Also keep in mind that from a practical working point of view , the " DCDM " can be any final delivery in a known colorspace . For example , a rec709 gamma 2.4 ProRes file or a 10bit P3 gamma 2.6 DPX sequence could be the final deliverable " DCDM " and the conversion to XYZ could happen at the same time as compressing to Jpeg2000 . <p> I 've had good results using openDCP to make DCPs for review screenings etc . It will take care of making CPL , XML and such . <p> 16 bits each per X ' , Y ' , and Z @ @ @ @ @ @ @ @ @ @ G and B channels . <p> The DCDM gamma-encoded X ' , Y ' and Z ' color channels are represented by 12-bit unsigned integer code values . These 12 bits are placed into the most significant bits of 16-bit words , with the remaining 4 bits filled with zeroes . <p> The image orientation shall place the first pixel in the upper left corner of the image . <p> The DCDM picture file shall contain only the active pixels in the image . In other 
@@44332742 @4332742/ <p> I just did a short HDV project for a friend of mine the other day , and I had no problem doing 29.97 1440x1080 . It did take me a couple of tries to figure it out . Basically , during color correction you have to manually change the pixel size with Clip Attributes to widen it out to 16x9 , and then when you deliver it , you go back to the original size and render at 1440x1080 . It can be done , but it takes thought and patience . <p> It seems like the problem clips were a few jpegs . I removed them from the media pool and it seems to be rendering fine . Why could n't width or height be an odd number ? 43971 @qwx453971 <p> On our Lasergraphics scanner , when we write files direct to ProRes , we can make them any resolution we want , provided each dimension is a multiple of 16 -- which makes sense , since ProRes is a compressed format , and 8 or 16px blocks are common in image compression . And because @ @ @ @ @ @ @ @ @ @ really an issue . <p> I 'm surprised that Resolve does n't just pad the jpgs with black if they 're too small to fit the timeline size , though . <p> BTW , the HDV project I did was also a roundtrip back to FCP , in this case FCP7 . I du n no if there 's an issue with FCPX or not . HDV is a bizarre format that I wish had gone away 10 years ago . It 's annoying that it hangs around ; it 's essentially a pseudo-HD version of 720 , and they 're doubling the 720 to 1440 and then using rectangular pixels to get it out to 16x9 in playback . My friend does enough of these with old equipment that I 'll probably just keep an old session lurking around so I do n't have to do a trial &amp; error on the settings again for 1440x1080. 
@@44332744 @4332744/ <h> Alexa RAW vs . Alexa ProRes <p> Fellow colorists , I had a somewhat heated debate last week with a DOP about the differences between Alexa RAW and Alexa ProRes . Discussed at what level of budget did it make sense to shoot RAW . I have been of the opinion that any indy movie ( not VFX heavy ) under 10 million USD should just shoot ProRes . Let 's take K or pixels out of the equation for a more apples to apples comparison . Obviously they should shoot LogC . Admittedly , I have graded maybe 20 features from Alexa ProRes , and only a handful from RAW , and the RAW ones were a few years ago . DOP believes that exposure latitude , noise levels , and overall colorimetry are so far superior that even $500K USD or lower films should shoot RAW . In fact it makes more sense for them since they can save on lighting . So , my question to you all would be : Have you seen THAT drastic a difference between the formats ? Are we talking @ @ @ @ @ @ @ @ @ @ processing ? <p> Admittedly , I have graded maybe 20 features from Alexa ProRes , and only a handful from RAW , and the RAW ones were a few years ago . DOP believes that exposure latitude , noise levels , and overall colorimetry are so far superior that even $500K USD or lower films should shoot RAW . In fact it makes more sense for them since they can save on lighting . 43971 @qwx453971 <p> I disagree with the DP and agree with you . The ProRes Log-C 444 files will look fine . The Arriraw wo n't gain you anything except require more disk space and a more complex workflow . No way will it make a difference in terms of lighting or noise , not one bit . <p> Maybe you can tactfully suggest a camera test where both Log-C and Arriraw files are used side-by-side . If they ca n't tell the difference in a blind A/B test , go with the Log-C . I do n't know of too many 12-bit or 14-bit monitors out there , so seeing that extra range will be @ @ @ @ @ @ @ @ @ @ I know of that shot Arriraw ( including the Marvel films ) shot Arriraw but then converted to DPX Log for post . So technically , even they are n't using the Arriraw ... though admittedly these are very effects-heavy films . If there 's 3500 total shots , and 2000 of them are composites , it 's a lot simpler dealing with all DPX files than it is to deal with part-Arriraw , part-DPX . <p> I find it quite amusing , that this is even a questinon for high and mid budgets . Even as a poor ( often one man show ) bottom feeder , I shoot nothing but raw since 3 years now , and never looked back . <p> If I can afford the drive space ( heck drive space is way cheaper than SR-tapes theese days , and you can reuse it - a 3TB drive is less than 100 bucks now ) , and I can see no big difference in workflow too . At least noting to write home about . <p> I go with Frank here . It 's not the @ @ @ @ @ @ @ @ @ @ post . The series I just finished was shot ProRes and overall I like it and can do nearly everything I want . But pulling keys shows the compression clearly ... All in all the difference is storage demand . Regarding a possible sell to streaming services a higher resolution is welcome to as they better like UHD content . <p> There 's a big difference on set if I 'm DITing SxS mags or Codex mags . With ProRes to the SxS , I 'm often wrapped and packed before the ACs are . Unless there 's sound-syncing needed , in which case I 'm out usually 15 minutes later . With Codex mags , I can be out 1 to 2 hours after most people have gone home . Most , but not all . If I 'm still on set , that means the Genny Op is still on set , and the driver of the camera truck is still on set and some PA 's are still on set and usually a line producer is still on set and security is still on set and sometimes @ @ @ @ @ @ @ @ @ @ is ESPECIALLY true if production tries to save money on drives , and buys enough storage space , but not fast enough writing storage . Oh , and it 's not viable for me to make editorial files and dailies on set if we 're shooting open gate Arri Raw . Not gon na happen . So you also need a near-set guy generating those files , with his/her own system . <p> I 'm not saying do n't shoot RAW . RAW is better . But it has some hidden costs before you even get to post that are easy to overlook and not account for . Does n't matter how good your images look if you run out of money and ca n't finish the movie ; no one will ever see your pretty frames . <p> In 2 years max , everybody is editing in raw , just because they can . Much like in 2005 , where we stopped doing proxies for Avid , since Premiere was able to handle the native files in realtime . <p> ... and running out of money , because of @ @ @ @ @ @ @ @ @ @ last summer and the cost for storage and backup was in the neighborhood of 1000 bucks . 43971 @qwx453971 <p> $1000 ? You sure about that , for a properly redundant setup ? There 's no question that drives are way cheaper than film stock for a feature , and that the price per GB has been plummeting , but the 2 indies I did last summer spent between $5k-$10k on storage in total , and we were not using the quickest drives out there , nor does that factor in storage on the side of editorial , it 's just storage on set and shuttle drives back and forth . An extra $2k on storage would have cut out at least $2k in OT across the board . <p> Regardless , my point was n't that Gigabytes are expensive , but that man-hours add up . Prohibitively so ? No , if you have the right people . But also make sure to account for the often over-looked things I mentioned above . <p> I 'll back up Nick on this . DIT and dailies costs are increased quite @ @ @ @ @ @ @ @ @ @ working in a standard way . If I 'm doing an Arri RAW job I bring a beefier system that can handle transcoding . That 's an extra weekly cost that would n't be incurred with ProRes . And as Nick said , it usually means a lot of overtime , forced calls for more than just the DIT . And RAID cards are required to keep the OT manageable and allow dailies on set . To some degree it really depends on how much footage is shot . Many directors/ADs/DPs have gotten sloppy with the transition to digital when it comes to how much footage they shoot . Some directors just never want to cut . Even when they 've been informed that footage gets pricy in OT on Arri RAW they are too used to rolling and rolling . So on movies where you have a Director or AD that wants to keep rolling , or just content that requires a lot of coverage you make the RAW thing even more expensive . If you are super efficient and conservative with how much you shoot then it is @ @ @ @ @ @ @ @ @ @ camera ( which I 'm seeing more and more these days ) doubles the extra cost . <p> Shooting RAW will also slow the delivery of dailies to editorial , director , producers . If shipping , it could miss a FedEx cutoff time and each day 's footage would be shipped an entire day later than if ProRes was being used . Editorial usually is n't happy with that . <p> Regarding editing RAW , that 's likely not going to happen in the mainstream standard workflows . Here is the first reason why ( which technology could overcome ) editorial proxies are important not just so the editor can have a lightweight format to work with , but because sound is synced to it , and a look is burned in , a look that the DP , Director and DIT created . And a look that the DIT is doing best-light grades with matching scenes/takes ( which has become more necessary since digital because IR filters that introduct so much tinting from filter to filter ) . Also scene-take information is added to the Avid ALE in @ @ @ @ @ @ @ @ @ @ set to give editorial a running head start requiring less busy work for the AE , and more time for rough assembly . <p> The second reason this will probably not happen is that camera manufacturers are always pushing the envelope in quality , bit depth , data rate , resolution . As soon as RAW editing really actually becomes more feasible then technology has come to a point where camera manufactures are n't afraid to make their files even more heavy duty and less directly editable . I think if camera manufactures normalize and stop doing this then sure it could happen . If Mores Law ( in the camera/sensor world ) slows to a hault then sure it could happen . <p> I do n't remember 2005 being when mainstream movie makers stopped using avid and editing proxies . Every movie I 've DIT 'd except one has been an avid job with DNX proxies . <p> If we 're talking about ultra low budget , then maybe Premiere is a thing , and if no proxies in premiere is a thing , then maybe a DIT managing @ @ @ @ @ @ @ @ @ @ and maybe a PA is just sitting with a laptop and USB 3 codex reader in a hotel room downloading to single western digital drives . <p> In this case , sure RAW would n't cost much extra . A PA is virtually free and you can have them working on a flat rate from a hotel . But this is a scenario that would only happen on an ultra low budget feature . <p> I 'm not arguing against RAW . I like RAW . It does provide some extra room to work . And I love the overtime and the extra gear rental . <p> I think RAW 's biggest advantages is 3K capability . But Arri 's new offerings allow ProRes up to the full sensor resolution , thus weakening the argument for RAW even further . <p> I completed a feature recently that simultaneously recorded 2.8K ArriRAW to the Odyssey Q7 recorder , and 2048x858 ProRes 4444 Log C to internal cards . We conformed the ArriRAW , naturally , but the copy of the media that was delivered to our office had a handful of @ @ @ @ @ @ @ @ @ @ in those instances as a placeholder and colored them while the replacement ArriRAW was shipped to us . <p> When I got to the step of copying color , I spent a little time comparing image quality ( 2K signal on a DP2K-P projector ) and my opinions are : <p> - There 's an appreciable difference in the texture and feel of the image . - The ability to really push keys and low-light scenes around is better on ArriRAW. - Neither of these differences are so significant that you 'd want RAW to significantly impede your shoot . - The idea to order a different lighting package because you 're shooting RAW is a terrible one ; the difference is n't going to save you if you 're always underlit . <p> The movie has an emotional climax where two characters stand in a field at dusk with the sunlight dying behind them . They shot the scene so fast they did n't even cut the camera , just shot the wide , ran up to get one close-up and sung around to get the other . But @ @ @ @ @ @ @ @ @ @ in the edit , you see the sky jump on each cut . It took extensive keying and low-light pushing , and it was probably the only scene in the movie where I felt the RAW presented a real opportunity that the ProRes did n't , and at exactly the right moment for it to pay off in the story . <p> $1000 ? You sure about that , for a properly redundant setup ? There 's no question that drives are way cheaper than film stock for a feature , and that the price per GB has been plummeting , but the 2 indies I did last summer spent between $5k-$10k on storage in total , and we were not using the quickest drives out there ... 43971 @qwx453971 <p> That 's closer to the answer I would give as a post supervisor . Total footage for a 4 week shoot and one or two cameras is going to be on the order of 10TB , and you need two sets of backups , plus a set of drives with proxies for the editor , and a backup for @ @ @ @ @ @ @ @ @ @ super-fast RAIDs . <p> Thank you all for the comments . Good stuff . The DOP has done extensive testing and comparing side by side with both the RAW and ProRes . I have not . I have only graded one or the other . I 'll talk someone into shooting a test for me . Funny , there was an entire feature I graded last year and I thought ( assumed ) that I was working with ProRes the whole time but it was all RAW . Obviously , I did n't do the conform on that one . Regarding spatial resolution , I 've graded a bunch of jobs recently that were 3.2k PR4444 . Looked great . Regarding the differences in bit depth , part of our discussion included grading for HDR 2k nits . I would think that the added depth would definitely help here . <p> Another discussion topic was that in any grading system , the RAW would be debayered into something to work with . In my case , it is always LogC and I work from there . I would assume that @ @ @ @ @ @ @ @ @ @ big difference , but I have n't done that in a few years . My DOP friend said to skip the whole LogC space and grade from the RAW . I have a feeling he IS referring to sceneLinear . I 'm grading with him again later this week . I 'll try and clarify a few things . <p> Total footage for a 4 week shoot and one or two cameras is going to be on the order of 10TB , and you need two sets of backups , plus a set of drives with proxies for the editor , and a backup for that . $10K would be cheap drives , not even super-fast RAIDs . 43971 @qwx453971 <p> Last time I checked a 3TB HD was around 100 bucks . So $400 would buy you 12TB , plus 2 x $400 = $800 for backup . That 's $1200 plus - lat 's say an other $400 for proxie drives and backup . So you end up with $1600 - and that 's only for the fist time . On your next film you can reuse most @ @ @ @ @ @ @ @ @ @ <p> Regarding editing RAW , that 's likely not going to happen in the mainstream standard workflows . Here is the first reason why ( which technology could overcome ) editorial proxies are important not just so the editor can have a lightweight format to work with , but because sound is synced to it , and a look is burned in , a look that the DP , Director and DIT created . And a look that the DIT is doing best-light grades with matching scenes/takes ( which has become more necessary since digital because IR filters that introduct so much tinting from filter to filter ) . Also scene-take information is added to the Avid ALE in this process . Basically the proxies allow the DIT on set to give editorial a running head start requiring less busy work for the AE , and more time for rough assembly . 43971 @qwx453971 <p> If you want to look into the future , you have to think out of the box Thatcher . Even today it is possible to have synced sound and a look , including metadata and @ @ @ @ @ @ @ @ @ @ that . I know , cause I do exactly that since quite a while . <p> With faster CPU/GPU , cheaper drives , and better programs , this will be even easier , faster , and cheaper in the future . And yes , Avid will probably not be a part of it - unless a miracle happens to them . <p> I do n't remember 2005 being when mainstream movie makers stopped using avid and editing proxies . Every movie I 've DIT 'd except one has been an avid job with DNX proxies . <p> If we 're talking about ultra low budget , then maybe Premiere is a thing , and if no proxies in premiere is a thing , then maybe a DIT managing looks and data on set is n't a thing , and maybe a PA is just sitting with a laptop and USB 3 codex reader in a hotel room downloading to single western digital drives . 43971 @qwx453971 <p> The " mainstream movie makers " use Avid , cause they have the time and budget to waste , for that sort of @ @ @ @ @ @ @ @ @ @ n't want to learn a new NLE ) . If you are on a lower budget , you must be much more efficient than the big boys , cause you do n't have time and money to waste . That does n't mean , that you have to go back to a time and money wasting workflow , onc you get a bigger budget . <p> Guys like Fincher - who has an enormous understanding of the whole post process , cause that 's where he is coming from - realized that . Now he talks hours in interviews , about how much time and money they saved , switching over to Premiere/AE for Gone Girl , and how smooth the workflow was , compared to the old ways . <p> Fincher is a guy who is able to think out of the box , which is pretty rare for " old Hollywood " - but times are slowly changing , and the new breed of filmmakers have absolute no problem to adapt new stuff in their workflow on a daily basis , if it helps them to be more @ @ @ @ @ @ @ @ @ @ , spread out over 30 days , that 's 120TB of material , a minimum of 30 drives . Three copies ( one plus two backups ) is 90 drives , plus another set for editorial . And then there 's the final deliverable drives , plus audio drives for the sound editors and mixers , and final deliverable drives . Let 's say it 's 120 total 4TB drives at just $120 each ... that 's $14,400 right there . My $10,000 estimate may be low . And you need very fast RAID drives for 4K color , so those will cost a lot more . <p> The last pilot I did , they shot more than 4TB per day ( on Alexa Log-C Pro Res ) because it was A&amp;B Camera , plus they sometimes had a C camera shooting second unit or stock footage . The overload of excessive footage is becoming a bigger problem every year , not getting better , particularly with features . <p> Fincher is a guy who is able to think out of the box , which is pretty rare for " @ @ @ @ @ @ @ @ @ @ 43971 @qwx453971 <p> Fincher is 52 . You think he 's " old Hollywood " ? <p> Whenever I have to deal with a young tyro under 30 , usually they have many , many false impressions about how much time and money something is going to take in post . By the end of the project , they usually get it , but the first couple of projects , they have to learn how to scale down their expectations in relation to the realities of scheduling and budget . It 's true we can fix a lot more things now in post than ever before , but the reality is that they 're better off getting it right in production provided it can be done in a reasonable amount of time . Particularly with the basics , like organizing footage , slating , choosing the right locations , getting the sound right , shooting charts ( my pet peeve ) , and doing camera tests ( my other pet peeve ) . <p> Just a note we did a 2 day shoot in Jan 4 x red dragons only @ @ @ @ @ @ @ @ @ @ grabbing quick shots we shot 5:1 6K @ 25 after 2 day of filming we had just over 6TB of data we end up using 3x OWC thunder bay units with 4tb drives set up for raid 5 so that about $3000 ( one plus 2 backups ) this is not including the proxy drives and audio so i think you have gone Cheap on that 14k Marc <p> Mind you i never put any thing on to single drives .... ever ...... apart from operating systems call me expensive but i like to be super super safe with footage and as these files are getting huge try and pulling back 4tb from a single drive over USB 3 ..... my life is to short .... And time is money these day too 
@@44332745 @4332745/ <p> Most of the training that 's available is definitely targeted at beginner and intermediate users , but I think the study of " advanced grading " is built around just what you are doing , and is largely what you make of it . It 's a lot like self-directed Ph.D-level seminars where you decide on the topics you want to tackle and then go after them . <p> So , including things that you 've already mentioned like cinema and photography , I would find classes on ACES , color science , compositing , roto , restoration , and even certain areas of visual effects like lighting , depth maps , tracking , and match moving . <p> I think going with iColorist and trainers like Warren Eagles is probably the wisest move . I know a few LA colorists who moved over to Resolve from a different system , and Warren got them up to speed very fast on some fairly advanced issues . If you hire them for one-to-one training , it 's not cheap but they can configure the class to specifically what you @ @ @ @ @ @ @ @ @ @ has an interesting class on Popular Looks and Colorist Strategies , and I would put both in the " advanced " category . <p> I think FXPHD and Ripple Training are both good for the basics , but when you go beyond that , you 're going to need to heavy duty stuff : looks , Powergrades , and all that . <p> I just started looking at Patrick Inhofer 's " Mixing Light " Insights library , and there 's some surprisingly good advanced tips there . A lot of them fall into the " stuff that 's always been there but I forgot about " category , but the deal for me is , if I can get three or four nuggets of wisdom out of it , the experience was worth it . Link is here : <p> One can make a good argument that it 's about as hard to learn color correction from tutorials as it is learning how to ride a bicycle from web videos , but these do at least illuminate certain obscure procedures and features inside Resolve , some of which might @ @ @ @ @ @ @ @ @ @ &gt; Seeing it done &gt; Reading about it . The topics discussed in mixing light are great because they cover a lot of " real world " issues that you wo n't face when in the classroom setting . Unfortunately there is no class that teaches " how to have an eye/vision of where you want the image to end up " . <p> Hands on &gt; Seeing it done &gt; Reading about it . The topics discussed in mixing light are great because they cover a lot of " real world " issues that you wo n't face when in the classroom setting . Unfortunately there is no class that teaches " how to have an eye/vision of where you want the image to end up " . 43971 @qwx453971 <p> I think that boils down to time . As an old pal of mine likes to say , " it takes years of experience to get years of experience . " <p> What about Alexis van Herkman 's latest ' Color Grading Handbook ' ? ( second edition ) If you buy the hard copy , you get @ @ @ @ @ @ @ @ @ @ the publisher 's website so you can work through the documented examples at your own pace , chapter by chapter . Absolutely essential reading for all the DSLR generation in my opinion but it may not be advanced enough for seasoned players . <p> van Herkman 's book ( and there 's another ' Looks ' version too ) is only around US$50 on Amazon whereas a local ( Sydney ) trainer charges nearly $500 for a one day ( Resolve ) class ! <p> Alexis ' book is very good , but it 's hard to beat getting direct one-on-one training from a super-experienced colorist on the order of Warren Eagles and the other ICA guys . The book will clarify overall theory and philosophy , but they wo n't necessarily get into the nitty gritty of " my client through this at me ... how can I come up with a workaround that will give them what they want ? " There are often four or five correct answers , and the trick is to find the one that works fastest and is most appropriate for the situation @ @ @ @ @ @ @ @ @ @ seem to recall taking a 2K class around 1999/2000 , and it was $1500 for a weekend with Kevin Shaw . And my employer at the time was too cheap to send all 20 staff colorists , so I was elected to go and basically had to return when I was finished and teach everybody else . <p> Find colorist ( not soft colorist that teach you more about tools , cause he do not do color that much ) who would like to teach you his own approach to the color . This is the fastest way to learn about craft , not about tools . Study old Masters , Impressionism , Fauvism , Expressionism and you will learn more than in any tutorials out there . good luck <p> Learn and innovate your own techniques . How do you think the most experienced colourists learned ? There was no ICA back in the day ... you learned on the job . The notion of colourist training is a new thing , just do the job and work out your own ways of doing things ... how else will @ @ @ @ @ @ @ @ @ @ ? <p> Actually in the old days colorists learned the craft by assisting senior colorists . ICA could n't even exist , as not many people in those days had a multi-million dollar telecine suite at their disposal for training <p> Actually in the old days colorists learned the craft by assisting senior colorists . ICA could n't even exist , as not many people in those days had a multi-million dollar telecine suite at their disposal for training 43971 @qwx453971 <p> I never got to assist . I 'm one of the handful of people who started as a colorist , back in the summer of ' 79 , when life ( and post ) was simple and easy . Technically , I was a tape op for one month , but it was time well spent . I was comparing notes with a co-worker of mine in the late 1990s and I think I helped train about 12 people in 25 years . Some of those are still around , some doing great . All you can do is give them a push on the bike and hope @ @ @ @ @ @ @ @ @ @ n't need them anymore . But they have to learn to pedal and steer by themselves . <p> Oh , no . I much prefer making my own mistakes , screw-ups , and disasters . No need to copy anybody else's. 43971 @qwx453971 <p> Let me clarify , when I said " copy " I meant using others ' work as inspiration . I mean things like " bleach bypass " and " cross processing " have an origin and I would be trying to " copy " the effect . <p> I guess I sort of look at it like hairstyle . I might feel like " I want my hair to look like Miley Cyrus . " So I go and chop away and maybe even learn how the stylist did her hair but since my hair is probably not the exact same texture and length my result wo n't be an exact " copy " . <p> I guess I sort of look at it like hairstyle . I might feel like " I want my hair to look like Miley Cyrus . " So I go @ @ @ @ @ @ @ @ @ @ did her hair but since my hair is probably not the exact same texture and length my result wo n't be an exact " copy " . 43971 @qwx453971 <p> Yeah , but can you sing and do you look good swinging on a wrecking ball naked ? 
@@44332746 @4332746/ <h> Ranges in Baselight STUDENT <p> I have posted about one aspect of this on another thread , but I thought it merited a separate thread , as information for those who have downloaded Baselight STUDENT , but are new to Baselight . <p> The Rec.1886 : 2.4 Gamma / Rec.709 colour space in Baselight has an alternate version , Rec.1886 : 2.4 Gamma Legal / Rec.709 , which is hidden by default in any colour space drop-down . Hold down the shift key to see the full list . I would suggest that there is an argument for considering using this as your Viewing Colour Space . I would not suggest you use it as a working colour space , even if you are using a " video " rather than " DI " approach " use the full range version of the colour space for that . <p> There are several reasons for this : <p> 1 . The h.264 encoder used by Baselight STUDENT appears to expect legal range input , so if you send it full range data the result will be incorrect . Note @ @ @ @ @ @ @ @ @ @ same colour space that you are viewing . You could just select the legal range vesion of the colour space in the Render panel , or apply a Full to Legal Scale under the Video LUT setting . <p> 2 . If you are monitoring via a T-Tap ( I do n't have other AJA hardware , so ca n't comment on that ) it expects legal range data , so sending it full range will be incorrect . Again you can alternately add a Full to Legal Scale , either using the Setup menu , or under SDI Output in the Display menu . <p> 3 . The Video Legal Overlay for both the Luma Waveform and RGB Parade appears to default to Legal . This will mean that the graticule will show the black and white levels in the wrong place for a full range colour space . I suggest setting Video Legal Overlay to Auto ( right click the waveform ) in which case it will correctly follow the range of your Viewing Colour Space . <p> However , if you do not have external SDI monitoring @ @ @ @ @ @ @ @ @ @ range image on your UI monitor . In this case your should use a full range viewing colour space , and set the Video Legal Overlay of your waveforms accordingly , and make sure the range is appropriately set for h.264 renders . <p> NOTE : I already had various versions of Baselight installed on my machine before installing Baselight STUDENT , so it is possible that some of what I am seeing is down to old preference settings carrying over . Check the behaviour of your own system for yourself . <p> I actually find the h264 extremely dark in the full BL , not to mention , inneficient . I usually use desktop software ( x264 encoder ) to encode mpgs from a prores for clients . I will try legal range to see if it makes it better . <p> I was under the impression that one should choose sRGB 2.2 gamma for mpgs tho ' . <p> The legal 1886 colour space is there for importing legal footage into Baselight . I would recommend to use always the Full 1886 space as the viewer colour space @ @ @ @ @ @ @ @ @ @ general works in full range on the display side of things . The legal scale should be done as very last step in the chain , if necessary : <p> If you have attached a SDI device you have the SDI Scaling Option to produce legal range on the SDI Signal ( Full to legal ) . The same way you can use the video LUT " Full to Legal " when rendering H264 , as H264 expect legal range . <p> I was under the impression that one should choose sRGB 2.2 gamma for mpgs tho ' . 43971 @qwx453971 <p> That 's a more complex subject that can be gone into properly in a forum reply . It depends what your MPEG will be played back on . In theory I would say you should still use BT.1886 for rendering to MPEG/h.264 , as it should be down to the colour management in the player to modify the image if the display is not BT.1886 . But not all players are colour managed . Indeed some people consider modification of the image by the player to be an @ @ @ @ @ @ @ @ @ @ . There is no one right answer . Personally I stick with BT.1886 . <p> I believe there is no warning in Baselight for H264 , like it is for Prores . But good point , we should add it . Thanks Nick . 43971 @qwx453971 <p> While we are at it ... I never really understood why that 's the case in Baselight ( prores encoder expecting full-range video while producing a legal range prores ) . Is there any way at all to produce a full-range prores in Baselight like you could do if you export one from davinci while setting video-levels to full ? Not that that 's a big problem .. just being curious . <p> Is there any way at all to produce a full-range prores in Baselight like you could do if you export one from davinci while setting video-levels to full ? 43971 @qwx453971 <p> Baselight used ( up to 4.3 ) to expect legal levels for its ProRes encoder . This changed with 4.4 to have the encoder do the scale for you . As I understand it , this was done @ @ @ @ @ @ @ @ @ @ After Effects , etc ) . If you send full range data to Resolve 's ProRes 422 encoder , you will produce a non-standard result . I would advise against that . ProRes 4444 is more complex . I have n't tested that 's behaviour in a while , as that varies with implementations . <p> Actually it makes it impossible for the user to do the wrong thing ( producing a full range prores ) . And as baselight 's settings/workflows usually assume a rather knowledgeable and mature userbase I do n't get it . I do n't expect a software like that doing some " hidden " level-scaling . And if the main reason for this is to discourage the user from doing the wrong thing I 'd argue that a warning ( like you get it if you try to render in legal range ) would 've been enough guidance . <p> Actually it makes it impossible for the user to do the wrong thing ( producing a full range prores ) . And as baselight 's settings/workflows usually assume a rather knowledgeable and mature userbase I @ @ @ @ @ @ @ @ @ @ software like that doing some " hidden " level-scaling . And if the main reason for this is to discourage the user from doing the wrong thing I 'd argue that a warning ( like you get it if you try to render in legal range ) would 've been enough guidance . 43971 @qwx453971 <p> In general I agree with you . I prefer professional software to give the knowledgable user the option to do whatever they want , at their own risk . Even if the default is to prevent you from doing " the wrong thing " I think it 's nice to have an " advanced " option to override the safeguards . <p> I seem to remember in the past I found a way to write ProRes in Baselight which included super-white image data , although with a quick test now I seem unable to do it . Of course with unclamped floating point processing , and Truelight Colour Spaces handling conversion , it becomes less necessary to expose legal range image data to the user . <p> Incidentally I did some brief testing in @ @ @ @ @ @ @ @ @ @ ProRes HQ in Resolve suffers from exactly the issues I would expect it to . If you are in a completely self contained environment , and know that the ProRes has been written this way , and are only reading it in applications which preserve super-white/sub-black data you may be just about ok . But it is risky . And any full range values which ca n't be represented in Y'CbCr will be clipped . Try writing fully saturated red , for example , to ProRes HQ in Resolve . You wo n't get back the same values . This is because an RGB triple of 1.0 , 0.0 , 0.0 will map to Y'CbCr values of 217 , 392 , 1035 which is outside the 10-bit integer range . <p> Resolve appears to handle ProRes4444 reading and writing the same way Baselight does , so full range data is preserved through a write/read cycle , because scaling is being done for you under the hood . Values &gt;1 are clipped on write and read . Although Baselight ca n't write super-white data to ProRes ( 422 or 444 ) @ @ @ @ @ @ @ @ @ @ Resolve will clip . <p> We implemented it that way because of compatibility reasons to camera manufactures , mainly . Apple specifies to encode only the legal range inside Prores . ARRI for example write their LogC data according to Apples standard to legal ranges . So the QT Alexa files are technically legal Rande LogC where the Arri raw files are not . So a user would need two colour spaces one for QT and one for Arri Raw , if we would not do this hidden scaling . We , like most others are following the standards . So when decoding we do a " legal to full " and when encoding we do a " full to legal " . I do not understand the reasoning behind this , as it wastes a bit of bit precision , but this is how ProRes is specified , I guess . I hope some of this makes sense . <p> We implemented it that way because of compatibility reasons to camera manufactures , mainly . Apple specifies to encode only the legal range inside Prores. 43971 @qwx453971 <p> Does @ @ @ @ @ @ @ @ @ @ ProRes . Certainly not all implementations do this , including Apple 's own ( FCP 7 , and if I remember correctly FCP X , include super-whites , if present when encoding ProRes ) . And Apple certified ProRes recorders , such as the Odyssey 7Q do not clamp either . Nor would I want them to . The SDI signal from a camera ( Sony , Panasonic and Canon , but not RED or ARRI ) may well include meaningful image data in the super-whites , and you would not want to clip that . <p> I understand entirely what you mean about ARRIRAW ( decoded as LogC ) matching LogC ProRes only if you include a legal to full scale in your ProRes decode . But equally , to use one commonly occurring example , an internally recorded S-Log3 MXF from a Sony FS7 will only match an externally recorded ProRes if you decode the ProRes without scaling . There is no one simple answer ! Baselight 's Sony : S-Log3 / S-Gamut3.Cine colour space will work correctly with FS7 MXF files . A modified version which includes @ @ @ @ @ @ @ @ @ @ S-Log3 files . <p> Does Apple really specify that you should clamp 64-940 when encoding ProRes. . 43971 @qwx453971 <p> Sorry for being unprecise . I mean scaling not clamping . We do a legal to full scale on input for ProRes Decode , and data outside of the legal range is just mapped outside of 01 in the input space . So no data is lost . But this is not true in general . There exist other software that clamps to legal range in ProRes decode. 
@@44332747 @4332747/ <h> Captain America : Civil War <p> Just got back from a screening tonight , and I really enjoyed the film , particularly from a color standpoint . It 's nice to see a gigantic summer tentpole picture that has reasonable color but still with a lot of style and impact . <p> I felt like parts of the film were cut too fast ; there were instances where I had a hard time figuring out who was fighting who in some scenes . There were so many chase scenes with Audi automobiles racing all over the place , for a moment I thought there was an Audi commercial within the movie . I caught some very obvious CG in spots , but there were other moments -- particularly the opening flashback sequence to 1991 -- that had some VFX shots that blew my mind . ( I wo n't reveal the giveaway of what happens in that sequence , but it 's in the first 10 minutes . ) <p> I think the overall look of the film was very good -- this is a far more natural @ @ @ @ @ @ @ @ @ @ Superman -- though I was conscious of quite a bit of post relighting here and there . Steve Scott from Technicolor has a major credit at the end as Senior Finishing Artist before the cast , which I think is a first for any feature film . And DP Trent Opaloch did a terrific job from what I 've seen , particularly the big Vienna airport sequence shot with Alexa 65 's . <p> The film is currently blowing up the box office around the world , and I bet will go past a billion bucks in another month . <p> Here 's some un-color-corrected footage , which will give an idea as to the extent of how massive this production was ... <p> I have n't seen the film but for me , Scott 's take on the modern blockbusters are always in good taste , without the heavy handed approach of others . 43971 @qwx453971 <p> I liked that as well . I thought it was generally a beautiful movie from start to finish -- stylized where it needed to be , bright and colorful when it needed @ @ @ @ @ @ @ @ @ @ Really , really good-looking film . 
@@44332749 @4332749/ <p> I think a lot of the looks were achieved with great art direction , great lighting , and great lenses , along with coordination with the costume and makeup department . So a lot of it may well have just looked like that on the set . Woody Allen is not a technical guy and does not go in for intense , over-the-top digital color . In fact , I believe all his features have been shot on film so far . <p> I have worked here on features where images needed only some 5% of adjustment . All other things were done in camera . With good pictures it is usually relatively simple to add some yellow cast or saturate red or what ever look you are after . Takes just some practice and ability to read scopes . Other than that it should be simple . <p> I have worked here on features where images needed only some 5% of adjustment . All other things were done in camera . 43971 @qwx453971 <p> It 's amazing how you realize " the better the cinematographer , the @ @ @ @ @ @ @ @ @ @ " I 've seen $1 million commercial campaign where a best-light dailies picture is literally good enough to broadcast . 80% of the important work is done in front of the lens : time of day , exposure , finding a great location , composing a perfect picture , blocking the actors to move into the best possible position , shooting during the right weather ... there 's a dozen factors involved . <p> Not sure who said it but yes it was something like that : " It is easy to make one shot look like Monet but try to do it in sequence " 43971 @qwx453971 <p> I 've usually heard it referred to Rembrandt -- as in " every frame a Rembrandt " -- but yeah , doing 150 shots in a row and making it look like a seamless scene takes a ton of work . Especially when it 's shot under difficult conditions . <p> The colourist of both films indeed is an artist , and I really like what he did . His name is Joseph Gawler , and you can check his IMDb @ @ @ @ @ @ @ @ @ @ a bit surprised that the Greek-video thread has as many replies as it does , but that this one does n't really have all that many . <p> Where does the problem lie ? <p> No one has any ideas as to what might have been done to this imagery , in Paris , for example , to get that beautiful , soft , yellowish glow ? Or how they made white sunlight red ( check the Villa dei Quintili scene on MovieStillsDB.com and here LONG ... or the scene at the Piazza del Popolo ) ? How did the Haussmannian buildings of Paris , with their greyish limestone facades , get this beautiful pale yellow colour ? <p> From looking through that group of stills you posted last , they seem to have shot the entire film at or around sunset which gives everything a pretty natural golden glow on its own . My issue with the look in the music video thread is that it did n't match what was in the frame ; everything 's artificially yellow rather than naturally . <p> Edit ; another thing I @ @ @ @ @ @ @ @ @ @ is the framing . Move the camera a yard to the right , angle it 15 degrees to the left and you 'd frame out the ugly cars in the background , which 'd do wonders for making the overall classicy sunsetty look work imo because you would n't have a bunch of unsightly creamy objects in your frame . <p> Lazily done but I think by taking them out it makes things look a lot better straight away . <p> Shot on 35mm by Darius Khondji , graded on the Lustre by Joe Gawler in 2011 . It looks to me like Joe primarily used printer points and saturation to swing the images yellow / red to emulate a photochemical grade . Great work , beautiful film . 
@@44332750 @4332750/ <h> Fixing shiny skintones <p> Working on a project right now where an actress has shiny skin for one of the takes . Set got a little bit rushed and makeup was overlooked for a couple of takes , but the shot has to be used . Unfortunately , the only option right now is to fix it in post , it would be very difficult to schedule a reshoot and it 's the only shot in the sequence that needs fixing . <p> Any tips on fixing this sort of problem ? I 've played around with the skin tones and blurred some of the highlights and it looks a lot better , but I 'm still not completely satisfied . <p> I attached a copy of the original image ( without any corrections ) in case anybody wants to take a look and offer some help on dealing with this issue in post . <p> Key and mask skin , lower the contrast a bit , another key from first one + luma key to reduce high lights with offset/gain . Than try lowering " midtone contrast @ @ @ @ @ @ @ @ @ @ to try Beauty box OFX plugin but makeup and softer fill light should be there in the first place . <p> There is no fix for terrible make up ... but you can always make it better . The only real fix is to get it right in the first place . This is kind of like what happens when the client says , " gee , I wish we had a backlight on that actor , " and I quip , " yes , that would 've made my job much easier . " <p> What I usually do in these cases is key out the highlights and then use the highlight and mid hue controls to try to add a color similar to the rest of the skin to the highlights . I then fine tune it using saturation , blur and LGG controls . In the end , it looks like the equivalent of digital makeup . <p> Last year I graded a three month political campaign where the candidate 's mother was shot with a vary shiny skin . These shots were used throughout the campaign @ @ @ @ @ @ @ @ @ @ was also shiny and she attended a few of the grading sessions with the candidate . I applied this trick and they were all very happy . <p> It does help if the footage was shot with a decent camera . In this case , F-55s . But I 've been doing this for years , with footage shot with all kinds of cameras , an even if it ca n't fix every single problem , it does improve things a lot . <p> Like everyone said , luma key and masks . I think one of the big things that makes it stand out is the fact that it 's very cool in comparison to the other side of her face - which has a more natural color . Though you may stray a bit from the cool fill , I think striking a balance between the two temperatures ( making the cool on her face warmer ) will make it less noticeable ... In addition to making it less contrasty and maybe a bit darker , if it does n't look weird . Not sitting in front of @ @ @ @ @ @ @ @ @ @ it off pretty easily in AE ( granted , 1 still is easy ) <p> Thanks a lot for the feedback everyone ! We 're working on the final grade right now and it looks like we 're going to be able to make it work . The good news is , it 's only shows up for one brief second in the sequence and the rest of the footage looks great . That and it was shot on the epic dragon , so there 's a lot of information to work with . <p> I 'm far less qualified ( pun ) than everyone else here but what I do when I have specular highlights that need calming down is play with the log highlight wheel like so , having dropping the high range . It 's hard to know how it 'd look in motion and the file 's heavily compressed etc but it 's usually a decent starting point . Here 's how they look side by side ; minor difference but it 's there . <p> When I see shots like that , it reenforces my @ @ @ @ @ @ @ @ @ @ day rate as a DPs is much higher than average for our market . There are things you can indeed do to help these people , but miracles will not be forthcoming . No matter what , these fixes will always look a tad digital and " artifacty " , or more than a tad . 43971 @qwx453971 <p> Naaaa , if we can just take the edge off this stuff , it does n't have to look " artifacty " at all . Knocking just the highlights alone down about 5 units , warming up just the highlights ( through a qualified key ) , misting and/or using NR for the highlights ... all this stuff can help out skin a little bit . It 's just a question of knowing how far to push it and when to stop . <p> Naaaa , if we can just take the edge off this stuff , it does n't have to look " artifacty " at all . Knocking just the highlights alone down about 5 units , warming up just the highlights ( through a qualified key ) , @ @ @ @ @ @ @ @ @ @ stuff can help out skin a little bit . It 's just a question of knowing how far to push it and when to stop . 43971 @qwx453971 <p> Of course , and that will look way better , but it will never look as good as something shot properly in the first place - sometimes somewhat less good , sometimes way less good . There will alwya sbe some subtle or not so sibtle point in time where the fix will give itself away , or just look weird in some way that 's hard to put one 's finger on . <p> Not to mention , shooting properly in the first place is almost always cheaper than spending hours and hours fixing issues post . Set time is generally more expensive than post suite time , but some set fixes take 10 seconds to save 30 post minutes , or more - or even take no extra time at all , just expertise going in . ( I 'm not saying this problem needs 30 minutes , I 'm just stating a general principle . ) <p> A @ @ @ @ @ @ @ @ @ @ element , but if you couple a great colorist with great footage , you get amazing results to a magical level not possible if one of those two elements is subpar . <p> Not to mention , shooting properly in the first place is almost always cheaper than spending hours and hours fixing issues post . Set time is generally more expensive than post suite time , but some set fixes take 10 seconds to save 30 post minutes , or more - or even take no extra time at all , just expertise going in. 43971 @qwx453971 <p> I can say some of the hardest days I 've ever had in post were issues when the actors had bad makeup on set , and the filmmakers had to rely on color-correction to try to solve those problems . That 's really tough . Great lighting and great makeup are impossible to fake . <p> It does make me appreciate the times the makeup and lighting are already there , and all I have to do is just enhance what 's right on the screen . I can sometimes make @ @ @ @ @ @ @ @ @ @ make bad pictures good . I can make them acceptable , but it 's not the same thing . <p> I can say some of the hardest days I 've ever had in post were issues when the actors had bad makeup on set , and the filmmakers had to rely on color-correction to try to solve those problems . That 's really tough . Great lighting and great makeup are impossible to fake . <p> It does make me appreciate the times the makeup and lighting are already there , and all I have to do is just enhance what 's right on the screen . I can sometimes make good pictures great , but it 's almost impossible to make bad pictures good . I can make them acceptable , but it 's not the same thing . 
@@44332751 @4332751/ <h> gamma shift workaround ? <p> I ve exported dnxhd shots from avid to nuke for vfx work , i ve also exported my dnxhd master to resolve for grading , i ve kept everything in gamma 2.2 in nuke but when i try to replace the vfx shots in my resolve timeline i m witnessing some gamma shift ( weirdly not in every shot ) is there a quick way in resolve to issue that gamma shift ( without the need of re rendering my vfx shots ) ? <p> It 's for reasons like this that I always prefer for the VFX artist or company to provide is with a ) at least 12 frames of handles head and tails , b ) 1 frame of a test pattern ( preferably one with color bars and gray scale that designed to work with their viewing LUT in Rec709 space ) , and c ) 1 frame of a slate telling what the shot is , what version it is , and what date it was made on ( and how many frames of handles are being provided @ @ @ @ @ @ @ @ @ @ you can massage it back into the edit without any problem . And the VFX department needs to be checking this stuff on their renders to make sure gamma shifts are n't happening . This kind of problem compounds exponentially when multiple people in multiple locations are all working on the same project , but none of them see each other 's work and all of them are working with different software and different operating systems . <p> I ve exported dnxhd shots from avid to nuke for vfx work , i ve also exported my dnxhd master to resolve for grading , i ve kept everything in gamma 2.2 in nuke but when i try to replace the vfx shots in my resolve timeline i m witnessing some gamma shift ( weirdly not in every shot ) is there a quick way in resolve to issue that gamma shift ( without the need of re rendering my vfx shots ) ? <p> regards J 43971 @qwx453971 <p> I would just grade the shot ' till it looks right . If it 's just a gamma shift and no information @ @ @ @ @ @ @ @ @ @ <p> I find DNxHD to be extremely prone to gamma and color matrix shifts , both in mxf and quicktime flavors . I would recommend either DPX / EXR image file sequences ( best ) or prores4444 ( acceptable ) . Although that may not be the case for you , it 's worth eliminating just to see where the problem is . I find in nuke the best way to avoid gamma shift is to export EXR linear light from Resolve , comp in Nuke , export EXR linear light from Nuke , apply linear to whatever colorspace you are working in once back in Resolve . <p> Are you 100% certain the read and write in Nuke are doing what they should be ? Like if you imported alexa log into nuke you should specify that it 's alexa log both in the read node and the write node . I know this is an obvious one but no harm in asking I suppose . If that 's all good and the problem is elsewhere it should n't be too much of a pain to export a different codec @ @ @ @ @ @ @ @ @ @ and see how that works possibly swap the write nodes for something like linear exr as mentioned above . <p> Yeah , i 'll go the exr route , i have many problems with dnxhd , sometimes it shifts , sometimes not , too inconsistent . I ve tried exr once but i think i ve messed up something in the export because when imported in nuke the exr looked whashed out and needed to be put in cineon colorspace to look right ... ( it should n't as exr are linear ) . <p> EXR does n't , inherently , have any gamma or colorspace , just whatever pixels you decide to put in it - the contents can be any gamma . To make your clip linear gamma ( 1.0 ) use the VFX IO lut appropriate to your current gamma space ( i.e. Gamma 2.2 to Linear ) . I do n't think there is any specific benefit to doing this in Resolve unless you are working from linear source ( like CG ) . You can convert in Nuke to the same effect , if you plan on compositing in linear . 
@@44332752 @4332752/ <h> Retime multiple clips + timecode question <p> I have shot ( quite a lot of ) clips at 50p , I wanted to transcode them all in a edit friendly codec ( mxf for avid ) . I have set my resolve project at 25fps , hoping that was going to interpret my clips , so , once transcoded would have been already at the correct frame rate with a nice slow-mo . <p> Unfortunately is not working as expected , in order to do that look like i have to slow'em down at 50% . <p> I ca n't find a way to select them all and slow them down , do it clip by clip is not an option . <p> Do you know if is it possible to do it , or if there is a way to get davinci interpret the clips at 25p ? Any other advice ? <p> Just another quick question . I have shot this clips with a sony a7s , unfortunately looks like the TC is not working , it 's a known issue . every clip start at @ @ @ @ @ @ @ @ @ @ was a way to get the resolve timeline TC burn to the clip transcoded . <p> You should be able to highlight all the clips in the Edit media window , then go into Clip Attributes and manually change the speed to 25fps . Be warned you 'll need to manually expand out each clip in the timeline , because it 'll now be twice as long as it was originally . <p> Trying to get timecode to work with non-timecode cameras is an exercise in futility . I swear to god , I 'm gon na make a T-shirt that says , " Stop Using Toy Cameras . " The only answer to me is to transcode to a non-H.264-based format like ProRes or DNxHD and manually add timecode , then consider the new clip to be the " master " from that moment forward . Once the new clip has real timecode , burning in visible TC is no problem . <p> I concede that there are editing programs that will let you use timecode from an Auxiliary track ( like channel 2 of the camera ) , @ @ @ @ @ @ @ @ @ @ will drift in terms of timecode phase over time . In other words , it 'll sorta/kinda work but it 's sloppy . <p> Burning TC was the wrong term , i did n't want to get visible TC , as you said i want to transcode to DnxHD and i was wondering if there was a way to " substitute " the 00.00.00.00 tc of the clip with the TC based on timeline . <p> I know about " not using toy camera " , I needed/wanted to try out that camera , and , as far as advertised it has " free run " and " rec run " TC , only problem , they get lost once imported , so it 's pretty useless . <p> Burning TC was the wrong term , i did n't want to get visible TC , as you said i want to transcode to DnxHD and i was wondering if there was a way to " substitute " the 00.00.00.00 tc of the clip with the TC based on timeline. 43971 @qwx453971 <p> The only way I can think of to @ @ @ @ @ @ @ @ @ @ edit timeline , then highlight Clip Attributes and manually type in the number . Bear in mind that if you have a non-standard speed ( like 50fps ) , timecode goes out the window since it 's no longer valid . Once you render the file and it becomes a standard speed ( 23.976 , 24.00 , 25.00 , 29.97 , or 30fps ) , then you can enter in timecode . <p> This is another reason why I generally ask clients to give me files with speed changes already baked in , because this way we wo n't have any surprises with segments being a frame or two off because of misinterpretations of timecode or different speed-change artifacts . 
@@44332753 @4332753/ <h> has anyone actually used thunderbolt on a windows box ? <p> I am planning my next build . Will possibly be based around the Haswell refresh w/Z97 chipset , or if I am feeling freaking crazy , the Haswell Extreme edition , rumored to be an 8-core monster , with the Z99 chipset ( and as of now unreleased DDR4 RAM ) <p> I am a PC builder obviously , and the motherboard I choose will almost surely come with thunderbolt on board . I usually go with Asus , as their overclocking is pretty simple . <p> I 'll be getting a bunch of storage as well . I currently have 2 of the CineRAID 4 bay " home " series in RAID5 which I connect via eSATA , and they actually work very well for me . Most of what I do is big 8 - 10 - sometimes even 12 and beyond multicam edits of live concert performances . <p> Thunderbolt on the PC still seems a little like the wild west . Would it be wise to base my fast redundant storage needs on @ @ @ @ @ @ @ @ @ @ Pegasus t-bolt 2 models ? Since I am currently able to edit 12 streams of 1080p by putting the footage across 2 of the cineRAID units connected via broke-assed old eSATA , surely a couple thunderbolt 2 models would far exceed what I currently have , correct ? <p> and yet , it seems very few people are actually doing this . which makes eric nervous . <p> would it be safer/better to go with an SAS card and some sort of external enclosure ? can someone post an example of this ? I know very little about it , except that I may not have an open PCI slot since I have 2 780ti 's . I ca n't seem to make sense of the stuff on Areca 's website , as there are a ton of options . <p> say I went with the ever popular Atto R680 + STARDOM SOHOTANK combo. would I see any difference in read performance vs. the Areca / Pegasus thunderbolt2 8 bay offerings , assuming all other things ( drives ) are equal ? as far as I can tell , the @ @ @ @ @ @ @ @ @ @ is equivalent to PCI 4x. advantage atto , obviously , but would that translate into any real world gains ? <p> How many 8 bay Sohotanks can I connect to one R680 ? this would be a pretty big advantage if its more than one , which I am assuming it is ... at which point I assume the 4x vs 8x would really start to matter .. <p> would I see any difference in read performance vs. the Areca / Pegasus thunderbolt2 8 bay offerings , assuming all other things ( drives ) are equal ? as far as I can tell , the Atto is a PCI 8x device , while thunderbolt 2 is equivalent to PCI 4x. advantage atto , obviously , but would that translate into any real world gains ? 43971 @qwx453971 <p> The Areca is very good , too , but traditionally their controllers have higher write speeds than read speeds , while its the exact opposite for the R680 . <p> How many 8 bay Sohotanks can I connect to one R680 ? this would be a pretty big advantage if its more @ @ @ @ @ @ @ @ @ @ at which point I assume the 4x vs 8x would really start to matter .. <p> when daisy chaining thunderbolt devices , is there a performance hit ? 43971 @qwx453971 <p> The SOHO tank is a basic SAS enclosure , that does n't include a SAS expander , which allows you to daisy-chain additional enclosures . It has two SAS ports , each one directly connected to four drives , so you 'll only be able to connect a single SOHO Tank to your R680 , unless you get something like this : LONG ... <p> Thunderbolt is very capable , but ultimately it 's designed for consumer applications . It will work in many professional situations , just like Firewire did , but at a certain point you can reach it 's limits . For now I think SAS is a safer bet , unless you 're interested in experimenting with Thunderbolt and are OK with idea that it 's still a bit of a work-in-progress on the PC . <p> two more minor questions - since each SAS port is connected to 4 drives , does that mean @ @ @ @ @ @ @ @ @ @ 4 drives each ? or can the tank be made into one large ( for example ) RAID5 or RAID10 volume , using all 8 drives in the same volume ? <p> any noticeable performance hit in daisy-chaining a couple SAS expanders together vs. having 2 cards and using 2 basic SAS enclosures ? <p> my one concern is that I end up trying to cram the Atto into a setup like I have on my home machine -- <p> this is the only slot left . it looks like it would fit , but BARELY . and it would be getting blasted with heat by the power supply from one direction and the GPU from another .. <p> as I am working with large amounts of taxpayer money here , I am most certainly not OK with experimenting , so I think SAS may be the way to go as you suggest . <p> however we do have a need for a couple smaller 4 bay raids to work with less capable machines , and I think I am going to go with a couple of the Areca thunderbolt @ @ @ @ @ @ @ @ @ @ perfectly well served by USB3 , and down the road when my X99/97 mobo gets built , I 'll move them over for some PC thunderbolt tests . which of course I 'll report back here <p> With most RAID controllers ( ATTO , LSI , Areca , Highpoint ) the physical drives are n't bound by the way in which you connect them . You 'll be able to combine the drives in your RAID admin utility however you like into one large volume or a few small ones . SAS expander enclosures will allow you to add more drives without any real noticeable hit in performance . The only real downside is cost , with 8-Bay SAS expander enclosures starting around $1200 . However , now that I see your photo , and being that you are on a PC , I would take a hard look at an LSI with two internal SAS ports . <p> It looks like you have several drive trays available , so you could forgo an enclosure altogether and add quite a bit of storage internally . LSI RAID controllers are on @ @ @ @ @ @ @ @ @ @ . They also come in a number of different configurations . An option that might work well for you is the 9260-8i ... <p> so I assume the SATA drives connect to the card via some sort of mini-sas to sata breakout cable ? <p> one thing I was always nervous about when considering internal RAID solutions is when a drive dies , how does one figure out which specific drive ? on external solutions , there is generally a light that turns red , and an interface that might show , for example , its the 3rd from the top . <p> I happen to just build my first raid yesterday . I went for an internal solution and now have 5 x 2tb configured in raid 5 with a Highpoint 2720sgl card . <p> Yes , you need a cable from minisas to sata . The sata plugs are actually labelled and numbered so without leds the management software will tell you which drive is corrupt or broken and by the number you can figure it out . <p> One thing that I could n't figure is that @ @ @ @ @ @ @ @ @ @ in one of the large slots , the 16x ones . Is that normal ? ( Gigabyte z77 up5 th ) 
@@44332754 @4332754/ <h> Favorite Films for Inspiring Color grade work <p> Two obvious ones come to mind , ' Oh Brother Where Art Thou ' . I may be wrong but I think that this was the first feature film where practically every frame had a colour grade applied . There 's a segment in the excellent Side by Side with DP Roger Deakins looking over the colourist 's shoulder . I think Resolve was used . <p> It 's an optical process , that is done while shooting . They have a device that gets bolted on the camera and sort of " pre exposes " the negative . You also can insert several filters for certain color effects . <p> It 's a method of contrast control that takes advantage of the natural physical properties of chemical-emulsion film stock to bring out detail in darker areas of the print . <p> The effect is produced by adding a small and even level of exposure to the entire image . Since exposure levels increase logarithmically , this tiny level of additional exposure has no practical effect on an image 's mid-tones @ @ @ @ @ @ @ @ @ @ the image into the film 's practical sensitivity range , thus allowing darker areas of the image to show visual detail rather than uniform black . <p> On-set flashing solutions include Panavision 's Panaflasher , which is mounted in between the camera body and the camera magazine throat , and Arri 's Varicon , which functions as an illuminated filter and can be viewed directly through the viewfinder for manual setting of the flash level . <p> Flashing is usually described as a percentage of exposure increase to the film 's base fog level . While the flash itself is often a neutral color temperature , the flash exposure could be any color : the color of the flash will be imbued disproportionately into the shadows of the image . <p> Cinematographer Vilmos Zsigmond used flashing very deliberately while filming " The Long Goodbye , " Robert Altman 's 1973 masterpiece . Zsigmond sought to create a sense of pastel light and subdued contrast appropriate to the film 's retrospective 1950 's L.A. mood . The MGM 2002 DVD re-release of " The Long Goodbye " includes a fine interview @ @ @ @ @ @ @ @ @ @ the film and his use of flashing to achieve them . In the March , 1973 American Cinematographer magazine ( the text is included on the DVD ) , Edward Lipnick discussed Zsigmond 's technique in detail . Lipnick credits Freddie Young with earlier use of flashing in cinema . The visual results in " The Long Goodbye " are wonderful . Zsigmond worked closely with Skip Nicholson , then Technicolor 's Manager of Photographic Services , to establish an acceptably predictable system to set the level of flashing to be used for a reel . For some scenes with deep shadow areas in which identifiable image detail was required , a flash " level " described by Zsigmond as " 100% " was employed -- though it is not clear that Zsigmond 's measurement system was that noted in the preceding paragraph . 43971 @qwx453971 <p> Most famous film , where they did this is ( besides seven ) is probably Saving Private Ryan . <p> Both Memoirs of a Geisha ( trailer only ) and The Master ( trailer and feature ) came out of my room ! @ @ @ @ @ @ @ @ @ @ good cinematographer .... 43971 @qwx453971 <p> Hats off to you Sir - you did an amazing job on both Walter . They are really inspiring and had a great influence on my work . The Master really blew me away . Never saw anything THAT close to the old ( 30es and 40es ) Kodachrome look than this . I always wanted to achieve this for my own projects . Would you mind do share some insights on your process ? <p> Hats off to you Sir - you did an amazing job on both Walter . They are really inspiring and had a great influence on my work . The Master really blew me away . Never saw anything THAT close to the old ( 30es and 40es ) Kodachrome look than this . I always wanted to achieve this for my own projects . Would you mind do share some insights on your process ? <p> Hats off to you Sir - you did an amazing job on both Walter . They are really inspiring and had a great influence on my work . The Master really blew @ @ @ @ @ @ @ @ @ @ old ( 30es and 40es ) Kodachrome look than this . I always wanted to achieve this for my own projects . Would you mind do share some insights on your process ? <p> Frank 43971 @qwx453971 <p> The process was facilitate by the fact that Fotokem is a full lab with 35mm and 65mm process . The director want the final product as close as possible to a old print so we started with the 65mm printed and cut neg like in the old days . The 35mm sequences where optical printed on 65mm as well . A Lab color times worked for few weeks to get the natural Lab/film process look for the movie in the traditional way . <p> In the mean time , We scanned the Original negative for the 35mm and the film internegative for the 65mm ( so we retain the " film print look " ) . we did a 35mm reduction print timed form the original final 65mm and in the DI we did a side by side color correction in 4K with film emulation LUTs in order to match the DCI @ @ @ @ @ @ @ @ @ @ intended . Some trim where put after the side by side match ( to taste ... ) <p> The whole Idea was to have a digital product that is as least manipulate as possible : no shapes/vignettes , no secondaries , no extra special sauce . All we did was to recreate the film look with the least intervention but to guarantee that the match in the 35mm/75mm/DCI was as close as possible ... ( the teh director change a bit his mind , but he can do it ... <p> To me , grading is intimately linked with shooting style . In that terms , I liked Harris Savides work so much , and first without knowing that he did both photography on David Ficher 's Zodiac and James Gray 's The Yards . <p> I rely that kind of " image school " ( like they are painting one ) to one of my fav ' american director : Nicholas Ray . To me he 's the " Prince of darkness " haha , I love dark , shadows and black of that period of US movies . @ @ @ @ @ @ @ @ @ @ brillant in a formal way and I learned that they shoot the series in 35mm . Very precise work in art direction between historical period and lighting of Louisiana is so well painted . <p> Otherwise , Michael Mann 's " Miami Vice " interested me a lot , as I found " Public Ennemies " ... hmm ... weird , it looks like an aquarium , with some " grainy " skin due to the sensor sensivity . In terms of color or light , " Miami Vice " reminds me some Honk Kong movies of Johnnie To but with style and flexibility of digital . <p> And " The Master " yes , was such a shock when I saw it in theater here . And that 's the reason of this necro-posting , I looked for comments on it ... and can not except better advice with someone who work on it ! ! 
@@44332756 @4332756/ <p> We used a Philips color analyzer and a Minolta , and also the Sony probe itself . Each typically yielded different numbers ( for various reasons ) , and our staff engineer in charge of calibration told me he just averaged all three to come up with a final result . My memory is that the numbers were n't that far off -- less than 1% -- but it does show there 's a degree of interpretation involved . I would bet if you used Calman and Light Illusion to calibrate the same monitor , the numbers would be slightly different as well . I would n't think that one is necessarily wrong , just a different interpretation of the numbers and the standard . <p> I worked for places that had their own in-house standard slightly different from Rec709 . Lucasfilm , for example , ran their monitors at 20fL , in the belief that this was a better compromise for D-Cinema and for home video . ( Not my decision -- this was what their engineering department decided . ) <p> The problem with trying to @ @ @ @ @ @ @ @ @ @ the lack of parts and new CRTs , which is a challenge . I know of a fairly major colorist who only replaced his BVM-32 in the last six months , solely because he did n't like the look of OLED . Eventually , he was finally kind of forced to do it because of the CRT replacement issue . It 's kind of like trying to find valves for a ' 57 Buick. 
@@44332757 @4332757/ <h> Alexa vs Red camera advice <p> Deciding which way to go camera wise and wanted to get some feedback from some working colorists here . <p> Currently have a Red Epic Dragon and I can either go the the new Helium or an Alexa Mini . Helium 8k resolution is insane to the point of being unworkable but there is a new color science promised that might be interesting . Alexa is Alexa and to be frank that 's the look that I 'm always trying to emulate . <p> What 's the general feeling here ; on a well lit and shot production is there really much difference between Red &amp; Alexa ? There are a ton of great looking Red movies and shows but obviously Alexa is the gold standard so is it hype or fact ? Is the highlight performance and skintone reproduction really much better on Alexa or can a good colorist easily get same results from r3d 's ? I 'm interested from the point of view of a non perfect shoot environment , is Alexa significantly more forgiving under less than ideal @ @ @ @ @ @ @ @ @ @ an issue ? i.e. is it sharp enough when compared to Weapon Dragon etc ? <p> We shot on Red One until a little over a year ago and then upgraded to a Red Dragon . We would 've loved to convince the boss that we needed an Alexa Mini , but sometimes you just have to look at it as a tool and see what makes more sense for you . For us , 6k , the ability to shoot higher framerates , price , the fact that we already had a lot of Red gear , and other factors led us to just upgrade to Red . We do smaller commercial jobs so it was hard to really push for an Alexa . If we did features I probably would have tried to push for an Alexa Mini for prores alone . We are a small shop , but I still hate when I 'm in color and I get some baked in shots at in redgamma4 instead of red log because it either got overlooked or I did n't communicate it well . That being said go @ @ @ @ @ @ @ @ @ @ a shoot or two so you can better judge . <p> We have a lot of features and adverts here now shot on Alexa on 3.2 k and in ACES workflow it just look really good out of the box . I bet you get the same results out of red also if someone does not select different " colors " like Zach describes . <p> If I was looking at buying Red camera , I 'd rather be looking at Dragon sensor . Helium does n't make sense , unless you 're looking at run and gun style shooting , but why would you then need 8k ? On the other hand , Red still does n't come even close to the " straight out of the camera " images , that you get with Alexa . Yes , with enough experience you can get great images out of Red camera , but that requires someone who knows the camera and someone in post who knows Red as well . Sadly , usually that 's not the case . Last weekends I ended up shooting with both Dragon Weapon @ @ @ @ @ @ @ @ @ @ cameras we ended up with very different images , each beautiful in it 's own right , but still very different . So , it 's up to the individual user 's taste , which one to choose . One final note . Red prides itself in always offering the upgrade path . As a result , many Red owners are feeling nervous , as almost every year they are confronted with the choice of spending $10k-20K on an upgrade or being left with an obsolete camera . On the other hand , ARRI does n't have any upgrade programs . Yet , ARRI users are happy to continue to use a camera , that does n't require periodic upgrades and over the years retains it 's value . So , you can buy a Toyota or you can buy a BMW ... <p> Anyone ever noticed moire on the Alexa sensor ? I shoot a lot of laptop screens and on the red an 85 definitely can be dicey . Would the moire on an Alexa Mini be worse or better ? Concerned that the lower sensor resolution will @ @ @ @ @ @ @ @ @ @ is not mandatory Like for netflix , i will get the Alexa over the red any time . i like both tonal range and color reproduction , less noise on the sensor and such . 43971 @qwx453971 <p> I generally agree with you , Walter , but Netflix does specify 4K capture for episodic TV and features made especially for Netflix . They will gladly buy a theatrical film shot on Alexa and then just run it in HD , but if they 're paying for the production , the delivery docs say they wo n't accept an upscale . ( Though they do n't explain what they do if 90% of the VFX sequences in a 4K film are 2K uprezzes . ) <p> Just did some tests today and blowing up some arriraw files looks amazing . I would say better than red dragon in terms of sharpness . Was super impressed . 43971 @qwx453971 <p> I do n't think this is exactly fair , and I think you 'd have to actually put the cameras side by side to get an honest test . You can make @ @ @ @ @ @ @ @ @ @ pleasing way , and they also have great color right out of the box , which are big advantages over some Red cameras . I think it 's also fair to say that Red has worked hard to narrow the gap with Dragon , Weapon , and Helium . <p> Anyone ever noticed moire on the Alexa sensor ? I shoot a lot of laptop screens and on the red an 85 definitely can be dicey . Would the moire on an Alexa Mini be worse or better ? Concerned that the lower sensor resolution will be more problematic ? Any thoughts ? 43971 @qwx453971 <p> I have been on set with an Alexa producing moire shooting a computer screen . <p> It 's just the nature of digital sensors , when you are shooting a small grid of things with a sensor that is itself a small grid of pixels that are then debayered to create an image . It is not specific to one camera , though if you are often shooting the same kind of screen with the same mm lens at the same distance , by @ @ @ @ @ @ @ @ @ @ your specific use depending on whether or not the grid of it 's pixels is lining up well or not-well with the grid you are shooting . <p> In the last two months , so far , I had done four jobs shot on Alexa mini with KOWA anamorphic lenses . The deal is , Alexa images are 2904x2160 in 4x3 sensor mode . With Cinemascope un-squeese ( 2:1 ) you end up with 5808x2160 ( 2:66 ) image . Yes , technically it 's still a 3k image even though for all intents and purposes it 's an almost a 6k image and yet , it 's considered to be not good enough for 4k delivery . Over the years anamorphic films , even though they were still shot on the same film were considered to be a superior images . So , why in this age of electronic cameras that had changed ? 
@@44332758 @4332758/ <p> Yep - I 've been burned by this as well . Prevented me from meeting a deadline , which was a pain seeing as I 'd done everything ' correctly ' , including finishing the grade early . <p> In my case it was the Denoise tools that had created the problem . Once I removed them I was able to render . Removing the Denoise broke a lot of my Keys though so the full workaround for me was : <p> 1 . ) Restart the Mac 2 . ) Denoise all the media &amp; render the result 3 . ) Restart the Mac again as the system was twitchy after the Denoise render 4 . ) Replace the original media on your timeline with the Denoised media + your grade without the Denoise tools <p> This was n't a ' Grading Software ' problem - this was a Mac problem . If I 'd used the Windows or Linux version of the software I would have been fine . Denoise is usually both a CPU and GPU operation ( same for things like Median Blurs ) @ @ @ @ @ @ @ @ @ @ A Z820 as 24 , and I believe a z840 has 36 ( I 'd need to double check it though ) . <p> I 'm never using Mac again for a pro job . I just ca n't trust it . Missing a deadline is brutal on your reputation . <p> What happens when you use node caching on your NR node ? Have your NR as your primary node . Turn on node caching . Color like normal on all nodes without caching beyond node 1 . Then when you export , use cached media . Just an idea . <p> Yep - I 've been burned by this as well . Prevented me from meeting a deadline , which was a pain seeing as I 'd done everything ' correctly ' , including finishing the grade early . <p> If I 'd used the Windows or Linux version of the software I would have been fine . Denoise is usually both a CPU and GPU operation ( same for things like Median Blurs ) . I think a Mac Trashcan has 6 cores . A Z820 as @ @ @ @ @ @ @ @ @ @ I 'd need to double check it though ) . <p> I 'm never using Mac again for a pro job . I just ca n't trust it . Missing a deadline is brutal on your reputation . 43971 @qwx453971 <p> Try Neat Video . Unlike Resolve 's own NR , in NV you can select to use CPU only . Yes , it will be very slow , but at least you 'll be able to render it out . Just a thought ... BTW , Z820 and Z840 can have a number of various core CPU configurations , but it would only depend on the CPUs used and not on the HP model . And , yes , you can easily ran out of GPU RAM on Windows and Linux as well , if your GPUs do n't have enough RAM , but at least there you can remedy it by replacing the GPUs with more RAM . <p> there is some command line stuff you can do to flush the cuda drivers but that may not help long term , that will just flush the memory but @ @ @ @ @ @ @ @ @ @ the amount of Vram you have <p> so you need to see if you can redice the over head ... some times this can work ... well it did for me a few times <p> if that titan black is the only card in the machine drop your display resolution down as low as you can bare 1920 x1080 as that 's using video ram <p> if your driving another display with the card switch that off <p> reboot and try and render then <p> the neat video trick will work but jake .... slow .... Ha ! Yes .... i had a issue with this before .... Neat did its bench test with cpu only 0.63fps <p> Anyway , sometimes you need a quick fix and even talking about this on here and going back and forth can be faster than a .7 fps average and potentially having to buy the plugin . I think the money is better spend retiring the old Titan . <p> Try Neat Video . Unlike Resolve 's own NR , in NV you can select to use CPU only . Yes , it will @ @ @ @ @ @ @ @ @ @ able to render it out . Just a thought ... BTW , Z820 and Z840 can have a number of various core CPU configurations , but it would only depend on the CPUs used and not on the HP model . And , yes , you can easily ran out of GPU RAM on Windows and Linux as well , if your GPUs do n't have enough RAM , but at least there you can remedy it by replacing the GPUs with more RAM . 43971 @qwx453971 <p> Oh - I love NEAT VIDEO . It is ridiculously awesome at what it does , and double ridiculous at the low price it is . <p> Unfortunately switching to CPU-only wo n't solve the issue when using a Mac ( in my experience only . This is OPINION not FACT ) . The average Mac Trashcan only has 6 cores . There 's no where near the CPU grunt that something like an HP z840 has to use . <p> Plus I 'm a filthy Freelancer so installing Plug-ins can be a pain-in-the-arse , or just plain not possible , a @ @ @ @ @ @ @ @ @ @ other things that you need to set up quickly sometimes ( so post shops really have everything all over the place - including important thinks like where you 're Caching to , as well as other settings that can slow the system down if non-optimized ) <p> I have a long history with NEAT VIDEO . Every frame of the last HOBBIT film went through NEAT VIDEO . A lot of grading systems incorporate NEAT VIDEO to a level well above what other plug-ins get . In some cases NEAT VIDEO feels more like a dedicated effect rather than external software . A bit like LIGHT SPACE in Resolve . <p> Although if you 're cunning &amp; you recognize that there are lots of different flavours of noise , and that not all of the image needs Denoising , then the Resolve Denoise tools can be very effective . It 's just the system slowing down that 's the problem . For everything else I 've actually gotten quite good results from the Resolve Denoise tools . <p> Like with anything as a Colourist : You just need to know @ @ @ @ @ @ @ @ @ @ a little creative with their application . <p> What happens when you use node caching on your NR node ? Have your NR as your primary node . Turn on node caching . Color like normal on all nodes without caching beyond node 1 . Then when you export , use cached media . Just an idea . 43971 @qwx453971 <p> Just be sure that the cache file can contain all the data you need to correct your shot . It 's easy to lock yourself out of a strong highlight recovery by using an insufficient cache file format . <p> i have a 980ti and it says GPU full with only one node with Neat Video . My timeline and footage is UHD . Any way to fix that ? 43971 @qwx453971 <p> A card with more RAM , I have had issues with 4k-5k material and I have a TiX w/ 12Gb . Looks like we need code that will break the frames up differently so they can be rendered . Had a 4k job last week with Resolve NR and Digital Anarchy Flicker Free OFX Plug-In ( @ @ @ @ @ @ @ @ @ @ because of " out of GPU memory " errors . <p> Interesting that after many years of " consumer " hardware being good enough , we may return to the days of expensive hardware specific to our needs , although almost no one is left to build it . All R&amp;D seems to be mobile device oriented . <p> Would be great if more cards " shared " the frame but as I understand it this is not the way it works . GPU RAM is not additive across cards is it ? 
@@44332759 @4332759/ <h> Adventures in Switching to PC for Resolve <p> A while back I posted about a PC system I was looking to build ( i6850K , Titan X Pascal , internal 16 TB RAID , ASUS X99-DELUXE II with ThunderboltEX 3 add-in card , M.2 SSDs for OS and render cache ) . Well , it 's built , and I used it to grade a 6K RED project in Resolve . <p> Thought I 'd post my impressions for any other lifelong Mac people looking / needing to take the plunge so they can grade faster ... <p> The tl ; dr is that working in Windows 10 is totally fine , and working in Resolve on Windows is 100% like working in Resolve on a Mac , especially if , like me , you remap the Ctrl key to the Command key and use an Apple keyboard . <p> But , man ... getting everything set up and working is an epic , tooth-pulling struggle . It 's worth it , I think , but if you 're considering building a PC , definitely allow yourself a @ @ @ @ @ @ @ @ @ @ client work . That 's 72-hours of troubleshooting starting once you get the machine physically built and booted up to the Windows desktop . <p> Here 's what happened when I brought my untested but functional PC to a client 's edit suite ( if that sounds foolish , well , it kinda was , BUT it was our only option to grade this RED 6K beast of a project that their trashcan Mac Pro was choking on ) : <p> Needed to install MacDrive to read client drives " no big deal , works great <p> After a few hours of troubleshooting , I panic and head back to B&amp;H to buy the new 4K Decklink Mini Monitor . I put it in the PCIe slot , download Desktop Video and ... no device detected . I look into the device manager , and there 's a little yellow exclamation point next to " Decklink . " It gives an error saying " The device driver is n't signed . " Blackmagic 's bad , I believe . <p> I call Blackmagic support ( which I did n't realize @ @ @ @ @ @ @ @ @ @ the helpful tech tells me to try a March version of Desktop Video . It does n't work ( because , yeah , that 4K Decklink card was not available back in March ) . <p> I go back to trying to get Thunderbolt to work ... I discover there 's a deep bios setting where the Thunderbolt was just set to " off " instead of " on . " Yikes . I flip the switch . Nothing . More bios deep diving , I realize you need to tell the bios exactly which PCIe slot the Thunderbolt add-in card is located in . So I read the motherboard manual , figure that out . <p> It works ! <p> I call the client in and we grade for a few minutes via the UltraStudio . Then it 's time for lunch ... <p> I get greedy and decide to install drivers for my Avid Artist panel while client is at lunch . It does n't work . Using the Euphonix software I see my panel , but even when I add it to " my panel " ( @ @ @ @ @ @ @ @ @ @ , Resolve refuses to " claim " it . The usual troubleshooting ensues , but it just is n't happening . <p> Even worse ... Windows no longer sees my Thunderbolt card . It goes offline for no apparent reason . Maybe I created a weird driver conflict by introducing the Euphonix software ? I doubt that 's the case , but Thunderbolt is n't happening any more despite further bios tweaks . And the panel is a lame duck . <p> I decide to give the Decklink 4K another shot . Resolve tech support tells me on the phone that there is no solution . Somehow I get the idea that there must be a way to make Windows ignore the unsigned driver issue . Indeed there is ... you can restart in recovery mode and start Windows in a mode that ignores device driver signatures . <p> It 's working again ! We have an image on the client monitor . <p> We grade for a few days with few issues . It stinks that I ca n't use the panel ; I have to use the mouse @ @ @ @ @ @ @ @ @ @ not terrible . I try to stick to printer lights and scroll wheel action . <p> Two big bugs rear their heads : right-clicking the node graph and selecting ( " clean up node graph " ) causes Resolve to hang and I need to restart the system . I 'm a messy node guy so periodically cleaning my node tree is pretty much muscle memory . But even once I figure out this is the problem , I still do it like 12 times . It 's ingrained behavior . Also , I discover that any clip that is part of a group grade FORGETS which group it 's meant to be in , every time I restart Resolve . Even if I saved and left nicely . Every time I restart resolve I need to take 15 minutes to scrub through the time line and reassign each scene to its proper group . Combined with the fact that I keep crashing Resolve cleaning up the node tree , this is hours of lost time ... Also client is freaking out saying " wait , is that the grade @ @ @ @ @ @ @ @ @ @ . Possibly this bug due to the fact that I 'm also using remote grading . Maybe this would happen on OS X Resolve as well . <p> Playback of the RED footage is good but not great . It is n't realtime at 6K at half debayer . I 'm bummed . It 's similar playback speed to the Mac Pro trashcan I was just dissing . My internal RAID ( 4x 6 TB consumer 7200 RPM drives in a RAID 5 configured by Intel Rapid Storage Technology ) is kinda slow . 150 MB/s write , 330 MB/s read . I 'm wondering if that 's the bottleneck . And that 's after I realize I need to enable a feature called " Write-back " which I guess caches some of the read/write . Before doing that it was hella slow . More tests to be done on that front ... <p> Source caching is great though ... I enable source cache for the whole timeline in color page , set the cache prefs to " automatic , " and select an M.2 SSD that I one day @ @ @ @ @ @ @ @ @ @ . Now things are flying . I finally get really excited when I realize I can put a decent amount of temporal NR ( like 2-3 frames at " faster " ) and spatial NR ( 10-20ish threshold at " faster " ) and get something close to real time playback . That 's a game changer . I did n't realize how much better the NR was in 12.5 . Now I do n't feel like I need to use Neat Video . Client is loving how all the dark footage is rescued and we 're moving fast . <p> We finish the job and render ... Premiere XML workflow with clip handles , rendered at source rez ( 6K ) . Render fails . " Does n't have permission to write to disk " error . A quick Google let 's me know that this could be because certain clips have special characters in the name ( like &amp;$@ ) . Turns out this is our problem . There were archival clips interspersed into the RED footage that the editor had used &amp; 's and $ 's in . @ @ @ @ @ @ @ @ @ @ OS X. Something to be constantly wary of because most freelance editors I know are cutting on old iMac 's and gleefully unaware of being careful with filenames . <p> Now the render hangs early in the piece on one particular piece of RED footage ... it did have NR plus an OpenFX glow . Kind of situation where the render time just creeps up and up and no progress . First it 's gon na take 3 hours , then 12 , etc .. Lots of cancelling and restarting . Finally we let it go overnight and it works ! Next morning says it only took 3 hours , which I 'm not sure about but whatever , it 's done , watched pot never boils ( except I forgot to remove the 2.35 output blanking , my bad , so we need to rerender again the next night ) . <p> We decide that , before I take my PC home , we should copy all the RAW footage to my internal RAID so I can do tweaks at home if the end client has additional notes . @ @ @ @ @ @ @ @ @ @ . Media Management to the rescue . But no , this fails three , four , five times , no reason . Just 2 hours of copying and suddenly " clip failed at xxxx.mov or xxx.R3D . " No special characters in there . Canon footage , RED footage , does n't matter . Just failure and no explanation . Trimmed clips or not trimmed . I give up on taking the footage home . <p> Job over , I bring my computer home , planning to connect my 2011 iMac via Thunderbolt and try to use it as a display with target display mode . Nope . Apparently 2009 and 2010 iMacs can work like this , because all they 're expecting is a MiniDisplay video signal . 2011-2014 iMacs need a Thunderbolt signal , specifically . Video and data . And even though I have a Thunderbolt3 port on my PC , and even though Windows sees a " Macintosh " connected via its Thunderbolt connection pane , pressing the command+F2 hotkey on my iMac , which is supposed to turn it into a display , does nothing @ @ @ @ @ @ @ @ @ @ hardware handshake that is n't happening . So I need to buy a whole new monitor ... <p> Otherwise Thunderbolt kinda works ? But it does n't with my 4 TB G-tech Thunderbolt RAID , though , says the disk is corrupt . Even MacDrive wants to " fix it , " which I 'm scared to do . I 'm trying to cut my new reel and all my files are on that drive . When I plug it into the iMac everything is OK though ... <p> So in conclusion , as I sit here wondering what monitor I 'm going to use for my in-home session tomorrow , or how I 'm going to access the files for my reel , I ca n't say whether building a PC for Resolve was worth it ... yet . But for sure it 's been an epic slog , with literally everything going wrong , every tiny step of the way . There is some fun and satisfaction in getting things to work , but the time investment is massive . Maybe I should have settled for a Trashcan @ @ @ @ @ @ @ @ @ @ , and just gone all in on the idea of source caching . <p> Well the thing is that the future looks more bright for windows users than trash can people ... if you see it from the bright side you are a step ahead of some . <p> I also have a PC Resolve system ... I usually tell the clients to get a copy of the project for " safe " and to format the disk/disks to exFat so I can work on pc and mac faster . ( There are a lots of pros and cons of exFat but i kinda feel there is the same kind of issues with macdrive and/or similar softwares ) <p> That iMac would make a great scope box machine sitting next to whatever monitor you do get . <p> I 've only ever had Windows Resolve , besides others machines or on my MacBook . It has got significantly easier to interact with Mac over time and I 'm sure will continue to improve . <p> The biggest issue on the Windows side of the game is drivers . Most Thunderbolt @ @ @ @ @ @ @ @ @ @ ports were so rare . I have n't had any issues with normal HDDs but anything RAID has an extra layer that could ( and has ) cause issues . <p> I did n't have much luck with Intel Rapid Storage and ended up using Windows Storage Spaces to make my SSD Cache a " RAID " 0 . <p> Unless the rumours about Apple and nVidia working together on a super trash can are true then going through the teething pains now should put you in good stead for the future . <p> When using Media Manager in Resovle , if a drive spins down because it is not being used for a bit , then Resolve tries to access it to copy a clip , it will cause Resolve to report " clip failed at xxxx.mov or xxx.R3D " , etc .... Resolve will not wait for the drive to spin up and errors out as if the drive were not mounted . The same behavior is exhibited when exporting while reading from multiple drives . If one spins down , then Resolve will export your clip with @ @ @ @ @ @ @ @ @ @ on a feature where there were 6 source drives . Resolve would copy the material from one drive for a while , causing all of the other drives to spin down due to them not being in use for a while . Then when Resolve would move onto the next clip and try to access a spun down drive , it would error out . <p> Solution : I installed an App that I found on the Apple App store call Amphetamine . Among other things , you can set it to ping your drives at a set interval to keep them from spinning down . Problem solved and it was free . There is probably an app for PC that performs the same function . <p> I feel your pain . Said that , a lot of it is self inflicted , starting with using untested system in a production settings . Building your own may be cheaper , but in the long run it is not necessarily cheaper . Full system warranty , especially in a production environment is a must , as attested by both of my @ @ @ @ @ @ @ @ @ @ an absolute champ , when it comes to a warranty repair . Anyway , for me having HP820 and SuperMicro helped to avoid many teething issues described . Personally I have no use for the Thunderbolt . If I need speed , I 'd rather use SAS or Fiber . To my clients I just tell to provide the disk with USB 2/3 or Firewire connection . I use Paragon on Windows , Mac and Linux . Paragon is a enterprise level solution and it never failed me . I connect my MC Color through the Hub and it is available to all three of my computers- Mac , Windows and Linux . Zero issues with connection and it took me all of five minutes to make it work on Windows . So , the good news , you 're already on the way ditching Mac , congrats ! My only other suggestion would be to get your system in a good working order before attempting to use it again in a production environment . <p> I built my own Resolve PC two years ago and I would do it @ @ @ @ @ @ @ @ @ @ to build and configure it but I wanted to learn how to do it . I wrote it up and had it published in RedSharkNews.com along with later GPU and 12G Decklink updates . <p> Yes , go to the Windows media management tool and tell the PC never to sleep and for the drives never to spin down . Ever . My ASUS z87 motherboard 's on-board 10Gbps TB port works well for external RAID SSD ( and even GUI monitoring as it is compatible with Mini Display Port ) but Jake 's advice is well considered . Unless you are a ' nerd ' with a lot of time on your hands , a ' turnkey ' system is a good idea - especially if you can find a business with experience in configuring high end ' video ' PC 's not just ' gaming ' PCs . <p> When it comes to internal RAID , I have just two WD Black 7200 RPM 2TB drives installed at the moment and the pair is currently half full but under Windows ' striped ' , ( similar to RAID @ @ @ @ @ @ @ @ @ @ just on 300MBs compared with 180MBs for a single 10,000 RPM drive . <p> Yes , go to the Windows media management tool and tell the PC never to sleep and for the drives never to spin down . Ever . 43971 @qwx453971 <p> Windows and OSX power management settings only keep the internal drives from spinning down . They do not keep external drives up . You need a shell script or app to keep external disks " awake " when not being used . <p> Wow you had a hard time there ! No luck . I 've been using a PC for over a year and I really love it . It takes time to get to know your system but once you know it , you get to be the master . I had many crashes on macpro , having difficulties rendering with amd graphics etc . I rarely have any crashes on my pc . It 's more of a live and learn system , if a macpro randomly crashes you will never know why . <p> haha .. I could have told u before @ @ @ @ @ @ @ @ @ @ before starting to work on it . I took me easily 4-5 days to make everything work , probably more . Good thing is , you really get to know your hardware . Its like a good fight in a relationship , in the longrun , it brings you closer together <p> I feel your pain . Said that , a lot of it is self inflicted , starting with using untested system in a production settings . Building your own may be cheaper , but in the long run it is not necessarily cheaper . Full system warranty , especially in a production environment is a must , as attested by both of my HP and SuperMicro needing a warranty repair . HP is an absolute champ , when it comes to a warranty repair . Anyway , for me having HP820 and SuperMicro helped to avoid many teething issues described . Personally I have no use for the Thunderbolt . If I need speed , I 'd rather use SAS or Fiber . To my clients I just tell to provide the disk with USB 2/3 or Firewire @ @ @ @ @ @ @ @ @ @ Linux . Paragon is a enterprise level solution and it never failed me . I connect my MC Color through the Hub and it is available to all three of my computers- Mac , Windows and Linux . Zero issues with connection and it took me all of five minutes to make it work on Windows . So , the good news , you 're already on the way ditching Mac , congrats ! My only other suggestion would be to get your system in a good working order before attempting to use it again in a production environment . 43971 @qwx453971 <p> +1 my setup is close to Jake 's ( 3 machines talking to one color/transport surface , HP machines , SAS for storage , ParagonHFS ) and the room is really trouble free , but i do think compareing a prebuilt properly spec 'd trashcan to a prebuilt properly spec 'd z840 is a more valid comparison , <p> The grief is about building a machine yourself and throwing it into production without testing and validateing the build throughly .... if you are really going to do @ @ @ @ @ @ @ @ @ @ and walk away happy <p> seems little to do with Windows , more about not being prepared and falling into a hole that was pretty obvious in the road ahead <p> as soon as everything is running the way it should , make a full system backup . If you install some new driver over software update and everything goes to s*** , try restoring to the last automatic restore point or just restore to your working full backup . <p> When using Media Manager in Resovle , if a drive spins down because it is not being used for a bit , then Resolve tries to access it to copy a clip , it will cause Resolve to report " clip failed at xxxx.mov or xxx.R3D " , etc .... Resolve will not wait for the drive to spin up and errors out as if the drive were not mounted . The same behavior is exhibited when exporting while reading from multiple drives . If one spins down , then Resolve will export your clip with the dreaded red frame . <p> This happened to me on a feature @ @ @ @ @ @ @ @ @ @ the material from one drive for a while , causing all of the other drives to spin down due to them not being in use for a while . Then when Resolve would move onto the next clip and try to access a spun down drive , it would error out . <p> Solution : I installed an App that I found on the Apple App store call Amphetamine . Among other things , you can set it to ping your drives at a set interval to keep them from spinning down . Problem solved and it was free . There is probably an app for PC that performs the same function . <p> as soon as everything is running the way it should , make a full system backup . If you install some new driver over software update and everything goes to s*** , try restoring to the last automatic restore point or just restore to your working full backup . <p> +1 my setup is close to Jake 's ( 3 machines talking to one color/transport surface , HP machines , SAS for storage , ParagonHFS ) @ @ @ @ @ @ @ @ @ @ do think compareing a prebuilt properly spec 'd trashcan to a prebuilt properly spec 'd z840 is a more valid comparison , <p> The grief is about building a machine yourself and throwing it into production without testing and validateing the build throughly .... if you are really going to do that , buy just a spec 'd z820 off fleabay and walk away happy <p> seems little to do with Windows , more about not being prepared and falling into a hole that was pretty obvious in the road ahead 43971 @qwx453971 <p> Ah , OK , cool . I had n't heard of Paragon . I will check it out . So far Mac Drive has been pretty OK aside from not being able to handle the G-Tech external Thunderbolt 4 TB RAID . <p> And apologies if this post came across as whining about using a PC vs. using a Mac . It was meant to be more of a log of my experience , a heads up to others making the transition from Mac and considering the DIY route . Like some have mentioned in the @ @ @ @ @ @ @ @ @ @ PC you 've built with an all-in-one-solution Trashcan . When you build a PC , of course you 're taking the responsibility of getting everything to work into your own hands . You are QA . <p> It would have been ideal to take more time with my setup before thrusting it into a client environment . But at the same time it was weirdly efficient to crash-and-burn in a live situation , because it really brought ALL of the issues screaming to the surface . If I 'd been testing my setup in the comfort of my home studio " as I first intended " maybe I would n't have encountered the issue of trying to render clips with special characters in the name , or I would n't have had the 12 TB of RED data spread across 5 USB drives and learned of the Media Management error . <p> In this particular situation , I 'd already spent 2 days grading on the client 's Trashcan . We were crashing roughly every hour for no apparent reason . Playback was dreadful , and attempting to even source @ @ @ @ @ @ @ @ @ @ error . The timing was just coincidental that my PC build was finished at that time , and it was sort of like , we can keep using the Trashcan and probably ca n't even do a final render ( nor can the director watch down sequences with the color ) , or we can take a flier on this new PC . The client was n't upset about all these technical issues that came up " it was a free solution I was offering them . Because the budget and deadline were n't changing . <p> Should I have gotten an HP or some other pre-built computer ? Maybe . But this machine I have now was built by my brother , who made quite a few DIY PC 's in his college days , and is now a medical doctor . I enjoyed designing the system with him and emailing the parts list back and forth , etc .. Hardware-wise the build is good , I think . Most of my issues have been at the software and bios level . I should also mention that it 's @ @ @ @ @ @ @ @ @ @ and I 'm looking forward to overclocking the CPU , GPU , and seeing what effect that has on my RED playback and Noise Reduction FPS . <p> Overall I 'm enjoying the nerdy DIY adventure and hopefully I can make a post soon that features less whining and more bragging . <p> Awesome . Do we think Blackmagic knows about the external spin down issue ? This is a great insight . If you do a web search for " Media Management Clip Failed at " you see a lot of people posting about the issue with no solution ... <p> When using Media Manager in Resovle , if a drive spins down because it is not being used for a bit , then Resolve tries to access it to copy a clip , it will cause Resolve to report " clip failed at xxxx.mov or xxx.R3D " , etc .... Resolve will not wait for the drive to spin up and errors out as if the drive were not mounted . The same behavior is exhibited when exporting while reading from multiple drives . If one spins down @ @ @ @ @ @ @ @ @ @ red frame . 43971 @qwx453971 <p> #Mindblown Great advice , thank you ! <p> I was wondering if this kind of software ( amphetamine , nosleephd ) helps to avoid those brief moments of lag when you change clips during grading ? 
@@44332760 @4332760/ <p> Even if you get that 17 " monitor calibrated perfectly to DCI standards , it will still be a vastly different experience when watching your content in the cinema on a big projection screen . Not just because of the OLED vs . DLP projection technology mismatch , but also because your grading choices regarding audience focus , image sharpness , use of shapes &amp; vignettes , etc. will be thrown off quite a bit , because the monitor is just so small . You should try to get a screen as big as possible or better a real projection setup . That would probably be way more important than 10bit and P3 if you 're on such a tight budget . You can totally master Rec709 for the cinema and have that properly converted to P3 without any loss . You might not even need the extra colors in the P3 gamut for your films . It 's not ideal at all of course , but I would n't want to grade a cinema release on a 17 " screen . There will be things you 'll @ @ @ @ @ @ @ @ @ @ My thinking about grading monitors is to invest in a FSI CM250 OLED and spend less on every other part of your setup if necessary . You get a decent sized factory calibrated completely accurate and reliable monitor with built in scopes which gives complete confidence in what you 're looking at . If it means ( as in my case ) that you ca n't justify the cost of a fast computer then you have to put up with caching everything ! <p> yes you are right need big screen , , , in that case barco projector or nec /jvc projector but than you need that much space and budget everything around it . What i am doing is regional film . and those are not Hollywood budget like ... <p> calibration is optional as in you can always call a professional to calibrate it rather than investing in it . after many nights of search on net and viewing videos on youtube and even when i see youtube videos or training videos of resolve most of them uses CM monitor as reference , , which makes it @ @ @ @ @ @ @ @ @ @ if not reliable and close to what you get to see in theater . I 've looked Lynda 's tutorial also and they recommend CM250 though .. <p> and i am anyways planing to check it in theater before final delivery .. just to be confident . <p> Actually I thought of getting CM Monitor as I 've graded on that monitor and it was very close in projection .. but when i calibrated my current setup and watched the output on projection it looked very dark and not what I was hoping for ... then i decided to invest in CM171 <p> Currently my setup is like <p> Mac Pro 6 Core with 32GB RAM <p> Dell Ultrasharp 24 inch for GUI <p> Asus 18 inch monitor for another GUI ( generally I use it for scopes from resolve ) <p> Even if you get that 17 " monitor calibrated perfectly to DCI standards , it will still be a vastly different experience when watching your content in the cinema on a big projection screen . Not just because of the OLED vs . DLP projection technology mismatch , @ @ @ @ @ @ @ @ @ @ image sharpness , use of shapes &amp; vignettes , etc. will be thrown off quite a bit , because the monitor is just so small . You should try to get a screen as big as possible or better a real projection setup . That would probably be way more important than 10bit and P3 if you 're on such a tight budget . You can totally master Rec709 for the cinema and have that properly converted to P3 without any loss . You might not even need the extra colors in the P3 gamut for your films . It 's not ideal at all of course , but I would n't want to grade a cinema release on a 17 " screen . There will be things you 'll miss and not see on that small screen . <p> Also , calibration is not " optional " 43971 @qwx453971 <p> I agree with everything you said , but I just wanted to point out for anyone reading this that the CM171 is not an OLED display . It 's LCD . <p> You probably know that Rec709 and DCI @ @ @ @ @ @ @ @ @ @ pixel format , but as a lot of folks have rightfully pointed out , you do n't really need P3 monitoring . But it does n't hurt either . If by ISFccc you mean in monitor calibration , then yes , I really think that 's important too . Just not that specific label/system . <p> If by optional you mean that you can hire someone to do it periodically , sure . But calibrating your monitor is absolutely not optional . Having one or more sensors yourself can be useful if for whatever reason you 're unconfident about your display . It 's also useful to have one yourself for your GUI monitors , to generate proper profiles . <p> My first choice is FSI CM171 over any computer monitor like HP / Eizo or like wise as it has more pro inputs 43971 @qwx453971 <p> Well that kind of depends on why you need those pro inputs . HDMI RGB has practically the same image information as SDI does . The only real downsides of HDMI that I can think of are cable length and the signal is @ @ @ @ @ @ @ @ @ @ can verify if it 's doing what you want with testing , which you should do either way . My choice was the CG247X which has a great gamut , 10 bit HDMI and DP and a pretty convincing P3 cinema ' feel ' in the dark , due to it 's thick black bezel and dimmable power/menu buttons . If you 're grading for theater you need to be able to imagine what you 're seeing on a big screen , which is a little easier if your screen is n't very little . <p> Actually I thought of getting CM Monitor as I 've graded on that monitor and it was very close in projection .. but when i calibrated my current setup and watched the output on projection it looked very dark and not what I was hoping for ... then i decided to invest in CM171 43971 @qwx453971 <p> I 'm not sure what you 're using as a reference display now , but keep in mind when calibrating that you should also tune display brightness to match your final screen . I think DCI P3 emulation calls for 48cd/m or nits . 
@@44332761 @4332761/ <p> Yes , good news ! Hope there will be a clear cut to the International Colorist Academy . In my opinion that 's how it should be . But when I read ' educational resource ' I ca n't help myself thinking about the ICA . <p> And let me say I do support Jim and Kevin with the CSI and have joined their organization . It 's about damned time colorists were represented in some way . I think it makes total sense ( even if the name is reminiscent of a certain TV series ) . <p> I du n no if I can bludgeon my clients to include " C.S.I. " after my name in the credits , but I 'll give it a go . At this point , I 'm lucky if they spell my name right on the check ! <p> This starts the process , and includes instructions on what types of references are required ( from either CSI members or clients , info about your company ( if you have one ) , etc . ) . CV alone is n't @ @ @ @ @ @ @ @ @ @ first step- I found out by doing the exact same thing you did ! <p> Interesting . I 've talk about this plenty of times over the years . Shame they went with CSI instead of ICS . I foresee many a chuckle at that name . 43971 @qwx453971 <p> They could 've gone with CIS ... although I wonder how many people remember Composite Image Systems , which is kind of a VFX company inside eFilm in Hollywood . <p> I think they are a little inundated at the moment , so you can expect a few days delay in processing all the orders and stuff . I 'm also hoping they can work out a deal with IMDB to allow us to use the membership as part of our screen credits . <p> This starts the process , and includes instructions on what types of references are required ( from either CSI members or clients , info about your company ( if you have one ) , etc . ) . CV alone is n't enough to get the ball rolling , and a necessary first step- I @ @ @ @ @ @ @ @ @ @ ! <p> - John 43971 @qwx453971 <p> that 's where i went a whle ago , but the site was not up and running at that point , thanks for reminding me ... <p> Now to register i have to have someone who already belongs to say i 'm a working colorist , unfortunatly i do n't know anyone who belongs , and there is not a accessable list of members i can check out to see if do by chance know anyone ... even with a few decades of IMDB credits and shipping 4 features , 28 TVseries episodes , countless national tv commericals so far this year alone , and its ' only mid May ... i still do n't count &lt;sigh&gt; <p> They say they want members ... and i have the cred 's , but cant get there from here .. makes me wonder if it 's a colorists society , or an old boyz club ? &lt;double sigh&gt; <p> Yeah , I have to agree . Having to put 2 sponsors into the application just to get your foot in the door is a bit @ @ @ @ @ @ @ @ @ @ the very old school and experienced DoPs I 've worked for as a DIT , I have very few in-roads to anyone in the industry that might already be a member . <p> i see no one from Vancouver there .... unfortunatly that list is not gon na help ... <p> Any current members feel like virtualy sponsoring a real working colorist ? PM me ! 43971 @qwx453971 <p> I would of happily sponsored you , but I just withdrew my application . Will be telling the other CO3 colorists and other decent colorist buddies not to bother . CSI branding in itself is a massive red flag . I 've been talking to Stefan Sonnenfeld for years about setting something up like this . Maybe it 's time .... <p> Hi Dermot Jim and I realise that it is difficult to find other members to sponsor even well established colorists , so there are lots of ways to apply . Full details at **37;939;TOOLONG and yes imdb credits help . CSI is definitely not an old boys club , the due diligence on members is so that when we @ @ @ @ @ @ @ @ @ @ colorists and not editors etc , they take us seriously and we have an audited paper trail that says we are actively maintaining standards . And to address some other points raised CSI has nothing to do with ICA . However , CSI has negotiated 10% discount for Full Members with ICA , Tao of Color and Mixing Light . There is a 5% discount for Associates . Part of furthering the art and science of color grading is education in the broadest sense . We looked at many names ( and there was a lengthy discussion on LinkedIn ) but in the end the name is not the most important thing . ICS did feel too close to ICA and might have been more confusing than we would like . No one has confused us with the TV program . <p> Tom , just asking this out of curiosity ... why would the abbreviation CSI be a red flag for branding ? From a USPTO TESS search there are 192 businesses that have a live national registration of CSI for their company branding . The difference in registration is @ @ @ @ @ @ @ @ @ @ - IC 035 Professional staffing and recruiting services vs . CSI - IC 009 Computer software platform for managing enterprise risk.http : //tmsearch.uspto.gov <p> Tom , just asking this out of curiosity ... why would the abbreviation CSI be a red flag for branding ? From a USPTO TESS search there are 192 businesses that have a live national registration of CSI for their company branding . The difference in registration is only the difference in the registered category like : CSI - IC 035 Professional staffing and recruiting services vs . CSI - IC 009 Computer software platform for managing enterprise risk.http : //tmsearch.uspto.gov 43971 @qwx453971 <p> Slap C.S.I after a colorists name on any credit and people are going to chuckle . I mentioned this whole thing to a very established feature DP I was working with yesterday . He cracked up laughing . So yes , it does affect branding . But , I do commend these guys for attempting to create such an organization . <p> Just out of curiosity , what would this organization offer to a member ? +1 on CSI being not a @ @ @ @ @ @ @ @ @ @ to laugh . 43971 @qwx453971 <p> I think creating an organization similar to the cinematography guilds makes much more sense . Having those letters after your name should be a thing of absolute pride and earned through years of experience , award winning work and a prestige catalogue of work . Members should be selected annually by panel of colorists and Cinematographers at the top of their game . Then you would have an organization that holds real weight . 
@@44332762 @4332762/ <p> I 've heard of HyperFS , but have not seen it installed in any shops around LA . It sounds interesting , although I have seen so many of these SAN replacements come and go , it would be nice to hear about any new options that are easy to manage and work well . <p> As for what 's used in post around LA .... <p> Xsan - Was huge during the **31;978;TOOLONG enterprise OS X era . Still an option and still good if you 're using mostly Macs , and even a few PCs ( with some limitations ) . Management around the time that 10.6 was at it 's peak was fantastic , easy , and very robust . Now there is barely even an app to manage anything as it seems Apple is determined to pare away everything , and just leave you with an Xsan ON-OFF button in a preference pane . <p> StorNext - What Xsan is based on , only with a much more comprehensive feature set , along with huge scalability . Most big shops and many medium-to-small shops @ @ @ @ @ @ @ @ @ @ . You also need a real budget though , as it does n't come cheap , and admin requirements for day-to-day management are pretty high . But , if you want to slam though a big feature or major episodic series , this is what you want . <p> SanMP - For people that want a fibre SAN , but with lower cost and easier management . Volume-level locking so it 's not perfect if you need really granular file access , or if you must have more than one user writing to a given volume at one time . It 's super-easy to deploy though , and is very scalable . If you do n't have huge bandwidth needs , 4GB fibre arrays and switches can be picked up really cheaply now , too , giving you a pro infrastructure for not a lot of money . SanMP is basically the same as FibreJet if you seen that around , too . <p> Dynamic Drive Pool - Somewhat newer company out of Amsterdam , that makes iSCSI based server storage nodes , that come with their own proprietary file @ @ @ @ @ @ @ @ @ @ 72TB chassis for around $40K , that you can expand out to about 360TB with additional chassis . They also have a very nice management front end that 's easy enough for a normal user to administrate . Good on Mac/ Win/ Linux as well . I 'm not a big fan of using networked storage as a SAN replacement , but I originally heard about them when number of Flame shops reported very good results using them . I have not used it myself yet , but I did spend an hour or two at their booth at NAB , and if I was starting a shop now I would be taking a hard look at this . <p> Wondering what filesystems people are using for shared storage . Taking a look at HyperFS over the next couple of weeks and I know StorNext is a popular solution but wondering what else is out there ? <p> Hardware is 3x Dell MD1200 ( 60TB ) SAS connected via an LSI SAS Switch into two Z800s with Dell H800 PERC controllers . Windows based environment but happy for Linux or @ @ @ @ @ @ @ @ @ @ <p> Everything can see everything as it stands . Just after a filesystem to have it working correctly . <p> Keen to hear horror stories or strong recommendations . 43971 @qwx453971 <p> Jack , The best system I know is from OpenDrives ( http : //www.opendrives.com ) . But that is much more than just a filesystem . They use a really customized version of ZFS . For a SAN infrastructure , I have used only StorNext . When setup correctly ( expensive as hell ) , it is insanely fast . For a hybrid NAS/SAN(ish) system I have and still do work with Facilis Terrablock and Smalltree . I have seen the terrablock well implemented and it does it 's job well . I have also seen it done really poorly . <p> I do n't think stornext is that expensive compared to other platforms , the software is free on Mac clients even . It is a lot more maintenance though . But performance is great , and on a well configed system uptime is outstanding . We 've had our running pretty much non-stop for 2 years . 
@@44332763 @4332763/ <h> Unconventional books for colorists <p> There 's some books that I 've recently added to my bookshelf that I think would be great for any colorist to own . The Color Correction Handbook , Color Correction for Digital Video , etc are DEFINITELY at the top of the list , but where do you go from there ? <p> This is a great handbook for how light works and how to use it to gain realistic results . It seems like it was originally made for illustrators and 3D texture artists , but it can really make you think about how light works in the real world . Often I am hyper focused on the image that I 'm staring at and not thinking about how the environment is effecting it . He goes to great lengths to explain the science behind why light works the way it does , but at the end says " Art is not physics " and shows when and how its ok to break the rules . <p> My mother in law is a retired art teacher and I asked her for @ @ @ @ @ @ @ @ @ @ a great quick reference book for the classics . It has inspiring reference images , as well as the brief history and techniques behind them . For those without a classic art background , this is a must have . <p> If we are doing our job as colorists , we are helping to make the story better . It often helps to get into the mindset of the editor . How are they attempting to build tension ? Why did they select this shot rather than the others ? Why are these scenes next to each other ? This is one of the reasons why we are different than retouchers or illustrators . We generally are working within a narrative , and I ca n't think of anyone who explains it better than Mr. Hollyn . <p> For a while I 've believed that there is a whole untapped world for colorists with blend modes . Some grading systems have a plethora of modes , while others have none or very few . This can be used as a cookbook for some general fixes or enhancements . Even though @ @ @ @ @ @ @ @ @ @ deconstructed and recreated , like Benoit 's awesome skin smoothing technique . Who knows , maybe we 'll see more adoption of blend modes at the next NAB and this book will become even more useful . <p> Bruce was one of the few film school professors I felt like was really able to translate the intangible nature of " visual design " into narrative storytelling . The book deconstructs visual elements of color , composition , depth , contrast , and numerous other " visual indicators " and demonstrates how filmmakers have used these elements of design to support and enhance their films . Part graphic design manual , part color theory primer , part image composition handbook , the book is a mix of things , but what I love about Bruce 's writing ( and teaching ) style , is that he 's very structured and systematic ( even using charts and graphs ) to track how these elements are combined in specific films to hammer home a specific visual storytelling technique or style . Worth a read for a colorist trying to enhance and craft the @ @ @ @ @ @ @ @ @ @ was one of the few film school professors I felt like was really able to translate the intangible nature of " visual design " into narrative storytelling . The book deconstructs visual elements of color , composition , depth , contrast , and numerous other " visual indicators " and demonstrates how filmmakers have used these elements of design to support and enhance their films . Part graphic design manual , part color theory primer , part image composition handbook , the book is a mix of things , but what I love about Bruce 's writing ( and teaching ) style , is that he 's very structured and systematic ( even using charts and graphs ) to track how these elements are combined in specific films to hammer home a specific visual storytelling technique or style . Worth a read for a colorist trying to enhance and craft the " visual arc " of a project . <p> I love the " Light for Visual Artists " book and just recently shared it with another colorist I work with . <p> The Bruce Block book sits by my @ @ @ @ @ @ @ @ @ @ the director ( who used to be Bruce 's teaching assistant ) kept referring to it and was very pleased that I spoke that " language " . When I took Bruce 's class many years ago , it was universally agreed that everyone wished they had taken it earlier in their film studies , rather than usually taking it in their last semester . So , please yes , get it and add it to your growing list . <p> My favorite technical book was Walter Murch 's In the Blink of an Eye , which is one of the best books on editing out there . The ASC has quite a few good books on lighting and cinematography in general , and I 've read quite a few of them . <p> Both the ASC Manual and 5 C 's of Cinematography are very useful in understanding lens problems , exposure , color temperature and so on . The new edition of the ASC Manual has much more on digital today than it did in previous versions . <p> This collection includes RED 's Rules of Engagement . @ @ @ @ @ @ @ @ @ @ Ops Guide , written by Phil Holland . Rules of Engagement is a collection of educational resources for both photographers and cinematographers alike . Spanning concepts central to digital imaging and motion capture " Rules of Engagement is a comprehensive assortment of learning tools and reference information for all levels of expertise . This custom-bound hardcover book represents the culmination of years of real-world application , white papers , online articles , and user feedback . Featuring over 220 pages of articles and shooting advice " and more than 200 contextual images and diagrams " the Rules of Engagement is the perfect addition to any camera enthusiast 's library . TheRED DSMC Field Ops Guide is a comprehensive tool for shooters to quickly reference while on set . 
@@44332764 @4332764/ <h> What colour science is happening in Adobe Premiere Pro CC ? <p> I 'm trying to find out what exactly is happening in the background of Premiere Pro CC with regard to colour . I have had a good look online and ca n't seem to find a clear answer to following : <p> - What colour space does Premier Pro CC operated in ? ( I 've read RGB , but is there more to it than that ? ) - What colour space is the image being displayed in when previewed ? or Is this something that is just controlled by my monitor ? - What bit rate is the video being displayed at 8bit , 10 bit , 32bit ? - What Gamma is Premiere Pro working in ? Is it similar to Final Cut ? <p> These may be very stupid questions to some , but coming from working in Resolve and After Effects where I need to choose the colour space , display profiles etc ... I 'm a little lost as to what is going on in Premiere Pro CC . <p> @ @ @ @ @ @ @ @ @ @ , but converts to RGB 4:4:4 ... a ) For GUI Display b ) For RGB-based effects ( then back to YUV again ) c ) For outputs if export settings or the target file format require it <p> CUDA effects are always processed in 32-bit float . Colorspace and Gamma are defined by your display . <p> Regarding color space and display profiles I think Premiere Pro CC of later breeds always expects Rec 709 . I think the " Mercury Transit Engine " defaults to Rec 709 always and I do n't think that can be changed . I also think anything not going through the " Mercury Transmit Engine " is completely RAW and is n't color managed at all . <p> On a side note when doing " realtime processing " with their Mercury engine you are NOT seeing full quality . I 've seen instances where the picture " jumps " a little after pressing stop and this is due to internal " better performance " vs " best quality " solutions . According to a contact I have with Adobe " in most cases @ @ @ @ @ @ @ @ @ @ , but it some you do . Especially when dealing with high resolution graphics or high contrast / colorful scenes . <p> After skimming through : Premiere processes everything in 4:2:2 YUV , but converts to RGB 4:4:4 ... a ) For GUI Display b ) For RGB-based effects ( then back to YUV again ) c ) For outputs if export settings or the target file format require it <p> CUDA effects are always processed in 32-bit float . Colorspace and Gamma are defined by your display . <p> Regarding color space and display profiles I think Premiere Pro CC of later breeds always expects Rec 709 . I think the " Mercury Transit Engine " defaults to Rec 709 always and I do n't think that can be changed . I also think anything not going through the " Mercury Transmit Engine " is completely RAW and is n't color managed at all . <p> On a side note when doing " realtime processing " with their Mercury engine you are NOT seeing full quality . I 've seen instances where the picture " jumps " a little @ @ @ @ @ @ @ @ @ @ better performance " vs " best quality " solutions . According to a contact I have with Adobe " in most cases you wo n't notice this " , which is true , but it some you do . Especially when dealing with high resolution graphics or high contrast / colorful scenes . <p> Bump for 2016 , with a question ... Does the newest current version of Premiere ( 9.1.0 in CC 2015 as I write ) support editing DCI P3 source footage , or does Premiere still assume rec709 ? If Premiere does now support a DCI P3 pipeline , does anyone have any experience with it and is it a clean pipeline or are there color shifts/degradation/etc ? Thanks , John <p> If Premiere does now support a DCI P3 pipeline , does anyone have any experience with it and is it a clean pipeline or are there color shifts/degradation/etc ? 43971 @qwx453971 <p> What purpose would you have for having to edit entirely in a P3 environment ? I think it would make more sense to use an external Rec709 -&gt; P3 LUT to get there @ @ @ @ @ @ @ @ @ @ with a compliant platform like Resolve or Nucoda or Mistika or Lustre or whatever . My opinion is that it does n't make sense to do the final conform in an NLE per se . <p> What purpose would you have for having to edit entirely in a P3 environment ? I think it would make more sense to use an external Rec709 -&gt; P3 LUT to get there , or at least do the final conform for DCI with a compliant platform like Resolve or Nucoda or Mistika or Lustre or whatever . My opinion is that it does n't make sense to do the final conform in an NLE per se. 43971 @qwx453971 <p> I completely agree . I did n't set up the post pipeline on this project and am just trying to help some colleagues stay out of trouble / keep their film looking good . The colorist is n't available for final conform and the editor does n't do Resolve . 
@@44332766 @4332766/ <h> XML sizing all over the place <p> I 'm working on a project with tons of stills and archival footage and the sizing is all over the place . They cut in a 1080p timeline and I 'm working in a 1080p timeline , why isnt the sizing translating ? Any work around ? <p> Conclusion : you can change the default scaling behavior of Resolve on the " scaling " tab with the projectsettings . However , if the have used both scale- and set-to-framesize on the same timeline , it will be difficult . <p> I changed the scaling to " center crop with no resizing " which fixes many shots but then throws tons of other off . ( that were correct before ) 43971 @qwx453971 <p> That 's a classic sign that both framesize options were used in Premiere . I 'm not sure , bud I think the only option is to go into Premiere and change one half of the shots to the other framesize mode . <p> Obviously , this is all speculation since I 'm only going by your post @ @ @ @ @ @ @ @ @ @ be save ) the premiere project and changing some shots you now are wrong . <p> Just an extra caution : If this is indeed the problem , it 's not just the scaling that is off . Any animations ( think Kenn burns ) will react differently than in the original . The only option ( I know ) will be to make sure all shots have the same framesize option and match it with the correct scaling setting in Resolve . <p> I had a similar issue a year ago on a feature . Resize data with keyframes was not coming across properly . After about 3 days of pulling my hair out trying to figure out what was going on . <p> I came to the conclusion that " Scale-to-framesize " was keeping all the keyframes , but with no data in them . " Set-to-framesize " would be correct as of the first frame , but would n't bring the rest of the keyframes across . <p> In the end , I bit the bullet and just did it all manually . Brought it over with " @ @ @ @ @ @ @ @ @ @ and matched the offline . <p> Source was 2K Arri ProRes . Resolve was set up correctly , and tried many combinations from prem and resolve frame scaling . <p> Will keep an eye on this too , as I 'm intreguied to see if its a simialr issue , and to see if I was n't the only one ! <p> I had a similar issue a year ago on a feature . Resize data with keyframes was not coming across properly . After about 3 days of pulling my hair out trying to figure out what was going on . <p> I came to the conclusion that " Scale-to-framesize " was keeping all the keyframes , but with no data in them . " Set-to-framesize " would be correct as of the first frame , but would n't bring the rest of the keyframes across . <p> In the end , I bit the bullet and just did it all manually . Brought it over with " Scale-to-framesize " set , and just went to each keyframe and matched the offline . <p> Source was 2K Arri ProRes . @ @ @ @ @ @ @ @ @ @ from prem and resolve frame scaling . <p> Will keep an eye on this too , as I 'm intreguied to see if its a simialr issue , and to see if I was n't the only one ! <p> As Remco said , <p> Good Luck ! 43971 @qwx453971 <p> I 've not had an easy time getting keyframes with stills moves to ease in or out . Sometimes the option to change the spline is not available and sometimes it is but it is not the easiest thing to navigate . I 'm sure there 's a bit of operator error there but it just seems a bit clunky and inconsistant . <p> As mentioned this is a common problem . From memory , differing ' anchor points ' between software programmes compound the problem . Tight management between editorial and grade regards those ' scale to/set to framesize ' options within Premiere is a must ..... and there 's no magic solution unfortunately . Fixing some sizing issues in Resolve is common . 
@@44332767 @4332767/ <h> Good desktop player <p> I 'm having a bit of a problem with playing video files in a stand alone player on the PC . VLC and Telestream Switch are OK to check the content of a file but they never seem to know how to interpret the video levels - everything is always lifted . I 've got my UI monitor roughly matching the colour reference monitor when comparing the viewer in Resolve and I know that my exports look good when dropped into a timeline in Resolve or Premiere Pro . Are there any other players or settings I might use ? ( I render to DNxHD mxf and often convert them to mp4 . ) <p> This is a never ending debate . It 's not the fault of your player , it 's a gamma mismatch between your export and your player . A lot of video playback assumes sRGB which is 2.2 . Most footage we edit in NLEs is normalized to Rec709 which is 2.4 . Resolve will default the levels based on the CoDec used , and whether it 's a @ @ @ @ @ @ @ @ @ @ , that 's just the container . <p> For playback outside of an NLE or other professional environment you should export H.264 , sRGB gamma , and auto levels . Many ways to accomplish this in Resolve . <p> VLC is pretty darn spot on for me . If you 're using an external monitor then that monitor is probably set to gamma 2.4 , your computer monitor is probably 2.2 , this will result in a small but noticeable gamma shift ( raised mids ) when you play the file on a computer . <p> If you want your broadcast monitor to be setup for grading web only content then set it to gamma 2.2 . Or as Jan says you can export your web deliverables with sRGB gamma ( 2.2 ) , Resolve 's colour management provides a workflow for this or simply add a timeline node which shifts the gamma down a bit and render you web deliverables separately . You ca n't use exactly the same gamma settings on export for web and broadcast delivery and expect consistency . <p> If you 're getting anything other @ @ @ @ @ @ @ @ @ @ play . <p> I 'm having a bit of a problem with playing video files in a stand alone player on the PC . VLC and Telestream Switch are OK to check the content of a file but they never seem to know how to interpret the video levels - everything is always lifted . 43971 @qwx453971 <p> If you are using Resolve with a monitor connected to the GPU ( as opposed to a genuine video card ) the output from the Resolve program window is 0-255 data levels , in order to get an ' accurate ' picture your monitor and your desktop needs to be set to data levels as well . <p> However when you use a video player , depending on how the video is rendered by the OS it could output with video or automatically expand to data levels . The best approach if you do not have a monitor connected to a video card is to always expand videos levels to data levels by setting the dynamic range to 0-255 using the GPU configuration screen of your GPU driver . <p> With respect @ @ @ @ @ @ @ @ @ @ do n't believe any of the popular video players automatically changes the gamma , but some allow you to change it on demand . <p> Here is a test to see if your expansion to data levels is setup correctly : <p> If you can read any text on the black and white blocks you do not properly expand to data levels . <p> Note that this test is applicable for monitors using the GPU , a monitor connected to a proper video card may ( or may not ) show BTB or WTW signals . <p> I 'm having a bit of a problem with playing video files in a stand alone player on the PC . VLC and Telestream Switch are OK to check the content of a file but they never seem to know how to interpret the video levels - everything is always lifted . I 've got my UI monitor roughly matching the colour reference monitor when comparing the viewer in Resolve and I know that my exports look good when dropped into a timeline in Resolve or Premiere Pro . Are there any other @ @ @ @ @ @ @ @ @ @ to DNxHD mxf and often convert them to mp4. ) 43971 @qwx453971 <p> I use 5KPlayer . Does n't mess with the signal , has Mac and Windows versions , plays a multitude of video and audio files and doas a lot more . And it 's free . <p> I 'm 95% sure it 's a levels thing not a gamma thing . I 'm using a SDI out to a FSI monitor ( which for my current job I 've set to 2.2 gamma as I know the primary destination is web ) . All exports and conversions look fine when brought into a Resolve or Premiere Pro ( they 're correctly interpreting it as video levels ) but lifted when opened in VLC or Telestream Switch . <p> I 've set the levels on my UI using the Nvidia Control panel to get a visual match looking at SMPTE bars and comparing them to the FSI . I 'm not worried how if they do n't match exactly , it 's only the UI , but they 're close enough while in PrmPRo or Resolve . Also @ @ @ @ @ @ @ @ @ @ close to my FSI too so that 's all good . <p> Interesting that the Video Level Checker in the link above works , I ca n't see any text , I kind of expected to see the sub 16/above 235 ... It 's all about how each player/NLE/browser is ( or is n't ) remapping the levels .... Perhaps I should look into not adjusting the Nvidia setting but try the built in Windows line up ( it 's much more limited though ) . <p> Take a look at Potplayer. https : //potplayer.daum.net/ I have been using VLC for a long time , but I have changed to potplayer and I never looked back . It is free , and you will have tons of settings to play with if you wish , I think it is worth a look . <p> All exports and conversions look fine when brought into a Resolve or Premiere Pro 43971 @qwx453971 <p> Just so we 're getting the full picture : Your monitor is set to 2.2 via SDI out and properly calibrated , which is a good baseline . <p> @ @ @ @ @ @ @ @ @ @ , timeline and output settings ? Assuming that the footage is Rec709 which technique are you using to convert to sRGB 2.2 ? You mentioned codec DNxHD . Does it change with different codecs ? <p> PS : If you frequently convert to mp4 ( which Resolve unfortunately does n't support ) , I 've found a very helpful hack of using ffmpeg to simply rewrap a mov container to a mp4 container without the need to re-render everything which saves a ton of time . <p> The other consideration when hunting for the perfect player , particularly if you work with a possibly less savvy web client , is that even if you find a great player , chances are they 'll just double click the and then yell at you ( figuratively ) . So whatever workflow you can sort out that is player independent is preferable . <p> Just so we 're getting the full picture : Your monitor is set to 2.2 via SDI out and properly calibrated , which is a good baseline . <p> How is your color mgmt setup ? Color mgmt mode @ @ @ @ @ @ @ @ @ @ is Rec709 which technique are you using to convert to sRGB 2.2 ? You mentioned codec DNxHD . Does it change with different codecs ? <p> PS : If you frequently convert to mp4 ( which Resolve unfortunately does n't support ) , I 've found a very helpful hack of using ffmpeg to simply rewrap a mov container to a mp4 container without the need to re-render everything which saves a ton of time . <p> ffmpeg -i &lt;source.mov&gt; -vcodec copy -acodec copy &lt;dest.mp4&gt; 43971 @qwx453971 <p> I 'm using default DaVinci YRGB , 2.2 gamma , Auto /Video levels , UltraStudio Mini **26;1011;TOOLONG , video levels , Rec709 , 2.2 gamma . ( You 've got me worried , I should double check all this now ! ) Viewers in Resolve and Premiere match my Flanders , viewers in VLC , TeleStream Switch , PotPlayer , 5KPlayer look lifted ( thanks for the tips on these , I quite like Potplayer. 5Kplayer seemed a bit hopeless as I could n't freeze frame pause ... none cured the problem though . ) I used Adobe Media Encoder to make @ @ @ @ @ @ @ @ @ @ web . It 's quite quick , good quality and the mp4 and original mxf match each other reasonably well . <p> I will re-visit all these settings just to be sure . My main question remains - is there a player which will interpret files as Video / data in the same way which Premiere and Resolve do and remap black accordingly ? <p> Good . What worries me in your list is that you have set the timeline to 709/2.2 . Rec709 as it comes from the camera in most cases either natively or after de-log LUT will be 2.4 , so your footage may be 2.4 but you are treating it as 2.2 . <p> My preferred setup these days is to setup project as DaVinci RCM which allows you to chose separate for input , timeline , and output . I set the input and timeline to 709/2.4 , use the camera LUT for slog to 709 transform . Then I set output to sRGB/2.2 for web projects and monitor to match . <p> That maintains integrity and forces Resolve to apply the proper transforms . @ @ @ @ @ @ @ @ @ @ . <p> What I 've found too often that the levels are a red herring . Tinkering with them seems to fix it , but it 's masking the root cause . Burnt by fire <p> My preferred setup these days is to setup project as DaVinci RCM which allows you to chose separate for input , timeline , and output . I set the input and timeline to 709/2.4 , use the camera LUT for slog to 709 transform . Then I set output to sRGB/2.2 for web projects and monitor to match . <p> That maintains integrity and forces Resolve to apply the proper transforms . It also makes it easy to switch to different outputs . 43971 @qwx453971 <p> I am curious as to why you would not want to set the input transform from slog to Rec709 first instead of setting input to Rec709 2.4 and then applying a LUT ? <p> I am curious as to why you would not want to set the input transform from slog to Rec709 first instead of setting input to Rec709 2.4 and then applying a LUT ? 43971 @ @ @ @ @ @ @ @ @ @ is mixed camera footage . I find more control applying camera LUTs on a clip level than project level . I do apply it as a clip attribute though , not through a node so it does n't create clutter. 
@@44332768 @4332768/ <h> Remote Grading/Streaming a Session <p> We 've got a client out of state with a gig coming up . I may fly up , but there is n't a system there to use ( so would have to bring , and ca n't slave to one that is n't there ) , so we 're looking into some elegant solutions for streaming the output of my broadcast monitor with as little lag as possible ... <p> We 've done this before with some kind of hacky methods with about 5-10 seconds of lag , but would love to improve on that .. Any ideas ? What are people using ? <p> I do n't really want to pay $30k for a Streambox .. is there a better alternative these days ? <p> We 've got a client out of state with a gig coming up . I may fly up , but there is n't a system there to use ( so would have to bring , and ca n't slave to one that is n't there ) , so we 're looking into some elegant solutions for @ @ @ @ @ @ @ @ @ @ lag as possible ... <p> We 've done this before with some kind of hacky methods with about 5-10 seconds of lag , but would love to improve on that .. Any ideas ? What are people using ? <p> I do n't really want to pay $30k for a Streambox .. is there a better alternative these days ? 43971 @qwx453971 <p> I use Resolve for remote grading . Obviously , there is no lag and there are no quality issues . Said that , you must have Resolve Studio on both ends , you need the same material on both stations ( that is much easier to do , than flying out instead ) , calibrated monitors on both ends and you need someone on the other end , who can assist you in setting Resolve up for a remote session . But that can be taken care of with something like TeamViewer . Scratch is very inexpensive , you can rent it for a session and it can stream your session , but the quality and lag , obviously , will depend on the connection , where @ @ @ @ @ @ @ @ @ @ . I had done a Resolve remote grading session with a mobile hot spot . <p> of in the other side there is compression coupled with an uncalibrated monitor , how you are going to solve it ? <p> non professional solutions will yield sub-optimal results . 43971 @qwx453971 <p> There 's access to a calibrated broadcast monitor on the other side ... Compression and lag are my real concerns .. but I could probably mail them a dongle and just have them set up a system I could slave to .. <p> You can set up skype to use the input of a decklink card as camera source . 43971 @qwx453971 <p> This is the first time I 'm hearing of this ! If what you say is true , this might not be that bad of a solution for a very inexpensive remote grading session ( provided the slave end has a calibrated display , and the client can live with a delayed &amp; compressed video live stream ) . <p> Streambox looks cool ; I 'll give that a shot . Tried using Skype w/Blackmagic input selected @ @ @ @ @ @ @ @ @ @ feed from my workstation , but just got black , even when Blackmagic 's Media Express could see the feed just fine . ( this is latest version of Skype on macOS 10.12 ) . <p> We 've got a client out of state with a gig coming up . I may fly up , but there is n't a system there to use ( so would have to bring , and ca n't slave to one that is n't there ) , so we 're looking into some elegant solutions for streaming the output of my broadcast monitor with as little lag as possible ... <p> We 've done this before with some kind of hacky methods with about 5-10 seconds of lag , but would love to improve on that .. Any ideas ? What are people using ? <p> I do n't really want to pay $30k for a Streambox .. is there a better alternative these days ? 
@@44332769 @4332769/ <h> RAW vs ProRes XQ post workflow <p> We 're about to shoot a short film and would like some input on the post workflow . So far , we 'll be filming with the Canon C300 and the BMCC 4K ( we may end up using only one of them ) ; as it 's a low budget project ( only the actors are really getting paid ) , we 're thinking of going either RAW or ProRes route . There wo n't be a lots of FX shots , mostly green screens and motion graphics stuffs . If we go RAW , we 're thinking about editing with ProRes XQ ( LogC , as it will allow for temp GFX to be final , depend on quality ) , for VFX ( green screens ) , we 'll most likely pull EXR ACES from the RAW camera source and will DI with RAW , EXR and ProRes XQ materials . If we go ProRes , we 'll edit with camera source and thinking about doing the FX with ProRes camera source ( or is it better to @ @ @ @ @ @ @ @ @ @ materials ( or EXR from FX ) . I 'd like to get your inputs about using ProRes through the entire pipe or going RAW ( with EXR from FX ) , what are the pros and cons. 
@@44332770 @4332770/ <h> Quicktime Player alternatives on Windows <p> If you 're looking for options in the wake of Apple dropping support for Quicktime on Windows , I 've been having a really good experience with MPC-BE , a light weight , highly configurable , open source player , with a nice interface , and support for a lot of different codecs . <p> This one is probably quite popular among VFX artists , DJV it plays back image sequences , also . mov many different formats ... It caches in RAM and it runs on win/linux/os x . I just think audio is not supported , but not 100% sure , I was never looking in to audio side of it.http : //djv.sourceforge.net <p> The later has a lot of interesting features such as video output support ( AJA only currently ) and that they are adding more or more QC-features . I think Telestream write all their I/O them selves , not relying on OS-API 's like QuickTime ( for better and worse ) . <p> I tried Switch first , but the free version watermarks some important codecs @ @ @ @ @ @ @ @ @ @ have to pay $295 to get it back . I love DJV , especially it 's cross-platform support , but without audio you still need a quick way to watch down exports or search for video files . So , I have image sequences assigned to DJV and compressed files assigned to MPC-BE . <p> Hmm , but MPC-BE is Windows only ? Part of the question here is : what can we tell clients to use ? Even taking our own opinions out of the equation , we all know clients that wo n't give up Apple unless you pry it from their cold dead hands ( as well as clients that would n't touch Apple products with a ten foot pole . ) <p> For clients I recommend VLC often . The only time that fails is for production formats or if the person using it has a wide-gamut display . If a client requests ProRes for example , 95% of the time they are Mac-based and there we have a solid player built in . If we 're dealing with image sequences ... Well those are a @ @ @ @ @ @ @ @ @ @ for review . <p> What would be great is a player that supports LUTS in a simple fashion . For example having say 5 QuickTime / ProRes files in the same folder as a LUT the player could default to that one or similar . Working with VFX / retouch work on say ALEXA footage can be very confusing when it 's either log-c or ungraded . <p> I also think it 's important to remember QuickTime as a player and API is dead / deprecated . However , QuickTime as a file-format is not . In OSX the migration to AV Foundation is more or less a complete circle . In Windows ... Problems are are arising . <p> I have been looking for decklink playout . Prelude does it , but often not with audio . I have found some paid options but no free one . I reckon someone could take the sdk and adapt vlc as a bmd player ... but then why not just use resolve ? 
@@44332771 @4332771/ <p> Anything for broadcast always goes through the legalizer . The legalizer should n't affect it a ton unless you are really pushing levels a lot . But in those situations you kind of now that what you 're seeing will be clipped . I toggle legalizer on/off when in doubt . You could also color correct through the legalizer , but I like being able to see the excess levels in the scope just so I know what else is there . <p> If you have a file based delivery , going through a legalizer may not be ideal with your workflow . There are a variety of filters that will provide the same level of QC as a legalizer , but the render can take some time . In FCP 7 , there is the broadcast safe filter to manage your luminance levels . In Avid , you have the Safe Color Limiter for luminance . Smoke 2013 Mac automatically legalizes the luminance levels . Saphire has a plugin to manage chrominance levels - Chroma Clamp . <p> There are a variety of filters that will provide @ @ @ @ @ @ @ @ @ @ <p> I do n't think I 'd agree . The software solutions I 've seen , have ranged from effective but somewhat less than elegant to utterly useless . I 've not seen a single soft solution that 's as effective as the HW solutions . And still use HW legalizers even for file deliverables , where required . <p> I agree with Juan , in a perfect world . However : I can say I 've been able to use a clipping curve as a Track correction in daVinci , plus a check pass just looking through scopes , and everything I 've done through Resolve has sailed through QC with no problem . ( OK , there was that time when the Symphony guy put the wrong audio track on the wrong trailer , but that was not my fault . ) <p> At least with Resolve v11 , there are a couple of new " broadcast safe " clipping functions that should provide this capability in final broadcast renders . The other way to go is just pay somebody to do a QC through a scope with @ @ @ @ @ @ @ @ @ @ actual printout of every illegal excursion . ( And I 've had plenty of illegal excursions in my day , lemme tell ya ... ) <p> I do n't think I 'd agree . The software solutions I 've seen , have ranged from effective but somewhat less than elegant to utterly useless . I 've not seen a single soft solution that 's as effective as the HW solutions . And still use HW legalizers even for file deliverables , where required . 43971 @qwx453971 <p> What 's the workflow for this Juan ? Lay off to tape through HW legalizer then create file deliverable from the tape ? <p> What 's the workflow for this Juan ? Lay off to tape through HW legalizer then create file deliverable from the tape ? 43971 @qwx453971 <p> When delivering Prores we play out live through hardware legalizer into FCP . It 's real time , actually a little quicker to setup than output to VTR . Unfortunately , you ca n't watch FCP capture like you can watch a VTR in confidence mode . You have to do a viewing @ @ @ @ @ @ @ @ @ @ When delivering Prores we play out live through hardware legalizer into FCP . It 's real time , actually a little quicker to setup than output to VTR . Unfortunately , you ca n't watch FCP capture like you can watch a VTR in confidence mode . You have to do a viewing QC of the file after it 's created . 43971 @qwx453971 <p> And laying off to tape and then capturing into a DNxHD or Prores is generally not the best idea because HDCAM and HDCAM SR have lower bandwidth than the aforementioned codecs . <p> Interesting . So what are you playing out live from ? And presumably you 're taking SDI out into a legalizer then SDI into your Final Cut suite ? I 'm used to taking SDI from Avid into Final Cut and vice versa but I 've never thought about doing so with a grading system . <p> And yes file based deliverables sound great in theory ... Still waiting for someone to tell me how to drop into a file ! <p> Interesting . So what are you playing out live from @ @ @ @ @ @ @ @ @ @ legalizer then SDI into your Final Cut suite ? 43971 @qwx453971 <p> Yes , taking SDI live through the legalizer into FCP . Playing back from any finishing system with SDI out . There are a couple of flash media recorders out there that will also capture to Prores or DNxHD instead of FCP . One thing though , since this is essentially a wild playback , the captured clip will have no accurate timecode so you have to place it on the FCP timeline in respect to the start time you need and save the file into a new Quicktime . This is not a new encode but a very fast rewrap . <p> I 've used a variety of software solutions for legal levels and I have always had clean QC reports . I 've used them for on air on commercials and broadcast tv documentaries . As far as quality , any differences were not seen by the naked eye . For us , it 's a great alternative to a legalizer when needed . <p> I 've used a variety of software solutions for legal levels @ @ @ @ @ @ @ @ @ @ @qwx453971 <p> My joke is , I 'm suspicious if anything I do sails right through QC and they find nothing wrong . I immediately say , " well then , you obviously did n't look carefully enough ! " <p> I know of major studio QC guys who 've whispered to me that they get yelled at if they turn in a QC report that says , " everything 's fine , no problems found . " So in some cases , they 'll make crap up to make us check it and say , " no , it 's supposed to look like that . " <p> Yes , taking SDI live through the legalizer into FCP . Playing back from any finishing system with SDI out . There are a couple of flash media recorders out there that will also capture to Prores or DNxHD instead of FCP . One thing though , since this is essentially a wild playback , the captured clip will have no accurate timecode so you have to place it on the FCP timeline in respect to the start time you need @ @ @ @ @ @ @ @ @ @ is not a new encode but a very fast rewrap . <p> Does anyone have any experience with Eyeheights software legalizers ? They are supposed to be good and based off of their hardware solutions . In the past we always used a HW legalizer for output to digibeta when doing transmission dubs . Since around 2008 or so , everything we deliver is file based and quite frankly this legalizing portion is a problem at times . As it stands we actually use Apple Color 's built in legalizer when we feel it 's needed . FCP 's is very poor from our experience especially in PAL-land . <p> I agree with Marc there . QC staff are paid to find fault , if they do n't regularly find them then there is no job for them to do ! I always supply my own QC to help answer any of their obvious queries . 43971 @qwx453971 <p> On some feature jobs , I supply a kind of " excuse list " with timecode numbers so they know all the weird stuff built into the film that looks like @ @ @ @ @ @ @ @ @ @ Kind of like " grainy B&amp;W flashback scene -- director 's creative intent . " <p> But I would never send out anything with illegal levels for broadcast . <p> At least they know not to flag stuff we already identified as deliberate issues . There were a few QC people at Fox and Miramax with whom I had a very good relationship , to the point where they 'd say , " uh , the transfer was basically OK , but could you check these three things ? " <p> Other people ( who I wo n't name ) would be unbelievable dictatorial and start going through the project frame-by-frame and providing a 10-page list of the most minute pixel-sized issues , 99% of which were imaginary or built in . Just crazy . <p> What always amused me is when the U.S. QC would pass the master , England would pass it , Australia would pass it , and then the guys in Germany would flag a legitimate problem that everybody missed . There 's always something ... <p> BTW , I 'll say that the best QC @ @ @ @ @ @ @ @ @ @ Roundabout Entertainment in Burbank . Tremendously sharp , observant people who catch everything and know the difference between a real problem and something ephemeral . 
@@44332772 @4332772/ <h> Tips for Starters <p> A couple of weeks ago I went out to the wild , scary land ; I started my freelance career as a colorist . I guess it 's a fantastic side of the industry if you get jobs and actually get paid for your work , but the truth is it 's very hard to get there and it will probably take some time . I 've now got a few jobs that makes me able to pay my bills , although it 's very tough sometimes . But my passion for color grading is so strong and I know that being a colorist is what I want to do for living so this is without doubt something I will put all my strength on . <p> My question is , do some of you experienced and successful colorists have any concrete tips for me that just started my freelance career ? Should I go out on internet and find agencies and show them my latest work ? Do you have any tips on agencies or companies that I should search for ? Anything @ @ @ @ @ @ @ @ @ @ get people to find me or send me grading projects ? How is a colorist aspect of this industry , how much should he/she hunt for jobs etc. ? <p> Do n't know if you could give me answers on those questions in particular , but are there any specific advices you would share with someone that is about to start his/her colorist career and want to do that for living ? The question is kind of abstract but all your thoughts and tips are more than welcome ! <p> I hope you get a few different colorists chiming in here , because there 's a whole lot of different experiences and circumstances and maybe some resemble where you want to go . I think the market that you 're in will affect the amount of work you have to chase down , and how much comes to you , and what modes of film/media you can work in . In larger cities with more post-production , you might find more prospective clients and more competition , so you do n't have to spend as much time creating business as @ @ @ @ @ @ @ @ @ @ attending to your clients . If you have a particular interest ( say , fashion ) you should find out where it 's happening . <p> If I can think of one thing that seems to matter a lot right now , it 's that there 's no longer any real value in simply having the gear , everybody can . Some of it they 're literally giving away . And buying the gear is like trying catch a falling knife . While you will likely need some base gear , do n't obsess over it . Spend some time and effort focusing on and improving whatever is going to give you a better eye , and finding a way that you can express it to clients when they need to be convinced . <p> In the era of the big post house , being an excellent colorist was your primary concern , but in today 's world especially if you are on your own , your skill as an artist is only one part of the equation . The other components are your marketing prowess and your business sense @ @ @ @ @ @ @ @ @ @ marketing to connect with and develop clients , skill as a business person to win projects and manage expenses , and skill as a colorist to complete the work to your client 's satisfaction . Weakness in any one area is enough to drag the other two down . You can partner with people who have strengths in areas you do n't , but your productivity as a group will need to grow in proportion to support the people you add . <p> A few other thoughts ... <p> 1 ) If you build it , they wo n't necessarily come . Do n't buy expensive gear thinking that it , by itself , will draw customers . At the very high-end and in specific niches it can help , but for the other 95% of cases it does n't pay off . <p> 2 ) A colorist who is good at color and great with people will be more successful than an excellent colorist who is n't . Especially when you are on your own , your networking and client-facing skills are just as important as your skills as @ @ @ @ @ @ @ @ @ @ . No matter how good you are there will be slow times , so do everything you can to minimize your fixed costs so that you can make it through down periods . <p> 4 ) What are you famous for ? A lot people like to say they offer " something for everyone " , but you are more likely to be successful if you pick a market and become known as the best at it . That notoriety will lead to new customers , especially since people like working with , and will pay a premium for , someone who is the preeminent artist in their field . <p> 5 ) Raise your prices . If we are going to succeed as a group , we need to not only offer value in what we do , but we need to charge for it . There is constant downward pressure on prices everywhere . If we do n't apply pressure in the other direction , who will ? Industries that attempt to compete in a race to the bottom are doomed . Take a look at the VFX @ @ @ @ @ @ @ @ @ @ amazing thing is , when you charge more , a number of interesting things occur ; You 'll find yourself doing better work because you 're being adequately paid , and you 'll be working with higher quality clients who 's primary motivation is n't to pinch you out of a dollar . You do need to offer quality in proportion to what you charge , but every opportunity you can , try raising your prices , and you 'll be amazed at how often a client has absolutely no problem with a rate you might have been initially scared to propose . <p> I had my training in large post houses , but did n't become a " colourist " ( ie doing final grades etc etc ) until I went freelance . My best advice is for your first few jobs try to find jobs with loads of time , and agree to a set price . This will enable you to have time to do a good job practise get better but ultimately not ruin your name before you get a chance to build your freelance credentials @ @ @ @ @ @ @ @ @ @ mouth can make you or break you . Best of luck try posting your work on here and get an honest critique , sometimes unavailable from your clients . <p> 5 ) Raise your prices . If we are going to succeed as a group , we need to not only offer value in what we do , but we need to charge for it . There is constant downward pressure on prices everywhere . If we do n't apply pressure in the other direction , who will ? Industries that attempt to compete in a race to the bottom are doomed . Take a look at the VFX business if you 'd like an example . <p> The amazing thing is , when you charge more , a number of interesting things occur ; You 'll find yourself doing better work because you 're being adequately paid , and you 'll be working with higher quality clients who 's primary motivation is n't to pinch you out of a dollar . You do need to offer quality in proportion to what you charge , but every opportunity you can @ @ @ @ @ @ @ @ @ @ amazed at how often a client has absolutely no problem with a rate you might have been initially scared to propose . 43971 @qwx453971 <p> Completely agree here are two golden nuggets for freelancing work for free , or full price . But never cheap . <p> My favorite advice here in this thread ! This really worked well for me a few years ago . You get faster every job , but in the beginning you 'll be like a turtle which makes it hard to charge by time , so instead charge by finished project . Clients also like it , in my experience , it gives them no surprises . <p> I 'd like to add another tip : Sometimes you have to find a way to get a potential clients ' attention . There 's nothing more tempting for a client then to see the actual result of a beautiful grade . Once they see their footage graded , there 's no turning back . You can use this in your advance . Grade some shots for free , just for demo purposes . I 'm @ @ @ @ @ @ @ @ @ @ also an editor ) . I take some shots from the job I 'm on and bring ' m into Resolve . Spent some time on ' m and sent them to the producer . I think 7 times out of 10 the result is a booking . Images speak louder then words . <p> There 's nothing more tempting for a client then to see the actual result of a beautiful grade . Once they see their footage graded , there 's no turning back . 43971 @qwx453971 <p> That 's a great tip , Pepijn . I do the same thing with great results . Even though I 'm not editing much theses days , whenever I am about to have meetings with prospective clients I ask them to bring some footage so that I can evaluate what the project . Because I load the footage in the machine to watch it on the reference monitor , I usually load a couple of shots in Resolve and do a quick grade so that they can have a taste of what it will look like . Like you said @ @ @ @ @ @ @ @ @ @ if it just a quick primary , they are hooked . <p> I have a few clients that are always producing new content . They usually prepare teasers to obtain financing and distribution . I grade their teasers for free . It only takes a few hours . But it becomes practically a guarantee that I 'll end up doing the work when the series/film is approved/sold . This early type of partnership pays off big time . 
@@44332773 @4332773/ <h> Colorist titles <p> I might be a bit dumb , but I still do n't really know the difference between a vfx colorist , a DI colorist and a colorist . And then there are dailies graders and on-set grading . What 's the difference , how different do they work , what 's their workflow ? Or are they just different titles for the same thing ? <p> I can talk about DI Colorist and Colorist because I have already been both of them . The DI Colorist is called this way because he 's the guy that is going to color grade what is called Digital Intermediate , a scanned version of the motion picture film , that is going to pass through the process of Color Grading , VFX , Finishing and then output back to film . <p> Although it originally used to describe a process of working with film . Nowadays it is also used to described the Color Grading of feature films , even when it was capture with a digital cinema camera and even when it is not output back to @ @ @ @ @ @ @ @ @ @ working on feature films and using projectors stead of broadcast monitors . <p> On the other hand , the Colorist is the guy working on Broadcast content . This guy , also grades contents for web , blu-ray , DVD 's , commercials , video clips , and even short movies intended to be screened on a cinema room . Of course , this guy will be using broadcast monitors generally . <p> I 'll let the other Colorist titles to be explained by someone better than me . Maybe something I said is wrong , so I welcome others to correct me as well . <p> Just to add that on feature films with complex vfx pipelines there are frequently colorist who work with/ are vfx artist , who do grading to match composites and elements for composites to the hero grade from the DI colorists , these are the vfx colorists . <p> Just to add that on feature films with complex vfx pipelines there are frequently colorist who work with/ are vfx artist , who do grading to match composites and elements for composites to the hero @ @ @ @ @ @ @ @ @ @ colorists. 43971 @qwx453971 <p> So do you mean that the vfx colorist match the cg stuff and get the whole shot to match to the filmed plate , before it 's off to the DI colorist , or colorist to be given a final look ? <p> For example , I had a filmed plate and a cg character was put in the shot . I had to grade the cg character to match with the overall look of the plate . Then the whole finished commercial was graded somewhere else . Another example ; The plate was graded , and then I matched the cg to fit in the graded plate . In one example the plate is graded after the cg is put in and in the other it 's graded before the cg . Is that what you mean ? Is that the vfx colorist ? sort of .... <p> Match before or sometimes after or even both . Depends on the workflow . But the vfx colorist does n't generally make an overall look , but does sometimes do entire shots to match an overall look @ @ @ @ @ @ @ @ @ @ the camera rushes and does a technical grade / balance pass for the compositors to work with via LUTs they then render the final vfx shots with no grade applied back out for the DI colorist to grade . 
@@44332774 @4332774/ <h> After Months Trying I Think I Have It Figured Out ! <p> After months of trying to find an easy and fast way to match cameras I think I have finally found it , and it does not add any artifacts . I have more testing to do in lots of different lighting situations but I will report back after I have it nailed down . Right now I am just happy to have a quick and easy way to do this ! <p> After months of trying to find an easy and fast way to match cameras I think I have finally found it , and it does not add any artifacts . I have more testing to do in lots of different lighting situations but I will report back after I have it nailed down . Right now I am just happy to have a quick and easy way to do this ! <p> I suppose it would be cool if a version is made to process a larger chart . With regards to matching cameras ( such as the ones above ) , get the @ @ @ @ @ @ @ @ @ @ scale until the luminance patches match . Then add the same gamma ( e.g. 2.4 ) to both and use mmcolortarget to generate a 3x3 matrix , which is what every true colourspace transform requires . You can then apply the 3x3 matrix in Resolve via a number of methods ( DCTL , OFX Plugin , DRX , etc. ) , and bypass the known 3D LUT related issues such as interpolation error . 
@@44332775 @4332775/ <h> Help . Davinci apparently deleted my user and projects . <p> Hi , I was finishing a grade and after a computer restart my projects did not appear there . It shows ' create new user ' , and my projects are missing . BUT , after another computer restart , Davinci wo n't even open . It shows the first white box whith the logo and so on , then dissapears and the project screen does n't show up . Windows program admin shows it 's there , but it does n't even shows . <p> I do n't know what to do . I had a project going on and maybe I 've lost it . Is there a way to save the database or the project info before uninstalling , if I 'm not able to load the software ? What do you recommend ? Thank you <p> First , take a second before freaking out . You need to provide some info , if you would like people to help you . Start with the list of your OS and Resolve version . Then @ @ @ @ @ @ @ @ @ @ Disk or PostgreSQL ? Then may be see , if any other apps or files show similar behavior , like not showing the contents of the folder or other apps refusing to start as well . It looks like you may have a drive damage , which may need a repair , but without any information it is impossible to tell ... <p> Thanks Jake . I have Resolve 12.5.2 running on Windows 10 64bits . PostgreSQL . I finally found the project folder and checked that I could open it on my laptop and all my nodes and corrections were there , so I just want to be able to open Resolve again . But after uninstalling it it still does n't show up . Logo screen , fades out , and it stays there , nothing happens . I suppose I now just want to completely uninstallit it and then install it again and manually copy the project folder . <p> Before going any further , try to back up your Resolve Database so you have a copy in case anything happens . Many of the command line @ @ @ @ @ @ @ @ @ @ a good cross-platform app called PostGres Admin that you can use to view , backup , and restore your databases . <p> Take a look starting at Page 4 and you 'll see menu examples for Back-up and Restore . Once you know how to use those options ( which is largely what the OS X tutorials cover ) , you 'll have more freedom to troubleshoot since you 'll no longer need to worry about losing your data . <p> Thanks Jake . I have Resolve 12.5.2 running on Windows 10 64bits . PostgreSQL . I finally found the project folder and checked that I could open it on my laptop and all my nodes and corrections were there , so I just want to be able to open Resolve again . But after uninstalling it it still does n't show up . Logo screen , fades out , and it stays there , nothing happens . I suppose I now just want to completely uninstallit it and then install it again and manually copy the project folder . 43971 @qwx453971 <p> As Jason suggested , before doing anything @ @ @ @ @ @ @ @ @ @ try reinstalling Resolve . But you may well be advised to look into C : volume repair first . <p> Learn from this experience : backup the project database at least once a week , and backup your current project every single day , just in case . These are good habits to get into for any software , not just Resolve . <p> Learn from this experience : backup the project database at least once a week , and backup your current project every single day , just in case . These are good habits to get into for any software , not just Resolve . 43971 @qwx453971 <p> I 'll do that from now on . In Premiere Pro I usually have my current project being auto-saved to a Google Drive folder , so not matter what happens , I 'll have a copy of that in the cloud and I would have lost like 5-10 mins of work . In Resolve if I manually backup the project at the end of a day I can loose the entire day if it crashes right before I backup it @ @ @ @ @ @ @ @ @ @ I said it before and I 'll say it again . Resolve , like some other grading software , MUST offer an option of automatic backups to a second location automatically without using some third party scripts or software . And while we 're at it , I still do n't understand the wisdom of Auto save not enabled by default . Yes , you can create a preset with it enabled , but still , why the hell not as a default ? ? ? <p> I 'll do that from now on . In Premiere Pro I usually have my current project being auto-saved to a Google Drive folder , so not matter what happens , I 'll have a copy of that in the cloud and I would have lost like 5-10 mins of work . In Resolve if I manually backup the project at the end of a day I can loose the entire day if it crashes right before I backup it . Is there a way to avoid this ? 43971 @qwx453971 <p> Export a . drp after a significant amount of work . @ @ @ @ @ @ @ @ @ @ from remote grades to local ; after my first balance pass ; more frequently when doing a lot of tracking or animating . It takes only a few seconds , and my . drp 's are in Dropbox . If my machine explodes , I can finish my grade at your house . <p> I said it before and I 'll say it again . Resolve , like some other grading software , MUST offer an option of automatic backups to a second location automatically without using some third party scripts or software . And while we 're at it , I still do n't understand the wisdom of Auto save not enabled by default . Yes , you can create a preset with it enabled , but still , why the hell not as a default ? ? ? <p> Has anyone found an easy way to export individual projects via command line / postgres ? I had a look at some of the postgres stuff , but could n't tell which tables related to which projects . <p> Would be nice if there were official Resolve tools for @ @ @ @ @ @ @ @ @ @ , making it easy enough to script up something to save every project you 've worked on in the last 24 hours or similar . 
@@44332776 @4332776/ <p> HDR10 is the open standard , and there 's a good chance at least on the consumer level , some companies wo n't want to buy in to Dolby Vision licensing unless it 's absolutely necessary . A game console is a big win though , probably bigger than a TV manufacturer , as a number of different TVs could work for most buyers , but there 's only one Xbox . <p> Actually , as well as this , there are a number of ' studio ' and ' broadcaster ' discussions ongoing regarding HLG as a better alternative ... I see a lot more discussions and political bickering before this formats war is really resolved one way or another . <p> HLG does make a lot of sense in many ways . Especially as you can have different HLG settings for different viewing conditions , which you can not have with PQ based ' HDR ' . <p> I think that being Dolby it will probably co-exist with HDR-10 . We have a format war already on sound with DTS and Dolby and we got used @ @ @ @ @ @ @ @ @ @ the best bet right now is to get TV that supports both Dolby Vision and HDR10 . I 'm actually checking some TVs for home and I was impressed at the Vizio P series , they support DV but soon they 'll release a firmware upgrade that enables HDR10 as well . Then there 's LG that supports both already , but I heard some retailers returning a lot of OLEDs for screen retention problems and the LED ones the blacks are awful . Vizio has really good black levels , specially the 65 inches and up . <p> how these displays handle normal content ? ( HDR wise ) ... do they " adapt " everything to HDR ( like they upscale everything to UHD ) or they can be set at lest say 200 nits for normal material and they switch to HDR only when the content is detected ? <p> The theory is as you say - HDR content has to be detected via the correct metadata before the display will go into HDR mode . Unfortunately , there are a lot of reports of this failing @ @ @ @ @ @ @ @ @ @ be manually set to HDR/SDR , but this is not normally an option in consumer displays . <p> The theory is as you say - HDR content has to be detected via the correct metadata before the display will go into HDR mode . Unfortunately , there are a lot of reports of this failing - in either direction . 43971 @qwx453971 <p> I saw a demo of HDR at IBC ( I will refrain from mentioning which stand ) where they had the same image on an SDR and an HDR monitor side by side . The HDR monitor looked brighter , but the SDR one had more highlight detail . They admitted that the HDR monitor must have been set up incorrectly and was clipping , but they were unable to change that at the show . If somebody demonstrating their own technology in a controlled environment ca n't get it right , what hope is there for consumers , a huge number of whom watch HD channels in SD on an HD TV , often in the wrong aspect ratio ! <p> The theory is as you @ @ @ @ @ @ @ @ @ @ correct metadata before the display will go into HDR mode . Unfortunately , there are a lot of reports of this failing - in either direction . <p> Professional displays can normally be manually set to HDR/SDR , but this is not normally an option in consumer displays . <p> Steve 43971 @qwx453971 <p> thanks steve <p> as i thought ... <p> i guess than i would like to ask direct testimony from owners of either LG or Vizio Series HDR ( since i am interested on one of these 2 for home ... LG seems to be the best , vizio seems to be cheap but good enough to step into a thing that is not even defined yet ... and consequentially to avoid spending 3x times more for a technology that will cost 1/2 price in 1 year ( referring to LG 7000 usd for the 65 inches HDR ... ) <p> i guess than i would like to ask direct testimony from owners of either LG or Vizio Series HDR ( since i am interested on one of these 2 for home ... LG seems to be @ @ @ @ @ @ @ @ @ @ enough to step into a thing that is not even defined yet ... and consequentially to avoid spending 3x times more for a technology that will cost 1/2 price in 1 year ( referring to LG 7000 usd for the 65 inches HDR ... ) <p> I would recommend to get a TV that definitely supports both Dolby Vision and HDR10 . Supporting HDR10 should n't be much of a difference to Dolby Vision to the manufacturer as both are pretty similar . I know , DoVi requires a Dolby processing chip or a chip design integrated or something but when the general tech is there it should be easy to implement . <p> In general I like DoVi much better as it transfers the creative intend while HDR10 afaik does n't exactly specify how gamut and pq are mapped if they exceed the monitors capabilities where DoVi has a predefined algorithm on the DoVi chip/IP module . <p> Sadly I have n't had the chance to see the LG or Vizio HDR displays but I 'd like to see displays with at least 1000 nits and quantum dot like @ @ @ @ @ @ @ @ @ @ series has around 1000 nits as far as I know and should be around 90-93% Rec2020 , but 384 backlight zones is pretty low . I ca n't find a number but I think to remember that the Dolby PRM 4220 also has a few hundred zones and the low backlight resolution leads to problems with images with small highlights surrounded by dark content . A good example are candles in the background with a dark wall behind . As the overall brightness of the zone is processed neither can the zone be dialled as dark as it would be nice for the background nor can it be as bright as it would be required for the candles as it would brighten up the black level too much . <p> Therefor OLED based devices are superior in this regards but lack in possible max brightness and do n't reach the gamut of quantum dot . <p> So maybe it 's best to wait a bit . Also I think those TVs only support HDR10 with HDMI 2.0a metadata over HDMI or coming via app , so currently probably low chance @ @ @ @ @ @ @ @ @ @ the overpriced ones ... ) or via USB . Or does anybody know if there is support for H265 files with HDR10 gamma information somewhere out there accessible besides in UHD mastering ? <p> I would recommend to get a TV that definitely supports both Dolby Vision and HDR10 . Supporting HDR10 should n't be much of a difference to Dolby Vision to the manufacturer as both are pretty similar . I know , DoVi requires a Dolby processing chip or a chip design integrated or something but when the general tech is there it should be easy to implement . <p> In general I like DoVi much better as it transfers the creative intend while HDR10 afaik does n't exactly specify how gamut and pq are mapped if they exceed the monitors capabilities where DoVi has a predefined algorithm on the DoVi chip/IP module . <p> Sadly I have n't had the chance to see the LG or Vizio HDR displays but I 'd like to see displays with at least 1000 nits and quantum dot like Rec2020 coverage and enough backlight zones . The Vizio reference series has @ @ @ @ @ @ @ @ @ @ be around 90-93% Rec2020 , but 384 backlight zones is pretty low . I ca n't find a number but I think to remember that the Dolby PRM 4220 also has a few hundred zones and the low backlight resolution leads to problems with images with small highlights surrounded by dark content . A good example are candles in the background with a dark wall behind . As the overall brightness of the zone is processed neither can the zone be dialled as dark as it would be nice for the background nor can it be as bright as it would be required for the candles as it would brighten up the black level too much . <p> Therefor OLED based devices are superior in this regards but lack in possible max brightness and do n't reach the gamut of quantum dot . <p> So maybe it 's best to wait a bit . Also I think those TVs only support HDR10 with HDMI 2.0a metadata over HDMI or coming via app , so currently probably low chance to play own material via a Decklink ( only with the overpriced @ @ @ @ @ @ @ @ @ @ know if there is support for H265 files with HDR10 gamma information somewhere out there accessible besides in UHD mastering ? 43971 @qwx453971 <p> I do n't think consumers care about how " creative intent " was transferred , if HDR10 is cheaper , it will probably get more support . <p> VIZO R covers about 85% BT.2020 , on par with some highend OLEDs . Panasonic/Toshiba/Sony has models with 512 dimming zones , that 's the most you get on consumer models currently . The Dolby one has 1536 zones and Sony has a prototype demoed at NAB with the same amount . With good backlight design and panel compensation ( eg. Panasonic and Sharp did really well ) , halo is not a big concern , especially for the average viewer . <p> I saw a demo of HDR at IBC ( I will refrain from mentioning which stand ) where they had the same image on an SDR and an HDR monitor side by side . The HDR monitor looked brighter , but the SDR one had more highlight detail . They admitted that the HDR monitor @ @ @ @ @ @ @ @ @ @ but they were unable to change that at the show . If somebody demonstrating their own technology in a controlled environment ca n't get it right , what hope is there for consumers , a huge number of whom watch HD channels in SD on an HD TV , often in the wrong aspect ratio ! 43971 @qwx453971 <p> This frightens me , and this could snowball into a worst-case scenario in another year . <p> I saw a demo of HDR at IBC ( I will refrain from mentioning which stand ) where they had the same image on an SDR and an HDR monitor side by side . The HDR monitor looked brighter , but the SDR one had more highlight detail . They admitted that the HDR monitor must have been set up incorrectly and was clipping , but they were unable to change that at the show . If somebody demonstrating their own technology in a controlled environment ca n't get it right , what hope is there for consumers , a huge number of whom watch HD channels in SD on an HD TV , @ @ @ @ @ @ @ @ @ @ This demo you saw , were they using curved , Samsung SUHD displays , like this ? <p> VIZO R covers about 85% BT.2020 , on par with some highend OLEDs . Panasonic/Toshiba/Sony has models with 512 dimming zones , that 's the most you get on consumer models currently . The Dolby one has 1536 zones and Sony has a prototype demoed at NAB with the same amount . With good backlight design and panel compensation ( eg. Panasonic and Sharp did really well ) , halo is not a big concern , especially for the average viewer . 43971 @qwx453971 <p> My only real experience is with the PRM4220 and backlight modulation is bad there in some circumstances as it can be quite limiting for some situations . I think it should n't be that much effort to increase the number of zones ( of course at cost ) ... <p> This demo you saw , were they using curved , Samsung SUHD displays , like this ? 43971 @qwx453971 <p> A certain company used one of those for demos and as long as you look at it @ @ @ @ @ @ @ @ @ @ was very good . But going a bit to the side revealed the bad viewing angles of the panel . 
@@44332777 @4332777/ <h> Avid to Resolve Inconsistent Conform <p> I 'm coloring a feature that was shot on Alexa and edited in Avid . I just want to get an AAF or EDL into Resolve of the online edit . <p> The movie was cut using DIT dailies that the DIT transcoded from the native 2K to a temp-colored 1080p prores . So I have this final cut and i just want to relink all the clips and online it to the 2K native ( except for some pickups shot on RED and BlackMagic that will need their own online ) . <p> The editor did n't use any AMA linking . But I 'm told the metadata I need is in the Native Alexa footage as well . I want to relink all of that footage as quickly as possible to the native , and I believe all I need to do it is the tape name and timecode metadata , which both the dailies and native share . <p> So , inside Resolve I imported all of the native Alexa footage , then I imported the AAF and unchecked @ @ @ @ @ @ @ @ @ @ source original files " and pointed to the bin of native footage . <p> At first glance it looked as though in the final cut timeline , all of the Alexa footage was online and correct , but repeatedly in a number of scenes some clips linked to a shot from a completely different scene . Some of them have conflict resolution badges , but when clicked , the other clips it lists as being in conflict ( sometimes 2 or 3 clips ) are not the correct clips either . <p> So , I 'm not sure what happened here . If the metadata needed to properly link to source was n't there or perhaps Resolve just linked from timecode ? Still I do n't see how that would explain the wrong clips it sporadically linked to because I do n't believe there was duplicate timecode . <p> Any ideas how I can make this initial conform more reliable instead of force conforming with clips in the media pool over and over ? <p> The chapter in the manual on conforming and relinking ( pp. 497-558 ) is among @ @ @ @ @ @ @ @ @ @ when there is a conform conflict , there 's an issue with the Reel Name or a duplicate file name . You can set the conform up so that it 'll automatically use the file name as the Reel Name , and 97% of the time , this solves the problem . If you have a handful of conflicts , it could well be these are effects or some other non-standard clips that does n't quite fit the list . <p> I always use a reference video made in the Avid and compare it to the conform so there 's no surprises later on . There are issues with repositions , retimes , speed ramps , and so on , but those are manageable to a point . I generally ask for an XML , an AAF , and an EDL if possible so that , worse comes to worse , I can print out the EDL on paper -- old school -- and see why things are blowing up . Sometimes one EDL format works better than the other ... but I have had AAF 's work perfectly on @ @ @ @ @ @ @ @ @ @ along an XML to Final Cut Pro , FCPX , Avid , Premiere , Resolve , or anything else , sometimes shots fall through the cracks . I consider it a huge success if I get 95% of the shots correct the first time . If there 's 50 bad shots out of 2500 on a feature , that 's not that bad . When it 's several hundred shots , that 's a big problem . I have definitely had this happen on shoots that used Red or Alexa as the main camera , then snuck in a non-timecoded camera like a GoPro or a Canon DSLR or something like that . I just grit my teeth and get through it . In cases where I can work closely with an assistant editor , I 'll get them to police the edit first and spot these problems before they happen . <p> Thanks , Marc . Have n't solved this yet , but I think it may be due to the fact that the timeline I exported from Avid as an AAF was using lots of subclips and some @ @ @ @ @ @ @ @ @ @ does n't know what to do with those because it 's not referencing the file itself but the subclip ? I 'll try decomposing the entire timeline and then exporting out again as AAF . <p> Also , I believe the DIT ingested the Native into Resolve at first and used that to create his dailies versions hey cut from , but I think he was logging Tape Name for Avid and not reel names , which could also be part of the problem , but maybe not since there are many sections where the conform was correct . <p> All the best advice has been provided above but in general , I 've found the conform aspect of color Correction to be very confusing and potentially an area where You can take a huge hit financially if clients do n't understand what is involved and how they provide a XML/AAF less Prone to issues . It is n't common knowledge that conforming is n't a " one click " operation and can potentially take a lot of time if trey have n't listened to you in terms of recommendations @ @ @ @ @ @ @ @ @ @ shots out of 2500 do n't come through , that is n't too bad and fairly common , especially if they had VFX . If they 've prepped properly it should always be fairly straightforward aside from a few clips , but if not , it is a game of manual relinking . An additional suggestion is to try and determine if the issue is based on you or the materials provided , as poorly organized materials will result in a tougher conform and that should be billed appropriately . Conform is misunderstood as a one click operation and it 's our job to ensure clients do n't continue thinking that way , or that you fix the issues on your dime if it was due to sloppy or poor editorial organization <p> There 's an option when you right-click a sequence in Avid that 's called ' collapse multicam edits ' ( if i remember correctly ) . You could try that maybe , if did n't do so already . <p> Edit : it 's ' commit multicam edits ' 43971 @qwx453971 <p> Multicam clips always are bad @ @ @ @ @ @ @ @ @ @ of always Commiting Multicam edits in MC before making an AAF as well as using the Remove Match Frame function to simplify the timeline when editors extend a shot with an edit added ( never understood why they do this ) . <p> Conform is misunderstood as a one click operation and it 's our job to ensure clients do n't continue thinking that way , or that you fix the issues on your dime if it was due to sloppy or poor editorial organization 43971 @qwx453971 <p> The worst thing about being a freelance guy is that ( generally ) we are our own assistants , so having to clean up conform messes is a drudgerous , evil , difficult task at best . I have sometimes lamented that we occasionally run into projects where the conform winds up being far more time consuming than the color correction because of client/editor issues . <p> The other pain for me , as the online editor as well as the colorist , is that often the speed ramp ( or any motion fx ) are rendered at a poor interpolation choice @ @ @ @ @ @ @ @ @ @ some of these is frustrating as they 'll need to be rebuilt anyway . This seems to be more of an issue with MC and FCP 7 than with Premiere these days but still a pain sometimes . 
@@44332778 @4332778/ <h> clip groups in resolve <p> I have a short film that I 'm learning on and would like to apply a grade to all the clips in the timeline . <p> So , I made a group that included all the clips and applied a post clip node grade . It applied to all the clips as intended . <p> I also want to apply a grade to a short group of clips , so I created a new group of the short group of clips . And ... It seems that it 's impossible to have a clip in more than one group . Is this correct ? When I make the new group , apply the grade to that group , my grade on the previous group disappears and the previous group 's member clips changes . <p> In SpeedGrade I could create a grading track over as few or as many clips as I wanted . Is there a way do accomplish this in Resolve ? <p> I grade one of the " group " clip first and then grab a still . The still @ @ @ @ @ @ @ @ @ @ ckick on the still ) to any group of marked clips in the timeline or just over middle mouse click ( TM ) inside the timeline without a still . <p> I also want to apply a grade to a short group of clips , so I created a new group of the short group of clips . And ... It seems that it 's impossible to have a clip in more than one group . Is this correct ? 43971 @qwx453971 <p> That is correct . You can only have a clip in one group at a time . <p> However ... you can create a new group and paste in the original group post-clip grade and then add something new to it . Or you can collapse the group grade and make it part of the actual clip grade , and then just create a new group . There 's a lot of workarounds . <p> Pepinjn above is correct that a Timeline node is another possibility , but it 's a bit of a pain turning those on and off . It can be done with effort @ @ @ @ @ @ @ @ @ @ a clip can not be a member of more than one group , and that it 's necessary to collapse the group before defining new groups if one will be accessing the same clips . In this way using multiple groups , if there 's any chance of overlap is at risk for surprises . And this is not stated in the davinci manual . Too bad it works this way ... <p> n this way using multiple groups , if there 's any chance of overlap is at risk for surprises . And this is not stated in the davinci manual . Too bad it works this way ... 43971 @qwx453971 <p> I can understand why they do n't allow multiple simultaneous groups . I generally find it better to just apply the look manually . As it is , the only time I even use groups is when trimming with a client . Usually , they 'll say , " hey , that entire hallway scene is too bright ... can we bring it down 10% ? " And I can group that one shot and drop it @ @ @ @ @ @ @ @ @ @ , I 'd rather do them by hand . 
@@44332779 @4332779/ <h> Custom ergonomic DaVinci Control Surface Desk <p> A couple of months ago , FARBKULT Post-Production and I designed a desk which integrates the Black Magic Design DaVinci Resolve Control Surface , because we always felt that the control panels sat on top of the tables a little high , both visually and ergonomically . So we went ahead and created the following prototype , on which me and several other colorists have already graded a bunch of feature films with incredible comfort . <p> The desktop is made out of solid oak wood , which was darkened using sal amoniac ( Salmiak ) . There is space for up to four 24 " screens on adjustable VESA-mount arms and we designed it , so that a client can sit to the right of the colorist too , with enough space for a laptop , a notepad or just a drink and a snack . There are also two touch-dimmable spotlights on adjustable arms at the sides of the control panel , so that both the colorist and client can take notes in the dark , without any light @ @ @ @ @ @ @ @ @ @ like to stand at work sometimes , I proposed motorized adjustable height table legs , so you can smoothly raise or lower the whole desk to fit your sitting and standing positions . <p> Below the desktop , there is a small slide-out compartment for a remote , which controls the motors to precisely raise or lower the table . By the way , the tabletop looks really thin in this picture , but it 's actually bevelled around the bottom edges , similar to the new iMacs ' backs , so that it looks thin and light from most angles , while actually being super sturdy . <p> In front of the middle control panel , there is a thin strip of black leather inserted into the table surface , which feels good on the wrists and also acts as a slide stop , so that you can put a backlit keyboard , a graphic tablet or a notepad in front of the trackballs . I 've worked with all of those pictured configurations myself already during conform , grading and approval and it 's great ! Of course @ @ @ @ @ @ @ @ @ @ too and that you can still sit close enough to the control panels in an ergonomic position . <p> The DaVinci Resolve Control Panels are lowered into the table so that their fronts sit flush on the wooden surface . The original angle of the panels remains the same . <p> As an added bonus , we had some inlets milled for notepads , glasses and pens , so that you always know where to find them in the dark . <p> If no client is present , we can use the right side of the table as a workplace to access our Clipster and watch its timeline on the projector or TV . <p> We are currently hashing out the details to offer custom tables like this for sale , so if you are interested in one , send an e-mail to info@farbkult.com . Also , feel free to post questions in this thread or send them in via e-mail . <p> Nice one ! really slick . We also lowered our panel into one of our tables . The loss of the original keyboard is n't really a @ @ @ @ @ @ @ @ @ @ with cradle for the panel ? And how 's airflow at the back of the panel ? And how 's cable management ? That 's stuffs I geek out about . = ) Nice cable management is a feast for my eyes ! <p> With all those remote controls you should look like something like crestron and an iPad solution for controlling everything . = ) <p> How does it look underneath with cradle for the panel ? And how 's airflow at the back of the panel ? And how 's cable management ? That 's stuffs I geek out about . = ) Nice cable management is a feast for my eyes ! 43971 @qwx453971 <p> I 'll see if I can post a picture . It 's basically a wooden platform , lowered on four threaded poles ( so you can precisely lower the panels ) . Air can flow freely around the panels there . At the back behind the panels , the desktop is carved out a little more too , to allow cables to go under the table and to allow access to the @ @ @ @ @ @ @ @ @ @ actually does not have to be a mess under there . Just put a power strip and a USB hub on the platform below the desktop , and all you need are 2 HDMI cables for the screens , one USB cable and one power cable going to the table . Tie those up with one of those spiral tube things for cables and you 're done . You can see it in the first picture on the left edge of the frame . I guess I could tidy it up even more , but we 're still arranging things a bit . I prefer velcro ties for cables , by the way . 
@@44332780 @4332780/ <h> LUT to replicate Youtube Compression <p> Does anyone here know of any LUT 's or Looks that can be purchased to simulate the effect of heavy compression on the internet . It be great to sample something like this to see if an aggressive grade would fall apart through youtube compression . <p> Sapphires SJpeg ( if i not mistake with exact name of plug-in , but its in Sapphire bundle for sure ) will give you jpeg compression effect with lots of parameters to tweak. 43971 @qwx453971 <p> I have used it extensively for quick simulation of compressed online video look , but it really does not looki convincing enough . The best thing to do if there is time is to actually encode H.264 with very low quality setting and import it back in. 
@@44332781 @4332781/ <h> CMIVFX : Mistika Advanced Channel Keying <p> Princeton , NJ ( April 27 , 2015 ) For over 15 years , node-based compositing applications were the backbone of a film pipeline . These days , we rely on procedural-based software to create both visuals and audio effects in just about any creative industry . To better assist the art of color correction and keying , a method of channel extraction was created called " Maynard Keying " ( Sometimes known as M-Key or Modular Key ) . This workflow relied on node-based software exclusively in order to achieve the goal of combining various channels into solid-object extractions which would work with semi-transparent areas . Many keyer tools have come a long way since the beginning . In some cases they can handle an entire key all by itself without needing to do any extra work . The problem that still exists is that not all shots have good lighting and reduced noise . Maynard Keying solves ANY SHOT you can throw at it because it allows you to build areas up and combine them with logic and math @ @ @ @ @ @ @ @ @ @ However , back just a few years ago , SGO was releasing Mistika without a node tree to work from which made +G+ " Maynard Keying+G nearly impossible to achieve . Thanks to this new version , both Mistika and MambaFX can now perform these procedural workflows allowing for the most complicated VFX problems to be solved . Take a fun journey through a legendary technique applied to yet another high end compositing software in this video . <p> Vimeo Teaser Trailer <p> Short Description " Maynard Keying " is the longest standing technique in compositing . All node-based applications rely on it to create better channel isolations or keys . <p> Introduction This video is meant to be a semi-informal discussion on how procedural nodes can help create a faster and more parametric workflow to the existing world of layer-based parenting in general tool sets . Detailed descriptions of the methods are explained with examples in later chapters . <p> Exploring Our OptionsThe method to Maynard Madness ? Leave yourself many alternative escape routes . When you have been doing this kind of work for a long time , @ @ @ @ @ @ @ @ @ @ closely to it as possible . THAT IS NOT A GOOD THING . At least not for creativity . You can stagnate by doing the same thing over and over again , and often times another solution could work and may even be more efficient . This chapter shows you how to get the same result with different roads taken . <p> Minimum " M-Key " After many years , the " Maynard Key " has been renamed to fit certain workflows . It has gone through many iterations such as M-Key or Modular Key . However , the industry has come around full circle in that the technique that was invented over 15 years ago , is the one of the only techniques that stands the test of time and can be used in ANY NODE SOFTWARE that supports channels . So we clarify the smallest , most compact elements of the Maynard key and call it the " M-Key . " The base of a Maynard Key can later be extrapolated and branched into variations that will later be combined with logical operations such as add , subract @ @ @ @ @ @ @ @ @ @ of alpha and other image channels using math nodes . <p> Edging MethodsNow , after learning how to build your area-based M-Key , you might want to also use " M-Edge " ( aka " Medging " ) instead of using commercial de-spilling tools . Some of you legacy VFX artists might remember the original " Maynard Light Wrap " technique back when Shake was the dominant desktop compositor for film . This very same solution has migrated to each node based app that exists out there , and thus , by user request , we will show you how to perform the operation in Mistika ( also MambaFX ) . The concept of " Medging " can be a bit more time consuming and each user will spend their own time layering up variations of this for later compositing . This chapter also relies heavily on channel nodes to extract out and visualize your updates in realtime . <p> Taking Out The GarbageExactly how it sounds . Garbage masking can be done in many ways . For NEWCOMERS to Mistika , the combination of Framing and Animated Roto can @ @ @ @ @ @ @ @ @ @ even more fun is how you can pre-clean your sequences so that all the labor is only dedicated to the areas that need it most . One of the biggest mistakes in compositing , is working on parts of an image that wo n't ever be seen in the final shots . ( Pride can make us lose focus a bit in this area so do not be afraid to take out the garbage ! ) <p> Maynard VarietyInstead of beating a dead horse with the exact same branch of tools , we will spend our time showing you alternative routes , which will all be combined into a MASTER MAYNARD KEY structure ( aka Master M-Key ) . It is very possible that your second or third branch of creativity might turn out better than the first one , so by copying that flow to another area , you could render better results . The trick is to keep an eye on the node tree to see which ones are producing the same results with less CPU . <p> SummaryThis video is dedicated to just a handful of users who @ @ @ @ @ @ @ @ @ @ inside of Mistika and MambaFX . Those users coming from other software can apply this knowledge to their new workflows , however , the only true constant in compositing is that channels are the only way to get an accurate representation of pixel based semi-transparency and object edges . For more information , you may always contact us at support@cmivfx.com for one on one time with a mentor . <p> About The InstructorChris Maynard is the owner and Chief Public Partner of the profit-sharing organization known as cmiVFX.com . Surprisingly , the letters " CMI " in the company name are not an abbreviation for Chris Maynard Institute . The initials actually stand for the first company name , entitled " Creative Minds Imaging , " which dates back to 1997 and is responsible for the creation of digital pre-press applications made from adobe products on roto gravure , which is a form of heat transfer printing . Since then , Chris has been the driving force behind a large majority of Application User Interfaces for the visual effects and computer graphics industries . He has designed application features , @ @ @ @ @ @ @ @ @ @ in the visual effects industry has used on some level . Applications like Photoshop , Painter , and Poser allowed Chris to break into the " commercial illustration and design world . " This was when he decided to move away from the field of engineering . Before graduating from Rochester Institute of Technology for Advanced Computer Graphics , Chris studied Laser Holography for real 3D applications . In 1998 , he became the first person in history to simulate holograms of helium neon laser light . He did this on a Cray super computer , using Houdini as the backbone of the project . Later , he moved on to become the Creative Director for several large corporations in which training for CBT ( computer-based training ) sparked his interest in combining all four of his practiced skills into one complete business operation . For the last decade , Chris has been assisting tens of thousands of customers deliver top shelf productions through training and extensive labor . He is a dedicated service provider for freelance compositing work whenever he is not managing cmiVFX . If you need him @ @ @ @ @ @ @ @ @ @ , just email him at chris@cmivfx.com . <p> Project ContentsAll cmiVFX videos come with all the training materials you need right from our website . No matter what time of day , your location , or how your feeling , cmiVFX will be there waiting for you ! <p> There are some notorious shots from Pirates of the Caribbean , blue screen ship to ship battle where they thought dense practical smoke would be a good idea . Motion blurred rigging , pirate swords , explosions illuminating the haze etc . There was hardly any blue left and rotoing hazy soft smoke is never fun . I never touched an actual Nuke script just saw screen grabs of the node networks , mind funking virtuoso work , every Keyer in Nuke many many times , custom Keyer gizmos , multiple debayers of the source , insane micro channel transforms to pull every last usable pixel . Layers and layers of alpha beautifully blended until it 's seamless in the finals . Probably 80% was edge work ! it 's all about the edges . Hats off and a keg of @ @ @ @ @ @ @ @ @ @ 
@@44332782 @4332782/ <h> How important are YCbCr scopes ( now ? /compared to others ? ) <p> I have heard many things about this scope . I know that Adobe Premiere uses this , but I have also heard that it is inaccurate ( which is amusing that they would still include an inaccurate scrope regardless if it were useful or not ) . Others have said this was just a hold over from before digital ( ? ) . Unsure if that holds any water . <p> How often would professional colorists reference this scope ? I can see justifcation for the other scopes , and I do understand what this scope is supposed to represent , but I can not imagine a scenerio where this would be useful , or unattainable from another scope , but I am all ears . <p> " inaccurate " might be the right words but it is also somewhat misleading . <p> Even if I have a instrument that does not measure the real value ( but a close one , and for various reasons ) I use the instruments for difference comparison @ @ @ @ @ @ @ @ @ @ hue/sat wise to the eye , I can use the vectorscope to compare two shots an see the difference . <p> YCbCr parade is useful if you 're delivering 4:2:2 and the spec sheet calls for legal YCbCr and does not care about the RGB gamut . Balancing color by looking at color difference signals in the YCbCr parade is counterintutive to say the least , and that 's why I prefer both YCbCr and RGB parades when working in 4:2:2 . In 4:4:4 YCbCr parade is largely irrelevant . <p> Unless legal RGB is a requirement , using RGB parade sacrifices some values that are legal in YCbCr domain but not in RGB domain . For example , there are YCbCr values that are perfectly legal but produce illegal negative RGB values . If you use RGB parade only you constrain yourself to legal RGB while sacrificing some of the legal YCbCr gamut . Typically this is not a huge issue until you want to hit some very saturated color . <p> Also , in terms of " purity " for the lack of better term , depending on @ @ @ @ @ @ @ @ @ @ be stored as YCbCr data . They are certainly transmitted via SDI cable as such when outputting to tape . YCbCr parade shows you the true condition of the signal , whereas RGB shows you recalculated interpretation of the hypothetical RGB transcode . It 's like trying to respect a 55 mph speed limit but having your speedometer calibrated in km/h only . This is generally not a detriment to the creative work in my opinion . <p> YCbCr parade is useful if you 're delivering 4:2:2 and the spec sheet calls for legal YCbCr and does not care about the RGB gamut . Balancing color by looking at color difference signals in the YCbCr parade is counterintutive to say the least , and that 's why I prefer both YCbCr and RGB parades when working in 4:2:2 . In 4:4:4 YCbCr parade is largely irrelevant . <p> Unless legal RGB is a requirement , using RGB parade sacrifices some values that are legal in YCbCr domain but not in RGB domain . For example , there are YCbCr values that are perfectly legal but produce illegal negative RGB values @ @ @ @ @ @ @ @ @ @ to legal RGB while sacrificing some of the legal YCbCr gamut . Typically this is not a huge issue until you want to hit some very saturated color . <p> Also , in terms of " purity " for the lack of better term , depending on the system you work on , your 4:2:2 media may be stored as YCbCr data . They are certainly transmitted via SDI cable as such when outputting to tape . YCbCr parade shows you the true condition of the signal , whereas RGB shows you recalculated interpretation of the hypothetical RGB transcode . It 's like trying to respect a 55 mph speed limit but having your speedometer calibrated in km/h only . This is generally not a detriment to the creative work in my opinion . 43971 @qwx453971 <p> oh ! you where talking about YCbCr parade , not the vectorscope ... <p> 1 ) My previous post is stupidly out of place ... 2 ) I think I never used the YCbCr parade .... <p> haha Walter , it 's ok . All 3 of you helped solidify what some others @ @ @ @ @ @ @ @ @ @ if it were to be not included , it is n't the end of the world - by a long shot . Thank you for your very detailed explanation Igor . 
@@44332783 @4332783/ <p> They are a bit different so comparing them might not be totally fair . I would classify Scratch as being very close to an all-in-one finishing system with a more general purpose conform , color , and composting tool set whereas , Baselight is a purpose-built , thoroughbred color grader . Both are good conform stations , but Scratch takes it a bit further with a lot of dailies and asset management functionality . On the other hand , Scratch is somewhat limited in terms of pure color grading by comparison , as Baselight has just about every color tool under the Sun , all of which are world-class in terms of their precision and implementation . There are very few people who will say " No " to using a Baselight , and it 's probably the only other system I hear mentioned as much in daily conversation as Resolve . <p> Scratch , however , does add quite a bit of compositing functionality that Baselight does n't include . It 's primarily layer-based though , and so while not as deep as Nuke or After Effects @ @ @ @ @ @ @ @ @ @ compositing , paint-outs , removals , and screen and sky replacements . The layers also give you interesting creative grading options using blend modes in combination with keys and masks that some colorist have gotten some pretty stunning results with . Baselight is also very layer-based , and while I have limited experience with Baselight myself , whenever I see a demo , it seems layers are used much more for organizational and order-of-processing purposes than for actual compositing . <p> Performance-wise they are both very capable , although Baselight has a bit of an edge in that it can scale a bit further by harnessing multiple GPUs like Resolve , where Scratch is bound to a single large GPU that is used for both GUI and processing . Even though , Scratch can get a lot out of that single GPU . I have even run it on a GT640 all by itself and was able to playback the most complex HD projects I have ever graded at their full frame rate with no problems . Scratch also has the advantage of being available on Mac or Windows , @ @ @ @ @ @ @ @ @ @ , where Baselight ( other than Baselight editions ) is really the kind of system you drop into a machine room next to your fibre SAN and HDCAM SR decks . <p> If you made me come up with an analogy , I would say Baselight is a lot like an M1 Abrams tank , where Scratch is more like an IAV Stryker . One is very focused on a single task , and gives you a tremendous amount of sheer firepower to accomplish it with , while the other is mobile , flexible , and gives you several good options for 80% of the things you will ever encounter , all in one application . <p> So the blending modes and compositing tools of Scratch : are they analogous to the range of tools in Baselight ? Or just a case of different approach ? In other words , are you able to ' roll your own ' Baselight toolset from Scratch ? <p> I 'm gon na say a qualified " no " to that question . The compositing capabilities of Baselight are n't as comprehensive or purpose-built @ @ @ @ @ @ @ @ @ @ latest 4.4 version of Baselight added a decent amount of composite-y type capabilities . I have n't have my hands on 4.4 yet , but I do n't believe that even the new stuff brings Baselight even close to on par with Scratch 's compositing tools - not that that was a goal of the 4.4 release . As Jason said , it 's still no Nuke/AE/Smoke , and the compositing interface is , well , let 's call it " unique " , but there is a lot of capability there . Baselight 's primary client base just is n't concerned with doing compositing &amp; color grading simultaneously , at least right now , and BL 's development reflects that . <p> The color-grading-related tools in Baselight are just flat-out more extensive and better than their Scratch equivalents - as Jason said , every tool under the Sun . The shape tracking &amp; keying in Baselight is *so* superior to Scratch that it 's almost unfair to compare the two , although Scratch can now import tracking information from Mocha , which ai n't no slouch in the tracking @ @ @ @ @ @ @ @ @ @ import is , but it 's better than not having it . There is , of course , the additional inconvenience of having to go into an outside application to get awesome tracking . <p> As for keying , there 's no way that I know of to get anywhere near the capabilities of the Baselight D-Key in Scratch . There are no log-scaled exposure/contrast tools in Scratch , and no real way to make them happen without using a LUT . The 6-vector tools in Baselight are pretty great ; I never even touch the Scratch equivalent ( I use the ' vs ' curves for that which I think works a million times better ) . <p> Also , you have to remember that , in the apples-to-oranges department , you 're comparing software-only to a turnkey hardware/software system ( assuming we 're discounting the BL Editions stuff ) . Baselight is not only software , but is also custom-spec 'd &amp; assembled hardware running it 's own purpose-built &amp; maintained Linux distro . Scratch runs on Windows/Mac on commodity hardware , and does so remarkably well , @ @ @ @ @ @ @ @ @ @ all this as a Scratch lover . The deal is , although these , and probably other features are either missing or not as good in comparison to Baselight , Scratch is a perfectly capable color grading platform . I only do occasional assist work on Baselight , and every time I do , I definitely find myself saying " Man , I wish I could do THAT in Scratch " , but I ca n't say it 's held me back . If I was Peter Doyle or a senior guy doing Superbowl commercials I might feel differently , but then again , at that level , I 'd have a whole different set of concerns working in those environments . 
@@44332784 @4332784/ <p> ( San Francisco , California--May 21 , 2013 ) RE:Match takes intuitive color and texture matching to a whole new level by addressing common problems on multicam and stereo shoots . RE:Match is a set of plug-ins to match one view of video or film to another so that it looks as if it was shot with the same camera and settings . RE:Match assumes that both image sequences are taken roughly at the same location but perhaps were not shot at the same time or from the same point-of-view . <p> For general multicam shoots RE:Match matches , as best it can , one sequence 's overall color appearance to match that of another . For stereo pairs , RE:Match can further refine the overall color match by taking into account that the two views are captured at the same time with a relatively small distance between the views . In addition , RE:Match is also able to propagate color changes into areas that are present in only one view . RE:Match can also make other details , such as reflections , specular highlights , and lens @ @ @ @ @ @ @ @ @ @ a video of RE:Match in action ! <p> Nick Brooks , Academy Award and Emmy Award winner for visual effects , says RE:Match provides " amazing color matching for stereo pairs ... really ! ! ! The specular and optical aberrations matching is high quality and very effective . Like all RE:Vision Effects software , it 's nicely parameterized for pro-users , but works out of the box . " <p> Johnathan Banta , stereo pipeline designer , says " The smart guys at RE:Vision Effects have created a very cool toolset that I have been lobbying for for a long time . Brilliant ! " <p> Features of RE:Match : <p> Ability to globally color correct one sequence to match another . <p> Ability to globally color correct a whole sequence based on a particular point in time , or can be performed separately for each pair of frames in both sequences . <p> Ability to perform automatic global ( whole image ) color correction based on a smaller window of pixels for each of the image sequences . <p> About to change without notice . 
@@44332785 @4332785/ <h> Fastest Transcoders and Workflows . <p> Lately I 've been doing several tests with several different encoding applications , and have been very disappointed in the results . Now one thing I 'd like everyone to keep in mind before I dive into details is the possibility that I am expecting to much , and if I am then please tell me so . <p> My test is similar to what I believe Jason Myres has already described on his website here H.264 to DNXHD 36 . <p> So far I have come to the same conclusion however , I am on a PC , and very beefy PC to be specific . So before I post the results and my fustration I 'll also post the specs . <p> Using MPEGS Stream clip my results are actually about 6x faster , however I 'm only utilizing 4-6% of my CPU resources . So the software is more optimized for the task , however why can I get much better speeds with my system ? <p> So I did one last test . I launched 8 Instances of @ @ @ @ @ @ @ @ @ @ each instance to perform 4 transcodes simultaneously . The good news is 500GB was transcoded in just a few hours to DNXHD 36 . However the average CPU usage was only 28% ! ! ! <p> This leaves me confused , and asking . Is there any piece of software out there ( Other than Colorfront , Scratch , and Resolve or DVS Clipster Software/Hardware ) that can take advantage of system resources more efficiently ? <p> Is there any piece of software out there ( Other than Colorfront , Scratch , and Resolve or DVS Clipster Software/Hardware ) that can take advantage of system resources more efficiently ? 43971 @qwx453971 <p> Multi-threaded transcoding is a tricky thing , and most of the developers that make applications that are good at it , charge you for it . Some of the more popular ones are Telestream Episode and Episode Engine , Vantage , as well as Harmonic Rhozet , and Digital Rapids . Other than Episode , most of them are way more than you 'll ever want to pay for on your own . <p> Even though I have @ @ @ @ @ @ @ @ @ @ Media Encoder was very slow for me , too . MPEG Streamclip is much faster , and you can run multiple instances on your machine at one time . It does alter the image a bit , although that may be ok for things like dailies/ proxy transcoding which it 's very popular for . 5DtoRGB will give you a very high-quality transcode , but is very slow , with moderate amounts of footage requiring several hours to convert . <p> For most people looking for fast solution , who willing to pay a moderate amount , Episode Pro is probably the most most popular option . There are also consumer options like Media Espresso , which I have been hearing more about lately , but I have yet to give it a try myself . <p> Testing on the same machine with CS6 showed huge differences in performance , with a delta of nearly 500% in extended tests . In other words , using Direct Export is almost five times faster than using the AME Queue ... <p> I like Sorenson Squeeze . It 's not free like MPEG @ @ @ @ @ @ @ @ @ @ mxf files so no fast import . I 'll encoded right to my Avid Mediafiles folder before . Toss them in a numbered folder , drag in the database files after a scan and ready to work . <p> I got curious reading this thread and started my own little test , transcoding a canon 5D mark II file of 60 seconds into DNxHD 36 , with various softwares . Unfortunately I could n't find the DNxHD option in either Adobe Media Encoder nor mpegstreamclip . Any advice on how to get that working is surely welcome . <p> I used a Hackintosh machine I build last week with a i7 3770K ( 4 cores ) processor . I monitored the time it took to transcode and the cpu usage in OSX activity monitor . Here 's my results : <p> Sorry , I did n't explain well . I used ama to bring them in and then to transcode to DNxHD. 43971 @qwx453971 <p> Ok , thanks , that makes a lot more sense . My only thought is that on larger jobs you 're tying up your Media @ @ @ @ @ @ @ @ @ @ big jobs you need a solution that can scale across multiple machines so you can get through all of your footage . But , 13 seconds is really fast , which is why I though your were only linking and not actually transcoding <p> Haha , so avid was sooo fast it made you think I was stupid yeah , I get the point of having a standalone solution for multiple deliveries etc . I think my pick would then be Sorenson Squeeze . It has many many features especially designed for this kind of work . <p> By the way , I looked at episode also mentioned here but I do n't think they do DNXHD transcoding , only reading . <p> Back then I was working in audio recording and live video production , trying to find a way to encode videos quickly from concerts and shows we were shooting . Then we discovered Compressor and never looked back . <p> I think , and correct me if I 'm wrong , that it used to come bundled with Avid software , but back than ( 2004 ... @ @ @ @ @ @ @ @ @ @ here was tape-based . But now it 's pretty much different ofcourse . And not to plug the software , but ... Multiple GPU accelaration , automatic upload thru FTP , watchfolders , scripts , email notification , it is all pretty cool . <p> I think , and correct me if I 'm wrong , that it used to come bundled with Avid software , but back than ( 2004 ... ) I never used it . Probably cause almost everything here was tape-based . But now it 's pretty much different ofcourse . And not to plug the software , but ... Multiple GPU accelaration , automatic upload thru FTP , watchfolders , scripts , email notification , it is all pretty cool . 43971 @qwx453971 <p> I guess what 's odd , is in your test , Sorenson with GPU was slower than Avid was without it ? I have used Episode , and while it 's slick , I am not in love with it for some reason . Setting up batches is tedious in Compressor and while Resolve is a very nice encoder , the @ @ @ @ @ @ @ @ @ @ . <p> So , yeah based on what you 've said I need to give Sorenson another try . <p> Gpu acceleration does n't occur on transcoding to dnxhd , but it does on transcoding to h264 and maybe some other codecs i might not be aware of . Will check that out soon ... <p> edit : So it only supports mainconcept h.264 encoding . However , I found this little paragraph as well : <p> " <p> Some AVID products are CUDA-enabled . These products may require a version of the NVIDIA drives that have a version number lower than the version number required by MainConcept 's H.264 . If this is the case , Squeeze will not allow GPU acceleration . AVID is in the process of certifying a newer version of the driver . When that is available and installed on your system , Squeeze will allow GPU acceleration . " ? <p> If I understand it correctly , transcoding to Avid codecs might be GPU accelerated in the near future as well. ? <p> Well ... After reading that paragraph twice , I think I @ @ @ @ @ @ @ @ @ @ only mean to say that whenever Avid certifies more up to date cuda drivers , the drivers will work with both Avid and Squeeze , so you can have the right drivers for Avid installed plus have gpu acceleration on h264 encoding ( so not on DNxHD ) at the same time , on the same system . Sorry for the false hope ! 
@@44332786 @4332786/ <h> Help the TIG <p> The TIG , as has been noted in previous days , has been subject to a hijack/spam attack which has caused untold hours of misery on the part of yours truly . After a stay overnight in the hospital for vision problems , I was able to , on return home , determine from where this attack originated . For the moment , memberships are frozen and subect to single-subject moderation . <p> It was with heavy heart that I had to resort to this and with a 21-year experience running the TIG that I had to resort to anessentially unfree course of action . Due to certain influences , a complete flow-stop has been originated for negative-flow transfers ( a scourge of today 's free-flow cash-transfer internet ) . <p> It 's also with heavy heart that , for the recuperation and hospital expenses incurred by an accident suffered by me ( originator , TIG , 1991 ) that these negative-flow transfers are depleting my funds and zeroing out my account balances . <p> The TIG Webiste , after the internet came to @ @ @ @ @ @ @ @ @ @ reels , extensive colorist information , the product of 2 decades of single-minded dedication , the donation of a tremendous amount of work by SoHoNet ( UK based ) with a 99% uptime ( significantly higher than existing informational websites for post-production ) and includes , in addition to an extremely professional interface , confirmed by a visit to http : **32;1039;TOOLONG -- among the highest-rated and calibrated colorist websites available . And kept spam-freefor all of those 21 years . 
@@44332787 @4332787/ <h> Batch DPX validation <p> I was wondering what tools , or even just general approaches , this group uses to validate DPX files . I have 2 fairly large institutional clients to whom I am delivering 2K DPX/WAV files over the next several months . <p> Given the quantity of files , and that they are intended to be part of a larger digital preservation effort , the clients , as I would do if I was the customer , have asked that we implement file verification and validation reporting into our workflow . <p> The verification piece is easy file-level reporting and not the subject of this particular inquiry . <p> For validation : we 've discussed MediaConch LONG ... which is an implementation checker , policy checker , reporter , and fixer that targets preservation-level audiovisual files on a batch basis using defined policies . <p> Ideally ? I 'm looking for something really simple , and " already built . " I 've done some looking around in my Venn spheres of motion picture archiving and digital video post , and have yet to see @ @ @ @ @ @ @ @ @ @ ideas among this group . <p> Thanks for any/all ideas , and rest-assured that I will " Like " all the replies here even if I do n't get around to clicking the actual button . I have unlimited likes to give , and should be presumed at all times . <p> Interesting , Jean Claude- thanks for passing this info along . At the very least , always nice to find a neat little DPX player . I will be playing around with it a bit and using your instructions for accessing the Information tools . <p> I tried it on both Windows and OS X : It had some interface errors when attempting to navigate the file menus in Windows , and the installer only works in Sierra in OS X. I am still on El Cap , so could n't open it up . 
@@44332788 @4332788/ <h> Slow render on ISIS/NEXIS <p> Apologies if this is a known issue -I did try and search but could n't find any info . <p> We have been getting slow render performance when rendering out individual clips from Resolve to either ISIS or Nexis shared storage . <p> There is a local SSD RAID , which is not-surprisingly faster to render to - but rendering locally creates a bit of media management headache as we multiple ops with varying degrees of media tidiness . Also backups are much easier to manage on the network storage . <p> I was hoping the problem might be sorted in Resolve V14 , but alas no , it persists . <p> Basically there is a brief pause of a second or two when what looks like closing the write of an individual clip . The same pause exists on the RAID , but it is much shorter ( maybe half a second ) . Over an hour timeline , particularity one with many cuts this time adds up . I 've tried inside and outside of the Avid MediaFiles network directory . @ @ @ @ @ @ @ @ @ @ ) and AVID storage ( r/w 400MB/s ) , and over a single clip render there is negligible difference at about 125fps at 1080i50 . It 's the individuals where it 's night and day . <p> Rendering from AVID , there is seemingly no performance penalty to the network shares . <p> Annoyingly , I do n't remember this being a noticeable problem when we used editshare on a 1Gb connection 
@@44332789 @4332789/ <h> Default Title Safe Boundaries <p> I 'm using Resolve more for Online Editing now . This includes doing titles . <p> With regards to the default title safe and action safe values , Resolve maps very differently from Premiere and Avid . Take a look at the image below . Note that Premiere and Avid are identical , I confirmed after making this graphic . <p> Premiere 's mapping for title safe <p> All though I feel like Resolve 's title safe is probably fine for most TVs at home , I 'd prefer to play it safe ( that is to say , title safe ) and stick with the wider value as seen in Premiere and Avid . <p> So how do I adjust the title safe value in Resolve manually ? <p> Using the Advanced Panel , I select MODES &gt; SAFE AREA and I get a number of preset aspect sizes . However none of the presets do want I would like . If I select USER ASPECT , I do n't now how to adjust the position of the mask . <p> Any @ @ @ @ @ @ @ @ @ @ assuming a 1920x1080 North American delivery for television broadcast . <p> I agree , Title and Action safe was really for when we were still back in the SD days when tubes came in all sorts of funky ways . Once HD came into play smpte basically applied the old rules to new technology . That article is correct . But when it comes down to it , it really does n't matter what we think but what the QC house views as correct area of safe . And that can change from house to house . <p> I agree , Title and Action safe was really for when we were still back in the SD days when tubes came in all sorts of funky ways . Once HD came into play smpte basically applied the old rules to new technology . That article is correct . But when it comes down to it , it really does n't matter what we think but what the QC house views as correct area of safe . And that can change from house to house . 43971 @qwx453971 <p> very much down @ @ @ @ @ @ @ @ @ @ thing works well i made 1 min movie out of it sent it to our QC to see if i lined up with there cage ... once it did that <p> i use it as an overlay on every thing i do and never touch the inbuilt generators in the edit system or resolve <p> purely for the reason Arthur stated this thread ... they don , t all match up 
@@44332790 @4332790/ <h> aces and resolve question <p> In the past I 've worked with Arri Alexa LogC in Resolve using an Arri/REC709 LUT in the middle of the grading stack . Working in Log Space before the LUT and in traditional LGG after the LUT . <p> For fun , I opened up a timeline in ACES yesterday . Basic color looked like a good start . So far , so good . <p> But all adjustments seem to occur before the LogC/REC709 transformation . It 's not bad , but I feel locked into the knee and toe curves built into the transform . I ca n't find any way to adjust the image AFTER the transform . <p> In the past I 've worked with Arri Alexa LogC in Resolve using an Arri/REC709 LUT in the middle of the grading stack . Working in Log Space before the LUT and in traditional LGG after the LUT . <p> For fun , I opened up a timeline in ACES yesterday . Basic color looked like a good start . So far , so good . <p> But all adjustments @ @ @ @ @ @ @ @ @ @ not bad , but I feel locked into the knee and toe curves built into the transform . I ca n't find any way to adjust the image AFTER the transform . <p> Is there a way to accomplish this using ACES ? 43971 @qwx453971 <p> No . That 's the way a proper ACES pipeline is implemented . ACES is all about working in scene referred color space . It 's not about log vs. video gamma encoding , and it 's not about " prior to a LUT " or " after a LUT . " Once you transform to a display referred color space ( Rec709 , for instance ) , you 're no longer scene referred , so you 're defeating a good part of the purpose and design of ACES . If you want to work in the way you described , you should not be using an ACES pipeline because it 's not designed that way . That said , there are some systems that will allow you to implement transforms to and from ACES color space as LUTs or transforms within the grading @ @ @ @ @ @ @ @ @ @ . Resolve is not . <p> No . That 's the way a proper ACES pipeline is implemented . ACES is all about working in scene referred color space . It 's not about log vs. video gamma encoding , and it 's not about " prior to a LUT " or " after a LUT . " Once you transform to a display referred color space ( Rec709 , for instance ) , you 're no longer scene referred , so you 're defeating a good part of the purpose and design of ACES . If you want to work in the way you described , you should not be using an ACES pipeline because it 's not designed that way . That said , there are some systems that will allow you to implement transforms to and from ACES color space as LUTs or transforms within the grading stack . Baselight is one that let 's you do that . Resolve is not . 43971 @qwx453971 <p> I 'm not sure I get this Mike . Photoshop has had a color management system for years without this " limitation @ @ @ @ @ @ @ @ @ @ Resolve ACES workflow , that one will be working on Log curve material ( or RAW converted to Log behind the scenes . ) It works well , until ... it does n't . <p> I think it 's an assumption of the Resolve ACES workflow , that one will be working on Log curve material ( or RAW converted to Log behind the scenes . ) It works well , until ... it doesn't. 43971 @qwx453971 <p> I do n't think that is correct . ACES has no tone map as it is linear ( but acesscct does have a toe to deal more ' naturally ' with the blacks ) . <p> I suppose if one insists on ' log ' handles for grading one could make a node to Color Space Transform to for instance Cineon then make adjustments and make a node to undo the Cineon transformation . <p> But the whole point of ACES , at least by my limited understanding , is to avoid ' LUT magic ' , gamut confusion , and other technical transformation complications for all cameras , have a standard @ @ @ @ @ @ @ @ @ @ and gamut to capture all ( and future ) cameras and to make it easy to tone map and gamut convert to ( future ) multiple display formats and technologies . <p> I do n't think that is correct . ACES has no tone map as it is linear ( but acesscct does have a toe to deal more ' naturally ' with the blacks ) . <p> I suppose if one insists on ' log ' handles for grading one could make a node to Color Space Transform to for instance Cineon then make adjustments and make a node to undo the Cineon transformation . <p> But the whole point of ACES , at least by my limited understanding , is to avoid ' LUT magic ' , gamut confusion , and other technical transformation complications for all cameras , have a standard scene referred space that has a large enough dynamic range and gamut to capture all ( and future ) cameras and to make it easy to tone map and gamut convert to ( future ) multiple display formats and technologies . 43971 @qwx453971 <p> Basically correct @ @ @ @ @ @ @ @ @ @ ACES , " where the values are presented as log representations of the full linear ACES values . The conversion is spelled out in the Academy ( and now SMPTE ) documents . This was done specifically for the reason you state , namely to make grading " feel " more familiar and predictable . Variations of log ACES were implemented by various vendors ( Filmlight , DaVinci ) and the ACES committee decided it was better to release an " official " version , which became ACEScc . ACEScct was an update to that to correct for the toe , as you mentioned . ACES Proxy is another variation that was designed to be carried over SDI video transports for on-set , live color work . <p> Referring to the previous post , Photoshop and ACES are very different things . The very heart of the ACES design was the notion of grading in scene referred space , not display referred . This allows for multiple deliverables for different targets , such as video , DCP , and others ( including , at least theoretically , HDR targets ) @ @ @ @ @ @ @ @ @ @ practice , re-grading is still routinely done , but it is at least theoretically less extensive on ACES originated projects . There is no notion of " log original " as you suggest . Each manufacturer is responsible for coming up with an input transform for their particular camera , which is not based on log output curves , but linear light . As an example , the transform from a log space such as Arri LogC/ArriWideGamut to linear light is part of the Arri IDT . The transforms to get to linear ACES from linear light are specified by the ACES documentation , but it does not rely on any specific input format . In fact , Red in particular has an SDK option to debayer to linear light values , which are converted to ACES equivalents directly . Getting ACES data out of a Red file does not normally involve a conversion to any log space . <p> Basically correct , except that ACEScc and ACEScct are essentially " log ACES , " where the values are presented as log representations of the full linear ACES values . @ @ @ @ @ @ @ @ @ @ now SMPTE ) documents . This was done specifically for the reason you state , namely to make grading " feel " more familiar and predictable . Variations of log ACES were implemented by various vendors ( Filmlight , DaVinci ) and the ACES committee decided it was better to release an " official " version , which became ACEScc . ACEScct was an update to that to correct for the toe , as you mentioned . ACES Proxy is another variation that was designed to be carried over SDI video transports for on-set , live color work . <p> Referring to the previous post , Photoshop and ACES are very different things . The very heart of the ACES design was the notion of grading in scene referred space , not display referred . This allows for multiple deliverables for different targets , such as video , DCP , and others ( including , at least theoretically , HDR targets ) by using different ODTs without needing to re-grade . In practice , re-grading is still routinely done , but it is at least theoretically less extensive on ACES @ @ @ @ @ @ @ @ @ @ original " as you suggest . Each manufacturer is responsible for coming up with an input transform for their particular camera , which is not based on log output curves , but linear light . As an example , the transform from a log space such as Arri LogC/ArriWideGamut to linear light is part of the Arri IDT . The transforms to get to linear ACES from linear light are specified by the ACES documentation , but it does not rely on any specific input format . In fact , Red in particular has an SDK option to debayer to linear light values , which are converted to ACES equivalents directly . Getting ACES data out of a Red file does not normally involve a conversion to any log space . 43971 @qwx453971 <p> I 'm not really sure photoshop is that different . In photoshop one works in a wide gamut " working space " ( ie. prophoto RGB ) , but views and grades on another , smaller space . The only difference that I see is that ACES requires viewing on a standardized device ( DCI P3 @ @ @ @ @ @ @ @ @ @ to convert to for grading . Other than that ( underlying math aside ) they are trying to accomplish the same things . 
@@44332791 @4332791/ <p> The other variable is screen size , and whether you have a separate projection booth . The Barco 's , Christies and NECs all require a separate , airconditioned booth - because the projectors get hot and the fans are noisy . But if you do n't need that amount of light many of the higher-end consumer projectors can provide great results . <p> I 'm still using an older JVC HD projector calibrated to Rec709 with a LUT box - on a 2.5meter wide screen . Works really well , and translation to the cinema is excellent . With any of these consumer projectors I 'd bypass the internal colour management and go for an external LUT box . A Sony VW-520 would also be a really good option . If HD native resolution is enough the Epson LS-10000 looks really good , but if I were to buy a new projector I 'd probably want it to be 4k . The JVC Z1 looks promising but is out of your proposed budget range . <p> I 'm planning to stick with my current projector until it @ @ @ @ @ @ @ @ @ @ . If I had to buy something now to keep me going until native 4k laser projection drops in price , it would probably be a JVC X7500 which is a newer version of my current projector which has been serving me really well . <p> I saw a mention on avs forums that the jvc pixel shift projector looked better than a true 4k Sony . I do n't have the model numbers but it made me take pause . These jvc units do seem like a good option for low cost projection . I will wait for the 4k DLP consumer units come out as I like the DLP punch to compare with the jvc before I replace my benq w6000 . <p> Hi , it looks like the X7500 is called the X770R in the USA ... It 's the middle of the three models . <p> I notice it says DCI compatible in the specs , but I would n't count on the projector covering P3 completely . It 'll probably get you 95 percent coverage , but that means you still risk some surprises on @ @ @ @ @ @ @ @ @ @ and go from there . And you would need an external LUT box to calibrate . <p> Just a word of caution on the e-shift technology with the JVCs : These projectors use 3 HD-sized chips and upscale to 4K . Some do have 4K inputs , but they still have HD chips inside and do an upscale . <p> Now I am not sure if this is still the case with the e-shift 4 technology ( vs. e-shift 3 before ) and if it is the case with all projectors ( especially as I#d like to think the top-of-the-line model for 35k has a true 4K chip ) , but double checking does n't hurt : <p> Yes the current JVC projectors are using e-shift to get to a 4k-ish picture ( the latest ones do have 4k input ) . Although reviews about image sharpness and detail are good I do n't consider them 4k . <p> The JVC Z1 laser projector is true native 4k - but with the 35k price tag . But the 4k chip with laser light source looks to be future . Within @ @ @ @ @ @ @ @ @ @ down to the lower priced product range - with Sony and Epson competing in the same area . <p> I was quick to discount the technique but I believe the pixel shift is actually displaying all of the 4k pixels , just using the shift to do so with smaller chips . As I mentioned the avs forum reports said the tech showed better than a true 4k unit . <p> The other variable is screen size , and whether you have a separate projection booth . The Barco 's , Christies and NECs all require a separate , airconditioned booth - because the projectors get hot and the fans are noisy . But if you do n't need that amount of light many of the higher-end consumer projectors can provide great results . <p> I 'm still using an older JVC HD projector calibrated to Rec709 with a LUT box - on a 2.5meter wide screen . Works really well , and translation to the cinema is excellent . With any of these consumer projectors I 'd bypass the internal colour management and go for an external LUT box @ @ @ @ @ @ @ @ @ @ option . If HD native resolution is enough the Epson LS-10000 looks really good , but if I were to buy a new projector I 'd probably want it to be 4k . The JVC Z1 looks promising but is out of your proposed budget range . <p> I 'm planning to stick with my current projector until it 's more clear where laser projectors and HDR are going . If I had to buy something now to keep me going until native 4k laser projection drops in price , it would probably be a JVC X7500 which is a newer version of my current projector which has been serving me really well . 43971 @qwx453971 <p> hi barend you have mentioned with a LUT BOXa and a home projector can do well for cinema conversion , can you kindly tell me what LUT BOX you are using and how does it work . will it suit a good HD projector ? <p> Hi , I 'm using an AJA LUT box , that converts SDI to HDMI and applies a LUT . The real catch is of course to @ @ @ @ @ @ @ @ @ @ . I use Lightspace to profile the projector and calibrate to Rec709 . Then when I create the DCP I apply a 709 to DCI P3 LUT. 
@@44332792 @4332792/ <h> Commercial production is looking for a colorist ( Moscow ) <p> Our company is searching for a colorist with knowledge of DaVinci for our new color grading department . The primary focus is TV commercials . We are one of the largest commercial TVC Production house of full cycle in Russia ( Top 10 ) and our office is located in Moscow . We provide accommodation , visa support and help in adaptation . Salary is discussed privately with the candidate upon the results of the interview . <p> If you are interested please send your CV with showreel to the following email **28;1073;TOOLONG 
@@44332793 @4332793/ <h> Baselight quick compositing questions <p> I 'm currently in the process of test-driving a Baselight 2 system to see if this might add to our current workflow ( Davinci/Scratch ) but there are some compositing issues that we ca n't seem to figure out . We 've had contact with the very nice staff of the Filmlight Technical Department but they also seem to be a bit struggling with the compositing that we would like to see done in Baselight . <p> Ofcourse , the Baselight 5.0 system will have a huge improvement in the compositing area but that is not what I 'm working on right now or expecting to be working on in the near future if we decide to add Baselight in our workflow . <p> I 'm very curious to see if any of you have a better ( mostly faster/easier ) solution to some of these . We are running Baselight 4.4m1 <p> - Simple cloning : <p> As of now I have brought it down to adding in a second input to a layer , giving it a shape/key at where we @ @ @ @ @ @ @ @ @ @ Then in the second input move the layer where the clone is coming from to whatever is needed to match up within the shape while looking through the shape . <p> Most compositing software does this the other way around , doubling a layer , shaping/keying wherever you want to clone from and moving it on top the layer that needs the fix . Is there any chance of doing this or do we just have to get used to doing it the other way around ? <p> - Sky replacements : <p> It happens time to time that there is a sky that needs to be replaced within a moving shot . When I select the replace BG from shape/key I just ca n't seem to get the second input image on any other blend mode then mix , though I would really love to add/overlay/hard light blend this layer in .. <p> And another thing , when we have a moving shot and I am trying to have the sky link back to a tracker , whenever I go into the transform option of the second input ( @ @ @ @ @ @ @ @ @ @ is grayed out .. Does this mean I have to insert the sky into a new layer first , have that use the tracker data and then have that tracked sky layer blend trough a key/shape with the layer previous to the sky insertion ? Seem like a long way around but it could be that I 'm missing a few simple tricks here .. <p> I 'd really love to hear your view on this , perhaps I 'm just blindly starting at the wrong settings and missing some simple step but it just does n't seem to work the way I ' d like it to work .. <p> Cloning is done with the Transform . Sky replacement is easily done with the Matte from FG Key . Once you select it , it will automatically guide you for the second input . You should look at the video at around LONG ... You can track your foreground and paste and invert that tracking data to the background for the sky . Baselight , unlike Lustre or Nucoda does n't allow for linking to the tracked data . @ @ @ @ @ @ @ @ @ @ 's a bit counterintuitive , but that 's how they do it . Good luck . <p> It happens time to time that there is a sky that needs to be replaced within a moving shot . When I select the replace BG from shape/key I just ca n't seem to get the second input image on any other blend mode then mix , though I would really love to add/overlay/hard light blend this layer in .. <p> And another thing , when we have a moving shot and I am trying to have the sky link back to a tracker , whenever I go into the transform option of the second input ( sky ) the option to have it follow a tracker is grayed out .. Does this mean I have to insert the sky into a new layer first , have that use the tracker data and then have that tracked sky layer blend trough a key/shape with the layer previous to the sky insertion ? Seem like a long way around but it could be that I 'm missing a few simple tricks here .. <p> @ @ @ @ @ @ @ @ @ @ , perhaps I 'm just blindly starting at the wrong settings and missing some simple step but it just does n't seem to work the way I ' d like it to work .. <p> Thanks in advance ! /quote <p> Hello Michiel , <p> there is another way to replace a sky , where you are able to use different blend modes . -create a layer -choose " second input " as a source -go to your stack and click on blend source -then press " sequence strip " on the gui , a window opens where you can choose the sky , press change -then use your Hue Ang oder DKey with shape to isolate the area where you want to place the sky -then you have to choose " use matte " for blend and put the result blending to the right . -you can now choose a different blending mode -if you want to move the sky , put a transform strip under the " sky strip " -same , if you want to grade the sky , put , for example a videograde strip under @ @ @ @ @ @ @ @ @ @ , but you have to move the tracker strip manual under the original source strip -in the transform strip " stabilize invert have to be activated . <p> -create a layer -choose " second input " as a source -go to your stack and click on blend source -then press " sequence strip " on the gui , a window opens where you can choose the sky , press change -then use your Hue Ang oder DKey with shape to isolate the area where you want to place the sky -track ( for example ) the shape to isolate the area where you put your sky -select tracker in the stack , press " copy " on the blackboard ( upper right ) and select the transform strip ( mention later ) and press " paste " ( upper right ) <p> -or select tracker , Press " copy Name to Clipboard , got to transform strip , go with the cursor to the empty field " Current Track " and press control v on the keyboard <p> -then you have to choose " use matte " for blend and @ @ @ @ @ @ @ @ @ @ now choose a different blending mode -if you want to move the sky , put a transform strip under the " sky strip " -same , if you want to grade the sky , put , for example a videograde strip under the sky strip -tracking is possible with the transform strip , but you have to move the tracker strip manual under the original source strip -in the transform strip " stabilize invert have to be activated . <p> you can change the order of a few of this hints , but i guess you get the idea . <p> Yes , if you insert stacks manually , there some more possibilities , but it is not easy to get behind all the secret of baselight ; - ) 
@@44332794 @4332794/ <h> Sony Launches X-OCN Raw Codec To Compete With RED RAW <p> Much like RED RAW 's R3D files , Sony 's new X-OCN Raw codec provides 16-bit image quality at about about a 30% smaller file size than the current Raw Codec being deployed on the Sony F55 , and half the size of Apple ProRes 4444 . It deploys a sliding scale algorithm that ups the encoding values with the raise in frame rate , so if you 're shooting in standard cinematic 24p , X-OCN will record at 661 mbps ( 389 Mbps for X-OCN LT ) and all the way up to 3303 Mbps for shooting at a high speed frame rate of 120 frames per second . 43971 @qwx453971 <p> Other Specs include : <p> 16-bit encoding <p> Similar to Sony 's F55 Raw format <p> ST and LT variants <p> ISO and Kelvin Balance post-processing <p> Up to 142% shoot time , 59% transfer time <p> MXF file format 43971 @qwx453971 <p> But the real benefits of X-OCN are apparent when you consider that Sony 's other 4K format , XAVC , shoots @ @ @ @ @ @ @ @ @ @ . Shooting in the LT version of X-OCN , you can get 50% better color and only user up 5Mbps more in the encoding rate . So the file sizes are quite manageable . <p> But it is another proprietary codec by Sony and it will likely only be available through the CineAlta platform . First on the list will be the the new AXS-R7 Video Recorder Designed for use with the F5 and F55 Cameras , the AXS-R7 will provide dual slot recording from Sony 's ACS cards . According to Cinema5D , the R7 has been delayed a bit , but should hit the market this Fall . <p> I believe OCN stands for Overly Complicated Nonsense . Sony should probably stop adding these new format and focus on improving user experience , for example the menu system . 43971 @qwx453971 <p> Yeah , there 's something to be said for this . <p> I would bet this is connected from their IP lawsuit settlement with Red . This is basically a version of RedCode/RedRaw , only for Sony cameras : compressed files containing a Raw signal . @ @ @ @ @ @ @ @ @ @ on the back of the camera ; Red does it in the camera , which is a subtle distinction . <p> In Sony 's defense , I like the look of the F55 and have done a couple of S-Log2 projects that I thought looked really , really good . No LUTs , no fancy stuff , just the straight color science and turning knobs . Client loved the look . <p> Obviously only lawyers , that were part of the original settlement in Red vs Sony lawsuit would know who owns what intellectual property . After Red and Sony sued each other , they settled and exchanged some very valuable IP , which very well could be the basis of the new codec from Sony . As a layman , I see one tiny but important distinction . Red original claimed , that they owned RAW compressed recording at speeds over 23fps and over 2k in resolution . That 's why some companies could still get away with doing RAW compressed recording , as long as they recorded under 2k . The moment Sony introduced F65 , Red pounced @ @ @ @ @ @ @ @ @ @ , long story short , the important distinction is a separate recorder . In Red patent description ( https : **34;1103;TOOLONG ) they describe a camera with an onboard recorder . So , it seems , Sony got a permission to do RAW compressed recording , as long as the recorder is a add-on . Or so it seems to me ... <p> We used the F55 on a Netflix series a few months ago . I really liked the color of the S.Log3/S.Gamut3.Cine setting . The recorder on the other hand ... <p> This recorder addresses some of the problems : more robust attachment to the camera body ( current recorder 's data pins can lose connection if the camera is hand-held or on a sandbag , causing the recording to stop mid-take ) , and cards now load from the side ( like most other cameras ) instead of the top . <p> Sony should probably stop adding these new format and focus on improving user experience , for example the menu system . 43971 @qwx453971 <p> That made me chuckle <p> I did a Fashion thing last @ @ @ @ @ @ @ @ @ @ the producers knew my background and needed an extra operator so they asked if i would do it .... i was also going to be doing the grade any way .. so i end up driving " C " camera for the day so i , m basically shoot things the editor could cut away too as the girls&amp;boys hurtle down the runway <p> it was all very last minute and it 's a ......... camera right .... lens , body , battery ..... and they all do the same thing .. first time i picked up a RED i was happy to shoot pretty much straight away ..... it all makes sense <p> When i turned up in the morning of the shoot grabbed the camera fired it up , then saw the menu system it was like a ...... " WT* " moment and it was OCN <p> after me cursing under my breath for about 10 minutes , having to drill down thru stupid menus to do the most basic thing .... one of the other camera guys came over and help me out . They had @ @ @ @ @ @ @ @ @ @ me get more and more frustrated <p> He just said " Its a nightmare is n't it " ..... and then sorted me out <p> on the flip of this , when i finally got the footage to Grade in resolve , i was just not happy with the colour i was getting from the luts.So it was the thing that pushed me in to using ACE 's where i got much more pleasing results quicker and ever since then any thing sony i do in ACE , s <p> i know people get wonderful stuff out of those camera F5/55 , FS7 etc but sony just do n't do themselves any favours with all the complexity they add on top <p> The moment Sony introduced F65 , Red pounced and got some valuable IP stuff in return . So , long story short , the important distinction is a separate recorder . In Red patent description ( https : **34;1139;TOOLONG ) they describe a camera with an onboard recorder . So , it seems , Sony got a permission to do RAW compressed recording , as long as the @ @ @ @ @ @ @ @ @ @ n't know exactly what the agreement was , since there were non-disclosures involved . They did say no money changed hands , and that was in the trades . 
@@44332795 @4332795/ <h> Assistant Colorist Rate <p> I have an opportunity to work as a freelance assistant/junior colorist and have been asked to give them my hourly rate . Resposibilities would include dailies , prepping jobs , and occasional grading of lower budget jobs . I have 4 years experience using Lustre , Resolve , and Scratch . What do you all think would be a fair rate for NYC ? <p> I think there 's not a huge difference anymore between the duties as an assistant colorist vs. an assistant editor , at least in terms of backing up files , organizing bins , conforming masters , prepping sessions , dealing with deliverables , and doing final renders . The IA 700 union rate for assistant editors was about $45/hour last time I checked , but that also hinges on number of hours and benefits . Assuming this is non-union , it 's whatever you can negotiate . <p> Thanks for the advice and links guys . Very much appreciated . I was thinking $50/hr so I guess I 'm in the ballpark. 43971 @qwx453971 <p> That 's a safe @ @ @ @ @ @ @ @ @ @ sure , see if you can find out rates for other positions , like editor , assistant editor , compositor/ vfx , and include those as another way to gauge the sort of scale a company uses to pay people . Some places pay better , others worse , and that will usually be consistent throughout the various positions at a given company . <p> 50/hr seems right for freelance basis . ( Coming in occasionally ) But 50/hr full time is 96k a year which would put you out of the price range . 43971 @qwx453971 <p> Nobody works every hour of every day of every week , not in a freelance world . Staff rates are much different than freelance rates , from my perspective . If they 're paying half your social security and unemployment and paying health care and other benefits , that 's a whole different deal . <p> Nobody works every hour of every day of every week , not in a freelance world . Staff rates are much different than freelance rates , from my perspective . If they 're paying half your @ @ @ @ @ @ @ @ @ @ benefits , that 's a whole different deal . 43971 @qwx453971 <p> And in your experience , if those things are included . What would be a reasonable rate for staff ? <p> It also depends per country how high your freelance rate could be . When I was freelance I kept a minimum of 350 euro 's a day . With extra after 10 hours . This way you protect yourself and the business . <p> And in your experience , if those things are included . What would be a reasonable rate for staff ? 43971 @qwx453971 <p> I think it depends on the city as well as the country . You might be able to survive on a much lower rate in ( say ) Ohio than you would in downtown Manhattan or in West LA , or London or San Francisco . So I 'd say there 's cost-of-living to consider as well . <p> The assistants I worked with at Technicolor were making about $1000-$1200 a week when I was there a few years ago , but it was considered an entry-level position with only @ @ @ @ @ @ @ @ @ @ cleaning film , preparing conforms , rendering final outputs , and prepping deliverables . There were also mid-level people were not quite making a top colorist salary but were functioning as " additional colorists , " working alongside the lead colorist in making changes , evening-out sequences , doing complex tracking windows , and handling editorial changes . This is in the world of features ; in TV and commercials , often one person is doing most of this work , though there are assistants that handle the file ingest and prep . <p> Thanks for the reply , because at the moment I 'm at a position of Colorist , i 'm responsible for what ever comes out graded . And after seeing the rates here and there ( also of assistants ) my staff rate seems a bit low at 2200 ( after taxes etc . ) . I think it is not a good idea to ask or to know what a colleague makes in the same company . This way you ca n't judge each other on how hard they work against how much they earn @ @ @ @ @ @ @ @ @ @ data for staff employees . So if someone would dare to share their rate ( approximately ) would be much appreciated . <p> Thanks for the reply , because at the moment I 'm at a position of Colorist , i 'm responsible for what ever comes out graded . And after seeing the rates here and there ( also of assistants ) my staff rate seems a bit low at 2200 ( after taxes etc . ) . I think it is not a good idea to ask or to know what a colleague makes in the same company . This way you ca n't judge each other on how hard they work against how much they earn . And also it 's hard to find some substantial data for staff employees . So if someone would dare to share their rate ( approximately ) would be much appreciated . 43971 @qwx453971 <p> is that monthly or weekly ? 8k euro a month is fair .. the rate often comes from how much that colorist has leverage to pull in clients . If its the house getting all the @ @ @ @ @ @ @ @ @ @ the colorist , the rate is much lower . <p> is that monthly or weekly ? 8k euro a month is fair .. the rate often comes from how much that colorist has leverage to pull in clients . If its the house getting all the work ( reality tv shops for example ) and not the colorist , the rate is much lower . 43971 @qwx453971 <p> That 's a monthly rate i 'm talking about , that results in in 26000 netto a year . I 'm at a big production company , with only in house TVC 's . So there was n't even the opportunity to take my old clients . Conclusion I can draw from this : .. I definitely need to aim higher haha . Thanks . 
@@44332796 @4332796/ <h> FCPX XML Issues into Resolve <p> We 're looking at a 30min short film , shot on mixed camera types . The majority of the film was shot on Black Magic RAW 2.5k . This film was edited off of proxies that the editor made from Resolve off the DNG files . <p> He edited in FCPX , he did audio syncing in FCPX , he said he made sure that he did not use any compound clips while he edited . ( I 'm not a FCPX user but does audio syncing make compound clips ? ) <p> He has given me the XML and all the drives with the raw files and proxy files . I can not , for the life of me , get this project to load up into Resolve with the native RAW files . <p> I 've tried the following options to no success : <p> -Loading the XML and telling it to pull form the raw file folders ( it get the RED shots fine ) . -Pre-loading all the raw clips into the media pool ( 8+TB worth ) @ @ @ @ @ @ @ @ @ @ " -Loading the XML with the proxies ( which is does fine ) , removing all the clips in the bin , choosing the timeline and importing " additional clips with loose file name match ... " from the sources of the raw media . -Smacking the computer screen . Banging head on desk . Repeat . <p> Every time I load just the proxies , it creates more than half the film as compound clips ( I assume this is related to the audio syncing ) . My guess is this has something to do causing problems with linking them to the raw clips ? <p> Anyone have any other thoughts or suggestions on this ? This film was more than 15tb of data and it would be a shame to have to end up coloring the proxies when they went through all the trouble of shooting raw . A real shame . <p> I do n't know about how FCP X does it , but in FCP 7 , if you were to sync by " merging " your clips , it created a new subclip type deal @ @ @ @ @ @ @ @ @ @ a similar thing . So while the editor may not have created compound clip while editing , it sounds like that is what FCP X is doing during the sync process . Is he unable to create a timeline that has compound clips flattened/removed ... whatever the proper wording would be ? I mean , he should try it just so he can tell you with a straight face that he in fact did n't use ANY compound clips ... <p> I do n't know about how FCP X does it , but in FCP 7 , if you were to sync by " merging " your clips , it created a new subclip type deal that defaulted to the audio TC ... Avid will do a similar thing . So while the editor may not have created compound clip while editing , it sounds like that is what FCP X is doing during the sync process . Is he unable to create a timeline that has compound clips flattened/removed ... whatever the proper wording would be ? I mean , he should try it just so he can tell @ @ @ @ @ @ @ @ @ @ n't use ANY compound clips ... 43971 @qwx453971 <p> He says that the clips are not compound clips . At least that is what he tells me . <p> Do the timecodes match the proxies ? Were the proxies transcoded to all start at 00 ; 00 ; 00 ; 00 perhaps ? 43971 @qwx453971 <p> Looking in my folders the clip names are the same , their start and end timecodes are the same . That is when I manually pull them into the media pool . <p> The compound clips look to have been renamed to the names of the audio files and their timecodes , when looking at the clip info , start from 00:00:00:00 . Is there a way to break down compound clips in Resolve ? <p> If everything else fails , export the whole movie as a single file Prores 4444XQ ( both FCPX and Resolve support it ) or Prores 4444 and export EDL . Use EDL to re-conform using the exported QT . It 's not ideal , and I would n't recommend it most of the time , but taking into @ @ @ @ @ @ @ @ @ @ minimal . You mentioned , that Red worked , so use those shots with R3D , but use QT in place of BM material . This way you can start doing what you 're hired to do- grading . Good luck . <p> If everything else fails , export the whole movie as a single file Prores 4444XQ ( both FCPX and Resolve support it ) or Prores 4444 and export EDL . Use EDL to re-conform using the exported QT . It 's not ideal , and I would n't recommend it most of the time , but taking into an account camera used-BM , the quality loss will be minimal . You mentioned , that Red worked , so use those shots , but use QT in place of BM material . This way you can start doing what you 're hired to do- grading . Good luck . 43971 @qwx453971 <p> I agree , I 'd like to get working ! Problem with that method is the proxies they edited with work , for the most part , fine in the XML but they are low res @ @ @ @ @ @ @ @ @ @ exports a ProRes 4444XQ it will be based off of the low res proxies . <p> I am assuming you have the original drive with the FCPX project on it ? <p> Hopefully , the editor followed this protocol : <p> 1 ) Load footage into Resolve2 ) Set to remote grades in the timeline ( if indicated ) - optional really3 ) Render proxies and export XML ( use the FCPX Roundtrip Preset ) 4 ) Save your project5 ) Load the XML into your NLE ( FCPX ) 6 ) EditTHIS IS WHERE YOU ARE STUCK : The editor may have to go back to his/her Resolve project and follow this process and then move everything over to your machine7 ) Export XML of FCPX edit8 ) Open original project in Resolve9 ) Import the XML but uncheck the " Automatically import clips into media pool " - Resolve will link to the original DNGs10 ) Grade11 ) XML if you need to return to FCPX12 ) Import new XML13 ) Final conform in FCPX <p> Then the more work they do themselves , the less it will @ @ @ @ @ @ @ @ @ @ <p> I agree , keeping audio completely out of the exported EDL/XML should help . What if you tried to use an EDL and not an XML , assuming it 's just basic cuts and dissolves ? 43971 @qwx453971 <p> Since what I 'm looking to do is link to the RAW files I do n't believe an EDL will help me with that . : -/ Getting the edit information into the project is n't the problem it 's getting the RAW clips in , and figuring out why there are compound clips that the editor assures me do n't exist , that is the issue . : - ( Or am I incorrect and an EDL will carry over the clip info ? <p> I had something similar few weeks ago . Was working with RED RAW but every time a speed effect was used in FCPX a compound clip would appear in Resolve , without having access to the raw settings anymore , unless you choose ' open in timeline ' when right-clicking the compound clip in the timeline . The editor never used any compound clips @ @ @ @ @ @ @ @ @ @ certain things from FCPX into its own compound clips . <p> Here is the thread I started then . you might find something useful in all the responses . <p> This is why it is a good idea to never do speed changes in an NLE when round-tripping . I always do speed conforms &gt; export &gt; re-import ( for me in a ProRes flavor ) &gt; edit &gt; XML . All editors should receive education about this if they want to have their projects professionally graded in Resolve . <p> I just did 12 spots , which were edited in Avid . All had many reframes and speed ramps . They all came in and went back to Avid with new AAF without a problem . We also rendered the timelines with all speed ramps baked without a hitch as well . <p> I have worked with Premiere CC with reframes and speed changes and things came into Resolve fine except for one reframe that would not come in for some reason . FCPX , however , has some problems but it looks like Pepijn found a workaround . @ @ @ @ @ @ @ @ @ @ I have not done a roundtrip in FCPX since the newest Resolve update . <p> I had something similar few weeks ago . Was working with RED RAW but every time a speed effect was used in FCPX a compound clip would appear in Resolve , without having access to the raw settings anymore , unless you choose ' open in timeline ' when right-clicking the compound clip in the timeline . The editor never used any compound clips in FCPX but for some reason Resolve tends to translate certain things from FCPX into its own compound clips . <p> Here is the thread I started then . you might find something useful in all the responses . <p> Good luck 43971 @qwx453971 <p> This will be helpful when it is a shot or two with issues . Certainly wo n't be going through almost every shot of a 30min film to break down each compound clip ; - ) . <p> I just did 12 spots , which were edited in Avid . All had many reframes and speed ramps . They all came in and went back to @ @ @ @ @ @ @ @ @ @ rendered the timelines with all speed ramps baked without a hitch as well . 43971 @qwx453971 <p> Just curious , were these spots shots in a RAW format ? I feel like some days it 's a hit or miss on whether re-frames and speed ramps work or not . 
@@44332797 @4332797/ <h> NR + Dead Pixels Workflow on DaVinci <p> I 've always been reading , not much of a writer in this forum , but now I need to open this thread to see if your experience could help us to get a better workflow to get around this matter . <p> Where I work , we 're doing this Cable Network show , shot in F55 and F65 . They shoot a lot in high framerate ( 120fps ) and that generates both underexposure and dead pixels . <p> We just finished the 10 episodes season , and the workflow was .. after the Color Correction was closed , we would NR each individual shot , put them back in Davinci , render the ones to be dead pixel cleaned in AE and round trip to davinci .. A bit crazy , and it would n't be an issue if we had n't had the last chapter . There we had to NR the 85% of it . And had to eliminate pixels in almost half of the duration of the chapter . <p> Due to the amount @ @ @ @ @ @ @ @ @ @ then clean every one of them separately as well ) it becomes crazy to handle . Also , as I said , we are roundtripping from resolve to AE and it really can become something painful . <p> So what do you guys think ? Is there a better way to deal with this kind of situations ? <p> Roundtrips to AE for fixes is really common . Its time consuming , but tough to improve on unless you 're ready to change a few things . If you have a strong Resolve ( xeons + cubix ) you can muscle your way through a lot of that with plugins ( Sapphire SGrain Remove , BCC Pixel Fixer , BCC Degrain , Tiffen DFX Clone , Neat Video ) and cloned shapes . That 's the easiest way to go if that difficult episode was outside the norm for you guys . <p> If you expect this to happen a lot more , a system like Nucoda with a few DVO tools would allow you to take care of complex fixes as you grade , without much additional effort . @ @ @ @ @ @ @ @ @ @ what you described , and would give you better results than you can achieve in AE , all without roundtripping . <p> Moving to Nucoda would mean moving to Windows for grading , but since it includes native ProRes and DNxHD export it 's usually a non-issue . Nucoda also has spectacular roundtripping with Avid if you 're doing your offline in Media Composer . <p> Since dead pixels do n't move , I use a very simple trick . I place a circle window over the dead pixel , making it just slightly larger than the pixel itself , and add a little bit of softness . I then blur the this circle . The result is that it blends the adjacent pixels , effectively hiding the dead pixel . This is a very quick solution and works 90% of the time . <p> Since dead pixels do n't move , I use a very simple trick . I place a circle window over the dead pixel , making it just slightly larger than the pixel itself , and add a little bit of softness . I then blur @ @ @ @ @ @ @ @ @ @ the adjacent pixels , effectively hiding the dead pixel . This is a very quick solution and works 90% of the time . 43971 @qwx453971 <p> That 's a very clever solution , Paulo . I have an old pal who had to remove all the dead pixels from Superman Returns some years ago , and I seem to recall it took him 6 months and cost the production about $500,000 . It 's a lot faster and cheaper now . <p> Since dead pixels do n't move , I use a very simple trick . I place a circle window over the dead pixel , making it just slightly larger than the pixel itself , and add a little bit of softness . I then blur the this circle . The result is that it blends the adjacent pixels , effectively hiding the dead pixel . This is a very quick solution and works 90% of the time . 43971 @qwx453971 <p> I use almost same trick , but blur is only half of the deal . I also add grain removed by blur using the same mask in @ @ @ @ @ @ @ @ @ @ for the response . I was skeptical about the whole posting thing . <p> In the first place I meant dead pixels just like some of you described .. static points that appear due to a heavy usage of the camera on set . Very usual among sony cameras .. <p> And .. I tried that thing of doing it inside the Davinci project but it really was n't working , mostly due to the amount of pixels that there were .. In some shots we cleaned hundreds of them . So .. I thought a that time that it was very time consuming and less effective than CC wire removal from AE , but .. I 'll try your solution . I do n't know if DaVinci will let us put 100 windows but , I 'll give it a try and let you know . <p> Moving to windows and nucoda is not an option for us , because the colorist is very comfortable in DaVinci Resolve and the whole station is built with this in mind . <p> But I 'll definitely check out those plugins you @ @ @ @ @ @ @ @ @ @ If you know any other or you have new methods aside from the ones discussed here , please , do write . <p> Since dead pixels do n't move , I use a very simple trick . I place a circle window over the dead pixel , making it just slightly larger than the pixel itself , and add a little bit of softness . I then blur the this circle . The result is that it blends the adjacent pixels , effectively hiding the dead pixel . This is a very quick solution and works 90% of the time . 43971 @qwx453971 <p> the blur in a mask will likely fail QC 99% of the time at least at a 1st tier QC house like **27;1175;TOOLONG , it may pass elsewhere tho . <p> If you are stuck in Resolve , and have to make a 100% QC pass , then round tripping is likely your best option .. it seems likely that sooner or later Fusion 's paint system will be avb from Resolve 's timeline , likely a FusionLink as a node much as Avid and Lightworks @ @ @ @ @ @ @ @ @ @ options inside Resolve are somewhat limited . <p> The restoration tools in Nucoda are up to the task as is the de-speckle tool in DS , both inside masks - i have a had to re-fix a failed Resolve " blur in a mask " , after the Resolve artist had a second shot at passing QC - this on maybe 50 shots , fortunately it only took a few min , but the difference is apparent esp when the dead pixel crosses an actor 's eye 's I can see why the " blur inside a mask " trick was a fail , i had never tired it before , always using DS 's paint tools so when i saw what was done in Resolve , even after the first attempt was failed by QC , i can understand why it failed <p> And Marc , DS 's paint system had de-speckle and resolution independence in the late 90 's , i was doing a ton of dirt fixes on film scans back then on DS maybe the months of work and 500k was earlier than that ? or @ @ @ @ @ @ @ @ @ @ n't use them ? <p> But I 'll definitely check out those plugins you talked about . Are those OFX ready for DaVinci ? 43971 @qwx453971 <p> Everything mentioned is available in OFX . Sapphire and Boris are good to go in Resolve , but I would verify for Tiffen . They are expensive , but if you can get them before the holiday sales are up , you can save some money . <p> Everything mentioned is available in OFX . Sapphire and Boris are good to go in Resolve , but I would verify for Tiffen . They are expensive , but if you can get them before the holiday sales are up , you can save some money . <p> there is a much easier way to get rid of hot pixels than the power window blur method . I 've done this in After Effects but it should also be possible in DaVinci Resolve . This solution is good if you have hundreds of hot-pixels . <p> Create a Layer Mask in Photoshop containing all the visible hot pixels . Use the pencil tool with hardness @ @ @ @ @ @ @ @ @ @ point of the hot pixel . Save this pixel map as TIF and use it as a Luma or Alpha mask . Duplicate your footage on the timeline and move the upper footage by 1 pixel to the right or left . Use the mask as Luma or Alpha mask between those clips . <p> I do n't know if that makes sense for you . I 'm not good at explaining . It 's pretty simple method you just use the adjacent pixels from the same clip but shifted by 1 pixel . The same procedure is done when a sensor is pixel mapped but here we do it in software . <p> You can use your pixel map on all your clips over and over if they 're stuck always on the same position . <p> Juan , we are looking to purchase two F55 's to replace our F3 's . I have never run into this issue with the F3 's , but I definitely want to be prepared for any hiccups like this that we might find in the F55 's . Could you possibly post @ @ @ @ @ @ @ @ @ @ Thanks . 
@@44332798 @4332798/ <p> Local Hero Post is a filmmaker-centric boutique post house that specializes in Digital Intermediate for feature films . We specialize in getting the most professional quality image out of today 's digital cinema cameras using Raw codec formats with end-to-end color management . Our services cover everything from on-set dailies and editorial systems , through conform , color , visual effects , mastering and deliverables . <p> The Digital Intermediate Operator assists the Digital Intermediate process through every step of the process , from conforming camera original media to the final edit , preparing color timelines , rendering versions and mastering final picture . The position supports the Senior Colorist , Technical Supervisor and DI Producer in the needs of the projects at the facility . <p> Responsibilities : Perform basic and advanced conform functions ( online editing ) , matching the camera raw footage to the edited program . Set basic color metadata settings for optimal use of camera Raw formats . Color grade , rename , sync sound and render conversions of dailies material . Work with compositing tools such as After Effects to re-time shots @ @ @ @ @ @ @ @ @ @ required for film , video versions , web , and digital cinema . Create preview Quicktimes , DVDs and Bluray discs as needed . Work with the mastering artist to complete and QC tape layoff masters and DCP screeners . Ensure proper setup of the equipment and projectors in the DI suite . May require client contact/interaction as needed . Communicate with the DI Producer and Technical Supervisor the status of the work as well as clearly define any issues that may arise . Any other duties as required . <p> Requirements : 2+ years experience in color correction with high-end software . Assimilate Scratch preferred , but can also train if you have experience with Davinci Resolve , Baselight , Pablo , Mistika or others . 5+ years experience with the common editing software platforms : Avid , Final Cut , Adobe Creative Cloud Basic experience with re-timing and manipulating shots in After Effects or other compositing tools ( Smoke , Nuke ) Strong interpersonal skills ; ability to interface with clients regularly and maintain excellent working relationships . Must be able to work independently with little supervision . @ @ @ @ @ @ @ @ @ @ background , able to troubleshoot system and connectivity glitches . Basic understanding of color correction , with a good eye for a quality image . Knowledge of film and color science . Must have thorough knowledge of various digital imaging file and data formats . Knowledge of the Assimilate Scratch software platform Must be able to maintain flexible schedule to meet production requirements ; ability to work day or night shifts as needed . Ability to see detailed imaging issues or technical problems . Strong communication and leadership skills . Creative problem solver , able to troubleshoot a software behavior and find workarounds . Resourceful and flexible with sound judgment and integrity . Must be a team player who can work with the needs of other operators at the facility . Demonstrate initiative and willingness to learn . <p> Position is for six weeks with potential extensions or full time hire . Salary will be commensurate with experience . Please no phone calls for job inquiry . Email a resume and cover letter to jobs@localheropost.com 
@@44332799 @4332799/ <h> IT FOLLOWS look <p> Hey , I just watched this really fantastic and creepy movie last night at the Melbourne International Film Festival . I am somewhat of a horror buff and it really impressed me . <p> Anyway , the look of the film is amazing . It 's got a very realistic , yes stylistic look , with a very neutral curve . It evoked the time period it 's set in ( including looks of movies of the period ) but also felt very modern . <p> If anyone knows anything about the project I would love to hear about it . The director 's previous film , The Myth of the American Sleepover , was shot on RED . My best guess is this was shot on Alexa or film . The Colorist is Mark T. Osborne <p> " Glad you enjoyed the film ! We shot mostly on Alexa in 2k Pro Res with Cooke S4s , and an Angenioux 11:1 . A couple scenes were shot on an Epic to take advantage of its smaller build . The director , David R. @ @ @ @ @ @ @ @ @ @ said , modern but hints to films from the 50s - 80s . I think/hope this was achieved through lighting , production design and grade . The modern aspect to the look probably comes from the nature of shooting on a digital camera itself . On the design end , there was a large effort by Michael Perry ( designer ) to amass as many props / design elements from different eras with hopes to craft a look that was n't specific to any one time period . This really helped in creating a unique look in which the world of the film exists . <p> Through lighting as well , I tried as much as possible to strike a balance between naturalism and some theatrical elements . Gregory Crewdson , the still photographer , was a large reference and love of both David and mine . All of his work seems to have an amazingly timeless quality to it . <p> Mark did a fantastic job with the grade . We tried to as much as possible to pay attention to skin tones , keeping them warm with a @ @ @ @ @ @ @ @ @ @ to the technicolor 3 strip . " <p> Kevin , thank you mate ! That was quite awesome of you to do . Funnily enough I was actually at another screening of the movie when this came through ! If you get a chance please thank Mark for his detailed response . <p> The Gregory Crewdson stuff definitely comes through , with the car in the abandoned parking lot , the still , composed frames with the cast scattered around . The light sources that are at once enveloping yet distant and motivated . The film definitely had a terrific sense of reality to it for this reason . I loved the deep photography in the daylight scenes - so refreshing to see photography that mirrors our own vision in such circumstances - even in the night scenes . The school hall way bit with the stumbling old woman was very Shining-esque in the best possible way . I could keep rambling on . Fantastic . <p> " Glad you enjoyed the film ! We shot mostly on Alexa in 2k Pro Res with Cooke S4s , and an Angenioux @ @ @ @ @ @ @ @ @ @ to take advantage of its smaller build . The director , David R. Mitchell , wanted a look that was exactly what you said , modern but hints to films from the 50s - 80s . I think/hope this was achieved through lighting , production design and grade . The modern aspect to the look probably comes from the nature of shooting on a digital camera itself . On the design end , there was a large effort by Michael Perry ( designer ) to amass as many props / design elements from different eras with hopes to craft a look that was n't specific to any one time period . This really helped in creating a unique look in which the world of the film exists . <p> Through lighting as well , I tried as much as possible to strike a balance between naturalism and some theatrical elements . Gregory Crewdson , the still photographer , was a large reference and love of both David and mine . All of his work seems to have an amazingly timeless quality to it . <p> Mark did a fantastic job @ @ @ @ @ @ @ @ @ @ possible to pay attention to skin tones , keeping them warm with a slight drift to magenta , in a very subtle attempt/reference to the technicolor 3 strip . " <p> The Documentary about Gregory Crewedson : Brief Encounters is pretty interesting . Each shot is like shooting an entire movie and costs about as much . There are some shots where he literally has a 3 minute window because of the outdoor available light to get the shot he wants , and he just ca n't come back and do it tomorrow . Nothing like a little tension . Needless to say there is also a " little " photoshop involved . <p> There are some shots where he literally has a 3 minute window because of the outdoor available light to get the shot he wants , and he just ca n't come back and do it tomorrow . Nothing like a little tension . 43971 @qwx453971 <p> Doh ! Vilmos Zsigmond had that kind of situation on the infamous Heaven 's Gate shoot 35 years ago , where they would literally wait all day for " magic @ @ @ @ @ @ @ @ @ @ shot . When somebody screwed up a line , they 'd cut and say , " OK , see everybody back here tomorrow , " and do it again the next day . In that particular case , the movie went about $15M overbudget and bankrupted the studio . <p> It is cool when you have the resources to wait for natural light to be just right , but I do n't think it 's practical for most pictures nowadays . Is there trailer up anywhere for It Follows ? <p> Congratulations to the filmmaking team of IT FOLLOWS on the wonderful ( and completely unsurprising ) reception the film has gotten in the States . Ca n't wait for it to come out here . Really a film that makes a case for going to the cinema ! 
@@44332800 @4332800/ <h> The Case Against " Look " LUTs <p> I 've kind of had my fill with neophyte DPs and directors who are operating under the mistaken belief that they can throw a " magic LUT " at their material and have it wind up exactly the way they want it to . <p> I just vented about this on the RedUser group , which has a lot of people like this on the group . My feeling is that a lot of these guys want a magic button they can press that instantly makes their footage look like 1979 color negative , or looks like 1937 2-strip Technicolor , or gives them the exact over-the-top intense look as some recent blockbuster . We know it 's not as simple as that . <p> Here 's part of my response to one guy : <p> The best way I can think of is to employ an experienced colorist to give you this look . The demo for LUT company points out , " we can get you to a starting point without you having to be a rocket scientist @ @ @ @ @ @ @ @ @ @ need to hire a rocket scientist to get the best results . <p> The problem with throwing one of these pre-fab looks at different kinds of material is , the LUTs do n't necessarily react in a linear , predictable way . A human can take that into consideration and manually tweak the look you want . <p> I think there are genuine benefits of certain kinds of special looks -- like Sapphire Effects and so on -- which are truly things you ca n't do in color-correction , like adding reflections and lens flares and stuff like that . Those are genuinely useful . But these so-called emulation LUTs are guesswork to me . I worked for Kodak in the early 2000s when their original 3D LUTs were developed by brilliant color scientists like Mitch Bogdanovich and Peter Postma , and even there , my observation was that there was a lot of trial and error involved , and the transforming LUTs did n't always work . There 's a degree of guesswork involved , especially when the material involved was shot badly ( or at least out of @ @ @ @ @ @ @ @ @ @ size fits all " solution for things like this . <p> I do n't dispute that there are real LUTs out there that get you to a solid useful starting point in Rec709 from log or Raw material . And I also do n't dispute display LUTs are necessary to get into certain color spaces . That 's just a fact of life . But I get antsy when people start to believe that a magic LUT will give them the look they want without any skill behind it . I have yet to see one of these so-called film-emulation LUTs that I ca n't match in a few minutes on a reasonable color-corrector . It ai n't rocket science if you know what you 're doing . <p> And of course , I have no problem with storing lots and lots of pre-made looks in Powergrade ( or whatever toolbox your software has ) . That 's a separate issue , and as far as I 'm concerned , that 's just part of the Swiss army knife at any experienced colorist 's disposal . But these $99 prefab @ @ @ @ @ @ @ @ @ @ is that it gives inexperienced people a way to get instant looks without any skill . Just my opinion . <p> Do n't get angry . If it was n't LUTs it 'd be a plugin or something else . ( Instagram ! ) . People like shortcuts . How well they work is secondary . Whether they do more harm than good is secondary . To me the lut conversation is a pretty good indicator of how well someone understands color . My favorite is when people just start throwing random LUTs on things . " I 've got gopro footage , let me try some Alexa LUTs on it ! " <p> Well , we 've got the data managers of the world posing as DIT 's to thank for this as well . Ca n't tell you how many times a Director says to me on the phone " it should only take an hour to grade , I really like what the data guy did on set " . What the " data guy " tends to do more times often then not is to put @ @ @ @ @ @ @ @ @ @ saturation , rinse , repeat for the remainder of the shoot . Then they come in and see what their material can really accomplish with someone who knows how to handle it properly and takes into account that small thing called the edit and sequencial matching etc . On one hand , they are thrilled to see how much better their stuff looks ... then i bring them back to earth on their time table <p> Do n't get angry . If it was n't LUTs it 'd be a plugin or something else . ( Instagram ! ) . People like shortcuts . How well they work is secondary . Whether they do more harm than good is secondary . 43971 @qwx453971 <p> It 's still sad and unfortunate . My point is that these things are a panacea , and do n't necessarily do what neophytes think they do . <p> I have vivid memories of the days of 3D LUTs in the early 2000s , and after four or five years of mucking around with them at Cinesite and Technicolor , I came to the conclusion that @ @ @ @ @ @ @ @ @ @ It 's all very vague and arbitrary . Granted , there are some real mathematical and hard physics reasons why certain LUTs are designed to be a certain way , but the reality is that there 's a lot more weasel-room with LUTs than directors and DPs know . Our joke at one facility was that we had " The LUT of the Month Club , " because everybody changed them so frequently . <p> But to sell somebody a $99.95 package of LUTs and tell them if you apply this to Red footage and it 'll make it look like Kodak 5247 film ... it 's completely silly at the least , and borderline fraud at worst . <p> If $99.95 for a set of LUTs is borderline fraud what 's the $3995.00 people used to pay amount to ? 43971 @qwx453971 <p> Hey , I think the original 2002 Kodak 3D LUTs cost a lot more than that . But those were legitimate display and output LUTs -- not look LUTs designed to create a specific style , which is what I 'm seeing out on the net @ @ @ @ @ @ @ @ @ @ a specific film emulsion , my response is , " what did you base this on ? What scanner was used for the comparison ? What settings did you use ? What exposure ? " It does n't even get into the question about how they 're emulating film emulsions that have n't been manufactured for more than 25 years . There 's so many variables , again , it boils down to " a LUT is anything you want it to be . " <p> In a lot of cases , I 'd prefer they use the word powergrade or something like that , rather than a LUT . It 's not a LUT , not in the sense I think of as a log-to-lin LUT or a P3 LUT or a LUT for a film-out going to a particular emulsion , calibrated for a specific lab . <p> HA ! While you guys were busy railing against mediocracy I have been busy cutting LADs off of every " dry seasoned " print I could find in our vault , or being used to shim up a film printer @ @ @ @ @ @ @ @ @ @ Big Bobby Houllahan 's Badass Bag of LUTs " <p> You want the look of some ECN-1 developed at a lab on ' Reefer Tuesday ' we got it ... Some film developed in the west indies before ' imagecare ' when they were running low on Part-B ? Pure Classic is what we will call it ... <p> Technical specs and mathematical accuracy ? Bah we do n't need none of that we will just sweep that under the rug of some smug self assurance .... <p> I think a print LUT is probably not the worst LUT you could use " creatively " but in general I do n't see the point . I tried grading some films scans and Alexa footage with the Arri LUT and I thought it was kind of restrictive , like grading in a box . I do n't think I would generally use a LUT except for viewing on set . I would rather have the full range of the data and fit it into the color space by hand than throwing in a thing which gets you 30% of the way @ @ @ @ @ @ @ @ @ @ work with . <p> LUTs confuse and upset Houllahan .... <p> Will use LUTs and Grade on set for money even though he thinks those are both dumb . <p> I think a print LUT is probably not the worst LUT you could use " creatively " but in general I do n't see the point . I tried grading some films scans and Alexa footage with the Arri LUT and I thought it was kind of restrictive , like grading in a box . I do n't think I would generally use a LUT except for viewing on set . I would rather have the full range of the data and fit it into the color space by hand than throwing in a thing which gets you 30% of the way to somewhere and seems to restrict what you have to work with . 43971 @qwx453971 <p> That 's essentially my feeling as well . Even when I was working with the Kodak in-house Wildstar LUT system at Cinesite , using Kodak 's own proprietary color science , we wound up having to go in and tweak the LUTs @ @ @ @ @ @ @ @ @ @ or predictable . I think color science is much more sophisticated and predictable now , but my objection is to the $99.95 " Earl Scheib " LUT paint-jobs that promise an instant look without skill or experience . And I also object to the term " LUT " being thrown around indiscriminately by people who do n't know what they 're talking about . <p> I think there 's five stages of dealing with the way LUTs are used today : <p> 1 ) ignorance ( non-colorists/DITs do n't know or care about LUTs , right ) 2 ) confusion ( you 're using the word LUT but it does n't mean what you think it means ) 3 ) annoyance ( No , Mr. Weekend DP , my job is not just a matter of applying different LUTs until I find one I like . In fact I rarely use what you would call a lut at all. ) 4 ) anger ( Marc , I believe you know this stage ) 5 ) acceptance ( fuck em , do as you please . I 'll just do it @ @ @ @ @ @ @ @ @ @ can do to improve it ) <p> I 'm fully in stage 5 . You get what you pay for . And I 'm not even talking dollars , but in , let 's call it intellectual honesty . If somebody legitimately thinks a $99 pack of " LUTs " is gon na replace , well , whatever it 's meant to replace , then they deserve to learn a hard lesson , and have it be a learning experience . <p> And there 's plenty of people out there who are gon na say " I told you so " . I 'm not one of them , but I 'll smile real big inside when somebody else says it . <p> My take on look LUTs is from my own experience . I used to do a ton of low budget music videos at night to get my name out around town / build a reel etc .. and I will put my hand up and say I used look luts mainly 35mm print style luts that we 're free online at one stage . I fell in @ @ @ @ @ @ @ @ @ @ . I used that lut a good few of them as it was easy to put it on track mode , balance the shots up and then go home 4 hours later instead of 8 . <p> The head of grading here then sat me down and said your getting lazy , all your grades are looking the same and if you keep going down this path you 'll never make it as a top colorist . He also showed me another colourists work that looked a lot like mine and warned me if I kept going down that path I 'd fade into the world of generic grading rather than people taking notice of my work . He put me on a LUT ban . It sucked ass for a day as the director on my next job was very into pushing for a printy 35mm look and the temptation to slip back into the old ways was there but I built the look by hand in about 5seconds ( it 's all in the curves ! ) as I knew exactly what the lut was doing I had @ @ @ @ @ @ @ @ @ @ . I 've not used a lut since and I feel like I 'm 10 times the colorist now and it seems to have paid off as when a senior colorist left us I was raised up the ranks and given my own grading suite . <p> I guess the moral of my story is if you do buy look luts or powergrades deconstruct them , learn how they work and then do n't use them on real jobs . <p> I 'm sure I 'll regret this post if my clients see it but hopefully its helpful to others ! <p> I 've been grading for about six years now but on an amateur/semi pro level . I got into it to increase my production quality and finished product . A big part of that was to get away from the " Click on Looks " . Its not that they all look the same as much as they rarely work out . For the life of me I ca n't understand people trying to apply the " Fast Food " approach to it . <p> This remind me @ @ @ @ @ @ @ @ @ @ the auto and preset tools in Photoshop . Only 2% said they did . The data Adobe collected from each user showed 75% . I do n't think we 're there yet but most of these companies sale instant gratification . 
@@44332801 @4332801/ <h> The transfer modes in Resolve . <p> I will try to make a comparison of transfer modes in resolve layer using the node mixer . We will use two corrector nodes , one with the minimum saturation and the other with the minimum luma channel in the gain . <p> We have these modes in version 8.2.2 Resolve : Before starting to compare images , I will be grouped for faster mentally choose which to use . As you know the order in the input layer of the mixer is a priority , taking precedence over the layer below this but this does not happen every single transfer mode with some detailed below . <p> What do we compare ? saturation , brightness , layer priority , and to what effect we could use . <p> ORIGINAL : This is the original image without applying any effect . <p> NORMAL : Not applicable mathematics , the output is equal to the input layer which is the priority of the node below . in this case there are two images with the normal mode just by changing the order @ @ @ @ @ @ @ @ @ @ <p> LIGHTEN GROUP : add // screen // lighten in this group the order of the layers does not change the result if we analyze the images according to saturation : <p> Saturation from most to least : add - screen - lighten Brightness from most to least : add - ( screen = lighten ) <p> ADD : Linear Dodge works like screen But with More intense results . utility is that connects the luma and chroma of the two layers , sum the two layers . <p> SCREEN : Screen by lightning brightens the lower layer based on the lightness of the upper layer . The result is always lighter , and Makes it a good model for correcting That Exposure in photos are too dark . <p> LIGHTEN : Lighten compare the two layers pixel for pixel and use the lightest pixel value . No part of the image gets darker . <p> Darken GROUP : darken / / multiply in this group the order of the layers does not change the result if we analyze the images according to saturation : <p> Saturation from most to @ @ @ @ @ @ @ @ @ @ : darken - multiply <p> Darken : Each pixel value Darken compare upper layer of the ITS to Counterpart 's pixel value of the lower layer and Chooses the darker of the two to display . takes the darker pixel of the two layers . <p> MULTIPLY : Multiply darkens the lower layer based on the upper layer of the darkenss . No part of the image will get lighter . Any darker tone than white Applied darkens the lower layer . White Becomes transparent . multiplies the shadows of the top layer in the layer below , so it is darker than darken . <p> Difference : Difference reacts to the Differences Between the upper and lower layer pixels . Large Differences lighten the color , and small diferencias darken the color . if we look we see that the chroma vectorscope has been reversed . if we look we see that the luma waveform remains in original condition . the order of the layers does not change the result <p> OVERLAY : Overlay multiplies the light colors and dark colors of the screens do not activate the layer @ @ @ @ @ @ @ @ @ @ the second layer are more whites and blacks by multiplication of the second layer become more black . the order of layers changes the outcome <p> And I leave this layer to the end because it is the most hated of all , at least for me ... Subtract : it has an effect of cancellation , if the two layers is the same pixel being black this is canceled. eg : at the point of saturation to 50 , a layer the sum and the other the remainder . in the active layer controls and the remaining secondary layer controls added . the order of layers changes the outcome 
@@44332802 @4332802/ <h> Ryzen 9 series of processors <p> So it appears the line up of the upcoming processors , codenamed Threadripper/Whiteheaven x399/x390 , has been leaked . It looks very nice for us , as these processors will be aimed more at " content creators " . PCIe lanes will go up to 44 , which will be enough for most of us and if not AMD will also release the Neaples platform that will have 64 lanes . <p> Here is the leaked line up : <p> It looks really nice IMO . Really high clock speeds , especially if you consider the potential overclocking abilities . Goes to show how much Intel has been holding back for the past couple of years . <p> This together with the upcoming Intel 's x299 line up is shaping to be a fun summer , just do n't forget to buy a good cpu cooler ! <p> Ryzen is very interesting and I 'm happy for the competition and plan to get a Ryzen system later this year . And currently for most use cases they are comparable in performance . @ @ @ @ @ @ @ @ @ @ uses the AVX512 vector unit in the i9 CPUs which might speed up those applications if the software is optimized . And vector units are good for fast processing of big data junks . But I do n't know how much this will affect our kinds of workloads . Just has to be thought trough I think . 
@@44332803 @4332803/ <h> Should the first frame of a spot NEVER be 0% black ? <p> I was having a discussion with an editor colleague of mine , and he maintains that the very first frame of a TV commercial should never be 0% black , if the spot is actually supposed to fade-up from black . His spots always start with the first frame having partial opacity between picture and black . The reasoning , if I am understanding him correctly , is that if the client is paying for 900 frames of air-time , then the first frame of the spot should at least be partially visible , even if the creative intent is to fade up from zero-black . Otherwise , complaints may be made by the client that they are getting shafted one frame of visible picture . <p> While this may sound like an editor 's in-joke , he is absolutely 100% serious about this . The reason why we even got into this discussion is that he noticed that the finished master that I graded and exported started with zero-black on the first frame , @ @ @ @ @ @ @ @ @ @ " fixed " this intentionally , or if it was a conform/mastering error . <p> I have never heard of anyone doing this before . As far as I 'm concerned , this makes as much sense as clipping off the first few milliseconds of an audio mix , even if it 's supposed to fade up from total silence . Also , I would think that if the spot previous to yours does n't fade to black , then you might actually notice the absence of a zero-black frame during the fade-up into your spot . <p> The other reason why this does n't make sense to me is that if I need to deliver a " black-to-black " deliverable ( e.g. no leader , slate or tail black ) for internet use , then you do n't want to see the partially-faded first frame when the web video player is parked on the first frame while it initially buffers for playback . <p> I am of course , willing to be wrong about this , if this is SOP in the broadcast world . Is this a valid @ @ @ @ @ @ @ @ @ @ not for the reasons cited . FFOP ( First Frame of Picture ) is always at exactly 01:00:00:00 ( for broadcast ) . If that first frame is pure black , it is very likely to be flagged in QC as a potential concern . Typically it can be fine , after some back and forth with QC telling them it is " creative intent " . However , it still sets up a situation where the various outlets that receive the content may not be aware of this , and have to decide if they think the black frame is a mistake or not . If it was starting with a fade-up from black , most engineers would probably infer from this that the frame of black is content , and not cut it . But the bottom line is that if there is *some* small amount of content there ( even if it 's like 5% opacity ) , it removes all doubt and sidesteps any potential problems . <p> The same goes for audio , where you would want to have some minimal presence if the audio @ @ @ @ @ @ @ @ @ @ . Engineers will be looking at the waveform and seeing a flat line has them worried the audio has been shifted out of sync . <p> TL ; DR - ( IMO ) always start with some audio presence and some minimal amount above black , even if it is " visually black " and only shows on scopes , so as to remove all ambiguity about what the first frame of content actually is . <p> This is a fascinating question . Tyler 's audio analogy really hits home for me . If you were to look at an audio waveform ( audio only , not for video ) , and see a few seconds of silence on the front end of an audio track , there would probably be a desire to edit that silence out at the very first sample of real audio , so that the track could be accurately queued for playback without delay . This makes sense for video , too . In Mel 's example , it 's only a frame or so , but the reality is the line has to officially @ @ @ @ @ @ @ @ @ @ at the very first frame of actual program content . <p> As an aside , I actually ran into this today . I recently purchased a well-regarded piano album , and while the recording itself is fantastic , for some reason they left several seconds of silence on the beginning and ending of each audio track . Beyond seeming a bit sloppy as far as mastering goes , I initially thought something was wrong with my player , but then realized it was actually part of the track . I can see this same thing happening in a broadcast control room . <p> I 've never heard of this requirement before now , but it 's nice to learn about it if it is indeed a thing . <p> I suppose the QC-based explanation makes theoretical sense , but if that were the case , then what is the point of having me comply with strict leader and tail-black specs , if their definition of " Start of Program " is the first detected non-black frame ? <p> I have written my own custom shell scripts that automatically creates deliverables @ @ @ @ @ @ @ @ @ @ my market , because some of them require me to send black-to-black deliverables , while others require slates and leader black with exact frame-counts and 2-pops before the start of the actual program . NONE of them allow me to cavalierly send a deliverable without exact leader/tail durations . One would think that such specs exist precisely so that assumptions can be reasonably made by QC systems when the first-frame of the program actually begins . <p> My own encoding scripts assume that our internal source-masters are at 23.98 , with a slate/leader duration of exactly : 07 . So I can just tell my script to start encoding from 168 frames from the beginning of the file , if I wanted to create a " black-to-black " deliverable . If everyone in my post facility follows the same mastering specs , then the automation of making deliverables works flawlessly . Because , you know , that 's the whole point of having a spec . <p> If QC flags the first frame of a fade-from-black as a source of " potential concern " , then that kind of implies @ @ @ @ @ @ @ @ @ @ to use a dip-to-black transition in the middle of the program ? That seems rather silly to me . <p> All that said , my own experience leads me to think that this is a largely academic argument . I have never had a spot get flagged by QC ( let alone , actually fail QC ) simply because the first frame of the spot was black due to a fade-up . <p> This makes sense for video , too . In Mel 's example , it 's only a frame or so , but the reality is the line has to officially be drawn somewhere , and that might as well be at the very first frame of actual program content . 43971 @qwx453971 <p> Should n't " the line " be located at the exact frame count , offset from the head of the delivered file ? So if the delivery specs say " : 05 slate , 1-frame 2pop , and 1:29 black before SOP " , the " first frame of actual program content " --ipso facto--is 210 frames from the start of the file ( @ @ @ @ @ @ @ @ @ @ have seen tech specs from networks ( particularly for spots ) that specifically say not to have any audio for the first 15 frames and the last 15 frames of the spot . Picture you can have . I have also very definitely worked on commercials where frame #1 was already at like 30% video or something from black , so it did not start from absolute black . So many spots just go BANG from first video and not a fade-up , it 's almost not a question ... but many of them will fade out at the very end . <p> So yeah , I think this is standard procedure . Many years ago ( long before I got into the color-correction business , and I 've been doing that since dirt ) , I was a " broadcast air controller " switching commercials live on the air , and we actually did try to leave a 1/2 second of black between commercials whenever possible . Nowadays , long after humans have been eliminated from that equation , I routinely see commercial blocks that are solid picture and @ @ @ @ @ @ @ @ @ @ 's a stylistic change , but to me , it 's kind of cluttered and overwhelming . Not my decision to make , unfortunately . <p> BTW ( to Mel ) , many spots delivered nowadays have no more 2-pop , no more bars , no nothin ' . You 're lucky to get a short slate and then a few seconds later , the finished spot . Read these current commercial specs from NBC/Universal as one example : <p> I suppose the QC-based explanation makes theoretical sense , but if that were the case , then what is the point of having me comply with strict leader and tail-black specs , if their definition of " Start of Program " is the first detected non-black frame ? 43971 @qwx453971 <p> it is a QC fail if 0 black on the first frame , last frame , or fades up/down for comemcical breaks , i have the been there / done that t-shirt , i think it was Deluxe , London that failed an otherwise clean show for that <p> I have seen tech specs from networks ( particularly for @ @ @ @ @ @ @ @ @ @ for the first 15 frames and the last 15 frames of the spot .... 43971 @qwx453971 <p> I 've delivered commercials to spec which specified silence in the first and last 12 frames . Picture should n't be black on the first frame and I always worked to spec which required a 10 second hold/freeze on the last frame after the ' end of program ' . Unfortunately I had something for a different broadcaster fail because I put a put that freeze on . Their spec ( which was confusingly written as I remember , that 's my excuse ) required that the program cut straight to black . I suggested that the QC/ broadcaster just edit some black on if they did n't like the hold but they shipped the tape back to me for me to do it . <p> I think many of these spec 's are based on the days when someone had to play a tape , see the leader/clock , and cut to it after the countdown - and people have an aversion to cutting to a source which is black ! <p> @ @ @ @ @ @ @ @ @ @ if that were the case , then what is the point of having me comply with strict leader and tail-black specs , if their definition of " Start of Program " is the first detected non-black frame ? 43971 @qwx453971 <p> I agree with Mel that QC should be using the SOP as defined in the specs , and not by whether there 's a visible frame of picture . I 've worked on a number of shows where it was the intention to have music at the beginning over black , with the picture fading up a few seconds later . These were creative decisions made by the director/editor , and I 'm pretty sure they would n't want a faint image at 5% for the first few seconds . I 've always indicated these sorts of programme starts on the record reports , so there 's no reason for QC to be in any doubt . <p> I agree with Mel that QC should be using the SOP as defined in the specs , and not by whether there 's a visible frame of picture . I 've @ @ @ @ @ @ @ @ @ @ intention to have music at the beginning over black , with the picture fading up a few seconds later . 43971 @qwx453971 <p> Programs are not the same thing as commercial spots . You can get away with almost anything in a show provided it 's in the continuity log that comes with the show , warning the people that the first X number of seconds are in black with audio . But commercials have layers of additional rules , as referenced by the document I linked to above . <p> In my experience you get the QC fail when you use a blank area in the timeline , which reads as 0 and a mistake . If you put a black solid in the same spot , you get a pass . The solid reads as " content " where the blank reads as a mistake . <p> That said , I think the idea of not beginning with a frame of black for the reasons your colleague cites is silly . If a frame of black make the transition look cleaner , which it likely will , then @ @ @ @ @ @ @ @ @ @ . My takeaway from this is that this is indeed a pedantic spec that continues to persist with some broadcasters because of legacy QC workflows . <p> I do think it 's telling though that I have literally never had a spot flagged because of this . Even DG FastChannel ( good . riddance. ) , who would consistently reject spots if a graphic was even 2 pixels outside of centercut titlesafe , never had an issue with zero black on head/tail frames . <p> All pedantic reasons notwithstanding , am I a horrible person for giving the middle finger to this silly spec ( ignoring the fact that I have personally never seen a spec sheet that mandates it ) ? Is this the broadcast engineering industry equivalent of Van Halen 's " no brown M&amp;Ms backstage " spec in their concert rider ? I just find it odd that a spec like this still exists in 2017 , when the only good reason that anyone can seem to justify for it is essentially , " that 's what some broadcasters specify " . <p> It 's probably the @ @ @ @ @ @ @ @ @ @ think this is a dumb spec , because in file based workflows , you can count discrete frames from the head of the file , and make assumptions of where the spot starts based on a pre-defined spec . I totally understand that QC would want to be able to automatically flag spots that may have an egregious amount of zero-black in them , since it may be hard to tell if the black was a delivery error , or actually an intentional choice . But one single frame of black at the beginning of a spot ? <p> Seems to me that any automated QC software worth its salt should be able to interpret a ramp in luminance at the head/tail of a spot as being a fade , and allow for a single frame of zero-black at frame 0 because of it . <p> I recently purchased a well-regarded piano album , and while the recording itself is fantastic , for some reason they left several seconds of silence on the beginning and ending of each audio track . 43971 @qwx453971 <p> If it was a piano piece @ @ @ @ @ @ @ @ @ @ may have been considered cadences from one part to the other . <p> I know it is a bit philosophical but silence after all is part of music , put to the extreme by John Cage 's 4 ' 33 and likewise fully black images are part of the visual arts as signified by Malevich 's Black Square . <p> I was having a discussion with an editor colleague of mine , and he maintains that the very first frame of a TV commercial should never be 0% black , if the spot is actually supposed to fade-up from black . His spots always start with the first frame having partial opacity between picture and black . The reasoning , if I am understanding him correctly , is that if the client is paying for 900 frames of air-time , then the first frame of the spot should at least be partially visible , even if the creative intent is to fade up from zero-black . Otherwise , complaints may be made by the client that they are getting shafted one frame of visible picture . <p> While this may @ @ @ @ @ @ @ @ @ @ 100% serious about this . The reason why we even got into this discussion is that he noticed that the finished master that I graded and exported started with zero-black on the first frame , rather than with partial opacity , and wondered if I " fixed " this intentionally , or if it was a conform/mastering error . <p> I have never heard of anyone doing this before . As far as I 'm concerned , this makes as much sense as clipping off the first few milliseconds of an audio mix , even if it 's supposed to fade up from total silence . Also , I would think that if the spot previous to yours does n't fade to black , then you might actually notice the absence of a zero-black frame during the fade-up into your spot . <p> The other reason why this does n't make sense to me is that if I need to deliver a " black-to-black " deliverable ( e.g. no leader , slate or tail black ) for internet use , then you do n't want to see the partially-faded first @ @ @ @ @ @ @ @ @ @ first frame while it initially buffers for playback . <p> I am of course , willing to be wrong about this , if this is SOP in the broadcast world . Is this a valid mastering protocol to be following ? 
@@44332805 @4332805/ <h> The future of Mac pro <p> That might not be such a bad idea , seeing all the problems we have with D700s . I 'm getting SuperMicro motherboard based PC in a couple of weeks , and I 'm keeping my 2012 Macbook Pro as a ProRes dongle . My Trashcan 's going to one of our media assistant as Adobe dongle 43971 @qwx453971 <p> There is not enough VRAM in MacBookPro for a reliable render . Instead you need a MacPro 5.1 as a render machine with something like GTX980 with 4GB of VRAM . <p> You do n't need a lot of VRAM if you use AME or Compressor or whatever to transcode a master file , like DNxHD/HR to Prores. 43971 @qwx453971 <p> My point was in response too this idea : " I might actually sell the Trashcan and get a cheaper Mac and use that as " The ProRes Dongle . " The original talk was about the Resolve Mac " dongle " not AME or Compressor . <p> Yes and my point was that you do n't need a beefy GPU @ @ @ @ @ @ @ @ @ @ <p> You 're still you 're talking about transcoding and I 'm talking about Resolve render . Double re-compression is not the best idea , if you can avoid it , but Remote Render on a MacPro is and nothing like MacBook is suitable for that purpose . <p> My point was in response too this idea : " I might actually sell the Trashcan and get a cheaper Mac and use that as " The ProRes Dongle . " The original talk was about the Resolve Mac " dongle " not AME or Compressor . <p> Heh , all I need to do is to transcode DNG or DPX masters to ProRes ( sometimes ) . 43971 @qwx453971 <p> On Windows - Scratch is probably your best bet here . It 's not expensive , it 's fast , the ProRes files are flawless , and if you 're working from already existing DPX files , There 's no extra step . This is what we do currently , because our workflow is primarily DPX , and ProRes is a secondary output format , after the already-rendered DPX master @ @ @ @ @ @ @ @ @ @ bet here . It 's not expensive , it 's fast , the ProRes files are flawless , and if you 're working from already existing DPX files , There 's no extra step . This is what we do currently , because our workflow is primarily DPX , and ProRes is a secondary output format , after the already-rendered DPX master . 43971 @qwx453971 <p> Scratch on Windows writes the official Apple Prores VERY fast and it does n't require a second computer . Definitely the way to go , especially if you set up the automatic Prores encoding script . <p> A client of mine who works in Premiere Windows purchased it and it works flawlessly . It is seen on the Mac as Apple ProRes and looks exactly like any official ProRes file in Media Inspector . I 've graded three TV episodes encoded with it already . <p> It currently does n't work with Resolve , but support is planned for the near future . <p> I tested ProRes export on a trial license of SAM Rio Assist and it worked like a charm . You @ @ @ @ @ @ @ @ @ @ background while doing other things . I 'm not sure how much it is to buy a license but it 's another option for ProRes on Windows anyway . <p> My move away from ProRes is going very well for the most part . Clients who use Premiere Pro on Mac are happy to accept DNxHD . mxf . If only Fusion would support mxf ... 
@@44332806 @4332806/ <h> Strange Rec709 out of gamut points on Pana dx900 <p> Only Panasonic can tell you the actual backlight technology . It may be RGB LED , or it may be GB-r LED , or even WLED ? ( I would have expected a full array for a high-end display though , not edge lit . ) <p> As for the profile results , it does show that the display has poor internal colour management . Panasonic are one of the few manufactures not using LightSpace for factory calibration , and the issues actually look similar to the errors FSI had some years back , before they swapped to LightSpace ... the calibration is allowing over-gamut excursions when the display illumination is low , with the gamut tracking back to the correct values as the luma increases . <p> Steve 43971 @qwx453971 <p> It 's not edge lit - that was my bad - it 's described as ' direct led ' and ' super mva ' . <p> This is my calibration report after creating a 3D lut , with the display in native mode . I did @ @ @ @ @ @ @ @ @ @ the results with just a quick profile based lut are very good . Guess a 4k lut box is next ... <p> Pepijn 's report shows what the display should be like - if the factory calibration was being done correctly . And PFS phosphor is similar/the same as GB-r LED when it comes to probe pre-sets . <p> What you will get with the rather course backlight array are halos around bright objects within dark scenes . But , that 's no different of any display with local area dimming - and you should be able to turn it off for non-HDR use . <p> It is also worth noting that direct control of any home TV is DDC ( Display Data Channel ) based , which is basically automated ' manual ' calibration , and so will do nothing for out-of-gamut issues , as reported here ( and which Pepijn 's 3D LUT has corrected ) . 
@@44332807 @4332807/ <h> The correct way of normalizing SGamut <p> In actual fact , Slog3 and LogC are very similar in terms of the gamma curve . Sgamut3 and Arri Wide Gamut are also similar in terms of coverage , but really require different matrices to correct for saturation and specific hues . 43971 @qwx453971 <p> Indeed . Although you need to make sure you interpret the ranges correctly ( S-Log recorded as ProRes is the easiest one to get wrong ) . Here are some graphs for those who are interested : <p> That is absolutely not true . There are loads of non-linear cross-channel affects which are really difficult or indeed impossible to replicate with standard lift/gamma/gain or curve tools . 43971 @qwx453971 <p> I believe you immediately , so my claim was probably wrong . What remains though is the simple fact that if a lut is not working for you for some reason you will have to find a way with curves , lgg , sat etc to get a nice image . <p> I have had multiple projects shot on a7s where the slog lut was @ @ @ @ @ @ @ @ @ @ due to inexperienced use of the camera . <p> I shot something a while back on the Sony A7RII to sLog ( which one I do n't remember ) . For fun , I used the same camera to shoot my DSC chart and let Reslove create a node to correct to the chart . The result seemed very color neutral , but a bit contrasty . Let 's call this node a " LUT " ( I 'm not sure how it 's made under the hood ) . I adjusted the exposure using lift , gain , and/or offset before the " LUT " . The results looked quite good as a starting point for grading . We have n't finished shooting yet , so no real work has been done on the movie , but I might just try this as a workflow when the time comes . Anyone else tried this approach ? At a glance , it seemed to solve the color matrix issue of the LOG conversion . <p> That is absolutely not true . There are loads of non-linear cross-channel effects which are @ @ @ @ @ @ @ @ @ @ or curve tools . 43971 @qwx453971 <p> I think you can do it with a combination of things . Sometimes , one node is not enough . <p> Note I 'm a huge fan of Baselight and Filmlight , and used Truelight for a couple of years at Lowry Digital in LA . I 've often said that that 's a rock-solid system that provides real technical LUTs in a way that 's very predictable and measurable , particularly once you understand the calibration process . <p> Having said that : coming up with a reasonable starting point for S-Log DSLR cameras is not rocket science . This is a fairly straightforward process . <p> I agree . I 've been working with SGamut RAW for quite a while , and have tried every approach I could find , including LUT 's , ACES and Color Managed workflow . " Correct " or not , I never seem to get quite as good a result as when I just turn the dials . <p> I agree . I 've been working with SGamut RAW for quite a while , and @ @ @ @ @ @ @ @ @ @ 's , ACES and Color Managed workflow . " Correct " or not , I never seem to get quite as good a result as when I just turn the dials. 43971 @qwx453971 <p> Turning the knobs can work to a point , but sometimes it does help just putting the initial camera Raw signal into a better place to start . I have zero problems with using a technical LUT ( if necessary ) or the built-in color science to Resolve like RedColor/DragonColor and RedLogFilm ( in the case of Red ) . In the case of SGamut , to me it 's just a question of shooting a known color chart like a DSC and finding a way to normalize it with curves and other controls . <p> I 'm not going to argue this with you , but your statement is just plain wrong , at least in terms of intent . A colorspace transform is absolutely mathematically based , derived from the originating colorspace definition and the destination definition . That does n't mean it automatically conforms to a desired look result , particularly if it @ @ @ @ @ @ @ @ @ @ result . So you grade to create that desired result , but the colorspace transform is not subject to artistic interpretation , just like a monitor calibration is not subject to artistic interpretation . It is an absolute , based on specific white points , color gamuts , and gamma curves . Making something look the way you want it to without at least some understanding of that is , in some ways , a kind of bullshit way of doing a job that is both technical and artistic . The technical part is significant , and an understanding of it will almost always lead to a superior result , one that has more depth , more color separation , and a more pleasing greyscale . There is a human element to all of this , but it does n't negate the underlying color science . One can ( and should ) be an artist AND have a solid technical understanding . The best colorists all do , or they work with people who do in a collaborative way . This is n't an either/or . <p> ... but your @ @ @ @ @ @ @ @ @ @ of intent . 43971 @qwx453971 <p> I 'm not sure what you mean by intent , but it 's clearly not " plain wrong " . I 'm on sure the same can be said for " Math does n't lie . " <p> People , often engineers , write algorithms and software to transform gammas and gamuts . It 's all an evolving work in progress . If it was just math , the first version of ACES would be exactly the same as the latest version , and the RAW debayers from Sony and Resolve would be exactly the same , and Redcine-X would n't be up to version 42 , and ... <p> The technical part is significant , and an understanding of it will almost always lead to a superior result ... 43971 @qwx453971 <p> I completely agree ! However , the best way to , for example , squeeze SGamut color values into 709 is more than just math . People write software . Decisions are made . Compromises are reached . It 's never perfect , and never will be . <p> Sometimes multiple @ @ @ @ @ @ @ @ @ @ results are obtained by keeping it simple and just turning the dials. 
@@44332808 @4332808/ <p> " You will see us do more in the pro area , " Cook said . " The pro area is very important to us . The creative area is very important to us in particular . " By Tim 's own definition , by simply releasing just the new versions of iMac , Apple will fulfill this promise completely . The same article continues : " There have been no hints that a Mac mini or Mac Pro refresh is on the horizon , but Apple is rumored to be planning to introduce new iMacs in 2017 , perhaps as early as March . Future iMacs are expected to be updated with USB-C ports and AMD graphics chips " . <p> On the other hand , with release of a brand new form factor MacPro , Apple will be very publicly admitting , that 2013-2017 design was a huge mistake and this famous moment will look even more sad : <p> Apple has changed their minds in the past . G4 Cube , iMac G4 &gt; iMac G5 , re-releasing iMovie and so forth . That said @ @ @ @ @ @ @ @ @ @ in - desktops - are becoming quite a sad affair . And the even sader thing is the solutions are so simple . 43971 @qwx453971 <p> This is sadly true . Well , now that Apple scuttled the autonomous car they were trying to build ( to compete against Tesla and also Google ) , maybe they now have time to concentrate on building a good , reliable computer for audio/video professionals that can be expanded and costs no more than the Windows equivalents . That I would buy . <p> Not sure if this has been posted before , but there is a 2-part podcast on Erik 's exact topic , in which Patrick Inhofer does a " Gear Heads Interview : What Mac Post-Production Professionals Need To Know About Building a Custom PC " with custom builder Eric Bowen . Over at Mixing Light <p> In that talk , Bowen explains the reason for infrequent Mac refreshes is because in order to get a certain price from vendors , computer companies need to buy a certain volume and Apple takes longer to sell out that inventory , so @ @ @ @ @ @ @ @ @ @ happened to insanely great ? 
@@44332809 @4332809/ <p> Test chart and workflow files made for the i1iO and i1iO2 devices in earlier versions of i1Profiler ( before version 1.6.0 ) , will have a different patch height when loaded into i1Profiler . You will need to adjust the patch height to the correct dimension and then re-save the file for current and future use . <p> The Reference Files for the ColorChecker SG and ColorChecker 24 for scanner profiling have been updated . These new reference files apply well to ColorChecker SG and 24 charts manufactured after November 2014 . For ColorChecker editions prior to November 2014 , please use the former Reference Files , which can be downloaded from www.xritephoto.com , including instructions how to replace and apply them in i1Profiler . <p> Profiling and optimizing profiles with very large patch sets ( &gt;3000 ) will require a very large amount of RAM . If profiling fails , reduce the amount of patches in the test chart . <p> The i1Profiler UI and text may appear very small on 4k/5k displays . <p> Monitor calibration does not work when multiple monitors are in mirror mode @ @ @ @ @ @ @ @ @ @ Note : On Windows XP , if a display is removed from a system , the operating system will put the primary display in mirror mode even though the secondary display has been removed . In this case , the user will get an enumeration error . This error can be dismissed and the user will still be able to make a profile . i1Profiler does not support the calibration of displays connected via AirPlay . <p> If you have problems loading the display profile after it has been created or if the system can not load the display LUTs , turn off Automatic Display Control ( ADC ) on the measurement page and try again . <p> i1Profiler uses OpenGL to display the profile 's 3D gamut . If you encounter a problem with the gamut preview , make sure that your video card drivers are up to date . <p> i1Profiler can import CGATS measurement files from other applications . However , the color engine has been optimized to work with the patch sets generated within i1Profiler . The profile quality from profiles made entirely within the i1Profiler @ @ @ @ @ @ @ @ @ @ . In the case of CMYK+N profiles , some legacy patch sets may not even build a profile successfully . It is strongly recommended that users build new charts within i1Profiler for CMYK+N profiling . <p> If you are using Ambient Smart Light Control when making your display profile , expect to get higher Delta E values in Display QA . This function optimizes profiles for visual appearance based on measured ambient conditions not for minimal Delta E. <p> The i1ProfilerTray does not update the time stamp on a rebuilt profile . If a profile is rebuilt using the ambient monitoring feature in the Tray , the original profile creation date is displayed instead of the rebuilt profile creation date . <p> The i1ProfilerTray application looks for connected displays at launch . If you disconnect or connect a display , the i1ProfilerTray will not see the change until it is restarted , the user logs out or the system is restarted . On Windows the i1ProfilerTray can be restarted from the ProgramsStartup menu , on Mac , the tray is located in the same folder as the i1Profiler application . @ @ @ @ @ @ @ @ @ @ device if the computer goes into sleep or hibernation mode . If this happens , disconnect and reconnect the device to restore the connection . <p> Installation of older applications that use the previous version of X-Rite Device Services may cause i1Profiler to not connect with devices or to not launch . If this occurs , reinstall i1Profiler to restore the latest Device Services . <p> If you encounter any problems connecting to your measurement device , please disconnect and reconnect the device to restore the connection . <p> Make sure your i1iSis power button is on when you connect the USB cable . If you connect with the button off , then turn the power on , the i1iSis will not be seen . <p> The i1iO device does not support single row charts . <p> You can not measure an optimization test chart that contains patches extracted from an image if you are using an i1iO or i1iO2 . <p> The version of XRD being installed by i1Profiler causes an issue in ColorPort where targets containing partial rows can not be read using the i1iO table . The @ @ @ @ @ @ @ @ @ @ not come into ColorPort . This affects ColorPort 2.0 and later . 
@@44332810 @4332810/ <h> The future of Mac pro <p> I 've worked extensively with MacPro 5,1 and 6,1 builds for DITs , Avids , Dailies , and Finishing suites . Both units are ... long in the tooth to say the least . I really hope a MacPro 7,1 comes out - and soon . <p> Dollar for dollar you will get more performance with a Windows build . If you are really thrifty you can build your own , it is simple enough . However , I highly recommend going with a vendor like HP . Get an extended warranty . If anything ever goes wrong , just mail your tower back and they 'll send you a new one . If you build your own system , if something breaks , you have driver issues - you do n't have anyone to call and ask for help . This can be extremely frustrating to go through . <p> Where you get the biggest gains on Windows builds : <p> Up to date hardware The MacPro 5,1 is SATA2 , USB2 , and PCI gen 2 . It 's out @ @ @ @ @ @ @ @ @ @ 6,1 has a single internal bay with an OEM max capacity of 1TB . Most HP and Dells will have 8 bays , or more . You can really spread out your boot , cache , and media drives to maximize your internal storage . Internal storage is obviously much cheaper than external . Pop in a single SSD rather than delegate to a multi-bay RAID for the same level of I/O performance . <p> Thunderbolt Thunderbolt 2 is n't all that fast . It 's really great for multi-bay RAIDs , and you do n't need to install SAS cards etc . But for pretty much everything else - Red Rockets , Fiber Channel I/O ... performance is better in a PCI slot in a motherboard and not in an external chassis . Besides , many Windows stations have Thunderbolt 3 so you 'd be better off ! Thunderbolt 3 is about the speed you need to start thinking about external GPUs . <p> GPU You can double the life of any workstation by purchasing a new GPU every two or three years . Right now the GTX1080 is @ @ @ @ @ @ @ @ @ @ you 're crunching through noise reduction nodes in Resolve . <p> I 'm against Hackintosh for two reasons : 1 ) No matter how you spin it , justify it , tell yourself it 's not that bad ... It is illegal . 2 ) Pirating an OS is just a bad idea . you ca n't completely , confidently hand off a hard drive to your client with their media and know 100% for sure there is not any malware present . You do n't know what is going on under the hood in your system . You do n't know if you installed some spyware when cycling through update patches gathered from an unknown torrent site . Your build is unstable and could break at any time , screwing you and your clients over . Back in the day , Apple issued a patch specifically to break Intel Atom support in OS X because of Hacktintosh Netbooks . LONG ... <p> iMacs are great machines , and FCPX and Motion are a perfect combination with them . I also see high-spec iMacs used in all sorts of places @ @ @ @ @ @ @ @ @ @ is they just are n't quite enough for many of types of color and finishing work . <p> So , we 're starting to see a real division in the creative world hardware-wise ... <p> Below a certain level - Macs are everywhere . Above it - they 're nowhere to be found . <p> I 'm against Hackintosh for two reasons : 1 ) No matter how you spin it , justify it , tell yourself it 's not that bad ... It is illegal . 2 ) Pirating an OS is just a bad idea . you ca n't completely , confidently hand off a hard drive to your client with their media and know 100% for sure there is not any malware present . You do n't know what is going on under the hood in your system . You do n't know if you installed some spyware when cycling through update patches gathered from an unknown torrent site . Your build is unstable and could break at any time , screwing you and your clients over . Back in the day , Apple issued a patch @ @ @ @ @ @ @ @ @ @ of Hacktintosh Netbooks . LONG ... 43971 @qwx453971 <p> The OS need n't come from a torrent site , or the updates for that matter . Full security is easily preserved , and as for implying Win x64 is a safer environment when it comes to malware ... The article you linked is over 7 years old . Not exactly an eternity ago , but in software years it might as well have been written on parchment and signed by Abraham Lincoln . However , I suppose you do have a point with regards to your first reason . To be honest , it does make it that little bit more difficult to sleep at night , until I remind myself that I have a powerful ( and upgradable ) dual operating system PC , and then I sleep like a baby . <p> The OS need n't come from a torrent site , or the updates for that matter . Full security is easily preserved , and as for implying Win x64 is a safer environment when it comes to malware ... The article you linked is over 7 @ @ @ @ @ @ @ @ @ @ in software years it might as well have been written on parchment and signed by Abraham Lincoln . However , I suppose you do have a point with regards to your first reason . To be honest , it does make it that little bit more difficult to sleep at night , until I remind myself that I have a powerful ( and upgradable ) dual operating system PC , and then I sleep like a baby . 43971 @qwx453971 <p> Got ta agree on that , I installed Sierra right from the Appstore to the Harddrive without modifications . It runs flawlessly only through a modified bootloader on the EFI partition . None of the patches for the bootloader are from torrents and all with an open source code . It 's not even about saving money . I 'd pay Apple a lot for a new MacPro in an old 5.1 case , yet that option just does n't exist at the moment . Also , none of my clients cared so far about that . Some were rather curious about it . <p> Yes of course it @ @ @ @ @ @ @ @ @ @ if Apple would really care about that they would be much more aggressive against it . Heck I could even add my system to my iCloud account without any issues . I do n't wan na justify building a Hackintosh but just add some facts based on my own experience . <p> I 'm no saint - I 've built a Hackintosh before . I setup a dual boot with Windows 7 ( this was 7 years ago ) . <p> At the time I was a diehard Apple fanboy . I think I bought 10 Macs in 10 years . I sold my iMac and used the cash to build a modest PC . I bought an OEM Windows license while I was at it . I was really impressed with Windows 7 and found myself using it as my go to OS . Not to mention playing PC games was a really nice change . Windows 7 is really everything Vista should have been . These days I 'm on Windows 10 and it 's great - Resolve is running very well . <p> After running into some @ @ @ @ @ @ @ @ @ @ just gave up on the Hackintosh and wiped the partition clean . Maybe it 's gotten a little easier , and I 'm fairly tech savvy , but the Hackintosh took a few solid evenings of tinkering with various builds and drivers . It really was n't worth the effort I was putting into it . <p> Once FCP7 was officially dead , I was sold on Premiere and began the OS migration . I quickly replaced ProRes with DNxHD as my intermediate format of choice ( they are nearly identical in quality ) . I noticed no other changes in my workflow really . I kept a clunker old iMac in the closet for ProRes encodes and mounting MacOS hard drives . <p> I kept an eye out on the HP Business Refurbished page and bought a Z420 for $550 and a Z220 for $475 - both of them quadcore Xeons . They have been the most reliable computers I 've ever owned . I 've added Quadro GPUs to both , SSDs , and RAM and they 're still humming along 3 years later . <p> Really the @ @ @ @ @ @ @ @ @ @ I wanted multiple internal hard drives . It was the fastest , simplest , and cheapest way to speed up my home office , and I could n't afford a MacPro tower . So I switched to Windows . These days there are no Macs that offer multiple internal hard drives . I loved MacOS so much , but the relationship was hurting my wallet so I moved on . <p> There days means it 's Mac Pro 5.1 . I do n't think he means the Trashcan crap . Trashcan is not a computer , that should be used for color grading . 43971 @qwx453971 <p> It 's all that the idiots at Apple are selling . I du n no -- I 've made a pretty good living for the last 3 years doing color on a Trashcan Mac . It 's frustrating for renders , but it does n't affect the results . Would I pick one again ? No . Am I about to switch to Windows ? Yes ( the countdown has started ) . Is the Trashcan usable ? To a point . I @ @ @ @ @ @ @ @ @ @ are using them , and no doubt there are dozens more that I have n't worked for . I 've always said it 's more about the colorist than it is the hardware ( or the software ) , assuming the monitor is good . <p> I 've made a pretty good living for the last 3 years doing color on a Trashcan Mac . It 's frustrating for renders , but it does n't affect the results ... Am I about to switch to Windows ? Yes ... I 've always said it 's more about the colorist than it is the hardware ( or the software ) , assuming the monitor is good . 43971 @qwx453971 <p> Marc , I assume you will go for an HP Z series or whatever the equivalent is when you make your move . I checked to see if there is any scuttlebutt on new Intel processors in the pipeline but nothing beyond the obvious and announced . Intel has really hit the wall over the past few years with their processors . As we get closer to the theoretical limit of @ @ @ @ @ @ @ @ @ @ like are having difficulty advancing at the rate we have become accustomed to over the past couple of decades . If I had to get a machine now , it would n't be a simple decision . I would look at number of PCI lanes for cards and SSDs as a big priority , a motherboard that has the best I/O to accept fast SSDs inside the case and a unit that will be as quiet as possible . The CPU would be a balance between clock speed , the number of PCI Lanes , Cores and price . I would look for a CPU socket that is not near the end of it 's life before Intel introduces the next latest and greatest iteration of the CPU categories . The next big processor jump is code named " Coffee Lake " but that is n't supposed to hit until first half of 2018 . Kind of far off . The GPU that is easy . <p> Get SCRATCH and run automatic conversion to ProRes with an XML script . You can setup output templates , output filenames etc. in @ @ @ @ @ @ @ @ @ @ , loads the file and processes it . Yes , it 's an additional encode , but might be faster than moving stuff over to a different machine and have that render . <p> Hi Mazze . That sounds like a nice way to do the conversions . Currently I use Nuke Studio to make Prores but it is so slow compare to encode on a Mac . I am planning to do a trial of scratch VR suit in the next couple of weeks to see if it fits in my pipeline and budget . Are there any tutorials on how to automate tasks like this ? <p> Get SCRATCH and run automatic conversion to ProRes with an XML script . You can setup output templates , output filenames etc. in SCRATCH and then run a script that opens/creates a project , loads the file and processes it . Yes , it 's an additional encode , but might be faster than moving stuff over to a different machine and have that render . <p> Marc , I assume you will go for an HP Z series or whatever @ @ @ @ @ @ @ @ @ @ checked to see if there is any scuttlebutt on new Intel processors in the pipeline but nothing beyond the obvious and announced . 43971 @qwx453971 <p> Yeah , the z840s are terrific machines . We 'll see how well it fits the budget . <p> Hi Mazze . That sounds like a nice way to do the conversions . Currently I use Nuke Studio to make Prores but it is so slow compare to encode on a Mac . I am planning to do a trial of scratch VR suit in the next couple of weeks to see if it fits in my pipeline and budget . Are there any tutorials on how to automate tasks like this ? <p> I might actually sell the Trashcan and get a cheaper Mac and use that as " The ProRes Dongle. " 43971 @qwx453971 <p> That might not be such a bad idea , seeing all the problems we have with D700s . I 'm getting SuperMicro motherboard based PC in a couple of weeks , and I 'm keeping my 2012 Macbook Pro as a ProRes dongle . My Trashcan 's @ @ @ @ @ @ @ @ @ @ 
@@44332811 @4332811/ <p> I think the latest EIZO CG line is the best you can get for your money if you want a computer display for video . The CG2420 is very interesting but lacks the external 3D-LUT storage function . If you want that , you need to look at the CG247X . <p> If you want a real grading monitor - you still have to buy one . Period . <p> All the stories about the cheaper Benq 's and DELLs are not worth it for me . I still have the old CG247 from 2015 and it still serves me very well with top of the line accuracy while really lacking the deep blacks from my Panasonic FALD UHD TV that is my clients display . That 's why I need to upgrade soon , too . But i 'm not interested in 1080 only anymore . I think I 'll give the CG248 a shot . <p> I think the latest EIZO CG line is the best you can get for your money if you want a computer display for video . The CG2420 is very interesting @ @ @ @ @ @ @ @ @ @ want that , you need to look at the CG247X . <p> If you want a real grading monitor - you still have to buy one . Period . <p> All the stories about the cheaper Benq 's and DELLs are not worth it for me . I still have the old CG247 from 2015 and it still serves me very well with top of the line accuracy while really lacking the deep blacks from my Panasonic FALD UHD TV that is my clients display . That 's why I need to upgrade soon , too . But i 'm not interested in 1080 only anymore . I think I 'll give the CG248 a shot . 43971 @qwx453971 <p> True . CG2420 and eeColor makes a great combo . CG248 suffers from poor contrast and uniformity . Personally I avoid all LCD panels from LG. 
@@44332812 @4332812/ <h> Assimilate announces the release of SCRATCH 8.6 <p> Assimilate is pleased to announce the release of SCRATCH 8.6 . All license keys that are under active support/subscription have been updated and now work with 8.6 . You can grab the latest build from our support site . For a quick overview of the new features in SCRATCH and SCRATCH VR v8.6 please see this video : <p> SCRATCH V8.6 + SCRATCH VR Suite - Release Notes <p> General <p> This update contains a number of changes in how certain formats are processed . As such , they are not fully backward compatible . Before upgrading , you should maintain a backup and are advised not to upgrade in the middle of a project . <p> Format Support Updates and Enhancements <p> New DNx MXF Encoder for high speed encoding of OpAtom and Op1A , including DNxHR . <p> New support for wide range of still camera raw formats like Canon , Nikon , Kodak , Hasselblad , Pentax , Leica and more . <p> Update on Kinefinity support with new high quality debayer . <p> The MXF DNx @ @ @ @ @ @ @ @ @ @ ) audio filename so the info remains available in an AVID roundtrip . <p> Option to create 10 bit H264 . mp4 or . mov output . Important to generate HDR output ; the files contain all HDR statistics ( such as color space and transfer function info and mastering luminance levels ) . Furthermore , the H264 maximum output resolution is now 8192x6144 . The higher resolutions are especially important in the VR environment . <p> ACES : this version contains all the latest standard transforms as published by the Academy . This version also introduces ACEScct ; this is a derivative of ACEScc ( also referred to as ACES log ) but has a different fall off in the shadows of an image . <p> Added an FFmpeg writer node to be able to output various ( less common ) formats ( like e.g. Flash Video or Windows Television ( . WTV ) ) . Note that FFmpeg covers may codec and container formats . Not all combinations of settings are possible and have been tested . <p> UI Updates &amp; re-design <p> The new Logon screen includes @ @ @ @ @ @ @ @ @ @ A Poster image is written automatically on exit of a project or you can override the image from the Tools menu in the CONstruct . <p> The Matrix layout has been updated and re-labeled for more intuitive access . Furthermore , all non-default values in the Matrix will now highlight , including the control-set selection panel . <p> Swipe sensitivity is now a user setting and SCRATCH will now display Swipe placeholders in both the CONstruct and the Player if a panel has been swiped away . <p> The Structure view has been incorporated into the Player Stack on the right . You can now easily switch between the source stack indented list or the structure view . <p> Added tooltips to better explain all the available functionality . Advanced users can turn this function off in the user settings . Note that this is a work in progress and currently only the generic functions in the Player have been labeled . <p> The functionality of the Process module has been consolidated to the top menu bar in the Player to have quick access to the render and cache functionality @ @ @ @ @ @ @ @ @ @ <p> The PQ and HLG transfer functions are now an integral part of SCRATCH 's color management . There is no need to manually search and apply LUTs . With a single click you can define the transfer function for any shot , output or display . <p> The scopes will automatically switch to HDR mode if needed and show the levels in a nit-scale . Furthermore , you can now choose whether the scopes should reflect the levels of your source media , your UI monitor or your reference monitor , which might include an additional color space transformation or custom display LUT . <p> On the project level you can define your HDR Mastering Metadata : color space , color primaries and white levels , luminance levels , etc ... This metadata is automatically included in the Video HDMI interface ( AJA , BMD , BlueFish ) for the display . The Luminance levels set in the meta data are also displayed in the scopes , where it will warn you if the current levels exceed the maximum levels entered . <p> Next to the static metadata SCRATCH @ @ @ @ @ @ @ @ @ @ like MaxCLL and MaxFALL . This metadata can be saved in a sidecar report next to your output or is included in the H264 media that you render . <p> The REC2020 color space matrices have been added for the SDI output settings for the proper YUV conversion . <p> Added the P3 D65 colorspace as an alternative HDR mastering . <p> VR 360 <p> Added 360 stitch functionality . You can now load all your source media from your 360 cameras into SCRATCH and combine it to a single equirectangular image . You can use an external template or ' pano ' -file for combining the sources or use SCRATCH 's own functions to determine the stitch setup . Within SCRATCH you have the full set of grading and compositing tools available to optimize your stitch and you can review it while you work in 360 mode in a headset or on a reference monitor / theater projection . You can render out the stitched equirectangular shot or continue working on the stitch node in SCRATCH . The main benefit is that at any time during post and mastering @ @ @ @ @ @ @ @ @ @ your source footage . <p> Ambisonic Audio . SCRATCH can load and playback ambisonic audio files ( AmbiX/FuMa B-format ) to complete your 360 immersive experience . SCRATCH will automatically recognize ambisonic audio that is included in an h264 file or you can load a separate *. amb audio file and link it to a shot / timeline . Playback of ambisonic audio is automatically linked to the motion of your headset or the direction of the video you set with the PanZoom tool . From the audio panel you can adjust the base vector / direction of the ambisonic audio when it is not in line with your video . When rendering out 360 media to H264 , SCRATCH will include and properly tag the linked ambisonic audio so that other players ( e.g. YouTube or Facebook ) will recognize it . <p> Next to using ambisonic audio , you can also load multiple mono or stereo audio files and adjust their position inside your 360 environment . Again - this is all done through the audio panel - in the ambisonic section . <p> Added overlay handles to @ @ @ @ @ @ @ @ @ @ in a 360 scene . <p> Stereo Link and Convergence are now also available for Equirectangular : convergence only applies to Yaw . <p> Misc <p> New Report tool , available from the Tools menu in the Construct . The Report generator creates an ( html ) report from the input parameters and the selected shot-range . You can customize the reports with your own logo and for certain reports include your own metadata column selection . <p> New Live Stream option . This new function is accessible - together with the Publish and Remote options - from the new Collaborate menu in the top menu bar in the Player . <p> Back by popular demand : the Link-softness option . <p> Nest and plug-in nodes by default now render to Project Cache folder . <p> SCRATCH no longer sets an explicit out-point when applying a plug-in . In the Matrix Fill/matte menu you see the media length of the underlying node . When copying the layer / plug-in to another parent , the out-point automatically adjusts . This way you do not get any freeze frame displays when the @ @ @ @ @ @ @ @ @ @ Added a border-option to video wall plug-in . <p> Added an Open Folder button to Process Queue dialog to open the file location of the selected queue item . <p> Added option in the Stabilizer to either apply the data to the Framing parameters ( as it used to do ) or alternatively create a layer with the stabilize motion applied . This latter solution has the advantage that you can easily group the stabilizing layer and add a second autonomous motion ( e.g. camera pan ) on the already stabilized image . Also , the stabilize function did not properly handle over-under 360 stereo media , and the yaw , pitch and roll parameters where not always properly translated into a stereo node . <p> Added the option to select an output-template with the Project setting which is then applied every time you create a new CONstruct . This will save you time rebuilding your standard output -tree . Also added a confirmation message before deleting output templates in the Output menu in the Construct . <p> The actual value of an on/off button was not properly visible when @ @ @ @ @ @ @ @ @ @ scripting update : now all project formats and defaults properties can be included in a project-create xml . <p> Added a Transformer Node to the output list ; this is the same node type as the Image File node but labeling it this way makes it more explicit / intuitive how to add re-scaling in your output tree . <p> Various updates for AAF conform that contain vari-speed adjustments in combination with dissolves . 
@@44332813 @4332813/ <p> I still think that there 's no way of simulating reality in a correct way . <p> Why are people doing films ? To recreate reality in a most correct way ? I wo n't deny that there are mathematically correct transforms of camera recorded footage back into REC.709 world . But that 's not what grading is all about ... At least for me . <p> The TO was asking about normalizing log-footage not linear light . My experience is that the Sony transformation LUTs look bad in most if not all circumstances . You really need to come with an idea of your own how to make that look good and apropriate to the story . <p> There is absolutely a correct way , and it is different for different log curves and color spaces . All modern cameras are capturing linear light , and how that information is transformed for display purposes , and how it is transformed for storage purposes are mathematically defined and usually two different things . Manufacturers know the characteristics of the cameras they produce , and they know the actual @ @ @ @ @ @ @ @ @ @ the captured scene " properly " for specific display targets , and they use it to develop log curves that allow the largest amount of the captured information to be stored with proper , perceptually accurate precision . T 43971 @qwx453971 <p> That 's how it suppose to work on paper . However , the LUTs that Sony provides are not doing good job in terms of normalizing color . And it 's not just my eyes , I have reference points and scopes to confirm that . <p> I think one of the general problems with these technical luts , if I may call ' m like that , is that they expect a certain picture shot at a certain exposure etc , and of course every picture is different . Furthermore with Sony Slog flavors I think this problem gets exaggerated because the change the lut does is quite big , in both luminance and colour . So anything slightly off in the log file becomes a bigger problem after the lut conversion . One thing I have done to handle this a bit better is to create @ @ @ @ @ @ @ @ @ @ the general problems with these technical luts , if I may call ' m like that , is that they expect a certain picture shot at a certain exposure etc , and of course every picture is different . Furthermore with Sony Slog flavors I think this problem gets exaggerated because the change the lut does is quite big , in both luminance and colour . So anything slightly off in the log file becomes a bigger problem after the lut conversion . One thing I have done to handle this a bit better is to create luts with different exposures. 43971 @qwx453971 <p> This only has relevance if you are putting the LUT at the beginning of your processing chain , which is not something that should be done under any circumstances . A LUT has a finite range of values , so you need to adjust the incoming image to fit that range prior to passing it through the LUT . That 's why the " correct " way is to put the LUT further down the processing chain , and use ( at the very least ) @ @ @ @ @ @ @ @ @ @ the range expected by the LUT . Making different LUTs does n't make a lot of sense if the primary purpose of the LUT is to perform a gamma and color space conversion ( which is what the purpose should be ) . Those things are fixed , and they do n't change simply because you have a shot that overexposed or underexposed . <p> That 's how it suppose to work on paper . However , the LUTs that Sony provides are not doing good job in terms of normalizing color . And it 's not just my eyes , I have reference points and scopes to confirm that . 43971 @qwx453971 <p> hence the suggestion to use ACES ; - ) <p> should note that Arri LogC is also presented diffrently between YRGB with the 709 LUT , and in ACES with Alexa IDT <p> If the Alexa double recorded ArriRAW and LogC , they appear pretty much identicaly in ACES the exceptions being precived sharpenss ; RAW = much sharper , and noise = ProRez looking much cleaner , due to compression artifacts and softening covering the @ @ @ @ @ @ @ @ @ @ atall surprised if SonySlog3 presents diffrently with a SonyLUT in YRGB , when compared to presenting with the IDT in ACES , actualy i would be surprised if they were the same , there should be a real world advantage to how the image presents to the system with scene refered input <p> It takes skills and being an artist to be able to stick a video recorded by a good camera into a narrow Rec.709 space and make it look good ! 43971 @qwx453971 <p> I 've been doing this for a very long time now , and I 'm quite aware of the requirements . Nothing I said negates that . <p> That statement undermines your premise . <p> Because if human eyes , brains and perceptions do lie we absolutely have to make changes to a video to make it look good ! <p> I mean what are you going to say : " I know the video looks bad but do n't worry it is mathematically correct " ? 43971 @qwx453971 <p> I do n't see how it undermines it at all . Tweaking a gamma @ @ @ @ @ @ @ @ @ @ way of creating images with too much or too little contrast in specific ranges of the resulting image , and not really understanding why . There certainly is some truth to the statement " if it looks good it is good , " but if the aim in most grading situations is to at least start out by displaying what was actually shot in a " faithful " manner , using a scientifically derived transform that is intended for that purpose is going to get you there a lot quicker and a lot more accurately than interpreting - or as often as not , misinterpreting - by hand with no sensible starting point . If you want to tweak from there , you 're certainly correct in doing so to achieve a particular result , but I have yet to be convinced by anything I 've ever seen that doing it by hand , from a log encoded image of any camera , using only one 's eyes and personal taste is superior to recreating the original image faithfully first using a proper transform for the purpose . I know @ @ @ @ @ @ @ @ @ @ the bad results I 've seen - especially in recent years - have come directly from using that approach . <p> Just my opinion . But one rather strongly shared by virtually all color sciences experts and most high end colorists I know ( and I know a LOT of them ) . As with all opinions , though , everyone is free to accept or reject it as you wish . <p> This only has relevance if you are putting the LUT at the beginning of your processing chain , which is not something that should be done under any circumstances . A LUT has a finite range of values , so you need to adjust the incoming image to fit that range prior to passing it through the LUT . That 's why the " correct " way is to put the LUT further down the processing chain , and use ( at the very least ) an exposure and contrast correction to put the exposure within the range expected by the LUT . Making different LUTs does n't make a lot of sense if the primary purpose @ @ @ @ @ @ @ @ @ @ space conversion ( which is what the purpose should be ) . Those things are fixed , and they do n't change simply because you have a shot that overexposed or underexposed. 43971 @qwx453971 <p> Many dop 's like to overexpose their slog2 recording with one stop . Why should you not use a lut that takes that 1 stop into consideration , especially if it brings you closer to a good starting point ? And why do you assume I 'm using a lut at the beginning ? I do n't recall saying so . I actually use them just like you described . If a lut is created based upon correct exposure it expecting certain values - ie middle grey - at a certain place in the log image . But if exposure shifts , these places shift too , so why not adjust the lut and stay as close to the original intended transform ? <p> should note that Arri LogC is also presented diffrently between YRGB with the 709 LUT , and in ACES with Alexa IDT <p> If the Alexa double recorded ArriRAW and LogC @ @ @ @ @ @ @ @ @ @ being precived sharpenss ; RAW = much sharper , and noise = ProRez looking much cleaner , due to compression artifacts and softening covering the noise seen in the RAW <p> i would not be atall surprised if SonySlog3 presents diffrently with a SonyLUT in YRGB , when compared to presenting with the IDT in ACES , actualy i would be surprised if they were the same , there should be a real world advantage to how the image presents to the system with scene refered input 43971 @qwx453971 <p> The fact that a manufacturer 's Rec.709 LUT does not look the same as ACES with a Rec.709 ODT is a very clear illustration that there is no one correct way to convert a scene referred image to a display referred one . It is always affected by creative choices , whether those are made by the manufacturer , the colourist or The Academy . Indeed the fact that ACES 0.7 does not look the same as ACES 1.0 , and Rec.709(800%) does not look the same as LC709A emphasises this still further . <p> Many dop 's like to @ @ @ @ @ @ @ @ @ @ you not use a lut that takes that 1 stop into consideration , especially if it brings you closer to a good starting point ? And why do you assume I 'm using a lut at the beginning ? I do n't recall saying so . I actually use them just like you described . 43971 @qwx453971 <p> I did n't say there 's anything " wrong " with doing that . I said that it was unnecessary . <p> If a lut is created based upon correct exposure it expecting certain values - ie middle grey - at a certain place in the log image . But if exposure shifts , these places shift too , so why not adjust the lut and stay as close to the original intended transform ? 43971 @qwx453971 <p> That 's why I specifically mentioned an exposure correction . That means either using a dedicated exposure control if you 're in a program like Baselight that has one , or an offset control if you 're in a program that does n't ( or if you 're using CDL 's ) . That @ @ @ @ @ @ @ @ @ @ middle gray and everything else follows . If doing that throws the highest or lowest values out of range , that 's what a contrast control is for . BTW , using ACES , assuming you 've got a proper IDT for the camera being used , essentially accomplishes this for you , although if you 're using ACEScc or ACEScct , you 'll still want to do an exposure correction in most cases . <p> I did n't say there 's anything " wrong " with doing that . I said that it was unnecessary . <p> That 's why I specifically mentioned an exposure correction . That means either using a dedicated exposure control if you 're in a program like Baselight that has one , or an offset control if you 're in a program that does n't ( or if you 're using CDL 's ) . That shifts all values equally , so you basically correct for middle gray and everything else follows . If doing that throws the highest or lowest values out of range , that 's what a contrast control is for @ @ @ @ @ @ @ @ @ @ a proper IDT for the camera being used , essentially accomplishes this for you , although if you 're using ACEScc or ACEScct , you 'll still want to do an exposure correction in most cases . 43971 @qwx453971 <p> But I still do n't understand the difference between an exposure correction done before the lut or in the lut itself . Actually I think there will not even be any difference in the resulting image . <p> But I still do n't understand the difference between an exposure correction done before the lut or in the lut itself . Actually I think there will not even be any difference in the resulting image . 43971 @qwx453971 <p> A correct exposure adjustment done before a LUT , or a correctly calculated one baked into the LUT will of course give an identical result . The Base Grade in Baselight 5 has a true colour space aware exposure control , but the " Exposure " control in Baselight 's Film Grade is just an offset . If you use offset you are doing a true exposure adjustment to the mid-tones , @ @ @ @ @ @ @ @ @ @ shadows , and indeed if you are not careful you may push some image data out of the input range of the LUT , in which case a compensated LUT is definitely better . <p> To put some numbers on that , S-Log3 black is code value 95 , 18% grey is 420 , and 1.5 stops above grey is 534 . So if you overexposed 1.5 stops , a straight offset to put grey back in the right place would mean subtracting 114 . This means that unless you reduce contrast as well ( in which case you 're deviating further from a true exposure correction ) anything black , or the 19 code values above black , will get clamped to zero . The situation in S-Log2 is worse . A one stop offset will clamp the blacks . <p> To put some numbers on that , S-Log3 black is code value 95 , 18% grey is 420 , and 1.5 stops above grey is 534 . So if you overexposed 1.5 stops , a straight offset to put grey back in the right place would mean subtracting @ @ @ @ @ @ @ @ @ @ well ( in which case you 're deviating further from a true exposure correction ) anything black , or the 19 code values above black , will get clamped to zero . The situation in S-Log2 is worse . A one stop offset will clamp the blacks . 43971 @qwx453971 <p> What would be a better way to shift the exposure up or down before applying the LUT ? <p> To correct an over or under exposed log video would it be a good idea to provide a inverted S-curve first , then use the offset and then apply an S-curve and then finally apply the LUT ? <p> What would be a better way to shift the exposure up or down before applying the LUT ? 43971 @qwx453971 <p> Transform from S-Log3 to linear ( with a Colour Space operator which both Baselight and Resolve FX have ) apply linear gain and then transform back to S-Log3 would be the mathematically accurate way of doing it . <p> Transform from S-Log3 to linear ( with a Colour Space operator which both Baselight and Resolve FX have ) apply linear @ @ @ @ @ @ @ @ @ @ mathematically accurate way of doing it . <p> I also like Arri LUTs a lot more than Sony , but what I 've also noticed that LogC to REC709 LUT make blue color a lot darker and brings more noise . 43971 @qwx453971 <p> When you 're dealing with a 3D LUT that incorporates a color matrix , the transform becomes quite camera specific , as it 's not just implementing a gamma curve . If you use a LogC to Rec709 transform that does n't incorporate a matrix , the color wo n't be nearly as affected . In actual fact , Slog3 and LogC are very similar in terms of the gamma curve . Sgamut3 and Arri Wide Gamut are also similar in terms of coverage , but really require different matrices to correct for saturation and specific hues . 
@@44332814 @4332814/ <h> Quick rundown of the different DaVinci Resolve grading panels <p> One alternative is to use the video Gain control as a " substitute " for the contrast control . Gain is basically a contrast control with a lowered pivot , and clearly does not feel quite the same . Nor is an S-curve available ( although I normally turn that off on a Resolve system anyway ) . But in terms of being able to adjust exposure and contrast simultaneously , it can be a very effective substitute , particularly when you 're grading for video color space . Some colorists I have trained for log grading use that approach for the very reason you 're suggesting . 43971 @qwx453971 <p> Now that I think about it , that 's actually what you 're likely doing on Cortex , since that 's a CDL based color corrector . Slope and gain are identical , so by using the offset trackball and the video gain , you 're getting exactly the same thing . <p> Now that I think about it , that 's actually what you 're likely @ @ @ @ @ @ @ @ @ @ color corrector . Slope and gain are identical , so by using the offset trackball and the video gain , you 're getting exactly the same thing . 43971 @qwx453971 <p> Yeah slope/offset/power is n't strictly contrast/pivot ; I guess it feels somewhere between that and LGG . But having got used to it it now feels like a quicker and more more natural way to expand a signal than simple negative lift and positive gain . I 've been wondering what the math is when you 're bringing CDLs into Resolve that translates the SOP values into LGG adjustments , as when you apply a CDL to a shot it 's only the LGG values that change within Resolve , not contrast and offset . <p> I 've been working my way through the articles on Lowepost and the colourists doing a lot of the big finishing jobs seem to not be using LGG at all which is what 's got me thinking about it . A lot of guys are predominantly using printer lights and contrast under a LUT , notably some big CO3 names , so I @ @ @ @ @ @ @ @ @ @ 's most high profile client they have n't optimized their flagship product ( advanced panel ) for the grading style that a lot of their colourists prefer to use , or at least made it more accessible . <p> Baselight gives you the option of video , film , telecine and technical grade operators that can be controlled the wheels on the Blackboard , *and* base grade 's coming soon . If BMD would release an update that offered a third option next to primaries and log that turned the lift wheel --&gt; pivot , gamma wheel --&gt; contrast and gain wheel --&gt; offset that 'd at least go some of the way to catching up . My 2c . <p> If BMD would release an update that offered a third option next to primaries and log that turned the lift wheel --&gt; pivot , gamma wheel --&gt; contrast and gain wheel --&gt; offset that 'd at least go some of the way to catching up . My 2c. 43971 @qwx453971 <p> Although I happen to prefer Baselight to essentially everything else out there , I seriously doubt that Blackmagic @ @ @ @ @ @ @ @ @ @ number of Resolve seats dwarfs the number of Baselight seats by a very , very , wide margin , regardless of the reason . And the percentage of that large user base that is using their panels is rather low , with most of those users pretty happy with it . Not to mention that there is really no reason for Resolve to have to become Baselight . They are two different products with two somewhat different approaches , orientations , and user bases . That 's a good thing , not a bad one . <p> I just added a Bt and Kb panel to my Tk . It appears that only 7 of the 16 menus can be used without the Mf panel . 43971 @qwx453971 <p> I 'm a little confused with this statement so maybe I 'm not understanding what you 're saying : The MF panel as far as I know has nothing to do with selecting the functions on the KB and BT panel , so having or not having the MF should n't make any difference to the BT / KB operation . @ @ @ @ @ @ @ @ @ @ and Resolve on our website : Go to Element product page . Select the Application Support tab . Select Blackmagic DaVinci Resolve as your application . The list of all downloads and documents available will appear underneath . One of them is called : Resolve And Element Configuration And Mapping <p> Y A lot of guys are predominantly using printer lights and contrast under a LUT 43971 @qwx453971 <p> Printer lights and offset are the same thing . The only difference is the predefined steps in the printer light controls vs. the continuous values in the offset control . And the fact that in Resolve the printer lights are mapped ( to the numeric keypad buttons ) - which is the same way they 're mapped in Baselight . When I was doing DI 's on Baselight , I often used that - particularly with film sources - because the predefined steps are logical , and also because " walking back " any correction was simple because of those predefined steps . I , and most colorists I know , usually used those controls for overall exposure , rarely for @ @ @ @ @ @ @ @ @ @ of adding some overall warmth or overall coolness to an image . <p> @Noel : I 'm sorry to hear you were n't happy with the additional functions provided by the BT . <p> I 'm a little confused with this statement so maybe I 'm not understanding what you 're saying : The MF panel as far as I know has nothing to do with selecting the functions on the KB and BT panel , so having or not having the MF should n't make any difference to the BT / KB operation . <p> We do publish the mapping of the Element panels and Resolve on our website : Go to Element product page . Select the Application Support tab . Select Blackmagic DaVinci Resolve as your application . The list of all downloads and documents available will appear underneath . One of them is called : Resolve And Element Configuration And Mapping 43971 @qwx453971 <p> My bad , just figured out how to get to those other 9 panels . It would still be nice to map some transport functions to empty buttons . Cheers . <p> @ @ @ @ @ @ @ @ @ @ Tk . It appears that only 7 of the 16 menus can be used without the Mf panel . In the 7 menus available , out of a possible 84 knobs , only 63 are used . Out of a possible 84 buttons , only 20 are used . So even though 64 buttons are blank , the fact that I am missing only 12 from the Mf panel means 9 major menus are completely unavailable . Back to the store they go ! 43971 @qwx453971 <p> Well , you could always go back to the store to get an MF panel . If you have 3/4 of the set , you may as well go all the way . I use a ton of the stuff on the MF panel , and even though the trackball has no function ( presently ) , everything else does . <p> Well , you could always go back to the store to get an MF panel . If you have 3/4 of the set , you may as well go all the way . I use a ton of the stuff on @ @ @ @ @ @ @ @ @ @ no function ( presently ) , everything else does . 43971 @qwx453971 <p> I do n't see the advantage of the Mf panel over an X-Keys panel . There are no knobs ( that work ) , and I believe all the button functions can be mapped with keyboard shortcuts to X-Keys for far less money . <p> I do like the other three panels now that I see all the functions are available , but with X-Keys ' ability to map keyboard functions not available on the Element , I think it makes a better combination . If some of the empty knobs/buttons on the other Element panels could be mapped to keyboard macros , that would be a different story . <p> I do n't see the advantage of the Mf panel over an X-Keys panel . There are no knobs ( that work ) , and I believe all the button functions can be mapped with keyboard shortcuts to X-Keys for far less money . 43971 @qwx453971 <p> I disagree . I use the panel pretty heavily , and I 'm used to the layout and the @ @ @ @ @ @ @ @ @ @ jog knob all the time , so that much of it does work . I also use an X-Keys 24 for the couple-of-dozen things that are n't mapped to buttons , particularly with mouse-clicks and drop-down menus . In general , several of those are n't on the big panels , either . <p> I disagree . I use the panel pretty heavily , and I 'm used to the layout and the A/B layers of the menus . 43971 @qwx453971 <p> If you are accustomed to the layout , no doubt there 's a good reason for the Mf , but for those of us who are n't , the X-Keys 68 with jog/shuttle can replicate the Ax12 + Bx12 + 7 = 31 buttons with 37 left over for things which can not now be mapped with Elements . <p> Element panels are also much better looking . If unused Element buttons , and/or additional Bt panels could be mapped to keystrokes like X-Keys , I would n't need X-Keys at all . But at present , there are too many often used functions that remain unmapped . @ @ @ @ @ @ @ @ @ @ Element Tk , Bt , and Kb ( where did they get those names ? ) , and X-Keys XK-68 + Jog/Shuttle . <p> I was very surprised to learn that as mentioned in the first post , the keyboard for the Blackmagic control surface is not backlit . With everything else being nice and bright , maybe they 'll update the hardware . <p> I also find it odd that there are buttons or options that are labeled as " reserved for future use " <p> If you are accustomed to the layout , no doubt there 's a good reason for the Mf , but for those of us who are n't , the X-Keys 68 with jog/shuttle can replicate the Ax12 + Bx12 + 7 = 31 buttons with 37 left over for things which can not now be mapped with Elements . 43971 @qwx453971 <p> I have to say , the X-Keys 68 would be an interesting companion for the Resolve Mini Panel , which has no jog/shuttle . <p> I 've had the BMD Mini Panel for about a month now , and I personally @ @ @ @ @ @ @ @ @ @ feels a lot more fluid and well put together . If you can stand the Resolve Exclusivity it 's an amazing panel . 
@@44332816 @4332816/ <h> Why Ca n't I See The Benefits of SGamut on the A7rii ? <p> SGamut ( and other variants ) on consumer Sony cameras is merely an emulation of the real SGamut seen on F55/F65 ( this was printed on A7S camera manual ) , the emulated SGamut approximates the chromaticity ratio of the real full SGamut ( that 's why A7S SGamut has a nasty green tint ) , the actual CIE coverage may not even be bigger than Adobe RGB ( what the camera circuitry designed to capture and process in RAW stills mode ) . <p> The main benefit of this emulated gamut is the ability to intercut with professional Sony cinema cameras and it only requires minimal to moderate amount of shot matching in colour grading . <p> while Lucas may be right with the A7 series only having a smaller amount of the big Sony cameras there is something more about recording in Slog2/3Sgamut/Sgamut3 . <p> The Log part is there to have a good way to store the full dynamic range of the sensor . While this is helpful with high quality @ @ @ @ @ @ @ @ @ @ bits available to save a stop is drastically reduced and heavy compression will kill even more information . <p> The wider gamut it there to save color information that may be lost when using a small gamut like Rec709 which can - especially with saturated color sources like colored small band LEDs or so - clip early in Rec709 and look dull . <p> And then there is the problem with your method : You tried to look what is there after the LUT which emulates what the camera is doing and slims down your colors to the smaller Rec709 gamut . If you change the saturation in front of the LUT or look at it with other methods you may see much more information in the material . <p> P.S. : I know this is a long answer for internet standards but way too short to really go deep into color management which is a huge topic . But I hope it clear up some points . <p> I tend to think that any form of log recording is a bit of a ' consumer gimmick ' on 8bit @ @ @ @ @ @ @ @ @ @ use naturally ' low contrast ' classic lenses such as the Zeiss ' Contax ' series , sometimes with filters too like Schneider Optics ' ' Digicons ' which lift blacks and reduce highlights without softening the picture or introducing unwanted side effects . The idea is to manipulate light before it hits the sensor then compensate in post . <p> I tend to think that any form of log recording is a bit of a ' consumer gimmick ' on 8bit cameras . When working with them , I prefer to use naturally ' low contrast ' classic lenses such as the Zeiss ' Contax ' series , sometimes with filters too like Schneider Optics ' ' Digicons ' which lift blacks and reduce highlights without softening the picture or introducing unwanted side effects . The idea is to manipulate light before it hits the sensor then compensate in post . 43971 @qwx453971 <p> 1 . Canon Log is optimised for 8bit and it works very well . <p> 2 . Digicon filters only slightly tones down highlight , it does not reveal actual detail with lifted black . @ @ @ @ @ @ @ @ @ @ create it . ) <p> 3 . Low contrast lenses work the similar way , blacks are lifted due to optical/coating charateristics , it does not reveal actual detail . Using this lens is purely for the look , not for increasing captured dynamic range . <p> I dealt with Canon log from a 1D-C once and was totally unimpressed by it . So much banding and shit going on there . Was really bad really . Felt like beginning to band when only thinking about turning a knob . <p> I dealt with Canon log from a 1D-C once and was totally unimpressed by it . So much banding and shit going on there . Was really bad really . Felt like beginning to band when only thinking about turning a knob . <p> From just scrolling through it , do n't have time to fully read it , this thread is all about Premiere fucking it up . I used Resolve and saw quite some banding there . I 'm not sure if I only looked at 4K MJPEGs or also on some of the Proxies/Backup wie recorded @ @ @ @ @ @ @ @ @ @ was pretty unhappy with the quality of the material and have n't had anything to do with Canon Log since that one time ... <p> I have an A7s as a kick around camera and it has it 's merits for what it is . I feel that the S-Gamut is basically impossible to use with the sensor and front end processing of the A7s and the 8-bit container . I have shot some commercial work with the A7s with the Rec-709 gamut and a tweaked color matrix in S-Log and if the lighting is controlled and it 's exposed right it can pass for something better than it is . <p> without stepping into a fan-boy flame war ... i 've graded all of one thing with each camera , the A7s was shooting to an external recorder , from memory ... what i had in hand was 4k ProRez444 , and the 1Dc was also a flavor of ' ProRez logish after being transcoded from what i do not know tho ... <p> the end game in my case is the A7s was very impressive , and fast @ @ @ @ @ @ @ @ @ @ Sony IDT looked good . <p> The show with the 1DC was also impressive , but for the wrong reasons , the footage i had was best discribed as crap .. no idea how it got that bad ... so we decided that it was beyond saveing the skintones , and made it all hi-con .. that piece was done in 709 <p> Yea , 1DC = impressivly bad camera with zero range to move if it goes off the rails .. so i recomend the A7S ( or an Alexa mini ) <p> Dave of course is referring to the A7R Mk II not the A7s . Both are 8 bit cameras but Dave , are you able to confirm whether A7r Mk II internal 5 Axis stabilization will work with older , manual 3rd party lenses ? I have no interest in the A7s but there are a lot of appealing features to the A7r MkII . <p> I believe the A7r ( I and II ) line skip and pixel bin for video recording where the lower res A7s ( I and II ) readout the full @ @ @ @ @ @ @ @ @ @ a step above in terms of detail and aliasing performance . <p> " There 's an important point to mention here . The A7S has been phenomenally popular as the first full frame camera to output ( but not record internally ) 4K without pixel binning . This remains the case with the release of the A7R II . The sensor in the newer camera was , it seems , optimised around the aim of outputting a cropped Super 35mm sensor area without pixel binning , with even this smaller area benefiting from supersampling from more pixels than are needed for 4K . This means that , once the video from the sensor is deBayered , the result is a true 4K image , as opposed to cameras where exactly 4K pixels are used as a source , which would mean that the deBayered image is less than true 4K .... <p> .... So , to sum up , as Sony told me themselves : if you want internal recording of 4K , buy an A7R II and use it optimally in S35 mode for a true , supersampled 4K @ @ @ @ @ @ @ @ @ @ , buy an A7S . " <p> I believe the A7r ( I and II ) line skip and pixel bin for video recording where the lower res A7s ( I and II ) readout the full sensor without line skipping and binning and this puts them a step above in terms of detail and aliasing performance . 43971 @qwx453971 <p> A7R II uses full readout in S35 mode , pixel binning in full frame mode . No line skipping at all . <p> Has the color science has remained the same from A7 version to A7 version ? <p> if so , and i think that 's the case , to my eyes there 's obvous benifit to SGamut . <p> On the advert i linked to i worked in ACES , only adjusted the exposure , contrast , pivot , saturation a small amount , no masks , no windows , no tracks , no luts , no nutt'n but a fairly fast grade with DVO Clairity on the base layer .. oh , and a whole lotta comp'n / reframeing to remove crew members , support vehicles @ @ @ @ @ @ @ @ @ @ in DS after the grade tho . <p> i worked from ProRez444 4k camera orignals for the most part , with a few shots taken from the internal recorder and transcoded to 16float EXR before starting the grade . <p> Might it be better suited to camera refered gradeing like ACES , RCM , Truelight spaces than to display refered gradeing ? <p> ... On the advert i linked to i worked in ACES , only adjusted the exposure , contrast , pivot , saturation a small amount , no masks , no windows , no tracks , no luts , no nutt'n but a fairly fast grade with DVO Clairity on the base layer .. oh , and a whole lotta comp'n / reframeing to remove crew members , support vehicles , etc etc .. all that finishing work was done in DS after the grade tho ... 43971 @qwx453971 <p> And the general public think that 's the picture they 'll get with that setup - totally unaware of the work done to get it to this stage . Same problem I have with all the GoPro advertising @ @ @ @ @ @ @ @ @ @ and try it ! <p> I looked at that Northern Lights clip first on my laptop in 720p and it looked fantastic then in 1080p via WiFi on our 4K Bravia and wow , did it fall apart ! Of course Youtube streaming compression and up-rez to 4K takes it 's toll but the noise in those last scenes was alarming . It really looked ' toy Handycam ' . Is there any link to download a lower compression version ? 
@@44332817 @4332817/ <p> Here is my description , what do you think ? This is a silver and desaturated look . The general tone is cold . There is something very soft . The blacks are neutral and it seems that they are soften and a bit lifted ( as a black promist ) . The skin tone are very bright and a bit cool . The white are absolutely white and a little bit soften . I mean my description is enough to feel this silver and soften look ? Do you have special tips to achieve it ? <p> I totaly agree on the importance and dependance of the set , light , camera , wardrobe and so on on a spefic look . But in this case this is not the way to think because of the variety of the multiple sources , lightning ... The Soret project is a multi-source camera settings but he achieved a very homogeous and soft result as it could be . <p> But in short , to achieve this look , my project is far simple : I 've got just one @ @ @ @ @ @ @ @ @ @ on a natural light with little ajujstment with reflectors and projectors ... <p> I feel that there is a specific work on a colorgrading point with this beautiful result <p> It 's very desaturated with lifted blacks and raised mids , kind of thin , " soft " look , with kind of steely whites and upper mids , but also lit very deliberately with soft lighting and lots of fill and a full exposure . You ca n't pull that kind of look out of ( for example ) a high-contrast lighting situation that 's underexposed . So it will hinge on the lighting and the specific art direction and exposure . <p> Hi Paul this one is a bit of tricky , it was shot over about 3 weeks a lot of the shots in there a captured live , no setup no addition lighting <p> the camera used are phantom , arri and 5d at some points they were shooting on 10 camera at the same time , as the only slots they could get with the athletes where during their training session and these guys don @ @ @ @ @ @ @ @ @ @ in the olympics <p> some of the normal rules are sort of out of the window , yes the narrative section are obviously stages .. but as for the rest of what is and what isn , t ? ? ? <p> so looking at it from that point of view , it 's a master class in colour work bring all sorts of media shooting styles lighting etc ... basically everything under one roof and giving it a coherent tone and feel <p> jean clement soret , Tom Tagholm , Luke scott , MPC and crew literally knocked out of the park ..... not quite as good as the guinness surfer for me but close .. it 's benchmark work ... i , ll never be that good but one keep trying HA ! <p> the wheel chair shot you have posted was shot on the phantom <p> however interesting it would take me while while to get the look in the wheelchair shot in resolve ... i , m sure walter could whizz that out <p> however from a photographic standpoint which i can do that look is @ @ @ @ @ @ @ @ @ @ fuji , s camera which is turn is derived from the look kodachrome gives when its printed on matt paper <p> Ha ! walter i did n't mean for you to do it ... i was just pointing out you Uber colourists can get some like this way quicker then idiots like me 43971 @qwx453971 <p> And i do n't want to do it either ! ( unless you get a good bottle of whiskey in my bay and we laugh our ass off for an evening ) . <p> My idea was to see the material and understand the best approach : giving you some directions ! . If i can pass you information on how i will approach it , you might learn how to do it yourself or at least have another point of view . I learn more from colorists that show me things that i do n't know that repeating at nauseam things that i already know how to do it . <p> i like to work thinking of the output : how many stop of latitude i want to map , or in another @ @ @ @ @ @ @ @ @ @ material need to have . I like to work with LOG material as much as possible in the timeline. i do n't  like to do correction after the master LUT/Scurve/look I do like to put some very like grain on top . I do like to reduce the high sharpening kernels you get in the cheap cameras ( like go pro ) i do not like Denoise. never . even when I 'm forced to use it . <p> So , in the timeline node I will put a master lut/Scurve ( whatever you fancy ) that will bring the material within the tonal range i like . If your hero camera is red , I will use a redlog-to-bt1886/rec709 curve . ( P3 DCI for theatrical , or sRGB for web , whatever applies ) . <p> after that , i will build the " look " : in this case one node to reduce saturation ( by 30-40% ) , put the midtones and dark midtones in the cyan region ( straight opposite of the skintone line ) then add another node to lift the dark midtones and @ @ @ @ @ @ @ @ @ @ be neutral and white can lean just a tad in the blue side . a tad . Add a very soft and very light vignette that darken the outside a bit and does a very very slight lense blur to it . JUST A BIT . You might add a tiny weeny bit of glow if you fancy Add a tad of grain , very fine . <p> group the LOOK nodes ( not the LUT ) in a compound clip and mute it . <p> now look at the shot to shot and match them for contrast , color , saturation , tonal distribution and make it as good as you can . Turn the look compound node on often to see playback if works ... Resist the temptation to build the LOOK in every shot : it will not be consistent . In the shots i will approach like this : <p> 1 ) convert the tonal range of the camera to LOG ( redlogfilm in your case ) 2 ) adjust exposure and balance with offset/printerlight 3 ) adjust contrast with gain to match them 4 ) @ @ @ @ @ @ @ @ @ @ balancing the light between hig/mid/low distribution ( I almost never do ) 5 ) do your masking to enhance your subject and darken your unwanted attentions . if it is a commercial , you might want to defocus the unwanted object as well , not recommended for long forms . 5 ) if you have cameras with high sharpen like GOpro , kill some of the edge sharpen 6 ) if you have very wide angles with the gopros , use the lens correction to fix it as best as you can . nothing speak cheaper than a pinhole camera . <p> once the basic structure is done , playback over and over with all applied and make tweaks until dead ... <p> Whah Walter ! What a great and precise explanation ! Thank you so much for sharing it ! I 'm happy to see that I 've got more or less the same approach . I hate LUTs look . I prefer thousand times make it by hand ! The have only a special difference is the way to balance the shot . I 'm not using offset @ @ @ @ @ @ @ @ @ @ do the global settings with the curves on a raw material but I adjust the details with my LGG and curves . I 'm trying not to change the look group but it is a bit tricky . I will try another time . And I never think about the soft vignette it is a great idea ! ! ! <p> Here is a Tiff raw material ( 3 frames ) and I just give you a color test to see where I 'm going . If you can tell me what do you think about it ? For me the skin is to much grainy but if I want to recover the skin tones ... <p> I 'm working on Scratch Assimilate My raw is a dragonColor - RedLogFilm ( with the dragonColor the density of the color are better than the ColorDragon2 for this project in my opinion ) . I just Scurve the luma with the curves I 'm balancing the whites and blacks with LGG I illuminate the skin with curves and I just desat a tiny bit the skin tone I saturate a bit the @ @ @ @ @ @ @ @ @ @ luma curve not just the black gamma but all the shot . I 'm doing a sort of black promist to soften a bit the blacks . <p> I 'm doing it just before your comment . So ... i 'm not doing the vignette , the grain and ... <p> One of the issues of trying too hard to hammer a look using LGG is that you will inevitably stretch the signal too much if you overdo it , adding noise where you do n't want it . Then you add noise reducer to fix it , then it look plasticky and you do n't like it , then you will try to tweak and throw the general tone off ... You will chase your tail . <p> simple , keep it simple . <p> i do n't know how scratch works , but i assume it can work in layers : make at least 10 and do the basic contrast/look in the top 3. ( use a " s " shape curve if you do n't want hard clipping ) Once that is done , try to use @ @ @ @ @ @ @ @ @ @ will get where you want without using gamma at all and you will have a less noisy image to work with . <p> Great topic ! Getting in the heads of other colorists is such a great resource ! <p> Walter , I 'm really curios about how you 're doing your color separation . You speak about tinting the lower mids , while leaving the blacks clean . Are you using LGG before the LUT for that , or doing it with curves ? Or are you doing it in the main SCurve ? <p> I 'm normally used to doing it in the order - SCurve or LUT -&gt; exposure and main balance with printer lights under the LUT -&gt; then color separation using LGG after the LUT . But , as you say , it makes perfect sense that working before the LUT would apply in a less aggressive and organic way to the image . 
@@44332820 @4332820/ <h> Eizo ColorEdge CG247X - Yellow line on edge of screen <p> I 'm the happy owner of a new CG247X Monitor . So far I love the monitor although I 've only had a chance to calibrate using Color Navigator to do a basic calibration - will need to make some calls and use someone 's probe with Lightspace or Calman when I can ! <p> I 've hooked it up with a Blackmagic Ultrastudio Mini Monitor to HDMI to use with Resolve and Premiere . I 've noticed one quirk though which I ca n't seem to fix ... I really hope someone can she 'd some light on it even if it means pointing out what a noob I am at this ! <p> I noticed a line of yellow pixels on the left of the screen , right up against the edge when I hooked it up to the HDMI - this issue only shows up with HDMI . <p> When I play footage from Resolve or Premiere it 's always there . I figured out though that if I change my frame rate or timeline @ @ @ @ @ @ @ @ @ @ , basically 24p no line , 25p yellow line 1 pixel wide running all the way up and down the sceen ... Weird ... Any suggestions ? Would love to hear if anyone has the same issues ? <p> It could be the monitor 's internal processing but I could never get a proper video signal out of the BMD 3G mini-monitor HDMI output . See if you can borrow or buy a second hand HD Link converter and convert the SDI to Display Port . With my particular monitor ( not Eizo ) the difference was ' chalk and cheese ' . 
@@44332821 @4332821/ <p> Really ? Why ? I 'm not trying to be difficult , I 'm just trying to understand the reasoning behind that statement . 43971 @qwx453971 <p> Just a personal preference . I understand why they do n't and I love the Alexa too but I still feel like film draws me into the " fantasy " a little more . Not trying to have a film vs digital war conversation here , just my own personal preference . <p> Just a personal preference . I understand why they do n't and I love the Alexa too but I still feel like film draws me into the " fantasy " a little more . Not trying to have a film vs digital war conversation here , just my own personal preference . 43971 @qwx453971 <p> I say a comparison done by Steve Yedlin Asc , where red , Alexa , Alexa 65 , Sony , 35mm , IMAX , where shot in the same light condition and prepared in 4k for viewing in the same way . <p> once you add halation , jitter and grain , all @ @ @ @ @ @ @ @ @ @ know that it might be difficult to " believe " , but that is what I saw . <p> Walter , do you prepare every film that is shot digital with a separate halation , hitter , and grain pass ? Grain I can imagine but I 'd be surprised about the others . 43971 @qwx453971 <p> I would guess that for the vast majority of viewers - and certainly for those who do n't work in the industry and have a personal attachment to such things - characteristics like halation , jitter , grain , and , yes , dirt and scratches are generally considered to be defects that are distracting and degrading to the image . Not things to be desired and/or artificially emulated . <p> I would guess that for the vast majority of viewers - and certainly for those who do n't work in the industry and have a personal attachment to such things - characteristics like halation , jitter , grain , and , yes , dirt and scratches are generally considered to be defects that are distracting and degrading to the image . Not @ @ @ @ @ @ @ @ @ @ <p> I admit , I am completely biased . I grew up shooting film starting in Super 8 , then 16mm , super 16mm , 35mm 4,3 &amp; 2 perf . I still own an Arri 435es camera package which still works great ( albeit does n't see any action anymore ) . Like I said , I love the Alexa too and digital acquisition in general for the convenience and image quality . But , for me there is still something special about celluloid that I love . I am also a bit sad that a lot of the younger generation does n't have the discepline that came with film capture versus digital . For example , I recently colored a couple projects with a really talented young cinematographer/director who admitted that he had no clue how to work a light meter and was dumbstruck that when you stack ND on a film camera that what you see is not what you get necessarily and that setting your stop by light meter is required . But , here I am getting all nostalgic . I still feel like I @ @ @ @ @ @ @ @ @ @ most days of the week but it does n't really matter . Our opinions are our own . At least I can say mine is based on personal experience as opposed to our Youtuber . <p> The only perspective I agreed with is they should shoot the Marvel films on celluloid . Other than that everyone else has expressed enough to support my initial reaction to this . 43971 @qwx453971 <p> I think once you get the budget over $50 million , I do n't think it matters what you shoot it on . Film costs are negligible . But if there 's 2000 VFX shots in the show , I can see the advantage of just shooting digitally to begin with to eliminate the delays caused by having to scan millions of feet of film . <p> I say a comparison done by Steve Yedlin Asc , where red , Alexa , Alexa 65 , Sony , 35mm , IMAX , where shot in the same light condition and prepared in 4k for viewing in the same way . once you add halation , jitter and grain , all @ @ @ @ @ @ @ @ @ @ that it might be difficult to " believe " , but that is what I saw . 43971 @qwx453971 <p> We did similar tests at Cinesite and Technicolor and came to the same conclusion . I also saw the ASC 's STEM tests ( Standardized Evaluation Material ) , and everything was surprisingly similar , even back in 2003 : <p> I would guess that for the vast majority of viewers - and certainly for those who do n't work in the industry and have a personal attachment to such things - characteristics like halation , jitter , grain , and , yes , dirt and scratches are generally considered to be defects that are distracting and degrading to the image . Not things to be desired and/or artificially emulated . 43971 @qwx453971 <p> You would be amazed by the number of TV shows and features that deliberately add grain in ( real or synthetic ) to digital images . It 's not an aesthetic I 'd use for everything , but I do n't have a problem with it if it 's specifically requested by the DP or the @ @ @ @ @ @ @ @ @ @ that for the vast majority of viewers - and certainly for those who do n't work in the industry and have a personal attachment to such things - characteristics like halation , jitter , grain , and , yes , dirt and scratches are generally considered to be defects that are distracting and degrading to the image . Not things to be desired and/or artificially emulated . 43971 @qwx453971 <p> I add halation to maybe half of the films I work on . It adds somewhat of a romantic quality . It 's also a nice way of adding contrast while maintaining a soft quality . <p> Maybe Marvel movies a bit suck in a good way ( though I enjoy them very match ) , where is a big discovery . After watching--- just a bit of pop corn , and a question what it was about . 43971 @qwx453971 <p> I think Warner Bros. would desperately like to know how to make their films suck as much as Marvel . Marvel 's own films have made nearly $12 BILLION DOLLARS so far , and I 'd say the @ @ @ @ @ @ @ @ @ @ saw that video pop up on my Facebook feed last week , and I was expecting it to be dressed down here , but you 're all very upset about it ... <p> I understand disagreeing with everything he 's saying , because duh , but there 's no need to be that vindictive . He 's not presenting the video as an expert opinion . <p> I remember more than one thread on this forum about how modern looks have gotten flatter and how some of you think it 's ugly and miss the good old days of contrast and saturation . This is basically the same thing , except for the fact that he does n't have the technical or artistic knowledge to fully explain himself clearly and without contradicting himself . <p> Throwing your IMDb profiles at him is condescending . Feel free to message him to enlighten him , but being aggressive about it wo n't get your point across . <p> As far as the " nerd who needs to get laid " comments , someone might say the same thing about this thread ... @ @ @ @ @ @ @ @ @ @ A+ projection and could barely make out any shadow detail . It was causing me eye strain with its aggressive , desaturated , murky low-con look . Truly , style over immersion . <p> Professionals do n't always know best . Think how much music production changed throughout the 80s just because there were all these new toys to mess around with . Do n't even get me started on the loudness wars which destroyed the natural dynamics of music . <p> It is irritating when armchair internet pundits espouse lots of loud opinions , when I think a lot of it boils down to petty jealousy and frustration on their end . I can get an idle comment here and there , as in : " I liked the film overall , but there were some weird moments in that 10-minute night scene towards the end that were a little muddy . " That I can understand . But to sit down and make a 5-minute video takes a lot of time and trouble , and to me it just made it obvious that the guy does n't know @ @ @ @ @ @ @ @ @ @ always enjoy about good critics -- good in this case meaning people who are well-informed , well-educated and experienced , with some degree of cache and credentials -- is that even if I disagree with them , I understand why they feel the way they do . Noted film critic Roger Ebert was one of those : I did n't always agree with his judgement of certain films , but you could tell he was a terrific writer and had great passion for what he did . I could respect him even while disagreeing with him on occasion . <p> Unfortunately , the internet has unleashed a ton of morons who clog all available bandwidth their opinions , to the point where the signal-to-noise ratio gets overwhelming at times . There 's way , way too many critics who want to tear things down just to stir the pot , and it always attracts more attention to criticize something successful ( critically and financially ) instead of something obscure . I 'm sure the color-correction critic is gloating over all the attention . <p> Members here will recall there were @ @ @ @ @ @ @ @ @ @ " version of Man of Steel . I thought re-coloring the film was a fairly obnoxious thing to do and pointed out that what audiences saw on the screen where what the director and cinematographer intended . It 's not a colorist decision -- it 's a filmmaker decision . You can always re-edit , remix , and re-color something and make it different , but all you wind up doing is just imposing your will on somebody else 's work . For the record , I did n't like the way that film looked a lot of the time , but I think lead colorist Stefan Sonnenfeld gave the filmmakers exactly what they asked for . You ca n't expect anything less from the colorist . <p> There 's a difference between an opinion and an uninformed " teaching your father how to fuck " -video where you tell Marvel Studios how they shot with the wrong camera and how their post crew should be replaced because they do bad work . <p> There 's a difference between an opinion and an uninformed " teaching your father how to @ @ @ @ @ @ @ @ @ @ shot with the wrong camera and how their post crew should be replaced because they do bad work . 
@@44332822 @4332822/ <h> Overview of Quantel RIO V4 by Jamie Dickinson <p> A great breakdown on the new interface and features in Quantel RIO V4 by Jamie Dickinson . If you 've never seen RIO in use before , definitely take a look , especially at the editorial and conform tools . There is nothing else quite like it . <p> Nice one Jamie i like that colour wheel that reminds me of Captures one colour controls for stills ... if fact there i load of great stuff in RIO and just think you had it sat in POGO FILMs and i never had a chance to see it fly <p> Back when I was on the Pablo ( with the full Neo panel ) , that was the old much slower hardware , you could n't place a LUT in the middle of the cascades and it was n't 32 bit float . The new Rio cures all those problems and is super fast . The Neo panel was and still is great - pretty much every parameter or group of parameters can be bypassed or reset from a button @ @ @ @ @ @ @ @ @ @ while still being able to temporarily bypassed individual things . I was always a fan of a system which can do a bit more than just one thing - grade , edit , composite , etc. ( the AvidDS was a great machine too ) so it 's good news that they 're continuing to improve and develop the Rio . <p> Back when I was on the Pablo ( with the full Neo panel ) , that was the old much slower hardware , you could n't place a LUT in the middle of the cascades and it was n't 32 bit float . The new Rio cures all those problems and is super fast . The Neo panel was and still is great - pretty much every parameter or group of parameters can be bypassed or reset from a button . You can do a lot in a single cascade while still being able to temporarily bypassed individual things . I was always a fan of a system which can do a bit more than just one thing - grade , edit , composite , etc. ( the @ @ @ @ @ @ @ @ @ @ good news that they 're continuing to improve and develop the Rio . 43971 @qwx453971 <p> True that ! ! I started on the iQ and watched the development going in a positive line . Now we have 2 Quantel Rio 's ( Neo + Nano-panel ) , 1 Pablo and 1 Rio Assist . <p> I loved working on eQ a couple years back . Although it was a rather old piece of hardware , it worked super solide and amazingly fast . It could n't compete with Flame in terms of compositing , but when it came to conforming and finishing it would just fly . I always wanted to get back on track with Rio , but did not yet find the time for that , sadly - so thanks for posting / creating that overview ! <p> Wrt the Neo panel : I think it 's the best and most thought-through panel on the market : <p> - it is super ergonomic with the placing of the keyboard , wacom and the two rows of controls - not overloaded with tech ( looking at you , @ @ @ @ @ @ @ @ @ @ distinction between the ( color coded ! ) controls - all-time-accessible ( ! ) keyboard - no need to ask yourself where to put your keyboard *around* the panel - keyboard has full keydrop - not a laptop keyboard - you can swap keyboard and transport modules - reset buttons for the encoders are actually independent buttons inside ( ! ) the encoders , and not the encoders themselfes ( one thing that is pretty not-nice with other panels , where you need to press the actual encoder and might accidentially turn it again after resetting it ) - it 's a matter of taste - but I like the design and color of it <p> And on top of all that - the panel is much cheaper than people think . Last time I checked it was 27K ( though not sure if it was pounds , or euros ) and you got the Rio software with it <p> Quantel also provides intruction PDFs to allow yourself to build your own Rio-workstation , so you do n't need to necessarily buy the turnkey . The system got much more @ @ @ @ @ @ @ @ @ @ it 's pretty much ahead of most color systems in terms of resolutions above 4K , stereoscopic workflows , HDR , and basic compositing . <p> Jaime , that 's a great video ! And its really great that they are pushing Rio forward with so many new features ! <p> The New Tracker from what I 've seen Alchemist is Awesome ! New UI is very cool Color spaces and color handling is very awesome too . <p> Love the look of the Neo Panel ( so pretty ) and what an ingenious idea to have the panels switchable ( Keyboard etc ) ! I also love some of their panel mapping ideas . That said there are a few things about Rio that are a little odd to me ( but is n't that every system ) <p> - Some functions require a Keyboard Modifier i.e CTRL , there is only one CTRL button on the panel and if you have the Keyboard on the opposite side its a little awkward . - No way to see inside outside/softness unless you are have matinee selected ( that should be @ @ @ @ @ @ @ @ @ @ section is the heart of a grading system and that section is rather small in my opinion . Which means once you get past a few cascades you get into scrolling to see cascades . If i remember correctly 6ish cascades and you 're into scrolling ? - No copying and pasting cascades from other shots ? - No way to see what cascades are on other shots ( or no way to recall just a select cascades from another shot ) - Limited compare modes - Tesla cards are awesome for 8K but Titans and Quadros are very powerful cards and they do n't require noisy boxes due to the passive cooling of the Tesla Cards - Kona 4 is a great card for everything up until 4k Rio should n't require the other high end card Kona card for 4k work <p> Their roadmap looks very promising and they seem to be making great strides very quickly . Not to mention Danny Peters is a super nice guy and is steering development in the right direction ! <p> I was a very big user of Pablo/Rio a few @ @ @ @ @ @ @ @ @ @ its development for grading work . Have to say I loved a lot of its workflow and features at that time . ( The majority of feature films I graded , especially in the earlier years , were Pablo/Rio . ) At the time it really was the only Hero Suite available . <p> And that is the workflow I loved - not just grading is isolation , but incorporating vfx , paint , graphics , and more , to get the best possible end result for the client . A lot of the feature films we did in Bombay and Madras were ideal , as we were able to do a lot of the songs directly within the system , without using external vfx systems ... <p> The only reason I stopped using Pablo/Rio was the development stalled , and someone inteoduced me to SGO 's Mistika . <p> Mistika became to ultimate Hero Suite based on my workflow requirements . <p> With luck , the new Pablo/Reo developments will help reposition Pablo/Rio , but as yet I have not been able to evaluate the changes for myself - @ @ @ @ @ @ @ @ @ @ that is very unlikely to happen . <p> However , Jamie does know his stuff , so I do appreciate his input an views . Maybe SGO should get him to do a similar evaluation of the latest Mistika developments too ? <p> Thanks Steve - funnily enough I did a run through of Mistika a couple of years ago for Chris Maynard at cmivfx.com . He encouraged me to try doing a video tutorial and with his help I did a beginners guide to Mistika which is on his website ( a bit out of date now though ) . I was shocked recently to hear that Chris had passed away . He was a really nice guy , incredibly enthusiastic and encouraging . Very sad . <p> I really like using systems which do a bit more than just one thing - Rio , AvidDS , Mistika ( and probably smoke too but I 've not used that ) . They all have different strengths and weaknesses in different areas . And of course Resolve should probably now be added to that list . Pablo/Rio has a really @ @ @ @ @ @ @ @ @ @ of them for years - it 's just that many people did n't know about it . <p> I thought my original video was a bit too long so I 've broken it down into sections which might be more easily digested - each section has links to all the others . Here 's a link to the section on the keyframe processing tools in Rio , which I always thought were pretty cool . <p> I thought my original video was a bit too long so I 've broken it down into sections which might be more easily digested - each section has links to all the others . Here 's a link to the section on the keyframe processing tools in Rio , which I always thought were pretty cool . <p> i do n't know if i really want to comment , it might be because I 'm at home sick with the first flu of the season ... <p> the new Rio is not impressing me. and I 've been on the platform sobe 2002. the color corrector did not have any improvement inge last 8 @ @ @ @ @ @ @ @ @ @ to test this week ) it is where it should have been about 10 years ago . <p> the keyframe lack of a dope sheet to easily move things around , just to say one thing , and do not get me started with the 32 bit math . <p> the box still crash randomly just playing back rendered material and it take the perpetual restart . <p> i presented a 25 page document few years back of the features that where broken/missing and only a fraction was implemented . 
@@44332823 @4332823/ <p> Technically the out of the box Rec 709 preset it 's a good starting point . However , you 'll need to calibrate the monitor after a while . I also own a CG246 but i started immediately with a calibration via Color Navigator . <p> Could somebody point me to an article about the basics and - more important - the philosophy and basic theory of monitor calibration ? <p> I thought that what the Decklink card transmits is an image in the Rec709 colorspace , and the Eizo gives a true representation of this image due to the fact that it can represent this colorspace and that it is calibrated in the factory to do so . <p> I guess my real ( newbie ) question is : why do I have to calibrate my monitor ? <p> First - EIZO monitors is produced for mass market in big quantities . And I do n't  think they have time to carefully calibrate every single device . Second - even if they do , every monitor drift over time . So factory calibration will have visible errors @ @ @ @ @ @ @ @ @ @ loaded with tons of useful info . Lightspace CMS looks impressive but does n't fit my budget unfortunately . So for now I 'm going with the ColorNavigator software that shipped with my Eizo . <p> out of the box the presets are somewhat accurate but ( as any screen ) the Eizo drifts fast , so these presets will be of no use ... these screens are meant to be calibrated and with the internal 3D LUT storage u can good/great color performance from them ... best way to do it is via Lightspace ... <p> see here for a full Eizo calibration guide and a comparison between Lightspace calibration and ColorNavigator calibration : <p> If I do a manual calibration using ColorNavigator ( which has its limits , I know ) and I set it to a luminance of 120cd , gamma of 2.2 and whitepoint of D65 ; then my monitor will still display colours that are out of gamut for Rec709 , right ? I presume the Eizo has a larger gamut than Rec709 so how do I calibrate it in a way that it displays @ @ @ @ @ @ @ @ @ @ seeing this wrong ? <p> u need to define a color target in CN when calibrating , e.g. Rec 709 ... IIRC , this is the first thing u need to do when setting up a cal ... u can do this by entering the color gamut coordinates manually or choosing to copy the coordinates from one of their presets ... in the following steps u then set gamma , luminance , black level etc ... <p> the Rec 709 3D LUT that is then stored inside the Eizo will map all colors to Rec 709 gamut ... 
@@44332824 @4332824/ <h> Strange Rec709 out of gamut points on Pana dx900 <p> I just updated my Mistika grading room with an impressive 58DX900 client display . It has great picture although it has poor screen uniformity easily visible when sending solid colors to the screen . <p> I made a manual calibration to rec709 with X1 display pro probe , Mistika as patch generator and the free LS version making continous measurements at the ten point gamma , white balance and primary options of the Pana . The post calibration profile has good delta e values but there are strange gamut values way out of Rec 709 gamut at low luminance levels . <p> Maybe Steve could chime in here . I 've noticed that with that setting ( adaptive backlight control ) the black level is 0 and I could n't use offsets I created with an i1pro . But when you switch it off the black level is quite high . <p> I saw this display is widely used as client monitor because of the screen quality . I don-t think this is an issue as it only @ @ @ @ @ @ @ @ @ @ it-s a faulty unit I prefer knowing it . Any other owner had the same issue ? <p> I have the same display and it 's absolutely fine after some use of the 10 point white control and LS . No strange out of gamut points here . For the actual measurement I 'd shut of backlight control , too . <p> I-m still deciding which lut box to pick up . The teranex mini would be the best option because of 4k capabilities ( not a must ) but I read that it-s too noisy . And my Blackmagic Hdlink pro is feeding both displays ( Pana and Eizo ) from SDI to HDMI so I can not use it as a Lut box . <p> The Teranex is not noisy anymore Zoltan , but beware that I ca n't get 10bit out of the Teranex to my Eizo . It would serve as the perfect LUT box for my Panasonic or Eizo , too . But without it being able to deliver 10bit via it 's HDMI out , it 's less usable than the HDLink . <p> @ @ @ @ @ @ @ @ @ @ your profile as those in mine . <p> Your calibration is much better than mine very good one , I will have to fine tune mine a little bit . The 3d lut is the way to go , it will be my next step . This week later on a Panasonic guy is going to call me as he is out of town . I 'll keep you informed . <p> Only Panasonic can tell you the actual backlight technology . It may be RGB LED , or it may be GB-r LED , or even WLED ? ( I would have expected a full array for a high-end display though , not edge lit . ) <p> As for the profile results , it does show that the display has poor internal colour management . Panasonic are one of the few manufactures not using LightSpace for factory calibration , and the issues actually look similar to the errors FSI had some years back , before they swapped to LightSpace ... the calibration is allowing over-gamut excursions when the display illumination is low , with the gamut tracking back to the correct values as the luma increases . 
@@44332825 @4332825/ <h> 1080 ProRes out of Premiere looks different than native 4k file in Resolve <p> I 'm doing some tests on a new camera and 4k workflow and I found out something annoying . The camera is a sony a7s II , and my plan is to shoot in 4k which is 4:2:0 then edit in Premiere on a 1080 timeline and output a 4444 ProRes to then color in DaVinci . <p> Now , in theory , squeezing the 4k 420 footage into a 4444 ProRes sounds like a very good idea , right ? The problem is , it turns out I get a different look in DaVinci if I import the 1080 ProRes rather than the native 4k clip . Both clips are on the same DaVinci timeline which is set to 1080 of course . <p> I just quickly made a test shoot of my girlfriend and here are the results : <p> 4k out of camera - slog <p> 4k out of camera - slog to lc709 lut applied <p> 4k out of camera - vectorscope ( 2x ) <p> 1080 ProRes 4444 from @ @ @ @ @ @ @ @ @ @ 1080 ProRes 4444 from Premiere <p> noticeable difference in saturation and hue <p> What 's the reason behind this ? Do you have any suggestions on how to approach it ? I know differences are n't huge but that choppy waveform looks bad .... <p> Did you have Render at maximum bit Depth , Maximum Quality turned on when you output from Premiere ? I know ProRes 444 should imply 12 bit ( or is it only 10 ? ) but perhaps Premiere is outputting a preview quality 8bit unless forced not too ? <p> Thank you all for your support ! I kept on testing and came to the conclusion downscaling 4k 420 footage in premiere pro does n't end up in a 1080 444 . It really seems like premiere looses some data while downscaling it so the final result still looks 420 . I tried every bit depth and maximum quality combination in media encoder , to no avail . On the other hand when downscaling is done in DaVinci Resolve it does a great job and 4k 420 indeed behaves like 444 when becoming 1080 ... @ @ @ @ @ @ @ @ @ @ best workflow to preserve image quality till the very end of the chain <p> I would keep everything at the original resolution , master it and then from this master make a 1080p downscale. 43971 @qwx453971 <p> That would work like a charm if I only worked with 4k footage and 1080 output without needing to crop , but sometimes I like to crop 4k to 1080 instead of scaling . Moreover , when I use slomo the camera records to 1080 only ... Upscaling 1080 to 4k in the timeline , rendering from Premiere to 4k , then downscaling to 1080 in davinci ? That sounds like my next test ... even tho I do n't feel comfortable upscaling 1080 stuff to 200% ! <p> I 'm doing some tests on a new camera and 4k workflow and I found out something annoying . The camera is a sony a7s II , and my plan is to shoot in 4k which is 4:2:0 then edit in Premiere on a 1080 timeline and output a 4444 ProRes to then color in DaVinci . <p> I still do XML to Resolve @ @ @ @ @ @ @ @ @ @ anything with warp stabilizer , or effects gets rendered out to DPX/DNxHD/DNxHR and put back on the timeline . Most of the time I 'm doing color while VFX are still being worked on and then I am the one dropping them on to my Resolve timeline . I do not treat resizes as an effect , I redo them in Resolve if they do not carry over . Unless they are keyframed , then I render them out and lay them back in . <p> Adobe has made it really easy to integrate After Effects with Premiere but rendering out VFX clips and bringing them back in later is the only way I can stay out of trouble myself . Dynamic link is asking for trouble IMHO and also slows down the premiere project waaayy to much for me . Do n't know if you use that or not . I will admit I will sometimes duplicate a clip&gt;dynamic link it to After Effects&gt;then delete the AE Comp in premiere as a down and dirty way to get a clip/clips into AE really quickly while still leaving the source @ @ @ @ @ @ @ @ @ @ a plug for Mixing Light as Patrick 's videos on comforming/prepping XMLS are worth a month subsciption alone . <p> Hope this might help in any way , keep in mind there are a lot of different ways to do it . <p> I still do XML to Resolve so that I can work with source material , but anything with warp stabilizer , or effects gets rendered out to DPX/DNxHD/DNxHR and put back on the timeline . Most of the time I 'm doing color while VFX are still being worked on and then I am the one dropping them on to my Resolve timeline . I do not treat resizes as an effect , I redo them in Resolve if they do not carry over . Unless they are keyframed , then I render them out and lay them back in . <p> Adobe has made it really easy to integrate After Effects with Premiere but rendering out VFX clips and bringing them back in later is the only way I can stay out of trouble myself . Dynamic link is asking for trouble IMHO and also slows @ @ @ @ @ @ @ @ @ @ Do n't know if you use that or not . I will admit I will sometimes duplicate a clip&gt;dynamic link it to After Effects&gt;then delete the AE Comp in premiere as a down and dirty way to get a clip/clips into AE really quickly while still leaving the source clip in the track below it . <p> Lastly , a plug for Mixing Light as Patrick 's videos on comforming/prepping XMLS are worth a month subsciption alone . <p> Hope this might help in any way , keep in mind there are a lot of different ways to do it . 43971 @qwx453971 <p> Thanks for the tip Zach , I very often use dynamic links because of how convenient the workflow becomes , but I 'll try rendering AE clips the old fashioned way and conforming the rest in Davinci , I guess it 's really worth it picture quality wise <p> If it needs any major WB or exposure changes ill tweak the metadata before exporting it but I usually do n't have to . In a perfect world , I 'd want any effects that get baked @ @ @ @ @ @ @ @ @ @ RedlogFilm . <p> ... Too bad this makes me wonder what is the best workflow to preserve image quality till the very end of the chain 43971 @qwx453971 <p> Number of clients shoot with these 8 bit cameras so we recommend a transcode to ProRes 4444 ( using dedicated transcoders ) before the do any edit if only to get proper timecode and reel numbers . Makes any conform so much sweeter . 
@@44332828 @4332828/ <h> Pantone announces the , restore and renew . Illustrative of flourishing foliage and the lushness of the great outdoors , the fortifying attributes of Greenery signals consumers to take a deep breath , oxygenate and reinvigorate . <p> Greenery is nature 's neutral . The more submerged people are in modern life , the greater their innate craving to immerse themselves in the physical beauty and inherent unity of the natural world . This shift is reflected by the proliferation of all things expressive of Greenery in daily lives through urban planning , architecture , lifestyle and design choices globally . A constant on the periphery , Greenery is now being pulled to the forefront - it is an omnipresent hue around the world . 
@@44332829 @4332829/ <h> 3D qualifier always apply to whole image instead of selection <p> It 's driving me nuts . When I display the selection of the 3D qualifier in highlight mode I see what I have selected . Great key . BUT when I get out of HL mode to see the image , the adjustment takes place ALL OVER the image like there is no selection at all . Am I missing something here ? ( And yes the swatch I just drew is activated in the qualifier tab ) 
@@44332830 @4332830/ <h> Please help with purchase of monitor for grading <p> 1 . You will learn it in a couple of days . It 's easy . 2 . Knowing how to calibrate is IMO a must for a colorist . It gives you a lot more knowledge about picture , display technologies , etc . Not to mention that it will give you A LOT more confidence . PLUS it 's fun ! 3 . Sending a 21 " monitor in a box with a courier service is not cheap . Consider that you should calibrate a couple times a year . 2 times or more . Lightspace + i1D3 is cheaper than sending the monitor to fsi. ( you also need a spectro but you can just use it once to create offsets ) . <p> Before you make a purchase do compare panel characteristics . Uniformity , black level and wieving angles , etc . Also the gamut differs a lot between AM210 and cg247x/cg2420 . If you are spending a lot of cash than do n't compromise . AM210 is a big compromise in my opinion @ @ @ @ @ @ @ @ @ @ you buy . I presume that the Eizo has a better panel than the low budget AM210 . <p> Sanjin , I have some experience with calibration . My problem is than I do n't  know who to trust because there are so many articles , workflows etc. and every manufacturer is claiming that their product/method is the right one . We have claims here that Eizo probe and their software is bad , how do we know that is the true ? I mean its not some small shed company who is producing monitors since last March , it is renowned brand with almost 50 years of experience . But , I agree FSI is FSI but low end model might not be a right choice . If I could afford dm240 I would buy it in a heart beat <p> For calibration you have Calman , Lightspace and agryll . Agryll is free . I recommend Lightspace as I use it myself . Argyll is also nice but more confusing to use . Do n't  know Calman . But it does n't  have a good rep here . @ @ @ @ @ @ @ @ @ @ It starts with i1D3 that is quite cheap . Than you need a spectrophotometer to create offsets for the colorimeter . You can borrow an i1 Pro for a day and create those . <p> The inbuilt probe in the Eizos is not accurate enough . That 's all . Plus you probabaly cant connect it to Lightspace or any other software that can create a 3D LUT . Also the probe is located at the edge of the srceen . Which is the worst place to place a probe . <p> I have some experience with DisplayCAL ( have it installed and using it for monitor calibration ) and i can borrow xrite i1 display pro probe . I 've also played with DisplayCal calibration through Resolve and I think I got pretty good results on my old Dell u2410 . I am using that lut as a 3d video monitor lut in Resolve at the moment <p> I did it long time ago so I forgot but I have to re-calibrate it soon now that I know bit more about the subject ! I 'm really beginner regarding calibration but I @ @ @ @ @ @ @ @ @ @ mainly motion graphics , compositing and grading ) so i now few things <p> The PVM A250 is neither cheap nor good ; profile with a decent probe and look at the grey scale tracking If you have PVM money I 'd buy a Boland BVB25 or the Flanders ; equivalent . I spent a lot of time calibrating all three at numerous Soho facilities and the Boland is my favourite . <p> My experience with the Sony and Boland OLEDs is quite the opposite . I found that the Boland would clip the negative pluge on SMPTE bars at baseline black ; you could n't discern the pluge below black . Also , try this experiment and see if you get similar results as me : Take a camera and aim it out a window , ideally a scene with multi-tonal content . Balance for daylight using the color temp control on the camera . So now your white point is somewhere around 6000 Kelvin . Now take the color temp control counterclock lowering color temp while watching both the Sony and Boland . In my experience , as @ @ @ @ @ @ @ @ @ @ ) , the neutral midtones in the shot take on a decidedly cyan cast on the Boland while the Sony maintains a kind of steel blue cast ( as one expects ) . In other words the Sony tracks the scene-referred content with more photographic accuracy than the Boland . I 'm curious to know what kind of results you get . <p> Both Sony and Boland are out of price range but your posts are just confirming my main problem regarding calibration ! Everybody have a different opinion , technique and measuring methods and everybody is claiming that their monitor is calibrated properly . That is driving me insane ! <p> Both Sony and Boland are out of price range but your posts are just confirming my main problem regarding calibration ! Everybody have a different opinion , technique and measuring methods and everybody is claiming that their monitor is calibrated properly . That is driving me insane ! 43971 @qwx453971 <p> Whether it is 3pt/6pt or 3D LUT cal , the calibration approaches ( that make sense ) are all the same . More importantly , there are @ @ @ @ @ @ @ @ @ @ All rather easy . <p> The only thing that can/will differ is the pre-cal setup of the individual screen . <p> The PVM A250 is neither cheap nor good ; profile with a decent probe and look at the grey scale tracking If you have PVM money I 'd buy a Boland BVB25 or the Flanders ; equivalent . I spent a lot of time calibrating all three at numerous Soho facilities and the Boland is my favourite . 43971 @qwx453971 <p> would be interesting to hear your experience on why you prefer the Boland over the CM250 . I get the CM250 to an avg of 0.43 dE2000 - that is over a 2,000 pt validation patch set . <p> What are your results with the Boland ? Any calibration reports you can post ? 
@@44332831 @4332831/ <p> It adds ProRes decoding on Windows and no longer requires QuickTime 7 for Windows . So Windows users can now run Resolve without installing QuickTime . Trimming and multicam performance is also improved as DaVinci Resolve 12.5.1 now uses its own native 64-bit code to read and write . mov files . In addition , Mac OS X , Windows and Linux users all get new ResolveFX plug-ins with improved GPU acceleration . This update also adds support for Sony X-OCN , DNxHR 444 alpha channels , AVCHD . mts clips and more . <p> There is a database upgrade required for this release so we recommend that you back up your database and any active projects before installing this update . <p> Peter <p> What 's new in DaVinci Resolve 12.5.1 <p> Edit Improvements Improved 2-up and 4-up , multicam and playback performance when using QuickTime ProRes on Windows Added menu items to allow selection of clips based on Flag , Marker and Clip colors on the edit timeline Added ability to import and export duration markers using EDL Added the ability for clips to snap to @ @ @ @ @ @ @ @ @ @ consistency of edit functions when Timeline is in full screen mode Added support for box wipe mode for offline reference wipe Added ability to extract AAF import log information as timeline markers <p> Color Improvements Improved performance for Spatial Noise reduction in Better mode Improved listing of attached and timeline mattes in the node graph with support for alphabetical listing Added ability to apply grades from a reference wipe using the viewer context menu Added ability to align keyframe timelines of color grades using playhead position and wiped still frame Next node and previous node operations now loop around the node graph The displayed ' node graph now automatically updates when the current still is changed Swapping nodes now also swaps the node labels Rippling grades now also copies node labels , Power Window labels and node cache settings Shift Up + Next Still will now append grade from the current still on the advanced control panel ( Studio version ) Added support for left eye and right eye grades for the timeline node graph ( Studio version ) Added ability to convert a mono timeline into stereoscopic 3D ( @ @ @ @ @ @ @ @ @ @ into stereoscopic 3D ( Studio version ) Added support for sequence and node render caching for stereoscopic clips and timelines ( Studio version ) Added compensation for stereoscopic slip when exporting timecodes in ALE ( Studio version ) Added support copying DolbyVision grades using stills and middle click ( Studio version ) Improved 3D and Qualifier panel layout in dual screen mode Improved behavior to stay on the same frame when joining two clips <p> Codec &amp; Format Improvements Added support for ProRes decode on Windows Improved QuickTime decode and encode performance on Windows Support for UHD H.264 renders on Windows Improved QuickTime decode and encode performance on Linux Added support for HEVC ( H.265 ) decode on Linux Added support for alpha channel in DNxHR 444 Added support for the Sony X-OCN format Added support for various AAC encode parameters on macOS Added support for QuickTime AAC audio encoding on Windows Added support for encoding QuickTime Photo JPEG files Added support for decoding AVCHD files from the Sony NEX-FS700 camera Added support for V-Gamut in RCM for improved Panasonic camera image handling Added ARRI LogC to Linear and Linear @ @ @ @ @ @ @ @ @ @ last frame in some Panasonic AVCHD clips Improved handling of AVCHD . MTS clips Added support for RED SDK v6.2.1 <p> Someone just mentioned on Red , that optimized media is generated using Prores on Windows . Obviously , there is no technical reason why Resolve on Windows ca n't write Prores . It 's strictly an Apple 's business decision . Anyway , this new ability to generate optimized media using Prores now proves it even more , that technically Resolve is perfectly capable of doing it . So , here is my question . Can Resolve also generate Prores Cache ? If you can generate cached media in Prores , can we just grab that and use for deliverables ? On Nucoda you can tell NLE , here is the folder where your cached media is and use XML/EDL/AAF . Unfortunately , unlike Nucoda , resolve does n't assign a simple name of the project to a single folder , so finding the cache folder is more difficult , but not impossible . NLE then finds all media in the cache folder and reassembles the timeline using that @ @ @ @ @ @ @ @ @ @ point it to the folder where optimized Prores media is and PPro should reassemble the Prores timeline . Use something like Mirazon , that allows writing Prores on Windows and now you can very quickly deliver Prores on Windows without doing an unnecessary intermediate render . <p> Someone just mentioned on Red , that optimized media can be generated using Prores on Windows . Obviously , there is no technical reason why Resolve on Windows ca n't write Prores . It 's strictly an Apple 's business decision . Anyway , this new ability to generate optimized media using Prores now proves it even more , that technically Resolve is perfectly capable of doing it . So , here is my question . Can Resolve also generate Prores Cache ? If you can generate cached media in Prores , can we just grab that and use for deliverables ? On Nucoda you can tell NLE , here is the folder where your cached media is and use XML/EDL/AAF . NLE then finds all media and reassembles the timeline using cached material . So , say you have PPro , point @ @ @ @ @ @ @ @ @ @ PPro should reassemble the Prores timeline . Use something like Mirazon , that allows writing Prores on Windows and now you can very quickly deliver Prores on Windows without doing an unnecessary intermediate render . 43971 @qwx453971 <p> Last time I looked , few hours ago , I was able to choose pro res as cache format , on my win system . However , cache files are not quicktime files nor standard image sequences , so I do n't think your idea will work . <p> Last time I looked , few hours ago , I was able to choose pro res as cache format , on my win system . However , cache files are not quicktime files nor standard image sequences , so I do n't think your idea will work . 43971 @qwx453971 <p> Well , now that makes no sense . I understand no Prores render on output , but also no Prores on cache , but yes on optimized media ? So , you optimize your media to Prores , but cache to DnxHR/HD at the same time ? Weird ... <p> Well , @ @ @ @ @ @ @ @ @ @ render on output , but also no Prores on cache , but yes on optimized media ? So , you optimize your media to Prores , but cache to DnxHR/HD at the same time ? Weird ... 43971 @qwx453971 <p> I can cache in prores . It creates ' . dvcc ' files . In HD Pro Res HQ setting gives a frame size of about 8 MB , in Pro Res QX it 's about 12 MB per frame . <p> Someone just mentioned on Red , that optimized media is generated using Prores on Windows . Obviously , there is no technical reason why Resolve on Windows ca n't write Prores . It 's strictly an Apple 's business decision . 43971 @qwx453971 <p> This could probably change one day if current pro models would be eol'ed for good maybe ? <p> I also do not see why Resolve could not write prores as it is done with sdk and not qt32 any more . Maybe i 'm mistaken but this is how it looks to me for now . 
@@44332832 @4332832/ <h> Full Set of Tangent Element panels $2600 <p> I am selling my Tangent Element Panels . I bought them back in June and only used them on a couple small projects . They are in great condition and I still have all the boxes . I 'll even throw in a powered USB 2.0 HUB . $2600 . Panels are located in Hollywood , I can ship them out or they are available for pick up . Buyer pays shipping . 
@@44332833 @4332833/ <p> I honestly do n't know where you got the idea that Lustre is only used on features , or that it is " too slow " for broadcast shows , especially when you know better . 43971 @qwx453971 <p> I had a long talk with Pankaj Bajpai , and in fact he actually let me kibbutz on some Lustre projects for a couple of days at Technicolor when he was there . His take was it took him an additional day to do HBO projects on Lustre , but that the extra time was necessary and his clients did n't have a problem with it . However : this was during the daVinci 2K days , before the Resolves came in . My understanding is that the big prestige 1-hour shows will go beyond the typical 2-day color/approval schedule . ( Game of Thrones does 5 days over at Chainsaw , so it 's very much a feature-style session . ) That 's a world beyond the indie feature/reality show/commercial field in which the vast majority of colorists here exist , where the one-man-band is the reality . @ @ @ @ @ @ @ @ @ @ Lustre can work great when you have about five shadow assistants working in the background to support it . In truth , there are quite a few Resolve shops ( like CO3 and I think Picture Shop and the old Modern ) that do the same thing with Resolve . I think the panel will get you there faster , and I have tried Lustre with a mouse and keyboard and in fact was trained that way at Autodesk with the lovely Lyne Lepage in Santa Monica for 3 or 4 days . It was her opinion ( as well as the Lustre product manager ) that the program was more efficient with a control surface , but I think VFX artists oriented to that way of working are more comfortable with that style . <p> I know the original question was Pablo Rio vs . Resolve , and I think you can make some good arguments either way . Baselight , Lustre , Mistika , Nucoda , Pablo Rio , Resolve ... lots of pros and cons . If cost was absolutely no object , I 'd love to @ @ @ @ @ @ @ @ @ @ a week on a $250,000 Resolve and see what emerges from the dust . If it boiled down to building 6 Resolve rooms vs. 4 Baselight rooms , you could make a good business argument for the former . I bow to Walter 's expertise with Pablo , since he worked for Autodesk for years as an engineer and trainer and knows the product inside out , better than any colorist I know . <p> I 'm 100% with Walter 's opinion above : Lustre can work great when you have about five shadow assistants working in the background to support it . In truth , there are quite a few Resolve shops ( like CO3 and I think Picture Shop and the old Modern ) that do the same thing with Resolve . I think the panel will get you there faster ... 43971 @qwx453971 <p> This conversation was not about panels , it was about the programs themselves . You stated that Lustre is so much slower than the others that you wanted someone to let you know when television shows with a short turnaround ( BTW , @ @ @ @ @ @ @ @ @ @ ) were being done on it , and I replied with a rather long list of examples . Pankaj has his opinion . You have yours . I have mine . But they 're all just opinions. , and all based on personal experience and personal affinity towards these products . If we actually want to talk about results , I would say , with a great deal of confidence , that if I gave the same show to Gareth Cook on Resolve , Steve Porter on Nucoda , and Larry Field on Lustre ( who works with the Autodesk panels , BTW ) they would all get it done in roughly the same amount of time and with a similar high level of artistry . They all have a deep understanding of their respective tool sets and can work very efficiently with them . That 's all that matters , because these things are just tools . They are not " better " or " worse " in themselves . <p> And you do n't need an army of roto artists to do a project on Lustre just because @ @ @ @ @ @ @ @ @ @ to operate Lustre solely with the keyboard and mouse for the same reason ... <p> If we actually want to talk about results , I would say , with a great deal of confidence , that if I gave the same show to Gareth Cook on Resolve , Steve Porter on Nucoda , and Larry Field on Lustre ( who works with the Autodesk panels , BTW ) they would all get it done in roughly the same amount of time and with a similar high level of artistry . 43971 @qwx453971 <p> I agree totally in that it 's more about the colorist than anything else . Still , let me see somebody get through a 800-shot 1-hour scripted network show in two 8-hour days with a keyboard and a mouse on Lustre , completely by themselves without assistants , supervised by a roomful of clients , and I 'll buy them dinner at Nobu . And a nice bottle of wine . <p> I 've ben using quantel product for 16 years . sorry but no , RIO kinda sucks with the pen , most of the control @ @ @ @ @ @ @ @ @ @ <p> Personally , I like having a pen in my hand in conjunction with a panel . That 's what 's nice about the Pablo/Rio Neo panel , the tablet is right there in the middle . Probably the Blackboard is nice for the same reason . <p> I 've ben using quantel product for 16 years . sorry but no , RIO kinda sucks with the pen , most of the control are too broad and " slippery " . 43971 @qwx453971 <p> I think the Rio works great with the pen . Obviously , working with the Neo is better . <p> But the GUI design is really thought out for pen use . The ability to change values by rolling circles over them , having a number pad pop-up , being able to increment by any value with up and down arrow ... All these make using pen only quite fast . <p> Still have n't figured out how to do a still store wipe with pen only though . <p> Still have n't figured out how to do a still store wipe with pen only though @ @ @ @ @ @ @ @ @ @ of software called StrokeIt that allows you to map any keyboard shortcut to a pen stroke . I am finding all sorts of uses for it , especially since it recognizes the software you are using , and allows you to set how it behaves within that software . Worth taking a look at if you like using a pen . 
@@44332834 @4332834/ <p> Yes if your special Mom bought you an Alexa and a set of master Primes and some Optimos and a Codex and some other super duper spendy stuff you would then be something .... not sure what but you better start getting some rental money to pay Mom back . <p> Oh , my ! I 've learned so much from this video ! I 'd better go buy some extra deep black LUTs right now ! And recalibrate my reference monitor 's blacks so that they start at 10% grey . That way I can decrease the value of the shadows even further until they look blacker than black . On , no ! My avatar needs deeper blacks , too ! What a nightmare ! ! ! Only the RED gods can save me . <p> Do n't be gentle , it 's a rental . ( By the way , I seen so much more Red material which is better then Alexa . Just because for every Alexa there are 6-7 Red 's around me . And half the jobs with Alexa --- it is @ @ @ @ @ @ @ @ @ @ many Red jobs were transcoded in one go and never went back to raw . ) <p> Lotsa Facebook criticism of the guy . I checked his IMDB resume , and let 's just say I 'm not impressed -- it looks like he 's been on 5 student films in his career . <p> Anybody is entitled to an opinion , but for him to go to the trouble of editing together a bogus video with incendiary remarks and no understanding of the facts ... he 's like the Donald Trump of video QC . Totally insulting to the artistry and hard work of everybody involved . <p> I come to this forum semi-regularly , certain to pick up quite a bit of knowledge over time . But since the beginning , I 've kind of felt that lots of the information ( and attitude ) is coming " from the top , down " . <p> Surely , YouTubers are not expected to have to go into a color grading suite before publishing " even if it would help when trying to have a " technical " discussion @ @ @ @ @ @ @ @ @ @ simple enough to understand : superhero comics are vibrant and exaggerated , while superhero movies are ( supposedly ) muted . Why ? <p> He does kind of go all over the place making both wrong assumptions and conclusions , even to the point of contradicting himself ( camera vs black , or DC having the " right " look , while Man of Steel ( DC ) was loudly/famously criticised for its muted look ) . He 's just an average , interested in movies/film making kind of guy with an opinion <p> Why not just stick to constructive criticism ? I liked Walter 's remark that it 's quite different when you 're distributing on paper ( reflected light ) compared to a lit up monitor or projected image . I also think that comics exist firmly in the realm of " make believe " and are often pushed to the extreme , while in movies the challenge is often trying to integrate fantasy elements into the real world to suspend disbelief . Stuff like this could probably lend itself as a jumping off point for a good @ @ @ @ @ @ @ @ @ @ " what you think of his proposal , not what you think about the guy 's accomplishments ? <p> Unfortunately , I do concede that his argument is so confused that we need the rosiest of glasses to filter it down to its bare essence . <p> I come to this forum semi-regularly , certain to pick up quite a bit of knowledge over time . But since the beginning , I 've kind of felt that lots of the information ( and attitude ) is coming " from the top , down " . <p> Surely , YouTubers are not expected to have to go into a color grading suite before publishing " even if it would help when trying to have a " technical " discussion about film and grading ? <p> His main argument is simple enough to understand : superhero comics are vibrant and exaggerated , while superhero movies are ( supposedly ) muted . Why ? <p> He does kind of go all over the place making both wrong assumptions and conclusions , even to the point of contradicting himself ( camera vs black , @ @ @ @ @ @ @ @ @ @ Man of Steel ( DC ) was loudly/famously criticised for its muted look ) . He 's just an average , interested in movies/film making kind of guy with an opinion 43971 @qwx453971 <p> The problem is not that someone has expressed his own opinion - he presents a case against low contrast , founded by whatever technical , creative , or personal reason it might be . Evidently , his personal vendetta against " lifted blacks " did not develop overnight . But that 's all fine - nobody can tell you how to feel . <p> The trouble is that the video is a grossly oversimplification of the DI process , that from a unique narrator position allows him to present his points as an " expert " . It is difficult to offer constructive criticism when the claims being made are inaccurate , unsupported , and contradicting . He could at least be classy about it . It 's pretty embarrassing and insulting , to be honest - this is the source of the controversy . <p> The role of the colorist is being constantly re-examined by @ @ @ @ @ @ @ @ @ @ , and as a result the industry is seeing a lot of grading being kept in-house , out-sourced , or even ignored - this cultural attrition has had a negative impact to our livelihood all up and down the chain . See here and here , for example . <p> As a general rule , I prefer to avoid the YouTube comments section by default , but there is ( mostly ) resounding support in favor for the video , and it 's popularity continues to grow as it makes it rounds across blogs and feeds . That 's misinformation being spread around that certainly does n't help our case . I would of course be over-exaggerating if I were to suggest that the outcome would be with one of us losing a bid or two in the marketplace , but the point remains . <p> Information and attitude coming " from the top , down " is the lesser evil twin brother . Imagine for a moment that was switched around ... Well you do n't have to with this particular thread , and you can see why it @ @ @ @ @ @ @ @ @ @ kind to most of us , and continues to be an open hub for creative professionals who have questions , opinions , and stories about this forever-growing science and craft . As with any other social arena , we reserve the right to respond to criticism ( and snicker ) in due process . Even in the best conditions , last time I checked , you do n't buddy-up to the people and projects you want to work on by gesturing your wrist at professionals and calling out their work publicly on YouTube . <p> In slightly related news , congratulations to Steven Scott for recently winning the 2016 Outstanding Color Grading HPA Nomination and Award , who supervised the grades on many of the projects the video pulls footage from . <p> Reading your answer here , I sense that there is a lot of baggage . I 've never heard of the guy in the video and likely never will again , but maybe you 've heard more of him . <p> I simply took the video at " face value " and assumed it was an attempt @ @ @ @ @ @ @ @ @ @ franchise . The fact the YouTuber is making false claims , draws erratic conclusions pretty much flies right by me . Most people are wrong most of the time , so I guess I 've developed a filter trying to understand what they really mean . I also try to give people the benefit of a doubt , so since this is my first encounter with this guy , I have too few dots to connect . You probably have more . <p> He does n't say that colorists suck or are unnecessary . Correct me if I 'm wrong , but the overall grading direction ( like contrast , saturation and color palette ) is a production design decision and not created behind closed doors by one individual . That is only to say , I do n't feel he was beating down on the skills of the actual colorist in his examples . <p> Most people are wrong most of the time , so I guess I 've developed a filter trying to understand what they really mean . I also try to give people the benefit of @ @ @ @ @ @ @ @ @ @ with this guy , I have too few dots to connect . You probably have more . 43971 @qwx453971 <p> No , this is the first time many of us have heard of this guy - and that 's the problem . With a yawn and propped up feet at the end of the video , its an indication that the internet will continue to see videos like this ( whether they are from him , or not ) and we carry the burden of proof . Speaking of which , at this point I 'm sure a lot of us ( including you ) just want to get back to work ! <p> No , but he does well to discredit the business without fully realising the minutia of it . <p> " I want to talk about Marvel 's color grading . Why do all of Marvel 's movies look like muddy concrete ? Digital cinematography can look amazing , but it has to be graded properly . I 'm going to do 10 seconds of adjustments , really just tweaking the levels and boosting the saturation a @ @ @ @ @ @ @ @ @ @ side . It 's a bummer because Marvel hires some really good cinematographers , and then does their work a disservice with how they handle it in post-production . But if we 're being honest , Marvel , just go back to shooting your movies on film , you 're making enough money , you can afford it " . <p> Information and attitude coming " from the top , down " is the lesser evil twin brother . Imagine for a moment that was switched around ... 43971 @qwx453971 <p> In case nobody here has noticed , we just experienced two elections in two countries on two different continents that , if nothing else , demonstrated rather graphically that there is currently a very strong movement away from intelligent discourse , civility , and reasonable debate at this point in time . Experience and knowledge , which have long been regarded as strengths when discussing any issue , are now relegated to being an albatross around the necks of those who choose to continue to value them . They 're now part of " the establishment , " and @ @ @ @ @ @ @ @ @ @ frowned upon , regardless of its true value . Anything posted anywhere on the Internet by anyone who chooses to post it is going to likely have as much value to many as something stated by people like Steve Scott , Peter Doyle , Stefan Sonnenfeld , or anyone else with real credentials . In fact for some , it 's likely to have even more value because it was n't reported by the " biased , left leaning " mainstream press . Getting something heard is , at this point in time , more valuable than actually being factually correct . Personally , I hope this trend runs its course , but I 'm not sure it will now that we have ways in which anyone can say whatever they want and have it available to a world audience . <p> That said , IMHO the original poster 's comments would have been fine - and , frankly valid - if they had been expressed as a personal opinion based on personal taste rather than as a technical criticism supported by patently incorrect statements and false arguments . He @ @ @ @ @ @ @ @ @ @ does not have experience with or understand , both technically and politically , and by doing it on a forum in which many of us do have that experience . <p> there is currently a very strong movement away from intelligent discourse , civility , and reasonable debate at this point in time . 43971 @qwx453971 <p> Scary , is n't it . <p> That 's the function of right hand parties . They cash in on general social anxiety and turmoil in times of distress to create a movement that destabilises the status quo and in turn puts them in power . But they are just as bad as others . It was funny how Trump was praised as the candidate that was more " clean " and honest but yet he was the candidate that says global warming does n't exist - which is such an obvious sign that he is paid , one way or the other , by the " oil lobby " , or is profiting by partnerships with them , etc . And it 's utterly hilarious how blinded people are to such stuff @ @ @ @ @ @ @ @ @ @ is a capitalist tool and via media , it 's mostly coming from where you 're posting from No pun intended . I 'm just starting to do a paper on the general myths ( Barthes ) , that the current most watched episodic television series are representing . It 's a very predicting result , but nonetheless , proving something is different than just knowing something and that can hopefully open a public debate . <p> Regarding what that guy said on youtube , I think getting overly agitated by what consumers are saying , just leads to losing years of life - or hair It 's pointless . I deal a lot with story and humanistic studies and boy people are to smart to be argued with . I 've learned a long time ago , to just stay quiet and wait until it passes But kudos to those that are willing to put some energy in to it ! <p> " I want to talk about Marvel 's color grading . Why do all of Marvel 's movies look like muddy concrete ? Digital cinematography can look @ @ @ @ @ @ @ @ @ @ I 'm going to do 10 seconds of adjustments , really just tweaking the levels and boosting the saturation a bit and let 's see how they fair side by side . <p> Note that real film critics put their remarks in context , and they 'll judge a film for what it is , not what they thought it could have been . Real critics also have credentials , particularly understanding the history of the subject and being able to judge it very objectively . We 're not getting that here . <p> Most importantly , no film critic would say , " well , if I had written it , I would have done this , " or " if I had directed it , I would have done that . " To me , it 's the height of egomania to say , " I know better than the filmmaker and I 'll prove it to you . " Just obnoxious beyond belief . <p> He also starts off from a flawed scientific point of view : if he 's grabbing Blu-ray rips and internet clips , @ @ @ @ @ @ @ @ @ @ What color space is he working from ? How is his display calibrated ? Did he use scopes ? I reject the whole argument right there just on those counts . <p> Seriously guys ? You 're going to turn this thread into a political discussion now ? It 's just a misplaced opinion of a super geek about some superheroes movies , who desperately needs to get laid and we 're turning this into " You 're not my president " . How about a little perspective here ? 
@@44332835 @4332835/ <h> I 'm tiRED of having to deal with Dragon footage . What am I doing wrong ? <p> Not to beat on the dead horse , which from the looks of it , very much alive Anyway , personally , I ca n't reconcile this little oddity . You 're talking about zero budgets , yet you 're shooting with Master Primes May be start using some Canon lenses and then you may have something left in the budget to hire a colorist ? Also , the world is rapidly shrinking . You may want to start looking outside of your immediate geographical area for a talented colorist . If you have an interesting project , you may even find someone , who may be willing to contribute for very little or no money . What is the point of using Master Primes , if you 're going to mess it with all those noise reducing acrobatics ? 43971 @qwx453971 <p> Somebody call the news services : I am in 100% total agreement with Jake . <p> It 's very true that in the war between production and @ @ @ @ @ @ @ @ @ @ out the compromises before a single frame of material is shot . Noise-reducing everything is not the answer . <p> And an old friend of mine 's favorite saying is , " the hardest thing about show business is the BUSINESS . " Raising the money and determining a reasonable budget is always very challenging , whether you have $1000 for a shoot or $100,000,000 . At least now , through crowd funding and social media , you have a fighting chance of finding investors and getting people interested in helping you see your project through completion . <p> I really like master primes in that the savings in post/vfx budget is worth it , even for low budget stuff . ( i.e. 6k camera and object solves are amazing with MP ) <p> BUT one wierd " gotcha " on master primes with red dragon &amp; helium is you ca n't do the T stop past T8 ( T6 for helium ) . When ever I hear about some soft footage with a master prime I always ask what is the T-Stop . This is my list of reasons @ @ @ @ @ @ @ @ @ @ ) alexa sensor is much bigger so the diffraction limit is n't hit as much ( i.e. that t-stop on the alexa did n't have a problem ) 2 ) no internal nd filter on red , so people tend to increase the t-stop when they do n't  have the time for a ND filter 3 ) No LDS , so it 's much harder for post people to yell at production when they see soft images ( i.e. since the footage does n't have the t-stop setting in the file ) 4 ) both the red evf and 5 " with the magnify it 's hard to tell when this problem hits , only the 9 " on magnify is it visible 5 ) master prime is so light efficient , that the T stop ( i.e. light based rather then aperture ) can have a smaller aperture then people expect it compared to lower cost glass <p> As noted above , helium has even a smaller sensor/pixel size so the t5-t6 diffraction limit kind of to me puts a huge cramp on master primes ( i.e. for some @ @ @ @ @ @ @ @ @ @ deep depth of field though I do normally shoot around t2 ) . If red had LDS , I 'd put up a little warning on the camera when the diffraction limit was exceeded . <p> I really like master primes in that the savings in post/vfx budget is worth it , even for low budget stuff . ( i.e. 6k camera and object solves are amazing with MP ) <p> BUT one wierd " gotcha " on master primes with red dragon &amp; helium is you ca n't do the T stop past T8 ( T6 for helium ) . When ever I hear about some soft footage with a master prime I always ask what is the T-Stop . This is my list of reasons why people have high t-stops problems with dragon/helium : 1 ) alexa sensor is much bigger so the diffraction limit is n't hit as much ( i.e. that t-stop on the alexa did n't have a problem ) 2 ) no internal nd filter on red , so people tend to increase the t-stop when they do n't  have the time for a @ @ @ @ @ @ @ @ @ @ much harder for post people to yell at production when they see soft images ( i.e. since the footage does n't have the t-stop setting in the file ) 4 ) both the red evf and 5 " with the magnify it 's hard to tell when this problem hits , only the 9 " on magnify is it visible 5 ) master prime is so light efficient , that the T stop ( i.e. light based rather then aperture ) can have a smaller aperture then people expect it compared to lower cost glass <p> As noted above , helium has even a smaller sensor/pixel size so the t5-t6 diffraction limit kind of to me puts a huge cramp on master primes ( i.e. for some scenes , I really like t8 to get a nice deep depth of field though I do normally shoot around t2 ) . If red had LDS , I 'd put up a little warning on the camera when the diffraction limit was exceeded . 43971 @qwx453971 <p> " Alexa sensor is much bigger " ? Did you mean Alexa sensor 's photosites @ @ @ @ @ @ @ @ @ @ thru his website ... i found it very useful website over the past few years for real information , as he is n't a camera company or some dude on a forum doing pseudo science with a colour chart and some lights <p> he is the guy that found water on the moon for nasa in 2009 Check out the about page <p> - import r3d 's into Resolve - import XML - link and fix any issues , or roundtrip them if Resolve ca n't sort it with it 's tools - set project to ACEScc - set camera metadata to default - allow on-set tint / temp / exposure - check DEB on - in the first node with only using exposure , contrast , pivot controls normilise the image to taste - in the second node set to L*a*b color science , and ballance the shots with RGB offset controls - in a subsequent nodes use LGG as needed , do as little as possiable to the images , leave them as clean and un-manulipulated as possiable , no windows , no keys , no curves unless @ @ @ @ @ @ @ @ @ @ use carefully , watch for noise introduced by these tools - in the last node use NR , qualify it into the shadows only , and once you think you have it about right , cut the values in half - export XYZ seguence from Resolve 's timeline , make your DCP from that 43971 @qwx453971 <p> Alright here are the updates . I used the above method to grade my footage . First off , here 's a reply to the things I was and was n't doing from the above method . <p> import XML ( Was doing that ) <p> - link and fix any issues , or roundtrip them if Resolve ca n't sort it with it 's tools ( Was doing that too ) <p> - set project to ACEScc ( Was not doing that at all . I 'll get back to this point below . ) <p> - set camera metadata to default ( been doing that ) <p> - allow on-set tint / temp / exposure ( Was doing that ) <p> - check DEB on ( was n't doing that . @ @ @ @ @ @ @ @ @ @ points , I 've been doing a few things but not all . - in the first node with only using exposure , contrast , pivot controls normilise the image to taste ( I use curves to normalise the image. ) - in the second node set to L*a*b color science , and ballance the shots with RGB offset controls ( Was n't doing that . This is completely new to me . Reading and learning what this is about . Thank You. ) - in a subsequent nodes use LGG as needed , do as little as possiable to the images , leave them as clean and un-manulipulated as possiable , no windows , no keys , no curves unless you really ca n't live without them , and then use carefully , watch for noise introduced by these tools ( Yup. ) - in the last node use NR , qualify it into the shadows only , and once you think you have it about right , cut the values in half ( no NR as I 'm using DaVinci Lite . ) <p> My biggest test here @ @ @ @ @ @ @ @ @ @ . I was just leaving it at the default , which is Davinci Yrgb . And this is probably the biggest thing I 've learnt here so far . I took the horse shot and tried the ACEScc method . The difference is that it gives you a starting point that 's too far from the final color corrected ( not graded ) image . Which means a lot more work to get the image to where I need it to be , before I can start grading it . The end result is not really that different . I got a slightly less noisy image with this method but no real eureka moment there . <p> Here are a few screenshots of what I 'm talking about . The almost black screenshot is my ACEScc starting point . The faded one is my Davinci YRGB starting point . The third one is my grade and the last one is my final result . Also , non of my LUTs work on the ACEScc color science . making it even longer to grade my footage . I 'm just reading @ @ @ @ @ @ @ @ @ @ is what I 've been doing wrong all my life . <p> I 've been reading a lot on RedUser forums and a lot of people are having the same issues as me when it comes to Dragon Sensors . To get silky smooth noise-less footage from a dragon sensor is almost an art in itself . some say 320 ISO , some say 800 . Some say with skin tone OLPF , the sensor records more noise in low light ... etc etc . It 's a mess . I just need to test the hell out of this camera . <p> Not to beat on the dead horse , which from the looks of it , very much alive Anyway , personally , I ca n't reconcile this little oddity . You 're talking about zero budgets , yet you 're shooting with Master Primes May be start using some Canon lenses and then you may have something left in the budget to hire a colorist ? Also , the world is rapidly shrinking . You may want to start looking outside of your immediate geographical area for @ @ @ @ @ @ @ @ @ @ , you may even find someone , who may be willing to contribute for very little or no money . What is the point of using Master Primes , if you 're going to mess it with all those noise reducing acrobatics ? 43971 @qwx453971 <p> Over the last two years , I 've come to know the rental guy pretty well . He let me rent out the lenses for 48 hours , provided I give him credits on our Facebook page ( tagging his company ) when we release the films . I do n't mean to judge but , where I live , making a no/low-budget film is all about how many favours can you pull without compromising the creative vision of the project . If a guy offers you Master Primes in exchange for credit for financing the film , I say why not . <p> I still think this noise might be mainly due to underexposure or maybe a combination of two/three things : <p> 1 . generally underexposed image 2. image lit in a way that you 've got little contrast in the lower @ @ @ @ @ @ @ @ @ @ you apply a contrast curve i.e. in a way to bring out contrast in that part , you probably amplify the visibility of noise in that part 3. haze <p> The almost black screenshot is my ACEScc starting point . The faded one is my Davinci YRGB starting point . The third one is my grade and the last one is my final result . Also , non of my LUTs work on the ACEScc color science . making it even longer to grade my footage . I 'm just reading up on properly working with ACEScc. 43971 @qwx453971 <p> ummmmm , some thoughts ... - the first image is not what i would exoect to see if properly exposed , maybe check your ODT ? it should be set to match your monitor <p> - the next thought is that it is easy to break an image with curves , they have their place , but using them to normilise an image that you want to keep as clean as possiable is not where i would start with them , i would go back to exposure / contrast / @ @ @ @ @ @ @ @ @ @ delicatly as possiable , and after the image is 90% in a good place <p> - the sample you provided earlier was very close when displayed in ACEScc , it only took a small adjsutment in L*a*b to land that near the target , so i 'd be surprised if the new sample was so very far off in exposure <p> ummmmm , some thoughts ... - the first image is not what i would exoect to see if properly exposed , maybe check your ODT ? it should be set to match your monitor <p> - the next thought is that it is easy to break an image with curves , they have their place , but using them to normilise an image that you want to keep as clean as possiable is not where i would start with them , i would go back to exposure / contrast / pivot in the first node , and any curves used delicatly as possiable , and after the image is 90% in a good place <p> - the sample you provided earlier was very close when displayed in ACEScc , @ @ @ @ @ @ @ @ @ @ that near the target , so i 'd be surprised if the new sample was so very far off in exposure <p> - in prefrences turn off " use S curve for contrast " 43971 @qwx453971 <p> The L*a*b thing is a completely new thing to me . Did n't really know about it until my last post . I 'm playing around with it and I 'm loving what it can do . I 'm so much closer to getting where I wanted to be with this one . I really ca n't thank all of you guys enough for guiding me . Will post results after a bit more tinkering . <p> I 've been reading a lot on RedUser forums and a lot of people are having the same issues as me when it comes to Dragon Sensors . To get silky smooth noise-less footage from a dragon sensor is almost an art in itself . some say 320 ISO , some say 800 . Some say with skin tone OLPF , the sensor records more noise in low light ... etc etc . It 's a @ @ @ @ @ @ @ @ @ @ of this camera . 43971 @qwx453971 <p> every film stock and digital camera have their own quirks and problems etc etc and RED cams are no different . this is not an Alexa which in comparison is rather a point and shoot cam ... <p> every iteration of RED cams has it 's own set of issues ... it is an annoyance that is part of it . <p> but - one of the most important thing to understand with . r3d RAW workflow : ISO is nothing but metadata. it means nothing at capture . at capture it merely gives u a preview ( on a crappy , uncale 'd screen ) of what the image will look like if u use the EXACT SAME in cam settings in post , which usually nobody does when developing a . r3d RAW negative because of the inherent huge creative potential - AND - the extra work that often needs to be done to clean up some RED and/or digital capture problems ... <p> as a metaphor , your ISO setting at capture is merely a brightness setting for the preview @ @ @ @ @ @ @ @ @ @ check what is really happening , always use RAW view on RED cams and observe the histogram , see how the light is distributed over the stimuli range , see what is hitting the sensor . Control exactly that . Where the sensor is hit . Avoid RG4 and all that crap for exposure decisions . The 1D will be set in post . <p> Digital raw workflow is NOT ( ! ) film workflow ( i.e. the capture to post pipeline ) . Anything can and will be subject to change in post . U can make it a film workflow , which IMO is limiting . Times have changed 10 years ago . <p> on RED Dragon , set a strict ISO to 250-320 at capture ( when using non-RAW view preview modes ) . Dragon is light hungry . Feed it what it needs . If necessary , u can always darken the image in post , which will be noise free . <p> re ur cam and ur earlier statement : like I wrote before , if ur cam exhibits wild noise when exposed properly in @ @ @ @ @ @ @ @ @ @ ergo : a technical problem . This is the first thing to evaluate . <p> I 've resisted joining in here but it 's too much . If you get a dark underexposed image when you use ACEScc then you have a dark underexposed shot , full stop . It 's the DP not the camera . The whole point of ACES is standards , it 's why I love ACES regardless of camera . It gives you a great base point , if your image looks bad with the basic IDT and the relevant ODT then the problem is with the original exposure and nothing else . So much misinformation and misunderstood stuff on the net , test if in doubt , if you ca n't afford to test your self then look around and see if you can find tests that you trust , I 'm biased here but look at www.cinematography.net . When I did n't identify the lenses being used in the lens tests nobody , I mean NOBODY , chose the super star lenses . In fact the idiocy was summed up by the message @ @ @ @ @ @ @ @ @ @ lenses if they did n't know what they were ! ! <p> Just go to basics and blind tests , trust your eyes and not the BS that floats all over the net . Is my pain medication running low ? Oh yes . <p> Feed it in the right place I do n't know if that happened here but if i.e. you have very bright areas in the picture ( like those " practicals " in that parking lot ) and the rest is mostly dark and you try to protect those hotspots , you push everything else down into the mudd and waste 2 , 3 stops of your dynamic range on that . So if you have this kind of situation and there is no money/time to light properly , it might be better to let those hotspots burn and get proper exposure on the important parts of the image to avoid noise . <p> Feed it in the right place I do n't know if that happened here but if i.e. you have very bright areas in the picture ( like those " practicals " in @ @ @ @ @ @ @ @ @ @ and you try to protect those hotspots , you push everything else down into the mudd and waste 2 , 3 stops of your dynamic range on that . So if you have this kind of situation and there is no money/time to light properly , it might be better to let those hotspots burn and get proper exposure on the important parts of the image to avoid noise . 43971 @qwx453971 <p> the problem is that if the fg object/talent is heavily underexposed when trying to protect the highlights , the scene is not well lit ... in any case , always capture the fg object/talent properly , that 's where the money is <p> So if you have this kind of situation and there is no money/time to light properly , it might be better to let those hotspots burn and get proper exposure on the important parts of the image to avoid noise . 43971 @qwx453971 <p> There may also be a happy medium where you bring up the exposure , then use highlight controls and/or soft clips to tame the hot spots . I do that @ @ @ @ @ @ @ @ @ @ " radioactive " and distracting . <p> There may also be a happy medium where you bring up the exposure , then use highlight controls and/or soft clips to tame the hot spots . I do that all the time , just to make the highlights less " radioactive " and distracting . 43971 @qwx453971 <p> yeah , or just raise brt levels where needed ( control the brt region or , include/exclude areas via shape/power window ) except highlights which ( when protected ) are already where they need to be ... <p> but that will possibly introduce ( a lot of ) noise doing so , if the bulk of the shot that needs to be relighted is far away ( on the histogram ) from the highlights that were protected ... this shot is an example of that <p> Post up a . r3d , you may be pleasantly surprised at what can be revealed from the file . One fact about . r3d files is you need to have an informed workflow - there 's more than enough rope in those parameters . <p> Post up @ @ @ @ @ @ @ @ @ @ what can be revealed from the file . One fact about . r3d files is you need to have an informed workflow - there 's more than enough rope in those parameters . 
@@44332836 @4332836/ <p> Do you mean their proprietary workstations , where they supply the whole thing built ? ... That means that all of the parts in the whole PC are readily available , unless you install something particularly funky . At least in the US , getting replacement motherboards from Supermicro is n't a problem . 43971 @qwx453971 <p> Yes I 'm talking about the 7047GR-TRF . Pretty standard GPU workstation . The motherboard died and it took that long to get the part over to the Netherlands . If you 're just using the case you 're not really on a SuperMicro , not trying to be snobby . But if you 're bringing that into the equation you 're talking about a normal put together PC and like I said , I think that 's the best for most situations where a Mac Pro wo n't cut it . In the comparison between SuperMicro and Apple , I think it 's fair to take easy of replacement into account . Not to mention weight . It takes two people to carry my SuperMicro anywhere . <p> Every machine @ @ @ @ @ @ @ @ @ @ Some live ' forever ' . I ran into a 386 with a WD 120MB harddisk which had been running for at least 20 years 24/7 . All it needed for me to fix the issue it had was a reboot . No professional parts in that machine except the network card . We all know what issues a Mac Pro has with heat , and I 've had that same issue with a stock configured SuperMicro with 3 GPU 's , a GUI card , a DeckLink and fibrechannel ( like in the Resolve 9 ( ? ) manual ) . At stock cooling it crashed the GPUs until it hard rebooted because of the watchdog function going beserk . The only way to set Server Mode cooling is through the web admin panel which can be a problem if the overheats occur quickly . Basically the same issue as the Mac Pro but with far more severe symptoms . And like the Mac Pro , fixable by tweaking the cooling . <p> Any DIY machine will have the same issues if not addressed up front , and a @ @ @ @ @ @ @ @ @ @ <p> Sorry for going on a bit of a Tangent . I guess this topic Rippled my nerdy Elements . <p> It 's entirely possible that they 'll do some meager refresh when it suits them . Problem is that it only suits them every 3 years or so , which is kind of a problem in a world where GPUs are incredibly important and ancient in 18months . <p> I used to care a bit more if the Mac Pro would be updated or not , but now I care a bit less . Some part of me still keeps a ' best case ' scenario alive , rationalising that ONE reason for a delayed launch would be yet another redesign . <p> I wish Apple could look at all this soberly and realise that since the Mac Pro never will be a great cash cow anyway , just go at with a ' break even ' mentality and try to cash in on connected more abstract benefits like Goodwill , Brand value and Halo effect . <p> Admittedly , I 'm generalising in broad strokes here , but @ @ @ @ @ @ @ @ @ @ the norm in the ' creative industry ' . I think that even if people ca n't connect the dots themselves , even someone who 'd never buy a Mac Pro knew deep down inside that those ' professional ' computers were good at really ' demanding ' stuff . Some of that sunshine always spills down on lower end computers like iMacs and MacBoo Airs . <p> In a world were Apple 's noncommittal becomes punchlines in the late night shows , Apple is approaching dangerous territory . It really WOULD hurt them if in the public mind , all their computers are good for is couch surfing and the occasional email . <p> There 's a big difference between " they are great computers , but on the expensive side " and " they are really expensive computers that kind of suck too " . <p> Some people would say Macs have always sucked since you 'd pay more for the same CPU , memory and graphics . They 'd totally discard software , materials , finishing and in some cases huge usability benefits ( like trackpads on @ @ @ @ @ @ @ @ @ @ needs to turn the trend around on the Macs . They 're doing just fine with their mobile parts . <p> The CAN do it for sure . But will they ? It would be a strategic blunder of huge proportion if they did n't , even if traditional computing is on the way back . I ca n't believe they 'd make a mistake like that . <p> The somewhat odd thing I find with Apple today is that they probably could have a normal computer like a Z840 or something and update it very easily and cheaply at a fixed schedule . New GPU out ? Let 's update ! Advancements in SSD-storeage ? Let 's update ! But they fixate on making experiences rather than basic , iterative tools . <p> A few years back - before the 2013 MacPro was release - a few patent findings where done about the " X Mac " where I think everything was very modular . Something like a combo of the old MacPro and thunderbolt / box-based solutions . Sadly this machine never saw the light of day . @ @ @ @ @ @ @ @ @ @ 2015 Mac and then in the 2016 get the " Best GPU upgrade " option . I also believe it had slots for SSD 's similar to the cheese-grader MacPro just more modular / the user does n't have to " go into " the machine approach . <p> I certainly hope Apple sticks to the pro-computer-game . I understand them backing down from the dual core massive monsters to an extent even if I believe the MacPro of today and the MacPro of yester-year both could co-exsist with out any problems . <p> I also believe they have to sort out their game in regards to who uses their machines . <p> Doing sound-produciton ? The 2013 MacPro is great BUT they have zero user for 2 GPU 's and would much rather have 2 CPU 's and / or more internal storage . <p> Doing 3D-production ? This varies a lot - some want massive amounts of cores , some use GPU 's - still the OPTIONS are what people need here . <p> Doing 2D / Photo production ? One fast CPU is normally king here and @ @ @ @ @ @ @ @ @ @ <p> But saying that it 's not a Supermicro if you did n't buy the whole thing is a little weird . I mean , they 've always sold their motherboards separately and in standard form factors , which anyone can by for a DIY or OEM build . We have a couple of them here . In the case of our Authoring system it was an OEM supplied by the manufacturer of the software . The other is a DIY server . The fact is , if you 're buying a motherboard , it 's the one to get if you care about long term reliability and build quality . That allows you to use ancillary components that you could buy anywhere , so replacements are n't really an issue . Even replacing the motherboard is easy . I have a friend in Croatia who uses SuperMicro boards in his work machines , and he 's never had a problem getting parts . I 've got to think it 'd be easier in the Netherlands than for him ... <p> I do n't really see the point in buying @ @ @ @ @ @ @ @ @ @ kind of work we 're talking about on this forum - it 'd be more expensive , and unless you have an air conditioned server room , most of their hardware is impossible to use in a normal office environment due to noise and heat output . <p> I would like to know what kind of technical specifications should the possible future Mac Pro have in order for you to use it ? Dual Skylake Xeons ? Thunderbolt3 ? USB 3.1 ? PCIe slots ? NVidia 1080Ti or Titan X ? <p> The problem with the direction Apple has been heading for a *long* time now is that it 's the opposite of what you 're looking for . It 's about fewer connectors , fewer wires , smaller packages , a " jewel-like " design aesthetic , and just enough power to say it 's high end , but nothing bleeding edge . Apple was never bleeding edge - they were just better and built more stable , solid , easy to use computers . That 's what the premium price was paying for . <p> The most recent @ @ @ @ @ @ @ @ @ @ with Apple . They 've got a new machine that 's underpowered and has this weird touch-bar interface that looks like it came out of Dell or HP 's design labs . And they 've upped the price point for that level laptop . That was the best they could come up with ? The premium is no longer worth it . <p> I do n't see them reverting back to a modular system like the old G5-&gt;MacPro Cheese Grater design , but in an ideal world , that 's what would make me think twice about getting another mac . Better - a rackmountable system ( that does n't cost a fortune because it 's rackable and is n't 1U high like the XServe machines ) . The most important things would be a modern CPU and many PCIe slots . With enough slots , you can add whatever interfaces you want . And that 's the point . It 's also exactly what wo n't happen . Apple does n't want you to have an expandable machine , they want you to have an expendable machine . You @ @ @ @ @ @ @ @ @ @ stretch the life of one machine to 6-8 years . <p> All that said , I think the bigger issue is the iOSification of the desktop OS , and the bizarre fixation on touchable interfaces . They 're great for phones and tablets and maybe for painters and compositors , but please keep them off my desktop machines . If the OS keeps going in that direction , it will have no place in a professional environment . <p> The problem with the direction Apple has been heading for a *long* time now is that it 's the opposite of what you 're looking for . 43971 @qwx453971 <p> Thanks Perry for ideas . But what if they would replace Mac Pro 2013 's GPUs and ports with what I wrote ? Okay , no PCIe slots , but at least something better . And they could fit two NVMe SSDs to the current chassis if they wanted to . What if the possible new Mac Pro would be like the one they have now , except a larger chassis for better cooling system , dual Xeons , and the @ @ @ @ @ @ @ @ @ @ . <p> Here 's a second idea . Why not let HP install OSX to its z840 family systems with proper device driver support ? That way Apple would n't need to design anything but make sure that OSX could be run on HP hardware with whatever options they have . If Apple does n't care about us professionals , why not let HP do it ? Apple could easily license OSX to them . <p> I can only speak for myself here , but I could never take the trashcan macs seriously . <p> 1 ) Thunderbolt as the primary external interface is a non-starter for us . This interconnect method is only used widely on macs , which means it 's more expensive to connect everything to a mac : hard drives , monitors , etc , because they have to support a niche interconnect . <p> 2 ) The form factor is utterly ridiculous . We have a central server room with almost all of our machines connected to 1GB and 40GBe networks , and a KVM setup for access from any of a number of workstations @ @ @ @ @ @ @ @ @ @ have to have ample space above and below to allow the cooling system to work , and would have to put them on a shelf , which is a waste of space . <p> 3 ) Lack of PCIe slots means we ca n't connect it to our 40Gbe network . It means we ca n't pop a SAS card in there for an outboard RAID or an LTO backup drive . It means we ca n't upgrade the GPUs when something new comes out . <p> on #3 alone , we 've been able to extend the life of our 2009 MacPros such that we 're still using them today , almost daily , for some specific tasks ( HD work on FCP7 , Tape capture and layback , some AE work , some audio work ) . That 's only possible because we 're able to customize that machine to do what we want . <p> What if the possible new Mac Pro would be like the one they have now , except a little larger for better cooling system , dual Xeons , and the custom made @ @ @ @ @ @ @ @ @ @ use that ? 43971 @qwx453971 <p> Sure , there would be people who would use it . But there are also people who have been waiting for years to upgrade simply because they want the next machine to be a mac . Trust me , I was that guy for almost 30 years . I 've been using Apple products since the Apple II and resisted Windows until I had to use it at work in the late 1990s . I still hate Windows . But at a certain point , one needs to look at ones business and ask " Is it worth holding out hope for a product that may never come to market , based on rumors ? Or should I just get something that works and does what I need , and make some money ? " I guess that 's a personal decision , but it seems like an obvious choice , if you ask me . <p> I just spent all weekend doing , redoing , doing , redoing , and redoing again a bunch of glitchy renders ( 5K -&gt; HD ) . @ @ @ @ @ @ @ @ @ @ been an 8-hour job became an extended weekend of painful death . When I ultimately pass away , somebody remember to put on the gravestone , " Died After Years of Frustration with Apple 's Mac Pro Render Glitches . " <p> I just spent all weekend doing , redoing , doing , redoing , and redoing again a bunch of glitchy renders ( 5K -&gt; HD ) . Just a total friggin ' nightmare . What should have been an 8-hour job became an extended weekend of painful death . When I ultimately pass away , somebody remember to put on the gravestone , " Died After Years of Frustration with Apple 's Mac Pro Render Glitches. " 43971 @qwx453971 <p> And I just finished a session with 4800x2700 R3D with multiple windows ( no NR or OFX ) to ProRes4444 native resolution , rendered perfectly @ 10fps on a oMP 5.1 with updated CPU , Titan X and internal SSD system drive WITH NO ISSUES ! Ai n't -+progress ? great ? Hope that system and my personal system ( same specs ) never dies ! 
@@44332837 @4332837/ <p> I think Avids ( at least some of them ) can do this . But I 'm pretty certain you ca n't with Resolve . I 've wanted this functionality for years - even if it was just in their little media express application . Nobody seems to want to make it happen . <p> We used to use a DDR called Rave2k , but it was Linux based - it used an AJA Kona3 board and DPX sequences to act as a true virtual deck . You could control it just like any deck with a 9-pin cable , and it supported timecode and a bunch of other features including high quality up and down conversions , and text overlays/timecode burn-ins , etc . But the company closed up a few years ago and you ca n't get it anymore . 
@@44332838 @4332838/ <p> Yes , that will often require caching for playback , but that 's always to a format that is what I would classify as ' Minimum-Display-Lossless ' . By that I mean that if the projector is accepting RGB444 then I 'm caching to something that is RGB444 . No 422 cheating or anything like that . <p> From this point on it becomes a client management thing . If I 'm caching to the absolute top-most level of effects , and it 's someone saavy who understands that bitdepth is really important on the pre-grade end of the chain but that from graded image to output its a rare shot where they 'll be able to see the difference between 10 and 12 bit with the naked eye then it 's easy - I would use an uncompressed 10 bit format depending on the system . If they they want 12 bit ' Because 12 is bigger than 10 ' then you 're caching to 16 bit . Because it 's the client 's time so , short of war crimes or the impossible I like to @ @ @ @ @ @ @ @ @ @ to roll the dice with a flavour of Prores but , for my own piece of mind , I just like to remove the whole ' compression ' thing from my already rather long list of other things that can bite me on the arse . <p> UHD/4K usually comes with a bit more grace time than HD/2K , purely due to the fact that ' It 's **29;1204;TOOLONG . With other work , depending on time , you may be tempted to cache to a format that you know that you can use as a deliverable . In that case you 're probably looking at . dpx or Prores . If an hour or 2 is n't going to be a problem , and storage willing ( as I have to admit has been the case for me with the 5 UHD/4K jobs that I 've graded ) then I will cheat and cache to a format that is totally useless to the client but which improves cache time and improves disk read speed . <p> By that I mean Bias alert due to having previously worked for the @ @ @ @ @ @ @ @ @ @ professionally on UHD/4K work on a Mistika . Plenty of work on Quantel Pablo , Resolve and Baselight but none professionally ( client attended ) at UHD/4K delivery level . <p> Anyway , now that I 've unburdened my conscience - Mistika has a cache and render option called . JS . Essentually it 's a lossless movie wrapper - think of it like a super-light . mxf wrapper . Rendering to . JS will always be at least equal-fastest to anything else on a Mistika , there will definitely be no flavour faster . The fact that it is not a sequencial format though means that I remove all of the incredible boring nonsense that comes with sequential picture formats like . dpx and . exr . <p> Although to be honest I 've never really had any playback problems with them either . Just storage BS with **26;1235;TOOLONG blah blah that I can barely be bothered even mentioning . Even so - question answered fully . <p> The Colour Grade effects was never usually what slowed things down BTW. 4K systems will usually be able to hand things @ @ @ @ @ @ @ @ @ @ ( plus the 0.5K up-res ) in real time . Effects that were pixel-dependant ( like Median Blurs or Denoising ) are the real-time killers . The other one would be Epic/Dragon - but that 's only because I always debayer 1:1 . Even if I was working HD/2K though I would always debayer RED 1:1 and then scale down with a Framing/DVE effect from there , and if you do n't know why then you really should . You do n't need to know the artistic side but a colourist should know as much about the cameras they 're grading as the DoP shooting them . It 's just my opinion ( but I 'm 100% right . If you 're not sure then just ask me . I will confirm it ) <p> If it 's VFX then it 's usually full-gate out to VFX . On all but 1 of the UHD/4K jobs I did the VFX house did n't want to be the ones who dropped the ball on resolution quality so they asked for everything . 1 vendor ( who shall remain nameless ) opted @ @ @ @ @ @ @ @ @ @ and rendered directly to UHD . They got lucky on all but one shot that they had to redo due to a zoom in . The Alexa wide gate went to VFX as it was and the extra 0.5K was up-res when it returned to me . Again , lucky me - I also provided the . exr to the VFX vendor , which made it easy to managed , and is a luxury I will no doubt lose as this all becomes more commonplace . <p> Final delivery render always off the RAW/VFX . EXRs though . Never the cache ( more out of **32;1263;TOOLONG for the client 's investment than because you could see any quality loss ) . <p> All VFX shots were stripped of gamma and rendered to . exr as Linear Light extractions of the camera RAW ( Linear - the great equalizer ) . I 've only ever sent Sony F55 , Canon C300 mkII , RED Epic/Dragon and Arri Alexa material to VFX for UHD/4K . One job insisted on ACES ( because it was super-cool at the time ) , all the @ @ @ @ @ @ @ @ @ @ for the camera ( SGamut3/Cinema Gamut/DragonColour2/Arri Wide Gamut ) . I love Linear Light , from there everything becomes as easy as I have been able to work out to match , whether through a transform to a consistent Log flavour ( the ' Working Space ' or ' Grading Space ' philosophy ) or , heaven forbid - through my ability to grade . Contrast is what makes camera matching a b*sdard , I 've never had a problem with gamut . On my first 2 jobs I moved everything to a single gamut ( and due to my deep respect of Sony/Canon/RED/Arri I will not tell you which I picked ) . On the last 2 I just left them as camera native and graded from there . Ca n't say that I noticed it slowing me down at all . Gamut has never been a problem with match-grading . ACES only ever annoyed me at the start when I was keying , but I have a solution to that now that now so no bother there either any more . <p> Note : Personal preference is not @ @ @ @ @ @ @ @ @ @ though . I respect the ACES logic of though when it comes to deliverables ( grade Rec2020 for instance and then not have to run a trim-pass to go to DCI P3 , Rec 709 , etc ) . That makes sense to me , and when I did it with v1.0.1 ACES the ODTs did indeed look the same as I toggled between the different flavours ( obviously not accurately for Rec 2020 , unless someone knows a display that 's professing to do 100% of that gamut that I do n't know about . Enlighten me if so - I will thank you for the info ) . <p> In conclusion : You 'll never get more info than what the camera recorded . Grade off that if it 's not VFX . If you think about it - it 's even more future-proof than rendering to ACES . exr . Your archive remains the camera RAW - hard to do better than that . If it 's a VFX shot then it 's always . exr . And if you 're smart you 'll get the VFX @ @ @ @ @ @ @ @ @ @ and anything else that you think might help you out , into the . exrs they return to you . <p> And always make friends with your VFX vendor . And if you spot a mistake that they 've made - tell them first . Do n't be a dick and rat on them to in an attempt to look cool to your client . No one likes that guy . Help them if you can because , I got ta tell ya - I 've made the odd mistake in the past , and it can either be no-big-deal/easy-to-fix , or it can be ... political . <p> I prefer **39;1297;TOOLONG . That 's just me though 
@@44332839 @4332839/ <h> Sony a7S S-log Dynamic Range vs Noise <p> I am very new to S-log and I have been running a lot of test trying to figure out the best Picture Profile to use on the Sony a7S , not only for dynamic range but for noise . <p> I totally understand that since the a7S is only 8bit , perhaps it not the best match for s-log ( banding ) , but I wanted to see for myself . <p> I watched this video from Sony 's ICE ' Independent Certified Expert ' Alister Chapman where he said they 're several ways to expose for s-log at 38:40 . One way is to ETTR , which as you can see in my tests produces the cleanest image ( 2-stops overexposed ) , and the other is to expose for middle gray and reduce two stops ( 2-stops underexposed ) . Alister says in the video that 2-stops under is about 38% IRE ( I am seeing on my scopes ) and is recommended for s-log , however there 's a ton of noise in the image . <p> @ @ @ @ @ @ @ @ @ @ ETTR rather than expose middle gray at 38% . Will I be giving up a ton of dynamic range if I do this ? <p> If you record with the Atmos Shogun , at least you can capture in 10 bit , which I think will help the noise/dynamic range problem quite a bit . I do n't expect a lot from these toy cameras , but my inclination is to underexpose at least half a stop and fill the shadows and protect the highlights . Traditionally , trying to force contrast into a shot that does n't have a lot of contrast is where you wind up exaggerating noise . <p> I have n't done anything with the Sony A7 , but I have done some stuff with the Panasonic GH4 , and it 's not terrible . They 've come a long way from where these toy cameras used to be 4 or 5 years ago . I still do n't like the inflexibility of most of these DSLRs with timecode and sound , plus they tend to overheat and have lots of other practical on-set problems , @ @ @ @ @ @ @ @ @ @ on the tests I did with your S-Log2 a7s footage , I 'd think that would eat a bit into your dynamic range and is n't worth it . I feel that S-Log2 ( and possibly that advice ) is much better suited for Sony 's higher end cameras where you can get away with ETTR much better than you can with the 8 bit 50mbps codec ( although much improved from AVCHD ) . I feel Marc has the best suggestion , underexpose a tiny bit to crush that shadow noise and protect for highlights or try an external 10 bit recorder . <p> I found a lot of noise in several of tests when I introduced a bit too much contrast to your test car shot , had to keep it fairly subtle contrast wise in the grade . <p> From my tests , I think I should be shooting ETTR rather than expose middle gray at 38% . Will I be giving up a ton of dynamic range if I do this ? 43971 @qwx453971 <p> You wo n't be giving up any DR at all , but you @ @ @ @ @ @ @ @ @ @ If the a7s slog2 curve is just like the one in the F55 , then it has 6.5 stops above mid grey and 6.5 stops below mid grey ( with noise in the bottom ones ) . So if you overexpose by two stops ( i.e. you choose ISO 3200 but expose as if you had set ISO 800 , which is exactly what I plan to do when my a7s arrives ) , then you have 4.5 stops above mid grey ( still more than what a 5D3 has , for example ) and 8.6 stops below ( with noise in the bottom ones ) . Being log , this is still easy to play with in post ( if it had a built-in s-curve , grading would become more difficult , but with log it 's not a problem ) . <p> Those have tiny sensors . I like them , but I also like big sensors ( more accurately , my lens set likes big sensors ) . 43971 @qwx453971 <p> I 'm in the same boat , but looks like we 're getting getting a proper @ @ @ @ @ @ @ @ @ @ small sensors at least get them to super 35 size . Considering a bmpcc now w the adapter anyway as the bmcc prores is 1080 as well <p> yes that was what i meant . prores and better . why bother with those dslr compressed cameras as in post they are just awful . ok sony has night vision but for my taste this pumped up " brightness " looks like night vision without decent shadows or contrast . <p> but this is just me . <p> we used to use cinestyle for dslr to make it " better " . it helped a bit it felt then but if we go back now it does not seem as good . <p> Margus , I 've been shooting with the GH4 for the past month and I must say that shooting in UHD ( 8bti 100Mbs ) then downconverting to 10bit HD gives you a very robust codec . I was really really surprised how far the image could be pushed before codec started to break . <p> Offcourse But raw is reserved for projects that need it , ie. @ @ @ @ @ @ @ @ @ @ time to be able to deal with a raw workflow ... And BM cameras are IMHO an experiment and not cameras . The first real raw camera is the F5 . <p> In the February of 2014 I grade feature completely shot on BM 2.5K . Everything was fine with workflow and image quality . Actually i thought it should be worse in the beginning of a job . Of course bm cameras is not alexa or red , but they are very capable . <p> The guy who wrote the WolfCrow article seems confused on the issue . I have n't used the camera or recorder , so I 'm just posting the link and what he said . It looked bogus to me , too , and my gut is that it would n't make any difference if they padded out the two extra bits from 8 to 10 in the recording . <p> I agree that pushing Log images around is going to be problematic if you do n't have enough bits to really capture the signal and withstand this much tweaking . Those toy camera images @ @ @ @ @ @ @ @ @ @ . <p> TBH bits are IMHO totaly over rated . First comes the codec bandwith , then comes the noise and only third come the bits and forth comes chroma resolution ( generaly speaking ) . This is speaking from my experience . Grading a 200Mbs 8bit noise free Slog footage is totaly diferent then grading 100Mbs 10bit noisy footage . Or even if the footage is n't noisy , the 200Mbs codec should win hands down . Banding often accours because of insuficient codec bandwith . IMHO bits are overblown by manufacturers for artificial price range segmantation of cameras and for marketing purposes . Same goes for chroma resolution <p> It looked bogus to me , too , and my gut is that it would n't make any difference if they padded out the two extra bits from 8 to 10 in the recording . <p> I agree that pushing Log images around is going to be problematic if you do n't have enough bits to really capture the signal and withstand this much tweaking . Those toy camera images fall apart very quickly if they 're not just @ @ @ @ @ @ @ @ @ @ I have seen C100 footage online where it was recorded to a 10 bit external recorder ( even thought it 's only 8bit hdmi out , and it was only marginally better . I also agree with you and Juan about shooting low contrast and adding back in post and how it is n't effective . I 've played with a lot of footage shot in Cinestyle by other users , and it just falls apart extremely quickly . Point is , you ca n't get blood from a stone . There 's really no way around it . The only saving grace would be an uncompressed clean HDMI 10 bit signal out of these DSLR/mirrorless cameras . I also disagree that bit depth for color is less important than bit rates . At some point it 's hard to distinguish what 's a 100mbps piece of footage , and what 's 200mbps . However , I 've graded 14bit , 12bit , 10bit and 8 bit footage and the higher the number , the MUCH more flexibility you have - in my experience . <p> Also I do n't  @ @ @ @ @ @ @ @ @ @ the camera . From what I understand , the general school of thought is that you ETTR for digital , and you expose for the shadows in film - but that 's only because it 's harder to recover any information if you trying filming those vice-versa . But ETTR is still good practice ... if the exposure is any farther " left " , the blacks will inherently be darker and the noise just increased when the contrast added back in. 
@@44332840 @4332840/ <h> Customising Window Layout Resolve 12 <p> BUT , is there any way with a 2x monitor setup that we can have 1x monitor as our ' Full Screen ' monitoring out and our second monitor as our ' Normal Workspace ' The scopes are on a floating palette , but nothing else . So customising is very limited . <p> We know we can buy a third monitor to simply have full screen , but our desk space is tight for this . We could also possibly buy a convertor for a different type of output monitoring . However I 'm sure there are thousands of DaVinci users with the same dilemma . <p> If someone at Blackmagic is reading this can you please make the above possible ? <p> On FCPX it 's as simple as WINDOW&gt;Show Viewers on second display . <p> If you want people to start using Davinci to edit in then i would suggest you add this feature ? <p> If we are being completely stupid and there 's an easy fix then please let us know . <p> There was a time @ @ @ @ @ @ @ @ @ @ , OK , maybe it did n't seem reasonable , but we did it without even thinking about it because that 's what it cost and we needed it for our job . <p> Something worth bearing in mind is that if you did get the option to have full screen on a second monitor then that display would be driven by OSX , and therefore subject to Apple 's internal colour science ( and somewhat beyond your control ) . The set-up with FCPX works fine , but for more colour critical control it is better to get a clean , untouched signal out to your monitor . The UltraStudio Mini Monitor outputs both HDMI and SDI simultaneously , which can drive both an external monitor and separate scopes , so pretty good value all things considered . <p> We need a permanent " sticky " comment explaining to newcomers why a color-managed output is necessary for all color-correction systems , why calibrated monitors are important , and why a $299 computer display will not work as a grading monitor . <p> As to a customizable GUI : BMD @ @ @ @ @ @ @ @ @ @ easily be changed . In the hardware version of daVinci ( the 2K and 2K+ ) , you had up to 8 different selectable desktops onto which you could place and size different elements of the GUI . Each desktop was selectable via a push on the panel . We do n't got that no mo ' . <p> As it is , BMD has at least leaned in that direction with the new 2-display GUI option for v12 , so they are listening and they do understand that people would like some degree of customization . My guess is that we 'll see a little more of this in future versions , but not soon . <p> Having just purchased a BMD convertor to get a pure video signal , can we now calibrate our monitor through this ? Or do we need to purchase a Flanders or some other type of ' Grading ' monitor ? 43971 @qwx453971 <p> By converter , which one are you referring to ? To get a baseband ( " pure " ) Video signal out of your machine you 'll need a @ @ @ @ @ @ @ @ @ @ like a Decklink , Intensity , or UltraStudio . If you 're attempting to turn an HDMI graphics card output into , say , SDI with a converter , that 's not the same thing . <p> As far as grading monitors go , there are some people who start out doing less critical grading on better ( Eizo , NEC ) computer monitors , however , even then they are using a Decklink or Intensity to drive it . <p> If you are being honest about " bringing grading in-house " then real monitoring is the main thing that will allow you to actually do that . That means an investment in an FSI , Sony , or other 10-Bit Grade One HD-SDI reference monitor as soon as possible , because until you have one , you 'll just be guessing . <p> By converter , which one are you referring to ? To get a baseband ( " pure " ) Video signal out of your machine you 'll need a dedicated video interface ( " video I/O card " ) like a Decklink , Intensity , or UltraStudio @ @ @ @ @ @ @ @ @ @ card output into , say , SDI with a converter , that 's not the same thing . <p> As far as grading monitors go , there are some people who start out doing less critical grading on better ( Eizo , NEC ) computer monitors , however , even then they are using a Decklink or Intensity to drive it . <p> If you are being honest about " bringing grading in-house " then real monitoring is the main thing that will allow you to actually do that . That means an investment in an FSI , Sony , or other 10-Bit Grade One HD-SDI reference monitor as soon as possible , because until you have one , you 'll just be guessing . <p> If the content is to be used online only then would it make more sense to grade on a " Computer Monitor " as this is the environment it 's likely to be seen in ? 43971 @qwx453971 <p> The problem is that even in an all-sRGB world , it 's the wild west . You could look at your content on 10 different computer @ @ @ @ @ @ @ @ @ @ are varying opinions on what can change between Rec709 and sRGB , but the theory is that the color space is very close and gamma is basically 2.4 instead of 2.2 . It 's not a gigantic change , but it 's just a simple technical LUT to get there . <p> Jason is absolutely correct that this NEC is not a grading monitor -- it 's what I would call a reasonable graphics display for computer use . Real Grade 1 grading displays are covered in this EBU document : <p> As a beginner in color correction , I invested in one of the best computer displays last week . I bought the EIZO CG247 and also purchased a calibration software and a probe for profiling and calibrating this monitor . <p> It 's not the most accurate and best looking solution , but it 's gon na work for me . The EIZO uses a 10bit wide gamut panel and 3D LUTs that can be altered by the calibration software ( Lightspace CMS ) . <p> The problem is that even in an all-sRGB world , it 's @ @ @ @ @ @ @ @ @ @ on 10 different computer displays and literally see 10 different looks . <p> There are varying opinions on what can change between Rec709 and sRGB , but the theory is that the color space is very close and gamma is basically 2.4 instead of 2.2 . It 's not a gigantic change , but it 's just a simple technical LUT to get there . <p> Jason is absolutely correct that this NEC is not a grading monitor -- it 's what I would call a reasonable graphics display for computer use . Real Grade 1 grading displays are covered in this EBU document : <p> Thanks for the excellent tech doc . I agree that with most users of computer displays it is the wild west , however my background is stills advertising . Colour ( Color ) for me is absolutely critical . <p> We have been working with stills in a fully hardware calibrated environment for a long time ( over 15years ) When we started out many printers used a ' Closed Loop ' calibration meaning they matched their printers to their monitors and vice versa @ @ @ @ @ @ @ @ @ @ profile . They would often have to adjust images to match their printing and monitoring workflow . Now that was the wild west . <p> This changed as people adopted ICC standards . http : **28;1338;TOOLONG However this was setup for ' Printing ' on paper , the display of Video or moving images is very much a different ball game . <p> My take on all of this , is to grade footage on a decent grading monitor obviously being fed by a proper 10bit video signal , if the footage is to appear ' online ' then also do a secondary QC of the final footage on a ' Calibrated ' ( D65 white point ) computer monitor . <p> Of course there is no control in terms of the final display conditions . ( e.g .. someone viewing online ) but we need to ensure a consistent workflow and reference point . <p> My take on all of this , is to grade footage on a decent grading monitor obviously being fed by a proper 10bit video signal , if the footage is to appear ' online @ @ @ @ @ @ @ @ @ @ footage on a ' Calibrated ' ( D65 white point ) computer monitor . 43971 @qwx453971 <p> Steve Shaw of Light Illusion has some good tech papers that go into detail about the differences between various monitors and what 's involved with proper calibration for broadcast and theatrical : <p> There 's also been many dozens of discussions about these issues here on the website . One ongoing problem is how monitors can change depending on which angle you view them , and also the problem of finding monitors that have reasonable panel consistency across the entire surface . That 's a big difference between a modest display and a real Grade 1 grading display . <p> I have to confess , as long as colorists have been fighting problems with monitors that do n't match and clients that ca n't grasp the need for calibration , it might actually be worse today than ever before . For example , in the 1980s and 1990s , I think the vast majority of clients were at least watching on a similar CRT display . Now , they could be looking at @ @ @ @ @ @ @ @ @ @ plasmas , OLEDs , god knows what . But then , it 's not much different from doing a very complicated sound mix and then have people listen to it on cheap earbuds , 3 " speakers , computer speakers , large home speakers , car speakers , and theater loudspeakers . It sounds different on all of them . It 's a real challenge to create a master ( picture or sound ) and have it translate across all potential destinations . <p> We are using NEC Spectraview Reference Monitors using hardware calibration . However I will confess my background is stills photography . We are looking at bringing more grading in house . <p> Having just purchased a BMD convertor to get a pure video signal , can we now calibrate our monitor through this ? Or do we need to purchase a Flanders or some other type of ' Grading ' monitor ? 43971 @qwx453971 <p> Yes . Calibrate your NEC using the spectraview software and device for REC709 targets . This will load the calibration in the monitor hardware . The icc profile in your system @ @ @ @ @ @ @ @ @ @ . Then you can go from your BM card through HDMI to your NEC and your calibration should still be in effect . I do n't think you can calibrate using Spectraview through the BM card output directly . <p> I have done this with my Eizo monitor with an AJA card and Speedgrade and it worked well . Just starting with Resolve now , but I plan to use the same strategy after I get the mini monitor card ... <p> As confirmation with AJA card/speedgrade , I could not see any difference between the HDMI or displayport output into the screen vs the AJA card output . Which confirmed that the icc profile in the system is not changing the signal . If resolve allowed full screen on the 2nd monitor , without an output card , I 'd skip the output card 
@@44332841 @4332841/ <h> FSI BM 230 pls give me an idea of a fare price . <p> UPDATE : SOLD . thanks for your input , it helped Hi guys , I 'm at a point where i might have to sell most of my gear ( havent decided yet wether to keep a home setup or not ) . Would be nice to know what can i sell my FSI BM 230 for . Bought it new at fsi webshop around fall 2014 , not many hours on it ( dont know where to look up exact number ) , located in Copenhagen , Denmark . thanks for help in advance <p> Hi guys , I 'm at a point where i might have to sell most of my gear ( havent decided yet wether to keep a home setup or not ) . Would be nice to know what can i sell my FSI BM 230 for . Bought it new at fsi webshop around fall 2014 , not many hours on it ( dont know where to look up exact number ) , located in Copenhagen , Denmark . thanks for help in advance 
@@44332842 @4332842/ <h> The future of Mac pro <p> " Apple Kills the Thunderbolt Display - Will the Mac Pro Be Next ? " On June 23rd , Apple announced that the aging , obsolete , overpriced Thunderbolt Display is being discontinued . No replacement display was announced , and customers have been directed to 3rd party products . What does this mean for the Mac Pro ? <p> i have always felt the nano ( trashcan ) Mac Pro was yet another attempt to relive the NeXT cube in all of it 's monolithic glory , just like the Mac Toaster was and to a lesser degree the Sunflower iMac and they all but the nMac Pro met an early death , never to be spoken of again . <p> I think most here would agree that the nMac Pro has been met with more derision than ALL of the failed Apple products I mentioned above . It will be interesting to see if Apple carries on with the current form factor or not . At the very least , we will know with a degree of certainty whether or @ @ @ @ @ @ @ @ @ @ hat with Apple or not . <p> The only thing I can see that would save Apple 's bacon with this crowd , should they continue with the current form factor , would be to release a caboose , a secondary box to house graphics ( and the like ) cards that would be tethered by a cable meticulously designed by Jony Ive himself . <p> I sure hope we get closure on this , as it 's been going on since Apple began to capitulate on updating the Cheesegrater MacPro . That is over 6 years ago now . <p> i have always felt the nano ( trashcan ) Mac Pro was yet another attempt to relive the NeXT cube in all of it 's monolithic glory , just like the Mac Toaster was and to a lesser degree the Sunflower iMac and they all but the nMac Pro met an early death , never to be spoken of again . 43971 @qwx453971 <p> And do n't forget The Cube , which was another " design over function " product that received really , really bad reviews . There @ @ @ @ @ @ @ @ @ @ I hardly know where to begin . Just the problem of lumping all the i/o ports together in about a 2 " space , where it 's very difficult to plug and unplug different ports at the same time ... that 's a nightmare . Please do n't get be stahted on the D700 GPUs . <p> I think everybody would 've been delighted if they had just come out with a slightly-smaller updated cheesegrater Mac with easy-access PCI slots , an upgradeable CPU , easily replaceable internal SSD , easily replaceable RAM , and the ability to use 3rd-party GPUs . If this had been done as a 19 " rackmount device , even better . <p> I was just at Melrose Mac this afternoon , and even the guy at the counter was kind of glum about the Thunderbolt monitor news , and he agreed with us that Apple could be turning its back on the whole pro audio/video market . <p> - The Cube failed miserably since it cost MORE for far less performance than a PowerMac G4 at the time and really had a performance issue @ @ @ @ @ @ @ @ @ @ . <p> - The Trashcan has been around for quite some time so I ca n't see it being a complete failure . The machine would however make far more sense as a " prosumer " machine . A high-end I5 / i7 machine with equal GPU choices . The perfect desktop for a lot of people . Silent , small yet relatively powerful . And external upgrades are decent for a lot of cases . <p> ... but the problem is there is n't anything above the Trashcan for us Mac users . There should be a real " MacPro " above it . Perhaps it does n't have to be the massive MacPro 2006-2012 case , but it should be more than a desktop and also more flexible . Then again , having both a MacPro , MacTube and iMac is apparently one to many machines for Apple . <p> The loss of the Thunderbolt Display I ca n't say is a loss more than the possible sign Apple is pulling out from desktops . And that Apple is one of they few companies that can design a @ @ @ @ @ @ @ @ @ @ by UK magazine RedSharkNews to write an opinion piece on this very topic . My story poses the point that with the release of Nvidia 's latest GTX 1080 GPU , video on Mac is fast becoming a dead issue . <p> The article has now enjoyed over thirty thousand reads and with more than seventy reader comments , it is currently the second most popular article RSN have ever published . The reader 's comments probably shed more light on this frustrating issue than the original opinion piece . <p> But there 's a new watch ! And it tells you to breathe ! And are n't you exited about small expensive wireless headphones that only last 5hrs &amp; that if they 're like the regular ones , you 'll be losing regularly ? <p> I 've been moving away from Apple gradually for a while now . iPhone and iPad have been replaced by Android ( iOS limitations drive me crazy ) . Mac Pro by custom built workstation ( runs circles around current top-of-the-line nMac Pro ) . The only two Apple products I use personally are @ @ @ @ @ @ @ @ @ @ Android alternative but could n't ) , a Mac Mini I use as a media server , and a Retina MacBook Pro 15 " fully loaded . My next laptop will definitely be a Windows gaming one because of the proprietary Apple SSD crap and everything else soldered on the motherboard . <p> I 've had a long history with Apple computers , since the first MacIntosh in 1984 . Over the decades , I 've indirectly sold a ton of Macs and other Apple products by recommending them to friends and readers , and everyone at home has a Mac and an iPod . But for the past couple of years I 've been steering people away from Macs . To me , a computer is nothing but a tool . And Apple tools are , in my opinion , no longer the best . It 's really sad , because Apple used to be a cool company . Nowadays , however , all they care about is money selling gadgets , and form over function . It 's as if someone you 've loved all your life all @ @ @ @ @ @ @ @ @ @ hope that they 'll return to being their former selves , but deep inside you know it wo n't happen . So , it 's time to move on . <p> My only obstacle now is with clients who still use one of the Final Cut flavors and ca n't properly format a project and just send me an xml . Because of them , I must still have the Apple editing software installed here . I guess I 'll just keep my laptop exclusively for running these programs when my transition away from Apple is finished . <p> I really did n't want to move - and have even started a thread a couple of days ago , asking which route to take - used nMP or Hackintosh ... and by Saturday I should be running my first Windows based computer in over ten years ... I hope apple will finally show something usable ( and expandable ) , but seeing what they 've been up to for the last few years - I highly doubt it . <p> But there 's a new watch ! And it tells @ @ @ @ @ @ @ @ @ @ small expensive wireless headphones that only last 5hrs &amp; that if they 're like the regular ones , you 'll be losing regularly ? <p> Does anybody here have first hand experience in having their ffmbc/ffmpeg PC based ProRes encodes rejected by QC ? ( and I mean first hand , not your mate 's cousin 's brother 's best friend ! ) Perhaps fear of QC rejection is just another Mac Community 's urban myth ? <p> I think the root of all this hand wringing over the lack of new MacPro is a simple lack of desire to explore other solutions . Yes , it 's easy to blame Apple for dropping the ball , but it 's hard to make a decision to move to another platform . OS is just another tool , that clearly should be chosen based on the performance needs and ease of use and not at the expense of performance , lack of flexibility , upgradability and huge price premium . It 's always a scary proposition to abandon true and tried tools and at times , it may feel , like @ @ @ @ @ @ @ @ @ @ this change will bring countless benefits . There are plenty of tools on other OSes , that can read and write to HFS+ and NTFS drives and equally many tools , that can read and write Prores . The most adventurous , that will put in the time to learn Linux will eventually get the payback in spades . Hardware is unlimited , there is no Big Brother , that you need to beg for an OS update in order to be able to get a new capabilities , because every software manufacturer is a virtual OS developer . All of a sudden your software now is running on top of enterprise level hardware/software with unrivaled flexibility and stability . So , I feel , it 's time to make the hard decisions , stop waiting for an Apple to deliver another lame hardware upgrade and move on to another platform . 
@@44332844 @4332844/ <h> 4K Client Monitors as of Nov 2015 <p> Hey guys so our facility is wanting to upgrade some of the current client monitors in the building to 4K models as a few of the projects we deliver are now asking for UHD ( 3840x2160 ) deliverables that we want them to be able to see at " full-rez " during our review sessions . We currently have a Panasonic VT60 Plasma as our client monitor in the main room we want to change out to a UHD capable display as a test . <p> I 've been following some of the discussions on the forums and I 'm trying to make a recommendation on a decent " middle of the road " 4K client-only display to try in there ( think sub $3500 , this is not the primary grading monitor , its just the client display ) . I 've solicited some feedback from my former calibration specialist that I used all the time in LA and he 's said he 's had better results calibrating the Samsung 4K consumer monitors at that price point , specifically @ @ @ @ @ @ @ @ @ @ have recommendations on a decent 4K client monitor at this point in the year ? Really feel like the VT60 's were a great standard for the price for client monitoring and we do n't have a great alternative yet for the money unless we jump up in price to OLED or something like that . I know that the Sony X9000 has been mentioned a few times on the forum by Craig Marshall and others ... any updates or thoughts on that model now ? ? <p> Well , I hope this statement does not come from someone who has never spent any real time with one . As noted previously , I have been using the Sony 55 " Bravia X9000B ( X9005 in some markets and most on-line reviews ) as a 4K client monitor , fed SDI directly from our Resolve 's Decklink 4K SDI card for nearly a year now and apart from the fact is does not display HD video at 25fps natively via HDMI 2.0 , ( it is of course , a native UHD display ) the 2014 Sony continues to stun @ @ @ @ @ @ @ @ @ @ quality and sheer visual ' presence ' . The 2015 model offers even better , more precise control over white balance and HDMI 2.0a HDR inputs . Something to be considered . ( We paid A$2495 delivered , for our 2014 model ) <p> Of course , my statements must be tempered by the experience and level of technical expertise ( or lack there of ) of our client base . <p> The only UHD displays that even remotely have a chance right now are the Panasonic CZ950 and the LG 9500 . The CZ will only be in limited release , but will be followed up next year by new models that should be more affordable . The LG is cheaper and available now , but they are wildly inconsistent from unit-to-unit , so if you buy more than one for your facility you 'll have a pretty brutal time getting them to match each other . What 's even more difficult is that they do n't translate well to other displays , so grading a project on a 9500 could possibly result in a " wtf " moment once @ @ @ @ @ @ @ @ @ @ perspective the VT60 is still pound for pound the best ( post production ) display on the planet right now . If you can , keep using your VT , and buy when the next gen of OLEDs come out next year . <p> Well , I hope this statement does not come from someone who has never spent any real time with one . As noted previously , I have been using the Sony 55 " Bravia X9000B ( X9005 in some markets and most on-line reviews ) as a 4K client monitor , fed SDI directly from our Resolve 's Decklink 4K SDI card for nearly a year now and apart from the fact is does not display HD video at 25fps natively via HDMI 2.0 , ( it is of course , a native UHD display ) the 2014 Sony continues to stun our clients with its resolution , colour balance , sound quality and sheer visual ' presence ' . The 2015 model offers even better , more precise control over white balance and HDMI 2.0a HDR inputs . Something to be considered . ( We @ @ @ @ @ @ @ @ @ @ Of course , my statements must be tempered by the experience and level of technical expertise ( or lack there of ) of our client base . 43971 @qwx453971 <p> The viewing angles on that display are a joke . I 'm glad it 's working for you . But every time I 've seen my work on one there is a 15 deg sweet spot , and the rest is cringe worthy . And yes I have spent time on it . <p> The only UHD displays that even remotely have a chance right now are the Panasonic CZ950 and the LG 9500 . The CZ will only be in limited release , but will be followed up next year by new models that should be more affordable . The LG is cheaper and available now , but they are wildly inconsistent from unit-to-unit , so if you buy more than one in your facility you 'll have a pretty brutal time getting them to match each other . What 's even more difficult is that they do n't translate well to other displays , so grading a project on @ @ @ @ @ @ @ @ @ @ moment once its shown on another display . 43971 @qwx453971 <p> Yeah , that would be a problem as " the powers that be " want something scaleable to all the online suites as well as the color bays ... so we 'd need a number of them and that would be a problem it sounds like with the LG . The Panasonic CZ950 is the 10K Mike Sowa inspired model correct ? <p> No one out there has had any experience with the current Samsung line ? <p> Yes , my 24 " HD reference monitor is in the room but can not be seen by clients so avoids the potential ' I want that red ' moment . The SDI feed from my Decklink 4K SDI card is a longish run to the X9000 so I convert it right at the set with BMD 's SDI to HDMI 4K converter . ( rather than running a long HDMI cable ) <p> PS Currently testing BMD 's new 12G SDI 4K ' Pro ' card with Resolve which as two SDI outputs - one of which can be configured @ @ @ @ @ @ @ @ @ @ HD grading monitors with UHD signals . <p> Yeah , that would be a problem as " the powers that be " want something scaleable to all the online suites as well as the color bays ... so we 'd need a number of them and that would be a problem it sounds like with the LG . The Panasonic CZ950 is the 10K Mike Sowa inspired model correct ? 43971 @qwx453971 <p> The CZ is the model Mike Sowa has been endorsing , but what 's more important is that Panasonic is actually seeking the input of the post community to guide their display development . A number of people are interested in the LG 9500 as an affordable alternative , but the reason its cheaper is that LG is less selective about their panels , and use a much less sophisticated backend . Since cost is probably an issue , if you have to buy now , one idea would be to purchase six of the LGs on credit , pick the best two or three based on their panel qualities and how well they match , and @ @ @ @ @ @ @ @ @ @ you want to do is put a 5% black patch up and look for a 3 to 5-inch vignette on the left and right the edges of the display as well as vertical banding that looks like this ... <p> All of these panels come from the same factory , so you wo n't be able to escape it completely , but this is what you want to look out for if you 're going to jump in now . The positive is that you 'll get great black levels , clean whites , good color accuracy , and off-axis performance that allows two people to view the monitor and actually see the same image . <p> No one out there has had any experience with the current Samsung line ? 43971 @qwx453971 <p> All of the Samsung SUHD stuff like the JS8500 you pointed out has huge off-axis issues , and really wonky RGB tracking that is very difficult to calibrate . The only UHD Samsung worth bothering with is the older HU8550 , which calibrated nicely , had decent off-axis response , but is edge-lit LED so it @ @ @ @ @ @ @ @ @ @ the display . If you have LCD reference monitors , that actually might be worthwhile though as you 'll be able to match them with a 8550 much more easily than you will with the LG OLED . <p> We bought a Samsung UE55JU7000T as a client monitor , and although if you are looking straight onto it is pretty decent I would not recommend it to anyone . You need to be sitting and looking absolutely straight into it . Sit half a meter towards the edge of the sofa and the image is not the same anymore . <p> The viewing angles on that display are a joke . I 'm glad it 's working for you . But every time I 've seen my work on one there is a 15 deg sweet spot , and the rest is cringe worthy . And yes I have spent time on it . 43971 @qwx453971 <p> 15 degree sweet spot ? Really ? It sounds a lot like you are referring to Sony 's earlier 4K Bravia with almost exactly the same model number . They used a totally @ @ @ @ @ @ @ @ @ @ ' from another manufacture using QD Vision 's quantum dot technology and called it the Triluminos display . The 2014 and 2015 models use a completely new panel built by Sony in Japan and when it comes to off axis colour balance , certainly our VA panel matches or exceeds our IPS displays . <p> Btw , I do n't want to single out Sony or any other manufacturer . In fact Sony 's displays are amongst the best available . It 's just that LCD tech is awful at large sizes , and higher pixel density has meant smaller pixel sites and with it even worse performance . Not to mention the whole market racing to the bottom end . It 's a recipe for the display tech actually moving backwards . 
@@44332846 @4332846/ <h> Current State of Native R3D Playback <p> For most of the RED projects I 've worked on , we 've generally transcoded to flat , high-quality ProRes/ DNxHD/ DPX from the beginning , or onlined with a RED Rocket later on . <p> Many apps now support R3D , but traditionally we 've found the usefulness of native playback pretty limited . Many of the booths I visited at NAB were still running RED Rockets in their systems , but I 've begun to hear reports that some systems like Mistika , Pablo , Premiere CS6 , and others are starting to offer usable , software-based , R3D de-bayer . I believe many of them are handling it by offloading to CUDA , as I know de-bayering via CPU will still bring most workstations to their knees . <p> So , are any of you having especially good experiences with native , real-time R3D file playback in any of the apps you use ? <p> I 'm on Resolve and FCP7 . I 'm considering Creative Cloud and Premiere CS6 when it becomes available . I do not @ @ @ @ @ @ @ @ @ @ plays OK on my Mac Pro in Resolve . Clients do n't know what 's going on , and do n't pick on this . But the times I 've had 5K and I need to go down to quarter debayer , they 're completely shocked that RED can look so bad , and I have to tell them why . Performance , feedback during work etc .. <p> I 've yet to do a job where I 'm handed DPX , ProRes or similar on a job shot on RED . I wish people would do that . I 'm sick of rendering things for hours , while for DPX , TIFF or ProRes , the render would be way faster . <p> What is good for when there are in use 3 dialed keys before full debayering , underexposed and in highspeed . When used RCX it also needs redrocket to see what to dial , 1/4 playback can not be used for reference . <p> And as I found there is no need to render every frame to DPX , just the crucial one , but this @ @ @ @ @ @ @ @ @ @ to come bact and back to render more DPXs and this can be annoying . <p> Typicaly i use full premium , leave the r3d 's on the clients drive hanging off a FW cable , and cashe 32float 2K or HD proxes . <p> We debayer with the frames coming across a FW cable , so it 's only going to run maybe 5-6fps , but that 's enough to get the debayer settings correct <p> It 's not alot longer to render selects cashes from an external drive than it is to copy all the r3d 's to an array to get fast playback from the selects needed ... really depends on how long the takes are ... moveing single 280gig r3d file to get 12 frames is nuttttzzzzzz .. but that really has happened <p> James was in yesterday , he conformed and tweaked and debayered carefully a 17 min reel , i was working on a F3/Slog show in another suite , after we were done for the day , i tweaked a commerical in his room , and kicked off his render - it was @ @ @ @ @ @ @ @ @ @ pull Tb 's of r3d 's to local storage to get to the same place . <p> Not a fan of dealing with a DiT 's version on what he thinks i might want burnt into ProRez , but making my own DPX ? yea maybe .. DPX is RT straight off the mark .. but i do like debayering in context .. that 's ' the biggest advantage .. that and working off Full/Premium so there 's no questions left unanswered . <p> Hi , the only system i work perfectly with native R3D is CLIPSTER . But all we know that is a software to conform not to edit , but clipster with his internal hardware ( without need redrocket ) works with R3D with no problem , i think since 3.7 or 3.5 version <p> 3 5K stacked layers plus an audio layer ... on a 6gig/i5 HP laptop with onboard Nvidia ( the built in chipset cheapie one ) - using the R3D 's from the system drive and getting real-time in 1/8 .. 43971 @qwx453971 <p> Three tracks of 5K R3D is way better than @ @ @ @ @ @ @ @ @ @ able to get one track of 4K R3D to playback in our earlier trials on Media Composer and Premiere . But , even with one track you were still limited to the most basic effects , and you 'd use up most of your CPU just taking care of playback . We decided to keep an eye on it , and have continued to transcode everything at the beginning of post in the mean-time . <p> It does look like there are still limitaitions though ... <p> Not work for R3D : 1 . warp stabilize , adobe warning about size of my video 5k. 2. multicam 4 video is ok , with 5.6.7.8. and more video 5k not work i have media offline error . 3. with multicam not have audio , lose . <p> I have recently as of last week began to work with Epic R3D files natively in Premiere Pro CS6 Project setting to 1080 23.98 . When play out I do however have to put playback resolution at 1/2 . I 'm running a 2011 MacPro with a Quadro 4000 . I 'm not getting @ @ @ @ @ @ @ @ @ @ I do n't have to do the clipfinder dance to Resolve . <p> Also another thing , though should not matter once Resolve V9 is released , is I can now create a sequence(s) for dailies that have on cam audio . Send to Resolve-Grade - then lay back the graded material over the original source audio . Then Sync the externally recorded audio . <p> Hi , the only system i work perfectly with native R3D is CLIPSTER . But all we know that is a software to conform not to edit , but clipster with his internal hardware ( without need redrocket ) works with R3D with no problem , i think since 3.7 or 3.5 version 43971 @qwx453971 <p> andres clipster have a red rocket ... they invented it ! the RedRocket is produced by DVS and re-branded by RED , i believe every cloister have one ( but look different ) 
@@44332847 @4332847/ <h> The function of load memory ? <p> Hey guys I need something explained .. Its mad basic but I just do n't  get it . What 's the point with " load memory " to a clip ? Append a memory/correction I can understand but add a correction and overwrite everything ? <p> Let 's say I have corrected most of my shots and i 'm now ready to apply the look i want . I make the look and save it to memory which is nice cause you have a nice keyboard shortcut for it . But then loading that look to another clip just overwrites the correction . And even if you delete the correction node before saving to your memory it still only makes sense to me with appending a correction to a shoot . <p> Can someone please explain a workflow for the memory save/load , sitting here with a AVID MC with save and load memory buttons and I cant seem to figure out how to use them in a productive fashion . <p> 1 . Do your initial balancing correction ... how ever many nodes @ @ @ @ @ @ @ @ @ @ . 2 . Add your ' look ' nodes after your initial balancing nodes ... How ever many it may take . 3 . DELETE the nodes from step 1 ( so all you have is your ' look ' nodes . The clip will look wrong but bear with me ... ) 4 . Right click the viewer and add the still to memory 5 . UNDO ( ctrl-z ) until you get your step 1 nodes back . 6 . But check it out ! The still is still in your memory ! So ... <p> Now , you can do you initial balancing pass on any of your clips and use the ' Append ' function to apply JUST your look to the end of the original correction ( from the still you added ) . The better you get at the balance pass , the better your look node will sit on top of it with out too many tweaks . <p> ( Also , you might want to change your topic question to better reflect your actual question . It will help others in the @ @ @ @ @ @ @ @ @ @ I can append by clicking at the the saved memory , this should be the default . What I do n't  understand is the function of " load memory " . That 's what I mean by append makes sense but then you have use the mouse all the time instead of " load memory " shortcuts 
@@44332848 @4332848/ <h> the SIZE of a grading monitor : 24 " vs 27 " or even 31 " <p> I 'm gon na buy grading monitor for HD ( rec.709 ) and DCI 2K ( DCI-P3 ) resolutions . Eizo is my choice ( together with i1 Display Pro ) and I think of the following models : <p> CG2420 + eeColor LUT Box <p> CG277 <p> CG318 <p> - but do not have direct experience with monitors bigger then 24 " . Before I decide on the monitor 's size I 'd like to hear all pros and cons - your first-hand experience opinions - of working with 27 " ( and bigger ) displays comparing to work on 24 " display . <p> I really need meaningful opinions of someone who have professionally worked with bigger displays . I do not have such experience but can imagine that the display size could have some more serious consequences besides table space occupation . <p> Not the question you asked , but the eeColor will not pass 2K image data . It is just HD . And none of those @ @ @ @ @ @ @ @ @ @ ) , so I would not worry about that colour space . <p> Not the question you asked , but the eeColor will not pass 2K image data . It is just HD . And none of those display will do DCI-P3 accurately ( very few displays will ) , so I would not worry about that colour space . <p> Steve 43971 @qwx453971 <p> I remember the closest you could get to DCI P3 used to be Panasonic VT plasmas , which have since been discontinued . I do n't know what people are using these days . I 'm hoping one of these 4ktvs with hdr/dolby vision/quantum dot etc will cover 100% of P3 . <p> EDIT : I just found out the new ipad pro launched yesterday supports DCI P3 . Holy smokes ! <p> EDIT : I just found out the new ipad pro launched yesterday supports DCI P3 . Holy smokes ! 43971 @qwx453971 <p> The 9.7-inch iPad Pro features advanced display technologies , including a True Tone display , which uses new four-channel sensors to dynamically adjust the white balance of the display to @ @ @ @ @ @ @ @ @ @ accurate , paper-white viewing experience . The advanced Retina display is 25 percent brighter and 40 percent less reflective than iPad Air- 2 , making content even easier to see indoors and out . It uses the same wider color gamut as the iMac- with Retina 5K display , delivering 25 percent greater color saturation- for more vivid colors . A custom timing controller , photo alignment and oxide TFT deliver incredible color , contrast and clarity . Night Shift in iOS 9.3 uses iPad Pro 's clock and geolocation to automatically adjust the colors in the display to the warmer end of the spectrum after dark and may even help you get a better night 's sleep . 43971 @qwx453971 <p> Not tested the actual screen yet , but I seriously doubt it is accurate ... having a gamut the same as the iMac suggests not . <p> In my eyes color and contrast are totally out of control on any iOS device . Apple said they have tons of optimization techniques but they all lack the manual control we need . They are made to make everything look @ @ @ @ @ @ @ @ @ @ link Andrew - I 've read the thread before - there 's no mention on the display size in the context that interests me . My grading monitor will be supplied with the signal from Decklink card - GUI issues connected with display size mentioned by Jason Myres does not apply to it , neither post by Tyson J. Grubb regarding TV signal , because I need a solution not only for a TV broadcast purpose but cinema grading as well . My cinema grading will be fine tuned in a nearby cinema . <p> The problem with assessing a grading monitor by " percentage " of P3 or Rec709 specs is that you have no idea how they measured the monitor , it does n't mention the problem of display consistency , nor does it cover the problem of viewing angle . All these things are major issues that frequently disqualify a lot of consumer and computer displays out there . <p> Size of display is not necessarily an issue provided the display is calibrated correctly . I have literally done D-Cinema jobs with 32 " monitors , but @ @ @ @ @ @ @ @ @ @ actually delivering the final project , and there were frequently minor touch-ups required . The biggest problem I 've encountered is that there are judgements you can only make for theatrical releases with a real projector , say 12 ' -20 ' wide . Anything less is going to give you inaccurate , unpredictable results . I would also say that trying to do a home video project on a 20 ' projector may yield inaccurate results as well , depending on how the display is calibrated . <p> 98% DCI-P3 does n't mean 98% accuracy , but 98% gamut match , which is a total different story . If you need DCI emulation with low budget projects , these emulations are good to get the idea of the bigger gamut . <p> If accuracy is important you better calibrate and work with REC709 and use a LUT later if you do DCPs inhouse . It has widely proven to be the better way . <p> As Marc says you need to verify those DCIs on a calibrated projector . It will look different for sure . 
@@44332849 @4332849/ <h> Worldwide list of 48fps theaters <p> The fate of High Frame Rate cinema appears to be riding on the public 's response to The Hobbit : An Unexpected Journey . If you want to check it out at 48fps , this is a very complete list of HFR theaters around the world . Warner plans to add many more theaters if the response to HFR is good , so check back later if there is n't a theater near you initially . 
@@44332852 @4332852/ <h> Behind the 6K finish of House of Cards ' <p> Behind the 6K finish of House of Cards ' Thursday , August 20 , 2015 4pm &amp; 7pm Join us for an exclusive presentation on the making and finishing of Netflix 's acclaimed series House of Cards - Season 3 . <p> This unique event will feature a screening of the original 6K EPIC DRAGON source footage , along with a demonstration of the work that went into meeting the artistic and technical challenges posed by the Emmy-nominated series " all presented live on the Baselight color grading system and projected in 4K . <p> Colorist Laura Jans-Fazio and Producer Hameed Shaukat will discuss their collaboration in maintaining the look of the prestigious political drama , and how their production workflow continues to evolve . They will also unveil the efficiencies behind the streamlined production-to-color process , from set to deliverables in uncompressed 6K and Ultra HD . <p> It 's hard to figure out , but it 's going to be at Red Studios on Cahuenga Blvd. in Hollywood near Melrose . Should be very interesting -- @ @ @ @ @ @ @ @ @ @ Laura and the DPs do a great job on it . 
@@44332854 @4332854/ <h> Auto Sync Audio via Timecode in Resolve 12 <p> If you are working as a DIT like I do your often asked to sync the secondary sound from the set to the dailies . In prior versions of DaVinci Resolve this was an easy task . First you created a folder in the Media Pool . I would give it the same name as the card for simplicity . You would then import the video media into that bin . You would then import the matching audio sound files that share the same timecode into that bin . If you wanted to you could make a sub-bin in the main bin for the files for neatness . Once both the audio and video files were in the bin you would select all the video clips and right click to choose the option to Auto Sync Audio based on Timecode and your embedded camera audio would be replaced with the multi-track wav file from the sound dept . <p> But if were to try that today using Resolve 12 ( Which is still how the manual teaches how to @ @ @ @ @ @ @ @ @ @ Sync to be greyed out . This can cause a small panic when you have lots of dailies to make in a short time . <p> The answer , found by pure luck , is to right click on the Bin that you created to place the Audio and Video media into . Then you can see the Auto Sync options for Timecode and Waveform available to you and working just as before . Hopefully this change will show up in the manual sometime soon . <p> If you are working as a DIT like I do your often asked to sync the secondary sound from the set to the dailies . In prior versions of DaVinci Resolve this was an easy task . First you created a folder in the Media Pool . I would give it the same name as the card for simplicity . You would then import the video media into that bin . You would then import the matching audio sound files that share the same timecode into that bin . If you wanted to you could make a sub-bin in the main bin for @ @ @ @ @ @ @ @ @ @ video files were in the bin you would select all the video clips and right click to choose the option to Auto Sync Audio based on Timecode and your embedded camera audio would be replaced with the multi-track wav file from the sound dept . <p> But if were to try that today using Resolve 12 ( Which is still how the manual teaches how to do it ) you would find the option to Auto Sync to be greyed out . This can cause a small panic when you have lots of dailies to make in a short time . <p> The answer , found by pure luck , is to right click on the Bin that you created to place the Audio and Video media into . Then you can see the Auto Sync options for Timecode and Waveform available to you and working just as before . Hopefully this change will show up in the manual sometime soon . 43971 @qwx453971 <p> Thanks for the tip , Rich ! As I am about to start an edit and grading project in R12 , I am wondering ... @ @ @ @ @ @ @ @ @ @ for organizational purposes and are using R12 ? If you create an audio subfolder under your video , is R12 smart enough to look in there like in R11 ? If using R12 as a NLE and for color , I might have 20+ video folders and 20+ subfolders . <p> I agree , there are a whole bunch of things moved around in Resolve 12 -- I 'm still not happy with the way Timelines are handled , making them just another item to put in a bin -- but a lot of capabilities are still there . <p> I 've found that Resolve will generally dig down into the folders , but I 'm reluctant to go very deep on folder structure just because of past experience with slowing the program down . But it should theoretically work . <p> You are correct Marc . I was looking at an older manual , my bad . I 'm glad it was added to current version . I mainly wanted to get the answer of how to auto sync out there into something that could be found in a @ @ @ @ @ @ @ @ @ @ place they look instead of the manual . <p> My overall feeling about syncing sound in general is that this is just a standard part of post-production , and has been going back to 1927 . It 's not that big a deal . Even without any slates , a good assistant can sync up 1 take in a minute or less if the sound is logged and has notes on scene &amp; takes . Even if you have 1000 takes on a big show , that 's only two days of work -- assuming the sound sync is n't drifting . <p> Famously , Thelma Schoonmaker and Martin Scorsese had a dozen assistants and took six months to sync up the Woodstock concert documentary in 1969 and 1970 . I 've done at least a dozen rock concert shows where we wound up having to do lots and lots of hand-syncing , and it 's just a necessary part of the job . While I think of myself as " only " a colorist , I eventually developed a knack for reading lips and syncing by eye very quickly @ @ @ @ @ @ @ @ @ @ Jay Leno Showtime special in the 1980s , which was about 20 hours of live 16mm documentary footage and standup routines . I 'd say 90% of that was done with no slates , just reading lips and making educated guesses . I 'm guessing it added another couple of days to the job , but it was n't a nightmare -- it 's just part of the job . It all got finished , the sync was fine , it looked good , everybody was happy , and I got paid . Not a big deal . <p> I find a lot of younger people in the film business expect things to be automatic or easy , but they often are not . I think you have to go into this expecting things to require effort and for them to require some expertise . As it is , it 's a miracle Resolve has tools like this , because if they save you even : 30 seconds on syncing every shot , that 's a day of labor saved . <p> My overall feeling about syncing sound in general @ @ @ @ @ @ @ @ @ @ , and has been going back to 1927 . It 's not that big a deal . Even without any slates , a good assistant can sync up 1 take in a minute or less if the sound is logged and has notes on scene &amp; takes . Even if you have 1000 takes on a big show , that 's only two days of work -- assuming the sound sync is n't drifting . <p> Famously , Thelma Schoonmaker and Martin Scorsese had a dozen assistants and took six months to sync up the Woodstock concert documentary in 1969 and 1970 . I 've done at least a dozen rock concert shows where we wound up having to do lots and lots of hand-syncing , and it 's just a necessary part of the job . While I think of myself as " only " a colorist , I eventually developed a knack for reading lips and syncing by eye very quickly . One of the hardest shows I did was a Jay Leno Showtime special in the 1980s , which was about 20 hours of live 16mm documentary @ @ @ @ @ @ @ @ @ @ that was done with no slates , just reading lips and making educated guesses . I 'm guessing it added another couple of days to the job , but it was n't a nightmare -- it 's just part of the job . It all got finished , the sync was fine , it looked good , everybody was happy , and I got paid . Not a big deal . <p> I find a lot of younger people in the film business expect things to be automatic or easy , but they often are not . I think you have to go into this expecting things to require effort and for them to require some expertise . As it is , it 's a miracle Resolve has tools like this , because if they save you even : 30 seconds on syncing every shot , that 's a day of labor saved . 43971 @qwx453971 <p> Thanks for sharing elements from your past . I agree with you how there is an expectation for easy or letting the equipment do the task for you amongst many parts of out @ @ @ @ @ @ @ @ @ @ ca n't speak for episodical or feature workflows ) it seems that post starts when they call cut . Which in itself is becoming less and less often in favor of filling up the card instead of cutting . It 's only data anyways not like we are burning film stock . All the metadata , secondary sound and color profiles are added to the dailies right then and very often the drives are shipped off at what ever the latest drop off time for Fed Ex is regardless if all of todays media made it on the drive or not . <p> The Post Houses seem to have less interest in preforming these functions . I have n't been able to figure out if they just have the time or no longer have assistants to do the jobs or it 's a budget thing and production does n't want to pay for it . What worse are the agencies that have an in-house edit department . They seem to have even less of an understanding for the overall process . Now I do n't like making blanket statements like @ @ @ @ @ @ @ @ @ @ not . <p> As this technology continues to evolve giving us great tools like Resolve that let us do things we would never have thought of just a few years ago , the expectations of what can be accomplished continues to grow . What seems to be getting lost is the ability to manage those expectations from all that is possible to what is actually realistic . <p> Thanks for sharing elements from your past . I agree with you how there is an expectation for easy or letting the equipment do the task for you amongst many parts of out industry today . In the commercial side of production ( ca n't speak for episodical or feature workflows ) it seems that post starts when they call cut . Which in itself is becoming less and less often in favor of filling up the card instead of cutting . It 's only data anyways not like we are burning film stock . All the metadata , secondary sound and color profiles are added to the dailies right then and very often the drives are shipped off at what ever @ @ @ @ @ @ @ @ @ @ if all of todays media made it on the drive or not . 43971 @qwx453971 <p> I think it is true that post starts the moment the day wraps , but what 's sad is that I think there 's been a trend for the last 5-6 years , since film began to fade , for crews to have a kind of disrespect for the frame . I 've been on 35mm film sets many times , and there was a palpable change the moment the AD yelled " Roll camera ! " , because everybody instinctively knew , " oooooh , that 's money being spent ! Film is expensive ! Let 's get this right ! " Now , because data is cheap , there 's more of a lackadaisical attitude , which to me has resulted in much too much footage being shot . Even if the material technically has no physical value , it still wastes time , it crushes the assistant editors and others who have to go through all the dailies , and also makes the editor 's job that much harder . <p> @ @ @ @ @ @ @ @ @ @ these functions . I have n't been able to figure out if they just have the time or no longer have assistants to do the jobs or it 's a budget thing and production does n't want to pay for it . 43971 @qwx453971 <p> This is a D.I.T. / assistant editor function , not necessarily that of a post house . Traditional post houses are involved in VFX , sound mixing , conforming , color-correction , titling , duplication , and delivery . Dailies is sometimes part of it , but I think in general the D.I.T. crew is now the main source for dailies . There are post houses that still have full-staff dailies departments -- Light Iron Digital is one , Technicolor is another , and so does Deluxe , eFilm and Fotokem here in LA -- but I think that 's largely a freelance business now . <p> My point is that people tend to react negatively when presented with something as simple as sound syncing , when I know it 's just not that hard to do . I could take a person off the @ @ @ @ @ @ @ @ @ @ they 'd be reasonably fast at syncing dailies . Where they might have trouble would be all the other stuff that goes along with that job : organizing files , entering the metadata , backing up data , creating viewable dailies files , sending out reports to various departments , and so on . The other crucial job function is knowing what to do when the train falls off the tracks : bad timecode , garbled data , camera unintentionally off-speed , multiple camera formats , and so on . Knowing how to solve those problems so that the editor is completely ready to go at 8AM is critical . <p> This is a D.I.T. / assistant editor function , not necessarily that of a post house . Traditional post houses are involved in VFX , sound mixing , conforming , color-correction , titling , duplication , and delivery . Dailies is sometimes part of it , but I think in general the D.I.T. crew is now the main source for dailies . There are post houses that still have full-staff dailies departments -- Light Iron Digital is one , @ @ @ @ @ @ @ @ @ @ and Fotokem here in LA -- but I think that 's largely a freelance business now . 43971 @qwx453971 <p> Not in studio level production . There is very , very little studio level production in which creating dailies for editorial and viewing , and archiving original materials is done by set personnel . It is usually done by the very companies you just mentioned , with the addition of Bling and a few others , because on those projects it 's not just making files . It 's creating and maintaining pipelines for VFX and other post steps , and providing expertise in those areas . It 's also creating archival elements ( primarily LTO tape today ) that have been verified properly , and in many cases , providing a colorist , working in a sensible viewing environment , to either match what the DIT does n't have time to match or do the color work entirely . Sometimes it 's done in facilities , sometimes it 's done on remote systems deployed on location . It really depends on the production location and the past experience and @ @ @ @ @ @ @ @ @ @ people tend to react negatively when presented with something as simple as sound syncing , when I know it 's just not that hard to do . I could take a person off the street , and with maybe 2-3 days of training , they 'd be reasonably fast at syncing dailies . Where they might have trouble would be all the other stuff that goes along with that job : organizing files , entering the metadata , backing up data , creating viewable dailies files , sending out reports to various departments , and so on . The other crucial job function is knowing what to do when the train falls off the tracks : bad timecode , garbled data , camera unintentionally off-speed , multiple camera formats , and so on . Knowing how to solve those problems so that the editor is completely ready to go at 8AM is critical . <p> Not in studio level production . There is very , very little studio level production in which creating dailies for editorial and viewing , and archiving original materials is done by set personnel . It @ @ @ @ @ @ @ @ @ @ , with the addition of Bling and a few others , because on those projects it 's not just making files . It 's creating and maintaining pipelines for VFX and other post steps , and providing expertise in those areas . It 's also creating archival elements ( primarily LTO tape today ) that have been verified properly , and in many cases , providing a colorist , working in a sensible viewing environment , to either match what the DIT does n't have time to match or do the color work entirely . Sometimes it 's done in facilities , sometimes it 's done on remote systems deployed on location . It really depends on the production location and the past experience and desires of the producers . 43971 @qwx453971 <p> Which is why dailies could and should be done in the cutting room itself ( or on location ) by assistant editors . Who are compensated per the union contract rather than whatever Name of Post House feels like paying . <p> Which is why dailies could and should be done in the cutting room itself ( @ @ @ @ @ @ @ @ @ @ compensated per the union contract rather than whatever Name of Post House feels like paying . 43971 @qwx453971 <p> It 's not a question of cost , and it 's not a question of guild affiliation . Creating dailies is a lab function , not an editorial function . The skill set required for dailies processing and archiving on studio level shows is different than that of an assistant editor , as are the hours required to do it ( both the length and the time of day it 's done ) , the turnarounds expected , the level of interaction with production personnel , the communication with studio personnel , the levels of verification and the hours required to create LTO tape archives , the addressing of numerous security concerns , and a number of other issues . It 's not just loading , synching , and logging . I 'm not saying that trained editorial personnel could n't do it , but the fact is that dailies processing today is more complex than it 's ever been , and further from the cutting room mentality than ever before @ @ @ @ @ @ @ @ @ @ is you 've never been involved in such projects , but anyone who has would tell you exactly what I just said . <p> Creating dailies is a lab function , not an editorial function ... dailies processing today is more complex than it 's ever been , and further from the cutting room mentality than ever before . If you do n't believe that , my guess is you 've never been involved in such projects , but anyone who has would tell you exactly what I just said . <p> I disagree . Loading syncing and logging may be all that assistant editors are tasked with today , but in the past all those things you list were very much a part of the job . Dailies work is no more complex nowadays than in the past , it 's just not spread out across multiple different businesses and locations , each doing one small part of the process ( lab , cutting room , telecine , etc . ) . And for that very reason - the fact that the large physical infrastructure required by film and @ @ @ @ @ @ @ @ @ @ done by one or two people with the appropriate gear and know how , as you mention . I think those people should be covered by the editor 's guild contract ( and obviously if you looked at my imdb link it 'll be clear that I have a selfish agenda along with my pro-labor idealogical bent . ) <p> I disagree . Loading syncing and logging may be all that assistant editors are tasked with today , but in the past all those things you list were very much a part of the job . Dailies work is no more complex nowadays than in the past , it 's just not spread out across multiple different businesses and locations , each doing one small part of the process ( lab , cutting room , telecine , etc . ) . And for that very reason - the fact that the large physical infrastructure required by film and tape are no longer required - the work can be done by one or two people with the appropriate gear and know how , as you mention . I think those people @ @ @ @ @ @ @ @ @ @ and obviously if you looked at my imdb link it 'll be clear that I have a selfish agenda along with my pro-labor idealogical bent . ) 43971 @qwx453971 <p> I 'm not going to belabor this , but I am going to post this one final response . <p> I 've been in this business since the late 1970 's , and at no time have assistant editors been responsible for color timing of dailies , working during " graveyard shift " hours to prep dailies , archiving the original camera and sound elements , dealing with uploads to digital dailies services and other entities , maintaining camera metadata , creating multiple audio mixes for different purposes , making and distributing DVDs , or any number of other things I did n't even bring up . Not on film , not on video , and not on digital files . Nor are they tasked with loading and synching today because that 's all done for them by the dailies operator . And before you point out that the assistant editors COULD do this , I will also say that @ @ @ @ @ @ @ @ @ @ mid 1980 's , many , many assistant editors - especially the younger ones ( surprisingly ) seem to have very little knowledge of how to do any of that . I constantly have to field calls with questions like " I ca n't find the bin . " Followed by " how do I make a bin ? " And even when I tell them " you push the button that says New Bin " they still ca n't figure it out . That 's the sad reality , regardless of what the guild wants you to believe . And , by the way , I happen to still be a card carrying member of Local 700 ( still pay my dues just because I believe in the need for the union , although I get absolutely nothing out of it other than a magazine subscription and monthly screenings ) , and as much as I would like to support their stand , they have done little to nothing to show me that they are training Guild personnel to do what it is you claim they should be doing @ @ @ @ @ @ @ @ @ @ assistant editors that I know that tells me they 're interested , either . All they want to do is complain that companies are somehow stealing " covered work , " but if I ask them to send me a list of qualified personnel who are on the roster , they ca n't . Because basically there are n't any . I do n't care whose fault that is , but it is true . <p> For the most part , dailies work IS being done by one or two people on any given show , whether they happen to be physically located in a post company , on location , in a production office , next to the editorial department ( yes , that 's often the case ) , in a hotel room , or anywhere else that dailies work is done today . They just do n't happen to be assistant editors because neither the assistant editors nor the guild that represents them have ever made any serious effort to embrace that work . The company I work for would be happy to hire guild personnel , @ @ @ @ @ @ @ @ @ @ there were qualified personnel available and willing to work for what the studios are willing to pay for that job . There are n't , and they 're not . So please do n't try to make this a political struggle of some sort , because it is n't . It 's simply what the industry has evolved to , good or bad . <p> Well , got ta advocate for what you believe in I guess . I believe that no matter what you want to call them , the people making $25-$30 an hour doing dailies should be getting paid at least assistant scale and that their work hours should count towards MPIPHP benefits . <p> The company I work for would be happy to hire guild personnel , or better yet , have production hire them , if there were qualified personnel available and willing to work for what the studios are willing to pay for that job . There are n't , and they 're not . So please do n't try to make this a political struggle of some sort , because it is n't @ @ @ @ @ @ @ @ @ @ , good or bad . 43971 @qwx453971 <p> Last I checked , Technicolor has no union people on staff in Hollywood or Vancouver or London , certainly not doing dailies . And that would be for Local 600 camera or Local 700 editing . They could well have DITs , but they did n't through 2010 . The staff in Culver City was union for part of Technicolor 's operation there ( now owned by Deluxe ) <p> I have seen companies that have used assistant editors for dailies prep , but it 's not common for big studio productions . Bling Digital is one of them . <p> Last I checked , Technicolor has no union people on staff in Hollywood or Vancouver or London , certainly not doing dailies . And that would be for Local 600 camera or Local 700 editing . They could well have DITs , but they did n't through 2010 . The staff in Culver City was union for part of Technicolor 's operation there ( now owned by Deluxe ) <p> I have seen companies that have used assistant editors for dailies @ @ @ @ @ @ @ @ @ @ productions . Bling Digital is one of them . 43971 @qwx453971 <p> I was being hypothetical , Marc . I was n't putting out job offers , I was n't implying any union or non-union status for Tech , and I was n't advertising for such people . I was simply illustrating a point , the point being that Local 700 does n't really have trained dailies operators available regardless of the potential employers union status . As for DIT 's , Technicolor does not hire them because they do n't provide those services , at least not in Los Angeles and London . They do work with DIT 's ( yes , Union DIT 's ..... ) on many productions and occasionally supply some equipment to them . Beyond that , I am not going to reveal or discuss internal information about my employer on a public forum other than to say that your statements are not completely accurate . However , this is n't a contest , so please , just knock it off . <p> And BTW , I can almost guarantee that if Bling hires an @ @ @ @ @ @ @ @ @ @ union contract because as far as I know , Bling is n't a signatory ( at least not in California ) . If they 're simply using someone hired by production , well , I think I already addressed that . <p> Well , got ta advocate for what you believe in I guess . I believe that no matter what you want to call them , the people making $25-$30 an hour doing dailies should be getting paid at least assistant scale and that their work hours should count towards MPIPHP benefits . <p> Sorry for hijacking this thread , and I 'll get you my resume , Mike .... 43971 @qwx453971 <p> You 're making an assumption about pay levels that is not really accurate . And you 're making an assumption about benefits that is not necessarily accurate , either . The picture post industry outside of production employed editorial personnel has never been heavily unionized in the electronic/digital era . The vast majority of picture post facilities ( not necessarily sound post production , which still has a strong union presence ) in Los Angeles and @ @ @ @ @ @ @ @ @ @ and have provided full benefits to full time employees that are largely the equal of those provided by the union . The employees have never felt the need to join the union so they have n't . At this point in time there are only two major picture post facilities that are operating under a union contract ( Level 3 Post and Universal Digital Services ) . In the case of Level 3 , it is a legacy contract that dates back to the days of Compact Video . And in the case of Universal they are located on the studio lot and owned by the studio , which is a signatory company . There are some individual employees at other companies that are still being paid under a union contract based on their previous employer having being acquired , but in general the picture post industry is and has been mostly non-union going back to the late 1980s . <p> And BTW , I can almost guarantee that if Bling hires an assistant editor , they 're not being hired under a union contract because as far as I @ @ @ @ @ @ @ @ @ @ not in California ) . If they 're simply using someone hired by production , well , I think I already addressed that . 43971 @qwx453971 <p> Correct , Bling and byDeluxe ( with rare exceptions ) and most other post houses are not union at all . <p> I would say there is one more big union facility in town , and that would be the Deluxe facility on the Sony lot , formerly owned by Sony . I believe they 're still all Local 700 over there , because that was part of the deal when Sony sold them the facility . <p> The picture post industry has generally been non-union , though I have worked at TransAmerican Video and also Compact Video , both of which were 695 back in the day . Compact famously went bankrupt on a Friday and reopened under a new name and tried to shut out the union , but the courts eventually forced the new owners to go back to the union contract . <p> CBS TV City is also all union , and they do have quite a bit of @ @ @ @ @ @ @ @ @ @ do a degree of editing , color correction , and VFX . And virtually all major feature film editors are in Local 700 as are their assistants , plus the post sups are in the PGA . But not at 90% of the post-production facilities . <p> What 's interesting to me is the number of TV shows that use a lot of non-union employees , but they still have the IATSE " bug " in the end credits . I 'm amazed how IA still allows this . Even worse when the colorist gets no credit , but I digress . <p> What 's interesting to me is the number of TV shows that use a lot of non-union employees , but they still have the IATSE " bug " in the end credits . I 'm amazed how IA still allows this . Even worse when the colorist gets no credit , but I digress. 43971 @qwx453971 <p> I think you already know the answer to that . Personnel who work for post companies and facilities are not " employees " of production . They are employees of @ @ @ @ @ @ @ @ @ @ . It is the production company 's choice as to who does the post work , and as long as that 's the case , and they 're not employing those people directly , the union basically has nothing to say about it . Personally , I see the union as vital and necessary in the freelance world of production , in part because for many years ( prior to the Affordable Care Act ) , it was almost impossible to get health insurance as an individual in the United States if you were ever sick at any point in your life . The only way for many , if not most people , was to be a full time employee of a company that saw fit to offer health insurance as an employment benefit . So in a freelance industry , the way around that , and the only way to have benefits that would follow you from job to job was to offer it through an " umbrella " organization that served all studios and productions - namely , Motion Picture Industry Health and Pension Plans . Even @ @ @ @ @ @ @ @ @ @ maintain your hours in order to maintain your benefits . But for many years it 's worked as a way of providing benefits to a freelance industry . Post facilities did n't fit that description as most of their employees are full time . In recent years that has changed somewhat , but employees at post companies have long been reluctant to commit to union representation , with its more specific job descriptions , initiation fees ( often waived for new signatories , but I digress ..... ) , and continuing dues . Particularly when they 're already being paid at higher than union scale , and already have benefits that are at least the equal of the motion picture plans . <p> I would say there is one more big union facility in town , and that would be the Deluxe facility on the Sony lot , formerly owned by Sony . I believe they 're still all Local 700 over there , because that was part of the deal when Sony sold them the facility . 43971 @qwx453971 <p> It was not " part of the deal " @ @ @ @ @ @ @ @ @ @ tried with Level 3 a number of years ago , which was to declare the contract null in void when the facility was sold . The employees had a new election and voted to remain in the union , so Deluxe had to accept that . This is all detailed in this article , among other places : Deluxe Culver City Post-Production Staff Vote For IATSE Contract Deadline <p> You 're making an assumption about pay levels that is not really accurate . And you 're making an assumption about benefits that is not necessarily accurate , either . 43971 @qwx453971 <p> I 'm not making assumptions - that 's what Deluxe pays , that 's what Bling pays , that 's what smaller operations like EPS Cineworks pay .... that 's the going rate . If Tech pays more , that 's awesome and I 'm glad because as you point it the overall skill set required is evidently greater nowadays than that required of assistant editors , who start at $36-and-change per hour on a 45 hour guarantee , or DITs who make even more . <p> Full @ @ @ @ @ @ @ @ @ @ . When you send a cart out here to Austin , and hire a local freelance schmuck like me to do the work , it 's another story though , correct ? Even though I 'm a member of an IA local , my work hours are n't recognized by that local because I 'm your casual or contract employee and not employed by the show itself . An analogy *in my opinion* would be if Panavision were providing shows with camera assistants along with their equipment packages and paying those assistants whatever Panavision pays people , and not what local 600 says ACs are supposed to get . <p> I thnk eventually the people who sign the cheques are going to notice that the dailies work is being done by one or two people with a computer and begin to wonder why they 're paying a post house to provide that , but only time will tell . <p> A post supervisor friend of mine advised me not to start making noise about this union business because it would piss a lot of people off . Evidently he was right . : - ( 
@@44332856 @4332856/ <h> Faster 4k ProRes Proxy rendering out of Resolve <p> I 'm trying to figure out how much improvement to expect by adding a new GPU and SSD our current workstation and was wondering if the LGG folks could help me out . <p> At the moment , we are using Mac Pro 5,1 ( latest OSX ) as a Resolve 12 workstation . It 's relevant specs are : <p> 12-Core 2.93 GHz Intel Xeon CPU ( L2 256KB , L3 12MB ) <p> 64 GB 1333 MHz DDR3 RAM <p> 1 GB ATI Radeon HD 5870 GPU <p> Western Digital 1TB 7200 RPM 32MB Cache SATA 3.0Gb/s HDD <p> 64TB 10GBe server ( which gets read/write speeds of 700+ MB/s ) <p> For HD ProRes 422 ( source ) rendering to ProRes Proxy , we 're currently getting between 60 - 80 FPS renders out of Resolve . For 4k/UHD ProRes 422 ( source ) to ProRes Proxy we 're currently getting between 15 - 22 FPS . All renders have a 1 LUT correction , specifically Sony 's LC709TypeA . I 'm trying to improve our @ @ @ @ @ @ @ @ @ @ GPU to an SSD and 12GB Titan X would give us a much better render performance.Will we actually see a performance boost with a 12GB Titan X and SSD upgrade , or are there inherent limitations in the Mac Pro 5,1 that make such an upgrade not worth while ? Are there any CPU or Bus/Memory bandwidth bottlenecks on this machine that I should be aware of ? Would a 6GB GTX 980 TI perform comparably to a 12GB Titan X on this system ? <p> You would definitely notice a switch from the single 7200rpm drive to a 3-SSD RAID0 setup . I use the 4th bay as a fail safe bootable backup . No discernible performance jump from 3 to 4 anyway . <p> The Titan X is great . If you understand and are willing to go through the process of doing a non-EFI card , then do it right . The extra memory is worth the price . I have not once had a GPU out of memory error since upgrading from the Titan Black . <p> As long as you are n't trying to monitor @ @ @ @ @ @ @ @ @ @ worth the upgrade . <p> ProRes encode is CPU bound . Before you buy anything , do a 4K test with ProRes4444 and ProResLT . When you render , open Utilities &gt; Activity Monitor .... CPU Tab . If your CPUs are n't pegged then the upgrade should help . If they are near 100% , then a CPU upgrade to Xeon x5690s might also be a good option . <p> I do n't think an ssd is going to make a huge difference if your media lives and writes to the nas . The GPU seems weak , but you can see other threads where we 've gotten into this , I do n't have a clean recommendation . Anything faster , like Titan-x , means no boot screen and funky driver/os support . Safest route is a supported GPU in the box , and an expansion chassis with a beefier GPU . But that costs real $ . Sticking a unsupported GPU as your boot screen display , means your kind of on your own for figuring that out when it goes wrong . Not where I want @ @ @ @ @ @ @ @ @ @ probably be an improvement , but all the same caveats apply . Of course this has been argued before in other threads . <p> If you 're just making dailies you might actually be one of the very few good candidates for the new macpro . <p> One other add , is that best case on an OG macpro is maybe 30fps renders . That 's an improvement on render times , but not like like a crazy one . On the nMP you might be able to get into the 40s , assuming your NAS does n't choke , and that your nmp does n't render with glitches . <p> One other add , is that best case on an OG macpro is maybe 30fps renders . That 's an improvement on render times , but not like like a crazy one . On the nMP you might be able to get into the 40s , assuming your NAS does n't choke , and that your nmp does n't render with glitches. 43971 @qwx453971 <p> The Resolve 12 builds after about 35 " seem " to have gotten rid @ @ @ @ @ @ @ @ @ @ I 'm cheating and doing 2 passes on 4K material , only the second with TNR . <p> ProRes encode is CPU bound . Before you buy anything , do a 4K test with ProRes4444 and ProResLT . When you render , open Utilities &gt; Activity Monitor .... CPU Tab . If your CPUs are n't pegged then the upgrade should help . If they are near 100% , then a CPU upgrade to Xeon x5690s might also be a good option . 43971 @qwx453971 <p> Just started a 4k ProRes 422 -&gt; 4k ProRes Proxy export and my CPU us being used at about 40% . So the CPU is definitely not being used to the fullest extent . The GPU is maxed . <p> On a related note : in Resolve 12 in the media page , by right clicking there 's the ' generate optimized media ' ( or something similar ) . I believe there is an option in the settings to specify what format this should be , e.g ProRes 422 HQ . But does this option actually create stand alone media ( . mov @ @ @ @ @ @ @ @ @ @ I 'm asking since I would like to replicate FCPX 's background ProRes creation without going through an export in Resolve . Not sure if it 's possible ... 
@@44332859 @4332859/ <h> 3x3 Matrix Transforms instead of 3D LUTs <p> Just curious ... I 've heard that more and more people are using 3x3 Matrix Transforms to color grade RED footage , and that 3D LUTs ( especially film emulation LUTs ) are used less and less by high-end facilities ... <p> Can someone explain what exactly are those 3x3 Matrix Transforms ? From what I hear , 3D LUTs are destructive , and 3x3 matrix transforms are non-destructive . <p> Matrix transforms are n't destructive , in a floating point environment . We do use them . They do n't in and of themselves transform you out of redlogfilm , they transform your colorimetry . You still need a gamma transform . In fact you need a linear transform leading into your matrix transform and then a delinear transform out . <p> The advantage vs 3D LUTs is that they wo n't result in things like quantization errors . <p> I 'd start reading up on what matrix transforms are and how they work , as it 's a deep subject , that is if you want to understand @ @ @ @ @ @ @ @ @ @ are mathematical operations , LUTs are pre processed tables with a certain precision , think of it as vector vs bitmap . Matrix processing can be used to remap primaries and white point . <p> I am currently using film emulation 3D LUTs in DaVinci Resolve ( starting from RedLogFilm , I use 3D Output LUTs so I keep all corrections in log mode , as recommended ) . If 3x3 matrix transforms are the better way to deal with Red footage , I 'd like to learn more about this route ... especially since I hear people are slowly abandoning 3D LUTs in favor of these matrix transforms . <p> Just so I understand the order of operations ( as I interpreted Juan 's post ) - from RedLogFilm ( log ) you go into a linear 3x3 matrix just to re-interpret the colorimetry , then go back into log mode , then again a gamma log-to-linear transform ? <p> Forgive my ignorance as I 'm not at all familiar with this new option - again , any leads would be greatly appreciated ! <p> Personal opinion disclaimer : @ @ @ @ @ @ @ @ @ @ I am always open for debate . I have been staying away from LUT 's for years now . When I am forced to use a LUT ( for whatever reason ) I always make sure it is at least 643 . I like the comment above " vector vs bitmap " as this is a pretty good way to describe the problem with LUTs . In recent years I have been performing many color transforms using GLSL functional math . IMO this yields the highest quality since you are using true curves ( or at least as true as float can achieve ) to map input and output values . The great thing about matrices and functions is that they are reversible/invertible . <p> There is no reason to say 3x3 matrix is a better way to deal with Red footage . If you need your primaries and your white point to be in a certain place , just use the RED SDK parameters to have them where you want , it 's pointless to redo it downstream . There 's no point either in opposing 3x3 matrices and @ @ @ @ @ @ @ @ @ @ implementation that counts . As pointed by John , when you use LUTs you should make sure they are precise enough . If you want a film look or certain other complex processings , the only way to have it since we do n't know how to represent what happens in a lab with an equation so we have to use LUTs . Since film out is less and less important , lab emulation is too . Ultimately when ACES works fully it should take care of that for you <p> 1 ) Cedric 's comparison of 3DLUTs to a bitmap ( vs vector ) is right on , but just to add a bit of detail to that , its also important to consider that the software which is reading the 3DLUT has many options in terms of interpreting a 3DLUT . Good software will interpolate between values in the 3D LUT . Take the simplest example ... Consider a unity LUT which does nothing , perhaps a 16x16x16 LUT . If we were not to use any interpolation , and fed a 10-bit image ( 1023x1023x1023 possible input @ @ @ @ @ @ @ @ @ @ data points in the 3DLUT we simple rounded to the nearest value ... If we did that with a 16x16x16 LUT we would have reduced the bit depth of out 10-bit image to 4-bits . <p> This is obviously totally unworkable . So what we do is to interpolate between the known values of the cube to create a transform of much higher precision . Obviously this will loose any subtle high frequency bumps in the shape of the cube , but does n't lead to quantization errors per-se . <p> One of the problems becomes that just like interpolating between pixels when you scale an image , there are lots of methods one can use to interpolate between values in a cube . Different software uses different methods , some leads to more quantization errors than others . If done well , there is no reason why using a 3DLUT should lead to quantization errors and more than a 3x3 matrix transform . <p> 2 ) In terms of understanding what a 3x3 matix transform is , think of it as a geometric transform that you apply to a @ @ @ @ @ @ @ @ @ @ you have seen gamut maps for a specific color space ( such as rec709 ) on a CIE chart which show the location of the primaries as a triangle , with the white point somewhere in the middle . A 3x3 tranform allows you to move those primaries and white point around . <p> Just to clarilfy , essentially all DI or color software does some form of interpolation when using 3DLUTs since if they did not the LUT would need to be 10243 in size to be useful . That said , I do not know what algorithm Resolve or other software specifically use . wikipedia suggests that most software uses trilinear , but I 'm not sure that 's correct . My experience is that many pieces of software will create slightly different results even given the same input and same LUT , suggesting that each software vendor has slightly different secret sauce . It would be really nice if this was standardized so that we could no that the same LUT would produce the same result in different packages . I have suggeted to many developers that they @ @ @ @ @ @ @ @ @ @ which are open and which are used by the foundry products , but no one seems very interested <p> In any case , my practical experience is that the quantization errorfrom 3DLUTs is mostly a problem when you are converting from color spaces which are radically different along the gamma curve . For example if converting from Linear light into 709 or CineonLog , most of the visable data is packed into the very bottom of the data range in the linear data , so the precision of the LUT in that toe of the curve becomes quite important . <p> Of course a 3x3 matrix wont help you there either , since you cant do a gamma curve conversion with a 3x3 matrix . <p> If you really want to get fussy with precision and smoothness of gamma curves you want to use a high precision 1D LUT or a curve . <p> The problem with using a 3DLUT for conversion which include a gamma adjustment is that the precision of the gamma adjustment is very low even for fairly high precision 3DLUTs . <p> For example , a @ @ @ @ @ @ @ @ @ @ gamma curve of that LUT is simply the line between black and white , and will have only 17 values in it . That 's not much precision , especialyl if the interpolation between those 17 data points is simple a straight line ( linear ) . Obviously a curve with 1024 data points such as a 10bit 1D LUT is way way better , as is a curve defined by a mathematical function . <p> Some software such as scratch gives you the ability to import a 1D LUT and convert to a curve which could further help eliminate quantization errors . <p> In our view , when converting between widely different gammas - such as Log to Linear Light - the best approach is a mathematical formula rather than a LUT - 1D or 3D . <p> When doing such conversion it is very unusual to be changing gamut as well , so such formulas works perfectly , and require no matrix either . <p> There can be a problem with high-bit depth 1D LUTs , as there is a danger with small error causing ' roll-back ' , @ @ @ @ @ @ @ @ @ @ due to rounding errors , or especially when using collated data , as from a probe based measurement . <p> Using good interpolation with LUTs , as most of today 's ' colour ' systems do , 1D or 3D LUTs of 333 points are usually perfect for all colour based conversions , with little variation when compared to 643 based LUTs . <p> But , as stated above , for simple gamma only conversions , a mathematical formula is the best approach , if possible . <p> When doing calibration you do need 3D LUTs as you must be able to correct non-linear distortions within the display colour ' volume ' , and no matrix or formula based correction can do this . These errors are often described as cross-colour , or RGB Separation errors . <p> Out of interest some displays with in-built correction are 3x3 matrix based , combined with a 1D LUT for gamma control . Such displays are never as accurate as they should be , and there is one PC manufacturer display in particular that promotes its calibration capability , but use this approach @ @ @ @ @ @ @ @ @ @ in DaVinci Resolve ( starting from RedLogFilm , I use 3D Output LUTs so I keep all corrections in log mode , as recommended ) . If 3x3 matrix transforms are the better way to deal with Red footage , I 'd like to learn more about this route ... especially since I hear people are slowly abandoning 3D LUTs in favor of these matrix transforms . <p> Just so I understand the order of operations ( as I interpreted Juan 's post ) - from RedLogFilm ( log ) you go into a linear 3x3 matrix just to re-interpret the colorimetry , then go back into log mode , then again a gamma log-to-linear transform ? <p> Forgive my ignorance as I 'm not at all familiar with this new option - again , any leads would be greatly appreciated ! 43971 @qwx453971 <p> buy the RP177 ( SMPTE177 linear color space transformation ) I think is 20$ , that is one of the official document on gamma+matrices+gamma and explain in good details why and how . <p> then there are dozen of references in books and publications ... @ @ @ @ @ @ @ @ @ @ is that the quantization errorfrom 3DLUTs is mostly a problem when you are converting from color spaces which are radically different along the gamma curve . For example if converting from Linear light into 709 or CineonLog , most of the visable data is packed into the very bottom of the data range in the linear data , so the precision of the LUT in that toe of the curve becomes quite important . 43971 @qwx453971 <p> that is beacuse the samples in the LUT that effect the images are restricted : for example in ACES you might have a 33x lut but only a third or a forth of the color saturation LUT samples ( the one diverging from the gray diagonal but near it ) are actually used efficiently to represent a RRT/ODT final transformation . so , in some case you might want to : <p> 1 ) split in 1D + 3Dlut ( like truelight ) to reduce the gamma approximation ) 2 ) increase the density 65x , 129x 3 ) use 1D/gamma math plus matrice 3x3 when applicable 4 ) use a non linear @ @ @ @ @ @ @ @ @ @ you scale logarithmically the samples along the greayscale diagonal AND logarithmically form the greyscale versus the RGB primaries , you will densuify the LUT where it matters the most . at that point a 16x16x16 " can " behave as a 33x or more where it matters . As far as I know only Truelight support a non linear distribution of points . 5 ) use a tetrahedral interpolation instead of a trilinear whenever applicable . the error in the inbetween sample is reduced by a fair amount . <p> Just to clarilfy , essentially all DI or color software does some form of interpolation when using 3DLUTs since if they did not the LUT would need to be 10243 in size to be useful . That said , I do not know what algorithm Resolve or other software specifically use . wikipedia suggests that most software uses trilinear , but I 'm not sure that 's correct . My experience is that many pieces of software will create slightly different results even given the same input and same LUT , suggesting that each software vendor has slightly different secret sauce @ @ @ @ @ @ @ @ @ @ so that we could no that the same LUT would produce the same result in different packages . I have suggeted to many developers that they standardize using the inperpolation methods provided by the OCIO libraries which are open and which are used by the foundry products , but no one seems very interested . 43971 @qwx453971 <p> LUTs are not necessarily as complicated as you think they are . Jeremy Selan of Sony Pictures Imageworks ( who just won an Oscar for color management-related work ) has an excellent white paper on LUTs , color science , and color management at this link : <p> I again say that colorists had to work with log film scans for 30 years without having any LUTs , and we did fine in terms of bringing them into linear Rec601 and Rec709 color space . It 's not rocket science . What is more difficult are camera manufacturers that push the envelope in expanding the available color spectrum and bend the rules in terms of levels and gamma curves . That gets more involved , and it gets worse when you have @ @ @ @ @ @ @ @ @ @ XYZ . That I would n't trust without actual LUTs . <p> That said , I do not know what algorithm Resolve or other software specifically use . wikipedia suggests that most software uses trilinear , but I 'm not sure that 's correct . 43971 @qwx453971 <p> Tetrhedral ( == simplex in the general N dimentional case ) is typically faster , and has better accuracy when the input space represents luminance on the diagonal . Code using a GPU is probably using a texture lookup , so it may be more likely to be trilinear , but there are some other algorithms used in texture lookup for the sake of speed . <p> My experience is that many pieces of software will create slightly different results even given the same input and same LUT , suggesting that each software vendor has slightly different secret sauce . 43971 @qwx453971 <p> Quite often there is a difference between doing an accurate color LUT interpolation , and doing it fast on a bunch of pixels , the latter sometime sacrificing a little accuracy for speed . <p> In any case , my @ @ @ @ @ @ @ @ @ @ a problem when you are converting from color spaces which are radically different along the gamma curve . For example if converting from Linear light into 709 or CineonLog , most of the visable data is packed into the very bottom of the data range in the linear data , so the precision of the LUT in that toe of the curve becomes quite important . 43971 @qwx453971 <p> If you are using a pure cLUT/3DLut that may well be the case . cLUT conversion can be much more accurate if combined with per channel 1D input and output curves or LUTs , which ( for instance ) is something ICC profiles provide , since the colorspace can be converted to be more perceptually or additively linear before the cLUT/3DLut . A judicious choice of 1D in/out curves goes some way to minimizing any piecewise linear errors introduced in the cLUT . <p> Of course a 3x3 matrix wont help you there either , since you cant do a gamma curve conversion with a 3x3 matrix . 43971 @qwx453971 <p> But an ICC profile can combine 1D curves/LUTs with a @ @ @ @ @ @ @ @ @ @ fussy with precision and smoothness of gamma curves you want to use a high precision 1D LUT or a curve . 43971 @qwx453971 <p> Or use a format that can represent the curve with an appropriate formula or piecewise tangential curves . <p> We prefer the 1D/3D LUT combination , and our standard preference is a 101 1D LUT , and 203 3D LUT - sometimes matched with a 3x3 matrix , but less often . <p> Converting from different colour spaces is really only a problem when the two spaces have widely different Dynamic Ranges - so you end up compromising the granularity of the result . <p> In those cases a two step approach can be of major benefit , with a pre-shaper input LUT used before the actual conversion - which is basically a 1D followed by 3D LUT combination . <p> In reality , if you see issues in the conversion the LUT path ( workflow ) is as like as not incorrectly configured . Very few of the professional system in use today have poor LUT implementation , but if they are fed incorrect LUTs @ @ @ @ @ @ @ @ @ @ ... <p> LUTs are not necessarily as complicated as you think they are . Jeremy Selan of Sony Pictures Imageworks ( who just won an Oscar for color management-related work ) has an excellent white paper on LUTs , color science , and color management at this link : <p> Out of interest OpenColorIO luts and some OCIO functions are available in Resolve 10 via ofx. http : //www.tuttleofx.org/ . They 're not GPU assisted . Speed , accuracy and stability in Resolve somewhat untested . <p> The TuttleOFX plugins cover image format I/O via what i think is OpenImageIO , as well , these crash Resolve of coarse being only setup to use ofx for effects , so tread carefully anyone who is interested in testing them out . <p> OpenColorIO luts will need downloading as well i believe and the ocio config file tweeked. 
@@44332860 @4332860/ <h> 1080 or 4K Monitor Workflow in Resolve for Least Amount of Scaling Artifacts <p> I want to buy an Oled monitor that of course has accurate colors . I have been burned too many times misjudging the blacks levels some projects . I have an idea that Oled will help a non-professional colorist like myself to confidently get the black levels right . <p> Which workflow with Resolve has the less amount of scaling artifacts while looking at grading monitor : <p> Content recorded in 4K -&gt; edit in 1080 timeline with 1080 monitor -&gt; publish to 1080 <p> Content recorded in 4K -&gt; edit in 1080 timeline with 4k monitor -&gt; publish to 1080 <p> Content recorded in 4K -&gt; edit in 4k timeline with 1080 monitor -&gt; publish to 1080 <p> Content recorded in 4K -&gt; edit in 4k timeline with 4K monitor -&gt; publish to 1080 <p> I raise this question because it seems that Premiere Pro does a great good at scaling while Resolve leaves a lot of artifacts in my current workflow . <p> This way you can still use the 4k to @ @ @ @ @ @ @ @ @ @ from you Computer/display . <p> In Resolve you can choose how your footage will be scaled . In the Project Settings you have several options . More quality needs more resources . <p> I chose a 1080 monitor ( non-OLED because wanted to stay sub 3000 bucks ) and always edit on 1080 timelines . <p> If I have 1080 footage= no scaling 4k footage= downscaling , which is never a Problem <p> If you often misjudge your levels you need a clean , calibrated signal path , not a monitor with better black levels . If you can spent the money , a calibrated HD OLED is the best you can get . FSI will make you happy instantly but set you back 5000 or more . More important is your signal pipeline . What are you using for monitoring right now ? And do n't forget that the ambient light levels have a great impact in how you judge contrast/black levels . You 'll want at least a dark room with some 6500K ambient light ! <p> I have an idea that Oled will help a non-professional colorist @ @ @ @ @ @ @ @ @ @ 43971 @qwx453971 <p> I 'd be very careful about using an OLED directly out of the box for grading , especially as you mention black being an issue . OLEDS have exceptionally low black levels , which can cause you - the colourist - to grade the shadows higher to overcome the deep blacks . When the graded material is then viewed on a ' normal ' display ( hone TV ) the shadows can look washed-out . We always recommend to lift the OLED black slightly - to around 0.03 Nits . <p> Looks like option " 1 . Content recorded in 4K -&gt; edit in 1080 timeline with 1080 monitor -&gt; publish to 1080 " is the way to go . <p> @Erik I am using a curved LG 34UC97-S 34 UltraWide monitor right now for everything , I have a 2nd monitor but it is really old and not accurate so I do n't use it anymore . I am looking for a new monitor . <p> window in hardware , ergo the screen . the sw ( e.g. DVR ) will push out the monitoring format @ @ @ @ @ @ @ @ @ @ have higher than 1080p resolution have a setting where you can set to display the native input format , ergo the screen will show 1080p windowed , without upscaling it to the screen 's max resolution . You do not want to grade on upscaled pixels . <p> Some screens do not have that setting , so be aware . Eizos usually have that setting . <p> Of course , doing that for a long time will affect uniformity of these screens as your only using ( wearing down ) the center 1920x1080 pixels ... that is why peeps usually buy a native 1920x1080 screen for 1080p monitoring ( or 2K screen ) ... 
@@44332861 @4332861/ <h> Future of self-employed colorists <p> The quick personal story : I 've been an editor for about 8 years , started getting into colour grading for my own projects about 3 years ago , and started to work as a freelance colourist about a year ago . Kind of by accident ( I approached a small post-house and offer to intern for them a few days a week in exchange for access to their grading suite outside office hours , so I could grade my own project on professional gear , which turned into occasional freelance work for that facility , who is now my main client ) . <p> I know that a good reason why I get work from there is because I am cheaper than other more senior colourists . But I think that the post-house is being completely honest with clients , offering different quotes and letting the client know what they are getting for each price . The low budget projects that would have been turned down in the past now fall to me , while the bigger projects still go to the @ @ @ @ @ @ @ @ @ @ like there is a " race to the bottom " . Or not as much as some people paint it out to be . I think it 's more than projects than would n't have gotten a professional grade ( or any kind of grade ) in the past now have access to it and use it to the limit of their budgets . We do have to admit that a lot of the gear has recently gotten a lot cheaper , and that as a result , it 's fairly normal for some clients to expect a drop in the price of the grade . The talent and experience of the colourist is part of the price , and that should n't have to be compromised , but part of what the client is paying is the investment that the colourist/company made in their gear . Some post houses are failing right now because their dry hire rates are n't adapting to that . <p> I do agree that a lot of self-proclaimed colourists ( which I 'm part of , I guess ) are hurting the professional ones @ @ @ @ @ @ @ @ @ @ it myself several times but I turn down projects that I feel I am not suited to take on . I 've even talked down rates that were offered to me because I felt they were too high for my experience ( I 'll admit it did take me one or two jobs to realise it though ) . <p> Truth is that it 's a skill that 's pretty easy to fake on a still frame or a showreel , but after getting a bad grade for a low price , I do think the clients will turn back and spend money on a professional in a professional environment . <p> I think in the end it 's all about " you get what you pay for " and that 'll always drive everything . Better colourists will have higher rates , and so will better facilities . Clients can decide how much they want to spend , as much as they can decide how much to spend on a camera rental , or production design . <p> Going back to the original topic of having your own equipment @ @ @ @ @ @ @ @ @ @ I 'm actually thinking of setting up my own " low-cost " grading suite to take on the projects that are too low in budget to get into a post-house . While I 'll always bring clients to a professional grading facility when I can , I do think we can offer lower-cost solutions to some clients . For example , if a client contacts me , with a budget of , say , 150% of my freelance rate I have no way of bringing them to a facility that would charge me 200% of my rate to get in their suite . For the client , that would double their budget if I charge my normal rate on top of it , which would mean I 'd have to turn them away . <p> I do n't feel like it 's outrageous of the client to ask for that kind of rate . Quite often , I have clients coming in with projects where everyone else has worked for free or cheap , and I 'm the only one getting paid correctly for the grade . <p> Anyway , @ @ @ @ @ @ @ @ @ @ this period as pretty exciting for colourists , because a lot more people are getting interested in colour grading . Not just wannabe colourists , but also clients , DPs , Directors ... and that they integrate it more into their creative process than before . Maybe I 'm talking more about student films and corporate videos than feature films and commercials , but still . The technology is exciting and people realise more and more how important a colourist is , even though they still need to learn how to recognise a good one . <p> Julien , I do n't think there 's anything wrong with you setting up your own suite . On the contrary . If you 've discovered that you love grading , having your own gear will give you the opportunity to practice more often and get better at it faster . We 've all started at some point and there 's no limit on the number of colorists in the market . <p> I agree with you that there 's more work these days , especially due to the general increase in production @ @ @ @ @ @ @ @ @ @ prefer to save a few pennies , even when they can afford to hire the best , others are on a very limited budget and really ca n't afford an experience colorist . The latter would n't hire a larger facility , anyway , so no one is losing . On the other hand , I find that bigger clients know how important a proper grade is and they wo n't put their expensive productions in the hands of an amateur . So more experienced colorists will continue to have a market . <p> But medium facilities with large overheads are indeed facing some though times . Budgets are not what they used to be - and this applies to all types of production work ( see FX facilities , for example ) . So keeping an expensive machine running while facing competition from experienced independent colorists with their own gear is definitely hard these days . But it 's the same with CGI , audio post , etc . <p> A business is a complex thing . We must invest carefully and in accordance to the market . It @ @ @ @ @ @ @ @ @ @ on low budget jobs to invest in a Dolby monitor , for example . They would be working a good part of the month just to pay for it . Add the other expenses and whatever is left at the end of the month might not be worth the hassle . The options are usually to slim down or to offer services that others do n't . But bigger facilities in larger markets can pay for expensive gear in just a few jobs . So one has to be pretty realistic about the market and type of clients served in order to be able to adapt and survive . <p> When Resolve dramatically dropped in price I was bummed . All of a sudden everyone acquired the software , that was good enough for most of the work . But my funk did n't last long . Almost immediately after it 's release , I set up my first Remote Grading partner in Moscow , Russia and i was off and running . It was over two years now and I 'm getting many requests for my services from all @ @ @ @ @ @ @ @ @ @ admit , that most of my remote clients are still mostly located in Eastern Europe . It could be , that in Eastern Europe people are traditionally had been more hungry and opportunistic . Or it also could be , that their Agency-Production-Post Production hierarchy is a bit more liberal , than in established countries of Western Europe and US , allowing for more experimental ways of working . Speaking the languages , that are mostly spoken in Eastern Europe or , for that matter , being based in Hollywood , I 'm sure , does n't hurt either In US , so far , I have only a few steady remote clients , with offices in LA and NY , but I think it 's only a matter of time , before many more clients realize , that they can have the best of both worlds- they can edit , grade it and finish in-house and they can hire the top talent for just a few hours . So , yes , the competition in color grading is growing , but the trick , at least for me , @ @ @ @ @ @ @ @ @ @ the lookout for the new trends , before everyone has ... <p> Jake , that 's definitely a very good way to expand one 's clientele . Especially if one lives in a smaller market and there are few opportunities . I have n't done any remote grading myself , but I 've graded projects from different locations and even different countries . The material gets sent to me , I do the grade , send proofs and , when the job is finally approved , I send the clients a finished master . <p> As I understand , remote grading requires that you have in your station the same material that is in the remote station . For TV commercials this is easy and quick as the material can be transferred via the internet . But how do you handle longer formats such as documentaries , TV shows and feature films ? Do you have the material sent to you by FedEx ? Also , how fast does your internet connection need to be in terms of upload/download speeds ? <p> Yes , it is a common misunderstanding , @ @ @ @ @ @ @ @ @ @ It 's incorrect . The material must match the resolution and the frame rate , otherwise it is perfectly acceptable to use differently encoded or compressed material for your master station . For example , one of my clients in Serbia uses Linux Resolve and he has to use DPX transcodes from Alexa Prores . I can use the original Prores on my Mac at the same time as he uses DPX . Or , in order to reduce the amount of data sent to me , I could use something like H264 or JPEG sequence . The result is exactly the same , because the final render always performed by the client from the original material on the client 's Resolve . R3D can be conformed and cut down prior to sending it to me . Yes , sometimes FedEx is your best friend , but if you 're not in a hurry , sending files over the net is getting easier with each passing day . If your client has decent FTP , even better . Internet connection can be even a dial up , if this is @ @ @ @ @ @ @ @ @ @ concern , than Resolve . Resolve just sends out a few bytes of telemetry at a time . Often , when I can lose Skype connection , while Resolve keeps on working <p> The rate for that feature has recently gone up if anyone who like to jump in at $400 ... <p> Colorist Needed ( West Hollywood ) <p> We need a colorist for a Feature Film . Its a great film , name actors attached and expected to do very well . We are really over budget and looking for someone who needs a great credit , and can work with our budget . <p> Location : West Hollywood <p> it 's NOT ok to contact this poster with services or other commercial interests <p> That 's what I do , along with a number of other people here . Two of the key benefits of a small shop are flexibility and low overhead . Those are two very useful traits to have right now . If you 're able to partner up with other shops ( VFX , Audio , 3D , etc ) on a project-to-project @ @ @ @ @ @ @ @ @ @ to temporarily scale for projects larger than you can handle or participate in ones that may have been outside your scope of work . <p> But , what binds it all together is good working relationships with the other artists , and you have to be building those long before the jobs ever come . If you can pull if off though , you not only get to have a lot of control over your schedule , but also what you work on , and how you go about it . Those are very appealing ideas to a lot of people , and probably a good reason you see many colorists and other artists , leaving full-time jobs at larger post houses to go it on their own . 43971 @qwx453971 <p> I am a commercial photographer/DP recently turned colorist ( still learning and a long way from being a pro ) and have been offering workflow and technically advice to a local post house here in Zambia . As a result of our long term collaborations and good working relationship we have recently come to an agreement with the @ @ @ @ @ @ @ @ @ @ them for their future projects as an added service to their clients after a I showed them what I could do for them with some sample footage from their works and some of the timelapses I did for them for their recent commercial . Bottom line is creating good networks with other artists in vfx , editors and audio post houses will most of the times result in jobs coming your way when you show your work . <p> I thought a lot about the future of colorgrading and colorists . I guess in the future there will be black and white with some little , little greytones between . <p> Means : Resolve , Speedgrade and Co. make it easy for clients without money to grade their stuff . Most of them would n't be able to afford a grading anyway as the grading part costs as much as their entire project . Now they shoot on a 5D , edit on Premiere or FCP X and put some color in a free software . Jobs : little documentaries , Low-budget internet-commercials , imagevideos , maybe even some shortfilms @ @ @ @ @ @ @ @ @ @ will not hire a colorist anyway . Maybe some editors do coloring now . Just one problem : Is the guy , doing the grade ( e.g. editor ) really a colorist ? I do n't think so . Then we have the other end : Highend-commercials and feature films with a descent budget . This bussiness belongs to the big post houses or at least to well-known artists and technical experts ( colorspaces , ACES , Workflows , .... ) Look at it from the customer side : The customer would never give someone a CocaCola , Toyota , Nivea , whatever commercial who has n't the right reputation for that . Simple : If you do n't have highendwork and brands like a CocaCola in your reel , you will NOT ! grade these kind of work in the future . The same with feature films . Me for myself I decided to go into this highend sector and I get job offers from all over the world . <p> About the colorists : <p> Lucky , who had the possibility to learn colorgrading on negative film and @ @ @ @ @ @ @ @ @ @ dailies/rushes for two years . This trained my eyes , made me fast and I learned to keep a look for month . It was a hard time , but the best training and a serious education . These days young " colorists " do n't have this oportunity anymore , except maybe some on-set DITs . So what happens : they ca n't get the maximum look out of the footage , scenes do n't match and in the worst case it looks like " video " . I saw editors , calling themselves colorists and were not able to increase the contrast properly . I saw compositing guys , who ca n't balance their shots . Instead of balancing , they key highlights and shadows and desaturate them . This works sometimes , but not always ... One guy exported a RED project from FCP as one Prores 422 Clip to grade it . He never heard about reconforming ... And all of this did n't start with Resolve , it started with Apple Color . <p> So sometimes they made something that I would call " ok @ @ @ @ @ @ @ @ @ @ they just fuck it up . I never heard of some guys as a " colorist " anymore . You do n't get so many chances . Especially some TV Editors do colorgrading after their work , when there is time . What they actually do it balancing some shots with a primary grade . And that 's ok . It is colorcorrection - not colorgrading . <p> And now : 4K and above ... ( which is nonsense , but forces further investion and will be a problem for small facilities ) We professionals are struggling with all this stuff sometimes . How will editor x , student y and all these fake colorists handle this ? Answer : they ca n't ! ! ! ! ! ! ! <p> So I am absolutely not worried . Who is serious will make his way . But:Low-budget and maybe even lower midrange budgets will have lots of competition . <p> I mentioned the grey tones above : mostly TV stuff , maybe TV series and telenovelas . Bu once you are in a workflow and have to follow technical guidelines @ @ @ @ @ @ @ @ @ @ text , short sentence : Serious grader will always have a job and there will always be money for colorgrading. " fake-graders " will get some chances on low-budget projects , but probably never take the step to become a senior . Even when they call themselves seniorcolorist . - ) <p> the business is evolving and morphing so quickly that it 's impossible to foresee where it will end up a month or a year from now . there is no business model that has clear advantages over the other without huge corresponding pitfalls as well . no one is " safe " . talent always has value , but whether that talent will want to compete in whatever marketplace it ends up being will decide a lot of things . all any of us can do is try and find a place that suits our abilities and access to work and try and keep up - like a stream over rocks ... wow did I just say that out loud ? 
@@44332862 @4332862/ <h> The definitive Orange-Teal Blockbuster Look thread ( let 's do it ! ) <p> But returning to this thread , what technique do you guys use to keep your pure blacks to stay black ? Desaturate them ? Log color wheels and push them in the opposite direction of the blueish tint of the grade ? <p> But returning to this thread , what technique do you guys use to keep your pure blacks to stay black ? Desaturate them ? Log color wheels and push them in the opposite direction of the blueish tint of the grade ? <p> Thanks 43971 @qwx453971 <p> Make a custom curve with the inverse of the cyan tint in the bottom end . <p> But returning to this thread , what technique do you guys use to keep your pure blacks to stay black ? Desaturate them ? Log color wheels and push them in the opposite direction of the blueish tint of the grade ? 43971 @qwx453971 <p> Log Shadows could do it . There 's always the chance the client actually wants tinted shadows . The real trick with @ @ @ @ @ @ @ @ @ @ relatively normal , and a LUT alone is not going to take care of human skin moving in and out of light and shadow . <p> Log Shadows could do it . There 's always the chance the client actually wants tinted shadows . The real trick with teal/orange is when the client wants the fleshtones to stay relatively normal , and a LUT alone is not going to take care of human skin moving in and out of light and shadow . 43971 @qwx453971 <p> Sorry for hijacking this as a quietly along-reading beginner but what is wrong with using my lum/sat curve for the purpose of desaturating the lowest part of the shadows ? <p> Sorry for hijacking this as a quietly along-reading beginner but what is wrong with using my lum/sat curve for the purpose of desaturating the lowest part of the shadows ? 43971 @qwx453971 <p> You 'll wind up " chiseling " away at low-frequency color information , particularly strong reds , which will get weird and crunchy in dark-medium light situations . Lum-Sat curves can be treacherous when lighting changes or when strong colors @ @ @ @ @ @ @ @ @ @ in some cases , but real-world is something altogether different . I think the reality is you have to tailor the correction for the nature of the shot , and be prepared to try 3 or 4 different things . 
@@44332863 @4332863/ <h> House of Cards ( yellow &amp; blue ? ) <p> Before we start throwing bets around , let 's look at the facts . It a Fincher show . Regardless of his role- director or a producer , he 's still the most controlling professional out there . Nothing he does is done by happenstance , every decision is very deliberate . He , along with DP is very much hands on with the color grading . They used PIX to exchange the notes . So , in his past films , that were shot on Red , starting with Social Network , he never used HDRx . How do we know that ? Because on his most recent film- Gone Girl , HDRx was never mentioned and even if Yan wanted to use HDRx , he could n't , because the conformed material was delivered for grading as DPX . And finally , it is a known fact , that all grading of 13 episodes was done in two weeks . There is not enough time to futz around with HDRx . Besides , based in the @ @ @ @ @ @ @ @ @ @ could possibly want . If they needed to light up the whole city of Washington , they could . There is no need for HDRx , when it all can be done in camera . 43971 @qwx453971 <p> Check out the inside cover of American Cinematographer - A large advert from RED showing the use of HDRx in Gone Girl . Does n't mean they used it , but it 's certainly been mentioned ! <p> Can you guys share the ad ? I 'm curious to see it . BTW , many of us went to the Gone Girl presentation and there Ian Vertovec never mentioned any HDRx . However , he did mentioned , that he used Neat Video on a couple of shots As I already said , the files , that were delivered to LI were DPX , unless they had done HDRx before DPX files were delivered for grade . <p> Can you guys share the ad ? I 'm curious to see it . BTW , many of us went to the Gone Girl presentation and there Ian Vertovec never mentioned any HDRx . @ @ @ @ @ @ @ @ @ @ Video on a couple of shots As I already said , the files , that were delivered to LI were DPX , unless they had done HDRx before DPX files were delivered for grade . <p> Not to belabor the point , but I would n't say it 's misleading . If you did n't know HDRx existed you would n't look at that ad and say " I want that HDRx thingy , " and if you did think it was HDRx , well , the camera does still include that option ... <p> That said , I 'd agree that the ad appears to be a clear failure . If their goal was to promote the new dynamic range of dragon ( without the need of HDRx ) it 's clear that many people are missing that message . <p> " Colourist Laura Jans-Fazio was able to use the HDR to get real detail in the window and composite that into the overall picture to achieve a more natural result . " <p> I suspect this might have been a one-shot thing , so it 's not like @ @ @ @ @ @ @ @ @ @ Laura did a nice presentation on HoC S02 at IBC last year , and she demonstrated her use of HDR-X to retain detail in a ( static ) shot with a window in the background . She did n't go into detail about how many shots it was , but it sounded like it was one particular set , not one particular shot . Locked off shots are certainly where HDR-X is most likely to work easily , and even then somebody crossing the window at any kind of speed can create artefacts . <p> I just discovered this fascinating piece on the production ( not post ) of House of Cards , and it 's very interesting to see how they use the camera and lighting in the show . Note that they were originally going to shoot the entire show handheld , and opted instead to go for a very rigid style , always on tripods and dollies , where the characters do all the movement and the camera moves very little ( if at all ) . <p> The green screen work in particular in the show @ @ @ @ @ @ @ @ @ @ the LCD displays look amazingly real , and are precisely timed to the background plates being keyed into the green screen . The way they synchronize this stuff and get it all to work is amazing -- it 's probably the best car process work I 've ever seen . 
@@44332864 @4332864/ <p> Jason , I 'm just about to go down the route of self-building a Z800 setup as our Autodesk-bought machine is an XW9400 and seriously slow , and we 're on a floating license . <p> As I do n't have the budget for the SDI daughter-board for the Quadro FX5800 that 's in the Z800 , can you tell me how you enable monitoring through the AJA board ? 43971 @qwx453971 <p> You enable it in init.cfg located LONG ... LONG ... <p> Directly below the VIDEO KEYWORD section , make sure this entry is present with no ' # ' included before it.Video AJAOEM2k , Serial1 <p> Also make sure the " VideoPreviewDevice " Presets you need are uncommented in the sections just below in init.cfg -If you have external sync connected to your system leave the DTVsync and 601sync presets uncommented. -If you DO NOT have external sync connected to your system leave the " free sync " presets uncommented . <p> One last note , you 'll need to have the AJA Xena 2Ke OEM board with the 0x75 firmware and run the DKU @ @ @ @ @ @ @ @ @ @ <p> That 's true , but when the installation script runs it asks you to look through the config file to verify it 's correct . It 's helpful to at least know what those settings should be , especially for someone new to the process , since init.cfg can be kind of daunting . <p> Jason , do you know if its possible to enable dual monitor out of the FX5800 ? 43971 @qwx453971 <p> I have never tried that before now , but I connected a second display to my Flame and it seems to be OK with it , although it only recognizes the primary display . The only change it 's seems you need to make is to replace the Flame wallpaper with a double-wide 3840x1200 version so it spans both displays correctly . As far as how the second display might affect performance during use , I 'm not sure . <p> They 're supposed to allow for a bigger desk , I have n't tried yet . I know there 's stuff for Flare too , to allow showing the result on second DVI @ @ @ @ @ @ @ @ @ @ . Autodesk sell 1 monitor setups , and that 's the way Flame is designed , you do n't need a second screen , I 'd rather go for a 30 ' with more res ( I have a 27 ' 1920x1200 IIyama that I love ) . 
@@44332865 @4332865/ <h> Pack of LUTs made by me <p> It 's stored in the current Bin you have selected . Where would you like us to store it ? 43971 @qwx453971 <p> I can never find them , Rohit ! If I highlight that clip in the Edit page and select " Show in Finder , " nothing happens . What I 'd like to see is where the actual file is stored in the finder ( under Mac OSX ) . I know where it is in the session . <p> I can never find them , Rohit ! If I highlight that clip in the Edit page and select " Show in Finder , " nothing happens . What I 'd like to see is where the actual file is stored in the finder ( under Mac OSX ) . I know where it is in the session . 43971 @qwx453971 <p> Rohit correct me if I 'm wrong , but I do n't think there is a file . I would not expect there to be . A generator creates the image directly in the timeline . @ @ @ @ @ @ @ @ @ @ test pattern generator into your monitor , you would n't say " but where 's the tape " ! <p> In simpler terms , a 3D LUT is a list of output colours for some of the possible input colours , and the output colours for input colours which fall in between the ones listed are calculated by interpolation , using the six nearest known values . There are several different ways this interpolation can be done . The most common is trilinear . A better one is tetrahedral . To quote Richard Kirk from the Truelight manual : <p> Trilinear interpolation is often used with 3D data . This may be faster on GPU architectures , but tetrahedral interpolation usually gives smoother and better results for RGB data where the cube diagonal lies along the neutral axis . 43971 @qwx453971 <p> No interpolation method is perfect . They are all making a " best guess " at unknown values . <p> I believe ACES mandates the use of tetrahedral interpolation if LUTs are used to perform the transforms . Newer hardware ( the Amira , the IS Mini etc. @ @ @ @ @ @ @ @ @ @ a check-box to use tetrahedral , although it defaults to trilinear . <p> haha , that 's fair . And it appears as if you already gained a customer in this thread . I am not judging . I understand that painters buy paint . But they do n't  make their own paint and hairdressers do n't  make scissors , but colorists can make their own luts . Regardless , best of luck with them . 43971 @qwx453971 <p> No problem David , I totally understand what you think , selling LUTs here may seem paradoxical In my mind , buying a LUT pack is way to save time and money rather than recreate a look from scratch . Thanks fo your support , I appreciate . <p> From a white paper about 3D LUTs written by Charles Poynton and Joel Barsotti . <p> One way to use a LUT to calibrate a display is to build a LUT that is fully populated . For eight-bit input components , the LUT would have 2563 entries ( that is , 16,777,216 ) . If each LUT entry has three components of @ @ @ @ @ @ @ @ @ @ memory . While not impossible in today 's technology , LUTs of this size are impractical . <p> The usual LUT technique is to use a LUT that is not fully populated , and to interpolate between entries . Essentially , each of the three input components is split into two components treated as an integer ( e.g. , 0 through 16 ) and a fraction f ( 0 G f &lt; 1 ) . The set of three integer components is used to access some number of LUT entries ( typically 4 , 6 , or 8 ) . Corresponding colour components of the LUT entries are then interpolated according to the fractions . In other words , in the usual case that the mapping represented by the LUT is a cube , the cube is considered to comprise a lattice of subcubes . Each LUT mapping requires access to some number ( perhaps all eight ) of the vertices of the appropriate subcube ; which subcube is chosen depends upon the integer components . <p> Some input colours are exactly represented in the LUT . For example , @ @ @ @ @ @ @ @ @ @ output triplet for the input triplet having 8-bit signal values 0 , 15 , 240 ( i.e. , 0 , 1-+15 , 16-+15 ) . However , such cases are rare : Ordinarily , the LUT will not contain an entry for the presented input value , and interpolation is required . Two interpolation techniques are common in LUT processing after calibration : trilinear interpolation and tetrahedral interpolation . <p> In trilinear interpolation , each interpolation operation involves fetching eight entries from the LUT , that is , all eight corners of the lattice subcube . For the four pairs of corners aligned along the red axis , four linear interpolations are performed using the red fraction component . The four results are formed into two pairs aligned along green , and two linear interpolations are performed using the green fraction component . The result pair is aligned along blue ; the final interpolation uses the blue fraction . For each pixel to be mapped , the technique involves eight memory accesses , seven multiplies , and two adds . <p> In tetrahedral interpolation , each lattice subcube is partitioned @ @ @ @ @ @ @ @ @ @ three ) comparison operations , each having complexity equivalent to an addition , to determine from which tetrahedron a particular interpolation must be computed . Once those comparisons are performed , only four memory accesses are required . Memory data rate is usually a significant constraint in real-time colour mapping , so tetrahedral interpolation is very common . <p> Tetrahedral interpolation can perform 3-D LUT interpolation at studio quality . Hardware , firmware , and software implementations are widespread . However , we believe that tetrahedral interpolation is not sufficient to construct LUTs . We seek to construct very accurate LUTs , so we want an interpolation technique that is more precise than tetrahedral interpolation . Our technique is described in the following sections . 
@@44332866 @4332866/ <h> Cineform becomes VC-5 now , under the wings of SMPTE <p> That 's huge , I was hoping for this for quite a while . Would be so nice to get rid of ProRes. ? <p> ? <p> SMPTE Standardizes GoPro CineForm Codec as the New Open Standard for Video Acquisition and Post Production <p> SAN MATEO , CA ( June 6 , 2014 ) " GoPro , maker of the Emmy- Award-winning HERO- camera and enabler of some of today 's most engaging professional video content , announced today that the core technology behind the GoPro CineForm Codec has been standardized by the Society of Motion Picture and Television Engineers- ( SMPTE- ) as the SMPTE ST 2073 VC-5 video compression standard " the new open codec standard for video acquisition and post production . <p> The GoPro CineForm Codec is a high-performance video codec widely valued in video post-production as the industry 's highest quality compression format . It is well established in the professional filmmaking marketplace and has been extensively used in Hollywood feature films , including Slumdog Millionaire and Need for Speed . <p> @ @ @ @ @ @ @ @ @ @ standard now means the professional production and post production community can access SMPTE-standardized codec technology for presenting high resolution imagery with a high speed , low-compute intensive workflow . The new VC-5 standard will provide a more efficient , cost effective and higher-performance workflow , delivering an 8x performance boost over the current JPEG 2000 format with higher resolution . <p> Good luck trying to get everyone everywhere to switch to that from ProRes with cameras like the Alexa shooting it as a native format . Cineform is also lower performance / lossy compared to ProRes especially in the 4444 and 444XQ formats . <p> It seems inevitable that Pro Res will eventually disappear ( Wishful thinking part ) . They do n't care about professional hardware or software . So a scalable third party open source solution seems like a step in the right direction . Of course I also like Marc 's $5 concept . <p> Good luck trying to get everyone everywhere to switch to that from ProRes with cameras like the Alexa shooting it as a native format . Cineform is also lower performance / lossy @ @ @ @ @ @ @ @ @ @ . 43971 @qwx453971 <p> Well actually a Cineform license is cheaper than a ProRes license - even a super small Chinese camera manufacturers like Kinefinety can afford it , so Arri should have no problem to implement it . <p> Tell that the RED-guys , that ProRes is better than a wavelet codec , if you want to make yourself a laughing stock . ProRes is still 32bit on Windows , and quite a pain in the arse - and do n't get me into gama shifting . <p> just an other codec package you install with one mouse click , just as you have to install QT or Redcode , and nobody ever complained about this . - nothing to write home about . <p> Actually it would be a logical , and clever step for Blackmagic to implement it in the URSA , so they would have a second raw option . 43971 @qwx453971 <p> Frank with all due respect i think you have underestimated the issue <p> with the respect to the UK and Broadcast over the next few years all final deliverables will go out as @ @ @ @ @ @ @ @ @ @ in the AS-11 wrapper <p> commercials will be switching over to this as well to standardise the whole industry for delivery <p> Most houses use DNX or Prores as there work codec and i can , t see any one changing as millions of pounds have been put into these infrastructures ... we are PRO RES House i cant see us changing any time soon <p> We get delivered Pro res by pretty much every one and goes back out to the online streaming guys as PRORES and for broadcast it 's DPP , and again Prores for commercials until the new spec come into force <p> i want people to shoot in pro res formats its way way easier for us to cope with in the edit and even AVID is pretty good with it now <p> For Film , people will use what ever they want if the camera shoots it Cineform/DNG/Pro res/ R3D/Pro tune , Cinepak .... who cares really ..... the only issue on the post side of things is can we mange it effectively and give the producers and directors what they want at the @ @ @ @ @ @ @ @ @ @ all of those formats most people have an idea of what it will deliver and some directors will use every they can to get the shots to tell the story .... my post production award of the year goes to the guys who had to pull together micheal mann , s black Hat <p> which was shot on Arri , 5D,7D , Go pro , Phantom Flex and Red <p> so that 's arri raw , . cine raw , H264 and r3d <p> You would proxy /offline what ever , which will be a light weight DNX or Pro res format and edit , then reconform it all from the masters with the VFX shots to be locked in and finalised graded , trim passes etc etc etc .... i have simplified that slightly Ha ! <p> then a film is going to go out as DPX , TIFF Cineon or open EXR and now some are being finished in pro res 444XQ ( indie wise away ) which i actually think will become one of the standards high end acquisition next to R3D /Arri Raw <p> which then it @ @ @ @ @ @ @ @ @ @ ( beautiful ! ! ! you can , t beat a film out sorry i love film ) <p> So i really can , t see how cineform is going to get in there and change this system away from pro res ..... personal i like cineform for all the reasons mentioned and i can see it being use a bit more <p> but its a bit more than " installing another codec with a mouse click , nothing to write home about " if you had said " its like moving the entire population of earth to live in domes on the moon " <p> Well actually a Cineform license is cheaper than a ProRes license - even a super small Chinese camera manufacturers like Kinefinety can afford it , so Arri should have no problem to implement it . 43971 @qwx453971 <p> Read Jason 's post above yours . The ProRes SDK is free to developers , they just need to go through an approval process before/after implementing it . Cineform last I heard did licensing deals PER unit sold . The numbers I 've heard per unit @ @ @ @ @ @ @ @ @ @ just a reasonable amount . <p> let me get something straight - say my facility was to switch to cineform for all our rendering needs , what container would we be using ? on premiere , expect for a weird avi container , I think the only choice quicktime ( . mov ) <p> so , would n't that negate getting away from the dysfunctions of prores , if you 're still in QT land ? ( we are a windows house ) <p> let me get something straight - say my facility was to switch to cineform for all our rendering needs , what container would we be using ? on premiere , expect for a weird avi container , I think the only choice quicktime ( . mov ) <p> so , would n't that negate getting away from the dysfunctions of prores , if you 're still in QT land ? ( we are a windows house ) 43971 @qwx453971 <p> This is the problem no one has a solution for . <p> In OS X 10.11 QuickTime / AV Foundation is qettting a considerable amount of @ @ @ @ @ @ @ @ @ @ / linux however . <p> would n't that negate getting away from the dysfunctions of prores , if you 're still in QT land ? ( we are a windows house ) 43971 @qwx453971 <p> Being in a QT container does n't necessarily mean you 're using a QT API. if the host app can recognize the stream , it can opt to decode it itself , rather than through the QT libraries . Case in point Resolve using DNxHD in a mov , will use it 's own DNxHD libraries , you do n't even need the DNxHD codec installed . <p> with the respect to the UK and Broadcast over the next few years all final deliverables will go out as DPP AVC intra 100 for HD and IMX for SD in the AS-11 wrapper 43971 @qwx453971 <p> Broadcasters are part of the problem . They are like huge ships , you do n't turn them around on a dime . That 's the reason , why wee still get " HD " program broadcast , with SD image information , and block artifacts in the size of @ @ @ @ @ @ @ @ @ @ there work codec and i can , t see any one changing as millions of pounds have been put into these infrastructures ... 43971 @qwx453971 <p> How does installing an other codec , cost millions of pounds ? You do n't have to change the infrastructure for that - heck an editor will not even realize he works with a different codec ( besides the fact , that it is faster , and better quality ) . <p> which then it turns into a DCDM or DCP or a Film out ( beautiful ! ! ! you can , t beat a film out sorry i love film ) 43971 @qwx453971 <p> Would n't it be nice having CineformRaw coming from ALL those different cameras ? And would n't it be even greater to deliver a single Cineform file to the cinemas , instead of dealing with DCPs ? <p> What I want to say is , this is 2015 - why should I use an old DCT based codec ( DNX/ProRes ) , when I can have a wavelet based ( Cineform/R3D ) instead ? <p> Personally I have @ @ @ @ @ @ @ @ @ @ I keep it uncompressed till final delivery . We are a super small , bottom feeder film/postproduction comnpany and we can deal with it . I always find it bizarre when huge post houses , with huge clients tell me they ca n't afford it . <p> let me get something straight - say my facility was to switch to cineform for all our rendering needs , what container would we be using ? on premiere , expect for a weird avi container , I think the only choice quicktime ( . mov ) <p> so , would n't that negate getting away from the dysfunctions of prores , if you 're still in QT land ? ( we are a windows house ) <p> Personally I have n't shot anything than uncompressed raw since years . And I keep it uncompressed till final delivery . We are a super small , bottom feeder film/postproduction comnpany and we can deal with it . I always find it bizarre when huge post houses , with huge clients tell me they ca n't afford it . <p> We are a super small @ @ @ @ @ @ @ @ @ @ it . I always find it bizarre when huge post houses , with huge clients tell me they ca n't afford it . 43971 @qwx453971 <p> Because you do n't have to interoperate simultaneously with ten other equally huge vendors on your projects . Just collaborating with DPX is difficult enough , much less sending out raw and ensuring every one of the hundreds of artists who touch it are all seeing the same thing . <p> Large post houses are just fine supporting whatever workflow a client wants ... as long as they 're ready to pay for it . On a recent project a studio pushing heavily for OpenEXR over DPX , completely baulked when they saw the difference in storage costs for the life of the project were over $150,000 . <p> I know we 're all very empowered now that we have Resolve , Creative Cloud , and Media Composer on our desktops , but the cost relationship between what happens on a indie project and a major feature is n't linear . It 's exponential . 
@@44332867 @4332867/ <h> Why do colors Bend on a Vectorscope ? <p> Doug , out of gamut or not , I think it 's just a simple matter of the Sony having much more dynamic range and color separation , and this is creating yellow highlights . I do n't think you are seeing a bend as a result of some type of clipping or Add to dictionary . You 're seeing the bend because there is significantly more bright yellow in the Sony image , as a result ( IMHO ) of the Sony camera thinking there is significantly more bright yellow in the image . Not as a result of color degradation due to clipping . <p> 1 . The light coming out of the taillight is not pure red , while this might or might not be obvious , it 's proven by the fact that the color chart displays other colors than red . Those are not overly saturated/illuminated color swatches , so their colors can not be explained by some type of a hue shift due to extreme color . If the taillight were putting out @ @ @ @ @ @ @ @ @ @ where everything appears as a shade of grey . You can simulate this look in Resolve by setting Luma Mix to 0 and going into Curves and lowering the Green and Blue curves to 0 . <p> 2 . The Blue channel of the A7 footage is weak through the entire luma range ; look at the parades and you 'll see what I mean , a very thin column for Blue . The Blue channel is very skinny compared to the Green channel . This explains your yellow bias , because yellow is Red plus Green . Less Blue = 's more yellow . If you want to balance out the channels , go into the RGB Mixer and increase the Green column of Blue Output . I found 0.79 gives a pretty " balanced " image and probably what you 're expecting from the taillight ( but the rest of the image looks wonky ) . <p> 3 . Why is the A7 's Blue channel so weak and is it accurate ? I honestly do n't know . It could come down to a bias in the @ @ @ @ @ @ @ @ @ @ that comes to mind is that the white balances may be significantly different for each camera . You could turn on a clean source of white light and custom WB both cameras to it and then run the test . But when I 've looked at the white and grey areas of images from both cameras , their RGB values are relatively similar , which against this being a WB issue . <p> 4 . Another test I would recommend is to slowly stop down each camera while running the test . This will show you if there is truly a shift due to highlights blowing out or not . <p> Could I raise the question of the light source ? A 12V tungsten tail light is almost 100% yellow-orange light , probably somewhere around 1700-1900K . If the Sony chip handles the highlights better without clipping it would be logical for it to look more orange . The right hand image would be technically more accurate . If the Canon ( on the left ) clips highlights and shows less yellow it would look more like the tail light @ @ @ @ @ @ @ @ @ @ ( scientifically ) in it 's rendering of the scene . <p> Ric , along those lines , I also think that the strength of the red plastic over the bulb does not apply evenly . I imagine that the center of the bulb where it 's much brighter is somewhat less affected by the red gel than the light coming off of the plastic reflector behind the bulb . <p> So we know that there is hot spot in the very middle of the bulb , and the A7 is showing a color shift to yellow in that hotspot . I believe that a considerable amount of that color shift may be " naturally " occurring and is not a result of a remapping of out gamut of color . 
@@44332868 @4332868/ <h> DaVinci 12 is not reading the MetaData of EX3 <p> Hi , It 's my first post on LiftGammaGain , so I hope , I am at the right place : I am grading a doc film , from AVID , delivered as AAF , but when I drop the Cards into Media Pool ( Mainly SONY EX3 ) , DaVinci zeroes out all Starting TC . All clips start at 00:00:00:00 , so the AAF of course wo n't relink . Strangely , in PPro , AVID , FCPX and Catalyst the TC Metadata is read correctly . What is DaVinci 's problem ? <p> It is only in the 12.5 version of Resolve that it even sees XDCAM EX files . <p> Have you tried to do the XML roundtrip approach starting off with your NLE ? 43971 @qwx453971 <p> Hi Andrew , <p> First of all thanx for the reply . Actually I do use Resolve 12 and it does eat the EX3 Files in the Media Pool , but just with all Starting TCs set to 0 . It comes from AVID so we @ @ @ @ @ @ @ @ @ @ the problem is that the TC Metadata of the EX3 and the MTS Files of the other Camera are not read by DaVinci . Weird enough , I tried it in every other possible NLE I have on my system and they all read the MetaData just fine . <p> I watched the tut , but unfortunately , his scenario is upside down . In my case all the TCs in the AAF are correct , and all the Media TCs from the Cards are coming in wrong , meaning all Start TC is 00:00:00:00 . <p> What happens if you re-wrap the EX3 files as QuickTimes using XDCam transfer or something similar ? I forget if Catalyst Browse can also do this. /quoHi Nick , <p> Well , first of all , I do not even have the old XDCam Softwares on my new system anymore and wether Catalyst does it .... no clue . But it 's a huge amount of card , which is tedious to rewrap all , while actually every other Software I have on my machine has no problem recognising it . Consolidating to @ @ @ @ @ @ @ @ @ @ . But I still wan na know , why is this happening for the future ( I hate unsolved problems ) : Is it a bug of Resolve or did I just overlook something . <p> Agreed , it would be a pain to have to re-wrap everything , and much better if it was n't necessary . But at least re-wrapping is much faster than transcoding , and has no re-compression hit ( or having to go to a much larger file size by using a high quality DNxHD variant to minimise quality loss ) . <p> For what it 's worth , I found I actually had a couple of XDCam EX clips re-wrapped by XDCam Transfer on my laptop , and Resolve ( 12.3.2 and 12.5 ) reads them and their timecode correctly . 12.5 also correctly sets the aspect ratio for 1440x1080 XDCam EX clips , whereas on 12.3.2 you have to manually set it to " DVCProHD " . <p> Agreed , it would be a pain to have to re-wrap everything , and much better if it was n't necessary . But at least @ @ @ @ @ @ @ @ @ @ re-compression hit ( or having to go to a much larger file size by using a high quality DNxHD variant to minimise quality loss ) . <p> For what it 's worth , I found I actually had a couple of XDCam EX clips re-wrapped by XDCam Transfer on my laptop , and Resolve ( 12.3.2 and 12.5 ) reads them and their timecode correctly . 12.5 also correctly sets the aspect ratio for 1440x1080 XDCam EX clips , whereas on 12.3.2 you have to manually set it to " DVCProHD " . 43971 @qwx453971 <p> Dear Nick , <p> Actually , the Transcoding to DNxHD is still faster , it took us 2 hours for a one hour documentary film , and I guess , there is no loss of quality when going from 35 MB/sec of EX3 footage to DNxHD 185x Files . Still I 'd love to know , where this blunder results from , since it 's the first time ever , I had DaVinci being incapable of reading MetaData , whilst every other NLE there is , managed to do so . <p> Actually , @ @ @ @ @ @ @ @ @ @ us 2 hours for a one hour documentary film , and I guess , there is no loss of quality when going from 35 MB/sec of EX3 footage to DNxHD 185x Files . 43971 @qwx453971 <p> I am surprised that you say transcoding is faster . Re-wrapping is just copying data , so there is no processing overhead . <p> It 's true the loss is probably negligible when going from 35Mb/s to 185Mb/s , but you are increasing the storage requirements more than five times . <p> I think that it is because davinci do n't work with oryginal EX mp4 . On OSX you have to rewrapp to mov . Now in 12.5 on PC you can work with EX but in MXF OP1A container ( after avid consolidation to EX 35 ) . For me it works well . <p> That 's not entirely true . First of all I got the Card Folder as is . Second I ingested EX3 footage for years successfully with the FCP7 Log &amp; Transfer tool , in X by direct importing , PPro by direct importing , and AVID with @ @ @ @ @ @ @ @ @ @ the XDCAM Tool always . Besides it 's obsolete now anyways and will be replaced entirely by Catalyste , I guess . <p> This is normal . Resolve ( and almost every other finishing tool I can think of ) only reads the data in the media file itself . Since the EX3 stores all of the TC and other data in a side car file within the folder structure and not within the media file itself , Resolve is going to show a start TC of zero . This is the case with a few other cameras as well that do not store the data within the media file itself . For cameras like the one you shot with , transcoding in an NLE is going to be your only option . <p> This is normal . Resolve ( and almost every other finishing tool I can think of ) only reads the data in the media file itself . Since the EX3 stores all of the TC and other data in a side car file within the folder structure and not within the media file itself , Resolve @ @ @ @ @ @ @ @ @ @ This is the case with a few other cameras as well that do not store the data within the media file itself . For cameras like the one you shot with , transcoding in an NLE is going to be your only option . <p> Rohit , please correct me if I 'm wrong here . 43971 @qwx453971 <p> I thought that first too , but there are two things contradicting that theory : <p> 1 . ) I tried other SONY Footage , that has the same side car system of MetaData in separate files and dropped them into the Media Pool and it all recognised the MetaData ( A7s , FS7 , FS5 and so on ) <p> 2 . ) In this project I also had one other camera that shot AVCHD ( . MTS ) Files and there was the same problem . 
@@44332869 @4332869/ <h> Key Master 1.4 for DCP/KDM management , delivery and control <p> KeyMaster is a web-based platform for the management , duplication , delivery and control of KDMs . It is delivered as Saas Software as a service in the cloud or as standalone software to be installed in a dedicated server . <p> KeyMaster is designed for every post-production facility , studio or cinema content distributor , available in different configurations like the professional version for yearly subscription of 1,9K EUR , including software updates , it works in any internet browser ( safari , chrome , explorer , etc ) . <p> distribution Lists ( only professional subscription ) creation of distribution lists to group screens together for the automatically creation of lots of KDMs for one movie , support of several filters like : by account , by country , by city , by location , for easily search of screens <p> movies manage , store and retrieve of DKDMs for original and supplemental packages of a movie <p> upload of the DKDM for each movie and version <p> deliveries create KDMs using data either of @ @ @ @ @ @ @ @ @ @ and one movie , selecting the validation playing time for it <p> KDMs overview and control of created KDMs <p> Email it send a ZIP file including the created KDMs <p> KeyMaster allows to use your own personal email server to send out the KDMs generated through automatic emailing. 
@@44332871 @4332871/ <h> Pack of LUTs made by me <p> Hello , I test my LUTs on high resolution stills and footages with color chart . I try to not degrade the image . <p> I 'm interested to learn about your process , can you tell me more about : " applied it to a linear gradient and i can see several area 's where curves appear to show math errors , doubling back onto themselves , and thereby creating artifacts " ? <p> Thank you in advance ! 43971 @qwx453971 <p> Bonjour Benjamin ; <p> here 's a linear ramp that i mentioned , created in a Discreet Logic Flame , it goes from 0 to 1023 , and Resolve 's software scopes underneath ; <p> now the same ramp with your 8700 LUT applied ; <p> in the center of the scope i enlarged an area showing stairstepping and jumps in what should be a smooth curve in the blue channel , this would be the apparent math errors i noticed this morning <p> I noticed this first on a Baselight this morning with external scopes , but @ @ @ @ @ @ @ @ @ @ i used that to show , these scopes are not really very good , and the Techtronic 's external scope i was using this morning seemed to show it more clearly . <p> I would also be quite interested to learn . As I understand these things are a bit of a highly guarded military secret for colorists , but since I do n't really work in a big facility with their own color science department , I try to learn where I can , so any advice would be highly appreciated <p> Stepan 43971 @qwx453971 <p> well I do n't know anything heavily guarded but its becoming easier due to a variety of tools , assuming you understand what a LUT can and can not do . I 'm a huge fan of the Mac app Lattice for lut evaluation and almost any conversion you can think of . In terms of simply making them , you can export a cube LUT from Resolve , 3d LUT creator , After Effects with the LUT buddy plugin ( free ) , and Adobe Premiere Pro , among a few . @ @ @ @ @ @ @ @ @ @ but many of the apps I mentioned can create a lut from any before / after image ( you provide it the before and after whatever color work you 've done , it creates a LUT for the changes ) . Hope that helps a bit . <p> here 's a linear ramp that i mentioned , created in a Discreet Logic Flame , it goes from 0 to 1023 , and Resolve 's software scopes underneath ; <p> now the same ramp with your 8700 LUT applied ; <p> in the center of the scope i enlarged an area showing stairstepping and jumps in what should be a smooth curve in the blue channel , this would be the apparent math errors i noticed this morning <p> I noticed this first on a Baselight this morning with external scopes , but Resolve is the software i have at home , so i used that to show , these scopes are not really very good , and the Techtronic 's external scope i was using this morning seemed to show it more clearly . <p> I hope this helps , @ @ @ @ @ @ @ @ @ @ 's ! 43971 @qwx453971 <p> Dermot , Do you mind sharing that linear ramp with me . I found many on google but I do n't know how " correct " they are . Thanks Taj <p> I think the " jaggieness " may not be inherent in the LUT , but may be an artefact of the interpolation . Here are a pair of waveform plots from Nuke , with the LUT applied in an OCIOFileTransform node set once to linear interpolation and once to tetrahedral . As you can see , with tetrahedral interpolation , although there are very small steps visible , they are pretty minimal compared to those with linear . <p> I think the issue is that a 643 LUT does not have vertices representing the grey scale points , so those are always made up by interpolation . Although intuitively you might think a 643 LUT would be " better " than a 333 one , this is not necessarily the case for grey values . An odd numbered cube size will not need to interpolate for the grey scale . So better still @ @ @ @ @ @ @ @ @ @ " is probably to convert the LUT from 643 to 653 using tetrahedral interpolation . I expect the resulting LUT will give a better result in a system which uses tri-linear interpolation . <p> I think the issue is that a 643 LUT does not have vertices representing the grey scale points , so those are always made up by interpolation . Although intuitively you might think a 643 LUT would be " better " than a 333 one , this is not necessarily the case for grey values . An odd numbered cube size will not need to interpolate for the grey scale . So better still would be a 653 cube . <p> The " fix " is probably to convert the LUT from 643 to 653 using tetrahedral interpolation . I expect the resulting LUT will give a better result in a system which uses tri-linear interpolation. 43971 @qwx453971 <p> For only 104 words , that was an insane amount of good information . Thanks , Nick . <p> I think the issue is that a 643 LUT does not have vertices representing the grey scale points @ @ @ @ @ @ @ @ @ @ Although intuitively you might think a 643 LUT would be " better " than a 333 one , this is not necessarily the case for grey values . An odd numbered cube size will not need to interpolate for the grey scale . So better still would be a 653 cube . <p> The " fix " is probably to convert the LUT from 643 to 653 using tetrahedral interpolation . I expect the resulting LUT will give a better result in a system which uses tri-linear interpolation . <p> haha , that 's fair . And it appears as if you already gained a customer in this thread . I am not judging . I understand that painters buy paint . But they do n't  make their own paint and hairdressers do n't  make scissors , but colorists can make their own luts . Regardless , best of luck with them . 
@@44332872 @4332872/ <h> Cineform becomes VC-5 now , under the wings of SMPTE <p> Getting back to the wrapper question , David Newman , one of the 3 original Cineform guys , has said that the . mov and . avi wrappers for VC-5 are good to go . However SMPTE has yet to ratify the . mxf wrapper ( but they 're close ) . It 's SMPTE 's goal to have MXF be the single wrapper for all essence files in the future . That pretty much has to be the case if they ever hope to get their IMF thing universally adopted . And then perhaps there will be peace on earth between post houses and networks . And yes , there is a pony under that pile of manure . <p> The issue of VC5 and how it affects ProRes is an interesting one . Realistically , content creators/owners have a lot of interest in moving off proprietary codecs such as ProRes and DNxHD . An example of this is recent moves by Apple to make an RRD ( Registered Disclosure Document ) under SMPTE for ProRes @ @ @ @ @ @ @ @ @ @ owners are nervous about Apple and the future of ProRes when it comes to many Masters that may be based on ProRes . The RDD is an attempt to reassure them . But the truth is , Apple could sell of ProRes in the future and changes to the license could have huge impacts on how you can access your Master . This is very unlikely but must be taken into acount as a liability .. There are very good reasons the work on ACES is based around processes and file formats that are non IP encumbered . <p> In talking to many big iron post production tool makers , there is a clear understanding that ProRes works well and is well accepted . Its going to be hard to convince the typical post production provider to more away from ProRes .. The use of ProRes does not affect them and they are very familiar and happy with the workflows . The liability issues will be driven by the big guys ( Studios , cable companies etc ) as I expect they are likely to make VC5 a prefered deliverable @ @ @ @ @ @ @ @ @ @ form ( VC5 in MXF ) . <p> VC5 specified container will be MXF once that document is ratified by SMPTE . We should expect some form of product into the market that allows VC5 in MXF . ( MXF being another SMPTE ratified Standard that has no liabilities ) <p> But really , what many in the technical committees I have spoken to expect , is that VC5 will be driven into wider use by HDR and the need for RAW files . RAW is a problematic workflow , and VC5 is uniquely suited for this form of workflow . ( Ie it makes RAW as easy as ProRes is today ) No other file format comes close right now . And as it is a ( near ) ratified SMPTE standard , it is unexpected that any proprietary implementation will get a lot of traction like ProRes and DNxHD had in the past . It really depends on how GoPro make this codec available or ubiquitous . And if VC5 does manage that , I expect it may erode ProRes dominance considerably . <p> There is also the @ @ @ @ @ @ @ @ @ @ big iron post tool providers . When it comes to 4K and the new demands on post production , CPU speed , GPU etc . Apple are inadequate . Many are turning away from Apple for tools at this level . ( Ie typically anything over HD , yes Apple MacPro can do it , but in real world applications , its simple not got the goods . ) Windows/Linux being the options . So its becoming more and more important for workable cross platform codecs . Definitely not ProRes . <p> In recent times , yes Apple has agreed to license to some specific big iron manufacturers , but we do not see it in Adobe , Sony or other generic forms that would allow the use of non apple commodity solutions . People have been asking for this for years . If you think this is not intentional ....... Consider Apple is using its dominance in a technology to gain an advantage . ( All well and good that is capitalism ) but then consider putting the future of the multi million dollar film in the hands of @ @ @ @ @ @ @ @ @ @ on the fact your project sits on top of their proprietary codec ...... Consider your position/million dollar investment and discuss it with your lawyer . <p> so that 's being rolled out now and soon for commercials BTW that covers our major cable platforms as well <p> only 2 camera i know of shoot Cineform the Si camera and knifinity one so where will all this new cineform be coming from <p> and i can , t see RED or Arri changing from r3d or . ari any time soon <p> what " raw problematic workflow " ..... ours is fine thanks and works really well <p> so can you explain what part of the industry this is going to be used in ........ remember we don , t want to transcode we want to edit , audio , effects , grade and bang that puppy out as time is money <p> Also <p> " but then consider putting the future of the multi million dollar film in the hands of a company willing to take advantage of such positions based on the fact your project sits on top of their @ @ @ @ @ @ @ @ @ @ it with your lawyer . " <p> and for the life of me i can , t work out what you mean by this Films in general don , t sits on Pro Res ... its a working part of films life sure for edit etc but Films live on film Reels or end up on DCDM , s . so how could shooting or working with pro res during the process affect the IP of feature film <p> Be it AS11 or IMF in the future . Yes these are mandated deliverables based on SMPTE standards and codecs . However , many features are still finishing in ProRes , and delivered in ProRes outside of the UK . Many still consider the ProRes timeline render is the original Master while the AS11 is a deliverable derived from that master . <p> You are correct , that going to a DCP , typically but not always , you may go through a DCDM . But in reality many consider the final ProRes file converted to a DCP is the actual " Master " It is the native codec used on @ @ @ @ @ @ @ @ @ @ the best quality or equivalent quality to a DCDM derived from it , but smaller and easy to play . <p> Where VC5 will go is into cameras . Coming in from a shoot , no need to transcode ( Like ProRes ) .. Can compress RAW but unlike r3d , does not need transcoding into a more manageable file format for editing . All other RAW ( Apart from CinemaDNG ) file formats are either proprietary compression like r3d , or not compressed at all .. but still proprietary to keep hidden capabilities of the sensor . CinemaDNG is also another path but with 4K RAW high bit depth , data management is very important . Using VC5 is very well suited for this world of 4K acquisition and high bit depth for HDR grading . <p> ProRes is likely to stay for a very long time , however , I can see VC5 taking a strong position where no other codecs are as suitable , and if it does become a ubiquitous file format for those reasons , is likely to erode ProRes for reasons based on mitigating @ @ @ @ @ @ @ @ @ @ will play on any Mac , out of the Box . <p> ProRes is likely to stay for a very long time , however , I can see VC5 taking a strong position where no other codecs are as suitable , and if it does become a ubiquitous file format for those reasons , is likely to erode ProRes for reasons based on mitigating risk . Its hard to beat how Prores typically will play on any Mac , out of the Box . 43971 @qwx453971 <p> Given that Apple wound up making their Apple Lossless audio format open-source , I 'm surprised they do n't do the same thing with ProRes . They do n't really make any money on ProRes -- I would bet 80% of their income comes from iPhone , and ProRes and QuickTime are tiny slivers . Call it a $1/year license fee or something ( per program ) , and stop making Windows users jump through hoops . It 's doable -- all they have to do is to want to do it . 
@@44332874 @4332874/ <p> i have , nt tried that combo but if you feed Beauty box in to FT .... it works really well use BB for skin then FT for eyes and lips eye bags etc and a bit of contrast adjustment ..... basically between these two plugins there is a great tool waiting to be made <p> I am color grading a documentary with a lot of interviews and the tracker is accurate even on fast moving footage when they are moving the camera around to get the scene composition . It 's accurate during fast pans when the face is blurred due to the shutter speed . <p> It can correctly track faces with different skin tones . I have interviews with white and black people and the tracker does not have any trouble with skin color ( as opposed to some other trackers that got into the news because they were " racist " ) . <p> The tracker is also accurate when the face is at angle , which is an extremely difficult problem to solve in Computer Science . <p> One problem that I do @ @ @ @ @ @ @ @ @ @ often goes outside the boundaries of the face . This means that large changes in the settings of the plugin start to affect the environment around the face . <p> Another smaller problem is that , as other people mentioned in the thread , the default settings are quite unreasonable and they need to be dialed down a lot . <p> One problem that I do have with it is that the mask it generates very often goes outside the boundaries of the face . This means that large changes in the settings of the plugin start to affect the environment around the face . 43971 @qwx453971 <p> Is there a way to adjust the sizing of the specific facial parameters ( head , eyes , nose , lips , etc ? ) , or is it just one thing that moves around ? <p> I hope that in a future version they will also allow me to move the individual green nodes in the mask . 43971 @qwx453971 <p> That 's the problem for me -- I would always wan na be fussy and tweak the eye position separately @ @ @ @ @ @ @ @ @ @ . Call me crazy , but I think all this is is a preset version of several windows , keys , and masks made to fit an " average " face . I think you can pretty do this as a PowerGrade ( with several nodes ) and then have individually trackable/movable pieces . <p> Looking at Jake 's message elsewhere about the paint features in Baselight , I would have much rather Resolve had more paint-type features instead of this thing . This just looks like an easy way for less-skilled people to do beauty work , or skilled people to do kind of quick-and-dirty work when they do n't have a lot of time . A serious paint tool would 've been much better from my perspective . <p> Actually it 's not the placement of individual features , that are problematic . I 'm pretty sure they detect placement of those features . It 's the complete lack of the keyer control . Even on a perfectly shot images keys can be iffy . On the shot , that I have used , even though it was @ @ @ @ @ @ @ @ @ @ . One can get significantly better results by sticking to the traditional face retouching techniques . <p> One can get significantly better results by sticking to the traditional face retouching techniques . 43971 @qwx453971 <p> Yeah , that 's what I figured . I generally wind up doing different masks on forehead and neck than I do on the face and eyes and so on . And I also use different amounts of softening and blurring ( or sharpening ) . There 's no automatic way to do it , but I have mulled over doing some PowerGrades that might get close . <p> I suspect the Face Tracking thing might be a " Gee Whiz " feature that gets a lot of press , but as Jake says , in real-life it may not prove to be better than what we already have . <p> I found an irritating bug . If I try to copy a node with Face Refinement from one clip to another it copies the entire face tracking data as well ( ! ! ) , not only the adjustment settings . Which is just @ @ @ @ @ @ @ @ @ @ to be copied over , not the face tracking points , which will of course not match . <p> Actually it 's not the placement of individual features , that are problematic . I 'm pretty sure they detect placement of those features . It 's the complete lack of the keyer control . Even on a perfectly shot images keys can be iffy . On the shot , that I have used , even though it was shot under controlled conditions , it 's still fails completely . One can get significantly better results by sticking to the traditional face retouching techniques . 43971 @qwx453971 <p> Agreed - at this point , I think I 'm still forced to do this type of thing in nukex ... it 's a total pain in the butt dropping out of davinci back into nuke for relighting faces . They need to get this keyed , also the keyed information needs to be integrated with rotopainting ( masks are hopeless in my oppinion ) . <p> Agreed - at this point , I think I 'm still forced to do this type @ @ @ @ @ @ @ @ @ @ in the butt dropping out of davinci back into nuke for relighting faces . They need to get this keyed , also the keyed information needs to be integrated with rotopainting ( masks are hopeless in my oppinion ) . 43971 @qwx453971 <p> Interesting . Personally I have no experience with that level of beauty work . Would love to see how this works . Do you now any online demo or YouTube video that demonstrates something like this ? <p> Dan Moran of MixingLight.com did a series awhile back on Beauty work and Digital Makeup within Resolve -- but note this is a pay site . And there 's several threads on skin softening here on LGG. 43971 @qwx453971 <p> Just doing a simple skin softening for beauty treatments is no longer good enough . Anyone with two eyes can now spot this a mile away and that is why more advanced methods must be used . At the very least , skin needs to be re-textured after the blemishes had been removed for more realistic look . <p> Here you go . The same guy has some AE @ @ @ @ @ @ @ @ @ @ <p> Very cool . All I 'd have to do is to export the shot from Resolve , open it in Mocha , track it , export data to Nuke , create clean plate in Photoshop and then put it all together including warping in Nuke , render it and import it back into Resolve . Or one can do it all using a free version of Baselight Student Edition V5 in less than 5 minutes , when it will become available for download 
@@44332876 @4332876/ <h> Test of ColorMatch w/ Charts in Resolve v11 <p> I just wanted to share some tests I 've been doing with the ColorMatch ability in ResV11 . Camera : GH3 , Charts XRite Colorchecker ( Macbeth ) and DSC One-Shot . <p> You can watch the video , but I shot them both in the same place/time and the two charts gave me two rather different looks . Also , the way your chart is angled can have a big effect on the results , as you 'll see . <p> I 'd be curious as to the opinions on this ( including from any BMD lurkers ; - ) . <p> Very interesting results . This is clearly a work in progress , and once again we learn that a one-button solution is not necessarily ideal . <p> I 'm still much happier with just a big grayscale chart and notes from the DP , but I 'm an old-school guy . Give me that and a reasonable exposure in Log and I can most likely get something usable out of it . <p> I just wanted @ @ @ @ @ @ @ @ @ @ ColorMatch ability in ResV11 . Camera : GH3 , Charts XRite Colorchecker ( Macbeth ) and DSC One-Shot . <p> You can watch the video , but I shot them both in the same place/time and the two charts gave me two rather different looks . Also , the way your chart is angled can have a big effect on the results , as you 'll see . <p> I 'd be curious as to the opinions on this ( including from any BMD lurkers ; - ) . 43971 @qwx453971 <p> Gray , Thank you for sharing . Your last note on the DSC chart visually referred to an X Rite chart . Perhaps I missed something there . I also have been testing the Resolve chart match function , and also have found on exact lighting , camera , lens configs , you can get varying results based on the position of the chart , light source to camera . My last shoot in Texas was 7 3 camera interviews with the same lighting fixtures cameras , distances and lens . I just started processing the rushes and @ @ @ @ @ @ @ @ @ @ I did all of them with DSC , but have also done some studio tests withe the X Rite , and have found variances within . As others have posted re color chart match , it might be best as a dailies tool , and not a final grade one button solution . I need to dig further . Will update as I do . <p> Good test Gray . Couple of observations , if I may . In retrospect , do you think it was a good idea to test this feature with images shot with a LOT of reddish reflection and spill from the wall ? Or may be it was a good idea to test it in a difficult situation . Who knows ... Also , say this feature does a decent job on the first shot , but the later shots , that you 're trying to match have no chart available and you need to do it manually . As you said yourself " Resolve does something " . We have no idea what exactly it does . If it does just primary offsets and @ @ @ @ @ @ @ @ @ @ , then great . But if in addition to all that Resolve tries to use something like curves to match individual colors on the chart , then you going to have a hell of a time trying to match what Resolve had done . And finally , after many years working , very seldom do I have a pleasure to use just a standard Gray chart , let alone any of the DSC or X-Rite charts . BTW , mostly working with conformed timeline I can see the possibility , where all charts will be cut off . May be this feature could be used for dailies ? Who knows ? Anyway , in my opinion , this feature looks a bit gimmicky and caters to the beginners looking for an easy way to get a basic grade ... How about instead BMD spends a little more time on improving Resolve 's basic color tools instead ? <p> Great comments . Yes , I felt that I wanted a " real world " test . I also did indoor &amp; other outdoor tests , but I liked the way the @ @ @ @ @ @ @ @ @ @ chart . That location actually is pretty neutral EXCEPT for the maroon wall behind me . The wall reflecting the majority of the fill light is grey . <p> In general , the neutral colors seem to track well , but it 's the more " aggressive " colors that seem to get moved in interesting ways . Also ( not so appearant in this test ) , there does not seem to be an associated attempt to adjust the exposure , just more an attempt to neutralize color casts . I do find it interesting that different charts give you different results . <p> As to " getting the chart in final color " : if the chart was shot at the head of a particular shot , you should be able to " use handles " option to get to it and produce your match . Then then match could be applied to all the other shots in the sequence ( as a starting point ) . Or ( and that 's a BIG " Or " ) , you can try to train editorial to clip &amp; @ @ @ @ @ @ @ @ @ @ liver them to you as an EDL for you to reference . <p> For me , there are two places where a chart really helps : A ) a scene with a weird white point or no visible true black/white point B ) a scene with a real color cast ( candle lite , red-lit bar , Jim-Cameron-Blue night ) . Here , a chart under real white light can really help keep the DPs vision as they intended . <p> Gary , Thank you for sharing . Your last note on the DSC chart visually referred to an X Rite chart . Perhaps I missed something there . 43971 @qwx453971 <p> Hey , Dennis . <p> I think what you missed was that the color I applied to the shot with the XRite , but called out as a One-Shot , was DERIVED from the One-Shot chart , but applied ( in this case ) to the XRite chart for the purposes of having all the corrections use a single , same reference frame to really emphasize the differences . <p> In fairness , the documentation in the Resolve @ @ @ @ @ @ @ @ @ @ " effect to just get the ball rolling before applying further correction . This ColorMatch seems like the next evolution of the " A " button . <h> Attached Files : <p> I found an image of two charts laying on the floor on Google images and did a quick comparison . <p> Sorry . No free lunch . 43971 @qwx453971 <p> Very cool , Dominic . Here , on my iPad , the match between the two charts seems to be fairly close with just a bit more saturation going in the DSC result , closer than my tests seemed to indicate . Still , they both left a lot of blue in the middle greys . <p> Great stuff ! I think the angles you use on the chart are not extreme , they 're pretty much what I see on set every day It also shows why , apart from maybe a system check chart at the start of the day , I 've pretty much given up on charts . They can be useful for dailies but for a real grade they wo n't be there @ @ @ @ @ @ @ @ @ @ the black chip on the DSC chart is a major reason for me only using it in very controlled conditions . I 've asked them to do a version with a matte chip , I realise that it wo n't give as good a black but it would at least be consistent ! They do n't agree : - ( <p> since I did n't take the original image , I just left it at 709 . SRGB yielded dark results . The DSC option got closer . You 're right about the sat differences . I just bumped the sat up on the xrite to match the level of the dsc version . The most noticeable shift is in the blues . <p> Moderator note : We 're combining all Resolve color chart threads into one as they are too similar to be maintained separately . <p> Hey LGG , <p> I 'm trying to put together a breakdown of #Resolve11 color chart feature , and I need a few test clips shot with a compatible color chart . I 'm looking for short clips shot with one of @ @ @ @ @ @ @ @ @ @ original r3d ) , ALexa in LogC , BMCC , BMPCC , BMP4k , Sony S-Log , and Canon C-Log . <p> I still maintain an automatic button can not replace a good dailies operator , and you need somebody in that chair making good , informed decisions . 43971 @qwx453971 <p> Thanks Marc , I do agree w you . Saw that video earlier . I just wanted to do further testing to see if it could be used to establish a decent starting place for a grade , especially when working w log material , and see how if it helps w matching . Figured I 'd line up a few different cameras and do a breakdown of the feature . Ca n't hurt but show that the feature is either slightly useful or as useless as the auto color button ( imo ) . <p> Good test Gray . I think it is still a work in progress from BMD . During my ICA classes this one feature probably gets the biggest ' OOOHHH ' from the students . People are always looking for a simple solution @ @ @ @ @ @ @ @ @ @ for the less experienced colorist , do you leave the Colormatch settings as ' God ' or do you get in there and tweek ? A little like using LUTs on set . DP says just use the LUT and do n't grade ..... but it looks slightly wrong what do you do ? <p> The reflectivity of the black chip on the DSC chart is a major reason for me only using it in very controlled conditions . I 've asked them to do a version with a matte chip , I realise that it wo n't give as good a black but it would at least be consistent ! 43971 @qwx453971 <p> I have been known to go in and glue down a strip of black felt as reference -- just like the old video camera charts we used to use in the 1970s . <p> Personally I think , if Resolve had color pickers for black and white points it could be much more useful feature ... 43971 @qwx453971 <p> that tool in DS is one of the reasons can blast though a first pass of a @ @ @ @ @ @ @ @ @ @ matching anything to anything inside regions and inside qualifiers .. not that i can not do it by hand / eye / scope , more that i can put an eye dropper on a wall in the bg of a dirty OS and match it to the wall on the opposite reverse angle instantly , and move on to the next scene <p> Or use it to build offsets for lenses , or or or . <p> At least Nucoda listens i am hopeful that something like this is on the roadmap for those guys , it 's fantastic tool if it 's the right tool at all 
@@44332877 @4332877/ <h> GA-Z87X-UD5 TH - Success ... Sort of . <p> So I shelled out some cash to get a bad ass PC once the new Gigabyte thunderbolt board came out , and I 've got it working ! ( so far ) . I 'm going to be trying to put this system through it 's paces on an indie job this week , ( I hope ) . <p> I 'll be installing some older drives and Raiding them 0 ( with a back-up of course ) as my media drives for this testing phase . <p> I have n't gotten the Thunderbolt ports to work , but that might be because I 'm on Mavericks ... Otherwise if anyone out there knows what they 're doing with hackintoshes , I 'd love to chat ! Please send me an email , as it feels pretty lonely out here ... <p> I 'll be dropping a deckling 3d and a older GTX 285 as a GUI card for this process , as well ... Wish me luck ! <p> How have you configured your pci-e slots ? The @ @ @ @ @ @ @ @ @ @ The Deckling is happy with four , but did you give the Cuda card or the GUI card the benefit of 8 lanes ? Also i get the impression that thunderbolt has to share its bandwidth with those three pci-e slots , does n't it ? Would be interesting to know how this board distributes it 's bandwidth exactly . <p> So I shelled out some cash to get a bad ass PC once the new Gigabyte thunderbolt board came out , and I 've got it working ! ( so far ) . I 'm going to be trying to put this system through it 's paces on an indie job this week , ( I hope ) . <p> I 'll be installing some older drives and Raiding them 0 ( with a back-up of course ) as my media drives for this testing phase . <p> I have n't gotten the Thunderbolt ports to work , but that might be because I 'm on Mavericks ... Otherwise if anyone out there knows what they 're doing with hackintoshes , I 'd love to chat ! Please send @ @ @ @ @ @ @ @ @ @ here ... <p> I 'll be dropping a deckling 3d and a older GTX 285 as a GUI card for this process , as well ... Wish me luck ! 43971 @qwx453971 <p> Hey Joseph just wondering what your bios options are for that board as am recently fitting on of them up and ca n't get anything to load . Just wondering if there was anything special that I needed to do in the bios ? <p> James , I really do n't remember if I changed anything in the bios ( other than over clocking ) ... But I think I only played with things to try to get the thunderbolt going . <p> I 've got a GTX 780 in slot one as my Resolve card &amp; a GTX 570 as a GUI card in slot 2 . Blackmagic Decklink extreme 3d in slot 3 . I have 3 3tb 7200rpm drives as an an internal RAID 0 as a media drive that backs up nightly . <p> I upgraded to 10.9.2 , and experienced a bunch of ( what I think ) are Kernel Panics , @ @ @ @ @ @ @ @ @ @ I 'd get the gray screen of death ) . <p> So I managed to stabilize the system ... by turning off two pieces of software : <p> Paragon NTSF for OSX &amp; Avid Background Services . <p> Once I did that ... Very solid ... <p> I also did some over-clocking &amp; installed basic liquid cooling for the CPU , which really helped out with RED performance . Getting 14fps or so fps on Epic footage . That was far easier than I imagined it to be . <p> Still have n't bothered to install Windows on a separate drive to test Thunderbolt , which is what I hear you have to do to get it to work , I just have n't had the time . <p> Overall , I think it was a very good investment for 2000$ , and will help tide me over in between my 2010 mac pro &amp; buying whatever my next computer will be . 
@@44332878 @4332878/ <h> Hackintosh - $3800 mac pro for $1100 <p> About a month ago my main PC died . It was only the power supply which was an easy $60 to fix but I sort of wanted an excuse to build a completely new machine with some of the latest cutting edge power available from Intel and Nvidia . I decided this might be the time to try building the Hackintosh so before I bought any parts I used the website www.tonymacx86.com to make sure I was going to buy parts that were said to be compatible . <p> After a week of reading what really seems like very cryptic instructions to get this working , I spent a weekend working on the build and I was successful in getting OSX 10.11 ( El Capitan ) running on my PC clone machine . I still have a few things I need to do like verify the speed of my USB 3.0 ports and install a wireless adapter in the machine and probably bluetooth . Overall I 'm happy to report that everything ( sound , network , graphics , cuda @ @ @ @ @ @ @ @ @ @ smooth . <p> So far I 'm running DaVinci Resolve 12 and FCP X on it without any problems and the speed is absolutely blazing . Partly because of the new hardware obviously , but it 's running as a " mac " and this is what has me most excited because the equivalent machine on spec is maybe $3800 USD from Apple but this machine was built for around $1100 USD . Below is my system and specs : <p> Caution about building : Building a Hackintosh and getting it working is not at all easy to do and you will likely encounter problem after problem that you need a lot of patience for in order to resolve before getting to the next step . Sometimes instructions you read online on what you need to do are cryptic and not easy to understand . The steps I wrote will link to specific guides , or portions to other guides that I had to use in order to get my Hackintosh up and running properly but one thing to point out is that you ca n't follow my steps exactly @ @ @ @ @ @ @ @ @ @ as you pick say a different motherboard or graphics card , the steps will change and you need to heavily research it or troubleshoot as you go along . Differences in hardware or combination of hardware will require different configurations , bios settings , and edits to the mac plists ... and more . Some people get really lucky with their install but for others it can be extremely frustrating and difficult to get through so let this be a warning incase you decide you want to try this for yourself and thought it would be a breeze . <p> One tip I can suggest is that if you want to try building your own Hackintosh is to read the threads on Tonymacx86 marked SUCCESS to find other people who have successfully got it running on specific hardware components you might be considering to buy . There is also several GUIDES section where others with successful builds have outlined steps for very specific hardware components - so again look for these in advance of buying any parts . It will make your life much easier ! <p> The processor , @ @ @ @ @ @ @ @ @ @ make any difference when it comes to doing an install . It 95% has to do with the motherboard and graphics card and problems when the mac OSX is being installed and tripping up over drivers for specific hardware , so these are things to look out for . Differences between motherboards will be chipset , bios , bios version , EUFI compatibility , network controller , etc .. and it 's this that will make the difference between an easy or hard build . If you read my build guide , you 'll see that I did n't bother putting in the graphics card until after I got everything running ok . The reason is that the install can easily trip up over Nvidia cards so it 's better to leave that for the end if you can . <p> How have your OSX upgrades been with your build ? Or do you do new install/build from scratch for each OSX upgrade ? 43971 @qwx453971 <p> I do a clean install every 6 or 9 months , and never upgrade in-between . I stay a few versions behind , @ @ @ @ @ @ @ @ @ @ keep the old boot disk for a few months in case I discover an issue later down the track . It 's a bit of extra work , but it runs absolutely perfectly for me . <p> Yes from what I gather , clean installs are best . I originally tried to install Mountain Lion on this machine but I was just running into too many issues with just the installation , so I tried El Capitan and the install went lots smoother . <p> Have the Z97x with the 2 built TB ports . Could n't get thunderbolt up on the Mac side . With windows install worked , just not with hackintosh. 43971 @qwx453971 <p> Saw that lots of people with that board have it working by first getting it enabled on windows and disabling some flag in bios . Try and contact some of them on the net . Should work ( no hotplug ) <p> Those TB enabled boards are rarer then gold on the internet . I finaly found one yesterday ga-z97x-ud7-th after hours and hours of searching and bought it as seems about the @ @ @ @ @ @ @ @ @ @ support for the TB apple monitor ( my gui mon ) on a hackingosh . Too bussy to start building now but at least i have it for in a month or so . Planning to go for an i7-4790k as well with a gtx980-ti to start with . I have given up hope on apple coming up with something usefull . The newest slylake boards have TB3 and is not supported by apple so also not on hackingtosh ( so far ) . So the general consensus is that for new TB2 hackintoshes you are out of luck <p> I have Thunderbolt on my Asus z87 Expert motherboard ( Win 10 64bit ) which is about 2 years old now so it 's TB1 @ 10Gbps . TB 1 is still good for external RAID drives and running the occasional extra GUI monitor too off the small Intel GPU as it is compatible with mini display port . This leaves the main GPU free for processing . <p> I got Thunderbolt II working on an Asus X99 Deluxe , on both Win 10 and El Capitan . External drives @ @ @ @ @ @ @ @ @ @ , but I was n't able to make my mid 2011 iMac work as a Thunderbolt display . By no means the end of the world , but still irritating nonetheless . <p> I have Thunderbolt on my Asus z87 Expert motherboard ( Win 10 64bit ) which is about 2 years old now so it 's TB1 @ 10Gbps . TB 1 is still good for external RAID drives and running the occasional extra GUI monitor too off the small Intel GPU as it is compatible with mini display port . This leaves the main GPU free for processing . 43971 @qwx453971 <p> Most def . I have so far all running on TB1 ( raids/TB monitor/BM minidisplay/ etc ) so will be happy if that ( TB1 on hackintosh ) works . <p> I got Thunderbolt II working on an Asus X99 Deluxe , on both Win 10 and El Capitan . External drives , Thunderbolt dock , and Blackmagic IO boxes all work , but I was n't able to make my mid 2011 iMac work as a Thunderbolt display . By no means the end of @ @ @ @ @ @ @ @ @ @ <p> Good to hear ! ! ! Yeah , the only solutions i heard working where Apple TB monitor working but not an iMac as TB monitor . I do n't  even think an iMac works as TB monitor on non hackintosh macs ( but could be wrong ) . Also i could theoreticaly live with the TB monitor not working as long as my other TB stuff is . But of course would be nice and will try for sure . <p> Yeah , the only solutions i heard working where Apple TB monitor working but not an iMac as TB monitor . I do n't  even think an iMac works as TB monitor on non hackigtosh macs ( but could be wrong ) ? 43971 @qwx453971 <p> ? <p> The mid 2011 27 " iMac can work as a TB monitor ( I got it working straight away using a Mac Mini as source ) , but no matter what I tried I could n't get the Asus displayed . There was mutual recognition of each other as hubs , but that was it . I spent by @ @ @ @ @ @ @ @ @ @ to get this one issue resolved ( and it was n't even a practical necessity ) , but eventually had to give up . I just hate not having a choice when in theory I should have a choice ? <p> Yeah i imagine your pain . I hate that too . Not of the giving up kind . Will keep that in mind when i get to it . Maybe the diff board could make the difference , who knows . For a rainy day ; - ) p.s . Sure you played with that but the TB monitor ( so i recon the iMac too ) is realy particular about resolutions/frequencies etc . I use SwitchrezX and got myself into black monitors more then once when playing . So maybe the handshake was good but he just did not like the freq/rez etc . Anyway forget what i said . Do n't want to talk you into loosing more time on that 
@@44332880 @4332880/ <h> DCP at 30fps out of spec : possible ? <p> We are restoring a movie that was original shot at 30fps ( 65mm ) . now we are down to the idea to release not only a new Blue Ray disk and UHD content , but for special screening the client might want a 2k/4k DCP that need to be coded and played at 30fps . <p> Is this even possible/acceptable ? How people deal with material generated at 30fps if you want to see them in a theater ? <p> Yes , SMPTE spec DCPs allows this . Assuming you have a series 2 projector and IMB that support SMPTE dcps ( they all should ) then it will work without issue . The projector just changes it 's refresh rate . This is how the hobbit could exhibit in 48fps , for example . <p> That said , I generally strongly recommend to client that they convert to 24fps for DCP , as there can be compatibility issues with older or not up-to-date systems . Better safe than sorry when it comes to limited exhibition ... @ @ @ @ @ @ @ @ @ @ used to exhibit . <p> Yes , SMPTE spec DCPs allows this . Assuming you have a series 2 projector and IMB that support SMPTE dcps ( they all should ) then it will work without issue . The projector just changes it 's refresh rate . This is how the hobbit could exhibit in 48fps , for example . <p> That said , I generally strongly recommend to client that they convert to 24fps for DCP , as there can be compatibility issues with older or not up-to-date systems . Better safe than sorry when it comes to limited exhibition ... where you ca n't be sure about the equipment being used to exhibit . 43971 @qwx453971 <p> Thanks Juan , I know that it " could " work , but I also re-read the current DCI recommendation and NO , there is no 30fps mentioned .. only 24 and 60 ( proposed ) . <p> We can not " convert to 24 " : it is a 1955 classic movie that was shoot at 30fps in 65/70 mm ...... that is not an option . And I think @ @ @ @ @ @ @ @ @ @ limited screening that can be controlled and verified . <p> We are restoring a movie that was original shot at 30fps ( 65mm ) . now we are down to the idea to release not only a new Blue Ray disk and UHD content , but for special screening the client might want a 2k/4k DCP that need to be coded and played at 30fps. 43971 @qwx453971 <p> I know that Blu-ray can certainly do 30fps , but I du n no about a DCP . If the projector is setup to allow the DCP to change framerates on the fly , then it should work , but I 've seen projectors become " confused " in some situations where the wrong flags were set in DCPs , creating havoc . My suspicion is that somebody would have to be in the booth to give the projector a kick if it hiccuped . <p> BTW , I did all the color-correction on a couple of 65mm 30fps Todd-AO jobs that I think Fotokem did the scanning on in the 1990s , including Oklahoma . I think the lenses 60 years @ @ @ @ @ @ @ @ @ @ sure if the 30fps really made a visual improvement . It was different , but not necessarily better . I believe SMPTE had a demo of Oklahoma and Around the World in 80 Days in 30fps , but the elements on the latter are scattered and incomplete , as I recall . <p> I know that Blu-ray can certainly do 30fps , but I du n no about a DCP . If the projector is setup to allow the DCP to change framerates on the fly , then it should work , but I 've seen projectors become " confused " in some situations where the wrong flags were set in DCPs , creating havoc . My suspicion is that somebody would have to be in the booth to give the projector a kick if it hiccuped . <p> BTW , I did all the color-correction on a couple of 65mm 30fps Todd-AO jobs that I think Fotokem did the scanning on in the 1990s , including Oklahoma . I think the lenses 60 years ago were so soft and wonky , I 'm not sure if the 30fps really @ @ @ @ @ @ @ @ @ @ not necessarily better . I believe SMPTE had a demo of Oklahoma and Around the World in 80 Days in 30fps , but the elements on the latter are scattered and incomplete , as I recall . 43971 @qwx453971 <p> " Oklahoma ! " is the one we are doing now in 4K UHD , I wish you can see how beautiful it is ! ! ! 
@@44332881 @4332881/ <p> I was attempting to note that the Quadro is double-wide and covers Slots 2 &amp; 3 , however I changed Slot 3 to Empty in the original post to avoid confusion . Flame still only uses a single GPU but , yes , I did upgrade to a Quadro 6000 late last year , and it was a great improvement . You can especially feel it in Lustre when you have a number of secondaries with keys and blurs . The Quadro 5800 would start to slow down with around 4-5 of them , but with the Quadro 6000 you can do a lot more . <p> Well , concerning Floating license , I can have it , I just talk to my reseller . Anyway from his point of view , you ca n't really build your own station as you need the validated hardware , which is confusing to my hear . Now it 's HP820 and K6000 and that 's all . <p> Anyway from his point of view , you ca n't really build your own station as you need the validated hardware , @ @ @ @ @ @ @ @ @ @ HP820 and K6000 and that 's all . 43971 @qwx453971 <p> You will be using validated hardware . You 'll just be sourcing it yourself for a more reasonable cost . At this point a Z820 is a good idea , and while the K6000 is very nice card , the Quadro 6000 works just fine on 2014 if you need to save some money . <p> You will be using validated hardware . You 'll just be sourcing it yourself for a more reasonable cost . At this point a Z820 is a good idea , and while the K6000 is very nice card , the Quadro 6000 works just fine on 2014 if you need to save some money . 43971 @qwx453971 <p> Thank Jason for the tip ! <p> And you 're 200% right concerning saving money : <p> - actually , the SDI outpout board I have ( quadro 5600 ) is compatible with quadro 6000 ( not K series ) : 5000 $ saved . I 'll see later for quadro K6000 , the step between 5600 and 6000 is already huge . - I @ @ @ @ @ @ @ @ @ @ 'm sure it 's still the same card actually ( it looks the same and functionality 's are identical ) : 2500 $ saved - Save on Z820 even with big cpu config ( about 3000$ ) <p> Yep , globally , it could be a big save to say <p> I have two drive arrays ( an old blue autodesk one ( 4 to ) and a la Cie Fiber orange ( 24 to ) ) . The blue is out of order cause it 's quiet impossible to find spares in France . I think it will be possible to change all the drives for some brand new SAS ones and make a very fast array with a decent capacity . <p> If I succeed , it 's indeed a good operation to go on on Premium 2014 ( and 2015 at NAB ) in those conditions ... <p> Lionel , can u confirm ACTUAL run/use of SDI daughter board from the FX5600 on a newer Q6000 ? from what i rememebr researching this matter back to 2012 , that old SDI that comes with fx5600 has different @ @ @ @ @ @ @ @ @ @ requared to match . never went that road tho. pls confirm it fits and runs , thx. same for the Aja 2K vs 3G , besides the KDU modification to actually make flame 2014 install it ( what could be done relatively easy ) can u pls confirm it actually functions as intended ? thx . <p> The SDI board for the 5500 is the 180-10209-0000-A01 or 180-10209-0000-A02 : <p> The board for the Quadro K6000 is the VCQK6000SDI-PB and looks just like the VCQFXSDIOPT2-PB , but uses a different ribbon connector . An SDI board is nice to have , but is only required if you need real time deliverables . Otherwise the AJA Xena 2Ke OEM is probably a more flexible option for monitoring . Since you have that , you may not even want to install the SDI card , but it 's valuable in certain situations , so I would keep it just in case . <p> The LaCie 12big you have is a re-branded Xyratex fibre array which is what Autodesk has specified with Flame for several years . It probably has a 3Gb/SATAII backplane @ @ @ @ @ @ @ @ @ @ continue using it . I would upgrade the drives to newer 10K SAS if you 're doing really heavy compositing with a lot of layers ( more speed , less space ) , or go for 3-4TB 7.2K drives if you 're mainly doing online/ grading ( more space , less speed ) . Later on , I would grab an LSI RAID controller and a 6Gb SAS chassis from a company like CI Design and install your drives in that , which will bump you up to around 1500 MB/s . The LSI card is supported directly in Red Hat so you wo n't even need to worry about installing Linux drivers . <p> Depending on your budget , you can get Quadro 6000s all day long on eBay for US$1000-1200 . The Quadro K6000 is very nice , and was initially pretty expensive , but I am seeing them online for around US$3800 now . <p> As far as your workstation , the Z820 would be a good idea , especially if you can get one of the liquid cooled models ( not much more ) . I @ @ @ @ @ @ @ @ @ @ more from a higher clock than they do from more CPU cores . An 8-Core over 3.0 GHz is a good starting point , with a 12-core being important if you 're working with RED and need to de-bayer a lot of R3Ds . <p> Overall it sounds like you have a lot of what you need already . If you can get a Z820 with at least a Quadro 6000 , you should notice a really nice improvement in performance . It will be a great set-up that should n't cost you a tremendous amount of money . <p> It basically allows you to master out to a deck or recorder without rendering . The AJA implementation in Flame/ Lustre is like most other apps . Its real time , but a couple frames behind your GUI during playback . <p> You can totally go either way , the AJA will just give you more I/O options , and the breakout box makes a nice patch bay . Also , if you need 4K monitoring , you 'd have to get a Kona 3G . But , in general @ @ @ @ @ @ @ @ @ @ choice depending on what you need . <p> The board for the Quadro K6000 is the VCQK6000SDI-PB and looks just like the VCQFXSDIOPT2-PB , but uses a different ribbon connector . An SDI board is nice to have , but is only required if you need real time deliverables . Otherwise the AJA Xena 2Ke OEM is probably a more flexible option for monitoring . Since you have that , you may not even want to install the SDI card , but it 's valuable in certain situations , so I would keep it just in case . 43971 @qwx453971 <p> As I have the SDI output board with my actual quadro 5600 , the quadro 6000 is a good deal as I use real time deliveries . K6000 SDI is expensive , not to talk about the card itself . Will see that in the future when prices will go down a little . <p> As for AJA , i have to take a look . The card generation has probably changed since 20009 , but I have no need for higher model , the one I use is @ @ @ @ @ @ @ @ @ @ with 820 and Flame 2014 . <p> The LaCie 12big you have is a re-branded Xyratex fibre array which is what Autodesk has specified with Flame for several years . It probably has a 3Gb/SATAII backplane , but that 's OK if you would like to continue using it . I would upgrade the drives to newer 10K SAS if you 're doing really heavy compositing with a lot of layers ( more speed , less space ) , or go for 3-4TB 7.2K drives if you 're mainly doing online/ grading ( more space , less speed ) . Later on , I would grab an LSI RAID controller and a 6Gb SAS chassis from a company like CI Design and install your drives in that , which will bump you up to around 1500 MB/s . The LSI card is supported directly in Red Hat so you wo n't even need to worry about installing Linux drivers . 43971 @qwx453971 <p> The same here , your recommandation is nice , but I 'll see later when money will be better The blue chassis with 600 go SAS will @ @ @ @ @ @ @ @ @ @ is enough in most cases . We 'll see if my actual raid card is ok to run on HP820 anyway ... <p> As far as your workstation , the Z820 would be a good idea , especially if you can get one of the liquid cooled models ( not much more ) . I would prioritize CPU speed as Flame and Lustre both benefit more from a higher clock than they do from more CPU cores . An 8-Core over 3.0 GHz is a good starting point , with a 12-core being important if you 're working with RED and need to de-bayer a lot of R3Ds. 43971 @qwx453971 <p> I have a good deal on a 2 x 3,4 Ghz with 32 go . I think it will be a good machine for renders ... <p> Overall it sounds like you have a lot of what you need already . If you can get a Z820 with at least a Quadro 6000 , you should notice a really nice improvement in performance . It will be a great set-up that should n't cost you a tremendous amount of @ @ @ @ @ @ @ @ @ @ . You opened us new perspectives with your great advices ! ! But I 'm afraid to post when everything will come , a fresh install is always a pain 
@@44332882 @4332882/ <h> New Features in DaVinci Resolve 14 Now Available <p> My " New Features in Resolve 14 " video training title from Ripple Training shipped last week , consisting of a seven hour tour through nearly every new feature of the app , large and small . There 's a thorough examination of great new features for editors and finishing artists , a view of all the new Color page features , and detailed explanations of each of the new ResolveFX from a colorist and finishing editor 's perspective . Additionally , there 's a close look at what 's available in the new Fairlight page so far , and a demo of the updated collaborative workflow 's bin locking for editors and clip locking for colorists . <p> This was a big one to create , but there 's a ton of good information to get you started with while Resolve is still in public beta . And as with all the Ripple titles , it 's thoroughly indexed so you can jump straight to the topics that most interest you . 
@@44332883 @4332883/ <h> Using more than 12 Gb RAM in Resolve 12.5 <p> I 've just put together a new system - as it currently stands : dual 8 core Xeons , 128 GB of memory , GTX 980 Ti . I 'm running Windows 10 . <p> In the preferences panel of Resolve , there 's an option to set " pre-allocated memory " for Resolve to use . The manual advises you not to set this above 4 GB , unless you have 64+ GB of memory available : <p> Most users do n't have to worry about this option , and should probably ignore it . To understand when this might be useful , DaVinci Resolve uses RAM to store images as they 're being processed . By default , Resolve only pre-allocates 50% of the available RAM of your workstation for this purpose , which typically comes to the 4 GB listed in this eld . Workstations with enormous amounts of RAM available ( 64 to 128 GB , for example ) , may nd it desirable to raise this cap in order to improve performance when @ @ @ @ @ @ @ @ @ @ although the maximum recommended pre-allocation is 12 GB of RAM . 43971 @qwx453971 <p> If you try to raise this above 12 , it resets it to 12 . Thinking that maybe this is just an initial allocation and that Resolve can still use more , I watched the system monitor while playing back some 6K Red files at Full Res Premium , my CPU 's were close to 100% , and my RAM did n't go above 12 GB use . Perhaps I need to push it further to see it start to use more than 12 , but it seemed to me that more should be getting used at this point . <p> Tech support tells me that this is the way it is , and that Resolve does n't ever need more than 12 GB ... I find that hard to believe ... With 16 cpu cores running , that 's less than 1 GB per cpu core . <p> Does anyone have any advice/input or a solution to this ? Based on previous experience with Blackmagic tech support and my own knowledge , I 'm having @ @ @ @ @ @ @ @ @ @ My son set this up for me recently so I ca n't give any exact details , also I 'm not a resolve expert . But how I understand it , is he created a ram disk for specific caches and resolve has a button to refresh those cahces . I also noticed that it is saves the ramdisk cache to disk on shutdowns - so it was n't going away on power up/down . Also windows 10 will ram disk cache generally in background mode for the entire file system and other cache areas which I did not specify exactly , but it is a bit hit or miss having it do it automatically . In AE btw I noticed i needed 2 gb per core for red raw 6k , so like you said the 12 gb limit does n't make a lot of sense . I btw have two 10 core intel cpus , 256 gig ram , m6000 on windows 10 . Perhaps a resolve and windows 10 expert could give more insight on this . <p> The pre allocation setting is for system RAM that @ @ @ @ @ @ @ @ @ @ have set it to a 12GB limit . This does n't relate to actual RAM use for the overall app so as Demont mentioned its quite possible to use much more overall . 
@@44332885 @4332885/ <h> Interesting concerns about DI from 2007 <p> while researching a bit about film stocks used by Harris Savides for " American Gangster " I came across this discussion from 2007 . I remember the time in 2007 when my colleagues at Bavaria Film labs first laughed about the first digital shot project , then slowly paniced and just a few years later had to learn new jobs in other industries when the lab closed . <p> Of course this is only the tip of a very big iceberg . The pace of technology is progressing at a staggering rate , and there 's very few jobs/professions that will not be impacted by it 's advance . <p> It 's hard to try and imagine what our industry will look like in twenty years ' time , but there 's certain emerging technologies that I 've seen that seem like definite game-changers . For example ... <p> Lytro Cinema ( Lightfield cameras ) will redefine the role of the cinematographer and those in the VFX industries . The idea is that you can shoot the ' lightfield ' in @ @ @ @ @ @ @ @ @ @ change its exposure , shutter angle , depth of field , and finally pan/track/crab around the set . I can see a further extension to this being that future lightfield cameras will perform spectrophotometric analysis on a scene , thus freeing us from having to make any decisions about the colour or lighting of anything on set , as it will be able to be subsequently changed at will . <p> AI ( artificial intelligence ) will continue to make inroads into jobs that were considered ' safe ' , due to their reliance on creativity , rather than repetition or sheer number-crunching . As this intelligence increases , our roles will become increasingly redundant as AI will be able to maximise profit for studios and production companies . From pre-production to final picture grade and sound mix , AI will drive all the decision-making . AI will likely be built without conscience or an ego to be bruised , so if it 's more cost efficient for a human to ( say ) roto and grade a number of faces in a scene , then you have a job @ @ @ @ @ @ @ @ @ @ 'll be relegated to blowing out the dust from the mainframe ! <p> David Mullen is a sharp guy who knows his stuff very well . He recently did an all-film indie project last year , The Love Witch , and it got some stellar reviews for its look . But he 's also done some beautiful digital projects , including the NBC TV series Smash . He does good work regardless of the medium , because for David it 's lighting first over whatever it 's being shot on . <p> Mullen is right in that there are aspects of film production that are being lost due to ignorance and economy . This has affected a lot of aspects of the business , particularly budgets , post , VFX , and color , not just the role of the DP. 
@@44332886 @4332886/ <p> Due to Resolve 12 's completely revamped User Interface and early feedback that following the Resolve 11 training in Resolve 12 was a frustrating experience , Mixing Light decided to *completely* re-record our 14+ hour training title . AND , we decided to wait until the official Resolve 12.0 software was available ( and not record our training on the Public Beta ) . <p> This time , we 've split it into 2 courses ... Resolve 12 Insights ( the fundamentals ) and Resolve 12 Deep Insights ( advanced concepts and Grade-Along ) . Each title has a full set of Resolve 12 project files and licensed footage from a professional documentary shoot , for following along . <p> Since these packages are so big ( they total over 20+ hours across 200 movies ) we 've included Flight Paths , that are playlists you can import into any iTunes-compatible media player for much quicker walkthroughs . The Playlists include : <p> You wo n't regret it . I did Patricks flight school ( highly recommended , which also included resolve 11 course as a base starting point @ @ @ @ @ @ @ @ @ @ . His style is rather unique in not just explaining buttons , menus and clicks , but methods , workflow and why you do certain things . A fresh breath in a world where 99% of the courses just teach you the program . It completely changed the way i approach a grade . <p> Just downloaded it , an easy sign up ... download was perfect . Wow this is a lot of material , really well organized . I like how it goes through every little detail , especially all the changes ( many which are n't so obvious ) that have occured since I last used resolve . <p> Pat does good work , and the Insights that his site provides occasionally give me a kick in the ass in thinking about how to solve problems in a vastly different way from anything I could come up with on my own . They 're very useful . <p> I missed out on the newsletter deal , but thank you for the LGG discount ! 43971 @qwx453971 <p> Honestly , my pleasure . I wish I could spend @ @ @ @ @ @ @ @ @ @ after my heart " but with all the Mixing Light / Tao of Color stuff I run out of the emotional energy to participate much here . <p> Honestly , my pleasure . I wish I could spend more time on LGG " it 's a forum chasing after my heart " but with all the Mixing Light / Tao of Color stuff I run out of the emotional energy to participate much here . 43971 @qwx453971 <p> We have special drugs in LA designed just to give you all the emotional energy you need . Trust me on this . <p> Does this replace the Colorist Flight School ? The website for the CFS still refers to v10 and v11 , so I 've been holding off on buying until it was updated for v12 . <p> Should I hold out for the Colorist Flight School to be updated for v12 ? Or given that Deep Insights appears to be embedded in the product , is the Deep Insights portion of the CFS automatically upgraded to the v12 version ? <p> I started the flight school 2 months or @ @ @ @ @ @ @ @ @ @ did it in DR12 without a problem . The at that time included base coursematerial , which you have to work through before starting the gradealong/main flightschool part , was DR 11 deep insights and was sufficient . If you are a 100% beginner wait untill Patrick updates the included material to DR12 . I suggest contacting Patrick directly and i am sure he can inform you on the options . <p> And to be clear , flight school is NOT the DR12 or DR11 course . The course is only the starting point for doing a full short feature gradealong with Patrick and he will evaluate a base and a final grade for you ( he actualy creates a personalised evaluation video for you ) . You get gigabytes of source footage ( red or proress whatevers suits your system ) <p> Does this replace the Colorist Flight School ? The website for the CFS still refers to v10 and v11 , so I 've been holding off on buying until it was updated for v12. 43971 @qwx453971 <p> The questions asked in this thread are common enough that @ @ @ @ @ @ @ @ @ @ in LGG ... <p> In truth , I 've been meaning to pull the current version of Colorist Flight School ( CFS ) from sale , until I update it . But enough people keep buying it that I 'm leaving it until the next installment is ready . <p> CFS was recorded on Resolve 10 . If you bought today , the interface portion of the training currently includes Mixing Light 's Resolve 11 interface training ... and it is relatively easy to do the Resolve 10 Grade-Along portion of the training in Resolve 11 . But I 've heard enough feedback that , unless you 're a patient person , mentally adapting the current CFS to Resolve 12 is very very difficult for many people . I do not recommend following CFS using R12 . <p> Instead , until the R12-specific training is released , I recommend doing the training in R11 ( still available as a download from Blackmagic 's website ) . Then , when you 're done , upgrade to R12 " since the foundation of R12 is R11 , you 'll only be confused @ @ @ @ @ @ @ @ @ @ That said , most members who submit their work for my feedback are working in R12 and they seem fairly adept at mentally translating what I 'm teaching in the short film ( in R10 ) to R12 . The important thing to remember about CFS is that the goal is to teach workflow . And workflow has n't changed with R12 . <p> The next question I usually get is : When is the Resolve 12 version of Colorist Flight School coming out ? The answer : About 6 weeks after the Premiere Pro version of Colorist Flight School is released ! <p> Yes ... I 'm doing a CFS Premiere Pro , to show how NLE-based pros how to attack grading ( before I do CFS R12 ) . Why ? Three reasons : <p> Colorist Flight School is being completely redesigned : One of the new things is a much more effective Scopes training ( an online version of what I did with college students for a few years ; which had them feeling naked without scopes in under three hours ) . It 's taking me @ @ @ @ @ @ @ @ @ @ a smaller Colorist Flight School based around an NLE makes the overall project seem less daunting . The Resolve-based CFS is always a gigantic project and doing a smaller NLE-based training package is a ' quicker win ' that helps feed my soul . <p> I was trained as an editor : So every now and then I enjoy going back and figuring out how to teach my old self how to do what I do now , using those NLE toolsets . It 's a kind of ' back to the basics ' for me as a colorist and a coach . <p> I 've just signed a deal with Focal Press for a color correction book : All my new training has to align with the book , so again , a smaller training package is practice for the workflow I 'm formalizing in the book . <p> Hey man , great . HAd not counted on it before the hollidays and not bothered you anymore as you are doing together with the MixingLght team such amazing work , that i guessed you had more important things on @ @ @ @ @ @ @ @ @ @ on ; - ) <p> That said , most members who submit their work for my feedback are working in R12 and they seem fairly adept at mentally translating what I 'm teaching in the short film ( in R10 ) to R12 . The important thing to remember about CFS is that the goal is to teach workflow . And workflow has n't changed with R12 . <p> . 43971 @qwx453971 <p> I can confirm guys . The CFS completely changed the way i approach grading and literally changed my setup . Now working with external scopes ( scopebox running on mac mini server fed via SDI into black magic mini recorder ) and could not live without it anymore for a second . I do feel naked when the scopes are off . Very wierd . That only after using them to this extent for 2 months . <p> And do n't listen to Patrick ; - ) . Go buy it even if its filmed on older release ( unless you are a complete and absolute beginner ) . I learned even more because of these slight @ @ @ @ @ @ @ @ @ @ bit harder to get it working and then you burn it in. ( i even nagged him for a copy of his even older DR9 based CFS to attempt to do it in DR12 . Will keep poking him ; - ) One example was the conform phase that just did not want to work for me . In the end i used all conform workflows thinkable , even a full manual shot by shot match against a reference movie to get me going . Guess what ! ! Conforming is a piece of cake now . If you are serious about learning something , then these things to me are an addition rather then nuisance . But that is just me ... Things will always change . Next time you have to do something in DR25beta or even another piece of software . Will be different that your current version , but workflows , thought patterns etc will remain the same/similar . <p> p.s . But to be fair to Patrick , if you decide to take that plunge , than be easy on him with questions stemming from @ @ @ @ @ @ @ @ @ @ ) 
@@44332887 @4332887/ <h> Mac Pro 5,1 - Internal or External eSATA RAID ? <p> Just wanting to pick some of your wonderful brains further on the best way to set up storage with my new ( to me ) Mac Pro 5,1 <p> Currently I 've got a 250GB SSD boot drive in one of the optical bays , and four big ( and mostly full ) individual SATA drives in the internal bays ( just storing a bunch of old projects ) ; I also have two 1.92TB Samsung PM863 Enterprise SSDs on the way , that will live on a Sonnet Tempo Pro SSD PCIe Card in RAID 0 for a 4TB scratch drive ( to handle cacheing duties and hold the raw files of whatever current project I 'm working on ) . <p> My basic strategy is to have my data tiered , so that anything reasonably recent , can be accessed reasonably quickly from a RAID 5 . What that means is that I 'll have the following basic system : <p> - Backup Storage ( copies of everything on individual harddrives , stored away - @ @ @ @ @ @ @ @ @ @ back things up that way ) <p> - Near Storage ( a moderately fast RAID 5 setup that 's big enough to hold a bunch of recent projects , so that they can be brought online to the scratch disk quickly , while still offering some basic redundancy ) <p> - Scratch Drive ( a 1000MB/s SSD RAID for cache files and the project files of the specific project I 'm working on at that moment ) <p> - Boot Drive ( an SSD boot drive with nothing but the OS and Applications on it ) <p> Most of that is simple enough , the one area I 'd like some advice on is the Near Storage ' component of it . I 'm wondering whether I 'd do better to put four big 6TB drives in the internal bays of the Mac Pro , and have them in a RAID 5 setup using Softraid ? Or whether I should get an external 4-bay enclosure , and setup them up as a RAID 5 in that , and connect the external RAID via the eSATA ports on the Sonnet Tempo @ @ @ @ @ @ @ @ @ @ would be a speed penalty to having the RAID 5 in the internal drive bays ( which are only SATAII ) compared to an external unit using eSATA ( that 's running through a PCIe port ) ? And would a four-drive RAID 5 even be able to max out the speed potential of SATAII ? ( if not , I 'd prefer to have the near ' storage , in the actual computer case itself ) . <p> Also , all four of my PCIe ports are filled now ( Titan X , Decklink 4k , Sonnet USB3 , Sonnet Tempo ) , and I do n't want to the added complexity of adding a PCIe expansion case at this stage just to add a hardware RAID controller . <p> I agree with Robert , eSATA is too limiting and frankly finicky for modern color work , especially since SAS controllers and enclosures ( using SATA drives ) can be found very cheaply on eBay . Grab an ATTO R680 ( or Highpoint if there 's no other option ) , a SAS enclosure , and go to town @ @ @ @ @ @ @ @ @ @ accessories to make the most of the space in your Mac Pro ... <p> Thanks guys , but I 'm a cinematographer by trade , so apart from the odd unsolicited post-job , I 'm primarily just grading projects that I 've shot myself - which means that I 'm rarely dealing with more than one or two projects simultaneously , and my data/project-management requirements are rather less intense than a full-time colourist 's ( apart from when I have 1.5-2.5 hour narrative film timelines to deal with ) . <p> Which is basically a long-winded way of saying I 've spent enough bloody dosh on this 7-year-old ( ! ) computer , and at this point in time , a PCIe expander chassis , RAID card etc. just is n't going to happen . So it 's either an internal 4-drive RAID 5 or an external eSATA enclosure with a 4-drive RAID 5 for my ' near storage ' at this stage . I 'm just wondering which of those two options is superior ( or if they 're basically the same ) ? <p> I 'll have the @ @ @ @ @ @ @ @ @ @ stuff , so the HDD RAID wo n't really be much of a working drive ( except , perhaps , for 1080p/2k ProRes projects ) , just ' near storage ' that I can bring materials online to the SSD RAID from . 
@@44332888 @4332888/ <p> It was shot on Kodak stocks . I met the filmmaker at a festival , she is trusty an auteur . She did all the costume and set work herself . Not certain , but I think they did a traditional timing and not DI . Amazing film . <p> Matthew , thanks a lot . That 's truly amazing . I was pretty sure they shot digital and was blown away how close they got to the look . Was n't expecting them to go the extra mile and do it completely old school . Crazy cool . 
@@44332889 @4332889/ <p> So for Arriraw , there is an intermediate transcode before final grading ? 43971 @qwx453971 <p> Not to me . I would just transcode to ProRes 444 at the right image size , and make sure the decode settings are correct . Color correct in ProRes 444 , then render to the final delivery format . You can theoretically work directly in ProRes if your hardware ( GPUs , processors , and storage ) is heavy-duty enough to handle it . <p> Marc , I 'm confused about what you said . When you said " You can theoretically work directly in ProRes if your hardware ( GPUs , processors , and storage ) is heavy-duty enough to handle it . " did you mean RAW ? Also , if you 're going to transcode to ProRes 4444 anyway , why not just shoot C-Log to PR4444 ? <p> There is no need to transcode . Resolve works with ARRI RAW the same way it works with RED or Sony RAW . Just conform and start grading . If your storage is capable of real time playback of @ @ @ @ @ @ @ @ @ @ a nMP 12 core dual D700 with thunderbolt 2 storage can play these files back in RT ? Also , as far as the original question , and I know what Arri says , for an HD spot , with a good DP , will there be much noticeable difference shooting at PR4444 ? The Raw will still need to be transcoded for the edit anyway . <p> Working with ARRI RAW is trivial , as ARRI RAW is uncompressed , ARRI GPU debayer implementation had been around for a while and it is excellent . It 's the playback of those massive frames is the big question . Your typical TB enclosure with 6 drives will not do it . <p> nMP will handle arriraw just fine . I feel there is a significant quality difference between arriraw and prores 444 , not that prores444 is bad , just arriraw is better . Especially at higher ASA , prores444 seems to pick up more compression artifacts from the noise than arriraw . For the camera side though , you still need an Alexa XT with codex mags to shoot @ @ @ @ @ @ @ @ @ @ shoot 120 at Prores 422HQ . <p> Just a note the Alexa XT will be quite a bit more per day than a older Alexa with dual SxS cards and Codex mags rent for quite a bit as does a fast codex reader solution , the USB3 reader is less expensive but can be a bit painful reading 512G Codex mags . I have no trouble with playback of Arriraw on my Resolve system with 8X 4Tb drives Raid5 even 120fps . <p> Marc , I 'm confused about what you said . When you said " You can theoretically work directly in ProRes if your hardware ( GPUs , processors , and storage ) is heavy-duty enough to handle it . " did you mean RAW ? Also , if you 're going to transcode to ProRes 4444 anyway , why not just shoot C-Log to PR4444 ? 43971 @qwx453971 <p> Sorry , meant Arriraw . <p> One could make the argument that if you shoot Arriraw and then transcode to ProRes 444 Log-C , what you have is a " working " copy of the camera footage , @ @ @ @ @ @ @ @ @ @ ( sort of like a digital negative ) . There are places that can use 2.8K Arriraw directly , but it needs a lot of horsepower to play in real time . <p> nMP will handle arriraw just fine . I feel there is a significant quality difference between arriraw and prores 444 , not that prores444 is bad , just arriraw is better . Especially at higher ASA , prores444 seems to pick up more compression artifacts from the noise than arriraw . For the camera side though , you still need an Alexa XT with codex mags to shoot prores 444 at 120 , a regular Alexa will only shoot 120 at Prores 422HQ. 43971 @qwx453971 <p> Describe the artifacts you see in ProRes 444 . I do n't see them , nor do my clients . <p> I also have not seen a significant difference between 1080p or 2K ProRes444 and Arriraw , The RAW has a bit more room to push and resolution but with allot more data to deal with . I have never seen any ProRes444 come out of a properly working Alexa with @ @ @ @ @ @ @ @ @ @ situations in which ARRIRAW provides a clear benefit over ProRes 4444 in most uses . Particularly not given the storage and io requirements . Just shoot prores , you wo n't really miss much . <p> My experience is exactly like Robert and Juan 's above . I think the real benefits of Raw boil down to digging into the exposure and color temperature and so on . The danger of working in ProRes is when somebody screws the pooch on the settings when the Raw files are rendered out in ProRes . If it 's done right , it 's easy to deal with , and ProRes 444 looks great to me and provides a fairly smooth workflow . <p> One could make the argument that if you shoot Arriraw and then transcode to ProRes 444 Log-C , what you have is a " working " copy of the camera footage , and then a " master " original for archival use ( sort of like a digital negative ) . There are places that can use 2.8K Arriraw directly , but it needs a lot of horsepower to @ @ @ @ @ @ @ @ @ @ see in ProRes 444 . I do n't see them , nor do my clients . 43971 @qwx453971 <p> Clarification , nots not intense artifacts , it 's really subtle but under certain circumstances , say tungsten , underexposed , higher ASA , I 've found arriraw handles noise better , and also a slightly sharper image from the down scaling , getting a cleaner image than prores444 . Shot some side by side tests and there are differences . Prores is going to be good enough for almost all things broadcast or for the big screen . Prores is definitely going to be an easier workflow . <p> Clarification , nots not intense artifacts , it 's really subtle but under certain circumstances , say tungsten , underexposed , higher ASA , I 've found arriraw handles noise better , and also a slightly sharper image from the down scaling , getting a cleaner image than prores444 . Shot some side by side tests and there are differences . Prores is going to be good enough for almost all things broadcast or for the big screen . Prores is @ @ @ @ @ @ @ @ @ @ <p> To clarify in my original question , this is being shot at 1920x1080 so no framesize downscaling. 
@@44332890 @4332890/ <h> Prores 422 or 4444 <p> I-m about to start a new documentary , and I would like to hear some suggestions . I have a Canon C500 and a Odyssey 7Q . It will broadcasted but also screened . I will record in 2K 25p , but I-m not sure yet of what codec . Most scenes will be handheld live action where people talk , so there will be 20-40 hours recorded . I will shot these in 10 bit 422 ( either SQ or HQ ) . Stockfootage or scenes on a tripod will be shot on 12 bit 444 , or in some cases 12 bit dpx . <p> How much more tolerance will 12 bit 444 give me over 10 bit 422 in grading ? What-s the difference between 422 YCC and 422 RGB ? What 's your grading experience of working with 422 coming from a 10 bit camera ? <p> There 's not a huge degree of difference between 422 10-bit and 444 10-bit , but all things being equal , I would always choose the latter if I could . I think @ @ @ @ @ @ @ @ @ @ effect on the decisions you make . There 's not a lot out there ( like almost none ) that can do 12-bit . ProRes 444 can technically handle 12-bit . <p> I think a lot will depend on how you light it and the nature of the material , so there 's a lot of " it depends " here . I have done documentaries shot on C500s and even C300s before , and we were able to get by OK . It 's the 8-bit material that 's dodgy . <p> I assume you mean 444 RGB , RGB is always 444 because you need three full channels to transfer the three color signals . <p> YCC ( or any or the other gazillion acronyms related to it ) is using one channel for the luma ( the Y , while the Y ' is the gamma corrected version ) . The two C channels stand for resp. the Blue and Red difference signals . <p> For 4:2:2 or 4:2:0 the C channels use a lower resolution image . <p> For instance for 4K video the luma Y @ @ @ @ @ @ @ @ @ @ are only 2K . <p> I would seriously consider shooting 4K , which I believe the combination of the C500 and 7Q can do . It will help with reframing , allow you to give *real* 4K deliverables , and 4K down sampled to 2K will give you a better image than straight 2K . I 'm sure space is a consideration , but I would certainly think about going the 4K route . <p> There 's not a huge degree of difference between 422 10-bit and 444 10-bit , but all things being equal , I would always choose the latter if I could . I think the type of display you use will have a greater effect on the decisions you make . There 's not a lot out there ( like almost none ) that can do 12-bit . ProRes 444 can technically handle 12-bit. 43971 @qwx453971 <p> Yes , my question is of course based on an emotional experience between codecs ( from proper exposed pictures ) . And how big would you say the step is from 10 bit to 12 bit ? <p> YCC @ @ @ @ @ @ @ @ @ @ it ) is using one channel for the luma ( the Y , while the Y ' is the gamma corrected version ) . The two C channels stand for resp. the Blue and Red difference signals . 43971 @qwx453971 <p> So apart from a different data processing , the result is the same ? <p> I think 4K is useful in some cases when you shot on tripod and in controlled , " easy " situations . Like stock footage . Or when your focal length isn-t long enough . Otherwise is just a matter of postpone the decision ahead of you . Why not frame it like you want it right away and trust your ability ? The image quality of downsampling is of course another matter . <p> I think 4K is useful in some cases when you shot on tripod and in controlled , " easy " situations . Like stock footage . Or when your focal length isn-t long enough . Otherwise is just a matter of postpone the decision ahead of you . Why not frame it like you want it right away and @ @ @ @ @ @ @ @ @ @ of course another matter . 43971 @qwx453971 <p> 4K is going to be higher fidelity image . Here are just a few reasons : <p> 4K downsampled to 2K will be a better and more robust image than 2K . If there 's enough of a benefit to justify the extra space and processing speed required by 4K , has no absolute answer . I 'd say in most instances the answer is definitely yes , but it 's certainly not axiomatic. 
@@44332891 @4332891/ <h> Film scanner under G100K <p> I am trying to get overview of scanning market , got past experience with Cintel DataMill . <p> What are the options under G100K ? Besides BMD Cintel which might come out during this year , but might not so good for the old film due to the reason that it 's not sprocketless . <p> What are the main concerns to think about ? It will not be used for feature film , but mainly for car industry old internal + commercial films , which are 16mm / 35mm positive , negative , optical or magnetic sound etc ... <p> Following list I have created . 1 . BMD Cintel ( very price attractive whenever it will come out , missing lot of technical info , also not sprocketless but it might get changed when it 's released or in updated version ) 2 . DigitalVision Golden Eye 4 ( over G100K but seems be still most flexible and compare to other cheaper solution in tech terms for archive footage . If anyone got experience with v3 or v4 of this scanner @ @ @ @ @ @ @ @ @ @ end state of the art machine but not for us ) 4 . FilmFabreik Muller HDs from Holland ( only G20k and with wetgate , willl release in sep 2015 but 35mm version will come out next year ) 5 . Arriscan ( I think same as above , very high end machines , not sure about price tag ) 6 . Filmlight NorthLight ( very high end machine , not sure about price tag ) <p> We have two of DCS-LA 's Xena machines , one Dynamic Perf optical registered with a 12-bit 4K Color camera , and one Pin Registered with a 4K monochrome camera , looking at either a 6.6K or10K monochrome Illunis camera for the Pin-Reg scanner right now . <p> They support IR in DPX and TIFF . <p> DCS have fully built machines and retrofit systems for legacy telecines . <p> These are area panel scanners , I think they are superior to line scan and I have had to redo work that came out of a Golden Eye due to multiple pixel vertical streaks in the scans but that may have been shoddy @ @ @ @ @ @ @ @ @ @ customer and Alpha tester and I have had input into the systems from the perspective of what we need as a lab . <p> We have a Lasergraphics ScanStation , 8mm and 16mm model . It can also do 35mm , 17.5mm and 9.5mm but ours is n't configured for those . It 's above your price range , though . <p> That said , they have a new model that 's slightly stripped down with options a la carte , called the ScanStation Personal . ( do n't look at me , I did n't name it ! ) . I think you might be able to configure a 35mm/16mm model in your price range , but you should check with them . The ScanStation line is designed for archival use : sprocketless , very gentle , very reliable , excellent software and support . And it 's fast . I think it 's a more polished machine than the MWA machine , which I think is its closest competitor for modern scanners . <p> We also have a 6k Northlight for 35mm . It 's an amazing scanner @ @ @ @ @ @ @ @ @ @ are out there , and you can get one well under $100k if you look around . But they 're slow . Very slow . Seconds-per-frame slow . <p> As for your other options , in my mind the Scanity and Goldeneye are non-starters because they 're continuous motion transports with line sensors . As Rob pointed out , you 're going to have issues with warping at splices on these . The sales guy for the GoldenEye tried to sell me on it by saying that they recognize when there 's a problem and flag it for their ( separately purchased ) restoration system . To me , that 's not a solution , that 's a workaround . Stick with either a pin-registered intermittent motion line scanner ( Northlight , Imagica XE , something Oxberry-based ) , or a scanner that uses an area sensor , where this just is n't a problem . I never understood why people thought the Scanity was so great . I 've seen much better quality scans off a Lasergraphics Director , which is less expensive and arguably a much better , @ @ @ @ @ @ @ @ @ @ maintain . <p> I think it 's fair to say there are quite a few modestly-priced scanners out there today that do great work . The only scanners I 've used that I did n't like were Imagicas , but I believe they 're no longer in business . <p> I do n't know anything about them , but I know Image Trends is yet another affordable film-scanner manufacturer : <p> Well , the Northlight does have an optional pinless gate ( bigger gate , optical pin registration like the ScanStation ) . We have n't tested it yet since we 've only had ours for a little while , but I 'm planning to do some tests of old film with the registration pins removed , just using the pin plate as a pressure plate . For shrunken material this would require a post-processing pass to stabilize the image , but that 's easy enough to do these days . <p> DFT makes an excellent case for the Lasergraphics ScanStation in that white paper , actually - it meets all the requirements in that document , for a lot @ @ @ @ @ @ @ @ @ @ uses a Bayer sensor , but one major drawback with the Scanity is that you will get warping at splices , because it 's a line sensor with a continuous motion transport . <p> We 've had to do restoration work on some feature films that were scanned on a Scanity , from cut neg , and it was a nightmare : warping at every single splice , just like you get on a Spirit or Shadow , or any other line scanner with this kind of transport . That simply does n't happen with an area scanner . <p> Area sensor scanners are the way to go these days , the machines can use either Color-Bayer mask or Monochrome sensors and RGB+IR illumination and will not have frame-time distortion like line scan systems do . Also line scan can have issues with dirt buildup that will cause linear streaks through the whole picture instead of an issue with a few pixels if dirt/dust somehow ends up on the sensor of a area scanner . <p> The other nice thing about an Area Panel scanner is that it is easy @ @ @ @ @ @ @ @ @ @ resolution more DR etc. sensor as they become available . We just changed our Dynamic-Perf Xena from a 3.4K CCD to a 4K CMOS sensor the CCD had a max framerate of 6fps and the 4K 12-bit CMOS has a max framerate of 30FPS at 4K and 12-bits . There are a few new sensors just coming to market like the 71Mp 10Kx8K one from Illunis . <p> Bummer ! I 'd understood from Jeff that it was going to be much more " reasonable " 43971 @qwx453971 <p> The Kinetta costs $129,995 loaded -- all formats , capture computer with 24TB storage , everything but monitors . We sometimes have trade ins that are less expensive . 5K costs a little more , but you can upgrade at any time . Been busy -- five new machines this summer -- three to one user who had planned to get Blackmagic Scanners but found that they do not exist ! <p> We have been testing 5K CMOS sensors but while they are faster than CCDs , they lack the dynamic range of CCDs , so for most Kinetta users , @ @ @ @ @ @ @ @ @ @ and print ) they ca n't reach into the shadows as nicely . We 're testing a new 5K CCD with huge pixels and ridiculous dynamic range -- arrives next week . <p> One thing I never understood about the MWA and Flashscan machines is why do they have so many rollers ? My theory is that the designers are paid by the roller . Small diameter rollers , stepped rollers , and dancer arms are all the enemy of damaged film . <p> What are the options under G100K ? Besides BMD Cintel which might come out during this year , but might not so good for the old film due to the reason that it 's not sprocketless . <p> What are the main concerns to think about ? It will not be used for feature film , but mainly for car industry old internal + commercial films , which are 16mm / 35mm positive , negative , optical or magnetic sound etc ... <p> Following list I have created . <p> 1 . BMD Cintel ( very price attractive whenever it will come out , missing lot @ @ @ @ @ @ @ @ @ @ get changed when it 's released or in updated version ) <p> Sprocket driven , no good for archival use . UHD. 16mm and 35mm only . Does not ship with the model shown in their brochure . <p> 2 . DigitalVision Golden Eye 4 ( over G100K but seems be still most flexible and compare to other cheaper solution in tech terms for archive footage . If anyone got experience with v3 or v4 of this scanner , please share ) <p> Apparently it is not friendly with difficult splices . Digital Vision did n't bother showing it at the AMIA ( archivist 's ) convention last year -- which is their prime market . Wonder why ? <p> 3 . DFT Scanity ( high end state of the art machine but not for us ) <p> Nice if you can afford it -- but grossly overpriced and their pricing method -- pay for a lot of software options when the hardware is already capable of it -- rubs me the wrong way . DFT is a difficult company re service -- ask any Spirit owner . They like @ @ @ @ @ @ @ @ @ @ Holland ( only G20k and with wetgate , willl release in sep 2015 but 35mm version will come out next year ) <p> A toy . Their current machine has only one motor -- rewind by hand . The designer died a couple of years ago and the new owners are more money people than film people . <p> 5 . Arriscan ( I think same as above , very high end machines , not sure about price tag ) <p> The two U wrap rollers on each side of the gate are very hard on splices . Slow , especially in double-flash mode . <p> 6 . Filmlight NorthLight ( very high end machine , not sure about price tag ) <p> Still around ? <p> Any other recommendation to check out ? 43971 @qwx453971 <p> Sure . I am biased , of course . Check out the Kinetta Archival Film Scanner . Just slightly over your price range depending on the exchange rate , but comes fully loaded ( including formats -- 8/S8 , 16/S16 , 35 -- with 9.5 , 17.5 , 22mm , and 28mm available @ @ @ @ @ @ @ @ @ @ all software -- everything but monitors . Sensor module can be swapped out by the user in 1 minute -- easily upgraded unlike many other machines -- so as new sensors are available your scanner can keep up with them . Many new improvements coming this summer . See kinetta.com for info ( web site is a bit out of date because I 've been too busy building scanners -- so far 5 for this summer . 
@@44332893 @4332893/ <h> Jean Paul Re+tegui - Color Reel 2017 <p> You have your opinions I guess , but no one 's giving any bad advice here . Yes , obviously it 's common sense not to purposely show a shot that would embarrass someone , but I do n't see colorists or anyone else doing that . <p> Well as you both say it 's not that big of a deal on the B/As I 'll try it without the white lines . I 'm currently working on a couple of projects and will exchange some of the repetitive shots with these new ones later this montth . <p> I live in Per+ and sadly we often do not work with the DP , instead we work with the director but many times we grade alone without any briefing and that 's what ends up airing . It 's very rare to be with the DP of the project during the color session here . <p> Well as you both say it 's not that big of a deal on the B/As I 'll try it without the white lines @ @ @ @ @ @ @ @ @ @ and will exchange some of the repetitive shots with these new ones later this montth . <p> I live in Per+ and sadly we often do not work with the DP , instead we work with the director but many times we grade alone without any briefing and that 's what ends up airing . It 's very rare to be with the DP of the project during the color session here . 43971 @qwx453971 <p> Cool man . Yeah , I think you will like it better without the white lines . The color and compositions in your reel are already excellent , so any more improvements you make from here is only going to make the reel that much better . 
@@44332894 @4332894/ <h> Looking for DaVinci 2k or Renaissance 8:8:8 Soft Panel <p> Ours has a busted screen ( one of the long skinny screens above the rotary encoders ) so I 'm looking for one to scavenge parts off of . Can be broken , as long as there 's at least one good screen on it . Or if you have a box of parts you do n't want , I 'd be interested in that too . <p> Perry , there are 100s of more features in the new DaVinci Resolve Mini control panel . Surely for $3k its better to get out of the 20year old panel hardware and learn the keyboard shortcuts and the new panel . <p> I grant you its cheaper today to repair the 20 year old panel . Again . But you miss the joy of camera raw , curves , NR , node and edit sizing etc etc. 43971 @qwx453971 <p> But very little of that is applicable to our use of Resolve . We 're doing fairly straightforward stuff here , primarily emulating what a lab would have done ( @ @ @ @ @ @ @ @ @ @ scanning their A/B rolls and making new digital masters from those , so the needs are modest ) . <p> And as Jason pointed out , when you can get a set of panels for under $100 at auction , and it does everything you need ... <p> And you keep $2850 for you and your family . Everything I contemplate purchasing has to pass the " will it bill more or allow me to do more , " the old " it 'll impress clients when they walk in the room " does n't hold much water anymore . The new panels do seem great though , played with them a bit at B and H in NYC . <p> But very little of that is applicable to our use of Resolve . We 're doing fairly straightforward stuff here , primarily emulating what a lab would have done ( most of our work is with older filmmakers who are scanning their A/B rolls and making new digital masters from those , so the needs are modest ) . 43971 @qwx453971 <p> I do quite a few film restoration @ @ @ @ @ @ @ @ @ @ surprised the number of times I get a really stepped-on IN or something where I do have to tie the curves into knots in order to get a semblance of a normal picture back out of it . I really , really do need curves , keys , soft clips , windows and a bunch of other stuff in order to squeeze the most out of the image . I 'd rather veer a little bit away from rigidly accurate ( and ugly ) and move towards more tech ( and better results ) , even if it 's just broad scopes and nothing super-tweaky . Judicious noise reduction is part of that . <p> I would disagree that accurate is ugly . Our clients are primarily archives and individual filmmakers , who want the film to look the way it looked when it was first made . <p> We 'll sometimes do restoration work to clean up things that have happened to the film , such as scratches and dust , depending on the budget and client 's wishes . But we 're talking about clients who want an @ @ @ @ @ @ @ @ @ @ long term archiving , plus a viewing copy that they can show at festivals , etc . What they want , what they ask for time and time again , is that the film look the way it did originally . Obviously there are some differences ( grain is more pronounced when you take the generation loss of intermediates and prints out of the equation , for example ) , but we have n't had anyone complain that they were unhappy with the result . On the contrary , they usually tell us they 're amazed at how much crisper it looks than in the prints . <p> And I will say that in the years we 've been in business , we 've made thousands of DVDs and hundreds of Blu-rays for clients . Many of those are commercial releases , and the most successful of those customers ( in terms of sales , reviews of image quality , etc ) were the ones that insisted on not messing with any NR , leaving grain intact , and taking care to make sure the film looks the way it @ @ @ @ @ @ @ @ @ @ <p> I 'm not saying that what you 're doing is wrong , it 's just not what our clients want . <p> I 'm not saying that what you 're doing is wrong , it 's just not what our clients want . 43971 @qwx453971 <p> The restoration work we did at Lowry was for MGM , Fox , and Disney . They 're about as demanding as it gets , and believe me , there was tons and tons of windows and keys and shapes and tracking and secondaries and everything you can imagine ... especially when the elements are faded and falling apart . Just getting the blacks black and keeping them black is a nightmare . ( Very big challenge for me on Disney 's Black Hole , which was many weeks of work , to name one ) . <p> I think there are always judgement calls in these things , and at some point , the clients depend on our input and taste to do a reasonable amount of correction without substantially changing the look . We all know of films where the director @ @ @ @ @ @ @ @ @ @ changes in color because of how they felt the film needed to be seen by contemporary audiences . Accuracy was not their concern . I 'm not a fan of doing that kind of stuff , like imposing an " orange &amp; teal " look to a film that never looked that way in the first place . But when I see a character standing in the mud and I can throw another 5 units of light on them to make them more visible , I 'll do it . <p> WB 's MPI does the same thing on their restorations ; I 'd point to Godfather as a good example of a movie that I think represents the original intent of the film , but also looks quite a bit different ( and far less grainy ) than any previous home video or theatrical film version ever did . 
@@44332895 @4332895/ <h> Prores 4444 levels <p> What 's the rational for assumption that Prores 4444 levels are full ( data ) whereas Prores 422 is typically assumed to be legal ( video ) range ? I believe those are assumptions that Resolve and some other software make when interpreting such files . <p> Without getting into a discussion of the pros and cons of data vs. video levels concept , why do you think there should be a difference between 4444 and 422 ? I think they should be the same , one or the other . <p> Also , for what it 's worth , I believe 4444 is acceptable in either Video OR Data levels , and should have a flag in the header of the file that states which it is. 43971 @qwx453971 <p> And I think you can " theoretically " set this manually in the render pages of most software . I 've gotten some DNxHD files that were in Data mode when Resolve expected Video mode , and it wound up looking very funky , but I have yet to discover whether the editor @ @ @ @ @ @ @ @ @ @ normal behavior . <p> I received a Media Composer generated 4444 Prores file today that had video levels . Neither Resolve nor Clipster interpreted it automatically . That can mean there is no flag or that Media Composer does not write the flag . Or it could mean that Clipster and Resolve are hard coded to interpret 4444 Prores as data levels . <p> Media Composer generally defaults to video levels , but the user can override that and create output presets with either data or video levels . <p> The thing that bugs me in this " data assumption " is that unless the host application manipulating the color and creating the files works in floating point precision with data levels internally , or unless the files are CGI renders , there is not that much advantage to data levels over video levels . Yet you lose superblack and superwhite when you commit to saving something with black = 0.0 and white = 1.0 . <p> there 's no way to have encoded excursions beyond the Full rate on RGB data . You ca n't have reds reder than @ @ @ @ @ @ @ @ @ @ data you can have excursions beyond the full range , ergo the need for " video " levels . <p> In other words , describing RGB data in " video " range would just be discarding potential bit values that ca n't possibly have any information . Where describing chroma subsampled data in full range would potentially clip values which would otherwise persist in a true RGB encoding . <p> I received a Media Composer generated 4444 Prores file today that had video levels . Neither Resolve nor Clipster interpreted it automatically . That can mean there is no flag or that Media Composer does not write the flag . Or it could mean that Clipster and Resolve are hard coded to interpret 4444 Prores as data levels . 43971 @qwx453971 <p> We 've had issues with Avid 's direct output to ProRes for years . The best workflow we 've come up with is to install Compressor ( $50 download from Apple ) on the same machine as the Avid . Render the Avid file out to a Quicktime , same as source settings . Then drop that quicktime @ @ @ @ @ @ @ @ @ @ numerous issues with Avid 's totally wacky quicktime handling . <p> Seriously , I do n't trust any quicktime we get from Avid , because we 've just seen a million weird things happen with their files . Frustratingly , the problems seem to vary from version to version of the Avid software , and it 's compounded by the version of the DNx codecs being used . That 's part of why we do n't generally accept DNx files from clients , and we ask them to convert to ProRes as described above . Then they can look at the ProRes file and know it looks right . <p> What you 're describing is one of the things that that workflow has addressed . <p> Seriously , I do n't trust any quicktime we get from Avid , because we 've just seen a million weird things happen with their files . Frustratingly , the problems seem to vary from version to version of the Avid software , and it 's compounded by the version of the DNx codecs being used . That 's part of why we do @ @ @ @ @ @ @ @ @ @ ask them to convert to ProRes as described above . Then they can look at the ProRes file and know it looks right . 43971 @qwx453971 <p> Wow , that makes me feel better . The last dozen or so projects I worked on were done with either Premiere or FCP7 , but the 2 or 3 Avid projects along the way had some weird quirks with the DNxHD files and I figured it was something I did ! Glad to see this is a known issue . <p> Wow , that makes me feel better . The last dozen or so projects I worked on were done with either Premiere or FCP7 , but the 2 or 3 Avid projects along the way had some weird quirks with the DNxHD files and I figured it was something I did ! Glad to see this is a known issue . 43971 @qwx453971 <p> Avid has *never* played nice with quicktime , and has been the source of so many headaches for us over the years . If it was up to them , you 'd be completely confined to the @ @ @ @ @ @ @ @ @ @ their system . <p> They had to be dragged kicking and screaming into the 1990s sometime around 2010 . <p> there 's no way to have encoded excursions beyond the Full rate on RGB data 43971 @qwx453971 <p> I rarely disagree with you , Juan , but I do here . While it is true that display referred RGB data is limited to the 0-1 range , scene referred RGB data can go way above 1.0 . And within an image processing pipeline , e.g. internally in a grading system , you may also get values below zero . <p> In the context of this thread , RGB data in a ProRes4444 file is normally limited to 0-1 . That is true . <p> I also think it can be used as a container for Full Range RGB data , but the intended viewing space is Video levels . So you have 50 10-bit pixel values stored below black and above white . Of course this can be a nightmare if someone interprets it incorrectly . I remember almost 10 years ago storing a full range P3 RGB master on @ @ @ @ @ @ @ @ @ @ regardless of the bold WARNING on the label , somebody went and made dubs of it for a client . <p> there 's no way to have encoded excursions beyond the Full rate on RGB data . 43971 @qwx453971 <p> In the context of final delivery you are correct . In the context of intermediate image that will be further manipulated in compositing or color , you do want this extra information . Stu Maschwitz eloquently described the need for black and white point be different from 0.0 and 1.0 respectively at Prolost some years ago . <p> I remember almost 10 years ago storing a full range P3 RGB master on an HDCam-SR , kinda like a backup ... But , regardless of the bold WARNING on the label , somebody went and made dubs of it for a client . 43971 @qwx453971 <p> I seem to recall there was some cockamamie method for doing uncompressed 444 on HDCam using two tapes at once . I 'm glad as hell that idea never caught on . <p> This is why I 'm nervous about HDR masters . You just @ @ @ @ @ @ @ @ @ @ it 's going to make it to air ... <p> I rarely disagree with you , Juan , but I do here . While it is true that display referred RGB data is limited to the 0-1 range , scene referred RGB data can go way above 1.0 . And within an image processing pipeline , e.g. internally in a grading system , you may also get values below zero . <p> In the context of this thread , RGB data in a ProRes4444 file is normally limited to 0-1 . That is true . 43971 @qwx453971 <p> Snagged on a technicality ! I was attempting to answer the original question , which is why this is the default behavior . <p> What 's the rational for assumption that Prores 4444 levels are full ( data ) whereas Prores 422 is typically assumed to be legal ( video ) range ? 43971 @qwx453971 <p> Resolve uses an ' Auto ' button for the range and when the file is rendered it does n't tell you in clip attributes or metadata section if it is legal or full . <p> Premiere @ @ @ @ @ @ @ @ @ @ problems , can not change its interpretation and have to rerender the file . <p> Filmlight states in the renderer of Baselight that Prores ( 4444 and 422 ) always expects Full Range levels . One can ask : " What is the reason that Prores should be always full range ? When you aks the support Filmlight treats it this like it was a standard or at least a fact . <p> But there is definitely no information about the range of Prores in Apples White Paper from April 2017 . It seems to be not specified by the originator of Prores . That might be reason why you have to find the right combination of tools or settings . An oldschool SMPTE might help too in these standard leaking times . <p> Hoping for a future boost like ACES pushed the overdue Color Management implementation in our tools that something or someone pushes fully compatible metadata exchange in our world . Its a kind of shame that flagging a file to be legal or full range is not working in our hightech industry . <p> there 's no @ @ @ @ @ @ @ @ @ @ RGB data . You ca n't have reds reder than peak red . When describing Yuv or actually YCbCr encoded data you can have excursions beyond the full range , ergo the need for " video " levels . <p> In other words , describing RGB data in " video " range would just be discarding potential bit values that ca n't possibly have any information . Where describing chroma subsampled data in full range would potentially clip values which would otherwise persist in a true RGB encoding. 43971 @qwx453971 <p> Exactly ! <p> Also if would be impossible to have RGB in anything but 444 because RGB unlike YCbCr requires 3 separate channels . 
@@44332896 @4332896/ <h> Pictures from NAB 2017 <p> The Adobe theater always had a great crowd and was consistently packed . <p> Although the Blackmagic booth was extremely busy , the Resolve demo theater was much more peaceful as everyone learned about all the new features and improvements . <p> The Avid booth was very busy with a huge array of demo stations for nearly every aspect of post and broadcast work . <p> The Dolby booth was very busy , it 's just that everyone was crowded into dark demo rooms looking at HDR . Kevin Shaw also gave a great talk on HDR , as well as a presentation to a packed room on the Colorist Society . <p> The Filmlight booth was very popular , and their demo stations were consistently filled with visitors , including a great LGG Meetup on Monday . <p> SGO had some great announcements , including several new VR and Color-related subscription products . <p> And finally , a panorama of the Blackmagic booth as that was the only way to get it all in one shot .... <p> View attachment 4943 The @ @ @ @ @ @ @ @ @ @ everyone was crowded into dark demo rooms looking at HDR . Kevin Shaw also gave a great talk on HDR , as well as a presentation to a packed room on the Colorist Society . <p> Not so much new hardware , as much as lots of training and demos on the DolbyVision workflow . They have a very good slide deck that is the focus of many of their presentations that explains things like Cinema EDR vs Television HDR , CMU metadata workflow , IMF distribution , etc . <p> Right now , it only works with Lustre , as ADSK has designed the software for it . However , it uses the same protocol and goes through Tangent Hub as all other Tangent panels and I am pretty sure the Lustre market in itself is too small to justify going into production with the Tsunami , so ... stay tuned , please . <p> There is not yet a mapper for this panel , Tangent is showing a demo-kind-of-mode with a beta-version of Tangent Hub that supports it . Another piece of Info : Power and ethernet are @ @ @ @ @ @ @ @ @ @ other two panels get their power and usb . Basically the middle panel functions as a power and usb hub for the other two . The finish is very sturdy and the surface in itself feel very good and high quality . <p> Still , keep in mind that this is still a prototype , and lots of things can ( and some will for sure ) change . <p> Right now , it only works with Lustre , as ADSK has designed the software for it . However , it uses the same protocol and goes through Tangent Hub as all other Tangent panels and I am pretty sure the Lustre market in itself is too small to justify going into production with the Tsunami , so ... stay tuned , please . <p> There is not yet a mapper for this panel , Tangent is showing a demo-kind-of-mode with a beta-version of Tangent Hub that supports it . Another piece of Info : Power and ethernet are connected to the middle panel , and from there the other two panels get their power and usb . Basically the @ @ @ @ @ @ @ @ @ @ the other two . The finish is very sturdy and the surface in itself feel very good and high quality . <p> Still , keep in mind that this is still a prototype , and lots of things can ( and some will for sure ) change . <p> Indeed - it 's got a lot from CP200 , ACS , Elements . Me particularly likes the heavy feel of the panels - but that 's just me - I do n't need to carry it . I 'll post some more photos to the thread on the controller forum . 
@@44332897 @4332897/ <p> Twixtor creates highly engaging slow motion shots without the cost of renting a high speed camera . In order to achieve its unparalleled image quality , Twixtor synthesizes unique new frames by warping and interpolating frames of the original sequence by employing RE:Vision 's proprietary tracking technology that calculates motion for each individual pixel . <p> ReelSmart Motion Blur ( RSMB ) automatically adds more natural-looking motion blur to a sequence . Proprietary tracking technology is at the heart of RSMB , so there is no handwork involved . Of course you can add as little or as much blurring as you need and even remove motion blur ! This is a useful tool to give video a more film-like feel , and is also useful to reintroduce motion blur for blue or green screen shoots where a fast shutter is necessary to pull a good key . Also included in some RSMB versions is a feature that allows you to blur with user-supplied motion vectors that , most likely , will come from a 3D animation system ( see the RE:Vision Effects website for which versions include @ @ @ @ @ @ @ @ @ @ Blur can see substantial acceleration using GPUs . Note that most currently-sold GPUs will provide customers with much needed acceleration , including even many mobile GPUs for laptops . The total speedup end-users will see will vary once application overhead is taken into account , including disk I/O , internal host buffering schemes , bandwidth of PCIe bus , codec used for footage , as well as features used in plug-ins . <p> Availability and compatibility : * HitFilm 3.0.0576 and up. * Nucoda , verified to work in versions released in 2014 and later. * DaVinci Resolve v11 and up ( RSMB Only ) . * Natron , v1 and up . <p> family of software products . <p> Twixtor , DE:Noise , RE:Map and RE:Flex are registered trademarks and DE:Flicker , Effections , FieldsKit , PV Feather , RE:Fill , RE:Match , ReelSmart Motion Blur , Shade/Shape , SmoothKit , and Video Gogh are to change without notice . 
@@44332898 @4332898/ <h> Exciting news from Tangent about NAB 2016 <p> Hi all , This year Tangent will be exhibiting at NAB , booth number SL5507 , so please drop by if you 're going to be there . We 'll have our panels working with a number of our partner 's software apps , and we 'll be showing the following exciting developments for the first time : The new Ripple panel With our new Ripple panel to be on sale very soon , you 'll be able to see it and try it out on our booth . We 'll have it on a number of different software applications and many of our partner 's will be showing it on their booths too . So keep an eye out for Ripples around the show floor . Adobe Premier Pro support for Tangent panels Yesterday Adobe announced that they 're including support of our panels in the next release of Premier Pro . They have also implemented full support of our Mapper application so you 'll be able to customise the layout of the controls on the panels . This @ @ @ @ @ @ @ @ @ @ looking forward to this as it 's been the source of a lot of requests from users . You can see the Adobe announcement here : LONG ... Color Finale ( FCP X ) support for Tangent panels We already have pretty much full control of FCP X with our panels thanks to the efforts of Sam Mestman and his extensive use of our Mapper . Now the guys at Color Grading Central have announced full support of our panels in their Color Finale plugin for FCP X. With Support for the Mapper included , you can customise your control layouts . We 're excited to see this expansion for the support of our panels within FCP X and we 're looking forward to the release of this new version . You can read more about Color Finale here : LONG ... Capture One technical demo for Tangent panels Last year we reported that Phase One , makers of high-end professional digital photography cameras , were doing a technical trial of our panels with their Capture One image processing software . Whilst this is not an official product , they @ @ @ @ @ @ @ @ @ @ at NAB . We were impressed last year when we saw the use they had made of the panels and the Mapper to create an intuitive interface for their software , and we thought it was a good demonstration of our products being used outside their obvious field . You can find out more information at : **32;1368;TOOLONG We hope to see you in Vegas ! Cheers , Chris <p> I mapped my wave to Capture One to try it out , but had issues with controls that randomly stopped and started working , I think the software just can not receive fast repeated keyboard commands so well . Also the increments of the adjustments that are mapped to shortcuts vary quite a lot , exposure , brightness and contrast have quite big increments , while white balance has small finer increments . <p> So to get reliable performance with the panels in Capture One , you 'd need them to directly support mapper ... <p> @Margus : The version we are showing at NAB has direct support of the Mapper . The version is not available to the public @ @ @ @ @ @ @ @ @ @ not sure when and if it will be released . <p> Will certainly buy it or the Tangent Element TK . But , I need users feedback to make my decision ... the trackballs seem to stick out a lot compared to the Element . I 'm afraid the trackballs will get in the way when reaching for the luminance dials . <p> Chris , the link to Adobe 's blog do n't work . Also , The trackball panel will work with Premiere pro ? Is that right ? What about the KB panel ? Will premiere pro have a brand new interface , like a color tab like Resolve ? <p> Will certainly buy it or the Tangent Element TK . But , I need users feedback to make my decision ... the trackballs seem to stick out a lot compared to the Element . I 'm afraid the trackballs will get in the way when reaching for the luminance dials . <p> Chris , the link to Adobe 's blog do n't work . Also , The trackball panel will work with Premiere pro ? Is that @ @ @ @ @ @ @ @ @ @ pro have a brand new interface , like a color tab like Resolve ? 43971 @qwx453971 <p> if i was to insist on a Tangent panel it would be Kb and a iPad with VS , you can do everything Ripple can do and sooooooo much more with Kb +VS <p> but a decent small workspace with Ripple , Kb , VS , Xkeys &amp; MidiTwister might be pretty usefull , but that lot is getting close to the price of a Artist Color , and probbaly a far better choice for Resolve as it 's softkeys can be mapped , and Tangent mapper can not <p> Let 's see what gets announced with " 250 new features " 12.5 , and cross our fingeers for small surface mapping <p> Shane , I do n't use Resolve . I use Premiere and Speedgrade . I really hate touchscreen ... Working on a Ipad is a big no no for me . There is nothing like physical buttons . I do n't want to look at the screen all the time to be sure I 'm hitting the right ' ' button ' ' . 
@@44332899 @4332899/ <h> AEO-Light 2.1 Open Public Beta <p> Colleagues over at the Association of Moving Image Archivists posted that a 2.1 beta version of AEO-Light ( an open source program that extracts digital audio from scans of motion picture film ) is now available at **25;1402;TOOLONG . <p> Regards- <p> - John Tariot <p> - - - - - - - - - - - <p> Colleagues , <p> The AEO-Light 2.0 team at the University of South Carolina along with our consultant Tommy Aschenbach invite you to participate in the public beta phase of our project . <p> Funded by the National Endowment for the Humanities , AEO-Light 2.0 is an open source program that extracts digital audio from scans of motion picture film . <p> The software is available for Windows and Mac computers and can be downloaded from **25;1429;TOOLONG <p> Agreed ! And it does . Not that it is a silver bullet , mind you . There has been quite a lot of work on this approach over the years in the archives community- spearheaded largely by Bob Heiber of Chace Audio in Burbank ( now @ @ @ @ @ @ @ @ @ @ to the field 's understanding of sound on film . 
@@44332900 @4332900/ <h> DaVinci Resolve Mini Panels - Review <p> Does it work if you trigger the reference reposition mode with a keyboard shortcut then use the input sizing controls ? 43971 @qwx453971 <p> If you mean the reference image sizing/position . I think that might be a submenu in the sizing page . I meant the wipe/split position , as in where on the broadcast image the wipe line is placed . Which is not adjustable from the panels . <p> I 've used 3 sets of advanced panels on and off for 5 years or so . So I have some familiarity with how the age . The dials and the material they 're made from has not been a problem at all . The build quality of the advanced panels is excellent . Where they have issues seems to be in the reliability of specific components . The built in keyboard is a common point of failure . They can also be a bit finicky in between power cycles , but nothing a restart does n't usually fix . In terms of finish material , it seems to @ @ @ @ @ @ @ @ @ @ ca n't speak to a definitive answer on that . The mechanics of it are different however . 43971 @qwx453971 <p> Certain keys go bad over time ( particularly Mark and Stop ) , and the ribbon connector on the keyboard goes bad for sure . I 've also seen the pots in the trackballs get " scratchy " over time -- I think from the blood and sweat and coffee spills over time . As it is , it 's amazing they last as long as they do . <p> Does it work if you trigger the reference reposition mode with a keyboard shortcut then use the input sizing controls ? 43971 @qwx453971 <p> I do n't think that works , but I 've found that Edit Size reposition is now much more reliable than it is on other panels . You can also adjust Edit Sizing with the panels on the Edit page , which is helpful . <p> overall a good controller , the weak link on the mini is tiny play and stop buttons . 43971 @qwx453971 <p> Absolutely agree . And the new V14 features @ @ @ @ @ @ @ @ @ @ increasing speed is a big thing , but from the fst/last frame buttons I want to get an instant response . So I would go to G13 instead of holding Prev/Next frame and wait for ages . <p> Nice work as usual Mr Salvo . The legends under the Col Boost row are a little hard to read under grading conditions , but with regular use you will reach for the correct control instinctively , that 's the advantage of a panel .... you just do it . I would go the extra 2K and get the Mini over the Micro . I chatted to Tangent at NAB about the increased competition , and they said they were surprised it had taken so long ! Tangent are smart guys so expect some cool stuff from them soon . Great for us to have choices 
@@44332902 @4332902/ <h> Baselight vs Davinci Resolve <p> OK , so I think what you 're finding is that you 're getting a nicer result now that you 're working with a true log grading implementation . <p> If I remember correctly , you were using Speedgrade before Resolve , which did n't have one either , so Prelight may be the first time you 've actually experienced grading your own log camera media with a technically correct log grading toolset . <p> I have always found that there 's something about the way Baselight mixes color that is somehow a bit more sophisticated and proper than just about all of the other systems , and I too find it very easy to get to a representative base grade using essentially a Film Grade strip and little else . This is likely due to both the color management ( best in the business ) and the grading tools themselves , but the result is that I can achieve a more satisfying result with Baselight , and I can do it more quickly and with less experimentation than any other system . @ @ @ @ @ @ @ @ @ @ ( not necessarily for everyone else ) it is absolutely true . 43971 @qwx453971 <p> That 's my experience with Baselight too . You get better results faster . When Resolve let 's you think of keying or Log-grading something ( to correct it ) , you 'll get nice results instantly with Baselights Video-Grade ( or Film-Grade ) . This was already there before the full implementation of the Color Management into Baselight ! It is the same with Sratch and Lustre . Both have very nice behavior of the " plain " Video-Grade tool . In other words I find Resolve is often somehow rough . One gets quickly an impressing result , yes . But you have to work further ( with additional tools / nodes ) to refine it . <p> Node-Tree vs . Layer : With Resolve the node-tree is great because I have visible clarity . But due to strict order of the tools in which there are processed , I need more nodes than I would like to have . In Baselight you can change the order of the tools in the cascade for @ @ @ @ @ @ @ @ @ @ , then tint with Film-Grade , then log-ish grade with curve crade , put a color-mixer into the row and then shift the blues to cyan in just one layer ! <p> Tools : Resolve evolves well over the time . Yes . The tools not really ! While the new tint/temp sliders will for sure attract new users , it 's easy to find out that they are simple the gain ball in the directions of **25;1456;TOOLONG . Baselights new Basegrade e.g. is a true new developed tool where developers thought about how to face the actual needs in colorgrading . And a tool like " Technical Grade " is a very good example that an old ( ? ) ( log-Tool ) can be used to achieve very nice looks with incredible speed . What I want to point out is , that it is a unpolite product policy of BM to change tools and to let the old version dissapear while color graders work based on that tools . Like it happened with the keyers when clean blacks and clean whites were introduced ( which are not @ @ @ @ @ @ @ @ @ @ . I find this tool is a good example for the missing seriousness in the product development of the real core of Resolve : Color ! I would not recommend using Color Match , which does not result in a Video-Grade or Log-Grade at the beginning of a dailies chain . It results in a black-box ( LUTs/keys ) with doubtful results . <p> Usability : The reason to give a great compliment to BM is usability . I think it is ( beside the price ) one key factor of the success story of Resolve . I wish this hard work of making a product more smooth to work while giving the operator every feasible option , will be further done for Baselight . Filmlight did a great job with the " color journey view " and hints like " this codec expect full range " in the render dialog . 
@@44332903 @4332903/ <h> Problem with Auto Sync in Resolve 11 <p> Hello , I am working on a feature where I am syncing sound and creating dailies using Resolve 11 . I am very familiar with how to Auto Sync sound and picture in Resolve , have done it for many years . However on this new feature , when I perform Auto Sync , the sound files all link to their corresponding picture files , HOWEVER , the audio is not in sync . The audio starts the moment the mixer hits record , so I hear all the pre-roll and then at the end , the sound cuts off when the picture ends . We have performed any number of timecode jam/sync tests , and all yield the same result . The timecode on the slate matches the timecode on the mixer 's recorder and it matches the timecode on the camera . The only thing different from previous experience is that we are not running " time of day " timecode , instead we start every day at 00:00:00 . Anyone else experience this ? <p> Starting at @ @ @ @ @ @ @ @ @ @ time of day . I always , always argue my clients out of using Hour 0 for anything . If they have to cross midnight , then I say start the shoot at clock time : 3:00:00:00 for 3PM , etc. , then when it crosses midnight it merely goes to Hour 12 , then Hour 13 for 1:00AM and so on . <p> Do a sync test : Camera should have both tri-level sync and timecode reference in from a timecode box ( Denecke or Ambient ) . Timecode slate should be jammed to timecode box . Sound recorder should be jammed from timecode box . ( Alternately , sound recorder can jam timecode box -- it does n't matter . ) Roll camera &amp; sound . Clap sticks at the head , roll for five minutes , then clap at the tail . Pull files from camera &amp; sound and check them in your editing program of choice . See if head clap is dead on , check to see if visible timecode matches embedded timecode , and check tail stix . If it 's not right , @ @ @ @ @ @ @ @ @ @ . <p> If there 's a drift problem , I would examine the files in a 3rd party program and see if there 's a drop/non-drop issue or a 23.98/24.00-frame issue . I would also try a different camera and a different sound recorder if the problem persists . Putting a lockit box ( or the Denecke equivalent ) on the camera generally solves these situations 90% of the time . I do not trust internal jam on cameras , though I will say I 've had the best luck with Arri Alexa , which will generally work except after speed changes , when it sometimes reverts to the dreaded 00:00:00:00 . <p> Marc , you have been a great source of info in the past , so I value your input . We are shooting Arri Alexa ( ArriRaw Open Gate ) , and using a 16 track Deva . Everyone involved , including me , has 20+ years experience in the feature film world . The is a very rare , singular film . We are shooting picture at 24.000 and sound at 24.000/48048 , but the sound @ @ @ @ @ @ @ @ @ @ can import them into Resolve . My Resolve project is in 23.976 , because Editorial needs 23.976 material for their project . This is a massive film that started shooting in 2007 on film , and now we are reshooting 80% of the movie on digital . Editorial is already set up in a 23.98 project , so we are trying to keep it consistent . We did 5 weeks of tests where everything worked smoothly , but we were running " time of day " timecode . After the tests , sound decided to start at 00:00:00 because of the " midnight " thing , as we have a lot of split shifts five the next 2 months . On Monday I will have the Sound Mixer go back to " time of day " and see if that solves the issue . I will let you know . Thanks again ! <p> Alexa and Deva are golden , very , very reliable . Is there a lockbox on camera ? <p> I think making the Resolve project at 23.976 is going to be a problem . What happens @ @ @ @ @ @ @ @ @ @ Resolve -- at 24.000fps ? Who made the decision to shoot at 24.000fps ? This is one of those cases where they really have to ask the post questions before shooting . If they had asked me , I would 've just said do everything at 23.98 -- camera , sound , slate , editorial , color . Very simple . Then at the very end , pull up to 24.000 , which is done every day for theatrical exhibition . <p> People worry way , way too much about crossing midnight . At the worst , what they can do is when it hits 11:45PM , just advance the timecode an hour and rejam everything . Otherwise , just use clocktime . <p> If sound is drifting , then I would pull it into Pro Tools and figure out if sound is drifting ahead or behind and then do a pull-up/pull-down to compensate , then bring those " adjusted " files into Resolve . I have seen weird situations where something got a double pull-up or pull-down and things drifted wildly , or ( worse ) when it had @ @ @ @ @ @ @ @ @ @ the place . <p> They are jamming the camera with the slate , and there has been no drift between the visual timecode on the slate and the camera timecode that I can read in the burn in window . Also there is no drift in the audio itself . Once I manually sync it , it stays locked in sync . <p> There were about 2 weeks of discussions about 23 vs 24 . Editorial has 7 years of legacy material already in a 23 project , so they need 23 files . The current production is photographing a lot of projected material where the actors are interacting with the projections , and the projection team wanted everything to be shot at 24 for a smoother workflow on their end . The DP agreed , saying that all the film previously shot was at 24 , so let 's keep it consistent . <p> I lobbied hard for camera and sound to be at 23 , but that 's because it would make my life easier . <p> However , once we settled on shooting 24 , and outputting 23 @ @ @ @ @ @ @ @ @ @ several sync sound tests . Everything worked . Auto sync worked , no drift , Avid took in the files with no problem what so ever . All seemed bulletproof by the end of our 5 week prep . <p> Now , on day one , Auto Sync does not work any more . And again we tried a multitude of tests , including running camera and sound at 24 and thenputting them in a 24 frame project in Resolve , which should be a no brainer . But again , Auto sync grabs the sound files and marries them to the picture , but not in any kind of sync . It lines up the start of the audio clip with the start of the picture clip . The only thing I have n't tried is switching timecode back to " time of day " . I will do that tomorrow and report my results . <p> Also , when the assistant editor brings in my MXF 's and the original audio files into the Avid , he has no trouble auto syncing them . Everything sync 's up @ @ @ @ @ @ @ @ @ @ is AutoSync and not that it 's drifting , then this is just a minor irritation , not a deal killer . Just manually sync everything by cueing to the clap and positioning the sticks . It takes less than 1 minute per take . If it does work automatically with Time of Day , and also these files work in Avid , then I 'd say this is a very real bug . <p> I know this is n't exactly in line with this post , but it 's in the same ballpark . I have no issue getting my sound to sync to my video in Resolve , it 's actually always spot on . My issue is that I 'd really like to be able to export an XML to my editor to load in premiere or fcp , but whenever I try it , the audio in the imported sequence has an offset . I should probably go back and see exactly how many frames , but it 's no longer in perfect sync . Any thoughts ? I 'm probably going to move to a @ @ @ @ @ @ @ @ @ @ well the audio quality transfers over when using the proxy files for audio . <p> I know this is n't exactly in line with this post , but it 's in the same ballpark . I have no issue getting my sound to sync to my video in Resolve , it 's actually always spot on . My issue is that I 'd really like to be able to export an XML to my editor to load in premiere or fcp , but whenever I try it , the audio in the imported sequence has an offset . 43971 @qwx453971 <p> When you say the audio has an offset , is it possible the picture has been offset ? Do n't forget to take into consideration there is delay in the LCD monitors most people are using for editing . <p> If it 's a 1-frame offset , tell the editor or assistant to dial that into the prefs . Problem solved ... assuming it 's a consistent +1 or -1 . 
@@44332904 @4332904/ <h> A before and after using Paint module in Baselight 5 <p> A while ago I had graded a video with this shot of the model . Using traditional beauty clean up tools of the skin were not good enough , especially considering the bumps on the skin of the model . Regardless , I had never liked standard skin treatments offered by the tools available in a traditional color grading software . With the usual beauty clean up , the skin always lost it 's natural texture and it looked quite a bit unnatural . So , I now had a chance to play with the new version of Baselight 5 and here is a quick result of before and after ( actually after and before ) using the new Paint and Texture Equalizer tools in Baselight 5 . <p> That 's the point of this post . Up to now , most of this kind of work was either done in something like Flame , Nuke or with a combination of Mocha/Photoshop . Baselight 5 is the fist grading software , that I see , that is @ @ @ @ @ @ @ @ @ @ , retouching , use of 3D models and GLSL shaders . <p> Baselight 5 is the fist grading software , that I see , that is truly capable of very complex VFX work , including compositing , retouching , use of 3D models and GLSL shaders. 43971 @qwx453971 <p> Pablo and Lustre can do a few things to a point , as can Mistika . In Lustre 's case , they basically have a Flame guy down the hall working on the same material , which I think might be more efficient for some operations . I would agree that all top-end color-correction programs would benefit from having some basic paint features and some expanded VFX features , particularly on removal tools and better tracking tools . <p> Pablo and Lustre can do a few things to a point , as can Mistika . In Lustre 's case , they basically have a Flame guy down the hall working on the same material , which I think might be more efficient for some operations . I would agree that all top-end color-correction programs would benefit from having some basic paint @ @ @ @ @ @ @ @ @ @ tools and better tracking tools . 43971 @qwx453971 <p> Yep , perfect logic , in order to do this in Lustre , just use Flame Hey , here is an idea , from now on ALL grading software can have all these capabilities , as long the Flame guy is doing all the work . Problem solved ... The difference , this was done VERY quickly , literally you could do this while clients are watching and there is no need for a second VERY EXPENSIVE VFX guy to be on call . Hey Marc , do you have a VFX guy in your facility available with Flame ready to go on a moment notice when you 're working ? What do you do , when you need to do a little beauty clean up , slimming of an actress in a moving shot or to paint out an object ? <p> Hey Marc , do you have a VFX guy in your facility available with Flame ready to go on a moment notice when you 're working ? What do you do , when you need to do a @ @ @ @ @ @ @ @ @ @ a moving shot or to paint out an object ? 43971 @qwx453971 <p> Naw , I know enough local LA out-of-work VFX people I throw them the job so they can get paid and make a living . I stick to what I do best -- just color -- and hire a good specialist to do what they do best . <p> There 's always a danger of throwing the kitchen sink into software to the point where it potentially could bog down . I think ( for example ) if one piece of software had every possible feature for conforming , editing , color , VFX , and sound in one package , there 'd be 9000 menus , 3700 buttons , and a manual about 7000 pages long . Would n't it be better to have maybe four programs -- editing , color , VFX , and sound -- and hire individual artists to do each task well ? <p> Having said that ... I just spent a week doing minor VFX tasks in Resolve for a feature , including muzzle flashes , blood hits , sky replacement @ @ @ @ @ @ @ @ @ @ minor jobs . If it were anything extensive , I 'd leave it to the pros . <p> I did mention I really like Baselight , right ? And Peter Postma might be in the top two or three best color scientists I 've ever worked with ? ( And also a good person and a stand-up guy . ) Cost no object , Baselight and Blackboard 2 would be very hard to argue with for any facility or any job . <p> Having seen BL5 paint recently with Sebastian and Hat over at Filmlight , I would have to say what 's available in those apps is n't remotely comparable from a capabilities and usability standpoint . It 's a fully realized paint tool with perspective tracking , that 's easy to use . Flame/Flare integration in Lustre is capable of similar things , but with significantly more effort . Pablo is famous for paint , but it 's not nearly as advanced . Mistika is probably a notch above that , and I will give it props for it 's temporal fix capabilities , but it 's still @ @ @ @ @ @ @ @ @ @ with perspective tracking , and ability to implement Baselight 's color and texture tools within it as a trackable mattes , has no equivalent that I can think of in another color corrector . Seriously . As a feature it 's probably worth $20,000 all by itself . <p> Is there a possibility that some of these paint tools would make it into the Editions versions of Baselight ? <p> Decent paint tools always seems to be one of those sorely lacking features in editorial finishing apps . Tracking a still patch across the scene or offset-cloning just does n't cut it . <p> I 'd buy Editions just for that function alone . 43971 @qwx453971 <p> I doubt it very much . It only makes sense to restrict the Compositing , Paint , use of GLSL Shaders and other high end features to Baselight only . Plus for all of this you will need some serious horsepower , which is difficult in case of BLE , as it must rely on the underlying foundation of AVID and hopefully soon , Premiere . On the other hand , the new @ @ @ @ @ @ @ @ @ @ of admission to the FilmLight 's world of color science . <p> Decent paint tools always seems to be one of those sorely lacking features in editorial finishing apps . Tracking a still patch across the scene or offset-cloning just does n't cut it . <p> I 'd buy Editions just for that function alone . 43971 @qwx453971 <p> i have three options at hand , one is using DS for everything , it 's paint system is pretty awesome , and when i work on commericals , it 's almot always in DS , what it gives n speed and grade management , it more than gives back in flexibality , comp and paint tools two is using BCC with Mocha in Resolve , slow due to OFX nuttyness tho , but very capabile , like Marc . light duty stuff that 's faster to do inteh box than it is to export , check , wait , import , check , cut in three is roundtripping to Fusion - that cuts out some of the I/O but you canot see the comp in context <p> so what does @ @ @ @ @ @ @ @ @ @ reasonable video card ? or does it have to be a trashcan ? <p> i have three options at hand , one is using DS for everything , it 's paint system is pretty awesome , and when i work on commericals , it 's almot always in DS , what it gives n speed and grade management , it more than gives back in flexibality , comp and paint tools two is using BCC with Mocha in Resolve , slow due to OFX nuttyness tho , but very capabile , like Marc . light duty stuff that 's faster to do inteh box than it is to export , check , wait , import , check , cut in three is roundtripping to Fusion - that cuts out some of the I/O but you canot see the comp in context 43971 @qwx453971 <p> I feel a little more of the context may help here . So , once I figured out the process ( no manual needed here , which in itself is ridiculous ) , it took me a little more , that 15 minutes to do this @ @ @ @ @ @ @ @ @ @ this paint tool allows more than just cloning . This paint tool can be used for anything- dodging and burning , local contrast , saturation , noise reduction , sharpening and softening using the same tool- Texture Equalizer ( amazing tool in it 's own right ) , literally , any operator you can apply in Baselight can be used to paint on , using transfer modes while controlling the amount with keyframes . Moreover , the Paint itself can be restricted by various keyers and shapes with variable softness and painted matte itself can be defocussed . And of coarse , all this craziness can be tracked in perspective a-la Mocha . So , I hope people do not miss the forest behind the trees . This is WAY more , than just a standard paint tool . This is a whole new ballgame . Marc can continue to make his snide and dismissive remarks , but fact of the matter , there is noting like it on the market designed for color grading , other than Photoshop <p> And this paint tool allows more than just cloning . @ @ @ @ @ @ @ @ @ @ burning , local contrast , saturation , noise reduction , sharpening and softening using the same tool- Texture Equalizer ( amazing tool in it 's own right ) , literally , any operator you can apply in Baselight can be used to paint on , using transfer modes while controlling the amount with keyframes . So , I hope people do not miss the forest behind the trees . This is WAY more , than just a standard paint tool . This is a whole new ballgame. 43971 @qwx453971 <p> that 's also is discribeing DS 's paint system pretty well , anything , includeing entire comp trees can be brushstrokes ... and the brushstrokes can be tracked ... incredably usefull once you have your head around it , that sounds like a good fit for my prefered way of working , maybe enough to get me off of DS , Resolve + OFX is not up to the task , neither it seems is Resolve + Fusion <p> I doubt it very much . It only makes sense to restrict the Compositing , Paint , use of GLSL Shaders @ @ @ @ @ @ @ @ @ @ for all of this you will need some serious horsepower , which is difficult in case of BLE , as it must rely on the underlying foundation of AVID and hopefully soon , Premiere . On the other hand , the new Basegrade in BLE alone is worth more than the price of admission to the FilmLight 's world of color science . 43971 @qwx453971 <p> I had the impression that Filmlight would translate at least the render capabilities of those tools to BLE . The interface would be amazing and would obliterate the use of several of my expensive plugins but I do understand that might be difficult . One can only hope . <p> Martin 's words : <p> We ca n't ship v5.0 until we have v5.0 versions of Daylight , Baselight for Avid , Baselight for Nuke and Prelight available in beta , or people would be producing BLGs which a lot of other people could n't read , bringing confusion and a bad experience . It 's taking us longer than expected to produce these v5.0 versions of all our other products . <p> I @ @ @ @ @ @ @ @ @ @ render capabilities of those tools to BLE . The interface would be amazing and would obliterate the use of several of my expensive plugins but I do understand that might be difficult . One can only hope . <p> Martin 's words : <p> We ca n't ship v5.0 until we have v5.0 versions of Daylight , Baselight for Avid , Baselight for Nuke and Prelight available in beta , or people would be producing BLGs which a lot of other people could n't read , bringing confusion and a bad experience . It 's taking us longer than expected to produce these v5.0 versions of all our other products . 43971 @qwx453971 <p> Yes , of coarse . It is imperative , that all FilmLight 's platforms , including BLE , Daylight and Baselight could open each other BLGs . My understanding , that is the reason why all FilmLight 's software was completely re-written from the ground up in order to have a common code across all of it 's platforms . That is why it took longer to implement V5 . So , for example , BLE @ @ @ @ @ @ @ @ @ @ , but it will not necessarily open a shader layer , because you may not have that particular shader available on the system used . But otherwise , the open BLG grade stack still will be valid . <p> Any Baselight product ( including Daylight , Prelight , Baselight for Avid and Baselight for Nuke ) will be able to load any v5.0 BLG and render its contents , including any Paint , GridWarp , Perspective operators . However , some of the operators will be read-only in Daylight , Avid &amp; Nuke ) - you wo n't be able to edit the contents ( other than moving keyframes ) or create new instances of them . The full list of restricted operators is : <p> Paint <p> GridWarp <p> Perspective ( this is the corner-pin operator , not the Perspective tracker which *will* be included in everything ) <p> Relight &amp; MatteXYZ <p> Shader <p> OFX <p> Sorry if this is disappointing to people , but all the new colour grading tools ( BaseGrade , CompressGamut , BoostContrast , BoostSaturation etc ) and the new texture management tools ( @ @ @ @ @ @ @ @ @ @ It 's great , Martin . I was hoping for paint because the paint tools on Avid are really lacking and it would be very usefull for the random boom fix on my offlines . Now , i use a combination of fusion with avid , and it 's not very great . Sometimes , I use BLE just for reframing and some stabilization . It 's a great offline tool and I keep telling my students about it . In fact , their reaction is always " Oh I should get avid to get this " . <p> So , I wanted to revisit an exercise on a beauty clean up I have done recently for two reasons . First , I saw a wonderful tutorial by amazing Ivar Beer using a couple of Matchbox shaders , that can be found here : <p> I was intrigued , as upon closer inspection I found my first attempt at beauty clean up not quite satisfactory . So , I wanted to see , if I could do better . And for the second reason , recently , as a part @ @ @ @ @ @ @ @ @ @ . So I was curious to see just how well this tool would compare to the manual painting tools available in Baselight . In this new video there are two grades . The first one was done on Baselight using Texture equalizer applied with paint brush and then matchbox was used to generate the new skin texture , which again was applied by a second brush . Unfortunately , it is not quite the same , that was done by Ivar , but you get the general idea . I did n't spent much time doing it , I 'd say 10 minutes max . The second video was done on Resolve 14 and I used the Face refinement tool . In the second grade , I attempted to match the basic grade and the skin texture of the first grade . Both grades used the exact same material- Red Epic 5k . I will let you see the result for yourself . If anyone would like to recreate this test for yourself , feel free to contact me offline and I 'll provide the same material for your personal test . Here is the result : 
@@44332905 @4332905/ <h> r3d debayer settings <p> Curious how most of you deal with debayer settings for r3d or even alexa for that matter . I do work in lustre but am curious how the rest of you deal with settings . Often I 'll at least take a look at camera settings as defined on set to get an idea of what the DP was seeing before the customary monitor tweak . But then I 'll grade in RedLogFilm , usually without a LUT . Some of the initial balance can be done directly in the debayer settings , at least to neutralize colorcast and to white balance . This can also be done in the grade . But is it better to start with a more neutral image off the debayer ? <p> i work in ACES noprmaly , so the onset stuff is useless anyway , i take on-set exposure , temp and iso , set the rest to default , and look at the offline refrence for an idea of the overall feel on the base pass . <p> but i almost always have communication with the @ @ @ @ @ @ @ @ @ @ my door because they asked for me and like the work i do , so there 's some street cred from the get go , and i also have some lattuide to veer away from the DiT art awyway <p> Curious how most of you deal with debayer settings for r3d or even alexa for that matter . I do work in lustre but am curious how the rest of you deal with settings . Often I 'll at least take a look at camera settings as defined on set to get an idea of what the DP was seeing before the customary monitor tweak . But then I 'll grade in RedLogFilm , usually without a LUT. 43971 @qwx453971 <p> That 's what I do , but I try to get a Reference Movie from the client showing what they 've been looking at from dailies during editing . At least this gives me a point of reference and an understanding of where to go from there . <p> I think if ISO , Exposure , and Color Temperature are set correctly , then the rest is pretty straightforward @ @ @ @ @ @ @ @ @ @ on the camera ) and RedLogFilm have been fine . <p> Thanks Mark and Dermot . I 'm on a slow transition to ACES so , I 'm not sure that 's going to be my go to for now . But as you say Mark , if they 're set correctly . I 'm not sure I always get temperature set correctly . I 'm happy to get into the weeds and tweak settings in the debayer . But perhaps as I move into ACES this will be a non issue . <p> I can tell you that on set , I assume the colorist is going to toss out much of what I do . I 'm flattered if you do n't toss all my CDLs or RMDs , but I 'm not hurt if you start from square one , rather , I expect it . <p> The flip side of that is that if the DP walks away from set thinking " Man , everything looked great today , that DIT put the image right where I wanted it , " then I 'm going to @ @ @ @ @ @ @ @ @ @ In theory , the image you get from set should be close to the intended final output . Well , as close as possible given that we do n't have windows , gradiation , or keying in CDLs/RMDs . So if everything is working according to plan , taking a look at the output from set should put you in the ballpark . <p> But I still expect , at the very least , for the Colorist to switch form RedGamma4 to RedLogFilm . Or on Arri content , to ditch the set LUT , and start from Log-C . I just did a job where I only used RMDs and did n't use CDLs because I know the post people really well . If Post , Production , and the people on Set are all on the same page , the tools within an RMD are really , really powerful and flexible . But since the communication and intentions are often not as fluid as they should be , RMDs are often misused . So yeah , if you toss them out , I get that . <p> those @ @ @ @ @ @ @ @ @ @ , final ... i get linear and that is the only choice , all others are grey 'd out and linear is one option i really like .... <p> i can use transforms once in the system to compreess the dynamic range to match RFL , but why on earth would i kneecap the images like that ? makes zero sense ... <p> that said i do look at the offline ref , often the editor has burnt in some color choices made in the editorial process over top of the Dit 's work <p> i do not get a sense the DP 's i work with are thrilled wiht the dialies , more like ; " good enough for the editor , we 'll get it rignt in gradeing " 43971 @qwx453971 <p> Copy that on the ACES front ; I have yet to work an ACES project , so I have a bunch of reading and learning to do there . <p> I 've certainly worked on " Eh , good enough " projects , but I 've also had the fortune of working on a few features @ @ @ @ @ @ @ @ @ @ the intent . At least , so I was told ( only one of those projects has been released so far , and there was a bit of drama around the final grade , unfortunately ) . But there again , I could n't even begin to speculate whether or not the editor is applying any changes on top of what we delivered . <p> I can tell you that on set , I assume the colorist is going to toss out much of what I do . I 'm flattered if you do n't toss all my CDLs or RMDs , but I 'm not hurt if you start from square one , rather , I expect it . 43971 @qwx453971 <p> @Dan Moran did a good MixingLight piece recently on how he initially attempted to veer from the temporary LUT on-set , but the client wound up being happier with the original look , so he returned to that . I would 've done the exact same thing -- you can tactfully disagree with the client to a point , but ultimately it 's their film , and @ @ @ @ @ @ @ @ @ @ they 're paying for . <p> There 's a pretty big sci-fi film that was recently in theaters , and the word is that the director and DP came up with a terrific look with the colorist ... but then at the end of the project , the studio execs and the executive producer overruled them and they wound up going back to the original dailies look , which was much darker and more subdued . Surprisingly , it wound up winning a few Best Cinematography awards . This kind of thing happens . Color -- like everything else -- is just an opinion . <p> I would always rather give them 90% of the dailies look only without blown-out highlights , crushed blacks , everything matching exactly , and windows brought in to even things out and add fill and reduce background spill when necessary . A brand-new look is fine , too -- whatever makes them happy . <p> So getting back to the original question ( although the info here is great ) , even if the camera settings from the RED footage seem quite a bit @ @ @ @ @ @ @ @ @ @ still leave them alone and correct in your grading software . Adjusting temp in debayer settings does n't improve your base for your grade ? <p> So getting back to the original question ( although the info here is great ) , even if the camera settings from the RED footage seem quite a bit off ( color temp , tint ) , do you still leave them alone and correct in your grading software . Adjusting temp in debayer settings does n't improve your base for your grade ? 43971 @qwx453971 <p> Tint you could probably compensate for , but color temp and ISO really need to be right in the Raw settings . I will sometimes knock it down a little bit , like from 800 to 600 , just if I think there 's a tiny bit more highlight detail there . <p> I seem to recall @Patrick Inhofer did an investigation sometime back on MixingLight , and he found whether you adjusted ISO in the Raw or just compensated with curves or primaries , it did n't make much difference beyond a certain point . Color @ @ @ @ @ @ @ @ @ @ the DP 's original settings from production on ISO , exposure , and color temp , but I 'll sometimes make some adjustments if it seems to help . My preference is to " normalize " the material in the first two nodes ( at least in Resolve ) , but that includes Offset and Raw as well as an overall color-correction in RedLogFilm. 
@@44332906 @4332906/ <h> New CG2730 and grading for indie doc <p> I 'm a photographer working on my first very low-budget documentary . I ca n't be sure that I 'll wind up with the budget for a colorist , which would be my preference of course , so I may be falling back on my stills colouring skills . I 'm using PP and the studio version of Resolve for colouring . <p> To the point--based largely on threads here at LGG I 've purchased the CG2730 which should arrive today . I am a little concerned though that perhaps I should have spent more on the CG277 instead , as after I ordered it I saw the Eizo chart saying the 2730 is for post production but the 277 is for " professional " post production in video . I 'm hoping it is only based on the 3D LUT rather than 16-bit LUT , but I also notice things like Rec709 presets listed in the 277 that are n't listed on the 2730 . <p> Have I made a mistake getting the CG2370 for grading my doc for @ @ @ @ @ @ @ @ @ @ ) , rather than the CG277 ? Will the CG2370 at least be very good , when perfection might be a standard I 'm not aiming for ? <p> Again based on what I 've read here , I plan to grade in REC 709 rather than P3 , even though the monitor covers 98% of P3 . I 'm on a nMP six-core with the D700 's , and do hope to get 10-bit colour with this setup but I 'm not even sure about that because I do n't have some kind of BM box that people talk about . <p> You 'll need a Decklink I/O card or if you 're on TB Mac , you 'll need at least a TB ' mini monitor ' ( there 's HD or 4K versions ) to guarantee you get a proper 10bit ' video ' feed from your computer to your monitor . Without one of the above , you will not see a reliable video picture out of Resolve , only a potentially compromised approximation . <p> If your new monitor is a 10bit ' video ' @ @ @ @ @ @ @ @ @ @ . If it is a ' computer ' monitor , then you may need to use a BMD SDI to Display Port converter ( I use the HD-Link 3D Pro ) or at least use the ( mini monitor 's ) HDMI out . If you select the HDMI option , you 'll need to be sure your new monitor will accept and display a reliable 10bit signal via it 's HDMI input . ( then comes the calibration ) <p> seems like that model is only available in Europe or something ? I do n't really see a lot of information about it sorry . <p> As you 'll read through the threads here on the forums , as I have been , it 's all about the depth of deep blacks . So unless the monitor can get down to 0.05 nit range " technically " speaking no it is not good for professional level grading . <p> NOW having said that I have graded a ton of projects the past few years that have screened in film festivals on large theatre screens , been sold to streaming @ @ @ @ @ @ @ @ @ @ using nothing but DaVinci Resolve , spyder 4 color calibration system and a decent monitor . I religiously use scopes and always checking the output on a dozen devices to get a good sense of my output . Yes the results have been pretty good but this process and the doubt is a huge waste of time and makes stress on me for no reason . Now I am in market for FSI or maybe CG247x .. but not happy about the viewing angle on the CG247x . <p> My only advise is get what you can afford but plan to replace it and upgrade as soon as you can - or bypass this and buy once cry once . Get a proper monitor that works for professional grading and as many people here on the forums have been teaching me WORKS ON THE BACKEND as well , . I believe this is critical too . Especially if you say you are using PP , how will you get a LUT on your monitor then for colouring from PP ? I do n't think Adobe does this in any way @ @ @ @ @ @ @ @ @ @ but it 's using your processing and therefore you take some level of a hit on performance . <p> I 'm learning all this is giant can of worms , and to avoid this you buy FSI monitor or high end Eizo with LUT capability , decklink card and you are done . Sort of . <p> I opted for BenQ 's PG2401PT as it has 14bit 3D Luts on board with a Rec.709 preset and GBr backlighting . At what , $800 ( B&amp;H ) it has proved to have been an excellent investment over the past three years . I drive it with a Decklink Pro SDI 4K and a HD Link SDI to Display Port converter as it is 10bit on DP only . For the work we do and the clients we work with , it is the first time most of them have seen their own ( 4K ) work look so good so it has proven more than adequate . <p> We are still waiting for BenQ to allow user calibration , so that we can integrate LightSpace CMS with it . Their most @ @ @ @ @ @ @ @ @ @ been moving backwards for near a year , so we are not certain the new June deadline will be met . <p> The Eizo CG270 has no 3D LUT capability , so can not be accurately calibrated ( it uses a 1D LUT and 3x3 matrix ) . The CG277 has full 3D LUT capability , and can be LightSpace CMS calibrated . See : http : **39;1483;TOOLONG 
@@44332907 @4332907/ <h> Mac Book Pro GTX 650M Specs &amp; Comparison <p> Apple has moved to the GTX 650M in their new laptops , which is a welcome change for a lot of us using CUDA . And it 's been a while . The last portable to include Nvidia Graphics was the 2008 Mac Book Pro with the 512 MB , GeForce 8600M GT with all of 32 CUDA cores . Some apps a lot of us use that will see a benefit from this change include : <p> To see where the GTX 650 sits compared to GPUs we already know , take a look a this article from Notebookcheck.com : <p> The NVIDIA GeForce GT 650M is a mid-range , DirectX 11.1 compatible graphics card that was announced in the first quarter of 2012 for laptops . It is a Kepler-based GPU built on the GK107 architecture and is manufactured in 28nm at TSMC . The graphics card uses a 128-Bit wide memory interface with either the more common but slower DDR3 for VRAM or the more expensive and faster GDDR5 . Due to a higher core clock @ @ @ @ @ @ @ @ @ @ faster than the 640M . <p> ArchitectureThe Kepler architecture is the successor to the Fermi architecture that first appeared in laptops with the GeForce 400M series . The GK107 Kepler core offers two shader blocks , called SMX , each with 192 shaders for a total of 384 shader cores that are clocked at the same speed as the central core . Although more shader cores are available in the Kepler architecture as compared to the Fermi design , the Kepler shaders are still expected to be up to twice as power efficient . However , due to the missing hot clock of the shader domain , two shaders of a Kepler chip are about as fast as one shader of a Fermi chip ( as the latter is clocked twice as fast ) . PCIe 3.0 is now supported by the mobile Kepler series and an optional Turbo mode can automatically overclock the Nvidia card by a theoretical 15 percent if the laptop cooling system allows it . The implementation of this boost mode is done in the BIOS , but it is ultimately dependent upon the manufacturer of @ @ @ @ @ @ @ @ @ @ GT 650M equipped with DDR3 graphics memory lies somewhere in the former 2011 high-end category between the GeForce GTX 460M and GTX 560M . The performance is exceptionally good in shader-heavy DirectX 11 games and benchmarks . However , the 128-Bit memory interface can be a bottleneck if DDR3 graphics memory is employed . Despite the slower core clock of only 735 MHz , the GDDR5-version of the card should be much faster . Demanding games of 2011 like Battlefield 3 will be playable in 1366x768 and medium or high settings . Less demanding games , such as Modern Warfare 3 , are easily playable with maxed out settings and 1080p resolution . <p> FeaturesThe improved feature set now includes support for up to 4 active displays . Furthermore , high resolution monitors of up to 3840x2160 pixels can now be connected using DisplayPort 1.2 or HDMI 1.4a if available . HD-Audio codecs , such as Dolby TrueHD and DTS-HD , can be transmitted via bitstream mode through the HDMI port . However , as most laptops will feature Optimus , the integrated GPU will likely have direct control over @ @ @ @ @ @ @ @ @ @ by the Nvidia Kepler cards . <p> The 5th generation PureVideo HD video processor ( VP5 ) is also integrated in the GK107 core and offers hardware decoding of HD videos . Common codecs such as MPEG-1/2 , MPEG-4 ASP , H.264 and VC1/WMV9 are fully supported up to 4K resolutions while VC1 and MPEG-4 are supported up to 1080p . Two streams can be decoded in parallel for features such as Picture-in-Picture . Another novelty is the inclusion of a dedicated video encoding engine similar to Intel QuickSync that can be accessed by the NVENC API . <p> The power consumption of the GeForce GT 650M should be best suited for medium-sized notebooks 15-inches or greater . 
@@44332908 @4332908/ <h> Colorist Podcast 011 : Dave Abrams <p> On this episode of the colorist podcast , we 're trying something a little different . We take a break from talking with a colorist to focus on something quite a bit more technical . Dave Abrams , monitor calibrator and owner of Avical , joins me on this episode . It 's about to get seriously geeky , and in the best possible way <p> Dave has been calibrating monitors for over seventeen years . He 's been through the transitions from SD to HD , tape and film to file based , and now 4K and HDR . Post facilities from all over the world rely on his expertise and knowledge of monitors to get the most consistent and accurate monitoring possible . High end home theater owners also use his services to get great looking images at home . And seeing both sides of each situation gives him a great perspective on the industry as a whole . <p> On this podcast , we talk about : <p> How HDR is changing the approach to calibration <p> The @ @ @ @ @ @ @ @ @ @ with your instincts <p> The HP Dreamcolor , Eizo , and other computer monitors for color critical evaluation <p> Room lighting considerations when setting up a grading room <p> Comparing OLED , plasma , LCD monitors <p> Considerations for large size client monitors used in combination with hero monitors <p> Projects that are destined for web distribution and how to monitor for them <p> Where the future is going for monitoring and what they still are lacking <p> I 'm a bit late to the party as I had n't even realized you 've been doing this podcast for quite a while now . Great episode - thanks for putting it together . <p> Dave makes a good point regarding there being a real gap in the market for a reasonably priced grading panel that 's designed for monitoring 4K HDR work . It 'll be interesting to see if NAB holds any new surprises . I know FSI have said in the past that they 've been working on 4K solutions for a while now . It 'd be great if they have something to show <p> I agree @ @ @ @ @ @ @ @ @ @ Scientific DM250 as our reference monitor in our calibration lab-- they are superb and extremely versatile . We 've been trying to pry on what is coming at NAB , but unfortunately , they seem to be tight lipped ... I guess we 'll just need to swing by the booth and find out ! <p> Definitely will keep an eye out for any news from them . I 'm sure they 're just as eager to get a panel out there once the economics make sense to do so . <p> I 'm currently speccing equipment for a new mastering room I 'll be building in the near future and from what I can see at the moment , the BVM-X300 looks to be the only game in town for doing 4K HDR Mastering and QC work . It just absorbs such a huge chuck of the budget , that you really need to already have the work lined up in order to justify it . <p> I wonder if Dolby plans on bringing out a 4K version of their panel any time soon . I feel like I @ @ @ @ @ @ @ @ @ @ years now . Not that that will help address the more financial constrained scenarios ... <p> I wonder if Dolby plans on bringing out a 4K version of their panel any time soon . I feel like I 've been waiting for that to drop for a few years now . Not that that will help address the more financial constrained scenarios ... 43971 @qwx453971 <p> I got the impression from the Dolby demo in LA yesterday that they do n't really want to be in the display business -- they want to instead encourage other companies to be licensed to make Dolby Vision products . <p> I agree with Marc , from what I 've heard from the Dolby Team was that the monitors were part of their research , development , and proof of concept . Once that ball got rolling they were hoping for partners to pick up the slack and make the displays while they handled the IP side of things ... Of course , you never know with Dolby , lots of talent and lots of capability over there to do essentially whatever they want @ @ @ @ @ @ @ @ @ @ that either Sony or their Partners can release a version of the BVM-X300 at a more aggressive price point ( $10k ) , but that may be too aggressive given the costs involved . Still a $15,000 version would be huge for the industry . I guess we 'll see what happens in a week ... 
@@44332909 @4332909/ <h> Sony F55 and FS7 underexposed issues ... <p> I 've graded enough projects now with these two cameras to have my own opinion but would love to hear about other peoples experiences and solves or success stories . <p> Consistently I am seeing really horrendous noise from these cameras when used doc-style , with blue and red channel noise so bad that it almost looks like RF interference . <p> It 's predominately SLog3 material recorded XAVC ( interframe ) ... but way under-exposed IMHO ... with mid-tone detail down around or sometimes below 30% . Metadata in Sony 's software shows it 's setup for ISO 1200 with no adjustment from there or sometimes pushed hotter . But the lighting environments I am seeing do n't seem to call for that ( more like 320-500 range ) like daylight exteriors . <p> It makes me wonder if this is just a case of exposing for the shadows while in Log or the wrong kind of viewfinder LUT , over use of the ISO as a tool for exposing ( all user issues ) or do these cameras @ @ @ @ @ @ @ @ @ @ just fail to record enough dynamic range that it 's forcing shooters to close way down to protect highlights too aggressively ? <p> cheers , <p> bk 43971 @qwx453971 <p> Not sure if it 's your footage or just a project you 're working on , but the camera is literally a rubix cube when it comes the combination of settings , modes , internal LUTs and exposure monitoring tools that I think most people inexperienced with the F series camera mistakingly just grab it and shoot without knowing really how to use the camera . <p> When you pull in any slog2/3 footage into Resolve this is where levels should be sitting as a baseline : Anything that is 90% white ( like piece of paper ) should be around 60% IRE and middle grey tones around 40% for slog3 and 30% for slog2 . <p> If the camera was in CineEI mode there are a tone of gotcha 's that could play into why there were mistakes shooting with the camera . 1 ) In CineEI mode the default internal Noise Reduction setting is OFF . This needs @ @ @ @ @ @ @ @ @ @ high in my opinion . I would not shoot with it off unless you intend to run everything through NeatVideo NR afterwards which is very time consuming and processor intensive as I 'm sure you know . 2 ) Some people unfamiliar with how the index mode works on the F series camera 's might actually be adjusting the ISO higher thinking they are making the image brighter and recording a brighter image when in fact going higher in ISO in CineEI mode only affects the viewfinder not the internal recording so if they increase the ISO they would likely lower the lens aperture to " expose the image correctly " but in fact the image being record will only end up dark and underexposed . In index mode what you actually want to do is set the ISO LOWER to 640-800 ISO for the F55 and 1000-1600 for the F5 or FS7 thereby producing internally recorded footage that is 1 stop OVER exposed but looked normal in your viewfinder . <p> Under the gun and pressure to shoot it 's likely the time needed to thoroughly go through the @ @ @ @ @ @ @ @ @ @ These camera 's are quite complicated little beasts and even now after owning my F55 for three years I still get baffled on occasion when a menu item is greyed out or missing for some reason and I have to go dig around to some other menu option to find out what I need to turn on or off to get it to work the way I want . But on the exposure side of things alone , I think many people lack the information they need to learn how to expose using the camera , and in fact so much so that I ended up publishing a 40 page 8 x 10 " e-book series called " The Ultimate Exposure Guide for Sony F5/F55 " which goes over exposure in immense detail based on everything I have learned about working with this generation of Sony F series camera 's and the previous one . <p> When you pull in any slog2/3 footage into Resolve this is where levels should be sitting as a baseline : Anything that is 90% white ( like piece of paper ) should be around @ @ @ @ @ @ @ @ @ @ and 30% for slog2. 43971 @qwx453971 <p> Unfortunately , it 's not that simple ! <p> Those IRE values from Sony 's white paper are the ones you would see on a waveform connected to the SDI output of the camera when shooting . They are not the values you should expect on the Resolve waveform . <p> The waveform connected to the camera will read 0% IRE for code value 64 on its SDI input , and 100% IRE for code value 940 . But *correctly interpreted S-Log ( 1 , 2 or 3 ) in Resolve should be read as " Full " range in the Clip Attributes , meaning that , for example , code value 95 , which is S-Log3 black will read as 3.5% IRE on the camera waveform , but about 9% on the Resolve waveform . The differences are most significant for blacks , as they are near the outside of the range . The effect of range scaling is less significant in the middle , so " about 60% " and " about 40% " is still reasonable . " About 30% @ @ @ @ @ @ @ @ @ @ mid grey . <p> *Resolve will " correctly interpret " S-Log internal recordings as " Full " based on the metadata . However external recordings such as ProRes HQ from an Odyssey 7Q will be interpreted as " Video " . For this reason I modified the LUTs I created for Convergent Design to work correctly with S-Log interpreted this way . <p> Are you saying that when the slog 3 footage from the HDMI/SDI is recording as proves , to a recorder you should set resolve video levels to full ? 43971 @qwx453971 <p> Depends what you are doing with it . If you are grading " by hand " from the S-Log3 , it does n't matter . If you are using the ACES S-Log3 IDT , RCM , or the LC709A LUT which ships with Resolve then yes . If you are using the modified LC709A LUT downloaded from Convergent Design then no . <p> Guys sorry I did n't mean to imply that the values from the camera IRE and code value chart would be identical to the scopes in DaVinci - they certainly are not @ @ @ @ @ @ @ @ @ @ in Resolve anyway . In full context of this thread , which is about why some footage might be underexposed by DP 's , the point is to know the values a DP works with to set exposure for a specific gamma curve on a camera which is mostly going to be with specific reference to middle grey and sometimes white . <p> As a DP myself and F55 camera owner , I mainly wanted to she 'd some light on why I think so many people screw up exposing on the Sony cameras . <p> Those IRE values from Sony 's white paper are the ones you would see on a waveform connected to the SDI output of the camera when shooting . They are not the values you should expect on the Resolve waveform . <p> The waveform connected to the camera will read 0% IRE for code value 64 on its SDI input , and 100% IRE for code value 940 . But *correctly interpreted S-Log ( 1 , 2 or 3 ) in Resolve should be read as " Full " range in the Clip Attributes , @ @ @ @ @ @ @ @ @ @ which is S-Log3 black will read as 3.5% IRE on the camera waveform , but about 9% on the Resolve waveform . The differences are most significant for blacks , as they are near the outside of the range . The effect of range scaling is less significant in the middle , so " about 60% " and " about 40% " is still reasonable . " About 30% " becomes more like " about 35% " for S-Log2 mid grey . <p> *Resolve will " correctly interpret " S-Log internal recordings as " Full " based on the metadata . However external recordings such as ProRes HQ from an Odyssey 7Q will be interpreted as " Video " . For this reason I modified the LUTs I created for Convergent Design to work correctly with S-Log interpreted this way . 43971 @qwx453971 <p> V-Log also uses data levels , one of the things I notice with V-Log is that Resolve is not able to correctly interpret the data levels using ProRes 442 ( setting Data instead of Auto in the Clkip Attributes makes it worse ) . However if @ @ @ @ @ @ @ @ @ @ money . <p> V-Log also uses data levels , one of the things I notice with V-Log is that Resolve is not able to correctly interpret the data levels using ProRes 442 ( setting Data instead of Auto in the Clkip Attributes makes it worse ) . However if I encode with ProRes 444 it is right on the money . 43971 @qwx453971 <p> I think your issue with V-Log is a separate one , related to how you are creating the ProRes from the . mp4 . In my ( limited ) experience , V-Log recorded from a Varicam LT as ProRes HQ on an Odyssey 7Q reads correctly in Resolve when interpreted as " full " . <p> Like the Sony table , the IRE values shown are what you would see from the camera output . For some ( LogC , REDlogFilm , Canon Log , BMD Film ) they are also what you see in post . For the S-Logs and V-Log they are n't . ACESproxy should n't exist in post , but ACEScc IRE values would match those shown for ACESproxy if you could @ @ @ @ @ @ @ @ @ @ in Resolve ! ) 
@@44332911 @4332911/ <p> I would recommend to use 16-235 video levels . The color space of full range may be misinterpreted by Premiere depending on the pixel format that is used by the encoder in the Fuji camera . <p> Some effects , many plugins and other Premiere operations do either clip or incorrectly treat out of range values so I think it is important as a first step to correct the out of range values and put them into legal range ( ProcAmp and the Lumetri curves work correctly ) . <p> It is also important not to clam the signal in your scopes otherwise you will not see the out of range data . <p> If you upload ( for instance with DropBox ) a few frames of both full and limited range video we can inspect it . <p> They are all full range and have a pixel format of yuvj420p , which is interpreted by Premiere as a Rec.601 file . ( The mov container has Rec.709 Color Primaries and Rec.601 transfer characteristics and matrix coefficients so perhaps Premiere converts it actually from Rec.601 to Rec.709 but @ @ @ @ @ @ @ @ @ @ would avoid that format and use 16-235 , that is the industry standard for non-log encodings . <p> Looks legal to me . In premiere you can run a video limiter that ensures no luminance values are out of range . <p> Maximum levels are way below 236 , you speculars are at 80IRE . Of course it is a matter of taste but I personally think speculars deserve a higher IRE and in this case you would probably want to warm them up a bit and lower the midtones , after all it is a night shot . <p> As advised above I did convert from 709 to 601 using following command : <p> ffmpeg -i input -vf colormatrix=bt709:bt601 output <p> Above command convert 4k 709 . MOV file to 4k 601 . MOV file . <p> Original file is recorded in camera . <p> Here is a problem : The original file is 100mbps and 1min footage is 1GB <p> The converted file is 5mbps and 1min is less then 100MB <p> Why output file is so small ? <p> But resolution vise it is as sharp as @ @ @ @ @ @ @ @ @ @ output file . After color correction in Premier they are still stable . Red color is no more blowing out . Should we report this issue to Fuji or Adobe ? <p> Incase someone wants to take a look on the few frames . See link below : <p> Because you did not specify a codec and settings so H.264 is used with default settings . If you want to stick to H.264 you could add -crf 15 ( lower is better , if you go below 10 you get into visibly lossless territory ) to get a decent quality encoding . <p> As Cary says , it 's a creative choice , but if your whites are this low just to make them broadcast safe , I would say you are being very overcautious. 43971 @qwx453971 <p> A long time ago , I replaced a long-time staff employee because he was being way too overcautious with peak levels on a network show . After the network complained about low average program levels prior to broadcast , the producers had me come in and check the master then do a @ @ @ @ @ @ @ @ @ @ poppier " look that got a lot closer to about 90-95ire for peak whites . The client dumped the first colorist and used me for hundreds of shows after that -- all because of levels that were about 20% too conservative . <p> Soft clip and careful use of highlight controls can allow having a bright picture without hitting out-of-gamut problems . I totally concede there are also shows that get very dark and dramatic , and there 's room for that as well under some circumstances . For commercials , I think getting too dark is a dangerous area . For feature films and music videos , it 's pretty much " anything goes " these days . <h> Attached Files : <p> Another thing I notice in Fuji xt2 in camera rec709 4k footage that ' skin tones ' are off . Converted to rec 601 footage all dkin tones are correct -right on skin line . After transforming rec709 footage on skin lines put colors much better BUT right way to go is conversion to rec601 and then load to premier . We will not loose anything as output files are still same or more mbps. 
@@44332916 @4332916/ <p> As I read it , it 'll be 8K sensors in a Super 35mm frame , meaning it will work with all traditional PL-mount 35mm lenses at the same focal length and settings . 43971 @qwx453971 <p> All everyone can think is the lens coverage , while conveniently forgetting the lens resolution . Majority of lenses will fail to resolve 8k , meaning , it 's a waste of sensor resolution without matching that sensor with proper lens resolution , meaning VERY expensive lenses . <p> All everyone can think is the lens coverage , while conveniently forgetting the lens resolution . Majority of lenses will fail to resolve 8k , meaning , it 's a waste of sensor resolution without matching that sensor with proper lens resolution , meaning VERY expensive lenses . 43971 @qwx453971 <p> Source for your claim ? Modern lenses like UP/MP or Leicas can resolve way beyond 8K , if you look at DXO , a lot of stills lenses can resolve more than 8K as well . <p> Source for your claim ? Modern lenses like UP/MP or Leicas can resolve way @ @ @ @ @ @ @ @ @ @ lot of stills lenses can resolve more than 8K as well . 43971 @qwx453971 <p> Please read my post again , especially the words VERY expensive ... Once more , every user is obsessed with the lens coverage , but no one , I mean NOT A SINGLE user talks about lens resolution . Very telling ... VV is a medium format neighborhood , so resolution for those sizes of images with medium format lenses , like modified Hasselblads for Alexa 65 and Panavision 65 lenses for DXL is one thing , trying to do that with dinky little lenses , that were never designed to be operated at such extremities of MTF is another . <p> U need 137 lp/mm to resolve the new 3.65 micron pixel pitch in 8K , assuming they use the same aspect as for 8K VV ... Sensor is slightly larger than S35 but most S35 lenses should cover that ... <p> Whether lenses have the resolution is a different story and is the only important question ... No need to wrestle with quite a lot more data if u 're actually only getting @ @ @ @ @ @ @ @ @ @ have less data to deal with , faster offload times and u need less powerful machines ... Image manipulation on 8K data stream ( at this point ) is A LOT for any machine <p> For that helium sensor size , it 's T stop diffraction limit is around T5 , so everything over that gets fuzzy . LONG ... Also between T2-T4 most of the shot is out of focus with cinema lenses depth of field . My gut feel is this is a lot more about dynamic range and noise reduction ( for high iso ) then having a lot of pixels for better resolution . <p> For that helium sensor size , it 's T stop diffraction limit is around T5 , so everything over that gets fuzzy . LONG ... Also between T2-T4 most of the shot is out of focus with cinema lenses depth of field . My gut feel is this is a lot more about dynamic range and noise reduction ( for high iso ) then having a lot of pixels for better resolution . 43971 @qwx453971 <p> Wow . So are you @ @ @ @ @ @ @ @ @ @ , and stop down to t16 so as to get everything in focus ? <p> Wow . So are you saying one ca n't pop on a zeiss compact prime , and stop down to t16 so as to get everything in focus ? 43971 @qwx453971 <p> Exactly , atleast for cinema lenses . Technically there are some work around on this that I think the cell phone camera 's use . Also this is effected by debayer and other wierd things , so it degrades over a bunch of t stops . Anyway for this reason , makes me think this is to add dynamic range and iso . Also I would hope they would do a wavelet transform from 8k to 4k , so people can still deal with raw but not at such a high pixel count and also would deal with sensor errors ( arri raw has a bunch of that built in - sensor error correction and double gain stuff ) . <p> For that helium sensor size , it 's T stop diffraction limit is around T5 , so everything over that gets fuzzy @ @ @ @ @ @ @ @ @ @ is out of focus with cinema lenses depth of field . My gut feel is this is a lot more about dynamic range and noise reduction ( for high iso ) then having a lot of pixels for better resolution . 43971 @qwx453971 <p> yeah , T5.8 for the red channel ... got ta see about DR and S/N ratio ... hoping ( --&gt; praying ) for improved color science ... 
@@44332917 @4332917/ <h> EIZO CX271 as a grading monitor <p> Over the past few weeks I 've been using an Eizo CX271 in a casual test to see if a good , color-accurate computer display can really work as an affordable grading monitor . Like a number of others , I 've been generally against this , but I think we are now at a point much like we were with plasmas where this could be very do-able , with some important caveats . <p> I put together a blog post about some of the pros and cons , since there are a few things you do n't get with a computer display that you do get with an SDI broadcast monitor . <p> This is n't a new idea , especially with the first gen DreamColors having been used as reference monitors for quite a while now , but for me , there are a couple things that make the CX271 the first really good choice for anyone looking for an affordable grading display : <p> -10-Bit HDMI with Deep Color support . -LED-Backlit IPS Panel with good black levels @ @ @ @ @ @ @ @ @ @ any display on the planet . -Free ColorNavigator calibration software that works on both Mac and PC. -Street price of less than $1500 . <p> I 've owned a CG243W for the last three years , and as much as I 've loved it , I would not have put it up for grading monitor duty . The IPS panel was good , but the minimum black level just was n't deep enough , and the HDMI input was only 8-bit . With the CX271 , you get a very good new IPS panel with 10-bit HDMI , that 's not only 27-inches , but also costs $800 less than what my 24-Inch 243W cost me a couple years ago . <p> I cover some of the compromises of using a computer display in the article , but if you 're ok with them , and need an affordable way to get into a color-accurate grading display , I do n't think there is a more complete , better-built , easy-to-use option than the CX271 right now . <p> I am pretty sure I read , maybe on here , @ @ @ @ @ @ @ @ @ @ how much difference do you think this makes and can you notice on the monitor ? 43971 @qwx453971 <p> They are 8-Bit+FRC . I mentioned that in my post , too , but so are monitors like the FSI LM-2461/2460 , CM240 , Z24x , etc . This has been discussed before , and I think the only real negative that some people mention is a small amount of noise or artifacting in shadow areas , but this is present in plasmas , too , and is often much less than the noise you 'll see from the camera anyway . <p> Jason , the CX271 is intriguing . I am looking for an affordable 24-27 " colorgrading monitor that I can accurately callibrate to Rec 709 . I do own the retail i1 display pro . I was first looking into the new Dreamcolor Z Displays but there is a lot of talk about uniformity issues and crushed blacks . I plan to feed a potential monitor a 10 bit 4:2:2 YUV signal via Blackmagic Mini Monitor HDMI . This could be the display I 'm looking for . @ @ @ @ @ @ @ @ @ @ i1 Display Pro ? And can the HDMI accept 4:2:2 ? <p> what 's the quality of that scaling , is there notifiable artifacts , can you clearly just noise amount in signal , etc ? 43971 @qwx453971 <p> I was able to run a bunch of footage through it from 4K F55 Raw , to Canon 5D Raw , and Alexa ProRes , as well as several test patterns , and I far as I can tell the scaling is transparent . Overall , I do n't think it 's an issue , and I doubt the scaler in the Eizo performs any differently that the scalers in many other high-end displays . <p> Jason , the CX271 is intriguing . I am looking for an affordable 24-27 " colorgrading monitor that I can accurately callibrate to Rec 709 . I do own the retail i1 display pro . I was first looking into the new Dreamcolor Z Displays but there is a lot of talk about uniformity issues and crushed blacks . I plan to feed a potential monitor a 10 bit 4:2:2 YUV signal via Blackmagic Mini Monitor @ @ @ @ @ @ @ @ @ @ for . How accurate is this Color Navigator Software paired with the i1 Display Pro ? And can the HDMI accept 4:2:2 ? 43971 @qwx453971 <p> The CX271 HDMI will accept up to a 4:4:4 RGB input , although like you I 'm driving it with a 10-bit 4:2:2 signal , the only difference is I am using an AJA Hi-5 to convert SDI from a Decklink SDI for Resolve and AJA Kona 2Ke for Lustre . <p> ColorNavigator is quite good , and one of the great things about it is that getting my CG243W and now the CX271 to match my other displays has been very easy . However , like a number of people will tell you , there is an improvement in absolute accuracy if you 're able to upgrade to some sort of external calibration solution . The compelling thing about the CX271 is that you can get started with ColorNavigator and at least know you are in Rec709 without a lot of effort or expense , and then later if your jobs warrant it , you can invest in something better . <p> Is @ @ @ @ @ @ @ @ @ @ With activated overdrive there seems to be strong overshooting of colors , could you please check ? 43971 @qwx453971 <p> I have and if you toggle OD on and off , at least for me , it 's very difficult to even see a difference . Maybe there is certain content that will make it more evident , but I could n't see what the issue was so I have left it on . <p> ( AFAIK ) ColorNavigator does ( still ) not support spectro offsets , so just using ur i1D3 w/o offsets makes these " calibrations " way less accurate ... besides that CN only samples a few points ... 43971 @qwx453971 <p> A few notes : <p> 1 ) The spirit of investigating the CX271 as a grading monitor is based on one question ; what 's the lowest possible , color-accurate , point-of-entry for those who need a grading monitor , but do n't have a large budget ? By definition a solution like that is going to lack a few things , and if you need those , then spend the money on a @ @ @ @ @ @ @ @ @ @ here is n't ColorNavigator Vs CalMan or LightSpace . We all know they 're better . The comparison is ColorNavigator Vs Nothing at all . Complaining that ColorNavigator does n't have this or that , is like complaining that the CX271 does n't have scopes . The idea of using this display as a grading monitor is targeted at people who have been trying to grade on their laptop screen , or their iMac , or a low-end plasma . For people in those situations , the CX271 with ColorNavigator will be a huge step forward . <p> 3 ) I would actually challenge the claim that ColorNavigator is " less accurate " given what is being perceived here as missing , because it has two huge advantages over nearly everything else ; it 's easy to use , and it 's free . At most high-end facilities , calibrations are done before every session , but almost everywhere else , like big production companies , broadcast houses , major networks , broadcast monitors often go months without calibration . And there are often big shows being graded on those @ @ @ @ @ @ @ @ @ @ every 6-12 months using LightSpace or CalMan and an expensive probe , but as the months go by , those displays get more and more out of shape . <p> With ColorNavigator , on the other hand , a shop could very easily have it installed at every workstation , and have a collection of i1 Display Pros circulating through the building , allowing those colorists , or online editors to calibrate every single morning before they start . Kick off CN for 5 mins while you go grab your coffee and come back to a display that is in much better shape accuracy-wise than a display that languishes between " higher-end " calibrations for months on end . I see this happening everyday , and frankly a bunch of Eizos and ColorNavigator would be a huge step up for those companies , too . <p> The whole point of calibration is to get the display into a color accurate state so u can use it for color critical application . Which in your case is not just some vacation photos u 're gon na edit in Photoshop for personal @ @ @ @ @ @ @ @ @ @ So clearly , we have a professional need here to be as accurate as we can be . <p> The point w/ CN being free is mood , Argyll is free and does some great work . Lightspace is what you wan na buy once you can afford it . <p> CN missing meter profiling is the biggest red flag u can have . Colorimeters - such as the i1D3 and even the US$7,000 K10-A - are inaccurate already when they leave the factory , and they will drift more and more over time . That is why a professional calibration approach always uses meter profiling , a process that creates meter offsets from a spectro meter ( reference meter ) for the colorimeter ( active meter ) . <p> This is being done so u get the best of both worlds : spectro with it 's color accuracy and ability to read any display type ( display source ) colorimeter with higher readspeed , better low light ability and better repeatability . <p> With this approach u can keep using ur colorimeters forever ( unless they develop mechanical issues @ @ @ @ @ @ @ @ @ @ simple process in CN disqualifies the application for Pro use . I have been asking Eizo for nearly 3 years now to implement this very , very basic feature for their calibration sw for their Pro screens . Which is very sad as I like the ( higher end ) Eizos and they can be calibrated very nicely , see here . <p> So in CN , u can either use a colorimeter w/o offsets , which again is not accurate and u will have to live with however far off the colorimeter is as any calibration solution solely and only relies on the readings of the meter . So u might get great delta E charts but u have visible color contamination in ur Greyscale etc ... this is why there has to be visual validation ( using dedicated patterns ) after every calibration ... if u are very lucky , ur colorimeter will not have drifted so much ( yet ) that it is visible to YOUR eye ... but it might be for the next guy , and naturaly everybody only ever checks a few test patterns @ @ @ @ @ @ @ @ @ @ ur footage have yellow contamination although it looks good on ur screen ... <p> The other option is to use a spectro only in CN . But then you get slow read speeds , and most importantly everything below 20% brightness will be very inaccurate as spectros struggle in low light ... <p> The third option is to use an expensive colorimeter such as the K10-A which has internal offset storage , so u create the reference offset via Klein 's own sw , store it inside the K10 and then simply select that profile within CN ... well guess what , CN only uses the default empty slot and does not allow u to select any other slot ... so I have the K10-A - the best colorimeter in the world - and I ca n't use it properly within CN ... <p> So what 's ur option on a budget ? <p> Buy an i1Pro sprectro rev D ( this is version 1 , make sure to get rev D ) used off ebay ( if u can still find any ) for maybe 150-200 bucks . A @ @ @ @ @ @ @ @ @ @ note : spectros can drift as well over time ( less than colorimeters but still - if u store the spectros correctly u can use them for years ) and it could be that u pick up a spectro that has e.g. a " green " push ... ( u would only know if u compare that spectro to an even more accurate reference spectro ) ... but u could either spend US$8,000 on a Jeti spectro or send ur i1Pro in for re-certification , which in case it is off will be re-calibrated it to be within allowed tolerances .... <p> You are only ever as accurate as your reference meter is , how accurate your pattern generator is and ( not to forget ) how accurate the math of the color engine is ( of ur calibration solution ) , which is why most Pros use Lightspace . <p> I have received an email from my Eizo contact in Japan a few days ago and he said that they will implement the K10 slot selection ... that is great for K10 owners but wo n't help anybody with @ @ @ @ @ @ @ @ @ @ I have n't used CN since March ... <p> Once CN has meter profiling , we could then move on and discuss final calibration accuracy , patch set size etc ( see here for a comparison of CN vs . LS ) but not yet ! ! ! 
@@44332918 @4332918/ <p> Yes , as Lucas says , the DCP scope resolution is 2.39 and every compliant DCP should be that . But 1080 and 2160 ( the heights of HD and UHD ) are n't evenly divisible by 2.39 , so when we deliver scope features and shorts , the DCPs exist as 2.39 and and HD/UHD versions exists as 2.40 . <p> I usually make the case for projects from film ( particularly movies made prior to 2000 ) that 2.40 makes more sense , because then it will cover up more of the splice lines at the bottom . 2.40 was the official SMPTE designation I think throughout the 1980s and 1990s , but prior to that , it was called 2.39 . The actual barrel of a Panavision anamorphic lens does say " 2.39 . " <p> 5 or 6 pixels ai n't gon na kill anybody , but it 's important to find out from the distributor what their specific specs are . If they ask for 2.39 , give them 2.39 . <p> Here 's a side note for you Marc since we are @ @ @ @ @ @ @ @ @ @ been watching 11.22.63 on Hulu . I notice that they either shot or matted for 1.85 . But this show is specifically for television not a feature release . It 's surprising to me that they would n't keep it 1.78 for television and use the full screen size . <p> Here 's a side note for you Marc since we are on the topic of Mattes . You said you 've been watching 11.22.63 on Hulu . I notice that they either shot or matted for 1.85 . But this show is specifically for television not a feature release . It 's surprising to me that they would n't keep it 1.78 for television and use the full screen size . 43971 @qwx453971 <p> No , I think the show is being done at 2.00 , very much like House of Cards . 1.85 would only be a teeny-tiny bit of blanking top and bottom in 1.78 . <p> Netflix , Hulu , and Amazon have kind of thrown out the rules on stuff like this , and it does n't bother me . I was surprised that Hulu @ @ @ @ @ @ @ @ @ @ thought was awful . The shows on Amazon and Netflix play a lot better continuously , and you get used to that very quickly . I get that Hulu is available both with and without commercials ( Hulu+ ) . <p> I caught some anamorphic lens flares in 11/22/63 last night , so it 's possible they were using anamorphic lenses with Red 6K on the show . ( And yes they were , according to IMDB . ) I did n't see a credit for the colorist , but it was apparently done at CO3. 
@@44332919 @4332919/ <h> A7S 2 . ARW HDR RAW Workflow ? <p> I 've spent the majority of the day working with this image sequence and ca n't figure out a workflow . <p> I have an image sequence of time lapse material . <p> Camera : Sony A7S 2 ( ILCE-7RM2 ) Raster - 7952x5304 <p> I used Adobe DNG Converter to get the files out of ARW and into DNG . Now Resolve can see the media . <p> However I think this material was shot in some kind of HDR burst mode . the image sequence cycles between three different levels of exposure in a rolling wave . It would make sense if these frames were blended together ? However I ca n't find any tool that can interpret this mix of exposure levels . <p> Sony Catalyst and Sony RAW Viewer both can not open these files . Capture One for Sony can , however Capture One does n't feature any useful tools for dealing with this media . I do n't see any HDR mode to blend the exposures or any export tools etc . <p> @ @ @ @ @ @ @ @ @ @ with a 3000 image sequence , I plan on doing something better with my evening . <p> My ultimate goal is to turn this image sequence into a 2160p ProRes4444 
@@44332920 @4332920/ <p> Which 4K ? A compressed file like ProRes , DNxHR , or H.265 ? Camera media like ARRIRAW , R3D , or MXF ? Those can all store a 4K frame , but storage-wise it 's not the resolution that makes them difficult to stream , it 's the bit rate , and all of the formats I just mentioned vary from a few to several hundred megabytes per second . All but the biggest camera media and compressed formats can usually be handled by something like Facilis ( via fibre ) or even Open Drives . However , if you mean uncompressed 4K image sequences like 10 or 16-Bit DPX , EXR , or TIFF then the buck generally stops at StorNext . <p> Codec varies but would at minimum like to be able to run 3x4k DPX which I know is a high order . <p> That Archion looks pretty promising , definitely going to check them out I am looking into StorNext as well , but have heard they are not the most user friendly of solutions . 43971 @qwx453971 <p> Three streams of 10-bit @ @ @ @ @ @ @ @ @ @ are n't many shared storage systems out there that reliably offer that sort of bandwidth to multiple workstations at one time , especially with image sequences . Be careful , as a lot of these smaller storage vendors will quote you big performance numbers , but then are nowhere to be found once you actually try it . <p> I have n't worked with Archion recently so things may have changed , but the last time I did , they were just rebranding low-cost raid arrays , and installing MetaSAN on them . If you talk to them , and that 's still the case , then I would move on and take a look at Facilis , if you need something capable and easy to manage . StorNext is better than either one , and has become much easier to use now that they are packaging complete SAN controllers with built in GUI management , but even though , you 'll want someone experienced or at least willing to learn the ins and outs of it to keep everything running smoothly . <p> I edit a lot of 4k stereo @ @ @ @ @ @ @ @ @ @ NAS , but I feel it 's about hopeless using a NAS unless the there in conjunction a super fast local cache . Right now my local cache is raid disk at 1.5gbytes per second , but I will be moving to raid SSD for local cache ( around 4ish gigabytes per second ) . I strongly recommend not trying to put the local cache on the network storage system , but have a mix of large block size with 10g speeds on the NAS with super fast local caches . With a super fast local cache ( i.e. 4 Gbyte 's per second local SSD raid ) , I worry less about the NAS being incredibly fast and am way more fanatical about it being incredably reliable with 10g ethernet connections to the macs , windows and linux workstations . Debugging 10g network issues across a bunch of various operating systems and versions takes forever ... can totally kill a deadline , also more important then raw speed is having a support contract where I can call them up and fix issues ( I have a remote management NAS @ @ @ @ @ @ @ @ @ @ I basically freak out if the NAS has any issue ) . 
@@44332921 @4332921/ <p> I actually transferred Targets for Paramount in the 1980s . I 'd say it 's grainy , higher-contrast , a little more saturation , and vignettes on the outside edges . But more of the ' 60s look is about the lighting and lenses than the color-correction per se . And I can show you 20 1960s films , each of which have radically different looks . 2001 ? Once Upon a Time in the West ? Mad Mad World ? The Sound of Music ? Psycho ? <p> hey Marc thanks for your reply , and that is interesting you worked on the transfer . <p> I agree that a " film stock " does n't automatically mean " a certain look " . I have this argument with die-hard hard-core celluloid fans all the time - not that I do n't think film is " better " just that when people say they want to shoot on film to achieve a certain look I 'm thinking that well before DI there could have been many other variables that went into making film " look " a @ @ @ @ @ @ @ @ @ @ and of course a hell of a lot more . <p> Really , people do n't know what they are looking at or why it is the way it is ... but they like to think they do . <p> I posted my question on cinematography.com about what film stock this film might have been shot on and how the look might have been derived and David Mullen ASC posted this kind reply : <p> .. though there were some ways of manipulating the image in post using an optical printer and dupe stocks . Plus some movies then were released on Technicolor dye transfer prints rather than Eastmancolor prints . <p> But when talking about the look of old stocks you have to figure in what you are currently looking at , an old print of a then-new negative or dupe , a new print of a now-old negative , a new print off of a new dupe made from b&amp;w separation masters , a video transfer and then you have to know what was used for the video transfer . A lot of variables outside of simply how @ @ @ @ @ @ @ @ @ @ <p> Yes , David is very , very hip on film technology . One issue with trying to judge " the film look " is often what people are really seeing is a film transfer , which is not the same thing . Your reaction is going to be based on however the film was scanned , whether it was done from negative or an intermediate element , who did the correction , when the correction was done , and so on . Watching a print from a film projector is a very different experience . <p> In truth , whenever I 've worked on a 1960s or 1970s film job and the studio has screened a reference print for me , usually the supervisor and I cringe because the film is beat to hell and turning magenta . I 've gotten 15 minutes into screenings like this , and the studio rep sighs , throws up his hands and says , " look , just make it look good . " And we call it a day . <p> Juan Salvo has a lot to say about the Film @ @ @ @ @ @ @ @ @ @ his conclusions : <p> But more of the ' 60s look is about the lighting and lenses than the color-correction per se. 43971 @qwx453971 <p> Absolutely . But also the set design , clothes worn by the talent , the accessoires , fashionable color palettes of the time , the props , particular hairstyles , body types ( physical attributes ) , posture and attitude as well as format ( aspect ratio ) and the correlated composing of shots , the shot design , editing styles , title design and in this case even vignettes that simulate weapons magnifying scopes are all stylistic elements that defined the look of films of particular period . Obviously we ca n't re-create most of these things in color correction/grading . <p> Next time better create a meme which explains a producer that shooting a commercial with a couple of nudists at a mediterranean beach " which should exactly look like that French classic from the mid 60 's " is n't solely doable in post production without adding at least some of the jizzel I 've mentioned above . <p> I found these @ @ @ @ @ @ @ @ @ @ first frame from Night of the Living Dead is from 1986 and the second from 2004 when it was recolored . Aside from the actual coloring itself , it 's easy to see how the transfer alone can make for such a different look . <p> On a different note it 's interesting if you read the wiki page , critics Siskel and Ebert seemed to absolutely hate recoloring of films and called it " Hollywood 's new vandalism " . Sigh ... <p> I found these under the Wikipedia entry for film colorization . The first frame from Night of the Living Dead is from 1986 and the second from 2004 when it was recolored. 43971 @qwx453971 <p> I did the color-correction on the 2004 HD colorized version of Night of the Living Dead over at the late Sunset Post in Glendale . The people who supervised the sessions ( Legend Films ) were very nice and gave me the whole explanation on their process : basically , they scanned the film in LA , then shipped the files to Mumbai and had hundreds of animators doing the colorization @ @ @ @ @ @ @ @ @ @ Once they were done , we got back the finished frames and I fine-tuned the whole thing . Much of what I did was toning down the " zombie green " creatures , which the client felt were too intense . <p> We actually spent 3-4 days correcting the image in daVinci 2K and I think the results were acceptable . Colorization still is n't indistinguishable from material shot on color film , but it 's more convincing now than it used to be . It 's also not what I would call " 1960s color , " either . 
@@44332922 @4332922/ <p> It will partially depend on your other hardware . I believe a modern single CPU maxes out at 40 total PCIe lanes , and a modern high end GPU requires it 's own 16 lane slot ( even though they do n't saturate all 16 lanes , they do over-saturate 8 lanes ) . So with 2 CPUs it 's possible to have proper piping to 4 GPUs , but you 'll also need a mother board that has the right slot placements . So regardless of software , your hardware may provide an upper limit . <p> Now , will you get diminishing returns on $ investment , or any weird issues with software or rendering ? That I do n't know . I 'll let others that are more knowledgable there chime in. 
@@44332923 @4332923/ <h> Is Mistika insight still alive ? <p> Mistika Insight has been a very useful solution in periods but being a rather time-limited solution does n't make it the best solution long-term . I can also imagine there would be a benefit for SGO to connect closer with its user base as this is for obvious reasons the best way to maintain a solid network of people that love working with a fantastic toolset . Somewhere I think Mistika would benefit largely from making the toolset more available as I have seen it scale very well from smaller portable systems up to extremely powerful machines . <p> I am therefore very excited to read about interesting development happening and look forward to see how SGO will make Mistika more available to interested parties and it 's current user base . Relationships need to be nurtured to stay alive , I very much think that applies here as well . <p> I for one would be very happy to continue training/working and promoting Mistika through my work if there was only better access to it . 43971 @qwx453971 <p> Hi @ @ @ @ @ @ @ @ @ @ on Mistika/Insight . We absolutely agree that relationships need to be nurtured to stay alive , hopefully the latest version of Insight will tackle some of the frustrations you have found and broaden the user base/community . As mentioned , we have some pretty big announcements to make this NAB all of which support broadening the Mistika user base and will provide ( much more ) user engagement . We 're almost ready with the latest version of Insight so would like to invite you and anyone else who is already comfortable with Mistika but wants to get up to speed with the latest , to trail/feedback on the current version ? Just let me know and I 'll get in touch re . licence details . 
@@44332924 @4332924/ <h> PS4 Pro HDR and HDCP issue with TV 's <p> Not related to colorist so much , but though it might interest some of you . <p> A friend of mine told me of a problem he had with is brand new PS4 Pro unable to output 4K HDR ( as this is the latest update on the new console released by Sony ) to its late 2014-2015 4K tv model . Did a little of research , and seems that Sony jumped a little too soon in 4K HDR gaming . Seems like different games are outputting different HDR value , and TV 's are switching from RGB to YUV420 and YUV422 depending on the game played . Looks like a major problem to mee <p> There seems to be a problem with HDCP as well . Older 4K TV 's have a 1.4 HDCP protocol , while the console is asking for a HDCP 2.2 . I had never heard of HDCP before , and as I understood it from a short read , this seems to be a protection protocol for TV streaming and content @ @ @ @ @ @ @ @ @ @ receive the signal from the console , often switching to 2K , some of them completely go blank . The console was released 5 days ago , Sony is blaming the TV 's , and the TV 's are blaming Sony . <p> Likely worked out , at least somewhat , with a software update on the PS4 Pro . I know Kotaku got one early from Sony , and got a 4K HDR TV from Gizmodo , and had various issues getting HDR to function properly . <p> Speaking as someone that plays too many games for his own good , the software updates to the PS4 over the past 3 years have been pretty good , and bugs have been addressed pretty well . Not a flawless record , but better than many products we 're used to dealing with . <p> The flipside , of course , is if the HDCP 2.2 is processed directly through a chip ( hardware ) and is incapable of downgrading to 1.4 . But the RGB/YUV420/YUV422 thing should be sorted with a software update and a mandate to game makers on @ @ @ @ @ @ @ @ @ @ compliant . <p> The flipside , of course , is if the HDCP 2.2 is processed directly through a chip ( hardware ) and is incapable of downgrading to 1.4 . But the RGB/YUV420/YUV422 thing should be sorted with a software update and a mandate to game makers on what the game needs to provide in order to be compliant . 43971 @qwx453971 <p> I think my friend mentioned that he bought a HDCP 1.4 to 2.2 converter . So maybe this in pair with a software update will solve all of the issues . 
@@44332925 @4332925/ <h> 8bit vs 10bit Camera Re-Test <p> I got some flak here on this forum when I posted my conclusions of an 8bit vs 10bit camera test a while back . Most of the comments centered around having way too many variables in my tests . They were right and I apologize for that . <p> In this re-test I am trying to keep it simple with the equipment I have access to . <p> Tests with a7Rii and FS7 : <p> Both recording to Shogun ProRes 4:2:2 <p> Both Slog2 <p> Both 4:2:2 color <p> Both at 4K <p> Both using the same lens ( however we forgot to turn on the FS7 lens correction ) <p> Both ISO 2000 <p> Both F4 <p> Besides using different cameras , the only difference is 8bit a7Rii vs 10bit on the FS7 <p> We did several setups besides the one you see above , some of them at wrong WB and some underexposed . I have tried several times to break the a7Rii footage , but when I do break it , the FS7 breaks at the same point as @ @ @ @ @ @ @ @ @ @ above I changed the hue of her shirt to match the color of her lips ( something that might be a common request ) . When I pushed the color to the breaking point ( not shown here ) the gradient of her shirt color breaks in the same gritty way for both cameras . <p> I have also tried on a close up of her face to change the hue of certain skin tones ( slight yellow below her nose ) to a much different color and both cameras break in the same macro-blocking way with the skin-tones . <p> Now I am not giving up on making the a7rii footage break before the FS7 , so I am looking for suggestions on ways to show the a7Rii breaking apart first with the existing test footage I have . Any suggestions ? <p> First , why ISO 2000 on both cameras ? You should use their respective base ISO ( 800 for A7R II , 2000 for FS7 ) , and use ND to match exposure . <p> In the case of high bitrate and minimal compression ( ProRes @ @ @ @ @ @ @ @ @ @ be quantisation errors . And this test scene lacks large areas of fine luma gradations , and the image was very likely dithered by noise . <p> A better test scene might be a wide shot of a clear sky at midday , frame half the shot of the horizon with plenty of details ( trees , grass , buildings etc ) and the other half of the clear sky . Overexpose a little on the two cameras by the same amount the get the best SNR . <p> Now do some heavy grading on both of them , 8bit image should be more likely to show banding in the sky . <p> Another point to consider is that Slog2 allocates more bits to the highlight , while Slog3 allocates more bits to the shadow . <p> I have also tried on a close up of her face to change the hue of certain skin tones ( slight yellow below her nose ) to a much different color and both cameras break in the same macro-blocking way with the skin-tones . <p> Now I am not giving up on making @ @ @ @ @ @ @ @ @ @ am looking for suggestions on ways to show the a7Rii breaking apart first with the existing test footage I have . Any suggestions ? 43971 @qwx453971 <p> I would start with your premise that 8bit breaks up colors easier than 10bit footage . <p> I think that is incorrect , I would say that 10bit simply breaks up more accurately but breaking up nevertheless . If you want extreme color changes I would think you have more success with raw footage . <p> I 've recently evaluated footage from a camera test where they had an a7S shooting with some log curve . There was a grey wall in the background of one scene which would just break down into horribly visible bandings as soon as you just looked at the contrast knob . Maybe they set up something else wrong in the camera too , but shooting in 8bit with a very flat " log " curve is just asking for trouble in my experience . Although Canon 's log works quite well with their C300 internal 8bit recording . <p> I 've been receiving a few projects every @ @ @ @ @ @ @ @ @ @ correctly . Most common issues are banding and compression artifacts . Sony should really disable certain settings in the FS7 , like recording Long GOP 8-bit 4:2:0 UHD in S-LOG . It leads to horrible pictures that can not be repaired . As well , people involved in post prior to color correction should be aware that the whole pipeline has to stay in 10-bit , otherwise your screwed as well . <p> That 's the biggest problem with S-LOG , the people using it are not always technically inclined to fully understand the workflow , the do 's and don'ts and how to use a monitor LUT when setting exposure ... <p> I , m Not being mean to Dave , i just do n't understand the point of these tests comparing 8bit to 10bit ..... i mean you can google this stuff any where and you will come up with the answers and if you have ever driven a camera professional ... you know the answers <p> I think felix and Pepijin make valid points its more about testing the individual cameras and looking at the footage then @ @ @ @ @ @ @ @ @ @ no shocks later ........ talking of shocks <p> +9 on the LONG GOP 8-Bit 4:2:0 UHD in S-LOG ......... your killing your image with every frame you roll on that step up ....... <p> what we is needed for younger filmmakers is something like this ... see link <p> What is the hardware setup for all of these tests ? Is there a full 10-bit workflow in place ? ( 10-bit monitor , displayPort cable , 10-bit capable OS , viewing on a 10-bit capable application , 10-bit capable workstation card ( not a gaming card ) , etc . ) <p> Good question , I am not sure , I have a LG 34UC97-S 34 " . LG says it has 10 bit color , but I am also using a GTX980ti which I think I hear a while back now supports 10 bit color , not sure it is very confusing . I am not a professional colorist . <p> Lucas , <p> My thought of matching ISO 's was if I did n't other people would say that was not fair . <p> Good question , I @ @ @ @ @ @ @ @ @ @ " . LG says it has 10 bit color , but I am also using a GTX980ti which I think I hear a while back now supports 10 bit color , not sure it is very confusing . I am not a professional colorist. 43971 @qwx453971 <p> Nope , 10-bit color in games . Most GTX cards can do 10-bit color with Direct X , but not applications like Premiere or Photoshop . For OpenGL and 10-bit color capable applications , you need a workstation card like a Quadro or a FirePro ( Note : recent AMD name change is now Radeon Pro ) . <p> NVIDIA 'S direct quote on the subject : NVIDIA Geforce graphics cards have offered 10-bit per color out to a full screen Direct X surface since the Geforce 200 series GPUs . Due to the way most applications use traditional Windows API functions to create the application UI and viewport display , this method is not used for professional applications such as Adobe Premiere Pro and Adobe Photoshop . These programs use OpenGL 10-bit per color buffers which require an NVIDIA Quadro GPU with @ @ @ @ @ @ @ @ @ @ BMD mini monitor before on previous 10bit vs 8bit tests and did not notice a difference . <p> Purpose : I always here people saying that 10 bit is always better than 8 bit yet I ca n't find much proof of that online when using 4k footage , what am I doing , I am testing my cameras and asking lots of questions and learning . <p> Tero , I have used the BMD mini monitor before on previous 10bit vs 8bit tests and did not notice a difference . <p> Purpose : I always here people saying that 10 bit is always better than 8 bit yet I ca n't find much proof of that online when using 4k footage , what am I doing , I am testing my cameras and asking lots of questions and learning . 43971 @qwx453971 <p> Record some low contrast footage , such as a far away object on a telezoom . Increase contrast dramatically in post on both the 10 and 8 bit footage . The 10 bit footage should hold better . <p> Tero , I have used the BMD @ @ @ @ @ @ @ @ @ @ did not notice a difference .... 43971 @qwx453971 <p> Dave , the ' mini monitor ' , capable of 3G 10bit 4:2:2 display is a good start but it must be matched with a calibrated 10bit monitor with either a 10bit HDMI input or better still , a 10bit SDI input ( or by using a 10bit SDI to Display Port converter ) Without that 10bit ' chain ' in place , you will not be able to observe any difference between 8/10bit . <p> So i have 2 monitors hooked up on my NMP one is 8 bit display and the other 10bit if i load up the ramp in photoshop on the 8bit display ....... banding .... drag the whole photoshop widow over to the 10 bit .... milky smooth . <p> just by doing that exercise you can go yes 10bit is better that 8bit ... before you shoot a single frame <p> However if you must ... film something ..... <p> if you live near the sea take both of those cameras down to the beach a on clear blue day , film directly at the @ @ @ @ @ @ @ @ @ @ <p> back in the edit suite <p> the moment you touch the 8bit footage you will see banding in the sky .. if you 're not seeing it already ... the 10 bit will last longer before it explodes <p> if you do the down scale 4k to HD thing the banding might not be as bad .... but that can of worms is for another day ...... so leave your timeline 4k <p> good luck <p> Just footnote as your seeing this in a GUI you won , t be calibrated ... so you will be working in what 's called " WHAT EVER COLOR " or WEC ..... it 's like SRGB or 709 ... but wrong <p> Dave , without your tests , none of us could discuss the complexities of what you 're trying to accomplish . This test is much better , and even though there are lots of suggestions again , you are definitely a BAMF for continuing to try these things . <p> Besides using different cameras , the only difference is 8bit a7Rii vs 10bit on the FS7 43971 @qwx453971 <p> That said @ @ @ @ @ @ @ @ @ @ in full Frame Mode , and the FS7 which is a proper 35MM video camera , does n't . Apparently the A7RII is non-binning ( pixel-for-pixel ) in 4K 35MM Crop Mode , which might have been what you used ( there are both 4K binning and 4K non-binning modes ) . However , even though the A7RII bins at full frame , according to tests , full frame is still very good . <p> I know you have a large number of cameras pass through your hands , so my point is to make sure that when you use any DSLR/ Mirrorless cameras , especially in comparison to normal 35MM cinema cameras , that they are in a non-binning mode , otherwise that could really affect your perception of your results in a comparison like this . 
@@44332926 @4332926/ <p> I recently had a chance to try both of these out , and really enjoyed using them . They have a great , fast typing action that 's half-way between the low-profile Apple Pro keyboards and a taller PC keyboard . The keys have a white LED backlight with adjustable brightness , and a satin finish that stays nice looking even after a lot of use . The matte black look with orange accents on the function keys makes it look like it was purpose-built for DaVinci when you have the Resolve interface up . <p> There 's a wired and wireless version , and you can get them for less than $70 online or at almost any of the big box retail stores . The only thing it 's missing is a built-in USB hub for a mouse or tablet , but other than that it 's a great , comfortable keyboard that works well in a color or edit bay . <p> We used the Logitech illuminated keyboards at Lowry when I was there working on Baselight , but they seemed to fail quite frequently and @ @ @ @ @ @ @ @ @ @ power requirements were beyond the limits of the local computer in the room . I gave up and brought in Littlights with blue filters , which worked fine . <p> After seeing the K740 in use at another facility in town , I got one for myself ; it matches the Nucoda Precision panels quite nicely . It was pointed out to me that I 'd be better off with the wired version rather than have the battery die in the wireless one during a session , though . <p> Yeah , that 's nice Juan . I might pick that up . My old wireless Microsoft keyboard 6000 is way too bulky now that I have my element sitting next to it . The wireless mouse has all the buttons too . But I guess it 's too old for Resolve cuz middle click does n't work <p> I have the K800 wireless , it is not the nicest keyboard , but it kind of a feels good , soft and nice . Ilumination goes off quite soon , than you wave around the sensor to get it back . @ @ @ @ @ @ @ @ @ @ " scratched " not even properly engraved as originals . And than you have both shining ... Moshi looks nicer , I do n't like the embossed and bubbled corners keys on my K800 <p> How about the Optimus Maximus ? Each key is a 48x48px OLED screen in full color . 43971 @qwx453971 <p> Those are very , very , very cool . When Baselight demoed their new Blackboard a few years ago at NAB , I was blown away by the concept of hundreds of little tiny OLED screens on the console ! I du n no how useful it would be in actual practice , and it 's doubtful most manufacturers would support a standalone OLED keyboard in terms of identifying the buttons . <p> Sounds good ! But too bad it 's so deep . I 'm also looking at another keyboard , the K290 , but rumors has it you need to press the FN key to get function keys to work as F1 , F2 and so on which obviously is a problem , and it 's not backlit - but I can live with that . 
@@44332927 @4332927/ <p> As most LGG folks are aware , Blackmagic Design announced the latest version of DaVinci Resolve at NAB this year - the DaVinci Resolve 11 beta is very stable now and works incredibly well . It incorporates many new editorial and grading features but the area we 've been testing thoroughly are the new Collaborative Workflow features . <p> Inside of Resolve 11 are a very powerful set of database driven project management tools which will be of tremendous benefit to finishing houses wanting to use Resolve 11 as the hub of their post-production and delivery pipeline . <p> We 'll be explaining and demoing how to set up and use the new collaboration tools this Saturday on The Lot in West Hollywood from 10am to 12 noon . We wo n't be covering the new editorial and grading functionality ( that will be another workshop in a couple of weeks time ) but instead we 'll be focusing on the project management tools - so this seminar is not really for individual colorists working on their own ... it 's geared more to finishing shops where they @ @ @ @ @ @ @ @ @ @ at the same time . <p> We 'll also be demoing how to install Resolve 11 on a new kind of shared storage platform called LumaZAN that leverages ZFS hybrid storage pools and a 10GbaseT infrastructure with both Mac and PC clients . <p> The price/performance value of Resolve 11 running across the LumaZAN collaboration platform and leveraging the power of AMD FirePro GPUs is quite incredible ... we talk often about the disruptive consequences of Moore 's Law on the M&amp;E industry and this is another example of how commodity hardware combined with COTS ( Commercial Off The Shelf ) software is moving the goalposts . <p> If you 're interested in knowing more about the collaborative workflow features of DaVinci Resolve 11 come along on Saturday - the session will be more IT oriented than artist oriented and is really meant for the person responsible for setting up and managing the client/server infrastructure of a small post-house . <p> More details on the event and the link to EventBrite Registration page are available on our website : <p> We show the workflow from ingest , setting up the @ @ @ @ @ @ @ @ @ @ through to VFX integration and final color plus deliverables with video outputs going to SONY HD grading monitor , Barco 2K DLP projector and 65 inch UHDTV ... not quite sure how we record that live . <p> For Webcasting , we might use a mixture of ScreenFlow and Keynote slides with Voice Over explanation ... what would help me get my head around how best to webcast the demo is to let me know what things would be of most value to you ? <p> Trying to think of how we can easily condense two hours of fairly intense demonstration into something that works well on the web ... appreciate if you could give me some feedback on what you think would be useful . <p> As I said above , the workflow session is really geared towards the person responsible for managing the IT infrastructure in a small to medium sized finishing shop . <p> What would you most like to see covered in a Webinar , given the practicalities of recording four separate machines working at the same time ? <p> The interesting thing about the Resolve @ @ @ @ @ @ @ @ @ @ the ' hub ' of the finishing pipeline and not just the destination ... it 's kind of like how Avid and Unity is at the moment .... it 's not just the technology that 's changed you also have to re-define the roles of certain folks involved in the post workflow ... there 's seems to be a new set of morphing responsibilities that incorporate some aspects of the assistant editor and the junior colorist roles along with the VFX Super and the Post Super roles ... the key to getting the most out of Resolve 11 is to rethink ' who does what , where and when ' ... being able to stay within Resolve 11 for online conform , editorial tweaks ( or actual editing - btw , the new editing tools are pretty damn spiffing ) plus VFX integration and project management before you even get to final color and deliverables is kinda amazing for piece of software that costs $995 ... you do need the full licensed version for the actual client machines in the collaboration team but the machine that hosts the ' Master @ @ @ @ @ @ @ @ @ @ <p> Resolve 11 is going to take a bit of getting used to if you want to exploit its full capabilities ... its definitely stretching the definition of a ' color corrector ' for sure . 
@@44332928 @4332928/ <p> Yeah sure is , sounds like a bit of a breakthrough for the industry having an OLED of that size that 's up to FSI 's standards . Could be the go to panel for client monitoring . Wish I was at NAB this year to check it out . <p> Also they 've got the XM31K coming up later this year by the looks of it - though it appears to be LCD based when looking at their landing page .. <p> XM31K coming late 2017 , private demonstrations starting summer 2017 . Interested in learning more about the XM31K ? Sign up here . <p> I 'd love to be able to ask the team why they are going for an LCD panel rather than an OLED . Seems like a strange decisions as an OLED would have put some competition on the market for the BVM-X300 . Perhaps they are trying to fill a different need for higher luminance panels ? Or perhaps they simply ca n't source reasonably priced OLED panels of the quality they 'd need at a full 4096 res . <p> @ @ @ @ @ @ @ @ @ @ some of the planning and thoughts behind the products from FSI 's perspective . Exciting announcements <p> I 'd love to be able to ask the team why they are going for an LCD panel rather than an OLED . Seems like a strange decisions as an OLED would have put some competition on the market for the BVM-X300 . Perhaps they are trying to fill a different need for higher luminance panels ? Or perhaps they simply ca n't source reasonably priced OLED panels of the quality they 'd need at a full 4096 res. 43971 @qwx453971 <p> Or maybe they feel that the new generation of LCD panels , with per-pixel backlight modulation ( I 'm assuming it 's this type - I have no inside info ) are superior in some ways to OLEDs <p> If specs are accurate and the XM31K really has a contrast ratio of 1,000,000:1 then that places its black level at 0.002 nits , correct ? ( 2000nits/1,000,000 ) . If so , black levels really are n't going to be a concern ! <p> I wonder if Steve Shaw has already @ @ @ @ @ @ @ @ @ @ . <p> The CM250 now cost $6,495.00 , is beautiful and only 24 " , I ca n't not imagine how much will cost this XM55U beast . <p> 1- In my case tick all the needs , one monitor for client and me , so no metamerism. 2- No need to calibrate myself with expensive gears , just send then and get it back . 3- High level of professional standard from FSI . <p> Did I miss something ? <p> LG Oled TV are getting better and better , and this year the new batch of oled are gone be just what we need and a very reasonable price , I just wish to see a affordable price for XM55U frem FSI headquarter . <p> I 'd love to be able to ask the team why they are going for an LCD panel rather than an OLED . Seems like a strange decisions as an OLED would have put some competition on the market for the BVM-X300 . Perhaps they are trying to fill a different need for higher luminance panels ? Or perhaps they simply ca n't @ @ @ @ @ @ @ @ @ @ need at a full 4096 res . <p> In any case , I 'm looking forward to hearing some of the planning and thoughts behind the products from FSI 's perspective . Exciting announcements 43971 @qwx453971 <p> According to my " inside info " , XM31K is indeed based on the nicked named " Super IPS " panel from Panasonic . FSI also managed to push the peak luminance from quoted 1000nit to 2000nit . There 's a high likelihood that the actual product will be delayed though , the technology is just too futuristic to commercialise so quickly . <p> According to my " inside info " , XM31K is indeed based on the nicked named " Super IPS " panel from Panasonic . FSI also managed to push the peak luminance from quoted 1000nit to 2000nit . There 's a high likelihood that the actual product will be delayed though , the technology is just too futuristic to commercialise so quickly . 43971 @qwx453971 <p> I think you 'll find the reported 1000nit devices will be upped to 2000nit on release . Also some vendors are showing a 1000nit prototype but trumpeting 2000nit specs. 
@@44332929 @4332929/ <h> New Tangent Hub Released : v1.3.3 <p> Just a quick note to say there 's a new version of the Tangent Hub on our website . V1.3.3 Implements TIPC version 4 . Mac OS X and Windows only : Bug fix for excessive CPU loading at user log in . <p> V1.3.2 ( not publically released so included now as part of v1.3.3 ) New ' Goto Bank n ' action for banked controls . New reverse direction option for parameters and menus on knob and trackerball controls . New sensitivity option for mouse emulation action.Mac OS X only : Improved handling of fast user switch issue with warning . <p> Enter your product type ( wave , element etc ) It will now show you updates for the panel itself ( like firmware ) but not the Hub etc so ... Scroll down and enter your application ( what you 're using the panels with ) You will now see a list of all downloads and documents for your panel and your application . The Tangent Hub will be in this list . <p> We split our @ @ @ @ @ @ @ @ @ @ manuals for just the panels ) Application Support ( Tangent Hub and documentation for your panel and the application ) <p> You can do the same by going to the product page for your panel . Then click on the Application Support tab . <p> Hi Chris- I have been in touch via support channels with you folks on this , but we have been having no luck getting a Tangent Element to work correctly with Lumetri Color in the current release of Adobe Premiere CC . I have some workaround mappings from you folks that I have n't tried yet ( but will do so when time permits ) . It was great to be able to use the Element for some of the projects that come through here which simply needed some quick fixes in Premiere and would love to see a fix . When in Resolve , all is well . Thanks ! - JT <p> @John : As our support explained , the problems you are having in Premiere are all internal to Premiere and we 've reported these to Adobe . We are pushing for @ @ @ @ @ @ @ @ @ @ having problems and we 're sorry for any trouble they are causing , but due to the fact they are internal to Premiere there 's not a lot we can do . Fingers crossed Adobe come up with a fix soon . <p> Thanks Chris . I will send Adobe a report just for the heck of it , and will cross fingers with you ( was n't sure if this release of Tangent Hub incorporated anything relevant to this ) . In the mean time I will experiment with the mappings that were sent- I do appreciate that bit of help very much , and will be in touch through support if I find out anything that might be useful to you , or to other users . - JT <p> Just a quick note to say there 's a new version of the Tangent Hub on our website . V1.3.3 Implements TIPC version 4 . Mac OS X and Windows only : Bug fix for excessive CPU loading at user log in . <p> V1.3.2 ( not publically released so included now as part of v1.3.3 ) New @ @ @ @ @ @ @ @ @ @ New reverse direction option for parameters and menus on knob and trackerball controls . New sensitivity option for mouse emulation action.Mac OS X only : Improved handling of fast user switch issue with warning . 
@@44332930 @4332930/ <h> creating a 70s look on digital <p> does anyone have any tips for creating a 70s film look on digital ? i 've been grabbing frames from some 70s films ( jaws , the french connection , the getaway , etc .. ) to look for any generalised colour characteristics i could try and recreate but am having difficulties . <p> so far what i think i 've noticed are strongly saturated reds and therefore skintones that seem on the red side of modern convention , fairly strong blues but weak green and yellow . magenta seems quite weak too unless in something quite bold like a bright pink shirt . i 've also noticed strong colour contrast but i 'm not sure how much of this was the art direction of the time . strong colours seem generally to pop more against others . <p> i 've been experimenting with the channel mixer as that seems quite powerful in creating film-ish looks by cross-contaminating the colour channels , but ultimately i 'm just feeling about in the dark mostly . i 'm sure there are LUTs out @ @ @ @ @ @ @ @ @ @ but i 'm more interested in trying to create this look from scratch so if anyone feels like sharing an insight or two i 'd greatly appreciate it . <p> I can think of several huge 1970s films that used lots of diffusion , Mitchell Fog Filters or the equivalent : Star Wars , Superman , and Summer of ' 42 are just three I can think of . The Deep is another one that kind of goes all over the map -- I worked on that one for home video for a couple of months , back in the day . Tough film . The Sting is another one -- that 's a pretty lush-looking film . <p> I think 1970s films have a lot of different looks . I would n't say that ( for example ) The Godfather , Three Days of the Condor , or Jaws had the same look . If you 're trying to create the look of Kodak 5247 , I would say it 's a little crushed , a little contrasty , a little grainy , and a little saturated . But @ @ @ @ @ @ @ @ @ @ <p> Watch about a dozen influential 1970s films and you 'll get an idea of how films looked in that decade . To me , it 's more about the lighting style than the color correction . 
@@44332931 @4332931/ <h> What exactly is a DCDM ? <p> To Paulo : I use DCP-o-matic too . The parts of its color and gamma transforms that I 've checked are quite accurate . But it ca n't output X'Y'Z ' TIFFs as the archive probably expects the DCDM to be . <p> To Geoff : Wow ! DCP Builder is High Geek . The author has thought of every aspect of the DCP , including my pet project for varying the individual compression rates for the X ' , Y ' , and Z ' . Colorists will be interested in DCP Builder 's option to apply a Bradform Transform to shift the usual video D65 to the DCI/SMPTE whitepoint . ( Incidentally DCP-o-matic now allows more white point shifting than that . <p> DCP Builder gives a fresh interpretation to ' DCDM ' . <p> A DCDM is similar to DCP , only the frames are in either DPX or TIFF format and both sound and picture are not yet wrapped into MXF files . <p> This DCDM image structure may then be transported by being mapped into either @ @ @ @ @ @ @ @ @ @ <p> DCI might have coined ' DCDM ' but they made such a mess of defining it that it got away from them . " What exactly is a DCDM ? " Nothing , exactly . Gavin put it well : <p> there is no " DCDM police " 43971 @qwx453971 <p> But while the digital cinema industry did n't want , and did n't accept , the constraint of TIFF files , that same constraint is valuable in the archival sphere . An archive asks for a DCDM for different reasons than a digital lab does . An archive has to keep many more movies much longer , and to plan their accessibility and sustainability . So maybe " What exactly is a DCDM for archives ? " has an answer . <p> An archive asks for a DCDM for different reasons than a digital lab does . An archive has to keep many more movies much longer , and to plan their accessibility and sustainability . So maybe " What exactly is a DCDM for archives ? " has an answer . 43971 @qwx453971 <p> I think an @ @ @ @ @ @ @ @ @ @ especially a motion picture . The Academy has a couple of tech papers on the problem of long-term digital archival storage : <p> So to clarify the OPs question . In practices what delivering a DCDM has translated to for me , has been a TIFF XYZ version of the feature , uncompressed , with accompanying audio , and a set of discreet WAV tracks which match picture in length . I also hand off a spec sheet . Usually it 's a PDF . Once I needed to modify and XML . I 've never had to concern myself with any of the rest of the spec sheet . <p> I think most of the posts in this thread are misinterpreting the distinction between a DCP and DCDM . A DCDM ( digital cinema deliver master ) is n't packaged . <p> If they 're uncompressed , that would work fine as an archival medium . But ... it 's not mentioned by the Academy . Last time I checked a couple of years ago , they 're still " refining standards , " so part 3 of the @ @ @ @ @ @ @ @ @ @ posts on the thread missed the distinction between the DCDM and the DCP , but Geoff 's mention of DCP Builder for making DCPs was very useful because that software 's author uses the term ' DCDM ' in a loose-goose way reflecting its use within the digital cinema industry . <p> I missed the distinction between " What exactly is a DCDM in the digital cinema industry ? " and " What exactly is a DCDM for archives ? " <p> The thread began with " Juan Salvo was so right " and now Juan 's latest post sounds very right too . He " hands off a spec sheet " along with the DCDM . An archive , especially , needs to get the title , frame rate , run-time , audio sampling rate , and other basic identifying facts associated with the 120,000 sequentially numbered . tif files and the six channel-identified . wav files ( using my 80 minute 5.1 audio film as example ) that constitute the DCDM . Perhaps Juan can post an example of a spec sheet he 's made . <p> Juan @ @ @ @ @ @ @ @ @ @ XML seems to address the need of an archive that expects to ingest the DCDM and play it . The particular ingester/player must be able to read the XML . ( But then the archive should have the responsibility to make the XML from the spec sheet . ) Anyhow , making an XML is much simpler than fiddling with the headers in 120,000 . tif files , as DCI required . <p> The question of whether a motion picture archive should ever accept a compressed images is tricky . What if the compressed images are the motion picture ? I.e. , what if I make a movie using JPEG2000 compression and X'Y'Z ' encoding at every step and wrap those exact JPEG2000s into an MXF to make the DCP for release ? Then the DCP , more than the DCDM , represents the movie itself . More simply , if a motion picture is that which gets released -- today a DCP , compression artifacts and all -- then why archive something else ? I suspect that part of the archive 's wish for the DCDM is a hangover @ @ @ @ @ @ @ @ @ @ a negative in addition to a clean release print . Part of that wish was so that the release print could be projected and , if damaged , replaced by a new print pulled from the negative . Digital replication changes that reasoning . Possibly the negative ( or DCDM ) can reveal some information beyond the movie itself . Did the camera , or the actress , have a tiny pimple one day proving that scene A was shot before scene B , and the like . Pentimenti matter to the study of art . But a better reason for a motion picture archive to not accept compressed images is that image decoding is complex , and the software to do it is a bother to maintain . Archives are smart to settle on a really simple bitmap-like encoding . A future digital archeologist can likely get the picture out of a bitmap even if all playback software is lost . TIFF is a good choice because it 's so long standardized and so widely used . Even banks use TIFF for financial document storage . Likewise CIE XYZ @ @ @ @ @ @ @ @ @ @ vastly superior to the silly ACES color space -- google : Couzin , ACES , XYZ . ) DCI 's choice of gamma 2.6 is arbitrary , but simple . <p> What if the compressed images are the motion picture ? I.e. , what if I make a movie using JPEG2000 compression and X'Y'Z ' encoding at every step and wrap those exact JPEG2000s into an MXF to make the DCP for release ? Then the DCP , more than the DCDM , represents the movie itself . 43971 @qwx453971 <p> Yes - and for that the DCI proposes the term DSM : Digital Source Master . This can be whatever you master to in the first place . Filetype and colourspace can be anything . Rec709 , full or data levels , gamma 2.2 , 2.35 , 2.4 , 2.6 ? P3 ? sRGB ? <p> The whole point of the DCDM is to standardize as much as possible as to avoid any confusion about the nature of the images . <p> Barend , you missed my point . The DCP is even better standardized than the DCDM . @ @ @ @ @ @ @ @ @ @ DCP are essentially the same ( except for the MXF wrapper ) . Yet an archive has reasons to prefer the DCDM for that film over the DCP . <p> I should retreat a bit . The DCP does not specify the exact JPEG2000 bit rates for the X ' , Y ' , Z ' , so in that sense it is not fully standardized . But it is better standardized than the DCDM in the sense that you can ingest and play it almost everywhere . And in the particular case I described , the JPEG2000 bit rates are features of the film . <p> To Pepijn : TIFF rev. 6.0 file specification has no tag for framerate , at least no normal tag . " Tags numbered 32768 or higher , sometimes called private tags , are reserved for " special purposes . If DCI were serious about saving framerate info in each TIFF they could have done that . A single informative . dat file accompanying the . tiff files and . wav files is a more sensible place for global metadata , and that one @ @ @ @ @ @ @ @ @ @ . <p> Just to bring some level of closure to this thread i dug out some dcdm 's today and had a look <p> they all marked DCDM most Tiff but some DPX but remember its DPX 's that get handed over to make the DCDM ( which are actually the DSM ) but they get marked DCDM some time .. that 's how people name things <p> all were spilt into reels and in side each reel is the folder Marked at there frame size so 2048 x 858 etc containing the image files numbered 1 to what ever they all have audio folders and in side those is your channels for 5.1 and sometime and a stereo as well <p> now it look likes the audio boys are on point as all the audio wavs with out exception are mark with the frame rate so " **26;1524;TOOLONG " etc <p> which i would call " Hard coded meta data " <p> some times they have extra folders like " TXTLESS element " and other bit well <p> i saw no XML anywhere <p> i had to pull one @ @ @ @ @ @ @ @ @ @ in the tiff ..... there is no FPS meta data in the TIFF header ( which i was pretty sure of anyway ) <p> So what can you take away from is <p> from the audio / and the file names of folders you have all the info you need to make a DCP even with out a PDF or txt file <p> Thanks gavin . Now I 'll try to knot the end of the thread by reporting what the archive in question expects the DCDM to be . <p> The DCDM image sequence must be 16-bit ( really 12-bit ) TIFFs . In this the archive agrees with the DCI spec and is less liberal than SMPTE 428-1-2006 . <p> Of course the images must be coded X'Y'Z ' with the 1/2.6 gamma . <p> The DCDM images ca n't be padded . In this the archive agrees with both the DCI spec and SMPTE 428-1-2006 . My 1920x1080 DSM will be 1920x1080 in the DCDM , rather than pillar-boxed " flat format " 1998x1080 as it is in a DCP . <p> The image sequence folder name @ @ @ @ @ @ @ @ @ @ frame rate . This deviates from the DCI spec and is nearer gavin 's sensible " Hard coded meta data " . I will name the individual . tif files per DCI : **37;1552;TOOLONG . <p> But the reel number is a joke , since I will make just 1 reel for the 80 minute movie . To have multiple reels today is absurd . We are all supermen and superwomen who can heft the longest movies . <p> However , the archive required an SMPTE countdown leader , and an audio pop at the " 2 " . The countdown is for 24 fps despite the DCDM 's being 25 fps . <p> The audio files must be . wav , and the filenames must show title , reel , and channel assignment . <p> The archive accepts additional metadata in a . pdf , as Juan suggested . <p> I do n't know if many other archives agree . At the least , points 1 and 2 are very reasonable . <p> Making the DCDM is actually easier than making a DCP , since you do n't need @ @ @ @ @ @ @ @ @ @ XMLs writer . It is possible to make the DCDM with ordinary editing software that does n't " know " it 's converting to X'Y'Z ' and making a DCDM . This can be the subject of another thread , if anyone 's interested . <p> Have you tried making a single folder for an image sequence of an 80 minute film ? Last time I tried delivering DPX I split it into 5 reels and the OS nearly choked even then . That was about 5 years ago and maybe Windows 7 32bit but I tried it on OS X and it still took ages ! <p> To Jamie and Juan : Thank you for the warning . I hope I can slip it by . At my end , when I used DCP-o-matic to make an 80 minute DCP , it output , besides the DCP , a folder called " Info " containing 120,000 little data files , one for each frame . My Mac OS 10.8.5 did n't hiccup . When I threw them in the trash , it also did n't hiccup . But I @ @ @ @ @ @ @ @ @ @ some software beyond the file manager can have trouble handling so many files in a folder . <p> The archive is asking for the DCDM on a Windows format ( NTFS ) drive . NTFS allows billions files in a folder , but a problem might arise with the software that converts from the Mac file system to NTFS . I 'll know in a few days . <p> Also problems may arise with the ingesting or playback software at the archive . This seems to me their problem . In a few years their new software wo n't have the problem . Thinking archivally -- preserving the work -- if I can get the 120,000 TIFFs to the archive in one reel , that is the best solution looking forward . Having separate reels , with separate Academy leaders and non-unique frame numberings , and the sound also cut up , is not how a continuous piece should be archived , because it increases the risk of archival error . <p> On reel sizes for DCDMs . It is interesting to look at slide 5 of the 2012 SMPTE @ @ @ @ @ @ @ @ @ @ DCDM reel for 24 fps movies ; 50,000 frames @ 48 fps ; 100,000 frames @ 96 fps ; 125,000 frames @ 120 fps . That 's always about 17 minutes per reel . They are considering ingestion not for real-time playback but for a DCP-making process . Perhaps they will dedicate faster computers to make DCPs for higher frame count movies , just so the process does n't take terribly long . But why are they expecting the faster computers to also be able to handle more files per reel ? Anyhow , Technicolor makes some DCDMs with 125,000 frames per reel , but these might never leave Technicolor . <p> Dennis i , m really glad your making headway with this , but i have to say i haven , t seen a DCDM that 's made in one Reel . <p> i may be wrong here but i sure i read somewhere that its split in to reels for the actual DCP as good practice as it sort of equates to film reels .. pretty obvious that one <p> however its also can been an issue with rendering the @ @ @ @ @ @ @ @ @ @ some times have an issue load it all at once hence the split reels and xml track of subtitles for each reel <p> if you don , t have subtitles or your subtitles are burned you could be ok ! ! ! ! <p> but i repeat i have never seen a dcdm that 's one reel so they may be other reasons i don , t know about <p> When I last delivered dpx for a master I split it into five reels and only put a slate on the first one . I cut the audio tracks and did the same with them although I was a little worried about splicing up the wavs like ( on a frame , not a sample ) that but I did n't get any problems . Is that normal practice ? <p> When I last delivered dpx for a master I split it into five reels and only put a slate on the first one . I cut the audio tracks and did the same with them although I was a little worried about splicing up the wavs like ( on a @ @ @ @ @ @ @ @ @ @ n't get any problems . Is that normal practice ? 43971 @qwx453971 <p> The thing is Jamie you could have just delivered one mass of DPX and the 6 discreet channels if you were delivering to a proper DCP facility like SDC over here and not bothered to cut in to reels <p> the big boys use Clipster and you can just hit the automatic reel button and it will chop it up in to less that 20 min segments <p> What you have is a DSM and there not that many rules to that one , In clipster you can fire it out as DCDM and they can drop the leaders in on the Clipster time line or fire it out to DCP ... its an amazing bit of Kit <p> however very pricey ... but if your doing it ... properly .... properly ... you will be doing via a Clipster ... and that 's why it costs more but its so worth it <p> some one delivered a DSM to me which was an H264 once ..... Oh how we laughed <p> I stand corrected . I finally @ @ @ @ @ @ @ @ @ @ archive had given me the OK to try the " experiment " of 120,000 frames in one reel . It went smoothly until I had to transfer it from my Mac system to an NTFS drive ( specified by the archive ) . Tuxera is supposed to allow this . The transfer got slower and slower . It divided the files into groups and moved the first 32,768 files in about 3x normal time . It could n't get through the second group . There 's no problem handling 120,000 files in a folder in either Mac OSX or in Windows . The problem arose with the crossover . Not knowing what systems and software the archive uses , I abandoned the experiment for a better day . 
@@44332932 @4332932/ <h> Making a career shift into color work , any suggestions ? <p> In Resolve I need more practice &amp; competence with things like order of operations such as which corrections to make when . I also need to understand some of the finer points on nodes ; splitting/combining , layers etc. , since they 're so different from Photoshop or physical paint mixing , which are the closest analogs to color grading that I 'm closely familiar with . I can figure it out , but I 'm probably too slow &amp; futzy for high volume turnaround . I also need to know more basics on normalizing some of log &amp; raw formats . We splurged &amp; bought a used Blackmagic pocket cinema camera this winter , ( mostly for home movies ... ) &amp; the latitude is so wide that there 's about a million things to do with it that I have n't even thought of . Then there 's the whole broadcast angle which I know only a tiny sliver of . Anyway , I 'd like to feel more sure of myself in those @ @ @ @ @ @ @ @ @ @ to do is get some opportunity &amp; then mess it right up . <p> I know this conversation has had some contentious parts , but it 's been so valuable . Thanks 43971 @qwx453971 <p> Everybody learns differently but I have found the best way I learn a software program is to actually do a real project with a real ( longish ) deadline . I would try to get some paying ( low at first ) UNSUPERVISED jobs and dig in . Just playing around has always lead me to get bored and waste time ( that 's me , YMMV . ) It 's too bad that you ca n't start as a film dailies colorist ( an assistant before that ) because you would have to get good and fast and that would serve you well , but that ship has sailed .... I would contact a non-profit that you had a passion for and see if they would let you produce a short video for them with the BMPCC and then edit and color grade ( not to start a new thread but I think it @ @ @ @ @ @ @ @ @ @ <p> If you do edit I would try and grow both editing and grading as that will lead to more revenue streams in the future , its really tough out there ! <p> Always remember that free work ( for businesses ) leads to more free work . <p> In Resolve I need more practice &amp; competence with things like order of operations such as which corrections to make when . I also need to understand some of the finer points on nodes ; splitting/combining , layers etc. , since they 're so different from Photoshop or physical paint mixing , which are the closest analogs to color grading that I 'm closely familiar with . I can figure it out , but I 'm probably too slow &amp; futzy for high volume turnaround . I also need to know more basics on normalizing some of log &amp; raw formats . We splurged &amp; bought a used Blackmagic pocket cinema camera this winter , ( mostly for home movies ... ) &amp; the latitude is so wide that there 's about a million things to do with it that I @ @ @ @ @ @ @ @ @ @ whole broadcast angle which I know only a tiny sliver of . Anyway , I 'd like to feel more sure of myself in those departments before moving on . The last thing I want to do is get some opportunity &amp; then mess it right up . <p> I know this conversation has had some contentious parts , but it 's been so valuable . Thanks 43971 @qwx453971 <p> Although there are a lot of opinions on here about how to learn Resolve . I think it 's important to learn from as many people/sources as possible . My introduction to color started behind a camera . As I have gotten older , I am more often in front of a monitor than sporting a Steadicam . Although I am not dead yet and just shot a music video on three different sets with three all-nighters - not bad for an old guy . <p> I would suggest a few options : First , there are some really nice people on here who are very helpful and generous : Marc Wielage , Margus Voll , Jason Bowdash , and @ @ @ @ @ @ @ @ @ @ friends so I mention them . There are , however , a lot of other nice people on here and you will find them . Second , at the risk of being flamed , I think there are some immediate tutorials worth considering if that is something that interests you : 1 ) The folks over at mixinglight.com ( good people ) ; 2 ) Rippletraining.com ( Alex and Jason ) , and frankly I kinda like this gal for basic , down and dirty orientation to Resolve 3 ) https : **37;1591;TOOLONG . <p> Resolve 's nodal operations are incredibly frustrating from the UX perspective and the concepts they employ . If you feel like you need to wrap your mind around nodes , just about any compositing software like Fusion or Smoke will be a better learning tool before you resign to the limitations of Resolve . <p> Resolve 's nodal operations are incredibly frustrating from the UX perspective and the concepts they employ . If you feel like you need to wrap your mind around nodes , just about any compositing software like Fusion or Smoke will @ @ @ @ @ @ @ @ @ @ limitations of Resolve . 43971 @qwx453971 <p> You 're so negative , Igor ! Hey , trust me : Resolve is better than the old daVinci 2K . <p> One key thing I would tell Phoebe is that meeting and having business relationships with clients is the core of getting more work . Often ( good or bad ) the successful colorists are those who are good with people and can establish a rapport with DPs and directors . A lot of that boils down to luck and timing . There are colorists out there who are not that great technically , but they can get the job done and make the clients happy . By the same token , there are colorists who are brilliant technically , but lack the people skills to get lots of repeat business . Ideally , you have both , but everything is a compromise , especially in business . <p> learning tools that are suboptimal is better than learning nothing , but learning tools at the top of the class at a minimum opens one 's eye 's to other possibalities <p> Phoebe @ @ @ @ @ @ @ @ @ @ then at some point loading Baselight Edtions would be a good way to get some exposure to a tool set beyond the confines of LGG , but i would not turn on the demo licence untill i had read the manual top to tail , and maybe wait for the upcoming 5.0 release as it has new color tools that work well for artists , and it has mouse/pen freindly UI option that is unique , well draws upon Cyborg2k and early Q UI a bit i guess , but not on offer anywhere else and allows one to make very precise changes with a pen or mouse <p> Your depth of knoledge in color theory and design already stands you out from the crowd of folks who can not bring that education/life/work experience into the suite <p> In terms of mixing colors as you mentioned earlier . i tend to use L*a*b alot in Resolve , and do most of my heavy lifting in there , if you understand the maths behind it , manuliplating the UI makes sense to me and the way i see color working , @ @ @ @ @ @ @ @ @ @ take an image and how cleanly and quickly i can do that .... unfortunatly there 's nothing in any of the training i have seen , and nothing in the manual what.so.ever . about this , but it 's the similar to working in L*a*b in P'shp , same maths underneath - so if you can draw upon that experince , and the UI works for you , then you will already be well on the way to offering something that others who have read the manual , taken online training and never stepped beyond LGG can dream of offering <p> BTW , i lean to fast &amp; futzy , it does not need to be " or " .. " and " can work too <p> Thanks so much for all of your help ! I decided I needed to devote more time to research &amp; practice so that 's what I 've been doing . You really helped me pinpoint the things I had n't known I had n't known , if that makes sense . <p> I also recently purchased a starter fsi monitor &amp; took a @ @ @ @ @ @ @ @ @ @ currently working with an independent filmmaker I 've worked with before to grade his first feature . <p> It 's interesting , coming from the fine arts side rather than the technical side , some things are very easy to pick up and some things are taking more time than I 'd hoped to pick up , but things are coming along . 
@@44332933 @4332933/ <h> LA " Coloring in HDR Workshop " from Filmlight &amp; Dolby 4/17/2017 <p> In conjunction with our long-standing partner Dolby , you are invited to a special presentation on Monday , April 17th , at the private Dolby Cinema Vine Theatre . <p> FilmLight and Dolby will demonstrate Dolby Vision technologies and workflows - using the Baselight color grading system alongside the Dolby Vision HDR projection to illustrate the advantages and pitfalls of various approaches to HDR grading and mastering ( along with delivering in SDR as well ) . <p> Covering the creative aspects of working in HDR , renowned Deluxe Colorist Doug Delaney will share his insights and experiences mastering 4K and HDR films for Sony Pictures . Peter Postma , Managing Director of Americas at FilmLight , will discuss the technical aspects of working in HDR , with an overview of current HDR systems , requirements for mastering in HDR and how they are addressed using the Baselight toolset . Tom Graham of Dolby Laboratories will cover the Dolby Vision technology overview and workflow . 
@@44332934 @4332934/ <h> Resolve 14 - The Fairlight Tab <p> I have to say , if you needed After Effects , Premier , inDesign , Media Encoder , and Acrobat Pro ( all of which I use ) , $600 a year for the latest version is not ridiculously expensive . I do agree that their $20-per-app monthly fee is silly . <p> I see Daniel 's point and I wonder how much longer Adobe can keep the subscription model going . There 's a lot of hostility to it out there from the editors I know . I 'm still reluctant to go for the idea of a Swiss Army Knife program that does editing and sound and color and VFX all as one program -- but I would have no problem with four programs you could use on one computer or on four separate computers , all part of a single suite . The danger to me is that Fairlight was always an also-ran among the sound mixers I know , and Pro Tools was always the gold standard there . But maybe that 's an LA thing . @ @ @ @ @ @ @ @ @ @ It 's a cost savings , if having the latest version is important - considering perpetual licenses were generally a couple grand , and a new version came out every 18 - 24 months . <p> The integration is worrying , I hope if it is a problem they will cleave the apps apart again . It 's even scarier because of the breakneck speed at which they integrated Fairlight . There are probably some serious bugs lurking beyond the surface . 43971 @qwx453971 <p> Yep , that is a concern . This is the problem I have with the idea of a blender , a hammer , and a wrench all in one product . After awhile , the toolbox gets kinda heavy and crowded . <p> As a freelancer , I like the subscription model , because it prevents the situation where I have to shell out an enormous sum to upgrade because one of my many clients did . It keeps costs predictable . If I go though a lean time , I can always cancel it for a while . 43971 @qwx453971 <p> I think there @ @ @ @ @ @ @ @ @ @ the initial investment -- say , $995 -- and then charge a very reasonable price for upgrades , like $99 per year , which would pay for the R&amp;D on new versions . ( Even 100,000 Resolve upgrades would be $10 million dollars a year . ) If you do n't want to upgrade , do n't upgrade ... live with the old version . But BMD goes with charging one price and then free upgrades , no subscription . Adobe goes with subscriptions that include upgrades . Avid essentially has both . <p> I almost do n't care what each software maker does , but I still wish BMD would offer some kind of " premium support " so you could call somebody at 2AM when a render failed for a non-obvious issue , or the thing crashes for some weird flakey reason . When I 've worked for Baselight facilities with factory support , they 'd log into the Baselight machine , see the problem , then work on a patch and fix it in a few hours . Amazing support from the guys in London and LA @ @ @ @ @ @ @ @ @ @ support is very good , when they 're open . <p> I just watched a couple of demos of the Fairlight section , and I noticed that many of the sound editing options are different in the Edit page than in the Fairlight page . I find that a little strange given that it 's one big program . I do n't dispute that Fairlight is probably the biggest alternative to Pro Tools out there and it can totally work for film , TV , commercials , and so on . <p> Another interesting problem : the accelerator card they make ( " runs up to 1000 tracks ! " ) is PCI only , which means it wo n't work in any Mac made in the last 3 years . And the manual is bereft of much Fairlight info at all . <p> But I would say that in time , Fairlight is going to be a very , very formidable competitor in the world of sound . I would bet in another six months , if they can price the mixing consoles competitively , it would very definitely @ @ @ @ @ @ @ @ @ @ in one room for color , Resolve in another room for editing , and Resolve in a third room just for Fairlight audio ... with a total overlapping of workflow in the same timeline . And it 's conceivable that the sound mixer would n't ever touch color , the colorist would n't touch sound , and the editor would just edit . It 's definitely a new way of doing things and I could see a lot of advantages , once people have had a chance to learn the new systems and get up to speed . <p> But I 'm still simultaneously shocked and non-plussed by the pricing of Resolve Studio at $299 . I know there was some huge grousing about the $995 price back in 2010 ( down from $100K+ ) , but you kind of wonder about the reasoning of the pricing structure . I would 've had no problem paying $995 for a program that could do all of this -- and in fact , had no problem paying $995 just for color 6 years ago . <p> II do n't dispute that Fairlight @ @ @ @ @ @ @ @ @ @ and it can totally work for film , TV , commercials , and so on . 43971 @qwx453971 <p> i personally prefer ardour resp. harrison mixbus over both of them . but that 's just my privat opinion . what really matters is a working path of exchange between different applications ! <p> if BMD would somehow document their project file formats and make them accessible to other applications as well , i would not fear any additional features and capabilities of resolve . but closed proprietary systems , which are isolated from the outer world , but strive to become the monopolistic general purpose answer to all our needs , can become very unsatisfying in the long run . <p> I 'm sure that if Avid really wanted to , they could combine Avid and Pro Tools into a single program ... 43971 @qwx453971 <p> FWIW , besides ideology of when programs should be separate or integrated , the there is a real technical problem that keeps MC and PT separate . It 's that MC can not do sample rate based sound editing . The smallest discrete unit @ @ @ @ @ @ @ @ @ @ to do that you have setup the project as Film project from the outset and use the Perf Slip command . And even then , MC uses some trickery that sets the project 's frame rate to four times its actual rate in some of the inner workings of MC . <p> FWIW , besides ideology of when programs should be separate or integrated , the there is a real technical problem that keeps MC and PT separate . It 's that MC can not do sample rate based sound editing . The smallest discrete unit it can address is 1/4 of a frame , and to do that you have setup the project as Film project from the outset and use the Perf Slip command . And even then , MC uses some trickery that sets the project 's frame rate to four times its actual rate in some of the inner workings of MC . It 's for this reason that MC can not open a PT project . 43971 @qwx453971 <p> Well , they 're pretty transparent as far as funneling stuff back and forth . It @ @ @ @ @ @ @ @ @ @ deal . <p> A sound editor I know and have worked with for years has told me he cringes whenever he 's handed an FCP7 or FCPX project . He said that Premiere " used to be horrible but got better in the last year , " and Avid has been fine since about 6.5 and up . Just having Avid hold on to the original track names for Pro Tools is very helpful . <p> Also , what sound mixer wants to put a 300 watt GPU in their machine that they do n't need ? That 's a lot of extra heat and noise . 43971 @qwx453971 <p> In theory , that should n't be an issue , because the sound editor could have a dedicated sound editing machine that includes a Fairlight card instead of a GPU , and the reverse for the colorist . BMD 's reps said you can start a render on another machine , which is also BMD 's workaround for providing ProRes rendering in Windows ; just render from a mac on the network . <p> Once we find out whether or @ @ @ @ @ @ @ @ @ @ whether or not the sound editor will need a GPU . <p> the sound editor could have a dedicated sound editing machine that includes a Fairlight card instead of a GPU 43971 @qwx453971 <p> It used to be that Resolve would complain if you tried to start it with a GPU that did n't meet it 's minimum specs , but I guess it would n't necessarily need to be a top end model . <p> Remember you 'll probably still want to be able to have a video replay when you 're sound editing , so if you want to play back the timeline direct from the Editor/Colourist , there will need to be some thought given to what GPU might be required . <p> I 'm guessing what will happen is that either you 'll run in some reduced quality mode , or you 'll be supplied with proxy media you can switch to , or you 'll have a version of the timeline with a low rez render of the final grade . <p> It used to be that Resolve would complain if you tried to start @ @ @ @ @ @ @ @ @ @ minimum specs , but I guess it would n't necessarily need to be a top end model . 43971 @qwx453971 <p> I 've been able to run the latest versions of Resolve on an Ultrabook with only integrated graphics , though I 'm getting a GPU soon ( via Razer Core ) . I can edit in Resolve without a GPU as long as Resolve is the only thing running , but color grading chokes it pretty quickly . <p> Remember you 'll probably still want to be able to have a video replay when you 're sound editing , so if you want to play back the timeline direct from the Editor/Colourist , there will need to be some thought given to what GPU might be required . <p> I 'm guessing what will happen is that either you 'll run in some reduced quality mode , or you 'll be supplied with proxy media you can switch to , or you 'll have a version of the timeline with a low rez render of the final grade . <p> Remote rendering on Resolve was introduced in V12 and @ @ @ @ @ @ @ @ @ @ Windows to Mac . 43971 @qwx453971 <p> I 'm surprised that remote rendering has n't gotten more attention in the past . The first I 'd heard of it was when the BMD reps told me about it , though there were n't any free Resolve machines available for them to demo it . <p> ... But I would say that in time , Fairlight is going to be a very , very formidable competitor in the world of sound . I would bet in another six months , if they can price the mixing consoles competitively ..... <p> I would be very surprised if we do n't see a Fairlight audio panel roughly the size of the Resolve Mini Panel , maybe 8-10 faders. which would totally be enough for small projects . Digidesign has similar panels for Pro Tools as well . <p> I certainly want a small mixer to replace the mackie MCU I used to be able to use with FCP7. 43971 @qwx453971 <p> Yeah , I used a Mackie HUI back in the day -- that was a very nice small-format control surface . Really @ @ @ @ @ @ @ @ @ @ one from Brian Wilson 's engineer and used it with Pro Tools for years and years . I think it was about $6000-$7000 back in the day , but no doubt BMD could make something similar and sell it for maybe $3000. 
@@44332935 @4332935/ <h> Implementing a 3x3 matrix in Resolve ? <p> Alex , I think the answer to most of your questions is : because Resolve is not Nuke , and such precision was never need for grading . If you 're choosing to use Resolve as a workflow tool , you 're kind of hacking it <p> The whole thread is a hackers ' thread anyway <p> Apart from this , I second what Juan said : OFX plug-ins. 3x3 Matrix should be relatively straightforward to implement . <p> Thanks Alex . I believe all that needed to be said , but preferably by someone with a bit more experience and clout than myself . The premise of this thread was in essence quite simple ( all of which was aptly contained in the title alone ) , and I was trying my best to find a solution . I had no other agenda , I was n't trying to cause trouble , and I certainly was n't intentionally trying to impact negatively on anyone 's revenue stream . I was just trying to solve a problem , that 's @ @ @ @ @ @ @ @ @ @ the contents of this thread , I ca n't help wondering if it would have been in the interests of everyone that certain commercial interests of some of the participants had been declared , otherwise it can not but possibly cast a shadow over such participants contribution , though I am in no way saying anything funny was definitely going on . Nick has stated in other threads his commercial interests involving LUTs , though that was not mentioned here , and I believe that Juan has been involved with Nick in testing some LUTs that have some connection to RCM , though that too was not mentioned . I 'm sure there 's nothing in that , but for the sake of greater clarity and a fairer appraisal of the contents of this thread then it would probably have been a better thing that people were aware of such information . <p> Paul , Nick and I were both trying to help you achieve your goal , nothing more . My testing of Nick 's work has no bearing on any of this . If it would make you @ @ @ @ @ @ @ @ @ @ sometimes test and aide in that work . On occasion I refer clients to Nick . That 's the extent of of our involvement . How any of that relates to or is an issue with this thread is beyond me . <p> I also agree with everything Alex said . I am hoping that the Fuse technology from Fusion , enabling users to create their own shader based custom processes , may make it 's way into Resolve . That would open up a lot of possibilities . <p> Paul , I certainly did not intend to come across as negative about what you are doing in order to promote my own services . If people can work out clever ways to achieve something , then more power to them . Other people need to employ me to develop stuff for them , and obviously I am happy that is the case , or I could n't make a living ! <p> Can someone tell me how someone would use a 3x3 matrix in grading and what does it do that ca n't be done with the tools provided @ @ @ @ @ @ @ @ @ @ " Color " subforum , at least a quarter of them could make use of a 3x3 matrix . Most questions about " I need a Lut to go from X display to Y display " Most questions about " how do I match this camera to that camera " <p> The fancy new RCM system is basically 3x3 matricies , some curves , and some metadata .. All hidden from the user . <p> Alex , I think the answer to most of your questions is : because Resolve is not Nuke , and such precision was never need for grading . If you 're choosing to use Resolve as a workflow tool , you 're kind of hacking it 43971 @qwx453971 <p> I could also say it 's because Resolve is not Baselight . The Filmlight guys have all of this stuff on lock . Sure , they have a background in proper hardcore colour management , not just lift-gamma-gain on video signals . But it 's 2015 , this stuff should be table stakes by now . <p> Resolve has pretensions of being an end to end @ @ @ @ @ @ @ @ @ @ . <p> I just tested generating a LogC to linear LUT in LUTCalc , and I get the same result you do . It would appear that ( at least as far as LogC is concerned ) the maths in LUTCalc is wrong . 43971 @qwx453971 <p> A bit of a leap back to page one of this thread , but thanks to Paul and Nick I realised that there was a bug in LUTCalc over the output scaling being changed correctly for some presets when changing input transfer function / gamma . That was why Nick was getting the wrong values , but when I made one up , manually setting output to ' legal ' things all dropped back into place . A UI issue plus my cameraman 's perception of nomenclature rather than the maths . Phew and d'oh ! <p> If anyone has given / fancies giving LUTCalc a try , I 've updated it plus a couple of other bugfixes and a new adjustable knee tool that I 'm fairly chuffed with . The desktop Chrome App and online versions are up , but the @ @ @ @ @ @ @ @ @ @ . Hopefully a day or two . Thought I 'd better mention it now though as I 'm embarrassed ! Plus any feedback on the online version would be welcome . <p> All the colour operations are carried out in linear space and I could add a matrix tool . My initial thoughts would be immediately after colour temperature adjustment , automatically make it a matrix of eigenvectors ( as happens with in camera settings , where the principal diagonal values are dictated by the user-settable values ) and incorporate an ' apply in xxx colourspace ' dropdown selection i.e. automatically convert from working colourspace to the ' apply in ' choice , perform the matrix and then convert back to working colourspace . That might help in keeping matrix values consistent between source cameras and take LUTCalc 's choice of internal working colorspace out of the equation . <p> Stellar work Ben ! Just tested a couple of transforms and levels appear to be correct . Your matrix tool suggestion makes sense and might I suggest you later expand this to allow user input of primary chromaticity and white @ @ @ @ @ @ @ @ @ @ derived matrix ( just for reference in an info panel ) or in reverse , enter a 3x3 matrix and display the derived chromaticity and white point . @Alex did something similar ( primaries to xyz ) with his nuke node script here LONG ... <p> I wonder if anyone has considered developing a lut standard that allows use of math operators i.e. where the header could contain a matrix and transfer function math ( as you can with CTL for ACES IDTs ) , plus an additional/optional cube component for any targeted shaping/gamut corrections/look etc . It could be a very flexible , universal solution and negates begging each developer individually to add new tools to their apps - although those tools would be most welcome in Resolve . <p> This would of course require devs to adopt/support yet another standard but it would be powerful . Just food for thought . <p> My ideal scenario would just be BMD just adding support for OCIO . It 's pretty unlikely given that they just went and re-invented the wheel with RCM , but you can always hope . More @ @ @ @ @ @ @ @ @ @ to user configuration . <p> Rather than creating a new Lut format , OCIO let 's to string a bunch of existing ones together . 3x3 matrices can either be represented in the config directly , or using a . spimtx file ( basically just a human readable text file containing the matrix values ) . <p> So a transform may be represented by a series of smaller transforms . ie : 1D Lut -&gt; Matrix -&gt; 3D Lut Or something more absurd like : 3D Lut -&gt; Matrix -&gt; 1D Lut -&gt; CDL values -&gt; Matrix -&gt; etc -&gt; etc . The config system is very flexible and powerful , but can hide a lot of the complexity from the end user if you do n't want them to get overwhelmed . <p> Autodesk 's system in the more recent versions of Flame is pretty slick ( sort of a half way point between OCIO and a more traditional LUT ) <p> Yes , OCIO would have been a great addition and expandable without BMD having to do much else . It 's a bit puzzling why they chose to @ @ @ @ @ @ @ @ @ @ as marketing goes , a BMD proprietary color management system looks cooler than a third party integrated one , even though OCIO is better in a lot of ways . TuttleOFX has basic OCIO config support and works in Resolve but it 's not GPU accelerated . A GPU accelerated OCIO plugin is coming though <p> Yes , the ability to add a node which could do a gamma transform and also a 3x3 matrix transform would be really valuable . Of course you can use something like Lattice or Lightspace to build a LUT to do this , but it would be better to be able to do it in resolve , and it would be higher quality especially since if you are doing a gamma &amp; color space transform a 3D LUT does n't have much resolution along the grayscale axis so not ideal for gamma transform so you really want a 1D+3D LUT which just adds an additional layer of complication . <p> Would be great to be able to transform gamma and 3x3 color spaces via a node . <p> OR implement CTL , but that @ @ @ @ @ @ @ @ @ @ an OFX plugin to do CTL . My guess is it will be slow and buggy <p> Gamma and 3x3 matrix should be very easy for BMD to code and very very fast for the GPU to compute in realtime . <p> ... might I suggest you later expand this to allow user input of primary chromaticity and white point or a matrix . You could then display the derived matrix ( just for reference in an info panel ) or in reverse , enter a 3x3 matrix and display the derived chromaticity and white point . @Alex did something similar ( primaries to xyz ) with his nuke node script here LONG ... 43971 @qwx453971 <p> Hmm Andrew , <p> you got me thinking about the primaries and white point suggestion - it 's ( relatively ) easy for me to add , and importantly for me easy to add in such a way as not to complicate the UI for people who do n't need it ( as a cameraman , I hate the matrix settings in camera - they are useful and powerful , but a 3x3 @ @ @ @ @ @ @ @ @ @ the desire for matrices seems to be in great part about transferring between colour spaces , primary + white point is much cleaner . <p> It also means that the generated transform can be used just like any other gamut , ie white balances , ASC-CDL etc. should be consistent . <p> Anyway , I 've had a stab at it , and uploaded it here : LUTCalc online testing . The total size is around 25MB , so might take a bit of time for the splash screen to go on a slow connection . <p> To use , just set either the recorded or output gamut to ' Custom ' and the box should pop up . You can change the default values , add extra options or remove them and select which options you want to apply under custom in the recorded and output selections . Everything should also save and load with ' Save Settings ' and ' Load Settings ' . To test that out I 've entered the suggested RED primaries and white points Paul linked to on page one . Here is the @ @ @ @ @ @ @ @ @ @ ' on the right hand side in LUTCalc ) : RED Colourspaces Settings . <p> This has been knocked up by me in two or three hours last night and this morning , so consider it distinctly alpha ! By the same token , let me know what you think / find . I 'm keen to make things flexible for those who need it , but not add confusion / potential for finger trouble to those who do n't ; - ) . <p> Very impressive Ben . I do n't know how many other people would use it , but I still think a custom 3x3 matrix option is useful . There are occasions where you want to apply a 3x3 matrix derived by means other than calculating it from primaries . Also there is the additional complexity of white point adaptation , using e.g. Bradford matrix . Rather than trying to put everything into your UI , it might be easier to let people calculate the matrix externally , and input the values . <p> I 'm with you on the matrix - I was just concerned @ @ @ @ @ @ @ @ @ @ they are doing which is too easy to play with disasterously by those who do n't and b ) how to have it work with internal processing colourspace of LUTCalc , when the matrix could and should be designed for the colourspace of the user 's choice . <p> Reading your post I had a sudden ' ding ' moment and I could actually place it in the same position as this tool I 've just added ( rather than in the middle as I suggested yesterday ) . I 'd just have to make sure that there is a dropdown for ' intended working colourspace ' ( or something more catchy ! ) that would then slap on a further matrix to LUTCalc 's working space , so that stuff like white balance adjustments are consistent and correct . It would also keep it safely tucked away from the casual user . It should n't be too difficult to do , but unfortunately I 'll probably not be quite as quick with that addition - there is a serious possibility of being beaten over the head with a MacBook @ @ @ @ @ @ @ @ @ @ it right now ! <p> By default with stuff like white balance I use CIECAT02 as the CAT as it 's from CIE , fits with the ACES or XYZ matrices which I have found from several manufacturers and seems to be the CAT that Sony uses these days internally to the cameras . Under ' advanced ' in the white balance tool there are a few other options , including the simplified ( von Kries-based ) Bradford . The proliferation speaks to what you say about ' additional complexity ' ! <p> I 'd just have to make sure that there is a dropdown for ' intended working colourspace ' ( or something more catchy ! ) that would then slap on a further matrix to LUTCalc 's working space , so that stuff like white balance adjustments are consistent and correct . 43971 @qwx453971 <p> If you want to cover all bases , you need " working colourspace " and " working gamma " drop-downs . Although most 3x3 matrices should be applied in linear light , some , such as the ( admittedly now depreciated ) ARRI @ @ @ @ @ @ @ @ @ @ LogC . <p> I get that this level of functionality can be dangerous in the hands of users who do n't understand what they are doing but it 's very intuitive to someone like me . I built a few tools in Matlab and Python for analyzing and computing colorspaces but they 're not half as pretty . <p> I think matrix input would be very useful . It 's simple math to calculate between XYZ and xy to derive primaries and wp and then input into LUTCalc but it would be one less external step , however , I agree with Nick about the complexities when factoring any CAT into the matrix conversion . <p> The CAT options available are very useful . As you say , ARRI and Sony use CAT02 and I half suspect Canon DSLR color science uses CAT02 but anything Adobe-related will likely use Bradford ( CinemaDNG standard recommends Bradford ) . <p> I 've started experimenting with turning my DRX hack into an app . Is that something that might be of interest to other people ? <p> Using it in conjunction with the @ @ @ @ @ @ @ @ @ @ my own ) I have used it successfully to create compound nodes which perform colour space transforms , extending the functionality of RCM. 
@@44332936 @4332936/ <p> You profile with Calman and LS , then generate LUTs . They are created with 2 different sets of patches . After you verify with LS , you are probably verifying using subset of LS patches ? <p> What I wanted to say - to me it seems logical that LS will perform better if you verify it against it 's own set of patches . It would be better to calibrate with similar sets ( not just similar number of patches ) and then do comparison . Or I just said something completely irrelevant <p> in my opinion method Steve used is pretty logical . It shows dE you will get on both systems in same amount of time . Also if you look closely calman produces some out of gamut errors , which is no good at all . <p> The verification used a huge number of patches ( 9261 ) deliberately to make the comparison equal . I did not use a subset of any patch sequence . As a result , there is no benefit to LightSpace at all in the final verification . @ @ @ @ @ @ @ @ @ @ Optimised patch set calibrations to prove that . Again , all verified with the huge 9261 patch set . <p> I would have liked to have Calman run a 213 profile , but I could n't get it to complete such a large measurement set . The software failed both times I tried , so I gave up . <p> I wanted to make sure there were no potential issues with the integration of the calibration software with the patch generator . There have been a lot of reports of such issues , so I just wanted to avoid them I know the DVDO works accurately with both LightSpace and Calman , and as I have one to hand , I used it . <p> in my opinion method Steve used is pretty logical . It shows dE you will get on both systems in same amount of time . Also if you look closely calman produces some out of gamut errors , which is no good at all . 43971 @qwx453971 <p> ... measured on LS . To me , it seemed like making DCP in Resolve ( RGB-XYZ @ @ @ @ @ @ @ @ @ @ Of course it will look good when using same math and color science . <p> ... measured on LS . To me , it seemed like making DCP in Resolve ( RGB-XYZ ) and checking XYZ conversion using Resolve XYZ-RGB LUT . Of course it will look good when using same math and color science . 43971 @qwx453971 <p> No , it 's not the same at all . It is the probe that is making the measurements - LightSpace then simply plots the results , so it makes no difference what software did the actual calibration . An inaccurate result is simply an inaccurate result . I could have just as easily used Calman to perform the verification , and the results would have been identical . <p> A calibration is a calibration - there can be no difference in the results when that calibration is verified . <p> Yes , although I only used a 173 LightSpace profile patch set for calibration , not the larger 213 . Argyll took 1 hour 40 minutes for a 2527 patch set . LightSpace took 1 hour 30 minutes for its @ @ @ @ @ @ @ @ @ @ run the exact same tests with the same results - he may have the data you are after . 43971 @qwx453971 <p> The tests I ran did not start out with a display with such large errors as the one you used but the conclusions were much the same . The CalMAN eeColor LUTs in the current version are bumpier than they should be and can cause visible problems if you are making large corrections to your native device . LightSpace compared to ArgyllCMS are more or less identical , for a given number of profile patches ArgyllCMS produces a slightly better average outcome using it 's default patch set ( optimized irregular patches ) while LightSpace edges out ArgyllCMS using it 's default regular grid patch sets . <p> The tests I ran did not start out with a display with such large errors as the one you used but the conclusions were much the same . The CalMAN eeColor LUTs in the current version are bumpier than they should be and can cause visible problems if you are making large corrections to your native device . LightSpace compared @ @ @ @ @ @ @ @ @ @ given number of profile patches ArgyllCMS produces a slightly better average outcome using it 's default patch set ( optimized irregular patches ) while LightSpace edges out ArgyllCMS using it 's default regular grid patch sets . <p> This is done automatically by DCG if using default presets . It will do a short profile to characterize the display and then feed that to the targen tool which generates the optimized patch measurement set , the only thing you need to specify is the total number of patches to generate . 
@@44332938 @4332938/ <h> DaVinci Resolve Linux Dongle <p> We run dual boot z840s with Resolve Linux on one boot drive , Windows on the other ( and I 've run Supermicro 's with Linux installs ) . Full Resolve Panels . <p> Jake is right that in general , the vast majority of those who are asking for Linux are simply not prepared for the overhead of managing a true Linux workstation - hell , it took me weeks to configure the storenext file system properly ( not really being an IT guy ) Sure , Paragon ( and others ) have NTFS and HFS+ ' plugins ' but the backlash about not being able to plug in common client supplied hard drive would be deafening . That 's not to mention things like Thunderbolt and all the other common things we 're so used to on mainstream OS setups . <p> Reading about this subject in many places , I 'm also positive that many ( not all , but many ) who support a free version of Resolve Linux simply do n't realize that Premiere or other common apps @ @ @ @ @ @ @ @ @ @ to mention OFX and other various things that developers simply do n't see a need to develop for a such a small OS audience . Indeed , I saw on a Facebook thread recently someone asking where they could find a Linux version Adobe Media Encoder . <p> While CENTOS is pretty great - it 's not Windows , it 's not Mac OS . <p> When I got on the Linux bandwagon it was for two reasons - ProRes on powerful , flexible hardware and more GPU support . <p> The first issue is much less of an issue than it was even a few years ago . To be honest , I 'm mainly booting into Windows these days and using a Mac with access to the same shared storage and database server as a ' ProRes dongle ' . Yeah not as fast , NR really kinda kills it , but using a spare Mac for remote ProRes render let 's me move on to the next job or task on my main system . Also , by booting Windows I get all the things I 'm used @ @ @ @ @ @ @ @ @ @ etc <p> As for the other issues I mentioned , in my opinion , popping 8 Titan X in an expander provides little real world performance gain ( see my ad selling a big lot of Titan X 's in the Market thread ) <p> While not scientifically tested , I 've found there really is nothing I ca n't do with 3-4 modern GPUs , two or 3 new Pascal Titan X 's probably suitable for nearly everything . But in the era of GTX 580s and similar , yeah popping 8 in an expander was huge , and InfiniBand linking etc to harness more CPU power was also huge and only Linux could do that . <p> I think we 're technically past many of these things ( at least for most workflows ) or in other words , I think there is a little bit of techno-lust and keeping ' with the Jones ' that 's at play with a Linux push - especially a free version . <p> I really think the Linux push has everything to do with ProRes . NO need to re-hash the @ @ @ @ @ @ @ @ @ @ Jake you 're right about that version ) then most of the Linux noise would go away . <p> BMD has protected themselves from all the noise of Linux support by only offering it only with the big panels - where the idea must be - if you 're going to spend 30k on the big panels you are serious enough have the ability to support all that Linux needs . <p> If I were BMD I would say here is a free version of Linux Resolve - ABSOLUTELY NO SUPPORT OFFERED . Get it out there and let people realize how ill-prepared they are for it . <p> My 2 cents . <p> P.S. I also wish people would stop perpetuating the myth there is one Dongle to rule them all ! Linux dongle and Windows/Mac Dongle . <p> If I were BMD I would say here is a free version of Linux Resolve - ABSOLUTELY NO SUPPORT OFFERED . Get it out there and let people realize how ill-prepared they are for it . 43971 @qwx453971 <p> That would be one way to go . I totally agree @ @ @ @ @ @ @ @ @ @ configure everything and get everything going for a few days . Once that 's done , it 's fairly rock-solid . <p> I have made my case for a couple of BMD people that they should offer " tiered support " : free 9A-5P support to everybody ( but you wait in the virtual line ) ; paid 12-hour weekdays support with faster priority ; and then 24/7 " mission critical " support for pro users who have a deal-killer problem at 2AM on a Sunday morning . But it 's been made clear to me they have no interest in becoming a service contractor to users . <p> They do have at least a couple of people in the LA office that know the Linux systems backwards and forwards , and they can diagnose typical issues on the phone pretty quickly , at least during normal business hours . 
@@44332940 @4332940/ <h> The Coloristos ColorCast - Episode 19 " Displays 2 " <p> In this Episode of the ColorCast , We discuss the current state of professional and consumer displays for grading and finishing . We talk about the rapid evolution of the display market over the past two years , and how technologies like OLED and LCD have finally taken over from long-time standards like Plasma and CRT . <p> We also touch on several different models that are making headway as the new standards in post-production , and how emerging requirements like UHD , 4K , and HDR will likely affect your next major monitor purchase . Finally , we cover the age-old question of whether or not technology has finally opened the door to more affordable color-accurate options , and what the minimum requirements should be for any display used in color critical work . <p> We 're rebuilding and expanding our post-side next year and the ever long question of displays is high on the debating list . At the moment we 're looking at : <p> - EIZO computer-monitors ( where applicable ) . We @ @ @ @ @ @ @ @ @ @ Here we might differ between studios but even their lower cost Flexiscan panels are relatively color accurate ( given 8-bit ) - SONY OLED video-reference monitors . <p> On the client reference side we 're a bit stumped however ... One solution is the FSI 50 " TFT screen which is quite good but way more expensive than a Sony / Philips / Samsung etc TV . On the other hand we love to have UHD monitoring as an option at least given we 've had our first UHD delivery this year . On the reference-side of things this is hard / expensive but looking at for example a high-end Sony ( consumer ) panel is relatively cheap in the grand scheme of things . Our main focus is still HD and SD but at least being able to view UHD-material would be a nice bonus . <p> The Canon is a very nice monitor . It 's of a similar calibre to the Dolby 4220 , in that it has excellent color characteristics , does full 4K , DCI-P3 , as well as HDR . However , it 's @ @ @ @ @ @ @ @ @ @ a modern color suite . It has found a niche in high-end on set monitoring , but I think it 's yet to be seen how successful it will be outside of that . <p> Nice work CPC . One display I am interested in seeing is the Vizio Reference monitor ( not quite out yet ) That is somewhat HDR 800 Nits and I believe is working with DolbyVision and does 80% of 2020 and will output 800 Nits . Should be out in January . <p> Great podcast , it was nice to listen to , while browsing the web , looking for affordable monitor options While doing this research today I found there is a new wide gamut Eizo monitor , the CS240 . Looks like a nice low budget option . Even lower then CX241 ... The thing with the Eizos is that they have the best frame/refresh rate options amongst all the well known computer monitors : <p> I 've worked with all kinds of PC monitors and the Eizos really win in this department . Most others do n't go below 50Hz or even @ @ @ @ @ @ @ @ @ @ folks unacceptable . And they do n't have PWM ... <p> In any case I 've just finished a short doc , that was projected in theatre ( 2k DCP ) . The project was graded on a 4 years old Dell U2410 that was connected via GPU . The screen was meticulously calibrated with an i1D3 to match 709 . The projection in the theatre looked awesome and more or less exactly the same as on the Dell . <p> I know this is a joke , but the results were good . The monitor reproduced a good gamma response , good gamut calibration , nice blacks ( I really love the ccfls of U2410 for having not having an excessive amount of blue in blacks ) , good separation , etc . I can post results if interested . What is not good off course is screen uniformity , black level and the fact that it 's only capable of 60Hz . <p> Anyway , just wanted to share this , as it 's interesting where we have come in terms of price and getting ACCEPTABLE results <p> @ @ @ @ @ @ @ @ @ @ had a lot of good points , especially about the investment of a monitor in the low-end/mid-range , when you should maybe ( and I 'm paraphrasing here ) - maybe save the money if you 're starting out/not getting paid , instead of taking some of your budget and purchasing a monitor that you may end up replacing down the line . ( having said that , of course it can be resold , or re-purposed for something else ) <p> Anyway , I had no idea that monitors such as the HP Z24x were so cheap ! ( I mean , for what they are ) <p> I would venture to say that , yes , for big money clients with deep pockets , these would not be the best investment . But for lower-end productions , these would be more than sufficient , especially this one with 99% adobe-rgb , for example , among other features .... agree ? 
@@44332941 @4332941/ <h> Alexa RAW vs . Alexa ProRes <p> Just a note we did a 2 day shoot in Jan 4 x red dragons only 3 rolling most of the day D cam was for grabbing quick shots we shot 5:1 6K @ 25 after 2 day of filming we had just over 6TB of data we end up using 3x OWC thunder bay units with 4tb drives set up for raid 5 so that about $3000 ( one plus 2 backups ) this is not including the proxy drives and audio so i think you have gone Cheap on that 14k Marc 43971 @qwx453971 <p> True , this would be using rock-bottom bare drives , not great drives . It could easily double if you used full-price premium drives , say G-Tech or the equivalent . <p> My point is that $10K-$15K for drives during the production of a 30-day feature is not a significant expense . Look at it this way : in the old days , $10K would be roughly the price for an hour of film plus processing and dailies . <p> The last pilot I @ @ @ @ @ @ @ @ @ @ on Alexa Log-C Pro Res ) because it was A&amp;B Camera , plus they sometimes had a C camera shooting second unit or stock footage . The overload of excessive footage is becoming a bigger problem every year , not getting better , particularly with features 43971 @qwx453971 <p> Holy smokes Marc . 4TB/day Arri ProRes ? Even with two cameras that 's excessive . I feel sorry for the editor . Did they over-cover every scene , do excessive amounts of takes , keep the camera rolling between takes , or all of the above ? <p> Directors are becoming much less disciplined than they used to and somehow there is no more weight to " the more you roll the more you spend " anymore . It 's less tangible . You ca n't hear the film(dollars) flowing through the camera anymore . <p> Holy smokes Marc . 4TB/day Arri ProRes ? Even with two cameras that 's excessive . I feel sorry for the editor . Did they over-cover every scene , do excessive amounts of takes , keep the camera rolling between takes , or all @ @ @ @ @ @ @ @ @ @ , yes , and yes . If they do 200 2-minute takes a day , plus pickups inbetween the takes , plus you have A&amp;B camera , plus there 's second unit stuff and the occasional third camera , and the director " prints " pretty much 50% of everything ... it adds up . 500 minutes for one camera would be 1.34TB of ProRes 444 , times 2 , plus audio , plus all the extra stuff . Certainly over 3TB , maybe 3.5TB ( which would take 4TB of drives with overhead ) . <p> If they shot in Arriraw 3K , it 'd be 6.38TB per camera . If this were ( say ) a 40-day indie feature , that would be very possible to get to that point . Even Red Epic 5:1 5K would be 2TB per camera , and I 've certainly seen that happen . I can think of one very high-profile ensemble comedy ( one that made over $100M ) where I think we wound up with 700 hours of footage , because the director had done a lot of 20-minute takes @ @ @ @ @ @ @ @ @ @ a lot of that was three cameras , with about 6-7 hours of material per camera per day . Plus I think 12 channels of audio . <p> A : I use multi-camera for stunts ; for all the dramatic action , I use single-camera . Shooting single-camera means I 've already seen every frame as it 's gone through the gate because my attention is n't divided to multi-cameras . So I see it all and I watch dailies every night . If you 're always shooting multi-camera , you shoot an enormous amount of footage , and then you have to go in and start from scratch , which is tricky time-wise . <p> Yep , I wish more directors did do that . Although ... I accept that A&amp;B camera is necessary for TV , because it 's the only way they can get through the scene and have enough coverage . But the only approach that works well for sound and the cinematographer is to do two cameras from the same angle and slightly different lenses , like a 2-shot and a close-up from the left @ @ @ @ @ @ @ @ @ @ helps nobody . <p> True , this would be using rock-bottom bare drives , not great drives . It could easily double if you used full-price premium drives , say G-Tech or the equivalent . 43971 @qwx453971 <p> G-Tech do n't make drives , they make expensive fancy housings , and bolt the same drives in them , that I would buy of the shelf . " Premium " drives are pretty much a marketing thing IMHO - in the last 20+ years I do this , I have not seen much difference . <p> For the record : The last feature I shot ( in raw - BM 2.5k and 4k ) was around 12TB and that was with up to 3 cameras . But yeah , some directors get crazy with multiple takes . <p> G-Tech do n't make drives , they make expensive fancy housings , and bolt the same drives in them , that I would buy of the shelf . 43971 @qwx453971 <p> Actually , Hitachi bought G-Tech about three years ago , so technically , Western Digital , Hitachi , and G-Tech are all @ @ @ @ @ @ @ @ @ @ company owns the drive manufacturing plant and the enclosure company . <p> " Premium " drives are pretty much a marketing thing IMHO - in the last 20+ years I do this , I have not seen much difference . 43971 @qwx453971 <p> I would argue that enterprise-class drives do make a difference for very heavy-duty users . I would trust Mr. Myres with whatever he recommended in this area , since he knows it extremely well . <p> For the record : The last feature I shot ( in raw - BM 2.5k and 4k ) was around 12TB and that was with up to 3 cameras . But yeah , some directors get crazy with multiple takes . 43971 @qwx453971 <p> I 'm speaking specifically of major Hollywood union studio productions . Indie films are a lot more reasonable . I 've done six or seven of those in the last 9 months , and none of those have gone past 12TB for all the footage , spread out over a 20-day shoot , all in 2K . One went a little higher but that was Red 5K @ @ @ @ @ @ @ @ @ @ I think the records I saw on studio features was about 500 hours ' worth of film on Dreamgirls , 400 hours ' of footage on Shine a Light , and over 300 hours of footage on Blades of Glory . My memory is that Grownups 2 went over 600 hours . One of the local Kodak reps told me they used to frequently deliver a case of champagne to the camera crew whenever they went over 1,000,000 feet ( roughly 250 hours , depending on format ) ; he told me that Charlie 's Angels 2 went over 2,000,000 feet of film , mainly because of slow motion . <p> This was an interesting vid from light iron to watch on data sizes and dealing with it , incase any one has not seen it about if you jump to 8.30 ish you get the total data foot print etc for the features for my last shoot i used the raid 5 shoe box thing ( but thunderbolt 2 not SAS as i mentioned previously ) idea and it worked really well <p> However unlike the lovely world of @ @ @ @ @ @ @ @ @ @ up with case of Champagne when they blew past 50TB worth of drives <p> Actually , Hitachi bought G-Tech about three years ago , so technically , Western Digital , Hitachi , and G-Tech are all one happy family . Right now , the same parent company owns the drive manufacturing plant and the enclosure company . 43971 @qwx453971 <p> And since I buy mostly Hitachi drives - that 's exactly what I 'm saying , they use the same drives - nothing rock bottom here . <p> And since I buy mostly Hitachi drives - that 's exactly what I 'm saying , they use the same drives - nothing rock bottom here . 43971 @qwx453971 <p> Eh , the enclosures are nice and it 's possible to find the G-Techs at tremendous discounts . I know a few LA post houses that use them by the hundreds , at least for drives that are passed around to various sessions and clients . They 're also extremely common for LA D.I.T. 's on quite a few shows I 've worked on . <p> I would argue that enterprise-class drives @ @ @ @ @ @ @ @ @ @ would trust Mr. Myres with whatever he recommended in this area , since he knows it extremely well . 43971 @qwx453971 <p> The general rule of thumb is that enterprise drives are designed for a 24hr duty cycle and come with a 5 year warranty , whereas desktop drives are designed for an 8 hour duty cycle and come with a 3 year ( or shorter ) warranty . You can use either one , but just like light bulbs they have an expected number of hours in them and then they fail . <p> Another popular notion is that enterprise drives must be used in RAID arrays as they need to have a certain amount of vibration resistance to avoid errors . I think this is in there with the half-life of a plasma . It was probably a thing a number of years ago , but given how technology moves forward , is probably a non-issue in most cases now . If you 're building a storage system that will be on continuously , enterprise drives might be a good idea , but even then , I have @ @ @ @ @ @ @ @ @ @ a long term penalty for it . <p> There 's a huge difference between enterprise and desktop drives . Operating as a single drive the differences are minor . When put in a raid , error recovery time limits come into play . Your raid will regularly need to be rebuilt due to timeout errors if desktop drives are used . End to end ECC are also factors , but vary from mfg . <p> Depending on the camera firmware and the grading application debayer you may see different versions of the Arri ADA debayer algorithm , so comparing ProRes from camera with Raw from camera vs ProRes from software debayer with Raw may have different results ! The difference in ADA debayer can have very noticeable results . <p> From the Arri website ; <p> The different ARRI De-Bayer Algorithms ( ADA ) control how the color channels are reconstructed . <p> ADA-5 HW is the lastest de-bayer algorithm used in the ALEXA cameras starting with SUP 11.0 . It offers improvements for bluescreen vfx shoots and reduces noise as well as aliasing . <p> ADA-5 SW is an @ @ @ @ @ @ @ @ @ @ HW is the camera hardware de-bayer algorithm used in ALEXA cameras in SUP 7.0 up to SUP 10.0 . <p> ADA-3 SW is an enhanced software de-bayering algorithm based on ADA-3 HW . Depending on the scene content , it can deliver more image detail than ADA-3 HW . <p> ADA-2 SW is a legacy software de-bayer algorithm . <p> ADA-1 HW is a legacy camera hardware de-bayer algorithm , that was used before SUP 7.0 . <p> The processing version controls which color transformations are applied to the de-bayered data . Version 4.0 represents the latest status of in-camera color processing . Previous versions are available to allow matching an output with previously processed footage . ADA-3 HW/SW is only available in combination with processing version 4.0 . <p> There 's a huge difference between enterprise and desktop drives . Operating as a single drive the differences are minor . When put in a raid , error recovery time limits come into play . Your raid will regularly need to be rebuilt due to timeout errors if desktop drives are used . End to end ECC are also factors @ @ @ @ @ @ @ @ @ @ a myth - I build my RAIDs with desktop drives since almost 20 years now , and never had to rebuild a RAID once . Usually I take them apart after 3-4 years and use the single drives as archival drives - and no , that also never gave me any trouble from sitting on a shelf . <p> Usually I take them apart after 3-4 years and use the single drives as archival drives - and no , that also never gave me any trouble from sitting on a shelf . 43971 @qwx453971 <p> Same deal here . I have triple-backups on some stuff made with old RAID drives , and they hold up fine . In truth , I can count on the fingers of one hand the number of times I 've had to go to the second set of backups for anything . <p> I have been working with digital/electronic storage for a very very long time since 85 ' infact .... ferrite core store any one in the days when it took two of you to lift the Ram <p> and there is one immutable @ @ @ @ @ @ @ @ @ @ not being DR Doom there are few things that can prolong the Inevitable and this may be helpful to somebody i know quite alot the young guys read this forum so i will post it <p> ESD one of the sure things to bring drives to an early grave , people do n't think about how they handle bare drives , they treat them the same a standard USB drives that are already in a case , the human body can hold a massive a electric charge and you won , t see it but a bit of " micro lighting " striking the PCB and who knows what you have done <p> I know people who have pulled a fully functioning drives out of a machine put it back into another machine and it did , nt work that could be one of two things .............. ESD or Evil Data Pixies that 's EDP <p> Raids . never buy drives in batches from the same place at the same time , odd are there from the same batch from mfg , so there build date will be pretty close and @ @ @ @ @ @ @ @ @ @ ( this should be obvious ) but when those Deals at hit newegg the credit card come out <p> Can any one remember the early IBM Deskstar AKA the DEATHSTARS so dubbed by data recovery companies , these things used to fail all the time not all drives are built equal so always research them before you buy them may not be the cheapest <p> By the way I have a very healthily pile of dead 2TB Green WD , s if any one is interested from my media raid at home <p> it all boils what your doing with them if you going to use them continuously , me i would put in enterprise in a daily working RAID ( saying that they fail as well , one went on our edit share last year ) <p> if your just archiving then the desktop should be ok ... but your call ... as your only going to spin them for while then put them on the shelf ( protected in " ELECTROSTATIC BAG " ... sorry my voice went up a bit ) <p> work wise we push every @ @ @ @ @ @ @ @ @ @ into the cloud in to S3 then to Glacier as we have zillions of data <p> for personal stuff it 's different but i supposed this would work for the guys with smaller shops with out the infrastructure <p> Cloud storage is becoming super cheap and if you have a google apps account and pay for 4 seat its about -20 a month you get unlimited google drive I film all the time and i have Go pro 4k , a BMPC 4k And LX100 as my personal kit and i generate huge amount of stuff ( that not including my stills cameras ) <p> so i use a local single drive for a local backup ( which goes on the shelf in " the bag " ) and program call ARQ ( google it ) on my mac to back every thing up into google drive in the cloud i have about 18TB of stuff up there now i don , t 100% trust it yet but i , m getting there <p> the big weak point with all the digital film technology is the storage starting . on @ @ @ @ @ @ @ @ @ @ we archive because we make so much data and it going to get worse ... 8K give me a break <p> and i don , t think any one has really nailed it but for me it " Hope for the Best , Plan for the Worst " <p> Can any one remember the early IBM Deskstar AKA the DEATHSTARS so dubbed by data recovery companies , these things used to fail all the time not all drives are built equal so always research them before you buy them may not be the cheapest ... 43971 @qwx453971 <p> Doh , the worst data loss I had in history happened with a Deathstar drive in a Pro Tools mix session around 2000 . I realized it was failing , made a backup ... and the backup failed and the original drive had a head crash . The good news is that I had a week-old backup ; the bad news is that it represented a week of work . I was able to recreate the work in half the time , but it was still a foul situation . From then @ @ @ @ @ @ @ @ @ @ in case disaster strikes . <p> That 's a myth - I build my RAIDs with desktop drives since almost 20 years now , and never had to rebuild a RAID once . Usually I take them apart after 3-4 years and use the single drives as archival drives - and no , that also never gave me any trouble from sitting on a shelf . 43971 @qwx453971 <p> I 've been in post environments where the RAIDs are running 24/7/365 , and drive failures are common . It 's worse when there are power interruptions . <p> I 've seen projects delivered on both individual drives and RAIDs . Lately I 'm seeing more RAIDs , probably because of speed and it 's easier to keep track of . 
@@44332942 @4332942/ <h> ICE QC/review Player for DCP , IMF , UHDTV and any digital raw format up to 4K <p> Direct playback in real-time of any type of file ( RAW data , Open EXR , TIFF , H265 , etc. ) or packages ( DCP , IMF , MXF , etc. ) from SD to 4K resolutions . Monitoring of the content on a reference monitor or a DCI projector , up to 4K Creation and management of Playlists <p> PLAY &amp; CONTROL YOUR DIGITAL CONTENT , INCLUDING IMF AND DCP <p> Validate DCP or IMF packages using the Assets Inspector and the Validation tools Generate custom comments PDF report of the validation process <p> Blue ICE especially designed to support any type of media file , including 4K DCP and IMF perfect tool to validate the majority of post-production , archive and broadcaster 's content With a playout capability of 4K 4:2:2 is allows H265 reviews for UltraHD without a glitch <p> Black ICE powerful architecture permits the real-time playback of 4K uncompressed files a must for monitoring during the post-production phase , or to display them during @ @ @ @ @ @ @ @ @ @ allows the play-back of Stereo3D 4K DCPs. 
@@44332943 @4332943/ <h> Red Epic Anamorphic in 4k , 3k , and 2k <p> Marc . Give it up , seriously . You ca n't create proper anamorphic bokeh or unique shallow depth of field characteristic of anamorphic lenses in post , period . People do n't want close enough anamorphic look for the same reason some are still preferring to shoot film over digital acquisition . I just ca n't understand your insistence on telling people , that their CREATIVE choice is invalid , because they can get sharper picture with spherical lenses . They are shooting anamorphic precisely because they WANT less sharp picture . Not everything is about resolution and sharpness . Some are looking for something different and it 's their creative choice . And did I mention , that this is why there is so many new anamorphic lenses available , that were n't available before ? <p> I never said you could . But I believe you can get close enough to the point that this is the way 90% of all major A-list films are being shot today . <p> I 'll say for @ @ @ @ @ @ @ @ @ @ my clients want to shoot with anamorphic lenses . That 's their creative choice . But if I 'm asked my technical opinion , I 'll tell them that I think shooting spherical and masking off in post is a much more practical solution because of the issue of reframing , giving them at least another available stop or two , giving them a lens that 's lighter in weight , and less expensive to rent . I concede that there are different looks with a wide variety of lenses -- particularly anamorphic flares -- that you can only get with anamorphic . ( Though bear in mind there were hundreds of VFX-created anamorphic flares inserted into J.J. Abrams Star Trek film , along with many real ones . ) <p> I was reminded by a colleague of mine today at lunch that 10-15 years ago , there were no real alternatives to anamorphic because all the other methods ( like shooting Super 35mm , used by Jim Cameron ) required an additional optical printer step , further degrading quality . Cameron did it solely because he did n't want @ @ @ @ @ @ @ @ @ @ flaw with his reasoning is that most of the optical effects in his Super 35mm pictures were done in 2.40 , and the VFX crew did n't always complete the effects all the way out to the full aperture . <p> Nowadays , I think all the VFX people and post supervisors understand this , and they will do more work outside the frame ( at least to 1.78 ) , even for a project released in 2.40 . The reasoning is that a 1.78 full-frame pan/scan transfer is almost always part of the final delivery package for features , in addition to the 2.40 letterbox . And nobody likes pan/scan . If you have to reframe but keep the headroom consistent with the theatrical feature , then I think opening up the bottom 1/3 of the frame is a wiser choice than just pan/scanning the 2.40 . The repositioning is infinitely more flexible this way ... but it 's all a question of which compromise works best . <p> I absolutely agree : it 's a creative choice on the part of the filmmakers , and I think I @ @ @ @ @ @ @ @ @ @ . <p> Let 's agree to disagree as to whether technically , anamorphic lenses are inferior to spherical . Have you ever seen a side-by-side test of them ? I did , back in 1999 , and that 's when I came to a lot of the conclusions I talk about here . <p> Marc . You 're being weird and I ca n't understand why . You agree , that this is a creative choice and then immediately start to talk about technical aspects of anamorphic vs spherical . For the last time , this is a CREATIVE choice . Stop it ! <p> Alexa RAW 4x3 sensor is 2581x2160 , resulting in anamorphic " virtual " 2.39 size sensor equivalent to 5162x2160 . Arri RAW sensor size for flat 2.39 is 2880x1205 . This is not even close to anamorphic 4x3 sensor size , even if it is a virtual size . And so , in this case using just one metric- lens resolution is not telling the whole story . With this much increase in the sensor size you 'd be better of using MTF as it @ @ @ @ @ @ @ @ @ @ MTF accounts not just for lens resolution , it also utilizes an appreciably bigger sensor size . This results in a comparable MTF with anamorphic lenses vs flat spherical image . And that this is why there is so many new anamorphic lenses available , that were n't available in 1999 <p> The last side-by-side anamorphic vs. spherical test I did was for School for Scoundrels , 2006 , for DP Jonathan Brown at Technicolor/Hollywood . We had also done several at Cinesite in projection in 2003-2004 . <p> I really love the choices Arri Alexa gives DPs , but I think the 4x3 version is pretty rare at this point . Just as if I were asked about anamorphic vs. spherical prior to start of production , I 'd recommend spherical , and if I were to be asked about Red vs . Alexa , I 'd recommend Alexa . But in both cases , if they want to go the other way and shoot anamorphic and Red , I 'm not gon na run screaming out of the room . You can make great pictures that way , @ @ @ @ @ @ @ @ @ @ about the exposure than I am the lenses , but all of it has an effect on the final image . <p> Choices are good , but the key to me is to make sure the post super , the editor , the VFX people , the DIT , and the people doing final color are all on board with the workflow . And that includes aspect ratio . Heck , I still ca n't get people to shoot sizing charts , so we 'll run into odd framing issues sometimes where you go , " hey ! What happened here ? " That is one major problem with shooting spherical : people who want common center vs. common top . I have worked on Super 35mm projects where different crews shot with different framing , and it was a nightmare to straighten it all out in post . So that was a case where the choices were there , but they made the wrong ones . 
@@44332944 @4332944/ <p> What expansion chassis ? That had been depreciated a few years ago . On the other hand , what " 8 x 1080Ti GPUs " Now you 're wrong the other direction- such system ca n't exist yet , as those GPUs are not out yet . Who needs 8 GPUs anyway . You 're the one who keeps spreading the fallacies . Increasing number of GPUs is a dead end and at some point after about 4 GPUs you 're reaching the point of diminishing returns . <p> " There are hardware configurations detailed below starting from a single dedicated GPU for image processing up to a eight GPU system suitable for demanding 4K and stereoscopic grading sessions . Following the release of DaVinci Resolve 12 a much broader range of GPUs is supported . While performance varies greatly , most modern AMD , Intel and NVIDIA GPUs that support OpenCL 1.2 or CUDA 2.0 compute capability will operate with DaVinci Resolve . " <p> I think a system like this would be tough to put together in the past , but if a PCI-E expansion chassis @ @ @ @ @ @ @ @ @ @ supported lots of cards ) , and eight GPUs were under $5K , it starts to be not-so-stratospherically priced . I saw the invoice for the Baselight 4 we had at Lowry , and it was $240,000 just with 4 GPUs and we frequently struggled . No question , things are faster now and will continue getting faster . <p> If you were spending $15K on a Linux box and $30K on the panels , I think another $10K on GPUs and an expansion chassis would not be unreasonable . Even including 100TB of fast storage and a 4K broadcast monitor , I think it could be under $100K ( minus the room itself ) . <p> Would it be warranted for , say , commercials or light-duty projects ? Probably not . If I was future-proofing for handling 8K and HFR and 3D features , then I 'd sure think about it if I were a facility owner . For indie people ? Naaa , it 's blue-sky stuff . I 'd be very happy with just two 1080Ti 's for what I do . <p> When I finally @ @ @ @ @ @ @ @ @ @ have you over and hand you a match while I spray lighter fluid on the piece of crap . I 'll be totally jazzed to get rid of it . <p> " There are hardware configurations detailed below starting from a single dedicated GPU for image processing up to a eight GPU system suitable for demanding 4K and stereoscopic grading sessions . Following the release of DaVinci Resolve 12 a much broader range of GPUs is supported . While performance varies greatly , most modern AMD , Intel and NVIDIA GPUs that support OpenCL 1.2 or CUDA 2.0 compute capability will operate with DaVinci Resolve . " <p> I think a system like this would be tough to put together in the past , but if a PCI-E expansion chassis was maybe $5K ( or you had a chassis that supported lots of cards ) , and eight GPUs were under $5K , it starts to be not-so-stratospherically priced . I saw the invoice for the Baselight 4 we had at Lowry , and it was $240,000 just with 4 GPUs and we frequently struggled . No question , things @ @ @ @ @ @ @ @ @ @ If you were spending $15K on a Linux box and $30K on the panels , I think another $10K on GPUs and an expansion chassis would not be unreasonable . Even including 100TB of fast storage and a 4K broadcast monitor , I think it could be under $100K ( minus the room itself ) . <p> Would it be warranted for , say , commercials or light-duty projects ? Probably not . If I was future-proofing for handling 8K and HFR and 3D features , then I 'd sure think about it if I were a facility owner . For indie people ? Naaa , it 's blue-sky stuff . I 'd be very happy with just two 1080Ti 's for what I do . <p> When I finally dump the Trashcan Mac , I 'll be glad to have you over and hand you a match while I spray lighter fluid on the piece of crap . I 'll be totally jazzed to get rid of it . 43971 @qwx453971 <p> The Infiniband expansion chassis had been depreciated already . It 's still available , but there is @ @ @ @ @ @ @ @ @ @ any longer . And no , the 1080ti GPUs are not out for another week . There is no such thing as " 8 x 1080Ti GPUs " It just does n't exist . Let it rest Marc and please stop giving advice about something you have zero firsthand knowledge ... <p> Well , both these new panels are an exciting option for on set use as a DIT . <p> Unfortunately I also run other apps that benefit from colour panels like Livegrade , and ( less often these days ) Scratch . <p> I tried a brief look but see no SDK or anything . Anyone in the know aware of any support , current or planned for use in third party apps ? I ca n't really justify the cost or space on my cart for having more than one panel ! <p> They would have to release these the day after I order a backlit Logic Keys keyboard for Resolve ! - Actually that makes me wonder are the buttons backlit at all on these new panels ? I often have to work in the dark @ @ @ @ @ @ @ @ @ @ and buttons until I develop the muscle memory ... <p> Two questions for BMD : is the brightness dimmable for the backlit and mostly for the screesn ( I do n't like the idea to have two screens at the bottom of a reference monitor if they are too brights ) . And a very interested question : will BMD continue to support third part panels in the futur ? I bought my Tangent Element 2 weeks ago ... it would be too sad and expensive to have to change it now ... <p> Funny that you would say that , it seems really odd to me that the buttons that are going to be used the most are the ones that are the futherest from the operators body , at the strangest angle and that will require the most amount of stretching to access all day long . <p> I also really wonder why those buttons are n't split on either side of the trackballs instead and the trackballs themselves are n't centered in front of the operator and screens , especially since there are only 3 trackballs and @ @ @ @ @ @ @ @ @ @ price but is actually a fair amount of money for panels that work with just one application and are not remap-able . <p> My problem with the full elements is that they just take up so much freaking horizontal space on my desk . Mini seems like it would really put things back into nicer proportions . 43971 @qwx453971 <p> Yes and no : I like the fact that all the buttons lay horizontally and side by side on the Element . I 'll be happy to test the upper part of the Mini ASAP cause I 'm a little afraid of the size of the buttons ( seems small compared to the advanced panel ) , that you will have to push after crossing the trackballs/knobs area ... 
@@44332945 @4332945/ <p> The other issue is that it 's not HDR-capable . There had been rumors before it was announced that it would hit 800nits , but in reality it only measures about 450nits with a 10% ( square area ) white patch . Not the end of the world , but 800nits seems like a developing minimum standard for consumer HDR , and given the cost , would have been nice to have . Like the shadow uniformity issue , this will hopefully also be solved in the near future . 43971 @qwx453971 <p> that 's what though ... i think that panasonic , sony , samsung and other will claim HDR on current models that are actually not capable of brightness of 800 + ... i think they are claiming to be able to do HDR but t 's actually a mix of high brightness ( like 400ish ) + some dynamic contrast , or local dimming etc ... I have the feeling that the VIZIO reference series is the only real panel out there capable of 800 nits ( for the consumer market ) .. <p> I @ @ @ @ @ @ @ @ @ @ come up with some kind of certification program so that nobody can put " HDR " as a sticker on their monitor and claim it can do HDR when it really can not . No doubt THX will have something like this , but there are n't a lot of THX sets out there these days . <p> On the other hand , there 's an awful lot of consumer sets that are using the terms 4K and UHD interchangeably , which I know drives Jason Myres crazy ... I 've started to call 3840x2160 " fake 4K , " but it 's the closest any consumer set will get to 4K. 
@@44332946 @4332946/ <h> Colorist Podcast 013 : Vanessa Taylor <p> Vanessa Taylor , freelance colorist for the films The Great Gatsby and Lady Macbeth , joins me on this episode of the Colorist Podcast . <p> Vanesa has over 20 years experience in the industry . She got her start working at Animal Logic , working on the Quantel Henry as an Online Editor . After that , she moved on to the DI department at Park Road Post . <p> She 's worked on projects from Joss Whedon , Baz Luhrmann and Peter Jackson . She 's based out of the UK , but travels worldwide to work on a variety of different independent films . <p> On this podcast , we talk about : <p> Her experience with Quantel Henry , Pablo , eQ and iQ served as a basis for her career <p> How she moved on to Park Road Post <p> Knowing when to do a visual effects shot in a color grading application <p> Working on a big budget feature film and where your contribution can fit it <p> Learning several color systems <p> Balancing the @ @ @ @ @ @ @ @ @ @ it means creativity for colorists <p> Working in other countries and their perceptions of color <p> Running a session and how to keep it flowing <p> The lowering cost of color grading systems and what it means for working colorists today <p> The formation of the colorist society and how we benefit from joining together 
@@44332948 @4332948/ <h> Eizo CG318 HDR upgrade <p> I find it bizarre , the paid upgrade is just an internal LUT modification performed onsite by Eizo engineers , the PQ HDR mode on CG318 will be using the display 's 300nit peak luminance which means it clips super early . With a nominal contrast ratio of 1500:1 , the black level will be around 0.2nit which is quite high for HDR . <p> From the Google translator : " We also exhibited HDR compatible monitors under development for reference use of HDR contents . In addition to faithful color reproduction , high brightness of 1000 cd / m 2 is realized , and the finish of HDR image can be reproduced realistically . " <p> And the video demonstration : <p> Do you really think it 's " fake HDR " ? By the way , is it possible to feed REC2020 from HDMI input ? It seems it only exists with HDMI 2.0 . And the CG318-4K would be limited to HDMI 1.4 . <p> From the Google translator : " We also exhibited HDR compatible monitors under development for @ @ @ @ @ @ @ @ @ @ color reproduction , high brightness of 1000 cd / m 2 is realized , and the finish of HDR image can be reproduced realistically . " <p> And the video demonstration : <p> Do you really think it 's " fake HDR " ? By the way , is it possible to feed REC2020 from HDMI input ? It seems it only exists with HDMI 2.0 . And the CG318-4K would be limited to HDMI 1.4. 43971 @qwx453971 <p> The difference between hdmi 1.4 and hdmi 2.0 has to do with the ability to pass metadata thru the signal and the support of higher framerates . Which colorspace you use has nothing to do with it . If Eizo is coming up with a 4K HDR monitor that can hit 1000 nits I would n't call it ' fake ' . But a peak white of 300 nits does not meet the specs of hdr . <p> The difference between hdmi 1.4 and hdmi 2.0 has to do with the ability to pass metadata thru the signal and the support of higher framerates . Which colorspace you use has nothing @ @ @ @ @ @ @ @ @ @ with a 4K HDR monitor that can hit 1000 nits I would n't call it ' fake ' . But a peak white of 300 nits does not meet the specs of hdr . <p> According HDR with the CG318 : I think that Eizo will bring out a paid upgrade ( it will be a LUT in the end ) with new color space transforms for HDR : You can decide if the signal is either clipped at the 300 nits max or remapped to 300 nits max without clipping . Then it will be possible to view HDR signals , but not in an appropriate way to do serious HDR grading . At least this is what I understood ... <p> The picture where you see the CG318 side-by-side with a HDR monitor does show that clearly . I think Eizo has some HDR monitors under development . That 's what they mean . 
@@44332949 @4332949/ <h> Grading BMCC vs . Canon C100 <p> I would say 60% of the work I do is camera work , with an abundance of run n ' gun shoots . I am thinking of trading in my Blackmagic Cinema Camera for a Canon C100 . To me the BMCC gives some great flexible footage to get creative in post but does anyone have experience with the C100 's files ? I know they are 8 bit/4:2:0/12 stops of DR vs. 10 bit/4:2:2/RAW and 13 stops , but is there an appreciable difference between the two ? Thanks <p> Thanks , Marc . I see what you mean about the C100 -- some of the footage I 've seen the image looks a little " thin " . The C300 is a bit out of my budget right now . The one thing that shines about he C100/300 is the low light capabilities compared to the Blackmagic offerings , but I guess you have to pay a price to see in the dark <p> The BMD Cinema Camera -- to me -- is noisy and not too sensitive . @ @ @ @ @ @ @ @ @ @ Ursa Mini . I do n't like the ergonomics of the Canon C100/300/500 ; it 's a weirdly-shaped camera and not great for shoulder mounting , and the controls are in weird places and the menus are non-intuitive . But I 've color-corrected some shorts shot on the C300 and one recently on the C500 , and I was surprised that the images were as good as they were . I 'd like to see a shootout between the C300 4K and the Ursa Mini 4K and see how they come out . No question , the C100 and C300 are much better for filmmakers than a DSLR . <p> The good news is that I 'm glad there 's lots of choices out there , and some really good , solid cameras that make very nice pictures . I 'm about to start working on a small indie horror feature shot entirely on the BMD Ursa ( not the Mini ) , and we 'll see how that looks . <p> The cinema camera is a fine camera for it 's price , but really not suitable for run'n @ @ @ @ @ @ @ @ @ @ but its internal 8bit codec up to the task nowadays . It 's okay if you dial in your look before recording , but not when you do extensive grading . <p> I have the Sony FS7 for run'n gun work . Its internal 10bit 4k/HD footage quality is comparable to ProRes . It 's small , has ND filters ... I rebuilt the camera , sothat the handgrip is now directly attached to the camera and put 2x 128GB cards for over 5h of HD footage into it . <p> I had a documentary in grading last year shot on FS7 . Hated it image wise . Took out so much of niceness from the locations it was shot . I would have shot it with my 2,5 k with some added mods but sure it would change pricing on it . Shoulder rig of course . Quality wise it would be miles ahead imo . Audio was shot separately anyway so only would they have to shoot images . But this is just me maybe . I 'm no dop but i have shot some run and gun @ @ @ @ @ @ @ @ @ @ is even tools for auto focus on manual lenses but yes it adds another grand to the pricing . Put on some vari ND and you are golden . <p> After all it is the question if you are after images or that it would be easy on set . Can not have both without AC i guess . <p> I did work with the BMCC on a huge project , too . Very easy to pull out nice colours in post . <p> It 's more difficult with all the Sonys , but I think you can dial in the look you want , too . There 's a lot DR and shadow Information if you shot s-gamut3.cine and s-log2/3 gamma . Skin tones are more tricky than with real RAW footage from Arri/RED/BM , but doable anyway . But the lack of any AA/lowpass filter on the BM cams + there sensibility of IR pollution is often a problem , too . Normal NDs and VariNDs definitely not recommended . Then there 's sensor size and the lack of wider focal length ... <p> A guy I work @ @ @ @ @ @ @ @ @ @ playing around with it for the last few weeks . If you go with the C100 , invest in an external recorder like a Ninja Blade . The internal codec on the C100 is garbage . Recording externally to ProRes helps the footage hold up a lot better in post because it bypasses the awful , internal compression to AVCHD . And I think the HDMI outputs 4:2:2 , versus the internally recorded 4:2:0 . But I 'm not 100% sure on that . <p> The only reason why I was considering the c100 is its " grab n ' go " form factor and low light capabilities which appeals to me a lot . IMO the BMCC camera far exceeds it in the " cinematic " image department ... Which brings up the changing aesthetics of documentary . There seems to be two camps evolving : the raw , 60fps PBS/Sony immediate look and the 24fps/high DR/Blackmagic " film look " . The c100 seems to have a foot in both ... BTW the ursa mini 4K/4.6 looks very promising but waiting for them to resolve the magenta/fpn issues @ @ @ @ @ @ @ @ @ @ you 'll get more " cinematic " footage with the BMCC per se . <p> The DR of the Sony is about the same as the BMCC , maybe 1/2 stop less that you 'll win back easily in low light situations . <p> I loved the BMCC back in 2012 because it 's RAW and cheap . And you can pull out a very cinematic image . But a good colorist will do the same with Sony footage . Just have a look at movies shot with F55 , which is absolutely comparable in terms of DR and color science . The F55 has a bigger gamut and better codec/RAW capabilities , but the look that can be achieved is close to F5/FS7/FS700 because of the same sensor and cineEI mode . <p> And in truth , I think the elusive " cinematic look " is really 90% about the lighting . I do n't think the rest of it is as important , though I agree if the chip does n't have enough dynamic range , that 's a deal killer . <p> I did an F55 project @ @ @ @ @ @ @ @ @ @ it held up . I have n't done an F5 job in a long time , but I seem to recall it being OK . <p> If you get the c100 , consider a prores recorder . I recently saw some footage shot on an fs700 and an atomos ? recorder , and it looks really surprisingly good when the light is right . Feels very organic . <p> If you get the c100 , consider a prores recorder . I recently saw some footage shot on an fs700 and an atomos ? recorder , and it looks really surprisingly good when the light is right . Feels very organic . 43971 @qwx453971 <p> Yeah , I was thinking of doing that . My partner just picked up an Atomos so maybe I 'll give it a try ... 
@@44332950 @4332950/ <h> 1.85 punch-in 3.2k to 1080 dailies ? <p> I 'm prepping a feature and discussion with the DP has set some parameters . We 'll be shooting with the Amira at 3.2k ProRes444 ( 3200x1800 ) and he wants to do a custom 1.85 framing for an eventual 2k master , with enough room for re-positioning and stabilization , as there will be a lot of hand-held camera work . <p> We can use web tools to create custom frame lines for the Amira , I 'm thinking an 80% scale 1.85 would be the furthest to go while still protecting for all the above , which would result in a center-punch frame size of roughly 2560x1384 . The other proposal I would present would be a 90% scale , but I do n't know if that will be sufficient for repositioning or stabilization , though considering the film will include vehicle interiors and practical locations - no sets built with flying walls - there is also the concern of the crop factor on the given lenses , and 80% will make them appear considerably tighter than full @ @ @ @ @ @ @ @ @ @ master , but I also have to take whatever framing we decide and create the dailies with that crop factor for editorial @ 1080 res . I 'm still undecided as to whether I 'll use my usual , Resolve , or use this opportunity to use Scratch , as we 're still in talks for web distribution of dailies , and Scratch Web has a nice feature set already in place . <p> Regardless , in Resolve I 'm not sure how to best achieve this - with a full-width output of the 1.85 aspect material from the 3.2k master footage , which we will center-punch crop as discussed above . Is there a way to globally set this with input node sizing or image scaling of what will be set as a 1920x1080 timeline for the editorial and dailies transcodes ? I 'm very new to Scratch , what 's the methodology applied to achieve it there ? <p> Thanks , oh and also , any feedback on the cropping options of 80-90% , thanks for that too . <p> I just worked a few days on some @ @ @ @ @ @ @ @ @ @ Arri Raw , spherical lenses . They were using an 80% crop . Recording open gate , of course , but it was a 1.85 aspect ratio , using 80% of the resulting image . Whether or not this is what your DP wants to use is up to him ( and testing ) , but I can at least confirm that this would n't be a unique workflow , that it has been used before . <p> In terms of workflow , you can do this with your Input sizing in resolve , like you thought . You just have to get the numbers right . I suggest using a framing chart , or making something up in Photoshop , that you can then scale in on and take a look at . In fact , I have the Frame Leader that was made by the previous DIT on the project I just mentioned . I used it to figure out the correct input sizing in Resolve so that my reference image out of Resolve ( for QC purposes ) was correct . If you 'd like , PM @ @ @ @ @ @ @ @ @ @ over to you . <p> Cool , I figured that was probably the way to go about it with input node sizing , and the plan is to use the Arri frame line generator to create the shape , then shoot a physical chart for reference . This is the result of playing around with it online already , I just need to find out if the Amira uses the WHOLE sensor or if the surround area is affected by shooting 3.2k mode vs. 2k or 1080 , as it would probably affect the generation of the frame lines with the online widget . <p> Chris , that is the best way to go : just have them shoot a chart on set every day , one for each camera . I l like a framing chart and a color chart if possible . At least this way , you know exactly what they saw on the viewfinder reticle display . <p> Cool , I figured that was probably the way to go about it with input node sizing , and the plan is to use the Arri frame line @ @ @ @ @ @ @ @ @ @ chart for reference . This is the result of playing around with it online already , I just need to find out if the Amira uses the WHOLE sensor or if the surround area is affected by shooting 3.2k mode vs. 2k or 1080 , as it would probably affect the generation of the frame lines with the online widget . <p> You know , I have n't worked with the Amira . I know the Alexa , open gate , is 4:3 aspect ratio , 3.2K pixels wide . I just assumed that if the Amira had a 3.2K mode , that the sensor was the same . Could be a bad assumption . If the sensor is n't as tall , you 're still basically doing the same thing , though . You just do n't have as much mostly-unneeded stuff being recorded above and below your image . <p> While Marc is n't wrong , above , you and I both know that shooting a framing chart every day wo n't happen . Framing chart per camera type , however , is something that should be shot @ @ @ @ @ @ @ @ @ @ , whether it 's a GoPro , Red , Arri , Canon , etc. , so that you can see if anything is going wonky , and then post can raise the flag the next day if they 're seeing anything wonky . <p> As for color charts . . . Marc , how often do you even get those with footage ? On anything I grade , it 's a media-managed folder , so any charts that may have been shot , are just *gone* . When I 'm on set as a DIT , this is brought up by other crew members , so if I push for a color chart , I often get dismissed . And also , sometimes , certain DP 's are n't too pleased to see a color chart ( Strange , I know , but it 's true ) . These are DPs across all experience and skill levels , from the corporate guys that are pretty young , to the ASC members that have been DP 's on big budget features longer than I 've been alive . I do n't @ @ @ @ @ @ @ @ @ @ is n't even going to get it . . . <p> Plus , the reality is , things move too quick on set . Especially the lower budget sets that could benefit greatly from chip charts . Ideally , every lighting change would be accompanied by a chip chart . Which will never happen . I usually use the few color bars that are on the Slate to check things , as they may not be a full chip chart , but at least they 're in every shot , and in 95% of the shots they 're in the same lighting that the subject is going to be in . And it 's 0 additional work for the ACs , and 0 additional arguing from me . <p> As a 2nd AC , I have only worked with a couple of DOPs in the past 12 years that were meticulous about shooting a color chart . One was shooting episodic television and one was shooting a big feature . If the DOP is not behind it , it likely wo n't get done . Since transitioning to DIT , @ @ @ @ @ @ @ @ @ @ setup but have n't had one at all in 4 years . Colorists have had to make due , and since they can , no-one on set pushes hard enough for a chart ( at least no-one that will be listened to ) . Using the color stripes on the sticks of the slate can be a problem if the A cam slate is lit by a flashlight with a Xenon bulb and the B slate by an LED bulb . <p> One thing I always push for and get in prep is a framing chart . I participate directly in composing the chart , getting it signed-off by the DOP , and shooting it . We usually do this on the last day of prep after last-minute lens/camera/personnel changes . Also a full workflow test with post . <p> As for color charts . . . Marc , how often do you even get those with footage ? 43971 @qwx453971 <p> I always ask for copies of all the original camera files , and I can go back to those in search of a color chart if there is @ @ @ @ @ @ @ @ @ @ of cheap productions use a lot of shortcuts , up to and including no sizing chart , no color chart , even no slates . Very , very stupid and unprofessional , in my opinion . <p> Colorists have had to make due , and since they can , no-one on set pushes hard enough for a chart ( at least no-one that will be listened to ) . Using the color stripes on the sticks of the slate can be a problem if the A cam slate is lit by a flashlight with a Xenon bulb and the B slate by an LED bulb . 43971 @qwx453971 <p> I have spoken to Charlie Parra , who runs the Denecke company ( makers of most American timecode slates ) , and unfortunately the color stripes have no meaning in a digital or film world . They 're just whatever paint they can get at that moment . I much prefer their B&amp;W slates , so I can at least get a vague reference of peak white and somewhat black , assuming no color gels on set . <p> Chris , @ @ @ @ @ @ @ @ @ @ them shoot a chart on set every day , one for each camera . I l like a framing chart and a color chart if possible . At least this way , you know exactly what they saw on the viewfinder reticle display . 43971 @qwx453971 <p> While I do n't disagree with color charts for each scene/location , those are a luxury , if not an outright fantasy to achieve realistically . ESPECIALLY in the world of tiered low-budget features shot in 3 weeks . <p> ... the sad reality is that a lot of cheap productions use a lot of shortcuts , up to and including no sizing chart , no color chart , even no slates . Very , very stupid and unprofessional , in my opinion . 43971 @qwx453971 <p> Exactly . And even though the show may have pros on the crew , we can only do so much with the time and resources allotted . That whole " tiered low-budget features shot in 3 weeks " thing feeds very significantly into that whole " stupid and unprofessional " thing . <p> .... but , @ @ @ @ @ @ @ @ @ @ are tied to your healthcare ... ROCK &lt;---&gt; HARD PLACE . <p> You know , I have n't worked with the Amira . I know the Alexa , open gate , is 4:3 aspect ratio , 3.2K pixels wide . I just assumed that if the Amira had a 3.2K mode , that the sensor was the same . 43971 @qwx453971 <p> Amira is 16x9 only , and the 3.2k mode of the sensor is 3200x1800 . That I do know for sure at this point . <p> While Marc is n't wrong , above , you and I both know that shooting a framing chart every day wo n't happen . Framing chart per camera type , however , is something that should be shot , preferably the first time that camera gets turned on , whether it 's a GoPro , Red , Arri , Canon , etc. , so that you can see if anything is going wonky , and then post can raise the flag the next day if they 're seeing anything wonky . 43971 @qwx453971 <p> Correct . No way it will happen daily @ @ @ @ @ @ @ @ @ @ can get it done on the short camera prep schedule , per camera , and that will likely be it , which honestly is normal going back to my film days as an AC 15 years ago . <p> ... When I 'm on set as a DIT , this is brought up by other crew members , so if I push for a color chart , I often get dismissed . And also , sometimes , certain DP 's are n't too pleased to see a color chart ( Strange , I know , but it 's true ) . ... 43971 @qwx453971 <p> THIS . There 's never a good time to do this . Maybe I could sneak onto set at lunch and get SOMETHING usable if I do it myself ... LOL . <p> Plus , the reality is , things move too quick on set . Especially the lower budget sets that could benefit greatly from chip charts . Ideally , every lighting change would be accompanied by a chip chart . Which will never happen .... 43971 @qwx453971 <p> Yup , no way that @ @ @ @ @ @ @ @ @ @ one will be what they can be , if they can be , given a feature film scheduled in 18 days . <p> While I do n't disagree with color charts for each scene/location , those are a luxury , if not an outright fantasy to achieve realistically . ESPECIALLY in the world of tiered low-budget features shot in 3 weeks . 43971 @qwx453971 <p> I 've generally been able to get them if I 'm right there on the set , but it 's usually at the end of the scene . I just yell , " keep rolling for chart , please ! " and they 'll go 10 more seconds . A lot has to do with who 's in charge and whether they understand the need for good post procedures . 1 or 2 charts a day is livable . <p> On other shoots , I 've given the 1st Assistant a bottle of Vermouth , and usually things are smooth sailing after that . 10 seconds of a chart on two shots per day will not kill them , nor will it make them lose @ @ @ @ @ @ @ @ @ @ on color chips for the slate made by DSC or one of the big chart makers , ca n't seem to find them with my google goggles at the moment . DSC One Shot style Slate Stickers FTW . I 've got the Spyder Cube thang on my slate , velvet black hole , mini chrome ball , it 's great . <p> I 've cut a couple of ads that had no slates whatsoever because " it would 've taken a lot of time " . Well ... You 're losing that preciously saved time in post now . 43971 @qwx453971 <p> Yep , I 've told people , " 10 seconds in post sometimes will cost you 10 minutes in post . It 's no problem if it just happens occasionally , but 200 takes in a day will be a problem , especially with timecode issues . " <p> I can think of one $10 million movie where 60% of the shots had " Marc Wielage Eyeball Sync , " totally winging it and looking by eye . Nobody complained , the movie made huge money , @ @ @ @ @ @ @ @ @ @ out and I had to be hospitalized after that project . OK , not literally , but stuff like this is frustrating . 
@@44332951 @4332951/ <h> You guys tried Wipster ? <p> Oh , I was excited to see the new Wipster pricing model . It cuts our cost in half . 43971 @qwx453971 <p> Different people will have different rates and experiences . All I know is , I average about 5-6 projects a month ( if I 'm lucky ) , and I might need to post as many as 20-30 videos during that time for approval . If I get charged $5 per video , that 's gon na be $150-$200 , which is insane . I do n't have a problem with $50 . Note also they want to limit the run time of the videos to 15 minutes , which kills it for feature reels . <p> I 've also done trailers where there 's six versions of every trailer and each one has to be uploaded and approved . They 're only 2-1/2 minutes long , but I get charged one rate for each of them ? Come on . <p> Different people will have different rates and experiences . All I know is , I average about @ @ @ @ @ @ @ @ @ @ , and I might need to post as many as 20-30 videos during that time for approval . If I get charged $5 per video , that 's gon na be $150-$200 , which is insane . I do n't have a problem with $50 . Note also they want to limit the run time of the videos to 15 minutes , which kills it for feature reels . <p> I 've also done trailers where there 's six versions of every trailer and each one has to be uploaded and approved . They 're only 2-1/2 minutes long , but I get charged one rate for each of them ? Come on . <p> Here are the latest pricing rates from Wipster as of 2/13/2017 : Since they 're calculating the costs by video , I would basically be roughly inbetween the $190 and $350/month rate . Given that I was previously paying around $40-$50 or so , this is a tad outrageous ... Call me crazy , but I think the Wipster people are on drugs if they think these prices are sensible . 43971 @qwx453971 <p> That @ @ @ @ @ @ @ @ @ @ versions of of the same video do n't count against the billing total . It does n't cost us to iterate . We 're saving because with 12 users on the team , we are no longer paying per user . <p> I really liked Frame.io , but it was a bit challenging for us because we had clients who just did n't want to sign up ; they 'd rather just check out a link and be able to share their notes without having to create and use a login . Wipster allowed them to collaborate without a signup process , so it was a clear winner . <p> That said , we 've used Vimeo for a long time , and I 'll have to check out their new review feature . <p> Wipster allowed them to collaborate without a signup process , so it was a clear winner . 43971 @qwx453971 <p> Actually , clients can view material on Frame.io without a signup process . I think only if you collaborate and change videos do you need to sign-in . Every month , once that project is @ @ @ @ @ @ @ @ @ @ people can be dropped and new people can be added on . 50 people is a lot of clients for a freelancer like me , and that 's more than enough . <p> My problem is the cost per video rather than the cost per gigabyte . To me , it 's just a question of how much total storage ; 100GB is plenty for me and will easily hold a compressed copy of several features at one time . <p> It 's very sobering to see that the $150/month Frame.io plan is actually far more beneficial from my point of view than the $350/month Wipster plan . <p> its the only one i know that let 's you do 360 video reviews ... bit more pricey but interesting <p> basically there are load of options all cheaper <p> its quite apparent from wipsters and their entire diagram / explanation of how the review process works .. they do n't really under stand the actual realities of .... well maybe they do for the agencies they deal with ... but not for the rest of us <p> the whole " quality @ @ @ @ @ @ @ @ @ @ with this .... is like something a government would come up with for some bizarre meaningless taxation method ...... design to quantify additional charges 
@@44332953 @4332953/ <h> i7 6700HQ vs 7700K <p> I am helping my wifey put together a laptop for editing and " compositing " , as here macbook finally died ( yay ! ) , but am a bit lost on the actual performance percentage difference between the 6700HQ ( or 7700HQ ) and the 7700k processors . Here 's the thing , we are choosing between two types of laptops : <p> - a big beefy one from Clevo with an overclocked and delided ! i7 7700k to 4.5Gz. - a slimer Gigabyte laptop with a 6700HQ and a better UHD panel that fully covers sRGB and ARGB . The panel also has better uniformity and blacks . <p> The beefier Clevo laptop has better thermals and more customisation options , but to be hones the slimmer one should be okey in that regard when using a cooling pad underneath the laptop . <p> I do n't know how much of a difference the two processor will make ( other hardware being the same : GTX 1070 , NVMe m.2 drives , etc. ) as I do n't have that much @ @ @ @ @ @ @ @ @ @ <p> The workflow will primarily consist of DNxHR or Prores UHD or HD footage . Premiere for editing and After Effects for the crazy stuff that she does in it for her art projects . No real time constraints on exporting as she usually has ample time to finish here projects . <p> The actual difference between the clock speeds is : - 3.1GHz Max all core turbo on the 6700HQ - 4.5 GHz Max all core turbo on the 7700K <p> This is a big difference I know , but seeing as this are only 4 core cpus there might not be such a real world difference ? The 7700K will be delided which will make sure there will absolutely be no thermal throttling , so there 's that . Also I have no idea how processing is CPU bound and how much GPU . <p> For myself , I would probably pick the notebook with the better panel . The Clevo has a 700:1 contrast ratio , 85% sRGB , 75Hz , IPS panel . The contrast ratio sucks as does the lack of sRGB gamut . But @ @ @ @ @ @ @ @ @ @ stuff , I ca n't recommend it without knowing the actual performance of it . <p> Oh and also this : is 32GB of RAM enough for Premier ? 5 to 20 minutes long projects . For After Effects she would use a fast 2000MBs NVMe drives for ram preview/scratch/cache drives . <p> 6700HQ is a laptop CPU , it only gets 2/3rd of the power that the desktop 7700k has . The trade-off here is speed vs battery life . But then , with GTX 1070 ( unless it is GTX 1070m laptop version ) it 's unlikely the battery will last you for long anyway . Do n't expect to overclock 7700k in a laptop though . <p> Clock speed is extremely important for premiere . I think with the new after effects playback system it is the same . Gets the faster clocked CPU , hands down . If you are worried about monitor quality I would get an external monitor . <p> The difference is 7700k is 30% faster . It is going to be noticeable , especially during compositing . Editing - depending on media @ @ @ @ @ @ @ @ @ @ like H.264 . Less so with formats like ProRes , DNxHD . <p> Tnx guys . Did n't know AE had new stuff like that . I have n't used it in some time now . <p> It 's not just better monitor , it 's also a slimmer laptop that can be used for browsing , research , typing documents , etc . <p> I was thinking that the difference in speed wont help that much as it 's still only 4 cores . I would expect a 25% difference with a 6 core not a faster 4 core . I could test this I guess on my workstation , by disabling all but 4 cores and then downclocking ... Wont be the same due to RAM having more channels and CPU being different architecture but alas it should give a basic idea . <p> I 've tried telling her this exactly . With Ryzen , she would get an 8 core workstation for half the cost of the clevo , with a better GPU and more RAM . But she needs an all in one package I guess ... for travelling purposes or what not . 
@@44332954 @4332954/ <h> Alexa vs Red camera advice <p> Anamorphic 's are really important in medium &amp; low budget stuff , because you instantly get a " film " look even though the production can just be nominal . 43971 @qwx453971 <p> Funny , I 've advised low budget projects of exactly the opposite ; Anamorphic glass costs more to rent than some very good spherical options AND it demands a more experienced 1st AC to land focus properly on Anamorphic glass , post is a little more complicated ( not for those of us sitting here on a Saturday night discussing the ins and outs of exact pixel ratios , but for the likes of people that tend to be working on the lower budget projects ) and thus costly , and even monitoring the image on set can involve some additional hardware , software , and even people , all things considered . <p> I 've worked on . . . 9 features now , in dailies or on set , and only 2 of those shot Anamorphic . One was a Tier 1 , and the only reason @ @ @ @ @ @ @ @ @ @ the people involved were long-time veterans of the industry and were able to call in a TON of favors . If my opinion is asked , I would never suggest a low budget project shoot anamorphic , despite the valid advantages you mention . <p> sorry guys , math in not an opinion , that you like our not . you exposed 2.9k horizontal , that you like it or not , that is what you got . with anamorphic lense , you get better vertical resolution , that is about it . a pixel does not know you squeezed more information in it , it is lost once hit the sensor . <p> Here is what I 'd like to suggest to end this discourse once and for all . I can ask my friend who happened to have both cameras- Mini and Dragon with both sets of KOWA and K35 . I 'm sure we could do a quick side by side set up of the two cameras . Could anyone , that is involved in this discussion provide a 4k screening room for an hour or so @ @ @ @ @ @ @ @ @ @ question ? : Does Alexa camera with Anamorphic lenses and RAW recording should outright be disqualified from being considered for 4k project based solely on a pixel count as opposed to a true 4k camera ? Or you feel , that this is the wrong question ? <p> I agree in principle , but I 'm not sure that those who are shooting the material would always prefer anamorphic over spherical , for many practical and artistic reasons . So it 's not really a completely logical comparison except in cases where anamorphic is specifically desirable . 43971 @qwx453971 <p> I agree on that point as well Mike . So , my point , if anamorphic images are acceptable for artistic reasons , then Alexa SHOULD be allowed to be used for Netflix production . <p> Actually , if you want to be specific , the current anamorphic mode on the Alexa is 2560x2145 , an aspect ratio of 6:5 that yields a " proper " 2.39:1 image matching the current DCI expectation when unsqueezed with a factor of 2x ... 43971 @qwx453971 <p> On the project I 'm working @ @ @ @ @ @ @ @ @ @ is 2904x2160 for a 2.66 unsqueezed image . It may be different in RAW mode ... <p> Here is what I 'd like to suggest to end this discourse once and for all . I can ask my friend who happened to have both cameras- Mini and Dragon with both sets of KOWA and K35 . I 'm sure we could do a quick side by side set up of the two cameras . Could anyone , that is involved in this discussion provide a 4k screening room for an hour or so , so we could provide the definitive answer to the question ? : Does Alexa camera with Anamorphic lenses and RAW recording should outright be disqualified from being considered for 4k project based solely on a pixel count as opposed to a true 4k camera ? Or you feel , that this is the wrong question ? 43971 @qwx453971 <p> 1 ) shoot frame charts/resolution charts 2 ) I can probably talk with the boss and use one of my rooms in the evening , 4k Barco . <p> Funny , I 've advised low budget projects @ @ @ @ @ @ @ @ @ @ rent than some very good spherical options AND it demands a more experienced 1st AC to land focus properly on Anamorphic glass , post is a little more complicated ( not for those of us sitting here on a Saturday night discussing the ins and outs of exact pixel ratios , but for the likes of people that tend to be working on the lower budget projects ) and thus costly , and even monitoring the image on set can involve some additional hardware , software , and even people , all things considered . <p> I 've worked on . . . 9 features now , in dailies or on set , and only 2 of those shot Anamorphic . One was a Tier 1 , and the only reason it was able to shoot Anamorphic is because some of the people involved were long-time veterans of the industry and were able to call in a TON of favors . If my opinion is asked , I would never suggest a low budget project shoot anamorphic , despite the valid advantages you mention . 43971 @qwx453971 <p> My starting @ @ @ @ @ @ @ @ @ @ budget work where it justifies the camera purchase rather then rent . I know some people consider 250k the starting point of low budget , but I often find that number is a bit fudged as it 's aligned more for SAG and not the real actual total cost - so I think of low budget between 500k to 25million ( either way I would n't recommend a camera purchase for a 250k production ) . When you get in the above 2million budgets , the film better look and sound completely tier one since that range is really tough now adays . Long story short , I 'd agree with you on anything under 500k , but again I would n't recommend a New RED or Alexa camera purchase for a micro budget/ultra-low-budget film . <p> The Netflix delivery specs do n't exactly cover the problem of anamorphic release in a 1.78 project . They are very specific about uprezzes : <p> " Up-resing of content is prohibited . SD sources can not be used to create HD deliverables. 4K video must come from a true 4K source and maintain @ @ @ @ @ @ @ @ @ @ <p> The problem with what they 're saying is that there are a lot of variables here . Interestingly , in another document they say that a Blackmagic 4K Ursa Camera is acceptable for program acquisition , but they imply that any standard Alexa ( not the Alexa 65 ) is not acceptable . So in other words , a $5000 camera is OK , but a $50,000 camera is not . <p> The problem with what they 're saying is that there are a lot of variables here . Interestingly , in another document they say that a Blackmagic 4K Ursa Camera is acceptable for program acquisition , but they imply that any standard Alexa ( not the Alexa 65 ) is not acceptable . So in other words , a $5000 camera is OK , but a $50,000 camera is not . 43971 @qwx453971 <p> C'm on , Marc . You know better than that . Cost is not the issue , not for Netflix and not for anyone else . You use Resolve , right ? So a " free " program is acceptable as opposed to @ @ @ @ @ @ @ @ @ @ Lustre , etc . ) ? <p> Specs are acceptable and IMHO necessary in our industry . Sometimes they 're more meaningful than at other times , but companies have an absolute right and in my opinion obligation to set some minimum standards for delivery elements , and if you want to work with them , you adhere to those standards . You do n't have to agree with their conclusions that led to the creation of those standards , but you should understand that they did n't pull numbers out of a hat , and it 's their money . You create material that meets those standards and move on . That 's the way it is for post production suppliers , and it 's the way it is ( or should be ) for production as well . <p> Pixel specs aside , I have always found the Alexa 's 2.8K or 3.2K raw image to appear sharper during display than any high-rez Red image when mastered for either 2K or 4K . From my experience shooting with both cameras I 've consistently found the Alexa 's sensor @ @ @ @ @ @ @ @ @ @ Especially when shot arriraw the image can have extreme amounts of contrast graded back in to produce a sharp , rich , extremely contrasty picture that still retains smooth gradations in the mid tones and high lights . The Red sensors are nowhere near as elastic in terms of DR and contrast . Under certain lighting conditions both cameras can produce almost identical results but in extreme situations the Alexa always seems to handle scene contrast , mixed color temps , and specular highlights much more favorably then the Reds . <p> The Helium 8K sensor is a bit of a head-scratcher for me . There are always compromises . I have n't tested this sensor yet but my gut tells me there will be problems with color separation that will negatively affect skin tones especially under mixed lighting situations . Conversely the Alexa Mini provides HD , 4K , raw , internal NDs , a full-ap 4:3 sensor , records to cheap Cfast media , and will probably be worth twice as much as the Helium in 5 years . Go with the Mini . <p> C'm on , @ @ @ @ @ @ @ @ @ @ not the issue , not for Netflix and not for anyone else . You use Resolve , right ? So a " free " program is acceptable as opposed to a " costly " one ( i.e. , Baselight , Lustre , etc. ) ? 43971 @qwx453971 <p> So , Mike , you 're saying you believe a Blackmagic Cinema camera puts out a picture exactly as good as an Alexa ? <p> Me personally , I think the Alexa looks better than any Blackmagic camera I 've seen . I also think that there 's a lot more to judging quality than resolution alone , particularly if the resolution is that close ( particularly with Alexa in 3.2K mode ) . <p> So , Mike , you 're saying you believe a Blackmagic Cinema camera puts out a picture exactly as good as an Alexa ? 43971 @qwx453971 <p> I did n't say that , although frankly , I 've seen images from the 4.6K Ursa that are pretty darned good , and with enough quality to satisfy some pretty discriminating VFX supervisors . You equated such choices with @ @ @ @ @ @ @ @ @ @ something that is true in many areas of current technology , particularly when you artificially limit the criteria . For example , a $35K Ford Focus RS might be faster than a $200K Maybach from 0-60mph , but it 's irrelevant because you do n't buy a Maybach for its 0-60 time . There are broadcast shows using Alexas , and there are broadcast shows ( at least on on Netflix , in fact ) using Canon C300 MkIIs . Both have their reasons and both are satisfied with the results , despite a rather significant cost difference . If you 're going to compare devices , cost is not always the differentiator between what is appropriate and what is not , or even what is best and what is n't . <p> There are broadcast shows using Alexas , and there are broadcast shows ( at least on on Netflix , in fact ) using Canon C300 MkIIs . Both have their reasons and both are satisfied with the results , despite a rather significant cost difference . If you 're going to compare devices , cost is not @ @ @ @ @ @ @ @ @ @ not , or even what is best and what isn't. 43971 @qwx453971 <p> I du n no . I think if you shot a show on a $1995 4K BMD camera and compared it to an Alexa shooting under identical circumstances , I 'd be extremely surprised if it beat the Alexa in any way except 4K resolution . I think there are a lot of people who get wrapped up in numbers and kind of lose sight of the bigger picture -- literally -- in terms of how it actually looks in the real world . <p> The number of people falling all over themselves crowing about 8K on the RedUser group is a little alarming . I do n't look forward getting those projects in for post , especially if they get it in their heads they need to actually finish in 8K . There 's already a growing number of users there complaining that they want a " affordable " HDR/Rec2100 grading monitor for their projects , unaware of what this actually costs in the real world . <p> I would advise you to purchase the camera @ @ @ @ @ @ @ @ @ @ all the accessories you are going to need and use . Extra cables , pelican cases , filters , insurance . Anything that truly gets you a functional camera . Get the best batteries you can afford , Firecrest NDs , Schneider True POLA . All the MB reducers you might need for your lenses . <p> And take a look at Helium files yourself . Test them , this a serious camera , just make sure you use RedWideGamut RGB . I really do not understand why people like Adrian post gut feelings , when I am seeing insane files coming from this sensor . <p> The Helium 8K sensor is a bit of a head-scratcher for me . There are always compromises . I have n't tested this sensor yet but my gut tells me there will be problems with color separation that will negatively affect skin tones especially under mixed lighting situations . Conversely the Alexa Mini provides HD , 4K , raw , internal NDs , a full-ap 4:3 sensor , records to cheap Cfast media , and will probably be worth twice as much as @ @ @ @ @ @ @ @ @ @ . 43971 @qwx453971 <p> Things are moving quite fast , likely in five years there 's gon na be a $1000 camera that will beat the mini in all aspects . <p> Things are moving quite fast , likely in five years there 's gon na be a $1000 camera that will beat the mini in all aspects . 43971 @qwx453971 <p> I do n't know if I 'd say that , but one thing I 've been telling camera users for awhile is : lenses last for a long time . Tripods and heads last damn near forever . Lighting and grip gear holds up well for a long time . G&amp;E gear in general holds its value . But cameras have kind of turned into computers in that a $50,000 model from seven years ago may well be worth only a fraction of that today . ( I 'm reminded of seeing a Sony F900 on eBay for $495 last year , with no buyers . ) <p> For a lot of people , it may make more sense just to rent the camera and then own everything @ @ @ @ @ @ @ @ @ @ the new thing . Otherwise , you have to get used to the idea that you spend $15,000 or $20,000 or $30,000 or $40,000 , then get rid of that every five years and buy a new one . If you 're shooting every single week , sure , the camera can pay for itself . But a lot of the neophyte users I see on RedUser are not in that category . <p> I do n't know if I 'd say that , but one thing I 've been telling camera users for awhile is : lenses last for a long time . Tripods and heads last damn near forever . Lighting and grip gear holds up well for a long time . G&amp;E gear in general holds its value . But cameras have kind of turned into computers in that a $50,000 model from seven years ago may well be worth only a fraction of that today . ( I 'm reminded of seeing a Sony F900 on eBay for $495 last year , with no buyers . ) <p> For a lot of people , it may @ @ @ @ @ @ @ @ @ @ own everything else . When the newer thing comes out , rent the new thing . Otherwise , you have to get used to the idea that you spend $15,000 or $20,000 or $30,000 or $40,000 , then get rid of that every five years and buy a new one . If you 're shooting every single week , sure , the camera can pay for itself . But a lot of the neophyte users I see on RedUser are not in that category . <p> It 's a very expensive hobby . I have warned people many times that they think they 're gon na get into the filmmaking business when they buy a $40,000 Red camera . What they do n't know is that you have to spend at least another $100K for everything else . And a really big kit would have more than $100K just in lenses -- lenses that wo n't become obsolete every five years . 
@@44332955 @4332955/ <h> Premiere 2017.1 Randomly Deleting Media <p> For any of you that updated to Premiere 2017.1 , there appears to be a bug where Premiere confuses actual media files with cache files , and when asked to clean up cache , will delete real camera media as well . <p> In the meantime , do one of the following : 1 &gt; Set your cache file location back to the default location . You can do this quickly by trashing your user preferences ( hold down Alt ( on Win ) or Option ( on Mac ) ) and then launch Premiere Pro . Or <p> 2 &gt; Create a custom folder on whichever drive you want to use and direct your cache files to this location . Keep all files other than the Media Cache file out of this location . <p> For any of you that updated to Premiere 2017.1 , there appears to be a bug 43971 @qwx453971 <p> Deleting media from hdd without user knowing about it ? It 's not a bug - it is a straight up virus . Antivir software should include @ @ @ @ @ @ @ @ @ @ bad record rolling up updates . It is a good rule of thumb not to update any Adobe software right away . It pains me to say it because I generally consider premiere a great tool . <p> Just did a fresh install of Win 10 and decided to try out 2017.1 . This is probably the scariest bug I 've ever heard of . Personally I have n't run into it yet . Adobe 's quality control for updates they push out to the public has really gone downhill since switching to a subscription model . Honestly wondering if it might be time to make the switch to Resolve for editorial work . <p> I 'm dumbfound . The messages are terrifying . This ca n't be happening to a professional program . People losing several days of work because of disappearing footage is totally unacceptable . I wo n't be surprised if Adobe gets sued by many of its users . <p> Wow ... Terabytes of data disappearing . People having to reshoot . Best cases restoring for days ..... I am NOT opening my premiere install . @ @ @ @ @ @ @ @ @ @ 3rd party ' project and assist in conform issues in Premiere , but damn ..... <p> The best thing is its of course ' our own fault ' . Here the official quote from adobe on this : However , incorrect usage of this feature has the potential for unintentional file deletion . <p> Incorrect usage ..... Amazing . Introducing a new autoclean feature that wipes your system and calling it incorrect useage . <p> Just read through the whole post on adobe forums and realized how many times I have been in situation in wich this kind of " bug " could be disastrous . <p> In an industry with crazy tight turnarounds , sometimes work being finished literally days , or hours before airing time . This is potentially career ending stuff . I get it that if you do n't have your media backed up your are probably asking for it . But now in addition to hardware failures we have to worry about software actually deleting media on us ? And what about the times when this couple of hours of restoring media is exactly the @ @ @ @ @ @ @ @ @ @ . Adobe just raised the bar , and it will stay up there in the clouds for years to come . <p> I 'm just thinking at the moment how screwed I would be if this occurred for me . At the moment my main workstation runs a RAID 10 array internally and then has an offsite backup copy done via backblaze . So I 'm protected to a degree from mechanical failure of drives , and then accidental deletion via the offsite , but retrieving terabytes worth of offsite backups is not a quick task . And that assumes that the cloud backup has actually finished uploading all of my files in the first place ... <p> Imagine a scenario where a few terabtyes worth of footage lands on the system to begin work immediately , before that initial offsite backup has completed , Premiere comes along and deletes media from my RAID array without me knowing . Boom , media gone . Unless it 's already had another backup before it 's handed to me , that leaves me very red faced and potentially liable . <p> My @ @ @ @ @ @ @ @ @ @ 1 . Media lands on my primary fast RAID , but is still considered unavailable for editing . 2 . Is immediately copied 1:1 to second slower/cheaper netword attached RAID . 3 . From this moment on the primary RAID media is released / allowed to work on ( email alert that new media has been ingested ) 4 . Within 12hrs the backup raid is synced/in sync with the offsite DR storage via high speed point2point wireless uplink . 5 . Backup raid is also synced to cloud storage provider , but that can take weeks and is more for when my whole building(s) burns down . <p> BUT ! ! ! Even with this super nice and safe scenario , if Premiere would have wiped the primary RAID , it would still take at least 24 hrs to restore from backup RAID . Most people did have backups apparently but are hit by restore time killing their **28;1630;TOOLONG by the look of it . So still screwed if on a deadline . Premiere goes in the bin .... Unforgivable and i feel with all the folks hit . @ @ @ @ @ @ @ @ @ @ ( and probably pay ) <p> EDIT : Let this be a lesson for Blackmagic , as there is also a similar button ( clean cache ) on Davinci ( which i have yet to touch ; - ) . Let 's not introduce an automatic feature for that in 14 final please . <p> " Premiere Pro CC 2017 ( 11.1 ) introduced a new feature to manage and automatically remove aging and unnecessary media cache files . This feature was designed to assist users in managing existing project media cache files more easily . In the default location for media cache preferences , there is no issue . However , incorrect usage of this feature has the potential for unintentional file deletion . " <p> Working with the new Premiere Pro CC 2017 ( 11.1 ) automatic Media Cache management tools to avoid unintended deletion of user media Premiere Pro CC 2017 ( 11.1 ) introduced a new feature to manage and automatically remove aging and unnecessary media cache files . This feature was designed to assist users in managing existing project media cache files more easily . In @ @ @ @ @ @ @ @ @ @ no issue . However , incorrect usage of this feature has the potential for unintentional file deletion . <p> To avoid this issue update to Premiere Pro CC 2017 ( 11.1.1 ) . <p> **25;1660;TOOLONG <p> Thanks Much Jason Myres , without liftgammagain I 'd have no idea what is going on here , as adobe 11.1.1 patch notes did n't even reference this problem . 
@@44332956 @4332956/ <h> Avid to Daylight Roundtrip Workflow <p> Other benefits that you get along with it are a renderless workflow , which means savings on drive space , and ease of interoperability with clients on Avid . I recently did a music video for a client as a test and sending bins back and forth made life much easier ( for both of us ) . 43971 @qwx453971 <p> And with the free read-only version of Editions , your client does not even need to buy an Editions license . You send them BLG files ( or an AAF with embedded Baselight effects ) and they can just play it in their Avid . <p> These are early days with this workflow and would only work for certain cases . If Resolve makes the most sense , I completely understand that . I still use it on many jobs myself ! However , I think it 's worth keeping an eye on FilmLight as their software progresses . 43971 @qwx453971 <p> I think that is a good approach . Recent history shows that cut down versions of software have a @ @ @ @ @ @ @ @ @ @ features that were cut out of the first release , and ultimately winding up much closer in capabilities to the original " full version " product . Examples of this would be Colorfront Express Dailies , Resolve ( the free version ) , Avid Express DV ( a while ago , but a similar story ) , and others . At this point , Daylight does not have any of the higher end conform capabilities and none of the timeline editing capabilities of Baselight . But going forward , who knows . I could see them releasing a " non-rendering " ( or limited rendering ) Baselight version that basically does everything full Baselight does , but requires you to render in another program ( Avid , for instance ) . Coupled with Avid , that would make for a potentially compelling workflow and approach for broadcast work that could likely be released at a much lower price than the current Baselight One , if they chose to do so . Hmm , come to think of it , maybe I 'll pass that suggestion along ..... <p> I would @ @ @ @ @ @ @ @ @ @ a another facility and expect the grade to be what I intended . <p> There may be differing workflows , but there lots of gotchas when workings with ble , certainly for me in broadcast . <p> Some of the these issues may be fixed , but certainly were n't 2 months ago and have been using the plugin for over a year . <p> If you grade in a full Baselight and send to ble , any colour transforms do not travel in the aaf . That means on a programme with 1000 cuts , with slog3 footage for example , you would have to individually change each of the inputs to slog3 in the ble . It gets pretty tedious , and the workaround I got from filmlight did n't work . Unfortunately you ca n't render in Baselight from media on Isis , so the media needs consolidated first to the Baselight raid . So using the plugin would be great and save time , but rendering is the only option for now . <p> At least it 's quite easy to tell something is wrong when @ @ @ @ @ @ @ @ @ @ the full Baselight determine the input from the metadata in a scene , sometimes the difference is less obvious , like legal video . This also will not pass on in the aaf . Again , the grade will not be as intended , and the editor may not notice that something is wrong . And in a mixed format programme where there are lots of possibilities , fixing it is certainly not fun . <p> If you use the arri photometric transform v2 , this requires a manual install in ble . And again , the grade will not look correct if this is n't installed in the Avid system , and you have to go into the input strip before it let 's you know that the transform is missing . <p> This may be fixed now , but the opportunity for things like this to happen , would mean I would n't send a aaf with grade to another facility and expect them to work . <p> Not being able to render would not be something I would be interested in . The render time in avid is @ @ @ @ @ @ @ @ @ @ ( my facility does n't have great avid hardware so probably a bit unfair here ) . <p> The ble is great for adding shapes in avid , grading difficult shots , and great for re-applying a grade to new versions of gfx ( or for textless versions of the gfx for worldwide delivery ) . It 's good for adding grain , the blur is excellent , great tracker . It 's got great colour tools but difficult to grade a 1 hour programme in , although definitely possible . I use the plugin all the time and would massively miss it in the avid . <p> Some workflows will , I 'm sure , be fine . But best to thoroughly check the results . And it probably needs managed , with the editor realising the potential problems . <p> If you grade in a full Baselight and send to ble , any colour transforms do not travel in the aaf . That means on a programme with 1000 cuts , with slog3 footage for example , you would have to individually change each of the inputs to @ @ @ @ @ @ @ @ @ @ and the workaround I got from filmlight did n't work . Unfortunately you ca n't render in Baselight from media on Isis , so the media needs consolidated first to the Baselight raid . So using the plugin would be great and save time , but rendering is the only option for now . 43971 @qwx453971 <p> You 're absolutely correct . I 've heard this many times before and I 'm hoping FilmLight comes up with a solution in the next rev . What was the workaround they proposed ? <p> Not being able to render would not be something I would be interested in . The render time in avid is way longer than the time needed in a full Baselight ( my facility does n't have great avid hardware so probably a bit unfair here ) . 43971 @qwx453971 <p> I 'm very curious to hear what your Avid config is . I know BL Editions currently works with AMD cards . Do you have one installed ? I 'm testing on a retina Macbook Pro and getting decent performance . <p> The workaround involved adding a @ @ @ @ @ @ @ @ @ @ choose identify colour space and then choose the correct colour space - so the conversion is completed in the layer and not the input strip . <p> We are running windows , z800s , but ca n't remember the spec . I 'll take a look tomorrow . It 's unfair for me to compare our Avids to our Baselight in render times , especially as they are probably going to be replaced soon , but there is quite a difference . To be fair , the render times for the ble are a lot better than many avid plugins . <p> I should also say , the render time is not the reason we do n't use exchanging grade via the aaf - it 's because of the colour space issues I mentioned , and things like a missing arri photometric v2 drt , where it 's not obvious that the grade is not being applied as intended . <p> I am currently running Editions on a free license , so ca n't absolutely verify the colour spaces , but I just tested an AAF round trip with a @ @ @ @ @ @ @ @ @ @ visually to come back correctly into Avid . The Update AAF dialogue box in Baselight specifically has options to include the input and viewing colour spaces , which would suggest they are supposed to come across : <p> Incidentally , there is no reason to use the ARRI photometric V2 DRT unless you are using the inverse transform ( using it to map a display referred source to an ARRI LogC working space ) . The forward transform in the V1 and V2 DRTs is ( I believe ) identical . <p> That 's great it 's fixed . It was a bug that filmlight were aware of , and it has only been recently fixed then . <p> From what I remember , you do n't get the option to choose which version of the arri photometric drt you use on the full Baselight , but could be wrong . If it used v1 in ble that would be fine , but it simply states that it is missing , and no transform is completed . <p> Ah cool , I could n't remember if you had the option @ @ @ @ @ @ @ @ @ @ point is that there are opportunities for there to be problems when sending to another facility , without checking that everything is working as it should . Which of course is always a good idea . <p> I should be able to check before the end of the week . I was given the workaround I mentioned on the 8th October , so was after that . <p> Ah cool , I could n't remember if you had the option to choose . But either way , I guess the point is that there are opportunities for there to be problems when sending to another facility , without checking that everything is working as it should . Which of course is always a good idea . 43971 @qwx453971 <p> Absolutely . It 's always worth checking , not assuming . <p> Also , you 'll notice in my screen-shot above I have legal &lt;&gt; full scaling LUTs enabled in my Update AAF settings . This works for the way I work in Avid but others have their setups configured differently ( particularly those who worry about the Avid UI image @ @ @ @ @ @ @ @ @ @ need to verify this too , and not assume that just because it looks correct in your Avid it will look correct in somebody else 's too . <p> I do n't mean to hijack this thread but it seems like asking here rather than starting a new thread would be OK .... Based on the tutorial vids on the Filmlight site and " quick start " guide , I get the impression that Daylight can not background render when being used as the dailies application it 's nominally designed as . Is that correct ? I really want to like it ... I 've been wanting to like it since Filmlight started making noise about it a couple of years ago , but no bg render is a deal-breaker I 'm afraid ... <p> I do n't mean to hijack this thread but it seems like asking here rather than starting a new thread would be OK .... Based on the tutorial vids on the Filmlight site and " quick start " guide , I get the impression that Daylight can not background render when being used as the @ @ @ @ @ @ @ @ @ @ correct ? I really want to like it ... I 've been wanting to like it since Filmlight started making noise about it a couple of years ago , but no bg render is a deal-breaker I 'm afraid ... 43971 @qwx453971 <p> Not quite true . The render queue is persistent and active even when you 're using the program . However , it is automatically suspended whenever you do anything in the main program , such as hit play , change color settings , etc . So it 's " background rendering " of a sort , but not quite as fast or efficient as some other programs that do n't completely suspend render operations when things are being done in the foreground . <p> Tried the ble workflow with avid at our facility and it totally failed . Do I miss anything or is there no option to import BLGs - either manually or via lens - if you are running the free version of ble4avid ? So the only viable workflow would be to work with an aaf roundtrip . Unfortunately I always got an error @ @ @ @ @ @ @ @ @ @ ( " ca n't update EDL " or anything like that ) . Support said it is a bug on their side and that they gon na fix it . But reading this thread it seems that the workflow works for others . Do you have to set up your avid timeline in any specific manner ? Right now I just dropped one Baselight-Effect over the entire timeline and exported an AAF . Do you have to apply the effect to the clips separately ? <p> Tried the ble workflow with avid at our facility and it totally failed . Do I miss anything or is there no option to import BLGs - either manually or via lens - if you are running the free version of ble4avid ? So the only viable workflow would be to work with an aaf roundtrip . Unfortunately I always got an error when I tried to export an updated AAF from baselight ( " ca n't update EDL " or anything like that ) . Support said it is a bug on their side and that they gon na fix it . But @ @ @ @ @ @ @ @ @ @ others . Do you have to set up your avid timeline in any specific manner ? Right now I just dropped one Baselight-Effect over the entire timeline and exported an AAF . Do you have to apply the effect to the clips separately ? 43971 @qwx453971 <p> You do n't actually " import " the BLG 's in Avid . Here 's what you do : <p> Export all of your BLGs from Baselight into one folder . In Avid , create a new video track and put a single instance of the Baselight plugin on it . Go to effects mode and open the Baselight interface . There is a pulldown menu for BLGs . You will need to create a new Lens , then edit that lens to point to the folder of BLGs you created . You then activate that lens . Provided the metadata matches ( very important , you must select the proper criteria for tape name and it must match the Avid tape name ) all of your grades will be applied to the proper shots . If this does n't work , you @ @ @ @ @ @ @ @ @ @ in more detail if you want assistance . <p> It is possible . The render queue is completely independent of any cursors you may or may not have open . The caveat is that background rendering gets automatically paused and restarted depending upon what you 're specifically doing at the time . So assuming you 're pretty busy in the interface , the rendering might be paused more often , causing the render to take longer . Hence my statement about efficiency . The ideal situation is to have two Daylight computers and use one for rendering only , but that 's a rather elaborate and expensive ( for individual users ) solution . <p> Export all of your BLGs from Baselight into one folder . In Avid , create a new video track and put a single instance of the Baselight plugin on it . Go to effects mode and open the Baselight interface . There is a pulldown menu for BLGs . You will need to create a new Lens , then edit that lens to point to the folder of BLGs you created . You then activate @ @ @ @ @ @ @ @ @ @ , you must select the proper criteria for tape name and it must match the Avid tape name ) all of your grades will be applied to the proper shots . If this does n't work , you 're going to have to explain what you 're doing in more detail if you want assistance . 43971 @qwx453971 <p> The problem is that you ca n't open the Baselight interface with the free version . You immediately get a prompt to enter your SN for the full version . That 's why I tried the AAF workflow which does not work on our system . Tried it with multiple Avid Projects / AAFs and always got the " ca n't update EDL " Error when I try to export an updated AAF from baselight . <p> You need to create an aaf in avid to populate a timeline in baselight . Once your done grading you use that aaf as a base and baselight injects its plugin information into the original aaf to create a new graded aaf . Baselight ca n't create its own aaf , it needs avid to @ @ @ @ @ @ @ @ @ @ there 's probably something in your timeline that 's not compatible with that process . I.e. they do n't match . Current software should give a warning when this happens . <p> The problem is that you ca n't open the Baselight interface with the free version . You immediately get a prompt to enter your SN for the full version . That 's why I tried the AAF workflow which does not work on our system . Tried it with multiple Avid Projects / AAFs and always got the " ca n't update EDL " Error when I try to export an updated AAF from baselight. 43971 @qwx453971 <p> Are you using Daylight or full Baselight ? I have not had issues doing an AAF Avid to Baselight round trip with the free version of Baselight for Avid , as shown in the screen shots in my earlier posts . I have n't tried ( and do n't know if it 's even possible ) with Daylight . <p> By the way , the AAF round trip is not a BLG workflow , as such . You just export an @ @ @ @ @ @ @ @ @ @ , with no Baselight effects applied . Import and grade in Baselight , then choose " update AAF " from the shots view ( from memory - I 'm not in front of Baselight right now ) . When you bring the AAF into any Avid with Editions installed , including in free mode , the shots have a per-clip Baselight effect . This is obviously a different workflow to the one this thread was originally about . 
@@44332957 @4332957/ <h> Pro Res 444 V Pro Res 444 XQ <p> Quick question is there a huge amount difference ( apart from the file size ) in these two formats of pro res <p> i have job shot on BMC4k in RAW about 90% is VFX shots its only 2 mins long <p> the lighting is solid all the way thru and really well exposed . but there is going to be tonnes of back and forth between VFX and edit and color and there is a load of green screen ... i was just thinking to convert the whole thing to XQ then everyone can handle it a bit easier <p> the other option would be EXR but that could get a bit tricky as they want to do temp edits all the time plus it going to be a monster on storage <p> My on Set experience is that XQ gives you more overhead , which often is only used in dark scene situations because of the upcoming noise . Yet I 'd guess the VFX guys would be really happy with EXR files . So maybe offline @ @ @ @ @ @ @ @ @ @ is quite variable , so if the scene allows , it will go down with the bitrate quite a bit . If the VFX guys are alright with that , it could be a good compromise . <p> I doubt you would see a difference , but I guess that pretty much depends on the image content . XQ makes sense if you 're gon na load it back in and do something with it - for final delivery , I would think the usual 4444 is just fine . For intermediate ... maybe not - but again , that depends on what you 're doing with the image . The more back&amp;forth , the heavier the treatment of the image in VFX and grading , the safer XQ is. 
@@44332958 @4332958/ <h> Digital Vision - Nucoda and Phoenix at IBC 2016 <p> Digital Vision will once again be showing our wares for your delight and delectation at the annual gathering in the home of Heineken , overpriced tiny hotel rooms and the most laid back people on the planet . <p> We will be showing Nucoda 2016.1 " our second release for the year and quite a major one at that , some changes to the interface , updates to the Precision Panel and a lot of really cool new features . We will be showing the ACES colour manged workflow , working in HDR with our friends at Dolby and give a preview of the GPU processing that is coming up later . Also routing from any layer , paint on any layer and other neat stuff . <p> Do n't forget Phoenix , definitely the best restoration tool around , new difference matte generator , paint on every layer " a Preview of DVO Reframe ( top secret ) and best of all DVO Dust II " probably the most impressive Dust and blemish removal tool you have @ @ @ @ @ @ @ @ @ @ and the amazing Golden Eye Film scanner will also be on the show floor . <p> For those that have n't seen Nucoda in a while this should definitely be on your list of things to see . The number of features and fixes in this next release are astounding ! <p> Unfortunately I 'm too busy to make it this year but if anyone ever wishes to see Nucoda in action or have a play we 're always happy to do demo days and let people have ' play time ' on the systems at my facility in London . Just ask 
@@44332959 @4332959/ <p> I knew already but you can tell it 's Lubezki straight away ; from the short primes to the tracking shots to even the way the skies are processed ; looks identical to some of the shots from his Instagram - https : //instagram.com/chivexp/ . Great trailer too ; did n't give anything away . <p> As a side note here the story of Hugh Glass the trapper which this film is based on , is pretty amazing so i , m quite looking forward to seeing this on the big screen plus , the Cinematography looks great and the BTS could be interesting <p> Watched this again through the smart TV/proper sound system . I 'm sure they 'd say the same about we do but I have the utmost respect for sound designers ; I love the little kick of bass that coincides with the gunshot at 1:49 . Would love something similar to the Soundworks Collection for picture post but idk if/how it 'd work . <p> I read somewhere that some shots for Mission Impossible and another movie I do n't remember currently where @ @ @ @ @ @ @ @ @ @ shots from the Alexa 65 may not got to the Revenant but it could be the first film primarily shot on it . <p> Watched this again through the smart TV/proper sound system . I 'm sure they 'd say the same about we do but I have the utmost respect for sound designers ; I love the little kick of bass that coincides with the gunshot at 1:49 . Would love something similar to the Soundworks Collection for picture post but idk if/how it 'd work . 43971 @qwx453971 <p> I know some sound effects editors , and sometimes with gunshots for major films , there might be combinations of five or six different simultaneous effects for one gunshot : one actual authentic gunshot recorded in that space , another gunshot mic 'd 100 feet away for reverb and ambience , a low-frequency whack for impact , a high-frequency " click " for the initial hammer , some kind of " whoosh " for general mids ... there 's lots going on . Same thing with body blows . I think nobody did it better ( no pun intended @ @ @ @ @ @ @ @ @ @ for face punches ... but that was n't intended to be realistic . <p> Yeah I learnt Pro Tools and did a little mixing in college , albeit more for music rather than film , but I still got a grasp of how much goes into it . Amazing what they do and how they do it . I 've wanted to ask an audio-specific person what made them want to go down that path , because I never for a second had the fascination with it or felt driven to become a killer boom op/sound recordist etc , but again I know they 'd turn it round and say the same thing to us and ask what 's so great about playing with colours all day long . <p> Yeah I learnt Pro Tools and did a little mixing in college , albeit more for music rather than film , but I still got a grasp of how much goes into it . Amazing what they do and how they do it . I 've wanted to ask an audio-specific person what made them want to go down that @ @ @ @ @ @ @ @ @ @ fascination with it or felt driven to become a killer boom op/sound recordist etc , but again I know they 'd turn it round and say the same thing to us and ask what 's so great about playing with colours all day long . 43971 @qwx453971 <p> Some things you just fall into . I came to LA to try to be a camera operator ( since I had done that for five years ) , and fell into being a colorist because it seemed like a good " temporary " job to try . More than 30 years later , it seems like that became my career instead . <p> I got into sound as a sideline only because I was fascinated by it and intrigued by film sound , plus I had done radio in my teens ( on the way to television ) . So all of it kind of fits together in a strange way . I only know enough about sound mixing to be dangerous , but I can do it to a point . <p> Cinematography is a whole different thing , and @ @ @ @ @ @ @ @ @ @ 're doing . I know enough about lighting , lenses , and filters to empathize with the people who use these tools and I try to come up with solutions for the problems they encounter on set . I love the idea of the Alexa 65 , but I think very few people will be able to put up with the budget and scheduling demands for a workflow with files that gigantic . Same thing with Red 8K . For huge budget studio pictures , you can solve all those problems . <p> My friend was the DIT on this . He has some amazing pics on instagram of the Alexa 65 in action . Helicopters , cranes , edge systems , snowcats and snowmobiles . I believe they had 2 of the 65 's working on it . <p> My friend was the DIT on this . He has some amazing pics on instagram of the Alexa 65 in action . Helicopters , cranes , edge systems , snowcats and snowmobiles . I believe they had 2 of the 65 's working on it . 43971 @qwx453971 <p> A @ @ @ @ @ @ @ @ @ @ ( shot in Atlanta with multiple Alexa 65 's ) , but I say nothing . All the Marvel pictures look beautiful to me , and I think they have the whole workflow thing down to a science . <p> I love the idea of the Alexa 65 , but I think very few people will be able to put up with the budget and scheduling demands for a workflow with files that gigantic . Same thing with Red 8K . For huge budget studio pictures , you can solve all those problems . <p> Or the SAME pixels . As the Alexa . Just more of them so you do n't lose dynamic range by shrinking the size of them . 43971 @qwx453971 <p> The way ARRI created this medium format sized sensor is pretty interesting and I believe not everybody knows how they achieved this . <p> It 's actually very simple and so smart at the same time . They 've taken three AMIRA sensors " same sensor/technology as the one from the ALEXA but with different physical measurements " and simply rotated them by 90- . @ @ @ @ @ @ @ @ @ @ the R&amp;D cost for creating a medium sized sensor capable of doing what the ALEXA 65 is capable of would have been so high that they essentially were forced to come up with different solution . <p> The way ARRI created this medium format sized sensor is pretty interesting and I believe not everybody knows how they achieved this . <p> It 's actually very simple and so smart at the same time . They 've taken three AMIRA sensors " same sensor/technology as the one from the ALEXA but with different physical measurements " and simply rotated them by 90- . I think it 's so clever . <p> I assume the R&amp;D cost for creating a medium sized sensor capable of doing what the ALEXA 65 is capable of would have been so high that they essentially were forced to come up with different solution . 
@@44332960 @4332960/ <h> GH4 v-log grading <p> Below are some frames from a short film project titled " Sal 's Auto " I 've been color grading in preparation for it 's world premiere at the Oakville Film Festival on June 24th 2017 . Any suburban residents to the Toronto area can check out the oakville film festival website offa.ca for more details . <p> This was shot on the Panasonic GH4 in v-log 4K and graded in DaVinci Resolve using a complete roundtrip workflow . I 've graded a lot of footage from various cameras over the years , including the F55 I 've now owned for over 3 years , and the footage from the Panasonic GH4 continues to impress me . This is the 2nd project shot with the GH4 that I 've worked on and I really love working with it . The latitude of the camera is quite huge and I find it very easy to get what you want from it in the color without any stress . <p> Despite coming up with various initial looks for the film that I thought looked cool ( @ @ @ @ @ @ @ @ @ @ blue hue ) for this drama set mostly in a garage location , the producer and director wanted a subtle look with less emphasis on the saturation of the location background and contrast to match the emotional tone of the story . With the footage from this camera I could really take the image anywhere I needed to . Again I 'm truly impressed with this camera . 
@@44332964 @4332964/ <h> " Furious 7 " In Laser Projection at the Chinese Theater <p> I just saw the movie at a SMPTE screening tonight at the TCL Chinese Hollywood theater in Laser 4K , and thought it looked fantastic . It was reportedly shot in 2.8K on Alexa and uprezzed , but it looked fine . Color was terrific -- there was some orange/teal in certain sequences , but it did n't bother me at all . The looks varied all over the place depending upon the location and intent of the scene , and I thought it was all very appropriate and really nice looking . ( Color correction was done by Tom Reiser at eFilm ; cinematographers were Marc Spicer and Stephen F. Windon , two DPs because of the movie 's extended schedule after the untimely death of star Paul Walker , to whom the film was dedicated . ) <p> The picture was phenomenally good , though -- hands-down the best digital projection I 've ever seen . Really deep blacks , super-bright whites ( they were quoting a projected white level of 22fL , which @ @ @ @ @ @ @ @ @ @ level of 14 fL ) , and detail and color were damn near flawless , sharp as a pin . I 'm told they 're using modified stacked Barco 4K projectors that are controlled in a way to allow sub-pixel alignment . We saw both 3D and flat images , and both looked fantastic . ( Some confused audience members thought this was a 3D presentation of Furious 7 , but when I saw the lack of 3D content in the first 5 minutes , I took off the glasses and enjoyed the movie without them . Out of the 900+ technical people in the audience , I 'd say 500 kept the glasses on . At the very end , the Imax host admitted it was not a 3D showing . Go figure . ) <p> I think the presentation could 've been better ; the Chinese Theater is still kind of shabby ( to me ) , and the seats are uncomfortable and not big enough . The whole place could use a coat of paint . I do n't doubt they spent many millions on rewiring the @ @ @ @ @ @ @ @ @ @ plus unknown thousands on a massive 90 ' x 45 ' screen , but jesus , can we get nicer chairs for an $18 premium-theater ticket ? <p> I did spot a whole bunch of CG Paul Walker shots , but they were generally very well done and I was watching very carefully for them . I already knew all the stuff in Dubai used doubles and CGI ( since the actor had died before those scenes were shot ) , and it was clear to me there was a lot of face replacement in the fight scenes and stunts . The final scene ( which I wo n't give away ) was also clearly face-replacement . One tip-off is they did n't always get the texture , lighting , and eye-lines right -- the CG Walker was looking a little bit in the wrong direction sometimes . I also heard some awful dialogue sound in the mix in the scene with the actor in the parking garage , where it was clear they could n't go in and loop it all and had to make do with the ( @ @ @ @ @ @ @ @ @ @ doubt if 99% of the audience would notice or care . <p> As great as the Imax Laser projected image was , the sound at the big Chinese Theater is still really crappy . Some things never change . It 's been bad for at least 35 years that I know of . Dialogue is muddy , there is n't enough high end for the center channel , and I think the surrounds were drowning out the fronts by at least 2-3 dB . Granted , some of this might be the mix , but this screening was supposed to promote the new Imax 12-channel " immersive " sound system , and if this is their flagship theater , they got some serious problems . <p> Oh , and the stunningly beautiful picture did not compensate for the thoroughly stupid plot and nonexistent acting . It 's a horrible , horrible movie . Technically , it was fascinating to watch , the stunts were incredible , and there 's more VFX in this thing than there were in the last three Transformers films . And maybe more edits , too @ @ @ @ @ @ @ @ @ @ to the 3rd power . But terrific from a technical point of view . <p> It 's a horrible , horrible movie . Technically , it was fascinating to watch , the stunts were incredible , and there 's more VFX in this thing than there were in the last three Transformers films . And maybe more edits , too . It 's an exhausting movie to watch . Dumb to the 3rd power . But terrific from a technical point of view . <p> And once again we miss each other at an event ! I had to leave after the Caucasses ( sp ? ) sequence . <p> Yes , stunning , bright &amp; clear . Curiously , while watching the few minutes of Stereo during the Intro , I was also impressed , but found myself saying " Yeah , it 's nice , but boy do I want to see that in HDR " . How quickly we get jaded . <p> We should have coordinated . I was there too . The Tomorrowland trailer was really good as well as the 8k scan of The @ @ @ @ @ @ @ @ @ @ good that you could pick some of the blowups by relative softness . The montage of the scenes from the previous 6 films at the end was a place to see digital vs. 35mm difference . The first of the films looked like it was shot on super 16 . I do n't know that it has , but the grain in all the film based shots was so large ! <p> Ha , ha . You can edit the scenes in just about any order and the plot would make as much sense . Who were those adversary operatives anyway ? International mercenaries led by an African warlord or something ? They could have at least painted out those double yellow lines on the mountain road that make the scene look more like British Columbia than Afghanistan . <p> Sometimes when I watch films I just tune out the plot and watch for picture and listen for sound . It 's a legacy from my days spent doing QC . <p> Ha , ha . You can edit the scenes in just about any order and the plot would @ @ @ @ @ @ @ @ @ @ anyway ? International mercenaries led by an African warlord or something ? They could have at least painted out those double yellow lines on the mountain road that make the scene look more like British Columbia than Afghanistan . 43971 @qwx453971 <p> The plot ( such as it is ) was bewildering : London to LA to Tokyo to Dubai to Afghanistan to I do n't know where . And half the time , I could n't figure out who the hell was fighting in the fight scenes . I love fast editing done well , but at some point it gets very confusing . <p> Speaking purely from the results , I was very impressed with what I see , and though the projectors are initially more expensive , I think theater owners will save money in the long run by not having to replace the light bulbs every month or so . And if it almost doubles the light output for 3D , it 'll make it tolerable for a lot more people . <p> But Dolby Vision and Rec2020 are going to be very , very hard @ @ @ @ @ @ @ @ @ @ we saw at the screening was on the verge of being too bright to me ... and Dolby Vision is theoretically possible of being 10 times as bright as a conventional display . Of course , that does n't mean the cinematographer and director necessarily have to use this dynamic range , but it worries me that these could reach retina-searing levels . 
@@44332965 @4332965/ <h> Resolve 10 Rec.709 feature to DCP <p> We have graded a 2K Scope feature with a Rec.709 monitor with Resolve 10 and we are currently looking at options for mastering to DCP and we are looking for insight and assistance through this process . <p> We are working from R3D footage with an original timecode base of 25FPS and the Resolve project is setup for 25FPS , however will need to change to 24FPS for the DCP . We already have audio prepared at 24FPS . <p> We are considering two software options , OpenDCP , or purchase the EasyDCP software for Resolve 10 . <p> If we were to go the OpenDCP route , I would appreciate advice on our suggested workflow : <p> - Export 2K DPX sequence from Resolve 10 - Use OpenDCP to create OpenJPEG sequence from the DPX sequence and perform both the XYZ color conversion and conversion of Rec.709 colorspace to DCI compliant colorspace . We assume this is done by selecting the source color as Rec.709 and selecting XYZ Transform - however unsure as to whether DPX Logarithmic would need to @ @ @ @ @ @ @ @ @ @ and video MXF files and DCP using the remainder of the OpenDCP process <p> Alternatively with the OpenDCP workflow , is it recommended that the Rec.709 conversion is done with by using the Rec.709 to DCDM LUT in Resolve and then export a Tiff sequence with the XYZ conversion through the Resolve output module , then only do a TIFF to OpenJPEG conversion in OpenDCP selecting the source colour as sRGB ? <p> I am unsure of the EasyDCP workflow process , but I have a few questions I hope can be answered : <p> - Will using a commercial package like EasyDCP impact on the quality of the DCP we deliver ( ie is the color conversion from Rec.709 to be DCI Compliant better ) ? I am interested in any advantages to using a package like EasyDCP over OpenDCP , aside from the ability to create encrypted DCP 's with the additional Encryption software ? - Using EasyDCP software for Resolve 10 , can it create a 24FPS DCP from a 25FPS Resolve Project , or would we still need to render the project out as a DPX @ @ @ @ @ @ @ @ @ @ the 24FPS audio and create the DCP from there ? <p> Finally my last question is with regards to creating a DCDM for final delivery . Would it be recommended that we do the Rec.709 to DCDM then to XYZ conversion entirely within Resolve ? <p> I have n't used EasyDCP or OpenDCP in production , but during our tests of Resolve EasyDCP plugin you can get correct gamma , if you set your Timeline Colorspace correctly in Project Settings -&gt; Lookup Tables -&gt; eacyDCP/JPEG200 Color Management . Set correct gamma to either REC709 2.4 or REC709 2.2 as your monitor is calibrated , then you will get the correct XYZ conversion . <p> I could n't get the correct gamma with DCDM export ( TIFF with XYZ ) as TIFF XYZ in Output module seems to be using P3 color space . I think if you can custom build you LUT , like with Lightillusion Lightspace CMS , you can add a " Track Node " with that custom LUT and render the TIFF/DPX files with correct XYZ color space . 
@@44332966 @4332966/ <h> Oled in 2017 ? <p> I am using the Eizo CG277 as my reference monitor and I am thinking of upgrading to an Oled monitor . I really love the image of an Oled . My main concern though is which one to choose ? The BVM-X300 is way too expensive for me although I love using it as a freelancer . So I was between the FSI 25 ' and the Sony PVM-A250 . These two monitors came out in 2014 and I am not willing to pay full price for them . Three years in our world is quite a bit . Are there any new monitors coming out this year ? There is a Dell Ultrasharp 4K Oled 30 ' 10bit coming out in two weeks for 4.000 but ... I would n't trust it for a reference monitor . On the other hand , this is a tricky one , the Panasonic TX-65EZ1002 is coming out in June . Would a top end TV display be good to use as a reference monitor ? I am not doing any HDR work only grading in @ @ @ @ @ @ @ @ @ @ show , but ... Mentioned panasonic and dell will be great machines and piece of technology . But they are not meant to be " reference monitors " I am afraid . First thing you need as a colorist is a monitor you can trust . Not the software , not the chair Top end TV display was never good to use as a reference monitor . Panasonic 65VT60 plazma was not even good as a client monitor ( although it was the best on consumer market ) . <p> If HDR and UHD is not your target , there is no reason not to buy DM250 ( at this moment before NAB ) . If you can afford one . Their great calibration service and " zero " latency mode was the reason we have chosen it over the Sony one . <p> Sometimes the friendly guys from Flanders pre-sale the Monitors they are about to display on shows like NAB . These Monitors will have a few hours on them , but are sold with a little price reduction . Get in touch with them to see if @ @ @ @ @ @ @ @ @ @ Sometimes the friendly guys from Flanders pre-sale the Monitors they are about to display on shows like NAB . These Monitors will have a few hours on them , but are sold with a little price reduction . Get in touch with them to see if this holds true for this year as well . <p> Sent from my iPhone using Tapatalk 43971 @qwx453971 <p> That would be my advise also , get in touch with FSI . They often have b-stock at the IBC as well . <p> One of my clients installed a pvm-a250 and I really enjoy working on it , too . Perhaps there are 2nd hand offers somewhere ? <p> I 've looked for second hand offers but I have n't found anything yet ... I ve thought of the FSI demo monitors of the NAB but I live in France ... I hope new monitors will be out in the market after the NAB <p> The reality is there are two fabs making large panel OLEDs . It 's not like LCDs which need constant refinement to be worthwhile . The larger form OLEDs @ @ @ @ @ @ @ @ @ @ 25 , 30 inch 4K variants are Sony RGB OLED . Not sure about Dell ... suspect LG . <p> The components of the display that drive the screen are immensely important . <p> In a choice between A250 and FSI , I 'd go FSI . But no sense in not waiting two weeks . <p> Nope , bought my AM420 at NAB at a nice discount and they were happy to ship it to Amsterdam , via Belgium for a nice recalibration . With invoice . 43971 @qwx453971 <p> Good to hear that . When i ask FSI two or three yearse ago about post NAB monitors they told discount is only for north america . Maybe because they do n't have Belgium department in that moment . Few months later it was not a problem . They open Belgium department and IBC start . <p> The reality is there are two fabs making large panel OLEDs . It 's not like LCDs which need constant refinement to be worthwhile . The larger form OLEDs tend to be LG WOLED panels . The 17 &amp; 25 , 30 @ @ @ @ @ @ @ @ @ @ about Dell ... suspect LG . <p> The components of the display that drive the screen are immensely important . <p> I 'm a choice between A250 and FSI . I 'd go FSI . But no sense in not waiting two weeks . 43971 @qwx453971 <p> According to a few industry sources , the Dell 30 " OLED monitor uses Samsung panel . LG currently do not produce OLED panels under 55 " . <p> can burn-in be a substiantial issue ? i 've seen a nearly new B7 with a giant burn smack in the center of the raster .. ouch .... 43971 @qwx453971 <p> Like a 2.39 letterbox ? That 's happened to us too , on all of our OLEDs . We 've usually been able to remove it , but it actually happens even faster than it did on plasma . Just having gradient up for a while will even do it . It 's probably one of those things that will get better as the technology matures ( like plasma half-life ) , but for now you really have to keep an eye on @ @ @ @ @ @ @ @ @ @ happened to us too , on all of our OLEDs . We 've usually been able to remove it , but it actually happens even faster than it did on plasma. 43971 @qwx453971 <p> nope , it 's center frame maybe 30-40% of the raster , kinda makes it a doorstop for now , hopefully it gets better as time goes by <p> was told it happened when an engineer was evaling HDR ... left a frame up while grabbing a coffee , this facility has a espresso machine that costs ( far ) more than the B7 did , so what ever time it took to walk down the hall , pull a shot and walk back .... <p> i 'd approach any offers of a great deal on used / refurbed / rebox 'd OLED of any brand with great caution ; - ) 
@@44332967 @4332967/ <p> i have been having more time to play with these new features and it would seem to me that Face refine/ soften and sharpen + sharpen edges ( with a touch of LAB ) all working together you can pull of some pretty good results very quickly <p> so base balance to start with <p> then using tracks on the skin patches with soften and sharpen changing the texture detail LAB blur on the colour channels to remove any small blotchy ness also negative colour boost <p> Face refine for just eyes /eye bags /eye light and lips <p> the a subtle use of sharpen edges on the whole thing <p> personally i 'm not a fan of hyper beauty .... removing pretty much every feature.so to me they look unnatural which is what you normally see on make up advert no ones skin is that smooth ... i know it big business <p> but sometimes you just need to do some clean up as we are all shooting a 9 TRILLION K resolution these day even the make up can , t hold up to that in @ @ @ @ @ @ @ @ @ @ HD first came out SD was so forgiving on actors faces ... every one looked amazing ... but very low rez <p> quick before and after images below <p> using the method above , all very quick about 5 mins to get something that pretty workable ( i would go off do another shot or have some tea then come back to this to tweak ) <p> and as i , m only using part of Face refine which is the tracking bit i can still work in ACEScct with no i 'll effects <p> on the after jpeg all the actual detail has vanished from the skin but on my monitor you can still just see the skin pores correctly ( should have dumped a load of grain on it ) <p> just looking at it now i need to knock the face back bit to much light ... i , m tweak already <p> all in all i , m pretty happy with these new toys however ... i 'm so tempted by that planar tracker in BCC .... h , mmmm <p> In tv I almost never get @ @ @ @ @ @ @ @ @ @ I think it 's a bit cultural . U.S. Tv shows look really different from the ones here in the Netherlands and some of that has to do with the amount of real make up and post beauty work . It 's quite interesting . I even once met an american girl who watched tv next to me and said : ' wow , the women look like real women on tv here ' . <p> For me personally , it 's hard to watch a CNN interview , where the female host 's skin has been softened . It makes it look ' fake ' which is not a good thing for a news orientated show . But again , I 'm culturaly biased . <p> We can not contest the choices to keep or not the pores of the skin but I confess that this pretty face deserves a little more softness , even a smooth skin so much the person seems fragile or asks to be " idealized " ... ( Ok I leave in full delirium ) . <p> It 's funny because I 'm the opposite @ @ @ @ @ @ @ @ @ @ ext. , I think it 's so unnatural . Like looking at someone from a foot away . I think subtle softening makes people look like they do In real life . I remember seeing a friend appear on a TV show - and this was back in standard def days . Even with a light makeup he had dark spots on his face , which I never noticed before . The next time I saw him I looked and the spots were slight and lighter - nothing I ever noticed . <p> For me personally , it 's hard to watch a CNN interview , where the female host 's skin has been softened . It makes it look ' fake ' which is not a good thing for a news orientated show . But again , I 'm culturaly biased . 43971 @qwx453971 <p> Ellen DeGeneres has skin-defocus applied live during her shows . But they also have very , very diffuse lighting on that set . I think people just accept that this is her look -- it does n't bother me . <p> There were jokes back @ @ @ @ @ @ @ @ @ @ being used on Barbara Walters ' ABC interviews , to the extent that Saturday Night Live would routinely show Walters half out of focus in their parodies . So this issue goes back a long time . At least now , only the skin is defocused and everything else is sharp . 
@@44332968 @4332968/ <h> Sony F55 and FS7 underexposed issues ... <p> I 've graded enough projects now with these two cameras to have my own opinion but would love to hear about other peoples experiences and solves or success stories . <p> Consistently I am seeing really horrendous noise from these cameras when used doc-style , with blue and red channel noise so bad that it almost looks like RF interference . <p> It 's predominately SLog3 material recorded XAVC ( interframe ) ... but way under-exposed IMHO ... with mid-tone detail down around or sometimes below 30% . Metadata in Sony 's software shows it 's setup for ISO 1200 with no adjustment from there or sometimes pushed hotter . But the lighting environments I am seeing do n't seem to call for that ( more like 320-500 range ) like daylight exteriors . <p> It makes me wonder if this is just a case of exposing for the shadows while in Log or the wrong kind of viewfinder LUT , over use of the ISO as a tool for exposing ( all user issues ) or do these cameras @ @ @ @ @ @ @ @ @ @ just fail to record enough dynamic range that it 's forcing shooters to close way down to protect highlights too aggressively ? <p> Look at the histograms -- those will generally tell you the truth of how the image is exposed . I think so many DPs are terrified of clipping the highlights , they go too far the other way and wind up underexposing everything by a stop . And they also generally need to use more fill light , but that 's me . <p> Great camera if you do n't rate for native 1250 iso . 800 or less should be pretty clean . Also be careful of fixed pattern noise when using higher frame rates . I think the camera needs to be black balanced specifically for the frame rate needed . I believe the IR filter is bypassed at higher rates . <p> Good responses here and they track with what I 've seen . It is a great camera for ideal lighting setups . <p> Project I am on now has histograms pretty consistently in the 20-30% range and skin tone seems to be @ @ @ @ @ @ @ @ @ @ shadow detail is . It 's partially because the DP wanted to always have shallow DoF so he 's ND 's and opening up and losing at least a stop unnecessarily but it 's also largely because of the nature of the environments which are largely uncontrolled . <p> IMHO ... it 's a poor documentary camera and I 'd rather pull images out of the mud from a RED or Alexa , even a C300 . If you 're limited to available light scenarios and shooting your subjects often in shadow or highly contrasting environments where you need shadows its the wrong tool for the job . <p> A lot of people complain about the noise floor of these cameras , but they 're all wrong , and are somehow inadvertently underexposing the cameras constantly ( I blame lack of light metering discipline ) . I 've shot a lot with the F55 , F5 and FS7 , and exposed correctly they 're incredibly clean and low-noise . Cleaner than an Alexa at 800 ISO in my experience . <p> The problem is Sony lied about the native sensitivity @ @ @ @ @ @ @ @ @ @ since the F3 , F35 and F65 all seem to ring true to their rated sensitivities ) , but their latest cameras simply do n't . <p> And each time I 've had an F55 or FS7 on a shoot , I run the same basic test to be sure that I can meter accurately on set . They all display the same basic trait of being approximately one stop slower than they are officially listed as . <p> F55has significant head room , but the floor is terrible . I always tell people to shoot it thick . Expose for 800 not 1250. 43971 @qwx453971 <p> Agree . <p> I own and shoot FS7 in Slog3 for 80% of the commercials I do . <p> The camera is 2000iso native , but should be set up to monitor at 1000iso for most shooting . We basically have three settings ... 1000/2000/4000 . Also , if folks are shooting to the internal cards I would recommend setting the internal noise reduction to low . I 've tested this a lot and there seems to be some very fine voodoo inside @ @ @ @ @ @ @ @ @ @ compression is applied . You can get a very clean look and no discernible downside . <p> Bottom line is that most of the issues I 've seen and heard about with FS5 , 7 , or 55 are due to DP failure . They must completely understand and test the camera . If they do n't it will bite back quite a bit . Its actually a very good camera for documentary setups and especially great when combined with a speed booster for a full frame look . <p> Just shot an entire film on the fs7 in Full Frame ( speed booster ) with these settings . Its very clean . Mostly we lit for 1000iso , but there were 2000 and 4000 moments in there that are also exceptionally nice when we planned accordingly . <p> Just shot an entire film on the fs7 in Full Frame ( speed booster ) with these settings . Its very clean . Mostly we lit for 1000iso , but there were 2000 and 4000 moments in there that are also exceptionally nice when we planned accordingly . 43971 @qwx453971 <p> @ @ @ @ @ @ @ @ @ @ they look really nice and should color-correct just fine . Glad to see Steven " Ned Ryerson " Tobolosky working -- he 's an amazing actor and a very funny guy . <p> Yes . The first two are pretty much right out of the camera . It was all shot with the internal cards in Slog3 . The last shot of Tobo is with the basic LUT applied and nothing else ... essentially an example of what I was looking at on set as we shot . <p> Stephen is a gracious man and an extraordinary actor . He and DJ Qualls really made the film sing . <p> I do n't know about SLog , but I do know with non SLog 4K 100Mbs XAVC-S footage out of the a6300 , the camera records into the superwhites but not into the superblacks . So on a 1 to 255 8-bit scale , the camera is recording 16 to 255 , AFAICT . This means you have tell your NLE or color correction program to use the full range of the scale . In Resolve this is done by @ @ @ @ @ @ @ @ @ @ clip ... Clip Attributes ... Video ... Data Levels Full . <p> If this is n't done , the highlights will be prematurely clipped , and the shadows may look crushed . <p> I do n't know if SLog3 records into the superblacks as well , i.e. 1 to 15 , but if it does , then you even more want to choose Full levels . As the black will go from looking oddly crushed to being prematurely clipped as well . <p> I do n't know about SLog , but I do know with non SLog 4K 100Mbs XAVC-S footage out of the a6300 , the camera records into the superwhites but not into the superblacks . So on a 1 to 255 8-bit scale , the camera is recording 16 to 255 , AFAICT . This means you have tell your NLE or color correction program to use the full range of the scale . In Resolve this is done by going to the Media Page , right clicking on the clip ... Clip Attributes ... Video ... Data Levels Full . 43971 @qwx453971 <p> That 's a @ @ @ @ @ @ @ @ @ @ and neither in Premiere . <p> Resolve and Premiere does not actually clip the data you could simply lower the gain to bring it back . As always be careful applying LUTs or effects with out of range data because they could clip . <p> I do n't know if SLog3 records into the superblacks as well , i.e. 1 to 15 , but if it does , then you even more want to choose Full levels . As the black will go from looking oddly crushed to being prematurely clipped as well . 43971 @qwx453971 <p> Slog3 requires full range . While Slog3 maxes out at 109IRE you would never get a negative IRE at the bottom . <p> Also note that Slog3 uses a different gamut than Rec709 , the primaries are different . <p> I do n't think that is true . A clip can use video levels while still using WTW as overflow . 43971 @qwx453971 <p> Cary , if a camera is recording to the WTW 's , then it 's not confining itself to video levels . I see how that can be characterized @ @ @ @ @ @ @ @ @ @ from temporary transients that are allowed in broadcast to allow for short lived specular highlight . <p> Practically speaking , there are two choices to recover the highlights in the situation we 're discussing . <p> 1 . Interpret the file as having video levels and then lowering the gain to " recover " the highlights . This works in some programs but not all and not always . For example , most of the optimized media formats in Resolve will not allow you to recover the WTW area . <p> 2 . Interpret the file as having full levels and if the shadows look lifted ( which they may or may not depending on how the program and display handle superblacks ) lowering the lift to lower the shadows . <p> If data is misinterpreted for video you might miss some values that you can easily bring back using LGG controls . Nothing is being wasted . <p> SLog3 is not using values in the highest areas of the codec per se . Most SLog3 cameras do n't do more than 13 stops . This means usually -8 and @ @ @ @ @ @ @ @ @ @ you can see that it will clip somewhere near 780 . Most cameras even clip before . <p> Eirk , of course there are ways to bring back the superwhites ( and superblacks ) . I mentioned two of them in the post above . But it depends on a lot of things . My personal preference is to use full levels if the camera records outside of the video range , because I want to see all of the information of the image w/o having to apply a grade ( which may or may not work ) . <p> Back to the original concern of the post which is that SLog3 is looking underexposed . If you interpret it as video instead of full , the shadows may look crushed ... depending on a lot of factors . <p> I just want to add to this conversation that I have the added benefit of knowing how the footage I 'm discussing here should look . That 's because myself and two other people shot it . <p> So I know how the footage looked on the back of the @ @ @ @ @ @ @ @ @ @ where the Zebras were set ( 100% ) and that the false color on the external monitors was also followed to expose to just below clipping . So from the perspective of myself and two other camera operators/DP 's there should be no clipping . <p> The only way to get the footage to look like we shot it right off the bat , is to bring it in on the Media Page using Full levels in Resolve ; not Auto and not Video . <p> I could bring it in as Auto or Video and lower the gain to recover the highlights , but then I ca n't use DNxHR SQ for a proxy format for optimized media , because for most optimized media formats , once the highlights are clipped prematurely , there is no recovery . Which means I would have to edit Long GOP , which I really do n't want do . <p> So from my perspective , for the reasons stated above , if you want non SLog XAVC-S 4K 100Mbs footage to look like it was shot , choose Full levels . SLog @ @ @ @ @ @ @ @ @ @ to deal with , and who knows what assumptions the LUT makes about the footage . <p> SLog becomes a bit more complicated because you have a LUT to deal with , and who knows what assumptions the LUT makes about the footage. 43971 @qwx453971 <p> If you use a LUT to delog in Resolve , which I would not recommend , then you should know if the LUT you use makes any assumptions wrt levels and gamut , otherwise it is like spinning the wheels in roulette ! <p> he only way to get the footage to look like we shot it right off the bat , is to bring it in on the Media Page using Full levels in Resolve ; not Auto and not Video . 43971 @qwx453971 <p> Yes , I have seen ( rare ) cases where Resolve misinterprets the metadata and applies the wrong levels . I once got a film scan that for some reason kept getting flagged as Video when it was really Full . I think I got about minutes in before I said , " uh-oh ... this ai n't @ @ @ @ @ @ @ @ @ @ Attributes out of Auto , it looked infinitely better and was far easier to grade . <p> If you use a LUT to delog in Resolve , which I would not recommend , then you should know if the LUT you use makes any assumptions wrt levels and gamut , otherwise it is like spinning the wheels in roulette ! 43971 @qwx453971 <p> The wrong LUT with the wrong footage at the wrong settings can absolutely be disastrous . 
@@44332969 @4332969/ <h> Premiere/Resolve Lite RED 4K issues . <p> Just finished up grading a student short . Was shot on a RED predominantly in 4k but in 3k for some slow motion shots . Delivery is at 3k , cropped to 4:3 . <p> Director insisted I grade using the . R3D files and an . xml which I would have rather not done but did n't put up a fight as I did n't think there would be any problems ( ha ) . Resolve had a rough time of interpreting the Premiere . xml with all the resizing due to rescaling the 4k shots and a lot of panning and scanning reframing going on with the 4:3 crop , so I just turned off all the resizing info off and graded it all at its native format , thinking I could just render everything out at the end with each shot 's respective native resolution and relink it all in Premiere . <p> However , whilst the 3K ( 3160 x 1620 ) footage is coming out and relinking fine ; I forgot that Resolve Lite maxes out @ @ @ @ @ @ @ @ @ @ was shot in ( 4096 x 2160 ) . So all my 4K files are really UHD with a black border - http : **25;1687;TOOLONG - which is n't good . <p> So obviously I 've messed up , and I know for next time to insist on grading using ProRes transcodes if it 's a complicated EDL , but I do n't mind because I 'm treating it as a learning experience and I 'm sure there 's a solution ( short of buying the full version of Resolve ) ; just not sure what that is . <p> I 'm thinking maybe I re-export out of Premiere as ProRes 4444s with an EDL , all in 3K , and I think I 've read there would be a way for Resolve to copy over my grades from my timeline with the . R3D footage to these ? But I do n't know if a ) that 's actually possible or b ) the grades would stay the same . <p> Worst comes to the worst I export everything as it is now in 3K and rescale/position it all @ @ @ @ @ @ @ @ @ @ there 's an alternative . <p> Resolve had a rough time of interpreting the Premiere . xml with all the resizing due to rescaling the 4k shots and a lot of panning and scanning reframing going on with the 4:3 crop , so I just turned off all the resizing info off and graded it all at its native format , thinking I could just render everything out at the end with each shot 's respective native resolution and relink it all in Premiere . 43971 @qwx453971 <p> I just had a similar situation with 5K files , and I wound up just resizing every single shot by hand . The conform and reframing took one solid day , which under the circumstances was not horrible ; I 've had worse . I have learned through painful sad experience that conforms can be 75% more trouble than the color-correction . But it 's part of the job . <p> I 'm thinking maybe I re-export out of Premiere as ProRes 4444s with an EDL , all in 3K , and I think I 've read there would be a way @ @ @ @ @ @ @ @ @ @ with the . R3D footage to these ? But I do n't know if a ) that 's actually possible or b ) the grades would stay the same . 43971 @qwx453971 <p> The ProRes grades would only stay the same if you decode the files as RedColor3 / RedLogFilm and make no adjustments within the Camera Raw settings . You could always keep a set of stills from the R3Ds and use them as comparison when you 're grading/trimming the ProRes files . <p> Thanks Marc . I always tell people that grading 's the easy part ; it 's getting it all into Resolve in the first place that 's the problem . <p> I ended up exporting the entire 15 minute film out of Premiere in ProRes 4444 , used scene cut detection to chop it up and then just copied over the grades over manually , having made sure the few Camera Raw adjustments I 'd applied in Resolve to select shots were applied in Premiere too . Probably not the most efficient way of doing things but only took an hour or so and it @ @ @ @ @ @ @ @ @ @ Know for next time . <p> We 've been having problems with Premiere -&gt; Resolve too , usually solve it in the same clunky way ( ProRes4444 export , scene detect ) which is a mess , but works for commercial/short work in a pinch . 43971 @qwx453971 <p> What 's the equivalent of ProRes 4444 for someone using Premiere to export as on Windows ? I 'm about to start work on another project that again using an . xml is n't an option so what should I ask them to encode to for me to bring into Resolve ? <p> I 'm on a Mac so when looking through the list of codecs I do n't know which ones ( apart from ProRes obviously ) I 'll have that they wo n't . Project 's at 2k if that makes any difference . <p> I forgot that Resolve Lite maxes out at UHD and not the native 4k that the RED was shot in ( 4096 x 2160 ) . So all my 4K files are really UHD with a black border which is n't good . 43971 @ @ @ @ @ @ @ @ @ @ on a feature film documentary with a wide mix of cameras and frame sizes . I got some RED shots at 5120 X 2700 pixels and I am able to grade them in Resolve 11.1 Lite and output them at the same size . From the delivery page I just used the Individual clip output with Source frame size resolution selected . <p> Thanks Marc . I always tell people that grading 's the easy part ; it 's getting it all into Resolve in the first place that 's the problem . <p> I ended up exporting the entire 15 minute film out of Premiere in ProRes 4444 , used scene cut detection to chop it up and then just copied over the grades over manually , having made sure the few Camera Raw adjustments I 'd applied in Resolve to select shots were applied in Premiere too . Probably not the most efficient way of doing things but only took an hour or so and it worked so just happy to have it finished with . Know for next time . 43971 @qwx453971 <p> A great way to @ @ @ @ @ @ @ @ @ @ consolidate and transcode option in the project manager . This is new in the CC release from Monday . I would recommend using Cineform as well . Otherwise , in the past , I have had to conform RED from Premiere and the issue is the way that Premiere manages sizing and the way Resolve does . Resolve auto conforms all clips to the size of the sequence and then creates adjustments from there . Premiere can be defaulted to auto adjust , but the resizing is part of the sizing slider . This results in Resolve going crazy and resizing those clips on top of it 's auto adjust . <p> What 's the equivalent of ProRes 4444 for someone using Premiere to export as on Windows ? I 'm about to start work on another project that again using an . xml is n't an option so what should I ask them to encode to for me to bring into Resolve ? <p> I 'm on a Mac so when looking through the list of codecs I do n't know which ones ( apart from ProRes obviously ) @ @ @ @ @ @ @ @ @ @ at 2k if that makes any difference . 
@@44332970 @4332970/ <h> OT : Alert ! ! Large worldwide ransomeware targeting windows system <p> You edited your last post . Yeah , I agree we 're talking about 2 different things . For average Joe blow user like me , not having auto update on is playing with fire , as it is effectively all I really have . <p> Yes , I had problems twice . First there was the auto update of the nvidia driver that corrupted all my rendered media over the span of a few days and forced me to re-render everything . <p> Another time the update broke a software license ( Lightspace I believe ) . <p> But that 's not to say Im against updating or whatever . I just wished I could prefer certain updates over others and have a bit more control . Of course I want security updates asap but nvidia drivers are kind of sensible on any workstation , especially with a few apps installed that rely in them . <p> So what is your advice for my WIN partition on my Hack then ? I 'm a little @ @ @ @ @ @ @ @ @ @ failed the last time I booted the WIN partition . <p> Should I : 1 . disconnect from LAN 2. close the above mentioned ports 3. connect to internet again and try to update <p> ? ? 43971 @qwx453971 <p> If you want to go total secure , The safest way would actualy be if you could download the patches separately using your mac/hack ) , copy them to your windows partition , make sure your machine has no acces to any shared storage , then disconnect your machine from the net ( wifi/etehrnet ) only then boot in your windows partition on your machine and apply the patches manualy . Only then reconnect to the net and do an auto update for remaining , other non related patches . <p> The key is no access to any important storage as we do not know yet if there are already adjusted v2/v3 versions in the wild that bypass some known blcoking mechanismes . Or if your machine may even already be infected . You donwt want it to start encrypting your shared storage on bootup now do you ; - @ @ @ @ @ @ @ @ @ @ bit more details as without it people could falsely assume they are safe . Firewalls are often confused for what they do , mainly for the direction ( inbound/outbound ) and location ( on server/pc , behind router , infront of router etc ) Generaly we can divide the traffic in 2 groups based on location and 2 based on direction . - Internet vs Intranet ( so the big bad outside world vs your local network ) - Inbound traffic or outbound traffic . <p> For the average Joe and let 's stick to that ( as the non average know about this and this post would be endless ) , pure inbound hacks from the internet are very rare and most people are behind an internet modem that default does not route traffic " INITIATED " from the internet to your local machines . That is something ( called NATTING or port forwarding ) that you manualy have to do in your router . So it basically acts as an all blocking inbound firewall . If you stick with that basic setup you should not need a special inbound @ @ @ @ @ @ @ @ @ @ net , eg if you host stuff like a website etc etc . That is the moment where you need inbound firewall rules etc etc and you step into a different world . Also of course when you are not behind such a standard internet modem . <p> For outbound traffic the story is the reverse and all traffic from any box in your local intranet is default passed on to the internet . And that is where worm , spyware , ransomeware etc gets you . They come i via an email , a link , a file and once in , they setup/initiate a connecttion to the outside world and as they are the initiator , it can become a 2 way connection and even a difficult to detect 2 way connection ( vpn tunnel ) ! ! ! A bit like when you ask for a website with your browser ( the initiator ) and then get data back to you from the net , routed back to your pc and then web browser . <p> So the first thing you need to do is install an @ @ @ @ @ @ @ @ @ @ or at least detect any outbound traffic from your pc/mac . The best are the once that after install block EVERY traffic and ask you with irritating popups if you will allow a certain detected traffic to pass on . It tells you which process wants to talk and what destination server/port . Based on that you can say yes / no , only now or forever and even very specific to allow/block only to a certain domain/port or much wider . Very handy for programs that register with their maker , which you allow , but do not want these to talk to other locations ( eg when they get hyijacked ) Total control . Downside is that its a lot of hassle in the beginning but once settled you can easily spot any shitty spyware that somehow has crept in , when it tries to contact the internet . The inbound traffic firewall features of software you install then also comes into play . Also if you have for example an infected pc that tries to contact another pc in your own network . The problem with the @ @ @ @ @ @ @ @ @ @ within your local network so a very difficult subject . ( eg you would have likely allowed SMB traffic between your pc 's , so would still have been exposed at that point ) <p> For Mac a good example of a very reliable tool for outbound traffic inspection/blocking with lot of flexibility in setting up all kinds of rules by hand if you need to is Little Snitch . Who does exactly what the name implies . Snitch ! ! ! For windows i can not give tips as been out of that world for a while now and its a fast changing world , but there are definatly similar tools on the market , but they have to have complete control over all network traffic otherwise they are useless if the operating system can allow stuff to bypass the tool , which i have seen in the past on some windows based firewall solutions . So investigate . <p> What 's with the internal OS Firewall ? Mine is deactivated by default , as my router does this already . But I can not configure anything here . @ @ @ @ @ @ @ @ @ @ 43971 @qwx453971 <p> Assuming you talk about your Hackintosh , get Little Snitch ( read my long other post ) as is cheap and for Mac the top of the line . A firewall that you can not adjust is not a firewall as you have no idea what it blocks and what it considers safe to pass . It also has an excellent real time monitor where you see who talks to who at any time on your computer . If its windows , switch it on for now and get a good one ( ask around ) to replace it <p> ps not related to this hack but a very serious attack vector recently : Check your internet router firmware and make sure that one is always up to date , " never " have remote ( WAN ) management on ( only allow admin access from internal net ) , and let 's not talk about a default password ! ! ! ! ! ! ! ! ! <p> The most cautious : Never open a link directly in its mail software : go through pooper If the @ @ @ @ @ @ @ @ @ @ If the message comes from a friend : it will recall " you have not got my mail ? " <p> If you have an attachment : pass it for analysis to an anti-virus . <p> The best protection : it 's yourselves : do not be tempted by clicks on unknown links <p> If you are working in a business or network : follow the instructions of the network administrator scrupulously. 
@@44332971 @4332971/ <h> Real world limitations with 5.1 Mac Pro Sata II bays <p> I 'm in the process of renovating a 5.1 Mac Pro . It 's been upgraded to dual X5690 3.46GHz and will get one of the new SSD blades on a PCIe adapter for some nice 1450MB/s read/write performance . <p> I 'm painfully aware of the Sata II limitations of the internal bays , but lately I 'm wondering what the real life implications are . Installing modern SSDs here will saturate the interface at around 270MB/s . RAID 0:ing two of these should get me pretty sustainable 500MB/s performance . <p> I read a document here where the old Mac Pro ( like mine ) had been compared to the new Mac Pro , specifically looking at graphics performance . But in a side note , I think it was mentioned that the draw from the hard drives was n't that bad . <p> Can someone in the know share their thoughts on the limits of SSDs ( in terms of Resolve performance and footage I can use ) in Sata II ? As single @ @ @ @ @ @ @ @ @ @ stock 5.1 but i have os on 2x ssd 's and media on 2x 4 tb drives in stripe . Striping os SSD drives does not seem to give super hi speeds but gives good response time still . System is snappy . <p> As we have cache option in Resolve that writes to SSD i feel like i can do pretty much anything . <p> But still i consider about building faster PC machine as it has faster BUS side and i can hook up more drives with less effort . <p> I already had a 5.1 3.33GHz but as a project I 'm building a 4.1 dual CPU that 's been flashed to 5.1 with dual X5690s , this SSD and two GPUs that I 'm looking into . I 'll mod the internal PSU for the gfx cards . Price is $680 for Mac Pro , $700 for the SSD ( funny " more expensive ) , $400 total for both CPUs and new $90 Wireless card ( AC ) with latest Bluetooth for handoff and stuff like that . A single 280X is , I do @ @ @ @ @ @ @ @ @ @ ( SSUBX " 1TB ) is the exact same I picked up . Seeing that price now , almost makes me feel good about getting mine for $700 . <p> To install that into a 4.1/5.1 Mac Pro you 'd need a PCI adaptor ( they 're cheap at around $13 , since it 's only an adaptor ) . <p> Too bad they 're getting more expensive and not cheaper though ! Maybe you can find a better price if you look around ? But it should be the SSUBX and not the earlier SSUAX . But still ... a SSUAX for half the price might be a better deal ... <p> Do n't you run into any PCIe limitations running a 1400 MB/s blade and two GPU 's over it ? 43971 @qwx453971 <p> The two lower slots are both x16 slots ( in 4.1/5.1 ) and that 's where the gfx would go . The two top slots are both x4 and would share bandwidth , but if you have two gfx cards you 're going to lose slot 3 anyway , because gfx card in slot @ @ @ @ @ @ @ @ @ @ go dual gfx I will lose my USB3 PCIe card = ( <p> In my case , the SSUBX sits in slot 4 ( at the top ) and that is where I got the advertised speeds in Blackmagic 's benchmark app . <p> I still have my R9 280X because I was waiting to see where the chips fall with AMD 's new Fury before buying . For my personal needs I think I 've decided on a single GTX 980 Ti for my system , unless I find a nicely priced Titan X. <p> And in light of the pains and woes of our Russian friend with 3 Titan X in his 8-core 5.1 , I wo n't do any extreme upgrades . I realize some of you guys run extremely beefy rigs , but I 'm sticking to reasonable price/performance . <p> And on a side note I can say that I have modded my Mac Pro 's PSU to provide additional 8-pin and 6-pin power . I could throw almost any two cards in there . <p> Let 's see what El Capitan and Metal really @ @ @ @ @ @ @ @ @ @ using two gfx cards when I finally retire my Mac Pro , but I think I 'll just pick up one 980 or Titan for now . 
@@44332972 @4332972/ <h> R&amp;S Clipster 6 System <p> What is the benefit of this system if I already have Alchemist XF , FinalDCP and current versions of AME and Compressor ? I 'm specing out a new room but wondering if Clipster is worth the cost if I have these other things . <p> I own a dvs fuze and it is real fast encoding an delivering . Even faster than realtime , but payed upgrades ( more than 5000$ per upgrade ) + year suscription for support are crazy . I am stuck on version 4.7.5 not gona pay for the upgrades ... ( Too hight ) <p> Clipsters are really useful . Granted they are quite expensive , but they have the ability to do a number of things : <p> - High-speed mastering for DCP &amp; IMF encoding and review - Transcoding of a huge array of camera formats including R3D/ Sony/ ARRI Raw to a many different intermediate formats . - Syncable , frame-accurate , Digital Disk Recording that can be connected to a SAN for easy laybacks directly to digital files . - File-based media playback @ @ @ @ @ @ @ @ @ @ timecode , so you can say , sync your offline editorial cut to your grading session - Fairly complete editorial timeline that works well for final assemblies , online editing , or quick editorial changes . - A very decent grading toolset that includes primaries , secondaries , 6-vector , shapes , etc. - A ton of different 3D stereo tools for doing manual and automatic corrections , convergence , stream type , QC , and playback . <p> A Clipster is basically a showcase of proprietary DVS hardware I/O and encoding cards all stuffed into a very powerful server . Even though , I do hear people mention that cost is becoming an issue ( up over $100K ) , especially justifying upgrades for older units . Unlike software based offerings , it 's difficult to value engineer the cost of hardware solutions down to a lower price point without huge increases in scale . Because of that , there are openings in the market for products like ColorFront Transkoder which cover the most important tasks ( mastering , transcoding , playback ) , for about half the cost @ @ @ @ @ @ @ @ @ @ finding to be enough for what they need . <p> Also : what I 've seen of the Clipster the toolset for DCP mastering is really solid . For instance nudging a single line in a subtitle a couple of frames . Or maybe moving a line in a subtitle up or down in the frame . Not something easily done in Easy DCP or FinalDCP . <p> What is the benefit of this system if I already have Alchemist XF , FinalDCP and current versions of AME and Compressor ? I 'm specing out a new room but wondering if Clipster is worth the cost if I have these other things . 43971 @qwx453971 <p> Disclaimer . We have a 4th gen Clipster running latest v5 of the software . We might look into upgrading to v6 but they have issues with their sales organisation so have a hard time to get a quote . Interesting problem to have ... <p> Okay . So with that said ... There are two key things that I absolutely HATE with the clipster and I 'm not the one to filter what @ @ @ @ @ @ @ @ @ @ countless times to the DVS guys so if they read here it 's not news to them . <p> 1 . their support sucks . And I mean that from the bottom of my heart , it really really sucks . Worst support I 've ever had to deal with which sort of is ridiculous since we paid 16k EUR/yearly for Clipster and a DVS SAN . And this is also the biggest reason we switched storage vendor which I told them to their face , but they did n't really seem to care . They do n't realise that people that 's not happy generate bad word of mouth like this little piece here . So yes , if you need stellar and quick support look for something else . With a small side note that it might be better in the states than in Europe . <p> 2. they do n't understand how to make a good UI/UX and they do n't listen to feedback in this area ( ... or honestly , not sure they listen to any feedback from small customers ) . This sucker have @ @ @ @ @ @ @ @ @ @ then there were some SERIOUS flaws in the UI making the everyday work in it tedious . They really do n't understand this themselves and all they go on about is how fast it is and that " this new generation is twice as fast at this and that and that other thing " . Oh , and they also like to repeat how many system Disney have ... But it still looks and drives like the old POS from 2009 ... =/ And there 's some really stupid stuffs in it ... <p> 3 . I said there were two key things that was bad . But I gona add a third bonus thing since I got all fired up just thinking about it . The feeling you get as a customer is that there 's two unix beards in a basement , totally shielded form the outside world , developing it . They do n't listen to feedback and their only interest is to make it faster . <p> With that said . Some positive things about it . It 's a true workhorse ! For sure the @ @ @ @ @ @ @ @ @ @ in . It flies with most formats but struggles really hard with anything fancy like alphas and such . So no matter what they say to you about the toolset in it , look at it as a really expensive mutated alien ffmpeg on steroids in a huge box . There are tools for grading and stuffs but all that is more or less depreciated and not deved for years and years . Have in mind that some formats are paid options and they 're for sure not dirt cheap . But if you just need something to bring files into and to get other files out from in a really really fast and consistent fashion the Clipster is your friend and worth the bucks . The support is NOT your friend though ... Also , if you 're looking for an online system , the Clipster is n't it . <p> Oh ... Snap . Have to add another thing I forgot . If you buy one , be really really clear on what NAS/SAN you have and be sure that you get on paper that it 'll perform @ @ @ @ @ @ @ @ @ @ than SNFS and NTFS . This have been sort of fixed or worked around in the latest 5 point release and we now use it fairly successful on our GPFS storage . But to spare you mental pain and stomach cramps later on , be super super clear about this to the sales guy from day one . If your NAS is a SNFS system you 're golden though . = ) 
@@44332973 @4332973/ <h> Creating LUT from two pictures ( Fuji XT2 ) <p> I would like to create LUT from two pictures . One is flat picture from fuji xt2 using film simulation " ProNeg Std " . Other is created in Lightroom ( LR ) using fuji preset . This picture is film simulation " Velvia " . Which comes out of LR fuji preset in ' develop ' module of LR . LR can not export LUT out of ' develop ' module even with pluggin . Both pictures are same . I want to create Flat to Velvia LUT . I have fuji xt2 footage recorded in camera using flat spectrum . Which I want to grade to Velvia in Premier . <p> Can I create LUT using these two pictures in Photoshop ( PS ) ? <p> Once I have LUT can that be applied on all clips of same kind ? <p> If PS can do this then I need quick workflow . I installed PS yesterday . I am good in LR and Premier . <p> If I can not create LUT ( as explained @ @ @ @ @ @ @ @ @ @ this . How can I get this using any other software/plugginn . <p> Thanks I will look into Matchlight IMS . Quick question this software has $cost . Do I need anything else along this software to use LUT in Premier ? Looks like there is another software which change LUT to more open format . I will not buy two modules just for LUT . <p> The Velvia LUT provided above is slightly different then Fuji in Camera/LR preset . See Vectorscopes . One issue is about saturation which can be adjusted by not applying 100%intensity . But there is HUE rotation as well which worries me . Also amout of blue channel is different on RGB Parade . <p> Because this is same image and we are just changing color - it is easy to compare <p> Because this is same image and we are just changing color - it is easy to compare 43971 @qwx453971 <p> It 's easy to make a LUT which will convert the first of your images into the second , using something like MatchLight or the MatchGrade operator in Nuke . However @ @ @ @ @ @ @ @ @ @ you can only be sure that the resulting LUT will correctly convert those colours . As Steve says , the more of the volume of possible source colours that is covered by your image , the better the resulting LUT will work on other images . 
@@44332974 @4332974/ <h> Neutral Lighting For Grade Suite <p> .... but he has also hooked us up with a 5 litre tub of their N5 grey emulsion paint . 43971 @qwx453971 <p> An economical way of getting N5 grey paint is to take a ColorChecker into your local paint store , and get them to match the N5 swatch in the matt paint of your choice . Of course if you prefer a lighter or darker grey , use one of the other ColorChecker grey patches . <p> If you are willing to pay about $30 per bulb , you can find a D65 that dims nicely like a tungsten . 43971 @qwx453971 <p> Hey Jason , been going through this thread as I design the lighting for a new suite . Can you recommend a **26;1714;TOOLONG dimmable D65 bulb ? I see some dimmable D65s on 1000bulbs , but the price point is around that of the bulbs you warned against . Thanks man ! <p> Hey Jason , been going through this thread as I design the lighting for a new suite . Can you recommend a **26;1742;TOOLONG dimmable @ @ @ @ @ @ @ @ @ @ , but the price point is around that of the bulbs you warned against . Thanks man ! 43971 @qwx453971 <p> Hi Cullen , I do n't know of one off-hand , but was actually buying some D65 lighting this weekend , and noticed that prices have definitely come down since this thread was started . I know Paul has been using a dim-able D65 fluorescent that does n't buzz , so maybe he can chime in ? <p> I had some that were about $30 / bulb but I think they were discontinued . probably because they burned out after a couple months ... do n't remember the site either . I just use LED now if I need that kind of thing . usually have them off as the monitor backlight and LED desk lights is enough <p> Actually , LED might be a good new option , and why " D65 lamps " seem more affordable then they were back in 2013 . I recently ordered these this weekend to create more ambient lighting around my room and will let you guys know how they work out @ @ @ @ @ @ @ @ @ @ option , and why " D65 lamps " seem more affordable then they were back in 2013 . I recently ordered these this weekend to create more ambient lighting around my room and will let you guys know how they work out . <p> Usually with these cheap LEDs , 1 . their CRI is poor ( 80 or lower ) , 2. their actual colour temp is higher than advertised , for example the 6000K light might have a temp of 7000K , with some green or magenta shift . <p> That can be true , but I have tested a lot of different bias lights , and buying expensive , high CRI lamps does n't always get you there either . Lights are just like monitors ; you never know until you try them out . <p> I 've setup my grading room with the proper fluorescent 65K lights long time ago . But sometimes I wonder why we use 65K lighting if our audience is almost always watching their TV under tungsten instead of daylight . There must be a good reason ... <p> I have idealume @ @ @ @ @ @ @ @ @ @ a new tube from them and color was way cooler - but correct . Replaced my second to match . Way better now , the were 2-3 years old . Their info was that the blue phosphors go first just like plasmas and oleds . Recommended to replace at half expected life . The provide blue gels now to extend if desired . <p> How do you measure your bias lighting ? I got a ideal lume since 2 weeks and still have the feeling the light is cooler ( with a magenta shift ) than it should be . I would also be interested how you configure the backlight to be 10% of the monitor output . <p> Lamp colour is generally pretty unpredictable considering the multitude of factors that effect the perceived result ( type , environment colour , wattage , age , warm-up timings , manufacturer , materials can all have an impact ) . you also have the concept that many manufacturers use the term " white " arbitrarily , for instance I have 2 consumer bulbs here both claiming " neutral white " is 4000K @ @ @ @ @ @ @ @ @ @ the E standard illuminant which assumes the spectral output of the source is equal/uniform , to find the theoretical white point of the lamp and plot it against the closest point on the Planckian locus ( wp at D65 or occasionally D50 depending on the the CIE standard they are using , most western manufacturers use D65 ) to get the CCT ( so the K number you see on the box and specification sheets ) . <p> what you are looking for are daylight simulators / broadband lamps ( which i am guessing is what the idealume lights are ? ) , however these types of lamp are generally expensive . 
@@44332975 @4332975/ <p> I was exposing for 32% middle grey which is not a common practice for a lot of the a7s footage I see out there . The background was roughly at F-8.0 and foreground around F-2.0 . I did n't have gels to put on the window to bring whites down to 59% IRE but if did then the DR on the camera would be quite impressive for recording xavc-s 8bit . I 've spent a lot of time with camera and believe forming the understanding of the strengths and weaknesses is key to using an tool properly . By the way focus is off a little because I did this by myself and did n't feel like measuring . <p> If you look at the chart below ( it may be what you got your figures from ) you will see that 59% is meant to be where 90% white is supposed to sit for S-Log2 . This does not mean peak white . It is where you put diffuse white , to allow room for " over-bright " highlight detail , so your window should not be @ @ @ @ @ @ @ @ @ @ . Although according to Sony 's numbers ( which I used to create that table ) 18% grey should sit at 32.5% , many people like to deliberately over-expose S-log(1/2/3) images by a stop or two , and then " print down " to reduce noise . This is probably what you have seen when you speak of the common practice in images you have seen . <p> It 's true that if you wanted to hold detail in the window at the same stop , you would need to ND the window . But I was just pointing out you would not need to bring it down to 59% . That should be the level of a lit white wall , and the window should be brighter . If you brought the window down to that level , you would be wasting the highlight latitude of S-Log2 . <p> I guess the point of this was 32.5% IRE is where you need to be for nice color rendition . Not over or under , consistency is the goal . I agree with you though I totally typed what I @ @ @ @ @ @ @ @ @ @ <p> Ideally , I tell clients they should be going for a " down the middle " exposure -- not clipped , not crushed . In a perfect world , I would rather they over-fill on the set a little bit , knowing we can almost always crush the mids and blacks down to add more drama and mood to the scene . But in truth , it depends on the nature of the project and where they need to go with it . 
@@44332976 @4332976/ <p> Taking into an account a possibility of more than one choice , still 88% total for both Resolves does sounds pretty remarkable . Also , somehow despite the chart , I know Resolve Lite IS the most popular grading software . Just do n't ask me how I know that I wish the total was a 100% . That would have meant , that all other combined grading software would have been just 12% , but it 's not . I guess , with multiple choices for each user it 's understandable . Otherwise , if the total was a 100% it would have been a bit easier to get a fuller picture . <p> The bit that got me was the rates . I was towards the lower end and this is because I have no equipment . I imagine that section was distorted by those who were freelance but with their own kit - even if it was just panels . <p> Well , who 's to blame for this . There are those with back pockets and undermining this industry . They do not have @ @ @ @ @ @ @ @ @ @ . That 's called overpatting the dog . It 's a great shame when you undermine the value of the industry for greed .. Say no more ... <p> I 'm pretty sure a good portion of the colorists reporting around $50-75/hr are less experienced and helping clients in a similar position . If you 're working your way up , you ca n't jump right to charging $150+ until you deserve it . I 'd bet they 're also the ones working on all those RED and DSLR projects , too . <p> But interesting you 've brought this up . I once sent three emails to the so called distinguished Colourist asking what the top rates were . There was silence . Now , if these 50 Dollar Colourists have no point of reference , well , what can the industry expect . And I 'm sorry to say , there are those distinguished gentlemen who are just about charging 50 Dollars an hr , there are so many colourists being produced that compertition is getting tough ! They just done't say so . <p> I once sent @ @ @ @ @ @ @ @ @ @ the top rates were . There was silence . 43971 @qwx453971 <p> You could use the IA Local 700 Union rates as a possible guide , as far as LA goes . Colorists and Editors basically make similar money ( depending on their level in the business ) , so I 'd say $3000/week as a staffer in LA is about right if you have 20+ years of experience and several clients to bring with you : <p> I know of people making $5000+ a week a certain facilities , and I know people lucky to make $2000 a week , so it also hinges on the types of clients they 're getting . If the work is modest-budget cable reality shows , you 're not going to make as much money as a person working on A-list features or prime-time network TV dramas . If you 're doing commercials , you 're going to make more if you 're doing million-dollar commercials for Chanel vs. $50,000 local used-car commercials . There 's a wide range out there . <p> But in the independent/freelance world , it can go per-project @ @ @ @ @ @ @ @ @ @ go per hour ... so all you can do is charge what the market will bear . For myself , I 'll vary my rate according to the relative budget of the production , and I 'm prepared to negotiate if I get the sense they have very little money to work on . If the number is too low , I 'll tell them , " for that kind of budget , I ca n't give you a week but I can give you the best job humanly possible for 2 days . " <p> There is room for professionals , amateurs and even hobbyists in the Color Grading industry , and the appearence of more and more people ( I 'm an amateur myself ) opened the door to better colour on projects that would otherwise not been able to have ANY color grading whatsoever . I believe this is a good thing , and even in Portugal , where the audiovisual production market is scarce , the pro projects always end up in one of the few post houses we have with the Barco projectors and top @ @ @ @ @ @ @ @ @ @ a 2000G budget ca n't afford to spend 500G/hour to do some color touch-ups , that 's where lower tier people come in and have their market . 
@@44332977 @4332977/ <h> LG Series 7 ( and possibly series 6 ) HDR TVs <p> I 've been discussing HDR calibration with Bram at FSI , specifically consumer displays ... The big issue is that consumer TVs can not be accurately HDR calibrated , due to a combination of limited internal CMS capabilities , and the ' burnt-in ' EOTF , with roll-off . <p> But , FSI ( Bram ) seems to found a bit of an exception ... <p> On LG OLED TVs you can use the service remote to manually force the display into ' HDR On ' mode from the service menu . <p> FSI have only been able to do a short test on the newer LG C7 OLED , but this may work on the 6 series OLEDs as well ? <p> When you do this ( at least on the C7 ) the display goes to its true peak luminance output without requiring HDR metadata over the HDM connection and without triggering a ST2084 EOTF . The upshot of this is that because that HDR metadata is not present the display does n't apply @ @ @ @ @ @ @ @ @ @ with an EOTF that is much closer to a power curve than it is a PQ type response . <p> Due to being short on time FSI were not able to do a large profile + 3D LUT calibration , but did do a quick profile ( grey only ) and exported just a 1D LUT targeting ST2084 clipping at 675 nits . It generated a response pretty close to targeted PQ EOTF with just that very rough test , as follows . <p> This quick test does suggest there could be a very workable way of using the LG OLED TVs + a BoxIO LUT Box for HDR/4K calibration ... with accurate EOTF and colour calibration . <p> I have a Panasonic DX 900 that has a max peak white of around 1300 nits . Of course there 's the roll of , at about 85 IRE it starts . But is n't there a way to calibrate it to a max of 1000 nits and have no roll of ? <p> I know someone who also has the service remote and already told me that the LGs can @ @ @ @ @ @ @ @ @ @ as I finally want to get some proper HDR viewing device . ( Of course OLED is n't as bright as some other technologies but currently they are the best compromise . Looking forward when there are proper 4000+ nits devices in a few years thought . <p> Not sure 4000 nits is a necessity . I 'm still smarting from my very brief experience with X300 . It is the most amazing monitor I had ever seen and it 's not even 1000 nits . If C7 can do anything resembling what X300 can do , I 'd be in heaven . <p> More nits is better , offers more creative possibilities . The image can really improve with 2-3 stops more . Has to be used with care but sitting in front of a Pulsar can be a nice experience too ... Alway keep in mind , with PQ you do n't have to use the full power the display can offer . And then you can push that few pretty nice highlights in one special scene at the max and take use of the range . <p> @ @ @ @ @ @ @ @ @ @ compromises- light blooming , off-axis color changes , 240V supply , extra noise with many cooling fans etc . Besides , any brightness over infinity is infinity , so increasing brightness will not increase contrast ratio . With OLED contrast ratio is already infinity . X300 is a much better overall monitor . More is in not always necessarily better . Just ask any girl <p> 4000 nits in Pulsar comes at a price with many compromises- light blooming , off-axis color changes , 240V supply , extra noise with many cooling fans etc . Besides , any brightness over infinity is infinity , so increasing brightness will not increase contrast ratio . With OLED contrast ratio is already infinity . X300 is a much better overall monitor . More is in not always necessarily better . Just ask any girl 43971 @qwx453971 <p> Sure , has its downsides and visible backlight zones and the fact that a small bright highlight ca n't be shown in front of a big dark area due to the backlight-tech sucks and OLED is much better and I 'd like to see 4000 nits @ @ @ @ @ @ @ @ @ @ Brightness matters for the human visual system . For example due to the hunt effect a brighter image looks more colorful even if technically we can measure the same color in a less bright image . <p> I 'm just saying having some more brightness does n't hurt . With no word I said that the current state of HDR is bad , just looking forward to a bright future . And having seen the difference between 600 nits Dolby PRM and the Pulsar I 'd say the future is bright ; - ) <p> Just putting it out there but I 'm not convinced by this whole eye strain thing . Unless there is some actual evidence to say otherwise but my real world experiences of grading HDR - and a fairly large amount of it - is that I find it less straining on the eyes . <p> We walk around in bright environments all the time ( unless you 're a colourist ! ) and actually low light is where you put most energy into resolving details . <p> I find with the brighter output my eyes @ @ @ @ @ @ @ @ @ @ matching and the additional sharpness from the brighter display and wider gamut seems to be easier to look at over longer periods . <p> This is certainly my real world experience on the X300 after a full year of regular HDR grading and mastering . <p> Let me know how you do with 4000 nits for 10 hours a day on the Dolby Pulsar ... 43971 @qwx453971 <p> Marc . Do yourself a HUGE favor and please attend the upcoming Dolby/FilmLight HDR demo . Once you will see the SDR and HDR grades on identical X300 monitors playing side by side while displaying the same material , I 'm am certain , you will stop this " fatigue " nonsense . Hint , they both look identical , with the only difference being real life-like specular highlights in HDR grade ... <p> Cool to hear that comments from people with some real time on the Pulsar / with HDR monitors in general . We only had a day at the Pulsar but we used some kind of experimental 3000 nits display for some time . Would n't say it felt @ @ @ @ @ @ @ @ @ @ the room with a less bright monitor . <p> As far as my limited experience with HDR grading does help , I can say that your max brightness is not what you see all the time . If you do n't have any spectacular highlights in your frame there 's absolutely no need to go into HDR nits territory . <p> Most of my clients seem to think that HDR gives a much brighter image overall with boosted colors and spectacular highlights in every frame . 
@@44332979 @4332979/ <p> I would suggest building optimized media , such as ProResHQ or DNxHQX - at your timeline resolution . If you ever question the quality of the optimized media vs the camera original you can turn this on or off through the menubar . I optimize anything that is n't Alexa as it 's too CPU intensive . <p> If you have fast enough disk speed it is perfectly common to render your shots as DPX sequences in RedLogFilm and grade from that , DPX would put the least amount of strain on your CPU . <p> If you want to live dangerously , running performance mode in Resolve 14 will probably give you a boost in performance . For instance on my 8core MacPro I can reliably get 1/4 debayer in R12 , and 1/2 debayer in R14 w/performance mode on . Your dual GPUs will probably perform best in this scenario . <p> You could also consider purchasing a used Rocket-X PCI card which go for around $2000 , or rent one . <p> The difference between 8core and 12core is about one level of improvement in @ @ @ @ @ @ @ @ @ @ 1/4 to 1/2 good . It 's decent but not massive . <p> Sadly the MacPro 5,1 is getting on in age . It 's processing power is currently on par with the core i7 6950k , a gaming CPU that is relatively affordable . I ca n't wait to see the new core i9s in action , they should be quite a bit faster and decently priced . <p> 12-core all way . I 'm actually a fan of the 8-core as far as overall value is concerned , but if you 're de-bayering R3Ds , you really need the cores ( and a lot of RAM ) . The new speed improvements in R14 will help as well . <p> 12-core all way . I 'm actually a fan of the 8-core as far as overall value is concerned , but if you 're de-bayering R3Ds , you really need the cores ( and a lot of RAM ) . The new speed improvements in R14 will help as well . 
@@44332980 @4332980/ <p> Do you mean the stitching-seams between the individual cameras ? Guess the VR-Clips are in equirectangular projection ? IMHO it is really hard to evaluate stitching errors without watching the content in rectangular projection aka 360-VR View . At least on a regular display ( like the option in the youtube-webplayer ) . A HMD like an oculus / htc-vive etc. is even better . AFAIK there is no option to do the one or the other in resolve . Maybe you can find an OFX-plugin to convert the equirectangular projection to a cubic which might get you half way as errors should be more pronounced on this one . If you can check it in another software the easiest and cheapest way might be premiere pro . There you can display the footage in VR and look around . If you have access to an oculus you could even display it directly on that with the free mettle player plugin LONG ... <p> That being said : In my opinion right now ( until blackmagic/filmlight etc. add VR-features ) the best and fastest way to grade VR-stuff @ @ @ @ @ @ @ @ @ @ in VR on your SDI-display . You also get some nice additional features like continuing windows/masks on the other side of the image that are at the edge of the frame and proper sharpening/blur-filter handling . If you want to do that in resolve you have to be very precise when manually " doubling " a window as the windows quickly get noticeable if they do not align them properly . And if the video is intended for HDM ( which I am assuming as it is stereoscopic ) do n't underestimate the different visual impact of the grading between watching it with a HDM , watching it in 360-VR on a regular display and watching it in equirectangular projection . Just did a VR-project and even between HDM and 360-VR on a regular display the feeling of the footage was significantly different . So if you have to do it in resolve make sure to regularly evaluate your grade on a HDM . And as these correction loops are quite cumbersome I 'd highly recommend using scratch . <p> Pepijin , This might help ease your stitching LONG ... @ @ @ @ @ @ @ @ @ @ ) , stitch lines are often an issue . I always check my work in Koloreyes . Julian , thanks for the info on ScratchVR . I am hoping to get on it some time in the near future . And I expect some sort of a toolset for VR will be in the next version of Resolve . They love a gimmick ...... and a chance to sell a dongle . <p> I 'm being given an equitectangular 4k 2:1 image ( left and right of course ) so it 's already stitched . The thing I want to check are the left and right edge , that will eventually connect to each other ( sorry for the layman terminology ) . I 'll have a look at that dctl ! Looks like what I need . <p> One thing that you could try is this : Pipe the SDI-out of your Resolve into the SDI-in of SCRATCH and use it 's live grading function to acpture the image . From there , you can either send it on to an SDI-display ( possibly second SDI-channel of your existing one @ @ @ @ @ @ @ @ @ @ GPU , or to an HMD . Obviously , requires SCRATCH ( if only viewing , no need for the VR-version ) and a dedicated ( AJA probably ) video-io ( and preferrably machine - but could just run on a laptop , if you 're not outputting 4K via SDI ) . <p> Did this a couple of times with Baselight suites , where the colorists wanted to stay in Baselight for the grade , but wanted to view content on a second monitor , or HMD . If no HMD present , we just added a Tangent Element Mf to the SCRATCH workstation to use the trackball and ring to control the view . <p> Not sure if a setup like this is a bit too much for you , but ... well - can always grab the trial and check it out . <p> Obviously , requires SCRATCH ( if only viewing , no need for the VR-version ) and a dedicated ( AJA probably ) video-io ( and preferrably machine - but could just run on a laptop , if you 're not outputting 4K via @ @ @ @ @ @ @ @ @ @ is a great idea . I would like to try this with a 2015 MBP with Thunderbolt 2 . Had a look at the AJA options and to get HD SDI input its a minimum $1.5K USD spend for the IO XT . Scratch lists Blackmagic as supported , so any reason not to go with the UltraStudio Mini Recorder at $138 USD ? 4K would be great , but for price portability and the ability to quickly view VR via a second input on my grading monitor , I 'm happy to stick with HD for now . <p> all good . I suggested the AJA in case you want to run SCRATCH on the same machine as Resolve ( which then would occupy any Decklink ) . Besides it probably not being the wisest idea to run SCRATCH and Resolve at the same time on the same machine anyways , if you run it on a dedicated machine , then you can use BMD , AJA , Bluefish444 for your video-io . The mini recorder however just takes in the signal , so to get the image out @ @ @ @ @ @ @ @ @ @ through the GPU 's HDMI/DVI/DP to your reference screen , or add a mini monitor to the set ( or an Oculus/Vive ) . <p> Anyhow , the way to set it up is quite easy : - enter project , hit the " Live View " button - now select your input and press " Start Capture " and the play button - swipe right to bring up the metadata stack ( or press " W " ) and set the projection of the current clip ( in this case being your live stream ) to " Equirectangular " . - if you now hit the globe icon in the viewport , it will jump into 360 mode . - the preview output to your reference screen can be configured if you rightclick and go to Settings ==&gt; Monitor - and if you want to map a panel to the 360 controls , you can do that in Settings ==&gt; General ==&gt; Configure panels . 
@@44332981 @4332981/ <h> Pegasus Promise r6 low speed/what is your configuration ? <p> I 've just migrated my storage from loose drives to a Pegasus Promise r6 ( Thunderbolt1 ) with 4x 6TB ( 2 bays not currently in use ) in a RAID5 configuration with 18TB accessible capacity . Compatibility of drives has been confirmed . <p> I am however experiencing very low read/write speeds in the range of 60-100Mb/Sec when it should be over 400+ . RAID synchronisation is finished and only spotlight ( os x ) is doing its indexing at the moment . Stripe size is 1Mb and express config ( choosing video server ) was chosen on initial setup of the RAID . <p> So no possibility of expanding the RAID-array without reformatting it ? Still might be worth the hit in cost for the performance .. and to avoid the time it will take to move 16TB of data back and forth ... <p> Regardless of using only 4 bays , you have a problem that you need to solve if y get below 100mb . Going to 6 bays filled is just hiding the @ @ @ @ @ @ @ @ @ @ has a problem , bringing down the entire array speed . If you have a good backup of all your data , take out all drives and test them one by one extensively for performance to find the one with issues . <p> Two more drives are ordered and I will set up a 6-disk RAID5 in the next days . In the meanwhile I transferred all data to backups from the low performing RAID5 ( 4-drive ) , re-configured and re-formatted the storage to default settings ( auto-config with stripe sizes etc ) . Then ran speed tests both in AJA 's and Blackmagic 's tool ; both report write/read speeds around 600-650 ( before finishing synchronisation in the RAID ) . <p> I 'm going to perform a few more tests with another stripe size before installing the additional two drives to see what is going on . <p> Seems that no drive is faulty . Then what might have brought the RAID down to those slow speeds ? Contacted Promise but as these 6TB HGST drives are not confirmed compatible there is not much troubleshooting I can @ @ @ @ @ @ @ @ @ @ 7200rpm , 128Mb Cache , 6TB drives , " marketed " for NAS usage LONG ... . The information I 've received on my end is that you might have some performance differences ( minor ) and obviously the lifespan ( 3 vs 5 years warranty + the actual MTBF . I am aware these are not the top of the line enterprise hard drives nor the 4TB drives Promise recommends ( as maximum for the r4/r6 - in their latest and somewhat limited list of compatible drives ) . <p> And obviously it also comes down to the actual choice of storage vs price vs performance . Even if some solutions are not 100% confirmed by the manufacturer- the retailer in this case ( specialised in storage solutions ) confirmed that it is compatible and have not reported any anomalies . <p> If I now discover that they might not work optimally with a certain block/stripe size ( from the recommended/default settings ) I do n't deem the storage unusable - but rather find the " why " and find " how " to make them work optimally . @ @ @ @ @ @ @ @ @ @ the current reconfigured 4-drive ( yet unsynched ) Raid5 I get speeds around 600Mb/Sec which I find completely acceptable ( yet to be seen what I get post-synchronisation of the r6 ) . <p> What drives would you consider fast performing ones ? <p> Thank you for any feedback , this conversation is interesting as you do n't really find much info elsewhere . <p> Another thing to also keep in mind is operating raids close to full ( % wise ) is never recommended and will have an exponentional detremental effect on performance . Lloyd from macperformanceguide.com has a great set of affordable tools to test your raid for that effect so you know what to expectrealisticaly and check the reliability of your drives properly as well ( your quick performance test will not show you if a drive is working properly . You basicaly need to scan /read/write the entire drive ) <p> And dontbelieve the hype on enterprice vs stock drives . In my personal experience i have run raid arrays perfectly fine on stock drives while enterprice class or raid classified drives failed like snowflakes . @ @ @ @ @ @ @ @ @ @ few spare drives instead and have money left . Just treat them like consumables and rely on something else for backups. 
@@44332982 @4332982/ <p> Filmlight makes a very compelling sales pitch for expanding dailies looks beyond straight CDL values . I like how you can load BLG files in Avid with a free plug-in . PreLight is a pretty useful utility too ! <p> The lowest point of entry for full Baselight is a Baselight One at $55K . Its a turnkey Linux system based on a Z840 , Kona 4 , internal SSD Cache drive , and a single Titan Xp for processing . You can use an Avid Artist , Tangent Element , or their Slate control surface with it . The next step up is to a Baselight Two ( 3x Titan Xp ) with a Blackboard 2 , which costs considerably more . <p> I agree and have mentioned this many times to the Baselight folks . $55K is a lot of money not just for owner operators that want to separate themselves from the free or even $299 Resolve crowd but also for small facilities where their mentality is why should we spend more money on software when the client does n't care what their show is @ @ @ @ @ @ @ @ @ @ able to pocket more money because of the smaller initial investment . <p> I think Filmlight has the best chances of higher adoption from these owner operators that care about their craft and are willing to spend more money for the better color and paint tools to make themselves happy and in turn offer a better service to their clients . <p> At 5k or even 10k for the initial software and a reasonable yearly maintenance fee to get the latest features I would think there would be many people on here that would go for it ! <p> As much as Baselight for Premier would be cool I do n't know really how desirable it will be when it does n't have paint or it relies on the host applications architecture for performance . <p> so i agree if they drop it to 5 to 10k with a rental option and build you own system option would be a good way to go <p> it would open they door to a lot of people but with out devalueing the software to much 43971 @qwx453971 <p> Daylight on Mac is @ @ @ @ @ @ @ @ @ @ account , then full Baselight on a Mac would probably need to be a sizeable jump up from that . Fwiw , numbers like $20-30K have been brought up in conversation , but they are mostly met with a polite smile . <p> Filmlight offers enough affordable options as it is , and unless the market changes in some radical ( new ) way , they probably feel they do n't need to reach down any further than they already have , especially given the new V5 updates . <p> I was told by a source in a position to know that they installed a full 8GPU Baselight and a Blackboard 2 for $120K in LA about a year ago . I did n't ask if it was new or used , but from the looks of the panel it seemed pretty new to me . I do n't know the current price for a Baselight X. <p> The engineer told me that a comparably-equipped Linux Resolve with the full panels and 8GPUs would be about $70K , but that 's a pretty top-of-the-line system . <p> When I knew @ @ @ @ @ @ @ @ @ @ a few years ago , he told me that the prices on Baselight varied depending on what options were chosen and so on , the number of systems purchased at one time , and the exchange rate . I saw an invoice on a Baselight 4 from about 7-8 years ago that was around $300K , so costs have gone way , way down since then -- I 'm assuming because of the competition in this product range . <p> I was told by a source in a position to know that they installed a full 8GPU Baselight and a Blackboard 2 for $120K in LA about a year ago . I did n't ask if it was new or used , but from the looks of the panel it seemed pretty new to me . I do n't know the current price for a Baselight X. <p> The engineer told me that a comparably-equipped Linux Resolve with the full panels and 8GPUs would be about $70K , but that 's a pretty top-of-the-line system . <p> When I knew Craig Risebury ( former president of Filmlight in LA ) @ @ @ @ @ @ @ @ @ @ prices on Baselight varied depending on what options were chosen and so on , the number of systems purchased at one time , and the exchange rate . I saw an invoice on a Baselight 4 from about 7-8 years ago that was around $300K , so costs have gone way , way down since then -- I 'm assuming because of the competition in this product range . 43971 @qwx453971 <p> thanks Marc bit pricey ..... do you think if we did a massive group buy on the forum like they do with Final DCP we could get it under 500 Bucks Ha ! ! ! <p> thanks Marc bit pricey ..... do you think if we did a massive group buy on the forum like they do with Final DCP we could get it under 500 Bucks Ha ! ! ! 43971 @qwx453971 <p> I try to see the glass as half-full : Filmlight has been very responsive to price fluctuations and they 're fairly competitive these days . There 's a lot to be said for their approach to color-correction , and there are several features in @ @ @ @ @ @ @ @ @ @ Both are usable , but there are clear differences . 
@@44332983 @4332983/ <p> From HCFR 3.3.0 or later ( free open source calibration software ) it 's available a simpler way to select any measurement run for Ted 's LightSpace CMS Calibration Disk users . For full HCFR release notes look here . <p> Using HCFR 3.3. x with Ted 's LightSpace CMS Calibration Disk you will be able to measure now all the available chapters from the CalMAN Session of the disk , here is the complete list of the supported chapters : <p> HCFR 3.2. x follows the same pattern order with Ted 's LightSpace CMS Calibration Disk Chapters for any of the above measurement options and the HCFR 's 8-bit RGB Triplets for each pattern are matching exactly the RGB Triplets that the Ted 's LightSpace CMS Calibration disk has been encoded . <p> Below you can find the updated list of the total CalMAN and ChromaPure Chapters that any HCFR user can use to take measurements . These Disk Chapters are accurate and match the HCFR 's Color Engine Calculations : <p> Instructions for x-Point Grayscale ChromaPure 's Chapters Measurements <p> Uncheck ' ' use round down @ @ @ @ @ @ @ @ @ @ ' ' -&gt; ' ' General ' ' Tab . This is affecting only Grayscale measurements . <p> Before starting the measurements you have to select from HCFR 's Preferences -&gt; ' ' References ' ' Tab -&gt; ' ' Color Checker Patterns ' ' drop down menu list the selection of the measurement run you want to perform . <p> Select ' ' GCD ' ' if you want to measure using ChromaPure 's Color Checker Chapter . <p> Note : The pattern sequence of CalMAN 's 10-Point Luminance is identical to the Color Checker Selection ' ' RGB Luminance Axis ' ' , so it can be selected from the drop-down menu available @ HCFR 's Preferences -&gt; ' ' References ' ' Tab . <p> Note that there differences between the RGB Targets of CalMAN vs . ChromaPure ColorChecker &amp; Skintones or Saturation patterns . <p> Some CalMAN license levels can use Quick Analysis Workflow to take Saturation/Luminance measurements runs to verify their calibrations using 4/5/10-Point Saturation with 75%/100% Stimulus Level or 4/5/10-Point Luminance with 75%/100% Stimulus Level . <p> To perform the measurements it 's required @ @ @ @ @ @ @ @ @ @ below the correct settings per measurement run : <p> For Color Gamut Calibration using 100% Saturation / 100% Stimulus Level : Go to ColorSpace layout page , Change the Stimulus Level to 100 . About all other settings of the Layout page it does n't matter where they are . <p> For Color Gamut Calibration using 100% Saturation / 75% Stimulus Level : Go to ColorSpace layout page , Change the Stimulus Level to 75 . About all other settings of the Layout page it does n't matter where they are . <p> For 4-Point Saturation / 100% Stimulus Level : Go to Saturation Sweep layout page , Change the Stimulus Level to 100 and set Saturation Sweeps to 25% Sweeps . About all other settings of the Layout page it does n't matter where they are . Go to Workflow Advanced Options and un-tick the Saturation targets use constant luminance . <p> For 4-Point Saturation / 75% Stimulus Level : Go to Saturation Sweep layout page , Change the Stimulus Level to 75 and set Saturation Sweeps to 25% Sweeps . About all other settings of the Layout page @ @ @ @ @ @ @ @ @ @ Workflow Advanced Options and un-tick the Saturation targets use constant luminance . <p> For 5-Point Saturation / 100% Stimulus Level : Go to Saturation Sweep layout page , Change the Stimulus Level to 100 and set Saturation Sweeps to 20% Sweeps . About all other settings of the Layout page it does n't matter where they are . Go to Workflow Advanced Options and un-tick the Saturation targets use constant luminance . <p> For 5-Point Saturation / 75% Stimulus Level : Go to Saturation Sweep layout page , Change the Stimulus Level to 75 and set Saturation Sweeps to 20% Sweeps . About all other settings of the Layout page it does n't matter where they are . Go to Workflow Advanced Options and un-tick the Saturation targets use constant luminance . <p> For 10-Point Saturation / 100% Stimulus Level : Go to Saturation Sweep layout page , Change the Stimulus Level to 100 and set Saturation Sweeps to 10% Sweeps . About all other settings of the Layout page it does n't matter where they are . Go to Workflow Advanced Options and un-tick the Saturation targets use constant @ @ @ @ @ @ @ @ @ @ : Go to Saturation Sweep layout page , Change the Stimulus Level to 75 and set Saturation Sweeps to 10% Sweeps . About all other settings of the Layout page it does n't matter where they are . Go to Workflow Advanced Options and un-tick the Saturation targets use constant luminance . <p> Tip For CalMAN 5 Users that will use Ted 's CalMAN 5 Workflows with Pattern Generators <p> For the users that are using the Ted 's CalMAN 5 Workflows , as you noticed , there is not available at any layout page the ReadAll button , since all workflow have been configured to work with Singe Read with AutoAdvance . <p> For the users that can use the CalMAN Workflow Design feature , there is no need to enter to design mode to add manually a ReadAll button , they can use the CalMAN 's Keyboard Shortcut for that . <p> Before that , the users have to enable the Keyboard Shortcuts feature from CalMAN 's Application Preferences and after than they can simply press F10 key which is the keyboard shortcut key for ReadAll at any @ @ @ @ @ @ @ @ @ @ that are using Ted 's CalMAN Workflows with External Hardware Pattern Generators or with CalMAN 's Internal Pattern Generator . <p> CalMAN 5 Tip for faster Start-Up <p> CalMAN 5 can load faster , for about 5-6 seconds if you remove all unused workflows from the CalMAN 's Workflow folder which is located ( C : Users*User **30;1770;TOOLONG 5 for BusinessWorkflows ) and keepe in that directory only the workflows you usually using or those can opened by your CalMAN license level . <p> Lately I 'm getting emails/questions from disk users that are using Ted 's LightSpace CMS Calibration Disk with CalMAN and others that interesting to use it . <p> There users that are using the latest CalMAN 5.6.0 with active annual maintenance and others that are using older versions of CalMAN 5. x that are thinking of paying the annual maintenance to be able to use the latest CalMAN 5.6.0 version . <p> So they are asking me what they will do with that fact ? Does they will be able the disk they already have ? They asking if I suggest them to download and use @ @ @ @ @ @ @ @ @ @ with CalMAN 5.6.0 or it 's working only with CalMAN 5.5. x or older version . etc . <p> I want to inform that there is no problem to use Ted 's LightSpace CMS Calibration Disk with CalMAN 5.6.0 version . <p> This CalMAN feature was an old function that transfer from CalMAN 4 to CalMAN 5 , so SpectraCAL decided to remove it , which was very useful , since users were reported problems when they were trying to connect to the available disks of that list . <p> This function was related with the capability of CalMAN to control your dvd player remote and send commands to go to next chapter etc. using an external IR transceiver the user had to buy for about $100 and configure it according to his brand of player , so CalMAN was sending next or previous chapter commands via IR to the player and CalMAN was measuring Grayscale for example without user prompt .... but the supported disks had no correct pattern order for CalMAN 5 . <p> This was a very old capability for those who had old DVD disks for @ @ @ @ @ @ @ @ @ @ says that ' ' Manual DVD support has removed ' ' , we are not using DVD anymore for pattern disk , we use Blu-Ray . <p> Ted 's LightSpace CMS Calibration Disk users which are using the disk with CalMAN have no problem with that removed feature because we never used that feature to calibrate or to connect with CalMAN , since Ted 's Disk was not available to that list of disk which was removed . <p> This feature removed because it was not working and had no reason to exist to CalMAN 5 . <p> For example if you install an older version of CalMAN 5 you will see that when you will select to connect from the Source Panel of CalMAN to a Pattern Disk it displays you a list with calibration disks , like AVSHD , DVE , AVIA etc .... while all these disks were there as a selection , it 's impossible to do a complete calibration based to these disks , below you can find the reasons : <p> Get-Gray : Do n't have the same pattern order as CalMAN request them @ @ @ @ @ @ @ @ @ @ each Grayscale/CMS measurement run and it has only 11-Point grayscale and CMS with mixed pattern order . <p> AVSHD : has incorrect 11/21 Grayscale steps that do n't match CalMAN default RGB Triplet values , from the tons of CalMAN measurements , you can do only 11/21-Grayscale and CMS with 100% SAT/75% STIM and 100% SAT/100% STIM , and a Saturation run that is very different from the CalMAN 's default one . <p> DVE : has 21-Step Grayscale with incorrect values that do n't match CalMAN 's RGB Triplets , do n't have the pattern order CalMAN request each patch , it has no any CMS patterns to perform a Gamut calibration , so this disk ca n't be used for a complete Calibration . <p> Spears &amp; Munsil v1 : has none pattern for measurements , no grayscale , no CMS ... you ca n't use it with CalMAN to calibrate with meters . <p> Spears &amp; Munsil v2 : has patterns for 11-Point Grayscale and CMS , but not with pattern order of CalMAN . <p> AVIA / AVIA II : They have zero pattern for measurements @ @ @ @ @ @ @ @ @ @ calibration with meters . <p> The disk was missing from that list is Ted 's Disk which It has the same order that CalMAN requests each pattern and it 's created based to the exact RGB Triplets of CalMAN 5 for each pattern . <p> You can perform : Manual , Semi-Manual or Automated measurements ( without any IR-Controller but by using a small utility ) <p> This shows that the Ted 's disk is the only correct way to calibrate right now with CalMAN and use the 100% of the CalMAN 's measuring features . <p> My disk has been released from 23 October 2013 , there is not reported a single problem by any user by using my disk with CalMAN and it 's been verified with patterns generated by Accupel , and verified the digital levels using DVDO 'S AVLAB TPG colorchecker cursor Mode also and it has zero digital errors . <p> During development , there where a lot of verifications over months for human errors during the creation of this disk , it 's impossible to find any error since all values has been verified @ @ @ @ @ @ @ @ @ @ by one . <p> Most of the Ted 's LightSpace CMS Calibration Disk CalMAN users have send to SpectraCALl support emails to ask a lot of times about when Ted 's disk will be supported , not by Pattern Disk list from inside CalMAN Software , but from the HTML page of CalMAN Software where it says the Pattern Disks that CalMAN Support . <p> There a lot of posts about this to AVSForum and SpectraCAL forums , but nothing changed . There is no problem from SpectraCAL site which is does n't mention Ted 's Disk because most of the CalMAN users are already using it with success and they are happy about their experience with my disk with CalMAN . <p> I hope it will be supported one day , to SpectraCAL site , as 2-3 words text addition to HTML pages of CalMAN feature list . <p> Here are some images I designed to showcase what you are checking when are you looking to some basic patterns all of us are using for pre/post calibration verification , with an RGB Cube Space presentation . <p> Using the @ @ @ @ @ @ @ @ @ @ channel ( WRGBCMY ) , this sometimes can be fixed be removing some additionally clicks from the contrast control also , you are checking these areas : 
@@44332984 @4332984/ <h> Which intermediary format should I use for H.264 Drone footage ? <p> I worked with some drone footage recently from a DJI Inspire . It records in UHD H.264 . <p> I know that in a situation like this , you should batch all of the footage to an Intermediary codec like ProRes . So here 's the question : If the source material is an 8-bit H.264 , is there any advantage to using ProRes 444 over ProRes 422 HQ or even ProRes 422 LT ? <p> Obviously if I 'm not going to gain any benefit , I 'd much rather store ProRes 422 LT files instead of ProRes 444 . <p> Without any testing I 'd go with Prores 4:2:2 ( non-LT , non-HQ ) . The bit depth is not an issue here , it 's just that you do n't want to aggregate more compression artifacts when using a highly compressed codec like LT . <p> But , depending on your workflow you may want to use LT or DNxHD 36 for editorial and then later relink to the H.264 camera masters in @ @ @ @ @ @ @ @ @ @ with a DJI vehicle with a 4k integrated camera . I do n't know if they all have the same camera system or not , but mine has awful aliasing . Stills are fantastic looking . The moving video is too sharp and shows terrible aliasing . You may need to run the material through a low pass filter . <p> Igor- I agree about the aliasing . The DJI Inspire does n't have an adjustable iris . As a result the only method you have to expose properly is by speeding up the shutter speed , resulting in very choppy looking footage . The ND filter it ships with helps , but is woefully inadequate for a sunny day . The slowest they were able to get the shutter was 1/200 or about 43.2- . We were able to dramatically improve the footage by adding some motion blur , but it 's just not the same as capturing it properly in camera . <p> Igor- I agree about the aliasing . The DJI Inspire does n't have an adjustable iris . As a result the only method you have @ @ @ @ @ @ @ @ @ @ , resulting in very choppy looking footage . The ND filter it ships with helps , but is woefully inadequate for a sunny day . The slowest they were able to get the shutter was 1/200 or about 43.2- . We were able to dramatically improve the footage by adding some motion blur , but it 's just not the same as capturing it properly in camera . 43971 @qwx453971 <p> I have overexposure in nearly every shot where there is something light colored on the ground like a white car . Completely blown out . <p> Yeah , mostly because they want to , instead of being required to do so . *cough* FCP7 *cough* P2 Cards *cough* <p> Given today 's standards of CPU clock speeds and multi-threading architecture , cutting direct H.264 is not actually that computationally difficult as it has been in the past ; especially with Adobe Mercury Engine in play for Premiere Pro . H.264 files are also relatively tiny ( because they carry very little keyframes in comparison to intra ) . It used to be an absolute nightmare . Having that said @ @ @ @ @ @ @ @ @ @ . Unless you like dropped frames , unresponsive playback , and a warmer processor then go nuts . Intraframe codecs are definitely more edit friendly , no argument there , but at the cost of hard drive space and bus speed saturation as each frame is now discrete . Also you lose post time during transcoding , if the material was n't intra to begin with . <p> Last year I graded a short that was shot on the FS700 , and I opted to re-wrap the . MTS files to keep the H.264 video stream instead of transcoding the entire thing to PR422 . No problems , even on a shitty external USB 3.0 client drive . Honestly it 's up to you mate .. how do you like your eggs ? over easy , hard boiled , soft boiled , raw .. depends on your mood and the rest of the dish . <p> I just wrapped a feature project as a DIT where the producers wanted to use some DJI footage . When we showed it side-by-side with our A and B cameras ( Alexa shooting to @ @ @ @ @ @ @ @ @ @ that they just do n't match . Love the idea of the thing . But it 's EXTREMELY limited in controls on-the-day . <p> As for why people still transcode ? Predictability , and uniformity . I know people on this forum come from all levels ; from the indie just dipping their toes in , to the colorists that have been grading broadcast or feature material longer than I 've been alive . From the lower end , it 's harder to see . Especially with the tools getting better and better . In one sense , Premiere ( and FCPX , and other tools ) are really good at hopping between formats and making up for your lack of knowledge and uniformity with good behind-the-scenes rendering and adjusting . On the other hand , because the equipment is low-end relative to the needs of high budget work , one can get used to certain hiccups and glitches . <p> On a $1m feature , all material is shot on one camera , or a few lower end cameras . On an $8m feature , there are often 2 @ @ @ @ @ @ @ @ @ @ Alexa , Red , and some other random stuff . On the $50m features , there are a bunch of cameras being used , especially when you start adding in stunts , splinters , VFX , and Aerials . <p> From the editor 's standpoint , you want uniformity . If all content is sound-synced DNxHD 110 , then you know that if you have an issue , it 's not due to weird **25;1802;TOOLONG conflicts . If you 're working on a 3-minute marketing piece and you 're a small shop , that does n't sound like a big deal , because you 're used to running into those things , to the point that they almost become invisible . If you 're working with 200 hours of footage between 5 camera formats from 3 different units and your Avid project alone takes 5 or 15 minutes to open and you have to spit out a client review file by the end of the day after having made 15 different changes spread between 4 reels . . . well , you do n't want to be dealing with any @ @ @ @ @ @ @ @ @ @ a question of scope and scale . As much as it seems like the improvements in hardware and software would help the big-guys , in practice it 's usually the opposite . There is big value in uniformity . It 's why there are still near-set ERROR? operators that transcode and review footage for editorial . <p> In regards to the original question ? Igor 's method , stated above , would be my advice . 
@@44332985 @4332985/ <h> Keep DM240 or go back to Eizo ? <p> I really need some expert advice , I have been pulling my hair about those monitors for a few days now . <p> I currently have the DM240 in front of me and while it 's an awesome device all in all , I find that it shares the biggest pet peeve I had with the CG247x - brightness uniformity in the border regions . LCDs will be LCDs . <p> Let 's put aside the fact that I probably need an OLED to fulfill my expectations and help me think this through .. <p> I wo n't be needing half of the awesome DM240 features like Live Grade integration , all the on-set features like tally light , flip and desqueeze , clean feed and signal conversions . I need a reliable monitor for my little grading suite that I am just beginning to use . This is one of the key factors here , I have n't been freelancing for too long and most of the time , I work in other post houses . At home @ @ @ @ @ @ @ @ @ @ hence the hesitation to pay for OLED . But what I also do is photography , so that 's a big plus over a possible Sony as both monitors have 1920x1200 on Displayport . <p> One big advantage for DM240 is that it has SDI in , an easy way to get 10 Bit 4:4:4 and not think twice about it . It also guarantees in sync audio on the 3,5mm output . And of course the biggest advantage of all : it offers the ease of mind of being a professional display by a company that knows what it 's doing . <p> The CG247x route however would involve more tweaking around with HD-Link ( another 600 EUR ) to go via SDI to DP as there seems to be a lot of uncertainty regarding the HDMI in bit depth . Then there is renting LightSpace to calibrate it properly . And it leaves the question of audio sync wide open . As Resolve does n't offer an audio delay , how do the Eizo people among you get audio and video in sync ? The PC system audio @ @ @ @ @ @ @ @ @ @ Monitor out . Does pulling audio from the HD-Link do the trick or does the Eizo add more delay ? <p> What would you do if you were in my shoes ? The CG247x plus HD-Link is only about half the price of the DM240 . Would the DM240 still be worth the additional money to you ? <p> Oh ... Hi Christian , I 'm really considering to buy a DM240 to begin 2017 . It seems that I 'm exactlly in the same position as you . <p> How bad is the uniformity in the borders ? Do you mean that you have light leaks or so ? Did you ask FSI directly about it ? ( I 'll be happy to see any measurement you can make if you have a probe and lightspace DPS ... ) . <p> SDI input is obviously a plus , than the fact that the native gamut of the FSI is supposed to be 100% DCI P3 ( not sure it is the case for Eizo is n't it ? ) . <p> Oh ... Hi Christian , I 'm really @ @ @ @ @ @ @ @ @ @ seems that I 'm exactlly in the same position as you . <p> How bad is the uniformity in the borders ? Do you mean that you have light leaks or so ? Did you ask FSI directly about it ? ( I 'll be happy to see any measurement you can make if you have a probe and lightspace DPS ... ) . 43971 @qwx453971 <p> First off , the DM240 is a great looking device , everything about it feels high end . This is why I struggled so hard to return it . And the ease of mind of having a precise factory calibration is worth a lot to me . But .. <p> The DM240 felt like the CG247X all over again . The same amount of shading in the same areas of the monitor . The upper left and right sides drop in brightness visibly , about two fingers wide . A quick measurement on a 100nits white shows a luminance drop in those areas down to around 83 nits , on the right side even as far as 77 nits . I felt viewing @ @ @ @ @ @ @ @ @ @ 's the same panel . I know LCD is far from perfect when it comes to uniformity , but the fact that my stupid HP z24x sitting next to it looks that much better in terms of uniformity is just something that is incredibly hard to swallow at a price point north of 3K . <p> I did e-mail with support but they confirmed that all monitors that leave the factory are within specs . And I believe that the image in most areas is as good as it gets on an LCD . But what good is a perfect calibration if the image auto-vignettes on the sides ? <p> Again , it 's maybe not always a big issue in real world scenarios . But once you see it , you ca n't unsee it , so I know I would never be relaxed with this monitor . I would always fear the day when a client notices the shading . On a busy image , you wo n't see it . But imagine a uniform colored background or a blue sky and yep , it is an issue @ @ @ @ @ @ @ @ @ @ go from here , really . My first cynic reaction was " if I want shading I can just as well go back to Eizo " but that 's not really an option I suppose . Maybe I 'll try once more to see if I get more lucky with the panel lottery . If that fails too , probably OLED . I du n no . <p> Oh , and yes , the FSI covers P3 fully according to the specs . And I found it really neat to have the HDR preview gammas and fiddle around with that a bit . There is a lot to love about this monitor and I would buy it again in a heartbeat if the panel was more uniform . <p> That 's the first thing I tried but did n't get any sound from it . Probably something wrong on my end . But let 's not spam Christians thread with this . Tnx for letting me know 43971 @qwx453971 <p> I will continue just a bit . Pretty much every other update to decklink drivers seems to break the output @ @ @ @ @ @ @ @ @ @ But to remain on the topic . My CM171 has some uniformity issues as well , but not in the extent that it feels like a problem . I 've been pondering updating to a bigger monitor , and DM240 is on my list . Could you post a picture with black screen on , I 'd like to know what to expect . <p> The DM240 felt like the CG247X all over again . The same amount of shading in the same areas of the monitor . The upper left and right sides drop in brightness visibly , about two fingers wide . A quick measurement on a 100nits white shows a luminance drop in those areas down to around 83 nits , on the right side even as far as 77 nits . I felt viewing angles were also comparable to the Eizo . Perhaps it 's the same panel . I know LCD is far from perfect when it comes to uniformity , but the fact that my stupid HP z24x sitting next to it looks that much better in terms of uniformity is just something that @ @ @ @ @ @ @ @ @ @ of 3K. 43971 @qwx453971 <p> The numbers you give seem pretty big as a shading .... no ? The fact that your HP seems better for uniformity does n't seems normal to me ( but I 'm not a monitor expert ) . +1 to see a picture even though it is not very relevant to mesure anything .... <p> And I do n't think it could be the same panel as the Eizo cause they do n't seem to have the same gamut ... ( at least in the manufacturer tech spec ) . But I might be wrong . <p> I also doubt it 's the same panel because of the gamut difference . But those screen uniformity difference are huge for a 4kG monitor ! Even for a 1.5k monitor I would find them unacceptable . I think Eizos with DUE fair a lot better ? <p> Sounds like you are paying extra $$$ only for the back-end , as the panels seem to be more or less in the same range , with Eizo maybe having better uniformity ... <p> Hi everyone , I 've been @ @ @ @ @ @ @ @ @ @ : <p> 1 . If your aim is near perfect uniformity we strongly suggest the OLED units in our lineup . We have three to choose from ranging in price from $5,550 to $8,000 . All use the same panel , only the backend hardware is different . Uniformity on these OLEDs will outperform virtually any other technology at any other price level . See Mike Nagel 's detailed reporting on our OLED uniformity here : LONG ... <p> 2 . As I think everyone knows there are limited number of LCD panel suppliers in the world . No one makes panels just for the professional monitor market . We always try to find the best panels available and we pay a hefty premium to do so , but real world limitations abound . The DM240 LCD shares all the features of the DM250 OLED , but is exactly half the price . Something obviously has to give with a $4,000 price spread between DM240 and DM250 and that price savings comes solely by virtue of the LCD panel being so much less expensive . We had a huge amount @ @ @ @ @ @ @ @ @ @ with all the features that entails ) , but that used a more cost effective panel alternative than OLED . There are a lot of users that need those features , but do n't have OLED budget ... that is how this product came about . <p> 3 . The only way to dramatically improve uniformity on the DM240 without negatively impacting other facets of display performance would be to build a very expensive zoned backlighting system to replace the backlighting system used now . The problem with this is simple , doing so would make the unit cost more than our entry level OLEDs and while uniformity could possibly be brought almost on par , the OLED would be a less expensive product with better black levels . In other words , we do n't feel there would be a significant market for that kind of product . FWIW , panel level digital uniformity correction on LCDs can lead to on screen artifacts and more importantly always reduces overall contrast . <p> 4 . For those of you considering the DM240 where the OLED alternative is a significant stretch @ @ @ @ @ @ @ @ @ @ : CRTs could have light falloff near the edges approaching 50% . Here are readings from top left , middle , and bottom right on a BVMD20F1U taken when it had a very healthy tube : 89cd/m2 , 111cd/m2 , 128cd/m2 . That is a 39nit spread on a CRT that in its days was considered more than suitable for very color critical work . This does n't mean you should n't strive for OLED performance if your budget allows , but I do personally believe that for those without the budget for OLED it certainly suggests a DM240 or other similar LCDs can be effectively used with no significant step backwards compared to historical norms . <p> As always our sales and support teams are happy to discuss any display questions you might have in much greater detail : **30;1829;TOOLONG or +1.678.835.4934 . As I hope is evident we are pretty straightforward about our capabilities and limitations , but do try to keep in mind that we build a lot of different products to try and address different operator needs and budgets . Not every product in our lineup @ @ @ @ @ @ @ @ @ @ ask questions , we will be happy to try and find the unit that best balances your needs and budgets . <p> Hello Bram , Thanks a lot for this detailed answer . I just gave a quick look to the doc mentionned ( the comparison of FSI Oled and LCD ) . Do you have some more infos about how the DM240 is supposed to perform compared to the CM171 . It seems that the dE is around 3.6 for the CM171 . Should it be less for the DM240 ? The doc said that you do not market the CM171 as a colorgrading monitor . But that is the case for the DM240 right ? Or would you say it is a monitor for DIT on set that can occasionnaly be used for colorgrading ... <p> And you are totally right to remind us not to expect that the LCD panel should behave like an Oled . But you know , will always expect to find the goose that laid the golden egg ... <p> The numbers you give seem pretty big as a shading .... no ? @ @ @ @ @ @ @ @ @ @ n't seems normal to me ( but I 'm not a monitor expert ) . +1 to see a picture even though it is not very relevant to mesure anything .... 43971 @qwx453971 <p> If you were to measure a proper uniformity grid , the HP would likely prove to be very uneven in values . But to my eyes it looks more uniform , especially the sides . I just loaded up a white frame in Resolve and measured random areas with the i1 Display Pro and the free Lightspace . Those that looked darker to me resulted in the numbers I wrote . Again , by no means a professional uniformity evaluation , the rest of the screen I did n't check as it looked uniform . <p> Unfortunately I ca n't take any photos as the monitor is already back in its box and prepared to be shipped in the morning . <p> And for what it 's worth , the Eizo did n't look any better with DUE on . <p> And also thanks , Bram , for putting the values into perspective . The uniformity @ @ @ @ @ @ @ @ @ @ I said , I really like the monitor a lot all in all . For me it 's a personal thing , after having gone through two Eizos with uniformity issues , it was the first thing I looked for . I ca n't unsee it and I would n't have been happy with the monitor . <p> The CG247x route however would involve more tweaking around with HD-Link ( another 600 EUR ) to go via SDI to DP as there seems to be a lot of uncertainty regarding the HDMI in bit depth . Then there is renting LightSpace to calibrate it properly . And it leaves the question of audio sync wide open . As Resolve does n't offer an audio delay , how do the Eizo people among you get audio and video in sync ? The PC system audio output will never be in sync with the BM Mini Monitor out . Does pulling audio from the HD-Link do the trick or does the Eizo add more delay ? 43971 @qwx453971 <p> Hi Christian , just a question : Why do you necessarily have to @ @ @ @ @ @ @ @ @ @ a 4:4:4 ( RGB ) Signal into the Eizo ? Then I understand . But am I wrong that , if one ( like me ) only want to get a 4:2:2 , 10Bit Signal into the Eizo then a direct connection BMD Card-&gt;HDMI-&gt;Eizo is working fine ? As far as I remember from other discussions here , a 4:2:2 , 10Bit Signal oder HDMI works with all BMD Cards but 4:4:4 , 10Bit not ( only over SDI ) . I think this is a Limitation of some Blackmagic Video Cards not from Eizo ? Can you or anybody confirm that ? <p> There seems to be a lot of uncertainty whether or not the Eizo does actually display 10 Bit via HDMI . I must admit I did n't check it properly during the few hours I evaluated the monitor . Someone here mentioned banding using the signal path you described , as well as on the BMD forum . My theory is that the Eizo does n't properly " tell " the Decklink that it supports Deep Colour ( 10 Bit ) . <p> I do n't @ @ @ @ @ @ @ @ @ @ HDMI out . I did a little test with my GUI display ( HP Z24x ) connected simultaneously to GPU via DVI-D and Mini Monitor via HDMI . I used the grey ramp generated by Resolve and it looked different when switching back and forth the inputs . Banding on DVI-D and no banding on HDMI . As expected . So IF people are seeing banding , I would be inclined to blame Eizo . But maybe others more familiar with HDMI technology ( and its " handshakes " ) can cast some light on the issue . Maybe in a different topic . 
@@44332986 @4332986/ <h> Smoke 4k Workflow <p> I had read somewhere that Encore Hollywood was doing the post for the Netflix show Marco Polo , which had UHD deliverables . It mentioned that they used Smoke &amp; Lustre for the conform and grade . <p> I kept trying to see if there were any additional write-ups about the process , as I am curious how they were monitoring and conforming UHD in both Smoke ( and also Lustre , if it is n't part of Flame ) . <p> Anyone here perhaps have any insight ? I know Smoke has just started adding the UHD and 4K resolutions , but there is still no way to monitor out 4k from SDI . Just wondering about this , as I use Smoke frequently as my conform tool . <p> Are you referring to Smoke on Linux or Mac ? They are very different stories . On Linux , Flame/Smoke/Lustre each require a minimum of a 16-Core 3GHz+ z820 , a K6000 , 16Gb Fibre or SAS , plus a Kona 4 card . The Kona is connected using quad 1.5G SDI out @ @ @ @ @ @ @ @ @ @ I have read about UHD support for Smoke on Mac , but have n't tried it . I know a number of people struggling with it though , and until there are more powerful options hardware wise , I imagine it 's going to be difficult at best . <p> I 've spoken with AD support quite a bit about this . Beyond the basic system resources required to process the huge increase in data compared to HD , you need a GPU with a very fast ( and large ) Vram buffer to guarantee smooth playback . That 's a little tough to come by on the Mac platform right now . <p> We use Smoke on Mac . If you have the Mac Pro ( Late 2013 ) it actually does a pretty decent job at playing back UHD and even 4k DCI . However , everything is of course limited by the storage speed . Any sort of uncompressed file format such as DPX wo n't play unless you have a very fast array &amp; connection . We have been able to stick with ProRes444 as the @ @ @ @ @ @ @ @ @ @ That is what peaked my interest , was that it specifically said Smoke . I believe that Smoke Advanced ( Linux ) is EOL , so I could n't imagine it supporting UHD . In fact , I believe that the Smoke Advanced migration path was just to Flame or Flame Premium . So to say Smoke and Lustre , and not Flame seemed odd to me . Perhaps they did n't bother actually monitoring picture with anything other than a downconverted HD signal in Smoke on Mac . <p> However , everything is of course limited by the storage speed . Any sort of uncompressed file format such as DPX wo n't play unless you have a very fast array &amp; connection . We have been able to stick with ProRes444 as the source codec as well as the cache format . 43971 @qwx453971 <p> Yeah , it 's just another example of Thunderbolt being pressed into service in a situation it 's not ready for . We really could have used a few PCI-E slots . <p> That is what peaked my interest , was that it specifically @ @ @ @ @ @ @ @ @ @ ) is EOL , so I could n't imagine it supporting UHD . In fact , I believe that the Smoke Advanced migration path was just to Flame or Flame Premium . So to say Smoke and Lustre , and not Flame seemed odd to me . Perhaps they did n't bother actually monitoring picture with anything other than a downconverted HD signal in Smoke on Mac . 43971 @qwx453971 <p> Since they 're all one product now ( Flame Premium ) , in facility circles it 's very common to refer to Flame in the context of VFX/ Compositing , Smoke for online , and Lustre for grading . No one says " Flame Premium-Editing " or " Flame Premium-Grading " . Autodesk marketing is the only one driving that . If you look at support docs ( e.g .. " What 's New in Lustre 2015 " ) , or talk with AD support , everyone still refers to the three separately . <p> As for UHD on Smoke on Mac , I would try to stick with codecs your TB arrays can handle , or hang in @ @ @ @ @ @ @ @ @ @ long-time proponent of Smoke on Mac , but now that Apple 's begun to consumerize their pro hardware , continuing to fight the good fight just seems less and less worthwhile . <p> Yeah , it 's just another example of Thunderbolt being pressed into service in a situation it 's not ready for . We really could have used a few PCI-E slots . 43971 @qwx453971 <p> I 'm really curious how people are liking working on new Mac Pros with Smoke . I 'm still chugging along with a 2010 Mac running Smoke 2013 . I tested Smoke 2015 on a 2013 Mac Pro last year and found render time to take a minute longer with the same shot on my older system . Autodesk had little to offer in explanation - something like " with more features comes more render time " . I 'm about to upgrade this year , and am weighing my options . More and more , Linux ( Flame ) seems like the way to go . <p> I have been running Smoke on both the cylinder , as well as a @ @ @ @ @ @ @ @ @ @ much of a difference between the two with render time or performance . If anything , I found that higher resolution frame sizes play better on the cylinder , but I am sure that has a lot to do with the GPUs in the 2012 machine . To be perfectly honest , I have n't done a head to head comparison , or really paid too much attention for render times - so please take that with a grain of salt . But I would say that if you can go the Flame route , it certainly seems like that is where AD is pushing all of the new features and power . For us , Flame is on the high end for what we would need on from a finishing system right now ( especially on price ) . Smoke is a great tool for conform , and some minor effects work , but we do n't use it much past that . It fits nicely in our pipeline between offline and grading in Resolve , as I find there to be a lack of real conforming tools within @ @ @ @ @ @ @ @ @ @ create a Smoke/Flame/Lustre **38;1861;TOOLONG system equivalent . My expectation is that a Fusion/Resolve system will be shown at NAB , but I hope they do n't combine the functionality in one program . Two programs would be fine . <p> That could be an interesting combination , but more for the price point then what would ultimately be the usability . I 'm not convinced that combining the tools would end up working too well from a dev standpoint . If nothing else , perhaps it would be more competition to nudge Autodesk . For me at least , I 'm not sure the ability to do more compositing would be as important as having a more robust conform toolset in Resolve . Right now SMAC has been one of the only things keeping us tied to the Mac platform . <p> We have 2 Smokes ( perpetual license ) - one on a MacTube , and the other on a 2012 Tower w a K5000 card . We upgraded the tower to 10.9 so that we could be running the same OS , same versions , etc - but @ @ @ @ @ @ @ @ @ @ going back to 10.8 , as the K5 card/Smoke does n't seem to do well with 10.9 . Supposedly , 10.10 is much better , but it 's not supported by ADSK ( but Flame Assist is ) . When they were both working well , I clocked the new one as about 10-20% faster renders , but not much difference in overall performance . <p> Since this was a 4k thread ... we 've done some tests and VFX work at 4k , but have n't done a full 4k delivery . If you use ProRes4444 as your render codec , either Mac worked fine with an 8 drive Raid . Using Uncompressed , you need a RAID that can pull 1 TB/s - and neither of our raids are capable of that , so we would n't be getting realtime performance anyway . <p> As mentioned above , there is no 4k/UHD video SDI output on Smoke , so we were only doing HD monitoring . 
@@44332988 @4332988/ <h> BLADE RUNNER 2049 by Roger Deakins <p> Whoa , I just caught the new full trailer on the Bladerunner sequel and had my little mind blown ... <p> The DP is the great Roger Deakins , and I believe the lead colorist at eFilm is Mitch Paulson . The work is pretty stunning -- there 's some looks there I do n't think I 've ever seen before , and I 've seen everything . <p> Relative to the time Blade Runner came out , the production design , VFX and the photography were so ahead of it 's time . I saw the director 's cut at Cinerama Dome maybe 10 years ago and all the miniatures looked so good compared with CGI . It 'd be nearly impossible for 2049 to beat that in relative terms . I hope they focused on the story instead . <p> Late last year I finished working on a documentary about Alan Ladd , Jr . the executive who greenlit Blade Runner and championed Ridley Scott . What I had n't known until then was that Blade Runner was trashed @ @ @ @ @ @ @ @ @ @ was n't until the overseas release that the film really took off . That 's really hard to believe . I was maybe 13 or so when I saw it first and it blew me away . And not just by the quality of production but also by the finely crafted anti-slavery allegory . <p> What I had n't known until then was that Blade Runner was trashed by the critics when it first came out . It was n't until the overseas release that the film really took off . That 's really hard to believe . I was maybe 13 or so when I saw it first and it blew me away . And not just by the quality of production but also by the finely crafted anti-slavery allegory. 43971 @qwx453971 <p> The movie was pretty much a bomb in its initial 1982 release , and some have blamed the light-hearted E.T . for some of that ( which dominated most of the summer of that year ) . I think it 's a brilliant film visually , but I 'm not a fan of the downbeat plot @ @ @ @ @ @ @ @ @ @ story Do Androids Dream of Electric Sheep . I liked the movie for what it was , and really enjoyed the visuals and effects , but I was dragged down by the story and performances . <p> But I 've often said , Ridley Scott is one of those directors where even his commercial failures are often interesting films that are totally worth watching . Blade Runner was an incredibly influential film that resonated for 20 years in many SF films and TV shows that followed . <p> Am I the only one to not be blown away by this ? I really hope it 's just the trailer , but I found it to have all of the affect , none of the soul . 43971 @qwx453971 <p> Hey , at least it looks good . The androids have no soul either , but as long as it 's entertaining , I do n't care . This is a colorist forum , so I 'm merely looking at this from the viewpoint of a colorist . <p> It 's hard to judge a movie 's content from a short @ @ @ @ @ @ @ @ @ @ another 4 months . For all we know , they could reshoot and re-edit all the way up to that time . Ridley is known for changing things . <p> Yup , gorgeous work as always . I miss the anamorphic 35mm look as always , kinda wish Deakins did n't go full on digital , imo his best work on digital does n't come close to his best work on film , but he 's a king . He confirmed on his forum that he shot Alexa XT as usual on this one , probably Master Primes , his go to lenses these days . <p> He also told me he timed the previous trailer &amp; this one himself ( well , not on his own I assume ) . <p> As much as I admire Roger Deakins I 'm little bit sad about not using anamorphic . Some years ago I remember Roger himself that he will not use anamorphic never even if the director ask for . But in Blade Runner I think anamorphic composition is a must , I did my tesis at filmmaking school about @ @ @ @ @ @ @ @ @ @ and there is a lot of work using the anamorphic to compose ultra wide , looking at the trailer I feel it 's a little bit off . <p> Sir Ridley 's new ' Alien : Covenant ' opens here tonight and I 'm torn as to whether to brave the winter chill and head off to a regional cinema of dubious quality ( as I always do when there 's a new Ridley Scott film released ) or simply wait till it comes out on BD then experience it on our 55 " 4K ' client ' TV under controlled ' home cinema ' conditions . <p> As much as I admire Roger Deakins I 'm little bit sad about not using anamorphic . Some years ago I remember Roger himself that he will not use anamorphic never even if the director ask for . But in Blade Runner I think anamorphic composition is a must , I did my tesis at filmmaking school about the scene where Deckard did the interrogation to Rachael , and there is a lot of work using the anamorphic to compose ultra wide @ @ @ @ @ @ @ @ @ @ little bit off . 43971 @qwx453971 <p> Are n't you mixing things up here ? Composition and framing has more to do with the aspect ratio than whether anamorphics lenses were used . The trailer is in 1:2.39 cinemascope , the same aspect ratio that Blade Runner was shot at . <p> With spherical lenses you wo n't get the streaky lensflares and anamorphic bokeh . <p> when all the blurb comes out how this was made i 'll be interested to see how Deakin lit this thing Jordan Cronenweth on the first one used a huge amount of neons in the city scenes as the primary light sources and he was big fan of over exposing actors face or under exposing them to create harsh lines <p> it was quite departure from normal lighting .. if that exists <p> it taught me a awful for photograph &amp; filming that shadow is more powerful the light ... it pretty much the best tutorial on lighting i have ever seen and really inspired me to learn about lighting <p> visually the first one is brilliant combination of physical elements smoke and @ @ @ @ @ @ @ @ @ @ set the mood with the over/under exposure <p> water is critical element in the blade runner look and the first thing i spotted in the trailer .. so there doing well so far <p> i have seen a quite few people shy away using physical elements on set like smoke/haze/fog it changes the tone of shot , but its also great for getting more depth on a wide and squeeze every thing together on a tele ... you have to practice with it to get it right ... and some younger director just haven , t be exposed to using it 
@@44332990 @4332990/ <p> It seems to be based on the Displaymate article and I 'd never heard about this website before . <p> Any though on the results someone ? What to think about HDR for 5000$ ... just a joke ? The fact that the color accuracy and the gamut would be good enough for a kind of grade 1 huge color grading monitor ? Sorry if the news had been related already ! <p> And this : " Unfortunately , this level of accuracy still does n't mean that it is guaranteed to absolutely match a smaller monitor . For instance , if you set up a 24 " Flanders Scientific on your desk and the E6 for the client to view , even when full calibrated , they wo n't match perfectly . This is because of the limitations of human vision ; we use a different part of our retina to view big images and smaller ones , and it is basically impossible to match your giant client review monitor precisely to your smaller monitor . " <p> " Unfortunately , this level of accuracy still does @ @ @ @ @ @ @ @ @ @ smaller monitor . For instance , if you set up a 24 " Flanders Scientific on your desk and the E6 for the client to view , even when full calibrated , they wo n't match perfectly . This is because of the limitations of human vision ; we use a different part of our retina to view big images and smaller ones , and it is basically impossible to match your giant client review monitor precisely to your smaller monitor . " 43971 @qwx453971 <p> There is so much uninformed bullsh ! t on these " advertorial " websites where it 's clear the people have no idea what they 're actually talking about . To me , RedSharkNews , NoFilmSchool . Pro Video Coalition ... they 're all variations of the same things : sites designed to sell ads and provide only a modicum of real information . What 's interesting is that the heavy-duty computer news/review sites ( Gizmodo , Engadget , Tom 's Hardware , AllThingsDigital , etc. ) all have fairly in-depth articles with a lot of technical information , so you can get honest @ @ @ @ @ @ @ @ @ @ world . But not for video production and post-production . <p> And I still get conflicting opinions on the blacks between 0 and 10IRE . Anyone knows , if that issue had been resolved in 2016 version ? 43971 @qwx453971 <p> No , it 's still there . The Panasonic EZ950 coming out this month will get closer . Apparently it will fix the vertical banding and edge vignetting in earlier panels , but the plugginess in the blacks has n't been totally resolved yet . I 'll be able to take a look at one sometime next week , and will let you guys know how it does . <p> There are extensive tests on LG 2016 OLEDs , they do n't calibrate well compared to Panasonic DX902B ( for Rec.709 ) . 43971 @qwx453971 <p> The green-channel tracking issues in all of the current LG panels can be pretty well ironed out with a LUT . The non-linearity in the blacks is still a problem . The DX , like a lot of LCDs , may calibrate ok , but the poor off-axis makes it a no-go . @ @ @ @ @ @ @ @ @ @ panel off the line with fewer flaws . There is no consumer monitor available that can touch the X300 , especially considering everything it offers in a single display ( 4K , HDR , Minimum Black level , Input options , RGB and grayscale tracking ) . I would also venture to guess that the Rec709 pass for most major studio films released this year has been graded on one , and that none of them were using a LUT box . <p> And I still get conflicting opinions on the blacks between 0 and 10IRE . Anyone knows , if that issue had been resolved in 2016 version ? 43971 @qwx453971 <p> Glad someone started this thread , I was gon na start one myself dealing with this " noisy blacks " issue . <p> I 've been working with an LG 55 " B6P for 4 months now ( 2016 version ) , and I got ta be honest ... the " black noise " between 0-10 IRE is really starting to bug me . The TV calibrates great with a LUT box attached ( we use calman @ @ @ @ @ @ @ @ @ @ , no problem there ( just do n't use the internal CMS on the set itself as you 'll get banding/artifacting on color extremes , stick with the LUT box ) , but the noise in the blacks is honestly pretty distracting . I 've actually had multiple clients complain about it , and THAT ALONE is reason enough to second guess this thing for me . Now some of the " noise " can be attributed to viewing 1080p content on a 4K monitor ... when you view 4K content directly on it , the black noise lessens a bit , BUT its still there . Compared to the Panasonic VT60 ( which is now sitting in my living room , delivering me beautiful netflix and NFL games ) , I 'm kind of dissapointed with the LG , and I start sweating bullets when I 'm showing clients anything dark ... or anything captured on a crappy compressed camera ... it just amplifies whatever noise is there on the bottom end by ALOT . <p> Thanks for this thread Julian , I have " fudged " the 20 @ @ @ @ @ @ @ @ @ @ artifacting and noise some , but I guess I 'll have to give it another crack . I 'm just suspicious that this is a " proccessing " issue with the set itself and how it handles information on the bottom end . If anyone else has any insights into this please let me know . <p> So would anyone recommend the the LG 55B6 as a client monitor ? How well does it calibrate ? Do you need a LUT box ? Chris Hall have you managed to get rid of the " black noise " ? Also I 'm reading that the TV should get an update today which should solve a few issues . <p> I know lots of houses in Los Angeles and NY who use is it as a client monitor , so its being used ( which was one of the reasons we decided to get it here ) . Still have n't been able to get rid of the " black noise " this past week with some further tweaking . Definitely would reccomend a lut box if you can due to the @ @ @ @ @ @ @ @ @ @ management controls ( box io , or at least a teranex mini will get you there ) . What 's the update coming out today ? ? 
@@44332991 @4332991/ <h> Univisium 18:9 ( 2.00:1 ) Questions <p> I am looking to shoot my next movie in the Univisium format . I feel like 1.85:1 is a little too close to television for my taste and 2.00:1 is a nice compromise . <p> I 'm mainly curious how the workflow would work , would I need to pass down a transparent matte to the editors or is it something most NLE 's support , dailies etc . Also how would projection work ? Would a DCP accept this format ( I know a few movies have done it ) or would the DCP just be full resolution and a mask on top . <p> I would shoot a framing chart at least at the beginning of the process if not every single day , particularly on A &amp; B camera and second-unit material , just to make sure the displayed frame in dailies and editorial is exactly what was intended during production . <p> I personally think 2.00 is kinda silly , but I do n't dispute there are people who like it . David Fincher is one of @ @ @ @ @ @ @ @ @ @ successful ) Netflix series House of Cards . My reaction is generally to ask " what is the biggest intended audience for the show ? " and if it 's home TV sets , be aware you 're just creating a smaller screen with less vertical area , which is not necessarily a positive thing ; it can actually distance the audience from the characters and story . I think 1.78 makes a lot more sense for TV made in the last 10 years , but there are always exceptions . <p> I personally think 2.00 is kinda silly , but I do n't dispute there are people who like it . David Fincher is one of them , and he uses it on the ( extremely successful ) Netflix series House of Cards . My reaction is generally to ask " what is the biggest intended audience for the show ? " and if it 's home TV sets , be aware you 're just creating a smaller screen with less vertical area , which is not necessarily a positive thing ; it can actually distance the audience from @ @ @ @ @ @ @ @ @ @ lot more sense for TV made in the last 10 years , but there are always exceptions . 43971 @qwx453971 <p> Interesting thought about distancing the audience , I know this is referring to TV but with so many films going for 2.39-2.40 , do you feel that still distances an audience from the subject or because it 's a film and not TV the audience is more subconsciously acceptable of it . <p> Interesting thought about distancing the audience , I know this is referring to TV but with so many films going for 2.39-2.40 , do you feel that still distances an audience from the subject or because it 's a film and not TV the audience is more subconsciously acceptable of it . 43971 @qwx453971 <p> I do still feel that it distances the audience -- to me -- but I accept that aspect ratios are a perfectly valid creative choice . More and more TV series these days are bouncing back and forth between 1.78 , 2.00 , and 2.39 ... just because they can . <p> I just watched American Gods and noted they tend @ @ @ @ @ @ @ @ @ @ the past , then 1.78 for contemporary scenes . It 's an interesting idea , and the show is so good , I do n't give a crap what the aspect ratio is . <p> But if it were up to me and a project was going to be seen primarily on TV and/or the net , I would always prefer going to 1.78 simply because you then get the biggest possible images possible in the format . 2.39 sounds very cool and " cinematic " until you have to watch it on an iPhone or iPad. 
@@44332992 @4332992/ <p> The biggest benefit is flexibility in the grade . Like Ola said , scaling is one thing , but if you change your mind on your white balance for certain shots or scenes , you 'll either be SOL , or need to go and deal with reconforming . <p> It 's always good to stick the with R3Ds if you can afford it ... But if speed and storage is limited , you might want to think about doing a first light pass with the R3ds ( either in a grading application of RedCineX ) and export THAT out to ProRes ... Then at least you will get a bit of leverage out of the raw files before flattening them out . <p> Contrary to what Red wants you to believe , there is n't some huge magical benefit to staying in raw aslong as you 've nailed the exposure , WB and tint prior to transcoding your footage . <p> When grading R3Ds , at some stage the raw data is getting debayered into RGB anyway , it 's just happening in realtime . If you @ @ @ @ @ @ @ @ @ @ same settings and assuming the codec you are using is up to the task , the results should be identical . <p> With regards to Prores 4444 - from a processing point of view it will be less taxing on your system . From a storage point of view you 're unlikely to see much savings in files sizes , but obviously that depends on the resolution of the Prores file . Also depending on what resolution / compression ratio was used for the R3Ds , you might actually end up with files that are larger than the original R3D footage . <p> One point worth considering is that Red RAW files contain multiple different exposures of RedLogFilm ... So , for example , if the files are used in a ' linear ' ( I prefer the term ' video ' ) floating-point environment you 'll actually have more highlight and shadow detail available to you than in an 800 ISO RedLogFilm . You 'll have the highlights from the 50 ISO and the shadows from the 2000 ISO ... HDR style . <p> Admittedly at the very top @ @ @ @ @ @ @ @ @ @ but still a very useful option . <p> I suppose you could export all of those exposures as RLF and blend them ..... 
@@44332993 @4332993/ <p> Is the 30 day trial code a limited time offer , or can we use it at a later date as well ? I currently am busy till the end of the month and have no VR footage in hand to play with . Would prefer if the free trial option was open for later as well . <p> Can we expect linux support ? I 'm planning to move our facility away from windows because EOL support of Win7 and Win10 being the disaster it is . I 'm always on the lookout for great software which is affordable for a small shop and has a linux version . <p> Unfortunately the 30 day trail is a beta version of the product so it is for a limited time , however it 's 30 days from when you sign up . We will likely keep the beta sign up open for another week so you will hopefully be able to take advantage of the free beta trial . <p> For the moment Mistika VR is available on just Mac and Windows , however Mistika Ultima runs off @ @ @ @ @ @ @ @ @ @ up these webinar training series ? I like his style of training . 43971 @qwx453971 <p> Hi Kevin , <p> Thanks very much for the kind comments . Much appreciated . Unfortunately , after being a loyal customer and supporter of SGO for 20 years , I am no longer associated with them in any way and am no longer a user of their software . There were a series of events during 2015 and 2016 , as well as some specific text messages received from SGO 's Geoff Mills at IBC 2016 , that very sadly led me to conclude I could no longer endorse SGO . I remain very fond of the core people at SGO in Spain though . <p> Hi Julian , There are two days left to sign up for the Beta Licence . https : **33;1901;TOOLONG If you use the code GOPLAYVR17 it will generate an activation licence link . The actual release is scheduled to go live in the next couple of days . Thanks , Tanya <p> Hi all , Mistika VR is now live and ready to download . Here 's @ @ @ @ @ @ @ @ @ @ discount code LGG25 you 'll get 25% off your first month . https : //www.sgo.es/vr-plans/ As always , any questions please let me know . Thanks , Tanya <p> It seems to me that nowadays you have to be interested in Mistika VR or you will not get a response from SGO at all . At least all my enquiries ( made through various communication channels ) about non-Mistika VR things have resulted in a big silence . <p> Hi guys , Apologies things may have seemed to have gone a little quiet . Mistika VR has been a huge push for us with releasing the software a few months ago , however Mistika Ultima is still very much the core of our business . Updates to the open version of Mistika Insight will be coming very soon as we implement the feedback we have received from the beta phase . If you need any additional info , please respond or email me at twalker@sgo.es Thanks , Tanya 
@@44332994 @4332994/ <h> Technicolour 3 strip technique <p> We are working on the technicolour strip look technique on a Baselight one with the Slate . Only having some struggles with channel splitting and combining . To explain how far we are now : <p> We are being able to create a stack with B&amp;W-R channel , B&amp;W-G channel and B&amp;W-B channel by adding them trough the Layer mode : Grade Blend source and choosing the second input . Then adding a Layer above each of them and change them to Cyan , Magenta and Yellow trough the colour matrix . <p> The only thing we are getting stuck at is how to mix these 3 layers together all equal . Like a parallel does in Resolve . We tried Blending modes and we able to mix B&amp;W-R with B&amp;W-G and then this result with B&amp;W-B but this does n't result in a desired result . <p> The Technicolor- looks each simulate one of the typical Technicolor processes . These processes have very vivid reds , which may go out of gamut " you may wish to reduce the saturation . Process @ @ @ @ @ @ @ @ @ @ projector and the camera . Process two simulates a Technicolor look using two dyes , cemented face to face . There were many choices of dyes : the orange-red and blue-green used here are similar to the colours used in Ben-Hur . This gives odd-looking skies , and salmon-pink tones for flesh and blonde hair . Orange and green dyes were also used " these gave better flesh tones , but could not achieve a mid-tone grey . The concentration and mixture of the dyes were varied to suit the material " something that can not be done with the modern processes . However , we can vary the contrast control to simulate the variations in dye density , and the white hue to give a convincing balance for the flesh tones . Process four simulates a Technicolor look using a three-strip camera ( the response of this to typical subtractive colours has been estimated ) , and three dyes . Three azo dyes have been chosen , which probably represents the 1950 reprints of films such as The Wizard of Oz , rather than the original prints , which @ @ @ @ @ @ @ @ @ @ from one film to another . The process also had a light grey exposure of the green channel in the black and white film base , which also carried the keylines and the soundtrack . Process five simulates a Technicolor look from the print off the Eastman tripack negative . The exposure in a typical negative material has been simulated . The before ' colours are a simulation of a Vision print , and the after ' colours are the simulated three-colour print . The grey exposure was usually omitted in process 5 . <p> Thanks , only the problem is not the actual look but the technique we were using .. We spoke with some people and the solution is not using this technique . It 's not really possible to mix 3 layers together ... <p> Thanks , only the problem is not the actual look but the technique we were using .. We spoke with some people and the solution is not using this technique . It 's not really possible to mix 3 layers together ... 43971 @qwx453971 <p> Were you able to find the look @ @ @ @ @ @ @ @ @ @ definitely send it in to them . The feedback might help them improve it . <p> Thanks , only the problem is not the actual look but the technique we were using .. We spoke with some people and the solution is not using this technique . It 's not really possible to mix 3 layers together ... 43971 @qwx453971 <p> I 'm not shure if i understand your approach , but i think it should work with add as blend mode . The order does n't matter with this mode , but you need to do it sequentially . Moreover I think there will be no benefit from the transfer to ymc because your real primaries are fixed . It 's possible to simulate the process in rgb and in fact you could get the main effect ( popping primaries , ) with only one colour matrix layer . <p> So what we tried to do is this tutorial for Davinci Resolve : LONG ... And like you said Knut , the solution was doing this effect only with one colour matrix layer was the sollution to get the @ @ @ @ @ @ @ @ @ @ the effect is quite strange and does n't represent a Technicolour look . But it was fun to figure some nice things out . <p> I agree , sometimes going through the steps to create a complex look can be interesting and educational . But I continue to believe that a true Technicolor look is more than just multiple layers . It helps to have access to real Tech IB prints and screen those , then make notes on how it actually looks . There are qualities there that no digital cameras can reproduce ... but you can at least come up with a suggestion of a vivid , colorful 1940s look , and I do n't think it requires 20 nodes , layers , and keys to do so . <p> It 's interesting to note that in the original 3-strip Technicolor camera , they exposed the green layer of B&amp;W negative ( which had a lot of the luminance values ) separately , then bipacked the blue and red layers of B&amp;W negative on top of each other . This is one reason why blue and red were @ @ @ @ @ @ @ @ @ @ ( And thanks to DP David Mullen for alerting me to this diagram . ) Technicolor eventually figured out that they needed to add a 4th layer in the printing process , which was a higher-contrast B&amp;W version of the green image , just to sharpen the details and enhance the blacks . It 's a very interesting story of how Technicolor worked , though some of the inside information is still secret to this day . Even though I casually knew some of the Tech lab guys , they would never divulge -- as one example -- how they could make an IB print at high speed and still keep it registered . <p> Today , if anybody ever asks me for a " Technicolor " look , I just crank up the saturation by 35-50% and pull back on the fleshtones with curves or keys . But I also warn the DP in advance that the lighting in Technicolor productions generally had lots and lots of fill , because they were basically shooting at an ISO of 50 . They were lighting at about 600 footcandles on the @ @ @ @ @ @ @ @ @ @ were cooked for 10 hours a day , under heavy makeup . <p> @Nick - Same results here and I 've tried every colorspace I can think of . The only way I could get a ' usable ' look from that matrix is to apply it in ACES Proxy then use the ACES Proxy transfer function ( 1D ) to get back to linear followed by the Rec1886 curve but even that does n't look right . I 'm not enamoured with Baselight 's Technicolor looks TBH . <p> Apologies in advance for my technical ignorance/naivety , but would it be possible to extract matrix values from a list transform ( or even a 3D LUT for that matter ) ? Quite probably not , but no harm in enquiring . <p> Apologies in advance for my technical ignorance/naivety , but would it be possible to extract matrix values from a list transform ( or even a 3D LUT for that matter ) ? Quite probably not , but no harm in enquiring . 43971 @qwx453971 <p> The answer is yes and no . You can extract matrix values @ @ @ @ @ @ @ @ @ @ often it 's much more than just a matrix transform . There are also process for doing a " best solution " matrix transform that approximates an effect . And there are a variety of adjustments which simply ca n't be done with a matrix . Matrices are inherently limited . <p> Thanks Juan . Do you know if it 's possible then to tell if the LUT or List is confined to just a matrix transform , or do the extra variations get hidden in amongst the code ? It would be cool to be able to reverse engineer the effect into it 's separate core ingredients . <p> Apologies in advance for my technical ignorance/naivety , but would it be possible to extract matrix values from a list transform ( or even a 3D LUT for that matter ) ? Quite probably not , but no harm in enquiring . 43971 @qwx453971 <p> A list transform is just a variation on a 3D LUT , being a list of input triples and output triples . They just do n't need to be in any particular order , or @ @ @ @ @ @ @ @ @ @ that a list transform/3D LUT is able to be represented by a matrix sandwiched between a pair of 1D LUTs , and you know what those 1D LUTs are , then you can derive the matrix . But many transforms are non-linear and can not be represented that way . <p> Yeah , if you would try this technique in Davinci Resolve by following the tutorial it will come out with some kind of the same result . Also to note that this is the look I was looking for with the reverence tutorial . This is n't a proper Technicolor 3 strip look at all ... Also what I understood is that the Technicolor look from Baselight is a start point from where you adjust to own taste . 
@@44332995 @4332995/ <p> How much lines are or are n't curved should mostly depend on hof far from the screen you are standing . For the person working on the screen it should be okey but for the person that 's just walking by and making a quick rewiev of your work form 2 meters IMO it could be a problem . <p> I have a 34 " ultrawide that 's curved , which I love for my day-to-day which is mostly compositing and graphic design . Its not bad for resolve but I prefer a 27 " normal screen for GUI with a flanders and a scope monitor . I 'd also say that 38 " is just too big , and that photo with two of them side by side for editing in Premiere would be so incredibly uncomfortable . The 34 " ultrawide pushes the flanders too far to the side for long day of grading . <p> The curve is essential for a 34 " though , I do n't notice any curvature of lines ( which is a ridiculous reason not to get a curve ) @ @ @ @ @ @ @ @ @ @ that someone is walking by and points out that something is bent ( again , so ridiculous to ever worry about something like this ) . I 'm sitting arms-length away from this beast so the curve just feels right . With a 38 " I 'd assume you 're sitting further away ( unless you hate your neck ) so its probably less important . 
@@44332996 @4332996/ <p> Tero i , m with you i need to see this before i make a judgement the images do look stunning .... mind you the storyboard they had to work from eg the original movie was some of the best anime i have ever seen the fight sequence and chase with at the beginning ... it 's not the foreground you look at it 's the background detail &amp; colours ... amazing <p> so they really have the chance to do something beautiful <p> but on the other hand this could go badly wrong i know there has been a massive upset with the casting in certain quarters <p> Off course not . I 'm just going from what I 've heard , read , seen so far . As it currently looks it 's a coming of age , searching my past , character arc . Which points to a typical holivud restorative classical paradigm . I do n't blame them for doing this and I did n't expect anything else actually . It 's just really disturbing looking at this as fan of the original . @ @ @ @ @ @ @ @ @ @ she is a scared , left in the dark character . I 'm just talking about the script and nothing more . Production design looks nice and so does everything else , beside some obvious green screens ... One more thing that to me was better in the original is the android body design . There they had living muscles combined with metal bones . That really helped make everything a lot more visceral . Here it looks like they went for a more " Ex machina " type of look . Which is just another holivud play it safe move - IMO offcourse . Also , I can not agree less with the option for the main character . <p> And I ca n't believe what became of this now . Just a nother blockbuster ... 43971 @qwx453971 <p> I 'm not judging story or characters -- I 'm just looking at the live-action images in the trailer and saying , " wow , this is a very striking picture . " Nothing more or less than that . <p> There are all kinds of movies that present great @ @ @ @ @ @ @ @ @ @ that have terrible stories and characters . In this section of the forum , we 're only talking about how things look . <p> To be honest I do n't care , I just do n't think scarlet fits in this role If the story world is going to be in Japan , than yeah , it 's kinda stupid to have a white lead . But still ... Scarlet IMO does n't belong in this role . <p> You 're gon na laugh , but I saw Benedict Cumberbatch in Dr. Strange the other day and thought , " I would absolutely believe this guy as James Bond . " I think he 'd have to gain about 25 pounds of muscle for the part , but it could be done . <p> I have liked Idris Elba very much in several parts he 's done , but I think he 's miscast in Dark Tower , and I do n't see him as James Bond . <p> The film looks/artwork leans a bit more to Hajime Sorayama , but there has always been a huge lean to western @ @ @ @ @ @ @ @ @ @ this film will introduce some people to the great works in anime which could bring them to films like " grave of the fireflies " ( ton 's of the great films are barely known in the west ) . <p> On the arri65 front , I have been following that entire workflow . Starting at the lens and going all the way through final grading I have been hugely impressed ( the arri 65 rental group that is involved through the entire process I find amazing ) . It 's like when the director starts with a arri 65 , that commitment to excellence follows through on every step , which I think is very visible &amp; appreciated by the audience . 
@@44332997 @4332997/ <p> It would take a big 5 minutes for the ceo of apple to call up the ceo of hp or boxx , say " hey ... could you create a branded hackintosh for us , we will make sure the I/O drivers will be updated for whatever you need to have dual nvidia 1080 . " The heat issue is bogus , would take them 5 minutes to change the code so rpm of fan default is 800 rpm and ramps to 1900 rpm when cpu peci temp is at 120F - they do n't want to do this because the trashcan sounds like a hair blower . <p> I think this is a good distillation of the facts from the meeting Schiller and Federighi had with the handle of tech reporters on this . 43971 @qwx453971 <p> Definitely one of the better articles I 've read today , especially in the face of others , which as they state were long-winded exposes ' about nothing . I 'm excited a new Mac Pro is on the way , and hope Apple understands the current model is not @ @ @ @ @ @ @ @ @ @ of sorts , though . We 've been coping with out-of-date Apple hardware for a long time , only to be told we have to wait yet again , and hope they actually get it right somehow . <p> I do n't depend on Macs like I used to , but on an emotional level , I 'm not sure I can take it anymore . <p> So many seem to struggle about what a ' pro user ' is . I see the problem since they tend to think of professions and ask themselves : is this pro ? Is that pro ? But that is a very backwards approach . It is in fact very simple : <p> It 's not a question about pro user , but rather about ' pro computer ' user . MacBook Pros and iMacs can cover lots of needs today . Nice , high resolution screens , fast storage , entry-mid level processing/compute power and basic expandability with fast i/o . The Mac Pro needs to be the catch all . And THAT is in fact the definition of that computer : @ @ @ @ @ @ @ @ @ @ MBP or iMac . Yes , those needs are diverse . Some want massive internal storage , some lots of CPU while yet others want unlimited GPU power . Instead of making one ultra expensive Mac Pro , they just need to fall back to that time honoured concept of classic PC modularity and let users largely build their Mac Pro to order . <p> Apple , let the 4.1 and 5.1 Mac Pro be your inspiration and just do a modern take on that . It was beautiful and efficient . <p> What still mindboggles me is that a multi billion company is so daft it does not even recognise a mac pro users actualy want if it bites them in the *** One look to the internet ( which they actualy mentione dthey did ) and they must wonder why certain users in 2017 still prefer buying an old 2011 mac pro model and pimping it to the max against some still relative hefty pricetags , and not a 2013 mac pro . As mentioned above it takes a company that size with such resouces no more the @ @ @ @ @ @ @ @ @ @ the same succefull concept and maintain the unique recognisable mac look and not turn into a plastic anonimous pc . Win win i would say . Simple but ellegant and hyper powerfull/customisable and usefull ( let 's focus on that for a change ) against little development costs . But its probably going to me a mac pro watch so pros can wear it on their wrist while rendering 6k , they all wanted that according to their investigation of all pro 's . Sigh .... So happy i went the hackintosh way and not looking back anymore .... <p> Uhhh ... reading about all of this icky desperation makes me glad I never invested that much into Apple and Eve 's walled garden of sin and entropy. 43971 @qwx453971 <p> I in turn hoped that with the death of Mac Pro The whole Prores Dance will stop . Apple will either be more generous in licencing or It will just die a natural death . I gues - the dance will continue now indefinetly . <p> The nMP 2013 also got a massive price-cut today making them way more affordable @ @ @ @ @ @ @ @ @ @ today though . In 3 years things have happened there , to say the least . 43971 @qwx453971 <p> Has AMD even introduced any workstation graphics in the past 3 years ? The W9100 is 3 years old with a mid-life ram bump . The WX cards do n't seem to replace the W9100 either ? I think Apple backed the wrong horse going AMD in recent years . <p> Has AMD even introduced any workstation graphics in the past 3 years ? The W9100 is 3 years old with a mid-life ram bump . The WX cards do n't seem to replace the W9100 either ? I think Apple backed the wrong horse going AMD in recent years . 43971 @qwx453971 <p> Nah ... the AMD Radeon Pro WX 4100 , 5100 , and 7100 were just phase one . Phase two WX replacements for the remaining series is right around the corner ... and they will arrive way before a new Mac Pro does ... he he . <p> It would take a big 5 minutes for the ceo of apple to call up the ceo of hp @ @ @ @ @ @ @ @ @ @ a branded hackintosh for us , we will make sure the I/O drivers will be updated for whatever you need to have dual nvidia 1080 . " The heat issue is bogus , would take them 5 minutes to change the code so rpm of fan default is 800 rpm and ramps to 1900 rpm when cpu peci temp is at 120F - they do n't want to do this because the trashcan sounds like a hair blower . 43971 @qwx453971 <p> From Apple 's point of view , that 's like saying , " Hey , drop off 12 year old kid from wherever , we 'll call him Mike and pretend he 's been our kid all along " . <p> I think we all want a slick box with where you can swap out and/or add GPU 's &amp; SSD 's . I 'll give up on swapping out ram ( 32/64GB should be fine for a few years ) or CPU 's , but if it 's a nice box that has a solid design , that would be great . <p> I really really hope @ @ @ @ @ @ @ @ @ @ When we were waiting for the 2013 model , I wondered if they asked potential buyers what they were looking for in a new MacPro . " Small , sleek , and stylish " would not have been on the top 10 for 99% of their user base - powerful , customizable , and expandable are what we need - now and always . Pay attention Apple ! Last chance , or you 'll lose the creative class forever ( if you have n't already ) . <p> It would take a big 5 minutes for the ceo of apple to call up the ceo of hp or boxx , say " hey ... could you create a branded hackintosh for us , we will make sure the I/O drivers will be updated for whatever you need to have dual nvidia 1080 . " The heat issue is bogus , would take them 5 minutes to change the code so rpm of fan default is 800 rpm and ramps to 1900 rpm when cpu peci temp is at 120F - they do n't want to do this because the trashcan @ @ @ @ @ @ @ @ @ @ have to crank my fan up to 1400-1500 just for editing in Resolve . Renders , I max out the fan . Not for long ... the re-build of my custom PC build will be back next week . I 'm done with Apple . <p> Article misses a couple of things , but makes one really important note . <p> The only thing apple actually announced is that they wo n't ship anything this year . <p> That 's a pretty worthless announcement other than to confirm : 4 years after a horrible product and 8 years after the last time they really upgraded a great product , they still have n't figured out what to do . 
@@44332998 @4332998/ <h> F65 Anamorphic in Premiere Pro CC <p> currently editing a new short film in the latest updated Premiere 10.3 . <p> The short was shot on Sony F65 Raw Lite at 4096 x 2160 2x Anamorphic lenses . Now it records that onto its entire 16:9 sensor unlike Arri where it records at 4:3 . So when I de-squeeze it looks ridiculously wide . <p> Generally when I cut Anamorphic Arri , it 's shooting aspect ratio once de-squeezed is 2.66:1 ; as the wings are to be clipped off , I have no problem dealing with this in Davinci as it has input and output sizing settings . And the same when I pull in the F65 footage , I clip off the wings with the same procedure . <p> However when in Premiere , editing the Arri full 2.66:1 desqueeze does n't feel abnormal as its close enough to 2:39.1 and the wings are n't that obtrusive . But using the F65 does ; here is what it actually looks like with its PAR set correctly to 2:1 . <p> Basically only half of the width @ @ @ @ @ @ @ @ @ @ setting my sequence settings to 2048 x 858 or 4096 x 1716 for a 2.39:1 aspect . Premiere can not unfortunately have a distinctive input/output scaling , as in tell it to " fit to frame with crop " so the top and bottom of the footage 's frame is flush with the height , thereby cutting off the wings . It has Scale to Frame Size which by default fits everything in . The only solution is to manually resize in motion down by 80% - only issue I have with this is that it 's an Absolute resizing which I do n't want to paste to all the shots then reset it all before I go to grade as I do n't want to carry on over those sizing values . <p> Now obviously one creates Proxies and crops the wings off the be the right aspect ratio of 2.39:1 - I have Proxies for this current project of the F65 however I would still like to know if anyone here has worked out a method to use the F65 Raw in Premiere effectively or know of a @ @ @ @ @ @ @ @ @ @ . <p> Open any of the source clips in Source Monitor , switch to Effect Controls - master effects should be active . Now drop the Distort-&gt;Transform " effect there , and scale down to 80% . Then select the Transform effect in the Effect Controls panel , copy it ( ctrl/cmd+C ) , select all other ARRI clips and hit ctrl/cmd+V for paste . <p> Now the original resolution of the clip remains the same , and the effect should not translate to XML when you later export it to Resolve . <p> Open any of the source clips in Source Monitor , switch to Effect Controls - master effects should be active . Now drop the Distort-&gt;Transform " effect there , and scale down to 80% . Then select the Transform effect in the Effect Controls panel , copy it ( ctrl/cmd+C ) , select all other ARRI clips and hit ctrl/cmd+V for paste . <p> Now the original resolution of the clip remains the same , and the effect should not translate to XML when you later export it to Resolve . <p> Is that what you @ @ @ @ @ @ @ @ @ @ I think so , let me give it a try , thank you very much , I 'll get back to you with my results . 
@@44332999 @4332999/ <p> Basically , the screen has a fixed maximum output that 's dependent on the content : if it 's displaying a full-screen white solid color , its max brightness is somewhere around 120 cd/m2 , if you 're displaying a small patch , say 25% of the display , the max brightness is way , way higher . <p> The example problem I 'm afraid of is this : say I 'm adjusting the luminance of the content in a window , and raise it significantly . Suddenly , the shadow detail dims , even though no adjustment is being made to that part of the image . I did a test where I took a dark image and put a big circular power window in the middle , and adjusted it to white , and sure enough , the shadow detail in the rest of the image dropped noticeably . <p> This happens even if the overall brightness ( " OLED light " in the menus ) of the display is lowered down to 100 cd/m2 peak . Am I crazy for thinking this is a fatal @ @ @ @ @ @ @ @ @ @ also seems to make it impossible to calibrate using XRite 's i1Display Pro software or DisplayCAL . <p> Does anyone have a similarly priced ( $2K ) suggestion for an alternative client monitor ? I 'm thinking about just taking this unit home and swapping it with my Panasonic plasma , as our studio does n't finish in 4K anyway . <p> That is why the light level specs usually specifies brightness at 10% of the screen . No monitor is capable of delivering 1000 nits at full screen . It 's not really that different from ABL on plasma monitor . <p> But , it 's there for a reason as an OLED is even more susceptible to burn-in than a Plasma , so make sure not to leave a bright , still image in place for more 60s or so . <p> There are n't a lot of monitors similar in price that will do much better . An LG E6 has similar issues ( green tracking , black non-linearity , rapid loss of peak brightness over time ) as a B6 , they 're just less severe @ @ @ @ @ @ @ @ @ @ specs , they have off-axis issues that make them difficult to use in post unless you 're the only person in the room . <p> My thought would be to hang in there by turning off ABSL for now , calibrate with a 3D lut if you can , and then wait until next year when the next round of OLEDs arrives . They will be a better long term investment . Anything you buy now will be obsolete as soon as the new models appear . <p> Thanks for the suggestion , but the link describes disabling anti-burn/power save dimming that dims the picture when the content is idle " the problem I 'm describing is different . The entire display dims if any part of the frame becomes bright , whether the content is in motion or not . <p> Thanks for the suggestion , but the link describes disabling anti-burn/power save dimming that dims the picture when the content is idle " the problem I 'm describing is different . The entire display dims if any part of the frame becomes bright , whether the content is @ @ @ @ @ @ @ @ @ @ and I returned it after a week or so , - mostly because of the black noise and because I did n't wanted to enter in the service menu , and the dimming looked awful in every bright scene , - and bought the Panasonic DX900 . The view angles are not that great , but since there is never more than 3 client in my room , and they can sit in front of the screen , it 's not really a problem . The Panasonic calibrates really well , you should give it a try if you need a client screen now . If you do n't , wait for the next year model . <p> That is why the light level specs usually specifies brightness at 10% of the screen . No monitor is capable of delivering 1000 nits at full screen . It 's not really that different from ABL on plasma monitor . 43971 @qwx453971 <p> Actually I just bought a Panasonic DXW904 few weeks ago and to my surprise it reached a 1000 nits easily full screen white . I will have to confirm @ @ @ @ @ @ @ @ @ @ way with it but I thought it reached 1100 nits . <p> My thought would be to hang in there by turning off ABSL for now , calibrate with a 3D lut if you can , and then wait until next year when the next round of OLEDs arrives . They will be a better long term investment . Anything you buy now will be obsolete as soon as the new models appear . 43971 @qwx453971 <p> Rubbing my hands with glee at the thought of buying a 3-month old " obsolete " used 4K display in March ... <p> Actually I just bought a Panasonic DXW904 few weeks ago and to my surprise it reached a 1000 nits easily full screen white . I will have to confirm this somewhere next week because I was still finding my way with it but I thought it reached 1100 nits . 43971 @qwx453971 <p> Did you measure it with a probe ? The reviews confirmed full screen white to be just under 900nits , still impressive but not 1000nit . <p> I just had a chance to measure the DXW904 again @ @ @ @ @ @ @ @ @ @ ranging from 10% white patch to full screen 100% . The numbers are referring to the patch size not the IRE level . Probe was i1 Pro , software Lightspace. 
@@44333000 @4333000/ <h> Getting a 3:2 pulldown look <p> I have a show that was mostly shot in 23.98 , that is finishing in 29.97 . There was some DJI Drone stuff natively shot at 29.97 . I am doing the Conform , Online Edit , and the Packaging/Mastering . <p> I thought I was being clever and cut the native 29.97 material into the 29.97 delivery project . The Director hated it and thought it looked too video . In the end , I took the 29.97 native stuff , re wrapped it to 23.98 , and used Premiere 's Frame Blend to get it into 29.97 . The result is not great but the Director thinks it looks more " film like " . <p> I 'm a big fan of re-wrapping media for standards conversion , but it is n't always a great solution . For instance in some of the drone footage we have moderately fast pans , and they look studdery in 23.98 , but look great in 29.97 <p> Are there any plug-ins that will process 29.97 media and give it that " film " @ @ @ @ @ @ @ @ @ @ The bottom line is that going from 30p to 24p ( and then to 60i ) is never going to look good . DJI should handle recording 24p just fine so that is too bad they did not do that if they would have wanted that film look . <p> If this going to be broadcasted in 60 the best option in my opinion would be to telecine the 24p to 60i and faux interlace the 30p DJI footage to 60i . <p> That might also get problematic for international deliveries . If you go from **33;1936;TOOLONG it 's going to get pretty mangled . <p> Is the footage interlaced ? Sometimes something as simple as deinterlacing can help get rid of that video feel . You 'll be cutting the resolution , but that might be more preferable than doing multiple frame rate conversions . I 'd try discarding one set of fields entirely and doing an interpolate instead of mixing the fields . <p> If the bulk of the project is 24p , and you want a film look while delivering 29.97 - do all of the conform @ @ @ @ @ @ @ @ @ @ , and then render out a 24fps master for archive &amp; posterity , and then export a 29.97 with a 3:2 pulldown added . Resolve can now do that on the deliver page - just choose that export frame rate . This , of course , does n't solve your drone original 29.97 footage problem . This is going to depend on the shot content ... 90% of the time , the best option is to treat the footage as if it were 24p ( " reconform " in CinemaTools or tell Resolve to read it as 24p ) - it will play slightly slower , but for most drone shots I 've come across ( high , wide establishing shots ) , it 's hardly noticeable , and it will blend with the other 24p footage very nicely . For fast motion and whip pans , etc ... - yeah , 24p " film " will always have that stuttering problem - I 'll sometimes try to minimize it with adding some horizontal blur . Beyond that , not much can be done - at least until we go @ @ @ @ @ @ @ @ @ @ forays into 48 fps and higher - I 'll be honest and say I HATED The Hobbit in 48fps . I 'm curious to see Ang Lee 's 120fps , but I do n't think that I 'd enjoy it . I 'm waiting for someone to use HFR selectively as a creative tool . The bulk of the movie could be 24 fps , and then certain action scenes could be done at 48 for a creative effect . It will make our jobs more complicated for sure , but could be very interesting . <p> If there is no sound that needs to sync to that shot , as most aerial drone shots in my past projects did n't , I just flag it in Resolve as 23.98 , it 'll play slower , but most directors liked that better actually , it looks more majestic somehow . That of course if the scene allows for that decrease in speed in that shot , if it 's a car chase scene it probably wo n't , but does n't hurt to try . <p> " Standards conversion relies @ @ @ @ @ @ @ @ @ @ that did not exist in the original material , either because you need extra or less pictures . DVO Twister is a fully motion compensated standards converter with option for converting between the following frame rates : 16p , 18p , 20p , 23.98p , 24p , 25p , 29.97p , 50p , 59.94p , 60p , 50i and 55.94i " also included is our 3:2 pulldown with automatic broken cadence repair . " <p> And if that is too fancy , there is a simpler DVO Three Two , that does the automated 3:2 removal with advance broken cadence detection and repair . And finally , for problem working with interlaced images , you can use DVO Deinterlace . The de-interlacer can maintain the fluid motion present in video ( e.g. 50p delivery from a 50i source ) or create a film look ( e.g. 25p delivery from a 50i source ) . 
@@44333001 @4333001/ <h> Apple Watch 2 <p> Just saw this on Youtube and thought it was a nice take on creative grading with obvious and well done coloring separating and / or combining the scenes , what do you guys think ? A lot of work went into masking the silhouettes too , I would assume . <p> I was the colourist assistant on this grade here in Paris . The colours were present on set , the colourist had to match them and make them pop out . Lots of VFX and flame . For all the product shots I had to import four mattes so that the colourist could grade separately each part of the watch . 
@@44333002 @4333002/ <h> Nucoda &amp; AJA 4k - ReadRegister Failed <p> Hi all , Despite uninstalling and reinstalling the AJA drivers , and switching between 4k and UFC firmware , I ca n't get rid of the AJA ' readregister failed ' error that pops up every now and then - the card is then offline and only a reboot brings it back . Ideas or help , anyone ? <p> I have never heard of that error before , and there are n't many mentions of it on the web . One thread describes it as being a driver issue , but I know you 've tried a lot of different combinations already . When it gets this ambiguous , the best way to cut to the chase is to call AJA . They 'll probably know exactly what the problem is. 
@@44333003 @4333003/ <p> I agree , lots of interesting things there . The one that made my jaw drop is that he color-timed the entire project in 7 days ( ! ! ! ) . Storaro does n't screw around . 43971 @qwx453971 <p> No , he does n't . But it also helps that Anthony did both the dailies and the DI , and got constant feedback as the shooting progressed . When you get it right in dailies , and you 're the same colorist doing the DI , it shortcuts a lot of things . Unfortunately , it 's also a rather unusual arrangement . <p> It also helps that Anthony is a really great guy and a talented colorist . <p> That 's a very inspiring article . I btw do a push/pull on blackshading on RED to get some of the ASA concepts Vittorio was talking about , theoretically you could do the same thing with the alexa double gain but I do n't think normal people are allowed to mess with that . <p> Long time lurker , first time ( hopefully not too ) @ @ @ @ @ @ @ @ @ @ in this article . But it is perplexing to me why Woody Allen has n't had a tougher time getting talented people to work for him . No one seems to give Dylan Farrow enough credit and / or respect . I 'm sure most people would n't want to hire Bill Cosby ... but why are A-listers so eager to still work with Woody Allen ? <p> That being said , this is a fascinating read .. but , I 'm sure for many of us , it feels odd supporting this filmmaker . <p> Long time lurker , first time ( hopefully not too ) controversial poster here ... I really respect the knowledge shared in this article . But it is perplexing to me why Woody Allen has n't had a tougher time getting talented people to work for him . No one seems to give Dylan Farrow enough credit and / or respect . I 'm sure most people would n't want to hire Bill Cosby ... but why are A-listers so eager to still work with Woody Allen ? 43971 @qwx453971 <p> My observation is @ @ @ @ @ @ @ @ @ @ 's work from the artist as a human being . There are actors , directors , producers , and rock musicians that I 've worked with who are incredible jerks in the real world , but it does n't detract from the fact that they 're very successful and talented in their chosen professors . That does n't mean I have to like them or want to pal around with them . I can enjoy a Woody Allen film without being repelled by his moral choices . I also do n't know enough about what really went on in his life to be able to judge him either way . <p> On the other hand , I have no plans to watch The Cosby Show anytime soon . <p> I think he 's saying that in his opinion Mr. Allen is almost surely as bad or worse than Mr. Cosby . Whatever that actually means is in the eye of the beholder , of course . For what it 's worth , the news media largely threw Mr. Cosby under the bus , while Mr. Allen has enjoyed being admired @ @ @ @ @ @ @ @ @ @ do with how they are perceived by the general public today . <p> Mr. Allen is almost surely as bad or worse than Mr. Cosby . Whatever that actually means is in the eye of the beholder , of course . For what it 's worth , the news media largely threw Mr. Cosby under the bus , while Mr. Allen has enjoyed being admired and celebrated by journalists . This has almost everything to do with how they are perceived by the general public today . 43971 @qwx453971 <p> I think judging morality starts falling into the provence of politics , which ( to me anyway ) is beyond the scope of this forum . 
@@44333004 @4333004/ <p> ( London--April 3 , 2012 ) At NAB 2012 , FilmLight will underscore the central role colour occupies in file-based film and television workflows by demonstrating innovative solutions for seamlessly managing and manipulating colour at every stage in the production pipeline . <p> In an NAB first , FilmLight will introduce FLIP , a new , real time image processing product that let 's the creative production team design looks in pre-production and apply and refine them on the set . Employing a powerful , Baselight GPU renderer with Truelight Colour Management , FLIP can create and store an unlimited number of pre-set looks and spatial filters which can then be applied and refined in real time on set . Directors and cinematographers will be able to set exactly the look ' they want before shooting begins , refine it on set and save the grade so that it can be applied to dailies and other downstream processes . <p> At the next stage of the process we will debut our all new near-set dailies solution , Baselight Transfer . Baselight Transfer is a fully featured real-time , 4K @ @ @ @ @ @ @ @ @ @ cinema cameras , including Sony F65 , Arri Alexa and Red Epic . It is a production-proven solution that was recently used in an F65 4K ACES workflow for M. Night Shyamalan 's After Earth . Baselight Transfer also offers automatic colour matching between on-set grading data from FLIP and raw camera footage ensuring absolute integrity with the look ' captured on-set . <p> Our groundbreaking Baselight Editions make the world 's most powerful colour grading toolset directly available to editors and VFX artists and provide the tightest integration between grading , editorial and VFX available . Baselight for Apple Final Cut Pro 7 already makes this possible and new for NAB will be Baselight for Nuke and a preview of the much anticipated Baselight for Avid Media Composer . <p> We will also feature Baselight 4.3 , the latest version of our award-winning grading software . Baselight 4.3 has many powerful , new features and represents a major advance in terms of responsiveness and productivity . Paired with our Blackboard 2 console , it sets the standard for colour grading and finishing . Facilitating real-time , multi-layered colour grading @ @ @ @ @ @ @ @ @ @ data with Baselight Editions , Baselight 4.3 is grading without compromise . <p> Finally , we will have a technology preview of FLUX - our open , scalable data management platform . Specifically designed for the requirements of the post-production industry , this is data wrangling for today 's complex productions of 100:1 shooting ratios and 1000+ VFX shots . <p> About FilmLight FilmLight develops film scanning , colour grading and colour management systems that are transforming film and video post-production and setting new standards for quality , reliability and performance . The company 's products are in use every day by leading studios , labs and post-production facilities around the globe as essential components in their digital intermediate , commercials and video production pipelines . Fuelled by some of the industry 's brightest minds , FilmLight is committed to delivering innovative tools that allow creative professionals to work at the forefront of the digital media revolution . Founded in 2001 , FilmLight is headquartered in London , where its research , design and manufacturing operations are centred . Sales and support are conducted through regional service centres located in @ @ @ @ @ @ @ @ @ @ Rio de Janeiro , Sydney , Auckland and Singapore , and through qualified partners worldwide . For more information , visit http : //www.filmlight.ltd.uk. 
@@44333005 @4333005/ <p> This is n't a dig at Jake or the people behind 3D LUT Creator Pro , but I believe the use of 3D LUTs should be called into question here . Let me start off by making it clear that I think 3D LUTs are awesome , provided they are used in either of the two processes for which they are appropriate i.e monitor calibration and film emulation . Without them , the life of a colourist would be a great deal more difficult . The only practical way to emulate or reverse real world ( analog ) responses is to record them using specialised equipment , calculating the difference between source and target , and applying the transform to a fixed set of linearly spaced input values of a three dimensional look-up table . It bypasses so much potential difficulty that any computer would have in processing information that does n't comply with the strict rules of its virtual/digital self-contained world . So hurray indeed for 3D LUTs . <p> However , beyond those two stated examples I believe 3D LUTs have long been bypassed by other @ @ @ @ @ @ @ @ @ @ if the software application has limitations in this regard and a 3D LUT is in fact the only way you can apply certain transforms then alas you 'll have to make do with this process , but for most serious ' applications these days this should not apply . <p> One clear example of misuse of a 3D LUT is as a technical LUT to apply a transfer function . Even if you ca n't directly apply a transfer function to a signal , a 3D LUT is still a poor choice compared to a 1D LUT . A 65 point 3D LUT has 274,625 rows of values , yet its effective transform function is based on the input value rows that have the same value in all three columns , which is just 65 . A huge 8MB LUT that hits performance for so little return . To put this in perspective , the standard VFX IO LUTs in resolve are 12bit , so 4098 points to remap a signal . This is n't an arbitrary number , you need at least that level of precision to work with @ @ @ @ @ @ @ @ @ @ a bit further by building 14bit LUTs , before the latest versions of Resolve relieved me of that burden . 65 points ? All that interpolation ? No thanks . <p> Again , this is n't a take-down of 3D LUT Creator Pro , it 's a very interesting product and the math would appear to be sound , but its strict adherence to using the actual 3D LUT format as its means of output does kind of undermine it . It 's like a creatively talented and technically proficient musician who has the latest equipment and does amazing stuff , yet hands you his work on a cassette . Kind of a waste . If the makers saw fit to adapt their product so that it could work directly in an application , or even just export the accumulative algorithms that are applied to the template 3D LUT input values along with the actual 3D LUT , then that would make a big difference . I had my own custom methods of building LUTs in Nuke and Fusion ( and good ole spreadsheets ) but I can see the @ @ @ @ @ @ @ @ @ @ a lot of people . All it has to do now is get with the times . <p> Why would I take you response as a dig at me ? I have nothing to do with the product or the designer , other than common language and an immense respect of what he was single handily able to achieve Said that , I think the designer shows exactly the attitude , that with very few exceptions is lacking in today 's color software design . Yes , Resolve introduced LAB color spaces and FilmLight just introduced a brand new grading tool- BaseGrade . Other than than , there are hardly any new approaches to the color manipulation since it was introduced many years ago . It 's always the same- a key and the garbage mask . What Oleg does is fundamentally different . So , instead of trying to key something , that may not work well under all those circumstances , Oleg decides instead to choose an appropriate color space for a given task . Also , instead of simply manipulating individual hues and saturations , he introduces @ @ @ @ @ @ @ @ @ @ pr groups of colors , while excluding other colors and still keeping it all very visual . So , yes , his reliance on LUTs may be a bit outdated and the inclusion of his methods into grading software would be immensely better , I would not dismiss his approach out of hand just because of those shortcomings . I would never rely on 3d LUT Creator for creating technical LUTs . For me it 's a strictly creative tool . I think , even with some shortcomings , it is better to have 3D LUT Creator , rather than not ... And in case you 're wondering , a software manufacturer already did look at the possibility of inclusion of Oleg 's technologies into a color grading software ... <p> I was just making sure it was n't perceived as such . A simple matter of etiquette . I concur with most of what you 're saying , which is also consistent with my original post in that I praised the developer and software and reserved criticism solely for the use of the 3D LUT format , and that @ @ @ @ @ @ @ @ @ @ product that emphasises 3D LUTs as part of of its means of operation is being marketed at idiots . People who do n't know the math or the general pros and cons , and are attracted by the misplaced hype and deluded belief that there 's a quick and easy route to better images . Good for business sure , but ultimately a fraudulent and soulless endeavour . Given the genuine good work and creative ambition of 3D LUT Creator it would be a shame to condemn it to the same status as those miscreants . <p> On a side note , you may perhaps be downplaying somewhat the recent additions to Resolve with regards to image manipulation , in particular RCM , DCTLs , and Resolve OFX ( both the new additions and the custom capability ) . No more limitations . <p> I was just making sure it was n't perceived as such . A simple matter of etiquette . I concur with most of what you 're saying , which is also consistent with my original post in that I praised the developer and software and reserved @ @ @ @ @ @ @ @ @ @ , and that alone . Let 's be frank here , any recent product that emphasises 3D LUTs as part of of its means of operation is being marketed at idiots . People who do n't know the math or the general pros and cons , and are attracted by the misplaced hype and deluded belief that there 's a quick and easy route to better images . Good for business sure , but ultimately a fraudulent and soulless endeavour . Given the genuine good work and creative ambition of 3D LUT Creator it would be a shame to condemn it to the same status as those miscreants . <p> On a side note , you may perhaps be downplaying somewhat the recent additions to Resolve with regards to image manipulation , in particular RCM , DCTLs , and Resolve OFX ( both the new additions and the custom capability ) . No more limitations . 43971 @qwx453971 <p> At this point 3D LUT is one of the most universal frameworks supported by every software , so I can certainly see the attraction of using it right now . So @ @ @ @ @ @ @ @ @ @ then why even mention OFX ? GLSL shaders what you should be talking about . It 's an ultimate do it yourself plugin framework . Flame is all in on that one and so now is Baselight . I think Mistika is on board as well , but I 'm not sure on that . RCM is fine , but not really thinking out of the box , more of the " me too " , not quite the level of the TCS ( Truelight Color Spaces ) . OFX especially does n't give me woody , sorry ... <p> When you 're finished re-editing your post you should look up the thread on DCTLs and Resolve OFX . It is not how it was before . No more limitations . <p> Sent from my iPhone using Tapatalk 43971 @qwx453971 <p> That 's fine , but I prefer an open format , like GLSL . Once it 's written , it can run on any platform , that supports GLSL shaders . And these plugins mostly are sold for just a few dollars with many free . One of the @ @ @ @ @ @ @ @ @ @ how OFX can compete with that . I also run on multiple platforms , beside Resolve . <p> That 's actually a pretty decent source of potentially useful code . I reckon I could make a go of implementing some of the examples there in future plugins . To be honest , I find the new method of writing functions in either the Cuda or OpenCL kernels more intuitive than GLSL , but I guess as long as the process is mathematically tight and GPU utilisation is fully realised then it 's not the end of the world if you code one way or the other . <p> Ultimately every platform is built on code , functions , mathematical expressions and so on , which means in theory it is possible to replicate each and every capability . I can replicate Truelight Colour Spaces in its entirety ( with no performance or precision compromise ) in Resolve , just with DCTLs and/or custom OFX plugins . Pretty straight forward . Re-writing formulas or creating new ones altogether is where the actual challenges arise . <p> Then stop slacking off and @ @ @ @ @ @ @ @ @ @ on the GLSL bandwagon with Fusion/Resolve combo . Having the same code working on different platforms would be great . At least that 's how Autodesk is doing it now with Flame/Lustre " color connect " workflow . I can see it also being implemented on Nuke/Baselight platform as well . <p> Wow . This was such an informative conversation to me . I 've been using just the Lumetri effects in Premiere and creating LUT 's in Photoshop ( mainly for grading ) up until recently when I purchased 3d LUT Creator pro . <p> I 'm not married to it or anything ... I 'm trying to get a lot more serious as a colorist and -- obviously -- you guys are waaaay ahead of me . <p> I saw a lot of tools being referenced above ... For someone at my level , where do you think I should focus ? What is your " most trusted " tool that you use over and over again that is " with the times ? " <p> That 's actually a pretty decent source of potentially useful code . @ @ @ @ @ @ @ @ @ @ of the examples there in future plugins . To be honest , I find the new method of writing functions in either the Cuda or OpenCL kernels more intuitive than GLSL , but I guess as long as the process is mathematically tight and GPU utilisation is fully realised then it 's not the end of the world if you code one way or the other . <p> Ultimately every platform is built on code , functions , mathematical expressions and so on , which means in theory it is possible to replicate each and every capability . I can replicate Truelight Colour Spaces in its entirety ( with no performance or precision compromise ) in Resolve , just with DCTLs and/or custom OFX plugins . Pretty straight forward . Re-writing formulas or creating new ones altogether is where the actual challenges arise . 43971 @qwx453971 <p> Hello Paul , <p> Have you realized an OFX Plugin for Resolve which allows clean color transforms ( e.g. Hue vs . Lum , which does n't work well in Resolve ) like the ones in 3D LUT Creator Pro ? <p> Have @ @ @ @ @ @ @ @ @ @ color transforms ( e.g. Hue vs . Lum , which does n't work well in Resolve ) like the ones in 3D LUT Creator Pro ? 43971 @qwx453971 <p> I 'm not entirely sure about the appropriateness of using the word ' clean ' when discussing colour transforms and LUTs , but that aside I do understand why there is interest in applying the many interesting and complex formulas Oleg has developed directly inside Resolve . However , whereas my approach is based on direct application of transforms , 3D LUT Creator is fundamentally based around LUTs ( or more specifically the modification of 2D grids that are then converted to 3D data ) . It 's a very impressive approach , and Oleg 's software skills far exceed my own , but as he explained to me himself he primarily designed the app as a tool for photographers and users of photoshop , meaning it is beholden to to the LUT format . <p> I was approached with regards to developing an OFX wrapper for 3D LUT Creator , and although the offer was generous ( and the chat @ @ @ @ @ @ @ @ @ @ but also a very nice guy ) , I passed because ultimately I did n't believe in the point of the plugin in Resolve , and I ca n't commit to a LUT based process when I know that there are more accurate and efficient ways to process images that are already available , or feasibly will be in the not too distant future . <p> I 'm sure an OFX wrapper will be developed by someone soon , and folks will be using 3D LUT Creator in conjunction with Resolve and be more than happy with the results , and if and when Oleg chooses to divert his attention to a more direct application of image processing then I 'll be the first person in the queue of people hoping to work with him . <p> Thanks for your reply ! An OFX wrapper for 3D LUT Creator would indeed be interesting . But that 's not excactly what I was after . I 'm after a solution ( it might be via OFX ) for more accurate and advanced ways ( and for me " cleaner " ) @ @ @ @ @ @ @ @ @ @ I 'm looking forward to this solutions you 're a talking about <p> The latest 14 beta release may address your original example ( Hue vs . Lum ) . As for the other solutions , I 'm not sure you 'll find what you 're looking for in those plugins , but perhaps there are other OFX plugins by other developers that might . There 's a student version of Baselight out now . If you really are after a more accurate and advanced approach then you should give it a go . <p> The latest 14 beta release may address your original example ( Hue vs . Lum ) . As for the other solutions , I 'm not sure you 'll find what you 're looking for in those plugins , but perhaps there are other OFX plugins by other developers that might . There 's a student version of Baselight out now . If you really are after a more accurate and advanced approach then you should give it a go . 43971 @qwx453971 <p> As soon as I 'm out of a longer project , @ @ @ @ @ @ @ @ @ @ I 'm already registered , but do n't have the time to test at the moment . I know that Baselight is better in many regards . Thanks for your time 
@@44333006 @4333006/ <h> Which way to go with DCP creation <p> The Resolve plugin with EasyDCP , is fast and looks really good . One concern I have had with it is that I have had DCPs fail verification by Technicolor and other smaller shops . I have also tried a bunch of different DCPs with Clipster 's verification tool and never gotten one that passes . Given that , I have take the DCPs that fail and then loaded them onto a number of cinema servers and they play fine . Still makes me nervous though . <p> Yes , every time I made a DCP with EasyDCP Resolve , I was still " hoping it would work " rather then " sure it would work . " With a lot of clients that can be a stressful way to do a project . In the end we ended up just getting a Clipster , which to be fair is many many times the price of EasyDCP <p> Yes , every time I made a DCP with EasyDCP Resolve , I was still " hoping it would work " rather @ @ @ @ @ @ @ @ @ @ lot of clients that can be a stressful way to do a project . In the end we ended up just getting a Clipster , which to be fair is many many times the price of EasyDCP 43971 @qwx453971 <p> We 've had issues with Final DCP , but none with EasyDCP . <p> We 've had more issues with projectors and Windows based cinema servers not readying EXT3 drives than anything else . <p> Hi . Have anyone of you tried CuteDCP for Premiere Pro ? I have used that for a couple of festival scereenings without complains . Pretty slow encoding but I am thinking of if it is a good solution if just delivering a couple of DCPs a year ? <p> EasyDCP is probably the most-bang-for-the-buck app and has proven to be pretty solid . Have had great results at reasonable speed with it - several times faster than any freeware-app . Besides Resolve and Quantel , there 's the Mediareactor plugin from Drastic for Avid , Premiere , SCRATCH , FCP and a couple of others . It can be supplied with an EasyDCP-option as @ @ @ @ @ @ @ @ @ @ above apps . <p> If there 's not too much of a budget , DCPC is a great ( mostly unknown ) freeware alternative , which I actually like much more than LONG ... Downside is , that it 's not the very fastest encoder ( still faster than OpenDCP in my tests ) , and Windows only . <p> Else ... Marquise MIST might be worth a look . They can do 2k DCP encoding at 120 fps with an Image Matters card , and 40 fps with latest Tesla-cards . Also they have great mastering features for subtitles , versioning , even editing , etc. - including primary grading , optional IMF-mastering and Digital Vision DVOs . Only drawback is , that it is Windows only . <p> Depending on which application I 'm coming from , I like to render out a DCI compliant Jpeg2000-sequence in XYZ color space . Most grading-tools , which in most cases are the last station before creating a DCP , support rendering to an XYZ j2k-sequence . Baselight and SCRATCH can do that ... not sure about Resolve . So once @ @ @ @ @ @ @ @ @ @ video needs just to be rewrapped , not re-encoded . This is quite a timesaver , because the video does not need to be encoded twice , but also I have full control over the conversion to XYZ ( speaking of unwanted gamma/color/black-level shifts , etc. - especially with freeware-tools ) . 
@@44333007 @4333007/ <h> Buying used i1Pro <p> I debated commenting on this thread b/c we resell various probes so I can hardly say I 'm impartial . That being said I think there is something very worthwhile to add to the discussion here that has not been mentioned so far . The i1 Pro ( Spectro ) does offer a good place to start as Jason correctly points out , but I 'd qualify that somewhat to say it is a good place to start if your main goal is to just do a simple white balance adjustment on any given display . <p> The last time we tested the i1 pro vs. a 5nm spectral bandwidth spectroradiometer on a number of different technologies the maximum deviation for any x or y chromaticity coordinate compared to the higher-end spectroradiometer was 0.0024 . More than acceptable IMHO for the money and actual within the combined stated tolerance ranges for the probes .. However , for more complete profiling where more than just white is being measured deviations can quickly get much higher ( e.g. 0.009 ) . That type of error , @ @ @ @ @ @ @ @ @ @ matrix for your colorimeter , is more significant . <p> Keep in mind that even the PR-650 is by this time a relatively old spectroradiometer and I believe was only ever offered in an 8nm spectral bandwidth version . That a K10A calibrated with a PR-650 would agree with the 10nm spectral bandwidth i1 is to be expected if both spectros have recently been re-certified ( one is 8nm the other 10nm ) , but the deviation of both of those compared to a 5nm or better spectral bandwidth spectroradiometer would likely be easy to spot . A couple of years ago we did a lot of extensive comparison testing between uber-expensive ( 40K+ ) spectroradiometers all the way down to the $1,000 offerings . The pattern we saw across brands was that once you got to a spectroradiometer with at least 5nm spectral bandwidth there was little difference in the way the vast majority of displays were measured . Sure the higher-end spectros would measure better into lowlights or measure faster or even be a bit more repeatable , but by and large where the differences started to really @ @ @ @ @ @ @ @ @ @ bandwidth spectroradiometer to 8nm+ spectral bandwidth variations . The results of the testing were compelling enough that we upgraded or sold-off any spectroradiometers we owned that were not at least 5nm spectral bandwidth . At the same time the results showed that , at least with the displays we are dealing with on a daily basis , spending more money for 4nm or 2nm spectral bandwidth spectroradiometers was n't really worth the money . That is not to say that for some applications ( laser projectors maybe ? ) there would not be a real world benefit to some of these excellent narrower spectral bandwidth probes . <p> All of the above opinions on the i1 Spectro notwithstanding I will say that for the money the i1 colorimeters ( e.g. i1D3OEM ) are hard to say anything bad about if paired with a good spectroradiometer . They perform admirably for something so darn affordable . In an ideal world having a high-end colorimeter ( CR100 , K10A , CA310 , etc. ) paired with a high-end spectroradiometer ( CR250 , PR670 w/ 5nm option , CS2000 , etc. ) is @ @ @ @ @ @ @ @ @ @ in the budget there are a couple of very solid options I think : <p> Buy a 5nm spectro and pair it with the i1 colorimeter . This way calibration matrix creation can be done in-house so you can measure pretty much any display you might come across very accurately . <p> or ... buy the best colorimeter you can afford and make sure it comes with appropriate and accurate calibration matrices for the display(s) you want to measure . <p> There are a bunch of other considerations when selecting probes of course ( speed , repeatability , reliability , form factor , polarization error , compatibility with various software , etc. ) , but spectral bandwidth of a spectroradiometer often seems to be overlooked and at least in my opinion it is one of the most important specifications to take note of ... 43971 @qwx453971 <p> Thanks Bram for sharing your knowledge , greatly appreciated . <p> Could you name any 5nm spectro that could be paired with the i1 pro ? Brand , model etc. ? <p> Could you name any 5nm spectro that could be paired with @ @ @ @ @ @ @ @ @ @ @qwx453971 <p> The Colorimetry Research CR-250 is a 5nm spectral bandwidth spectroradiometer and the PR-655 , PR-670 , etc. from Photo Research are all available in a 5nm spectral bandwidth version ( 8nm is standard without the option ) . There are others , but these are the ones we personally own at FSI so I can only really comment intelligently on them . <p> Please note we resell the CR-250 so I ca n't claim to say this with any degree of impartiality , but the CR-250 is just phenomenal . We 've paid 4 times as much for some of the other units we use in house , but the CR-250 performs just as well and is way more compact . Even if I did n't sell it I 'd be in love with it ... if you travel with some of these other ginormous spectros for years and then switch to a CR-250 you 'll understand why the form factor alone makes life so much easier . <p> The Colorimetry Research CR-250 is a 5nm spectral bandwidth spectroradiometer and the PR-655 , PR-670 , etc. from Photo @ @ @ @ @ @ @ @ @ @ ( 8nm is standard without the option ) . There are others , but these are the ones we personally own at FSI so I can only really comment intelligently on them . <p> Please note we resell the CR-250 so I ca n't claim to say this with any degree of impartiality , but the CR-250 is just phenomenal . We 've paid 4 times as much for some of the other units we use in house , but the CR-250 performs just as well and is way more compact . Even if I did n't sell it I 'd be in love with it ... if you travel with some of these other ginormous spectros for years and then switch to a CR-250 you 'll understand why the form factor alone makes life so much easier . 43971 @qwx453971 <p> Thanks again Bram . <p> The CR-250 is way out of my budget . But allow me to explain the user case here , because I 'm still wondering if an i1 pro would n't simply be good enough . <p> My primary display for grading is the @ @ @ @ @ @ @ @ @ @ myself because of FSI 's excellent recalibration service , which provides a far better accuracy with expensive tools the price of my car . ( actually 3 times the price of my car ) . <p> Why I bought the i1 pro and a copy of Lightspace was to be able to install a 46 " -55 " HD client display ( Sony Bravia or Samsung ) , hook it up thru an AJA LUT Box and create my calibration luts with an i1 display pro in combination with an i1 pro for the offsets . <p> So do you think the i1 pro would be good enough for such a scenario , involving consumer hardware ? <p> So do you think the i1 pro would be good enough for such a scenario , involving consumer hardware ? 43971 @qwx453971 <p> That depends very much on your definition of good enough <p> What you will likely find is that the wider gamut and/or spikier the spectral distribution of the display happens to be the more problematic a 10nm spectral bandwidth spectroradiometer will become , but certainly having a spectral distribution @ @ @ @ @ @ @ @ @ @ a 10nm bandwidth spectro , is likely better than using no specific matrix at all . Certainly the white balance of the display I would expect to be more or less accurate even if using a 10nm spectral bandwidth spectro as reference because as mentioned the greatest error on white we were able to record in spectro comparison was 0.0024 , an acceptable level of deviation as even the best spectros usually have a +-.0015 or +-.0010 chromaticity accuracy tolerance ... and that is for illuminant A , things tend to get a bit worse on actual displays . <p> I was going to suggest sending it to Calman or Light Illusion so they could recalibrate it as they could also provide all the offset tables etc. but it seems Bram is basically saying it is not really worth it as you would get better results just getting a good colorimeter for a similar cost . That is why I got the C6 and why my i1 Pro is just sat on the shelf . <p> Is it worth 350 usd - as is no additional fees ? I found @ @ @ @ @ @ @ @ @ @ Europe , I will have shipping cost+ import charges . I m planing to buy only to create offsets for my i1 Display Pro , do you think recertification is needed ? Also since I don t have the original case can you suggest an alternative ? <p> I just realized that I mistyped the name , it is an i1Pro Rev.D 42.17.79 , and not an i1 Display Pro , unfortunately I ca n't edit the post . Can anybody share his i1 Diagnostic 4 log please , I just want to compare my results , to see if this spectro is working as it should , and is it suitable for creating correction matrixes ? The lamp Burning time is pretty low but there is a lot of Dark measurment count , and I do n't know if that 's bad or not . 
@@44333008 @4333008/ <h> Oscars 2017 <p> In 2011 and 2012 we ran a search for the best picture colorists - as usual burried deep in the list of credits . In conjunction with CSI we are again trying to raise the profile of those artists that studied every frame of these films . PLEASE HELP FILL IN THE MISSING INFO . and we will make the results public and if necessary update IMDB . Thanks Category film source Colorist System Company Location <p> I felt that the grade on Arrival is a bit too dark , even for theatrical environment . 43971 @qwx453971 <p> By now it 's a pretty typical story . As per information from the film editor , colorist had a completely different version of the grade , which director and editor found to be very interesting , but at the end , higher ups decided , that grade should not stray too far from the dailies look and that 's what they ended up with ... <p> By now it 's a pretty typical story . As per information from the film editor , colorist had a @ @ @ @ @ @ @ @ @ @ editor found to be very interesting , but at the end , higher ups decided , that grade should not stray too far from the dailies look and that 's what they ended up with ... <p> I 'm surprised that they even have a Baselight in the building ! Last time I waltzed through , they strictly had their own ( highly-customized ) version of Lustre , then they had a couple of home video rooms with Resolve . <p> I 'm surprised that they even have a Baselight in the building ! Last time I waltzed through , they strictly had their own ( highly-customized ) version of Lustre , then they had a couple of home video rooms with Resolve . <p> I was likely wrong about Natasha Leonnet using Baselight . I was probably confusing her with Tim Stipan or Tom Reiser . EFilm is a bit different now than it was in the Joe Matza/Bill Feightner era. 43971 @qwx453971 <p> Very third-hand info , but apparently it 's much more of a mix now . For instance , like your links mention , Tom Reiser @ @ @ @ @ @ @ @ @ @ on Baselight . Natasha Leon and Mitch Paulsen are on Lustre . They are also using Resolve there now , too . <p> Very third-hand info , but apparently it 's much more of a mix now . For instance , like your links mention , Tom Reiser , Tim Stipan , as well as Jill Bogdanowicz are on Baselight . Natasha Leon and Mitch Paulsen are on Lustre . They are also using Resolve there now , too . 43971 @qwx453971 <p> The Lustre systems at Efilm were never " standard " Lustres . Efilm originally licensed the color processing technology from Colorfront back before there was a Lustre . The version they ran for a long time was known as E-Works , I 'm not sure what version they 're running today . It 's probably changed from the days when Steve Scott was there . <p> The Lustre systems at Efilm were never " standard " Lustres . Efilm originally licensed the color processing technology from Colorfront back before there was a Lustre . The version they ran for a long time was known as E-Works , @ @ @ @ @ @ @ @ @ @ . It 's probably changed from the days when Steve Scott was there . 43971 @qwx453971 <p> Talking to friends that used to work at eFilm , it was n't so much eWorks that was custom ( other than maybe branding ) , but more the color science pipeline , and DI processes outside the app ( like scanning and recording ) . For at least the last few years , they have been using release versions of Lustre , and the somewhat rigorous pipeline created by Bill and the original color science team is giving way to much more of a roll-your-own environment . <p> For clarity , I 'm refering to the last several years . Early on ( say late 90 's , early 2000 's ) , definitely , I 'm sure everything was very custom as the entire DI process was still being formulated . But , according to former eFilm guys I know , the idea that they use anything other than standard Lustre releases faded away a while ago . <p> Fences was shot anamorphic on 35mm and graded with Lustre and Nucoda @ @ @ @ @ @ @ @ @ @ at Efilm on the Lustre . Jack Lewars works at Technicolor/ Postmarks NYC on the Lustre . <p> Interesting little fact- All were graded with only keyboard and mouse . <p> The Lustre systems at Efilm were never " standard " Lustres . Efilm originally licensed the color processing technology from Colorfront back before there was a Lustre . The version they ran for a long time was known as E-Works , I 'm not sure what version they 're running today . It 's probably changed from the days when Steve Scott was there . 43971 @qwx453971 <p> Apart from the Eworks logo and a different color GUI , there was no difference between the two apps . 
@@44333009 @4333009/ <h> What 's new in Autodesk Lustre 2016 <p> You can now use Lustre to master High Dynamic Range content using Dolby Vision . Lustre 2016 is Dolby Vision Enabled and exposes additional UI elements when a suitable Dolby Content Mapping Unit connected to your Lustre system is detected . <p> The new float conversion LUT floatidentity.fclut is useful for projects with EXR files that have gamma or log encoding baked in . For a single clip instead of a whole project , you can continue to use **25;1971;TOOLONG in the Transcode settings . <p> Viewer Filtering <p> New Viewer Filtering options : In previous versions of Lustre , the Filtering option used a fast-filtering filter when the viewer was resized . Some users reported content displaying a high level of noise ( showing a moire effect ) . To avoid this , a new Quality Filtering option is available : Setup/Grade No Filtering : The viewer is not filtered ( same as Filtering Off in previous releases ) . Fast Filtering : The viewer is filtered and uses the same filtering as in previous releases . Quality Filtering @ @ @ @ @ @ @ @ @ @ does not show moire on content with noise . <p> Note : Filtering is only for viewing and does not affect grade rendering , but does affect SDI Preview . The state of this option is saved with the user profile . <p> New Grade Flags <p> New System / Grade-Based Flags User Option ( in User / Display &amp; Interface ) : It is now possible to define if the flags of a grade are propagated to the system-level flags system . <p> The two choices are : System-Based Flags : When loading a grade that contains flags , these flags are automatically added to the system-level flag file ( located in the home of the application ) . This means that all projects have access to the flags . This is the default state when creating new user settings . Grade-based Flags : When loading a grade that contains flags , these flags are only available when using this grade or when creating a new version of this grade . <p> You can now use the Render Flag hotkey to enable or disable any selected flag . In @ @ @ @ @ @ @ @ @ @ located next to the Flag Visibility . By default , this icon is set on the Render Flag but clicking on the left of any Flag Visibility will set the Flag on/off keys to the Select Flag . <p> Lustre Second Screen <p> The Lustre Second Screen time-out is now set to 24 hours , where in previous releases the service would disconnect after 60 minutes . <p> AJA Kona 4K Support <p> You can now use the AJA board with Dual Link RGB ( 10-bit 444 ) in 3G mode for HD , UHDTV , and 4K rasters . Select Dual Link . Select 3G . Select 4:4:4 . Select the HD/UHDTV or 4K timing you want to use . <p> Working with Containers and Matte Containers in Lustre : <p> By default , Flame renders only the timeline effects required for the sequence . Effects located inside Containers &amp; Matte Containers that are not required are not rendered . Since exchanging content with Lustre might require rendering more content , it is now possible to render all timeline effects located inside containers and matte container . The Full @ @ @ @ @ @ @ @ @ @ PreferencesTimeline Rendering ensure that all effects are rendered . This option also works foreground rendering , Burn and Background Reactor . This preference is saved on a project basis . <p> Before rendering , make sure to select the Container or Matte Containers . If the Containers are not explicitly selected , only the visible effects will be rendered . This behavior happens when selecting an empty top video track and clicking Render . In this case , the rendering is optimized to only render the required effects , even if the Full Container Render option is enabled . <p> Matte Containers can be used in Lustre when a sequence is imported from Flame Premium . Matte Containers availability is not limited to the Source Grading import method , but prior to importing sequences with Matte Containers in Lustre , make sure to : <p> Render any timeline effects to avoid ending up with missing media in Lustre . Disable any Comp effects applied on Matte Containers . If the Comp effect is not disabled , only the output of the Comp will be available to Lustre . <p> MediaReactor @ @ @ @ @ @ @ @ @ @ Transcode menu . Modifying the preprocessing options of MediaReactor imported media from the Flame Family products now displays the proper result . The edited values now match in both applications . <p> For RAW media formats , the following options are available : <p> Colour Menu : From File Header : Uses the default attributes of the media , as captured by the camera . This option is selected by default . Gamma : Use to modify the clip 's Gamma curve . Black : Use to modify the clip 's black point . White : Use to modify the clip 's white point . Exposure : Use to modify the clip 's exposure . Red Gain : Use to add or remove Red to the clip . Blue Gain : Use to add or remove Blue to the clip . <p> Note : Because Transcode settings are source based , use the MediaReactor entry in the Selector to copy settings from shot to shot or to a selection of shots . Proxy Rendering <p> Interop Proxy Workflow <p> It is now possible to render proxies from Lustre to Flame @ @ @ @ @ @ @ @ @ @ Enabling the Proxy Rendering option , in the Project Management / Network Rendering menu , generates both High Resolution and Proxy Resolution footage at render , for all projects . If disabled , only high resolution footage is generated at render and proxy generation must be done manually in Flame , Flare or Flame Assist . The Proxy Rendering option is disabled by default . <p> When importing CDL or SDL data from an EDL , a flag is displayed on the shots to which CDL or SDL data was imported . CDL data import displays a purple triangular flag . SDL data import displays a green triangular flag . <p> For revision purposes , you can rename these flags to whatever you wish and , when new CDL or SDL data is imported from EDL , the flag returns to its default name , enabling you to track changes over time . <p> You can now invert the selection of flagged shots , from the Flag List . Holding ALT while clicking on the Select button , inverts the current selection . <p> When moving a Flag or a @ @ @ @ @ @ @ @ @ @ now updated and displays the actual position of the moved item . <p> The S3D EDL Export option is now named SDL . CDL and SDL ordering is now streamlined between the Cut and the Assemble menus . <p> Miscellaneous Lustre Improvements <p> Dolby Vision XML Support <p> HDR Grade Files generated in Lustre are now compliant with the Dolby Vision XML specification 2.0.4 . <p> Wiretap Gateway Optimizations <p> Decoding of Wiretap Gateway imported files has been optimized . This affects all file formats and multiple streams ( like Multi-Channel in a single or multiple media files ) using the same Wiretap Gateway server . The issue was visible with compressed TIFF files used as Matte . Playback speed should now be on par with uncompressed formats . <p> Lustre now supports the Drastic plugin , for importing and viewing media . <p> Note : PreProcessing options can not be modified in Lustre . The settings from the file header is used . PreProcessing options of RAW formats modified in Flame will not be seen in Lustre . <p> Viewing and RTD Filtering <p> A new Custom Filtering viewing @ @ @ @ @ @ @ @ @ @ . Custom Filtering uses the Reposition Filter , as defined in the Project / Rendering menu . <p> Note : Viewing Filtering only affects what is displayed in the viewer . It does not affect rendering . In Image/Reposition menu , you can define the Filtering method that will be used for rendering , from the Image / Reposition menu . This setting does not affect how media is displayed in the viewer . <p> For Real-Time Deliverables , filtering can be enabled from the Image / Reposition menu and , in this case , the Viewing Filtering is bypassed by the Image / Reposition filtering , since RTD uses the graphics card for the SDI output . <p> Lustre Second Screen Improvements <p> The update issue of Lustre Second Screen on iOS 8 devices has been fixed . <p> GPU RAW Media Decoding <p> Lustre supports GPU RAW Media Decoding for the ARRIRAW and RED file formats , providing much faster decoding speeds . GPU RAW Media Decoding is enabled by default , as long as the following conditions are met : <p> For ARRIRAW media , the Debayering @ @ @ @ @ @ @ @ @ @ . <p> For RED , Colour Science must be set to version 3. x . This is the default . <p> Linear CDL Grading via the Numeric Keypad <p> You can now use the numeric keypad for CDL grading in Linear mode as well as Logarithmic mode . Four digits are now displayed in the parameters values while performing CDL grading . For more information , see CDL Grading Using the Keypad. 
@@44333010 @4333010/ <p> Additional transforms , encodings , documents , and reference images are included as part of the ACES Version 1.0 release . Please carefully review the ACES Version 1.0 documentation package for details on new features and enhancements for ACES Version 1.0 Filenames have been updated to conform to the ACES System Versioning Specification RRT <p> New set of rendering primaries have been introduced to improve gradeability and vectorscope behavior . The new primaries , known as AP1 , are near the spectrum locus but exceed anticipated device gamuts , including ITU-R BT.2020 at a range of white points . The global desaturation is now applied in RGB space prior to the RRT tone scale . This was done to improve the overall look of the images based on end-user feedback . <p> The red modifier and glow module variables have been modified . This was done to improve the overall look of the images based on end-user feedback . <p> A clip of negative values has been added prior to the application of the 3x3 matrix that converts ACES to the rendering primaries . This is added to @ @ @ @ @ @ @ @ @ @ ACES values turning positive and saturated . <p> The RRT tone scale has been modified to address end-user concerns that the default rendering in v0.7.1 unnecessarily crushed shadow detail . <p> The output luminance of an 18% scene reflector was moved from 5.0 nits to 4.8 nits to slightly darken the overall image in response to end-user feedback . <p> The hue restore function has been removed to improve grading behavior and address rare instances where image noise could be enhanced . The RRT tone scale has been modified to allow for the use of b-splines in the new HDR ODTs . ODTs <p> New set of rendering primaries have been introduced to improve gradeability and vectorscope behavior . The new primaries , known as AP1 , are near the spectrum locus but exceed anticipated device gamuts , including ITU-R BT.2020 at a range of white points . <p> The ODT tone scale has been modified to address end-user concerns that the default rendering in v0.7.1 unnecessarily crushed shadow detail . <p> The hue restore and smart-clip functions have been removed to improve grading behavior and address rare instances where @ @ @ @ @ @ @ @ @ @ have been modified to allow the ability to achieve device black on-set and more quickly in the DI environment . Rec.709 , Rec.2020 , and rgbMonitor ODTs supporting dim surround environments have been added Rec.709 ODTs now have a runtime flag for full range or legal range output . The default is full range . ACEScc ( formerly ACESlog ) and ACESproxy tranforms have been updated Miscellaneous code cleanups . Removal of unused code <p> Keep in mind that they are specification &amp; recommendation Committee , not a software company . <p> For them , the 1.0 indicates their confidence that they are finally ACTUALLY ready to release ACES into the world . As far as AMPAS was concerned , their numbering system clearly indicated that everything prior was a " Beta " program designed to elicit the feedback that we now see applied to this release . <p> ACES 1.0 is ... the result of over 10 years of research , testing and field trials . <p> ... you 'd think there 'd be more news about it . <p> Edit : I did a search for " aces @ @ @ @ @ @ @ @ @ @ 's basically just us and AMPAS . Note to marketing ... do n't release production version of future-proof color pipeline during a holiday . <p> ACES is about recreating the scene lighting in post , so no ACEScc ( formerly ACESLog ) does not give you log images to work with . The purpose of it is to allow log-based tools/systems to work correctly with ACES data . <p> If you wish to see this in action open a greyscale in an ACES linear project and attempt you use the Printer Lights/Offset tool . If you know what the tool normally does to footage then you 'll see on the scopes and visually on the greyscale where it 's going wrong . ACEScc makes the tool function correctly . <p> VFX can use ACES in it 's linear form , whilst DI can use ACEScc to grade with . 
@@44333011 @4333011/ <p> These announcements so close to NAB are interesting . They have a very Apple-like trajectory . I 'm wondering if this is part of a plan for Blackmagic to begin moving away from large , expensive tradeshows , toward a combination of their own roadshows and video announcements like this . <p> Yes , I think it might be a more efficient way to get to customers . I would guess that based off their last video that they 'll be showing their new live internet broadcast systems and probably some Resolve stuff . Of course I could be completely wrong . <p> I 'm missing a " serious " camera for video that is sized between the Canon XC15 and C100 . Something that I could use with family and friends and small projects without feeling like a one man production company . I 'm currently using a 1Dx mkII , but I really like the XC10 form factor . I think the following would be a cool product to see from BMD : <p> i bet its the introduction of magnetic timelines and a complete replacement @ @ @ @ @ @ @ @ @ @ commonly used grades And in the delivery page of course the option " send to blackmagic phone " <p> p.s . Dongle will also get a proprietary connector but you can buy an adaptor from BMplug to USB optionaly. 
@@44333012 @4333012/ <p> Hi Scott , the Judd modified setting is an alternate Color Matching Function that most top emission RGB OLED monitors have available or use to offer better perceptual white balance matching to most other display technologies . Its use is pretty ubiquitous , how I personally would use it as well , and also approved by recent SMPTE standards so do keep using it , but it is not related to black level . You can use the menu toggle or my preference use the R709-JM color space setting on the monitor . <p> The monitor looks great . Amazing deep contrast , rich colors . Here is the issue : I will grade something and it looks well saturated and contrasty on the Flanders . Outside the Flanders ' environment , the grades look pretty desaturated and rather bland - even in VLC and my new iPad2 Air ( which really is the external test for me , as I do n't trust computer monitors ) . Colors are correct , but I find myself going back and bumping up saturation and contrast ( perceptually more than @ @ @ @ @ @ @ @ @ @ LCD monitors can not maintain saturation in lowlights , they typically can only maintain their widest gamut at brighter light levels . So what you get on an LCD is a collapse towards grey as luminance drops . That is likely the primary difference you are seeing . An OLED maintains saturation very well even at low luminance levels . Objectively an ideal display holds saturation to the intended target regardless of luminance levels . This is one of the primary areas where an OLED outperforms an LCD , but it is not talked about much as ' black levels ' tends to become the primary point of discussion . LCD behavior is quite variable from one display to the next and as they improve over time you tend to get less and less of this collapse towards grey so I would n't suggest chasing that moving target , but if you really wanted to you could profile and emulate the behavior of such a display . The nice thing about an OLED like this is that you get this giant linear container so you can basically make it behave @ @ @ @ @ @ @ @ @ @ / black levels . You can make that pretty much whatever you like on an OLED . <p> okay , just to clarify : the built-in Judd function is usable - it 's there for a reason . <p> I personally prefer to use create cLUTs with custom Judd as it further improves the cal ( at least in my tests ) , which is what I recommend . I 'll try to squeeze the very best out of every cal ... <p> but w/ LS and CM u go use either approach and compare for urself ... FSI are very , very flexible screens with a lot of options , so sky is the limit ! 43971 @qwx453971 <p> Until I have a proper offset for my probe from Flanders , I am hesitant to recalibrate the monitor , as Flanders just sent it to me 60 days ago - having calibrated it using the same hardware pipeline that I have in my suite . Thank for the advice and help . I appreciate it . 
@@44333013 @4333013/ <h> Balancing a newborn child with your client/facility obligations ? <p> I took 2 weeks off with our first child , which was n't enough , 6 weeks for the second . Imagine the most demanding job/client you 've ever had and times it by 4 , the demands a newborn ( particularly your first ) places on your household is not to be underestimated . Your wife will need all the help and support you can give , their health ( particularly mental health ) should be more important than a business losing a little bit of money . That 's why they have a mark up on what they pay you , your personal leave should be part of their overheads . Shit , we 're making crap for tv , it 's not like we 're saving lives or changing the world . <p> I 'll also echo the fact that if your clients become unreasonable they 're not the ones you want long term . They end up being the ones who suck up a facility 's resources and refuse to pay the overages bill @ @ @ @ @ @ @ @ @ @ situation in regards to being the only colourist on staff in my office , with good freelancers hard to find . I 've got a 5 week vacation soon and there was some hesitation getting it approved , I ending up suggesting that if I was that important to the business that I could n't take leave we should renegotiate my contract . <p> Shit , we 're making crap for tv , it 's not like we 're saving lives or changing the world . 43971 @qwx453971 <p> Yes . This is a quite important bit to understand . As soon as One does - shift in perspective is quite noticable <p> Of course never say that to a director sitting i the room Unless this is someone You worked before with and You know he/she is on Your page on this More often than not They actually think They are changing the world . And in some cases it may be true but 97% of the time this simply is not the case . Of course I kid The Directors - Directors I kid <p> My beautiful new @ @ @ @ @ @ @ @ @ @ I managed to figure out how to take 3 uninterrupted weeks off . And BOY am I glad I did ! <p> People are seriously not joking when they say that a newborn will figuratively and literally suck every minute of your existence for the first few weeks . Someone in this thread mentioned that newborns " do n't really do much " early on in their life , and suggested that I might be able to swing going into work after the first week or so . This is patently false . That poster must have had the most sedated baby that ever existed , because even though our baby has been a relatively " good " baby , I am still sleeping only 2-3 hours a day ( in total ) due to incessant feedings , " skin to skin " sessions , diaper changes and simply staying awake out of neurotic paranoia , just watching her in her crib to make sure that she does n't spontaneously die for no reason . <p> If I had been stupid enough to commit to any grading sessions in these @ @ @ @ @ @ @ @ @ @ and possibly suffered a heart attack from complete lack of sleep and overdose of caffeine . <p> Also , if I had gone into work a week after my daughter was born , I would have missed her first smile , as well as the " tummy-time " session where she first successfully lifted her own head to shift sides ( which is apparently a big deal ) . <p> So now the next chapter in my life is to see how my clientele will tolerate my steadfast commitment to my family first , above all other things . Of course , I wo n't be a dick about it , and will always work with them to make sure I can fulfill my commitments without leaving them in the lurch simply because I have to pick up my child from daycare at 5pm every day . As Marc inferred , I 'm certain that some of my clients will find this annoying . But I 'm well past the point in my life/career where I am willing to lose any sleep ( what little of it that I now @ @ @ @ @ @ @ @ @ @ fact that I have a life outside of work . I figure that if it comes to the point where I lose all of my clientele because I refuse to put their priorities over my family priorities ( esp . if they keep moving the goalposts on what those " priorities " should be ) , then that will be the point when I quit the post-production business and find some other job that will allow me to not stress so much about all this stuff . <p> Maybe I 'm being way too pollyanna about this , but I think life 's way too short to wait around hoping that the advertising &amp; post-production industry will be able to rehabilitate itself so that people do n't feel guilty , simply for wanting a life outside of work . <p> Congratulations , Mel ! It sounds like your family &amp; your priorities are aligned nicely . I hope things go smoothly both with your new little one &amp; your transition back into work . I do n't really know how to say this , but to me it has always @ @ @ @ @ @ @ @ @ @ that adds to rather than subtracts from work when you work in a creative field . Wishing you all the best ! ( Also , your daughter is indeed beautiful ! ) 
@@44333014 @4333014/ <p> I saw that today . It shows how scared Intel really is of AMD . But I wonder how much does it make sense for intel to start a core number race with AMD , as AMD has it 's infinity fabric , they can just add more cores , where as intel has to rewrite the architecture ... <p> Ooooo , an extra 4 PCIe lanes ? That 's not a ton more , but that 's the first increase in lanes from a single processor I 've seen in . . . well , since I 've been keeping track 5 or 6 years ago . There are n't a whole lot of people that saturate all the PCIe lanes . VFX , Colorists , DITs . . . maybe a few other positions in other industries , but not a lot . There is n't a lot of incentive for Intel/AMD to increase the PCIe lanes . Which is unfortunate for us because we could actually utilize more lanes rather well . <p> Worth noting , always , that a GHz is not always a @ @ @ @ @ @ @ @ @ @ the i3/i5/i7/i9 line . The reason the performance is so close between the current top Xeon and the current top i7 is that the Xeons , typically , get updated months after a new top consumer processor comes out , and in the past few years that has lagged to being a gap of well over a year . The Xeons keep getting delayed and delayed . So the Xeons are a few generations back from where they " should " be . If they were n't , they 'd blow the i7 out of the water . <p> In a perfect world , the features of this new i9 would be integrated into a new Xeon by early 2018 , giving us a ton of cores and a ton of lanes and great power management . I 'm not holding my breath , though . So a beefed up i9 might be the best we can count on for the next year or so . <p> Let 's wait for some real comparisons when both platforms are out there . Also I 'd suspect that some pro video Apps @ @ @ @ @ @ @ @ @ @ and there Intel sadly is way in front of AMD which might result in much better performance in Apps supporting AVX512 . <p> Sure , I 'm very happy about AMDs new platform and the competition and might get some AMD power next . Just wanted to remind that the pure GHz and Core count are n't everything and a fast vector instruction set can make a huge difference , if software uses it . <p> Btw. some more Threadripper info is out . 64 PCIe Lanes confirmed but sadly not much more . At least that makes some room for GPUs , Storage , BMD cards etc. without getting to the PCIe Lane limit as fast as with Intel . ( Worst move with i9 is , that now only the 10 core supports 44 Lanes while in last generations there always was a middle ground option with 40 PCIe Lanes . <p> Worth noting , always , that a GHz is not always a GHz . Xeons have better architecture for our uses than the i3/i5/i7/i9 line . 43971 @qwx453971 <p> As far as I understand it , @ @ @ @ @ @ @ @ @ @ qpi 's but that 's it . Sintetic benchmarks usually show this very nicely as core per core xeons are behind appropriately for there lack in Mhz . <p> I 'm not sure if Intel is addressing this but the " gold " processor would be something like a 10+ core system that can scale just as well as a 4-core system when only 1-4 cores are used . This is a problem today where high end i7 's in a lot of tasks will beat multi-core systems . <p> Nahh ... Intel has never been scared of AMD . I like AMD , and I started out running AMD processors many moons ago . But benchmarks and facts mean a whole lot more to me than opinions and emotions . LONG ... <p> I think you kinda negated yourself there a bit And that 's a processor that was half the price . So I think Intel has been jolted quite a bit by AMD . 43971 @qwx453971 <p> Nahh ... I did n't negate anything . Amateurs and gamers are more concerned about price , while professionals are @ @ @ @ @ @ @ @ @ @ list there are 50 Intel processors shown before AMD Ryzen even shows up . <p> I think price is a concern for everibody not just amateurs. 500G however is n't a lot . It might be for a gamer living in his mothers basement or for people that do n't turn over much profit in post . Anyway ... I do understand what you mean . <p> Still , AMD is a threat to Intel . They are going to eat up a big chunk of Intel 's revenue . And that 's all I was saying . 
@@44333016 @4333016/ <h> Is converting an 8 bit render to 10 bit worth the effort ? <p> In a real world application , is it worth taking footage that was recording in 8 bit and encode into a 10 bit result ? <p> Or for that matter , record using a camera with 8 bit out via HDMI recorded into a device that records 10 bit 4:2:2 ( such as the new Blackmagic recorder that will be coming out ) <p> I understand the argument from some people is that even though it is only 8 bit initially that converting to 10 bit still assists with some dithering , and I 've run some tests myself with minimal improvements , but I was curious if anyone sees the effort , especially with color grading of performing this additional step . <p> In a real world application , is it worth taking footage that was recording in 8 bit and encode into a 10 bit result ? <p> Or for that matter , record using a camera with 8 bit out via HDMI recorded into a device that records 10 bit 4:2:2 @ @ @ @ @ @ @ @ @ @ coming out ) <p> I understand the argument from some people is that even though it is only 8 bit initially that converting to 10 bit still assists with some dithering , and I 've run some tests myself with minimal improvements , but I was curious if anyone sees the effort , especially with color grading of performing this additional step . 43971 @qwx453971 <p> I think an argument could be made to do so , but depends on the application you use to do it . For example , h.264 's shot on 5D or 7D , tend to hold up better for me after converting to ProResHQ or similar 10bit file . Alternatively , I have also " told " the grading software to treat the h.264 's as 10 bit and it has worked equally well . <p> If you 're recording to an external recorder over hdmi then yes there would be some advantage in bypassing h264 or other heavily compressed codec . But regarding bit-depth . Not so much . That said , I do tend to convert h264 sources to prores . <p> @ @ @ @ @ @ @ @ @ @ 8-bit integer problems . But , from an overall pipeline perspective , it does n't make a whole lot sense to do it ahead of time . It 's more important to process at a bit depth higher than 8-bit throughout your pipeline and store the result as at least 10 bits in the end . You can incorporate dithering in the pipeline instead of creating a larger intermediate set of files at the expense of storage . <p> Juan 's methodology is good though because H.264 is very costly to decode compared with Prores or DNxHD . You trade some storage for system performance . <p> H.264 is a bag of hurt . I just had to deal with some AVCHD material , and that was a nightmare as well . Every new camera that comes out seems to have a new , F 'd up format . It 's maddening , particularly when clients have no clue that these H.264 Long-GOP variants are all bad for post . <p> I have a lot of experience in this are as I regularly transcode 8 bit 4:2:0 Long GOP to @ @ @ @ @ @ @ @ @ @ magical 10bit quality but you do get timecode and an intra-frame format which will edit well and not break down in the grade as might the original 8bit file . <p> I regularly shoot with a high quality , older Super 35 video camera which outputs a clean 8bit , uncompressed 4:2:2 signal via HDMI . I record this directly to 10bit Prores 422 HQ with an Atomos Ninja Star recorder . The difference between the camera 's internal 28Mbps AVCHD and the 25P signal recorded direct to ProRes at nearly 200Mbps is like , you guessed it , chalk and cheese ! The difference is truly startling and I 'm looking at it on a state of the art , 10bit Sony 55 " 4K monitor which is driven by SDI from a 6G Decklink SDI 4K card . The screen up-rezzes all HD to 4K so this TV takes no prisoners . If your HD pictures have any issues , you 'll see them at 4K ! <p> Some cameras record at 8-bit and some at 10-bit . Shogun , like all Atomos products record in 10-bit quality @ @ @ @ @ @ @ @ @ @ a 10-bit color space , populating the 8-bit 256 RGB colors into the longer 10-bit space . This allows 10-bit editing &amp; compositing . No color info is added , just registries for 10-bit processing . <p> As a reply to the OP , it should be noted that some 32bit floating point editing software will automatically and internally transcode 8bit 420 files into the 10bit 422 space so there 's no advantage in transcoding first except that you would be working with material without any timecode . In my experience , I would suggest that is a bit unprofessional , especially if any round tripping or back and forth of files is planned . Some popular DSLRs and NLEs have done a lot to dumb down our industry . <p> As Ryan posted earlier , Atomos products record 8bit data into the 10bit space so your Pro codec will have timecode embedded into it . However , if the camera ( ie : DSLRs and Handycams ) does not generate timecode , the newly recorded files whether they be ProRes or DNxHD , etc will each have a timecode @ @ @ @ @ @ @ @ @ @ ' re-striped ' with a correct , sequential timecode ( including user selected reel numbers ) with free PC apps like ClipToolz . This process is fast and transparent because it is simply a ' re-wrap ' to a new container , not a transcode . <p> I have a lot of experience in this are as I regularly transcode 8 bit 4:2:0 Long GOP to 10bit ProRes 422 HQ . You do n't gain any magical 10bit quality but you do get timecode and an intra-frame format which will edit well and not break down in the grade as might the original 8bit file . 43971 @qwx453971 <p> I have not found this to be the case . To me , the camera does all the destruction by capturing in 8-bit to begin with . Timecode for ProRes 422 is fine , and I have seen broadcast shows delivered in this format . In fact , HDCam and all of standard-def broadcasting was 8-bit for many years . I would always much , much rather get 10-bit material from the production if the camera is reasonable . <p> No @ @ @ @ @ @ @ @ @ @ , then you have a chance of getting more out of the camera than it can internally record . But it wo n't work for ( say ) DSLRs , which are doomed to 8-bit processing , period . Inter-frame / Long-GOP formats are just awful for post ; Itra-frame generally works fine . <p> Yes , of course most if not all DSLRs capture poor quality 8bit 420 internally ( especially the hugely over-rated Canon 5D Mk II ) but there is 8bit and there is 8bit . Some of the new mirrorless full frame cameras like the about to be released Sony A7r MkII record S-log2 internally to a very high bitrate codec using the entire full frame sensor without the traditional line skipping or pixel binning and it should be noted that the Panasonic GH4 ( another low cost , mirrorless DSLR ) will already supply an uncompressed 10bit 4:2:2 signal out of HDMI to external recording devices . 
@@44333017 @4333017/ <p> The older 980 Ti is still a strong performer . 1080 slightly weaker and 1080 Ti stronger . <p> Dual 1080s give twice the performance . <p> Typical tests ( please visit site for full info ) : <p> As expected , performance in pro apps sees basically linear increase with added GFX power , even in an older Mac Pro . That is of course not to say that these cards on a modern motherboard with the fastest CPUs out there , would n't perform even better . <p> I 'd be curious to know how well the MacPro 5.1 handles 4k 12bit CinemaDNG image sequences . We work with these fairly often - lighter weight than 10bit DPX files , but more CPU intensive because of the debayering . Resolve on our Windows 7/X99/5930k setup is able to play them much more smoothly than DPX , easily faster than real time ( in our case , 24fps is realtime ) . I believe the debayering in resolve is all CPU . <p> A 5,1 MacPro that can handle 4k image sequences with a 1080 or 1080ti @ @ @ @ @ @ @ @ @ @ for it though , which may be tough to find . <p> So hopefully I 'm not totally hijacking this thread , but this is really interesting to me . I 've been waiting until after NAB to start on the long list of upgrades we 've got planned , just in case there are any surprise announcements from BMD or others at the show . We 've been planning to build a supermicro-based Resolve system that could run the Studio version on either Linux or Windows ( it 'd be dual boot , mostly because I 'm curious about the performance differences on the same hardware ) . The thinking is that we 'd upgrade the machine to the full version of linux when an affordable dongle comes around on the used market , to get ProRes output ( right now we use Scratch for that ) . Looks like Chelsio cards will give us the 40GbE we need , since they have Mac Drivers and we know they work with our setup ( we use them in a FreeBSD box ) . A MacPro 5,1 will cost us @ @ @ @ @ @ @ @ @ @ build . <p> Our main usage is single-node color correction . We almost never use any NR , we rarely use multiple nodes , masks , tracking , etc . We do a lot of scaling/downconversion in Resolve , and we do a lot of quick timecode burn-in reference files . <p> What are the downsides of using a 2010 or 2012 MacPro for Resolve in 2017 , assuming you 've got a pair of GTX1080s in there ? Where are the bottlenecks ? <p> You can do a couple of things to a Mac Pro . I 've done most of the mods . For instance , my Mac Pro has AC WiFi and Bluetooth 4 LE that allows for Apple 's Handoff . Those might be trivial mods , but makes the computer feel a little less ' old ' . It 's always a bummer when new features are introduced and you ca n't check them out due to properly old hardware . Anyway .. for a workstation , not that important . <p> Known bottlenecks are single thread performance , internal HDD bays at 3Gbit/s and @ @ @ @ @ @ @ @ @ @ , but there are two x16 PCI ( and two x4 ) . The PSU is a limitation out of the box , but like I said in the other thread , that can be modded if necessary . <p> I 'm using an Apple native 1TB SSD in a cheap adapter in one of the x4 PCIe slots and get 1500MB/s read write ( roughly ) . That is a nice boost if you want at least one fast internal drive . The internal bays max out at around 268 MB/s , but if you RAID SSDs over two or more you can create pretty large volumes that are ' OK fast ' . You can still have two large HDDs up by the optical drives for storage . <p> In terms of PCIe what annoys me the most is that I ca n't have 2x GFX+USB3+PCIe SSD . With two GFX cards , there is only one slot left for either USB3 or SSD . <p> But in all honesty ... there is much to consider . I 'd suggest Macrumors.com Mac Pro forum . " All that @ @ @ @ @ @ @ @ @ @ there . Some development takes place in the Hackintosh community , but several members post in both places , so if you have a ' real ' Mac Pro , I 'd check Macrumors.com . <p> Thanks . To be honest , we 've been very happy with the performance on our current system , which only has a single 1080 in it . Not a 1080ti , either . I had originally put two in there , but then there was n't room for the 40GbE card , so we took one of the 1080s out . Did n't really notice any decrease in performance , and that single 1080 is also running the GUI . I should run the standard candle test on it just to see where it sits relative to others . <p> So I think a 5,1 with a single GTX 1080 would be sufficient for our needs . Our biggest bottleneck is disk speed , with 4k DPX sequences . They 're a killer . We can run them off the internal RAID , but with SMB to our SAN , even at 40Gb @ @ @ @ @ @ @ @ @ @ another issue to solve , once we clear the current project backlog off that machine . <p> The nice thing about the 40Gb network is that it would n't really matter if we did n't have USB3 or eSATA on the mac itself , because we could just put one of those on a machine on the network and copy the files that way . The network is so fast , there 's no bottleneck there . <p> With a single GPU like the 1080 , do you need to mod the PSU , or is that just for multiple cards ? I 've got no problem doing that - looks easy enough . 
@@44333018 @4333018/ <p> " By analyzing images in each scene and correcting the color and contrast of each object individually , object-based HDR remaster can reproduce scenes with the detailed texture and appearance of real life . " <p> " By analyzing images in each scene and correcting the color and contrast of each object individually , object-based HDR remaster can reproduce scenes with the detailed texture and appearance of real life . " 43971 @qwx453971 <p> It 's an up-conversion trick for SDR material , can be turned off obviously . <p> The most important technology baked into the Z-series sets is undoubtedly the Backlight Master Drive . At CES , the 85-inch prototype contained 1,000 zones , but the consumer units may have expanded that functionality , down to pixel-level . This means that the LEDs that comprise the backlight are each individually dimmable . The effectiveness of this backlight array is enhanced by a high precision lighting and dimming algorithm . In addition , the optical structure of the LED beam has been developed to further improve contrast , while reducing diffusion and flare across pixels . <p> @ @ @ @ @ @ @ @ @ @ 4K Processor X1 Extreme , arrives with 40% more processing power than the 4K Processor X1 , which can be found in the Sony X-series of HDR televisions . The new processor brings with it new capabilities , including " object-based " HDR remastering of non-HDR 4K and HD material , dual database processing and Super Bit Mapping , which beefs up 8-bit and 10-bit sources to " 14-bit equivalent gradation . " 
@@44333019 @4333019/ <h> Trends <p> So I 'm really digging that moviesincolor tumblr . And over the past week Roxy ( who curates it ) has been focusing on Martin Scorsese . Obviously we 're all really familiar with his work , which is why seeing shots from these films over the years brought something to my attention . Let 's look at the ones she 's posted chronologically . <p> Means streets . Rich almost lush red palette . Like a port wine . <p> Taxi Driver , olive and fuchsia . King of Comedy , grays overwhelmingly , where we see chroma it 's hints of magenta and green . <p> Goodfellas ... green , magneta are dominant notes , but hits all over the spectrum in here . <p> And then the more recent films ... Gangs of New York . Teal and orange . The Aviator . Teal and orange . ( This one is n't really fair , it 's meant to look like technicolor 2-strip , so it 's gon na be pretty teal and orange ) The Departed . Teal and orange ... there @ @ @ @ @ @ @ @ @ @ are all brilliantly shot films . Stunning frames , great color . I 'd happily watch any of these films again ( maybe not one of them ) , but it 's interesting to see how aesthetics have changed and developed over the years . <p> I am about to work on a feature that pays tribute to the slasher - horror genre . They want it to look like Scream or Friday the 13th . It will be interesting for me to throw out the current trends I try to emulate daily . <p> The trend towards everything being orange and teal is actually kind of sad to me . I know it makes people " pop " with the contrast between skintones and the teal , but I miss the carefully chosen colors schemes in the other examples . They fit the mood of the pieces so well . With everything going teal and orange , everything looks ... well ... the same . <p> David makes a good point . Perhaps the filmmaker always wanted to hit this palette . There were photo-chemical processes that allowed this @ @ @ @ @ @ @ @ @ @ harder to achieve . However it 's not just Scorsese that has been using teal/orange , it 's seems like the majority of films we see are using the same palette . <p> Which , reinforces Aaron 's point . Looking at the last three stills , they could 've all been from the same film , if one ignores the period dress . <p> I think one of the issues with the DI becoming democratized is that many lower grade ( ; - ) ) productions see another film with a ' look ' and show up in the color suite with the basic idea that they will make their film just like that other film they saw and once that is three or five deep things just kind of pile up . <p> I do n't know if it 's good or bad but when you see a film like the upcoming Coen brothers film or ' The Master ' which just makes such an impression that great care was taken in all aspects of the production and craft of the look it seems to me that the @ @ @ @ @ @ @ @ @ @ sometimes the customer is always right because that Boat or Ferrari that most Colorists have ai n't gon na pay for themselves ; -0 ) . <p> Also I think the photochemical timing process is not to be underestimated , ' The Master ' looked like nothing else I have seen recently and it was an answer print . <p> Question about Teal and Orange ... It is becoming a default look in so many movies , so if only for that reason , I 'm not a huge fan - but I would like to know how people achieve it and how much trouble they go to to get it . I 've tried it a couple of times to varying degrees , using RGB curves , using a basic push /pull in the primaries , also tried adding a selection keyed off skin tones . I 've never been entirely happy with the results . Any tips ? <p> If you 've used print emulation then you 'll understand that you often need to grade out the shadows from blue/green by adding warmer tones ( reds/yellows ) to @ @ @ @ @ @ @ @ @ @ matrices and tints you can get a really nice strong look without having to ' guess with the trackballs ' - I 'm not a big fan of trackballs 
@@44333020 @4333020/ <h> Panasonic GH4/GH5 ? <p> I see the newly announced GH5 will shoot 4K internally at 10bit 4:2:2 so on paper , this looks spectacular for a cheap mirrorless DSLR BUT the pictures I see here from the GH4 , even when shooting externally via the 10bit 4:2:2 HDMI output fail to impress me . <p> On paper , these MFT cameras look so good , many owners and enthusiasts ca n't understand why they are not used on big budget productions world wide . <p> What is the experience of international colorists here , who work with material from multiple cameras including DSLRs ? I 'd been keen to hear of their opinions concerning 10bit vs 8bit on very cheap cameras . Legit or just marketing fluff ? <p> i 've delt with Gh4 crapola - shot as a crash cam against Draagon A &amp; B cams , the DP is a very good , with a list of features under his belt and CSC ( Canadian ASC ) after his name <p> the camera was pure crap top to tail , dynamic range of a go-pro , @ @ @ @ @ @ @ @ @ @ go on .. the best thing about this was the the camera got dropped into the ocean in a tragic accedent shortly after seeing the rushes ... <p> i 've delt with Gh4 crapola - shot as a crash cam against Draagon A &amp; B cams , the DP is a very good , with a list of features under his belt and CSC ( Canadian ASC ) after his name - the camera was pure crap top to tail , dynamic range of a go-pro , black sun , crap skintones , banding , i could go on .. the best thing about this was the the camera got dropped into the ocean in a tragic accedent shortly after seeing the rushes ... 43971 @qwx453971 <p> That was pretty much the conclusion I had come to . Our local High School bought a bunch of GH4s so yes , for student films ... I borrowed one and shot some ProRes HD directly from the camera 's 10bit 4:2:2 HDMI out via an Atomos recorder and IMO , the pictures were inferior to an uncompressed 8bit 4:2:2 HD signal from @ @ @ @ @ @ @ @ @ @ with . I should point out I was not shooting Log or 4K , just good ' ole Rec.709 HD video but it 's possible the Log settings on these small cameras demand 10bit but I can not see where ' 10bit ' on a $2000 camera can compare with ' 10bit ' from say , a $10,000 camera . <p> When people say : " But it will shoot 4K internally at 10bit 4:2:2 " , this reminds me of the time when clients rushed in to our Broadcast TV business , which was based around Betacam SP , with their first Digital Handycams and exclaimed : " But it 's Digital ! " <p> I used to be a big proponent of micro 4/3rds . I collected a lot of vintage manual lenses from the 70 's and 80 's .... the whole deal . Then after years of APS-C and M43 , I went full frame , and the improvement was just remarkable . <p> M43 definitely has it 's place , and it has helped push forward the mirrorless era , which is probably it 's @ @ @ @ @ @ @ @ @ @ lot like car motors .... there 's no replacement for displacement . While they can make nice images , DSLRs are at a dis-advantage from the outset compared to true cinema cameras . If you add to that by trying to make do with a smaller sensor , I think it 's just too much physics to overcome , no matter how good everything else about the camera might seem . <p> I used to be a big proponent of micro 4/3rds . I collected a lot of vintage manual lenses from the 70 's and 80 's .... the whole deal . Then after years of APS-C and M43 , I went full frame , and the improvement was just remarkable ... 43971 @qwx453971 <p> I tend to agree . Over the past five years , I 've built up a large collection of fully manual Zeiss ' Contax ' full frame primes plus a couple of Contax zooms ( several of which maintain a constant aperture ) so now , with a collection of full frame glass worth several times more than modern mirrorless DSLRs , I 'm @ @ @ @ @ @ @ @ @ @ View on S-35 by use of a popular focal reducer . To the best of my knowledge , this is not possible with a MFT sensor . <p> However for my personal use and with advances in mirrorless cameras for full frame video , I 'm thinking of Sony 's A7 S &amp; R series of full frame cameras here which will both accept my lenses direct ( via a passive C/ to E-Mount adapter ) or S-35 ' windowed ' via my current C/Y to E-Mount focal reducer . All in all , collecting FF lenses would appear to have been a good long term investment . 
@@44333021 @4333021/ <h> 8 bit vs 10bit display <p> I ' m finding myself taking on more grading projects at home and have decided to update my monitor . I plan on using Flanders since I 've worked with them at work and really like their quality and price point . However I ca n't decide if I should spend that Much more for the 10bit over the 8 bit display as my projects are primarily docs and stuff for the web . What are the pros and cons of each ? Any suggestions would be helpful . <p> If your content is going straight to the web , I imagine that an 8-bit monitor will do you just fine , assuming your audience will largely be watching from consumer displays anyway . Getting a 10-bit monitor will only help you spot banding issues and catch other related artifacts ; though while looking at the FSI website , I do n't see that much of a price gap between the CM171 ( 10-bit , 17.3 " ) and the BM210 ( 8-bit , 21.5 " ) anyway ? <p> One of @ @ @ @ @ @ @ @ @ @ 24 " 10 bit FSI . My last session there it had a board issue and was displaying an incorrect image . I only had 30 minutes before the clients showed up so I swapped the 24 " out for their 17 " field monitor which is the new CM171 10 bit 17 " monitor . I honestly did n't have much of an issue coloring on the smaller monitor , It is still a very nice 10 bit monitor . <p> I have also worked extensively with the older version of the BM210 and the 8 bit panel definitely hurts you when working with RED footage . We were mainly grading for DVD and streaming release of indie features with that monitor but everything was either shot on RED or Alexa and the 8 bit display can definitely hide some banding issues that may come to light . <p> Even if you get a 10-bit monitor , do n't forget to set the output to 8-bit when working on projects meant to the web . It makes a lot of difference , mainly on shots that you add some @ @ @ @ @ @ @ @ @ @ ... 
@@44333022 @4333022/ <h> Baselight v5 Coming in 2016 <p> Its true fake . The promises they made in April 2016 at the NAB show , could not perform . Now promise in 2017 . The professional team but promises do not withstand criticism . It is a pity that such promises are not fulfilled . <p> So , let me get this straight , you 'd prefer half baked version of Baselight instead of a fully vetted version , just because FilmLight originally estimated shipping V5 in 2016 ? I 'm sure FL would be happy to sell you 4.4m1 version and would have upgraded your Baselight , when v5 became available , so I do n't understand what is the issue , that made you so unhappy . FilmLight prides itself in delivering product , that is ready for production on day one , unlike some products , that are shipping what is essentially beta products , regardless of it 's readiness , relying on users finding bugs and fixing them at later versions . This philosophy is actually very refreshing . I do n't know about you , but @ @ @ @ @ @ @ @ @ @ " v5 is already in private beta testing , meaning it 's close to being finished . And finally , what " New Area tracker " are you referring to ? There is already a New Area tracker in present version of Baselight . <p> Alex , the current version of Baselight is widely considered one of the best - if not THE best - of all the color grading systems available . Their approach has always been to release point releases for maintenance purposes , but only release major updates when they 're ready . And yes , that means waiting longer for those major updates than with some other vendors . But most of their users really do n't care because the toolset they already have is superior to most everything else . There 's really nothing a colorist needs to do that ca n't be done with the existing toolset , and usually in a more efficient and more precise way than just about every other system . The base grade tool is something no other vendor has , and they 're taking time to get it @ @ @ @ @ @ @ @ @ @ a wide range of colorists so that it can be something everyone will want to use from day 1 . <p> If you want to hold back from buying a new smart phone because you 're waiting for features , that 's fine . Phones seem to get updated multiple times a year because they 're consumer devices and companies that make them want to get everyone in an " I must have each and every upgrade " mentality . Baselight is a professional toolset competing in a very narrow market and reliability and performance are far more important in that market than spiffy new interfaces and features every 3 months . As someone who has actually used Baselight , I fully support and appreciate their approach . <p> But I always understood that if you publicly promised something , you should stick to the promises . Otherwise , your words are very cheap . 43971 @qwx453971 <p> In that case , the words of nearly every technology company - not to mention countless other industries - are " very cheap , " because no company knows how long it @ @ @ @ @ @ @ @ @ @ finish it . Particularly when it comes to software , because bugs come up that are difficult to solve , problems are discovered by beta testers that necessitate delaying release , new devices arrive from other manufacturers that need to be supported , and countless other things occur that are not predictable . Such is life in the computer age . If you think Filmlight is one of the bigger offenders , try taking a look at companies like Colorfront ( whose releases are often delayed so long that they need to change the version name because it 's based on a specific year ) , Autodesk ( very similar situation to Colorfront ) , Blackmagic ( they delayed the release of a camera for over a year and a half ) , Apple ( they have current products that have n't been updated for almost 4 years , and they 've sometimes released software that was so bug ridden it gave the program a bad name for a long time ) and numerous others . <p> If you look at the fine print on product announcements , it always @ @ @ @ @ @ @ @ @ @ statements that are optimistic and might or might not come to fruition . " In other words , we think we can do what we 're talking about and get it done when we 're saying it will be done , but we might be wrong so limit your expectations . Or , to put it more bluntly , we might succeed or we might fail . No promises . Release dates are never " promises , " as you seem to be referring to them as . They are optimistic projections which may or may not come true , and they 're always expressed as such . That 's the reality . <p> +1 what Michael said . All companies do this . I would add Avid to this list as well . When they get released , they get released . I would n't rush to install a major release anyway until a point release comes out and fixes all the bugs that are there anyway if they wait long enough for the handful of beta testers to figure out all the bugs it would take even longer @ @ @ @ @ @ @ @ @ @ . <p> I 'm just excited to see someone excited about something other than Resolve . I 'm sure once everyone is using Base Grade , the new paint , and keying individual EXR layers , the wait will be quickly forgotten . <p> And I 'm excited to see someone who 's excited for another excited user I 'm also exited to see the implementation of the Matchbox GLSL shaders . I find it ironic , that FilmLight managed to be the first color grading software , that included this functionality even before the original Matchbox proponent- Autodesk . Right now this functionality in Lustre can only be done by using DirectConnect ( I think that is the name ) with Flame . <p> Its true fake . The promises they made in April 2016 at the NAB show , could not perform . Now promise in 2017 . The professional team but promises do not withstand criticism . It is a pity that such promises are not fulfilled . <p> Alex . 43971 @qwx453971 <p> Hi Alex , I never heard them make any promise that it will @ @ @ @ @ @ @ @ @ @ and the answer was always beginning of 2017 . Yes , it is a bit evil to make us excited about the new features and lthen et us wait so long . But I rather wait and have a perfect product without to many bugs . <p> Hi Alex , I never heard them make any promise that it will be in 2016 . I asked them a few times and the answer was always beginning of 2017 . Yes , it is a bit evil to make us excited about the new features and lthen et us wait so long . But I rather wait and have a perfect product without to many bugs . 43971 @qwx453971 <p> That 's funny . What you have not heard of in 2016. since April 2016 they have hung on the site message " Coming soon in 2016 " A week ago they changed the inscription . 
@@44333025 @4333025/ <h> Reference Monitor advice <p> No - not any specific clients , although we have done such matching ourselves for clients evaluating the Sony . Metameric failure is an issue with any OLED , and you need to use the ' Perceptual ' approach to calibration to generated the needed offset matrix . ( Do not use Sony 's suggested Judd offsets as they do not work ! ) See : LONG ... <p> Also , the better the Spectro you have ( the narrower the bandwidth ) the better . ( Also true of laser projection ) The CR-250RH at 4nm is good , but the CR-300RH at 2nm is ideal - and is being used by many high-end display manufacturers as the go-to Spectro ... <p> Interesting news from Canon - but everyone I know who evaluated their 4K hated it ... <p> I need advice however - a problem that 's not too bad to have - a decent OLED , more than a PVM . Given the choice of a BVM-E250A with 1000 hours on it or a new Flanders DM250 at a similar price @ @ @ @ @ @ @ @ @ @ - but everyone I know who evaluated their 4K hated it ... <p> I need advice however - a problem that 's not too bad to have - a decent OLED , more than a PVM . Given the choice of a BVM-E250A with 1000 hours on it or a new Flanders DM250 at a similar price which would you pick ? 43971 @qwx453971 <p> Flanders . Same OLED panel in both . The DM250 has their top-of-the-line CFE3 color management back end . You will have more accurate internal calibration than with the Sony . <p> Thanks Patrick - is that what you use ? Client facing ( " zunzheng " logo and all ) ? BTW my son 's just arrived in Vancouver , planning to find a way to live there ... 43971 @qwx453971 <p> I have used just about every Sony 24.5 " and 17 " OLED . Always a problem getting satisfactory calibration without using an external LUT box . I finally purchased a pair of CM250s and use them daily on set . The DM250 has a few extra useful features but I am @ @ @ @ @ @ @ @ @ @ dark environment , no one sees the logo and rarely asks me about what kind on monitors they are looking at . Flanders monitors are quite common on set here in Vancouver . <p> Cool FSI seem most popular with DITs and people working with LUTs in the field - this is for use in a suite , so the kind of gold standard reference thing is important . Once it 's set up , it stays put . Not to diminish those qualities at all ... just a slightly different daily use case , looking at graded rather than raw + LUT . I can see the benefits of lots of LUT control - but I really want to calibrate occasionally and grade , not tweak , most days , on something that does n't move around on me . <p> To add to that , I do n't have a huge need to " user calibrate " if the panel is in a stable zone that engenders confidence - whether or not it 's absolutely accurate matters less than meeting a standard , however flawed , that everyone @ @ @ @ @ @ @ @ @ @ . I know that 's the antithesis of accurate calibration but given the diversity of delivery platforms these days , I really just want my clients to be confident they 've hit some kind of midpoint that 's the same as they 're used to seeing . Not scientific I know ... <p> But that is the point of using an accurately calibrated display when grading . It is the only way to know you are ' hitting some kind of mid point ' . Without being accurately calibrated you will potentially be ' skewed ' towards an inaccurate average , and the potential is that when viewed on an even poorer calibrated display the inaccuracy will be exasperated . <p> Yep , I get that . So you 're a fan of the Flanders approach - and I think I am too , just do n't want to feel on the back foot in having to defend why a Zunzheng monitor really is as good or better than a Sony ... So how do Sony charge what they charge ? Are there any hidden smarts in there ? The @ @ @ @ @ @ @ @ @ @ E250A is much less . <p> Short of an X300 , price is less of an issue than cred - I know , I 'm shallow ( well , my clients can be ) so there 's value in avoiding difficult questions ( and Sony knows this ) . <p> While the FSI factory is in China , at Zunzheng 's facility , the design of the monitor is all FSI ( with in house colour management via LightSpace CMS - which is why we know the display 's so well ) . Where do Sony manufacture their monitors ? I have been told some is also in China , but honestly do n't know . <p> However , it is the design of the internal image processing electronics that is key - and FSI really do have that right . <p> ( It is worth stating the while FSI do use LightSpace CMS for their factory calibration , and we have helped with their colour management , we have absolutely no financial involvement in FSI , and make no financial gain on nay FSI sale . we just appreciate @ @ @ @ @ @ @ @ @ @ for the wise words - I evaluated a earlier BVM-E250 against a CM250 from Flanders - so neither was the ACTUAL model I was looking to buy - and ended up ordering a new DM250 , against a used BVM-E250A . <p> Happy with my choice on that - decent P3 , good Rec.2020 emulation . Now to figure out how to gracefully toggle in Resolve between P3 projects and Rec709 TV with everything correctly set up for each ... 
@@44333026 @4333026/ <p> Man has been on the moon so how hard can it be ? Most of the things on your list are just " making it tidy " things imo. 43971 @qwx453971 <p> Naaa , there 's busted stuff and stuff missing . I think that goes beyond tidy , particularly if it was working fine before and now is n't there , or is drastically changed , or has been made far less usable . <p> There 's a couple of other issues but I wo n't comment for the moment . The good news : just did two 16-minute renders with all kinds of defocus windows and TNR and had zero Mac glitches . ( I think . ) <p> 29 . Dockable , floating or full-screen scope-window . Every part of the Resolve GUI has a place on-screen expect the scopes . For example scopes could take the place of the key frame section optimally on a single screen system . 43971 @qwx453971 <p> Scopes are docked with key frame timelines , or floating , or full screen on second monitor . <p> Actually , all @ @ @ @ @ @ @ @ @ @ have to do it double click on the codec info where version used to be and it comes back . This was reversed in the last version . I actually prefer it this way initially so that I know to treat clips differently when not using ACES . I wish they would allow you to customize the info that shows up here . Showing the grouping info would be very helpful to toggle on . <p> A couple of audio related issues ( in addition to all the great suggestions outlined above ) : 1 ) How do you remove VST effects that are not used in Resolve , but will boot up for those of us who also do audio work in Logic and ProTools . For example , I do not need to see ( nor can Resolve make use of ) Steven Slate Drums , Kontact , or Absynth . Solution : allow the user to remove certain VST products from loading up in Project Settings or Preferences ; 2 ) It would be nice to be able to undock and move the audio mixer around so @ @ @ @ @ @ @ @ @ @ mixer over to the main GUI screen where the timeline is - right now , I have to look at my second monitor ( to the left ) while missing information I want to see on the main GUI screen ( over on the right ) . I 'm starting to get cross-eyed <p> I realize there are a lot of issues to address , but these would be nice issues to tend to at some point - especially , #2 . Oh ... and thank you for audio automation ! ! ! ! The audio engineer and editor in me is thrilled ! <p> Just finished the 2nd of 3 projects in Resolve 12 and I 'm moving along . The editing is very " peppy " ( for lack of a better word ) , and most of the control response is fine . Zero issues with the Avid MC panel -- though mapping could always be better . Keying is cleaner and better in v12 , and if I did n't know better , it seems to play back R3D files with fewer problems . <p> Knocking @ @ @ @ @ @ @ @ @ @ . Operating temperature of the machine has gone from 137 degres to 142 degrees ( ! ! ! ) with v12 , but it 's OK for the moment . <p> I love the new update , but things are definitely not working as well as I hope they would be so far . I certainly hope the full release can address the many issues as well as tidy up some of the smaller UI related stuff . Currently using it as a test run on a small personal project and I 'll be compiling a list for minor improvements and complaints as well . ( Disclaimer : I have yet to read the new manual and have only been playing for a day , so hopefully some or more of these issues are addressed in there somewhere when I get around to reading it ) <p> MAC OSX YOSEMITE <p> 1 . Compound nodes . <p> It would be nice to keep node names after you decompose a compound node . <p> I have literally failed 14/14 times to make 2 compound nodes work with a layer mixer . @ @ @ @ @ @ @ @ @ @ layer node to the output , or the moment I connect the 2nd compound node to the layer mixer that 's already connected to the output . <p> When inside the compound node , highlight differences does n't work . You ca n't even see what your key looks like except in the tiny node thumbnail . <p> 2 . Working with Red footage . <p> As Marc noted , it was very useful in v11 when you could see the camera metadata side by side with the clip settings . <p> Let 's say I accidentally added Clip A 's grade to Clip B. When I undo it , all the nodes are reverted , but the clip decoder settings do not revert . An undo button just for these settings would be nice , but this would be all fine and dandy if we could compare the clip settings to the project &amp; metadata settings . Having the option to " save " the current state of a clip 's settings to be included in the Raw pallet for comparison would be even better in my opinion . @ @ @ @ @ @ @ @ @ @ Relink to clips " option after copying your footage to another drive or folder using the Media Management menu ( formerly consolidate clips ) , ALL of the clip settings in the Camera Raw pallet is reset . Big wtf for me . By default , it should n't do that or they can give us a check box that allows us to choose whether we want that to happen or not . <p> 4 . MISC <p> When I try to delete an entire bin with missing clips in them , Resolve crashes . When I make a large selection of the missing media to delete , it also crashes . 100% . The only time it does n't crash is when I delete clips individually or select only a few at a time . <p> Titles . Oh dear god titles . I opened an old project with lots of titles in them ( for subtitles ) . Imagine having to change hundreds of titles one by one because the director wanted a different font ( would be much easier for the editor to do it in the @ @ @ @ @ @ @ @ @ @ option change a mass selection of titles ' options including font , size , spacing , tracking , alignment , etc . <p> To be updated . My patience ca n't take anymore crashes for today . A few dozen too many . <p> 1 Multicam clip shares same remote grade on every cut , this is not good - most of the time every angle would need different grade . I know there is possibility to grade sources on multicam timeline , but its not perfect . <p> 2 In delivery page if you go to print to tape , without any recorder connected , and then return to file delivery presets would gone . <p> 3 Most of a time 3D keyer would not subtract patches for me ( new patch appears in list , but nothing happens ) . <p> 4 It would be good to have ability to change optimized media resolution on a per clip basis ( or per source format ) , 6k red does n't  need to be cashed in full res for online delivery , but 1080p from some DSLR or GoPro @ @ @ @ @ @ @ @ @ @ be great to have shortcuts to select previous/next node . <p> 6 It would be great if printer light hotkeys would not only affect offest , but with modifiers shift/opt/cmd will affect lift/gamma/gain if primary wheels active and **25;1998;TOOLONG if log wheels active . <p> Have done two short sessions with V12 last week , I enjoyed the improvements , but as for critiques : <p> 1 , Some UI elements are too " adaptive " , not only making it harder to map keyboard shortcut based on screen coordinates , but also making it harder to reach using a mouse . The drop menu of Qualifier panel is the hardest . Because the text label " 3D " / " HSL " is significantly shorter than something like " Hue VS Sat " . I can see a better solution is to make the actual click sensitive area wider and unified , instead of based on the length of text . Simplicity is good , but I think unity of user experience is also important . <p> 2 , I also found some short cut malfunctions . like the @ @ @ @ @ @ @ @ @ @ . And if click L or J multiple times for fast forward/backwords , then press K to stop , and use L/J again , sometimes they will just function normally , sometimes they become " previous/next frame " . <p> 3 , The Clip names on the editor timeline become " messed " , twice . Might be a graphics-related bug . <p> 4 , The " Apply " button used to be on the " project settings " panel is gone , and pressing " Save " automatically closes the panel . Which is not efficient when you are messing with settings . <p> 5 , Can not single click on the lower part of a node to Disable/Enable it like used to . <p> 5 , Can not single click on the lower part of a node to Disable/Enable it like used to . 43971 @qwx453971 <p> It 's actually on the top part now . Totally throws me off . Also , I 'd like to add that now that the disable/enable is on the top part as well as the number of the node on the @ @ @ @ @ @ @ @ @ @ to rename my nodes by left clicking ... <p> I think v11 used to allow you to label a fresh node from left clicking the top ( could be remembering wrong ) , but now you have to right click to name a fresh node , and double left click the label to rename a node . But if your clicks are n't fast enough or you accidentally click the number , it will disable the node instead . <p> Found a couple of thngs : loading saved window shapes do n't load was hoping that clipping after a lut or soft clip would be solved , not so ( yet ) crossfades in audio are mute no mp3 import crashed on moving an audio fade <p> was hoping that clipping after a lut or soft clip would be solved , not so ( yet ) ... 43971 @qwx453971 <p> Clipping is generally a destructive operation , even in a 32-bit processing situation like Resolve . Once you 've smashed highlights or crushed blacks with LUTs or clips , you ca n't get them back . I think this is @ @ @ @ @ @ @ @ @ @ a correction that gets you 90% of the way there without doing any destruction . Use clips at the very end of the node tree . <p> was hoping that clipping after a lut or soft clip would be solved , not so ( yet ) 43971 @qwx453971 <p> A LUT by it 's nature has to limit the range of its input , but that does not need to be 0.0-1.0 . The output range is unrestricted . So although people tend to think " LUTs clip to 0.0-1.0 " , that is not entirely true . It depends on the LUT . Most of Resolve 's built in LUTs may do , but for example some of the VFX IO LUTs can handle input much greater than 1.0. 
@@44333027 @4333027/ <p> What I like about all of this is how much power and performance Resolve is gaining . And now the Studio price has been lowered from $999 down to $299 . Altogether , the power , performance , and price is going to put more pressure on Adobe 's evil subscription scheme at $50 a month for $600 a year . There 's only three remaining tools I absolutely have to have from Adobe and that 's After Effects , Dreamweaver , and Photoshop . The rest I no longer use , so I feel like all of the other tools I 'm paying for are just a waste of subscription money . <p> Overall , I 'm starting to see a lot more consumer backlash and resistance specifically over the pay-to-play type subscriptions . I think Blackmagic 's marketing team is picking up on that dissatisfaction . And it appears they are using that undercurrent to take a shot at Adobe . Here 's a semi-blunted image from the Resolve 14 page . LONG ... <p> I agree Marc . I think you 've said it like @ @ @ @ @ @ @ @ @ @ the most . I guess , I never thought Blackmagic 's intent was to replace an entire sound suite with the Fairlight addition . I only see it as a welcome ( and powerful ) convenience when it 's right there in the app . Would it have been better as a separate app like Fusion ? ... maybe . But like you and others have said , as long as the Fairlight addition does n't disrupt the stability of Resolve , then I think everyone will be cool with it . <p> As for other questions ... Would it be interesting to see Fusion as the next tab added ? Yeah , I think so . Would Resolve be the ultimate mega monster for post at that point ? Yeah , I think so . Would Alexis Van Hurkman have a heart attack while trying to rewrite the Resolve manual at that point ? Yeah , I think so . <p> Pro Tools is certainly an industry standard for now , but in the future it wo n't be . Anyone who understands the pulse of " La La @ @ @ @ @ @ @ @ @ @ and music composers are already using alternatives to Pro Tools like , Ableton Live , Cubase , FL Studio , Sonar , Reaper , Studio One , etc . At the moment Ableton Live and Cubase are at the top in popularity , but a little dark horse named Bitwig Studio is off and running as the new DAW gaining market share at record pace . <p> ... 43971 @qwx453971 <p> Neundo is pretty popular , too . Not even sure if I 've spelled that right . <p> In my repeated Requests to BMD In writing and on the phone to get hooks built in for Mackie control for mixing levels , ( just like FCP had ) they told me they are getting that request a lot . The audio portion was really holding back Resolve as an NLE . I noticed a lot of problems playing audio with the mixer levels exposed . <p> Just like they knew the underlying realtime video playback engine was problematic , BMD searched for a solution and delivered in spades . <p> fairlight is n't just a new tab or feature @ @ @ @ @ @ @ @ @ @ also needed because of how Resolve was problematic with plug in like Izotope which I own but was n't using much because of it . <p> It 's clear BMD does n't just look for fixes and bandaides , they look for and deliver solutions . It 's mature and responsive to people 's needs . I wish Apple were the same . <p> The NLE function itself was something asked for by colorists needing some simple tools to fix last minute requests from clients after grading was In progress , and BMD over delivered . Pehaps they saw requests from people like me asking to make this basic functionality a replacement for FCP7 . I actually held onto fcp until resolve 12 . <p> someone In this thread mentioned no one wanting to be forced to grade in Premier . The facility I bought my advanced panel from told me the reason they were moving away from resolve is because in their market , the clients did n't  want to pay for high end grading . their work had become Mainly reality shows and they wanted to grade inside @ @ @ @ @ @ @ @ @ @ that for them . I wanted to do the same thing just on a different platform . So this clearly is a trend . <p> When I jumped on FCP the week it was released , there was lots of issues , missing feature , industry pushback and disdain and we all know how that eventually did gain market share . <p> Resolves end to end picture workflow has given me confidence to be able to deliver feature projects ( which I am hoping to move into ) . I may not do the final audio work in house , but for my short form projects , having proper audio tools , NLE and grading without round trips , and the ability to add real shared workflow when needed is spectacular . <p> I 'm also a grading junkie and often ca n't wait to see what I can make things looks like and often grade a bit before the edit is done or even started . <p> There 's only three remaining tools I absolutely have to have from Adobe and that 's After Effects , Dreamweaver , and @ @ @ @ @ @ @ @ @ @ There really has n't been anything substantial enough added to photoshop since CS4 to justify upgrading it , so we still use that , quite successfully , on an old 3,1 MacPro . Paid for a decade ago ... <p> We use AE on that same machine , and my only complaint is that it 's the 32bit version , and could really use a speed bump . But I 'm not willing to spend money on the subscription for that , simply because it 's not something we use enough anymore to justify it . When we do , it 's for stuff that ( in most cases ) could be done in Scratch or Resolve , or sometimes even in FCP7 ( which still works fine for a lot of stuff ) . <p> the subscription thing is funny - I really do n't have a problem paying monthly for Scratch . We get it when we need it , and the cost works out . But I feel like , at least for us , Adobe has become increasingly irrelevant over the past few years and we @ @ @ @ @ @ @ @ @ @ old versions . <p> I really like access to the fairlight tab , especially to manage smaller projects , to do basic audio compression for rough cut outputs on festival submissions . That said , it looks like it 's still a pain to do very simple channel mapping . <p> 99% percent of what I want to do with sound on resolve is simple . Take mono tracks and properly assign them to the channels I want to , to do 5.1 monitoring and output of a finished mix . 43971 @qwx453971 <p> THIS ! ! ! At my job we use the premiere XML roundtrip workflow . I really want to be able to stay in Resolve for delivery . 85% of what 's keeping me from that is audio mapping . I talked to a BM engineer at the booth and he said with Fairlight this in now possible . I have n't had a chance to test the beta but this is the first thing I am going to try . <p> I 'm gon na be one of the naysayers who thinks that Fairlight is overkill @ @ @ @ @ @ @ @ @ @ overkill for Premiere or Avid as well . ) I would much rather they release Fairlight as a standalone program on its own . To me , complex sound editing and mixing is a completely different skill set than color or picture editing ( or VFX ) . 43971 @qwx453971 <p> The advantage of having them all in one app they way that BMD has done it is that clicking on a new tab replaces the process of exporting an OMF or XML or AAF or what have you and then importing and conforming it so that you can start working on color or audio . <p> But of course there will be people who start calling themselves sound designers or colorists because they can click on the " Fairlight " or " Color " tab and get a professional tool set . <p> Overall , I 'm starting to see a lot more consumer backlash and resistance specifically over the pay-to-play type subscriptions . I think Blackmagic 's marketing team is picking up on that dissatisfaction . And it appears they are using that undercurrent to take a shot @ @ @ @ @ @ @ @ @ @ has said many times in interviews he 's absolutely opposed to software subscriptions for a variety of pretty good reasons . <p> The advantage of having them all in one app they way that BMD has done it is that clicking on a new tab replaces the process of exporting an OMF or XML or AAF or what have you and then importing and conforming it so that you can start working on color or audio . 43971 @qwx453971 <p> The disadvantage is that you now have a bloated program with twice as much code and more potential for flaws and crashes . <p> And then there 's the people who will believe , " now that I have a color program AND an edit program AND a sound mixing program , I 'm qualified to do all three ! " <p> I used Pro Tools for about 12-13 years in my old home studio ( which I spent about $50K to build ) , and I only considered myself only an amateur at best . I shudder to think what 's gon na happen when people with no money @ @ @ @ @ @ @ @ @ @ speakers in a closet . <p> I still say this is a hammer and a wrench in one tool . I do n't have a problem with Resolve having some editing functions , because that 's necessary for conforming and finishing . <p> The disadvantage is that you now have a bloated program with twice as much code and more potential for flaws and crashes . 43971 @qwx453971 <p> True . You ca n't win ' em all , I suppose <p> And then there 's the people who will believe , " now that I have a color program AND an edit program AND a sound mixing program , I 'm qualified to do all three ! " 43971 @qwx453971 <p> Yes , the " My mom bought me a Red , and now I 'm a cinematographer " syndrome , though now they do n't need mom 's money to pull it off . <p> I used Pro Tools for about 12-13 years in my old home studio ( which I spent about $50K to build ) , and I only considered myself only an amateur at best @ @ @ @ @ @ @ @ @ @ when people with no money and no sense try to mix a movie with $99 speakers in a closet . 43971 @qwx453971 <p> The ones with more money and even less sense are even more worrisome . <p> I still say this is a hammer and a wrench in one tool . I do n't have a problem with Resolve having some editing functions , because that 's necessary for conforming and finishing . 43971 @qwx453971 <p> I like the idea of not having to conform , but the three dedicated suites that I had a chance to get demos of just seem so much easier to work with ... <p> put more pressure on Adobe 's evil subscription scheme at $50 a month for $600 a year . 43971 @qwx453971 <p> I do n't think Adobe is really feeling that pressure . Subscriptions keep going up , and the cloud offering is pretty tremendous . It 's also far from evil to try to actually find a model that makes the hard work of developing software worthwhile . <p> BMD sells hardware , as does Apple and Microsoft ... @ @ @ @ @ @ @ @ @ @ . Autodesk is also moving everything over to subscription . As will likely be the case for any other heavy duty software company . The reality is that 's the only model which has any chance of supporting standalone software development going forward . Folks who want robust software development should welcome it , as it motivates developers to be constantly upgrading and adding features which are actually useful to users not marketers . The leaps that premiere has made as a subscription application are tremendous and the suite is a must have for anyone working professionally in post today . As is Resolve . <p> I suspect the only reason software sales were a buy once forever type deal in the past is because the computer industry was experience insane growth in user base . That growth has slowed now . <p> BMD is currently relying on growth in the industry as a whole . Petty talks about bringing high end tech to the masses over and over again . My understanding is he started the company so people could buy cheap capture cards to hook up rented decks @ @ @ @ @ @ @ @ @ @ indie projects . These doofuses who think they can do everything are key to BMD 's continued success . <p> I suspect the only reason software sales were a buy once forever type deal in the past is because the computer industry was experience insane growth in user base . That growth has slowed now . 43971 @qwx453971 <p> That and the internet . Which if the aim is to hit a fixed target , will eventually get there for free . That is , that what people were paying for was access . Piracy bit into that tremendously . And open source software development was getting there . Also once you have a good enough solution , it 's possible for companies to just seize innovative development and keep cashing in ( read : protools ) by lowering the price over time you keep growing the user base and continue to generate revenue for a while . But eventually that value becomes commodotiesed and the commodity cost of a copy of a piece of software is basically zero . <p> Truth is editing is headed the same way as @ @ @ @ @ @ @ @ @ @ And so the solution from an incentivization perspective is to change from a commodity to a service . Then you can sweeten the deal in other ways , by adding related functionality ( a suite ) or services ( creative cloud storage ) or in the future things like cloud processing and machine learning . I for one wish there was another player competing with Adobe in the cloud , and honestly think it 's just a matter of time and opportunity before google jumps into the game and eats up the entire bottom end of the market . <p> I do n't think Adobe is really feeling that pressure.View attachment 4967 Subscriptions keep going up , and the cloud offering is pretty tremendous . It 's also far from evil to try to actually find a model that makes the hard work of developing software worthwhile . <p> BMD sells hardware , as does Apple and Microsoft ... who also sell services . Google sells you to advertisers . Autodesk is also moving everything over to subscription . As will likely be the case for any other heavy duty @ @ @ @ @ @ @ @ @ @ model which has any chance of supporting standalone software development going forward . Folks who want robust software development should welcome it , as it motivates developers to be constantly upgrading and adding features which are actually useful to users not marketers . The leaps that premiere has made as a subscription application are tremendous and the suite is a must have for anyone working professionally in post today . As is Resolve . 43971 @qwx453971 <p> Well said ! <p> It 's exactly this reasoning why I had trouble paying my Avid subscription vs Adobe . I feel I pay for development and where Adobe shines , Avid is barely moving . <p> I still say this is a hammer and a wrench in one tool . I do n't have a problem with Resolve having some editing functions , because that 's necessary for conforming and finishing . 43971 @qwx453971 <p> From where I am standing right now , this fairlight move by BM is very smart . I am an editor and this makes resolve very sexy for us . It-s been a hit in my editors @ @ @ @ @ @ @ @ @ @ and even tho I am an Avid editor , which is very capable on its sound fearures , it 's no protools . Actually , I believe integration with Protools was one of the top feature requests on our last ACA vote . Now , avid has been siting on top of game leaders for years , and we still ca n't get them to properly talk to each other . <p> I recently moved into this space with a colorist plus a sound designer . We 've been discussing if we should by a server but got into the conclusion that we could live without one , only using HDs , since our work rarely overlap each other . Now , with a software that could potentially unify all of those , I think we need to rethink that . That 's how gamechanging this could be . <p> It 's my dream to have a sound person working alongside me , not having to rely on ediload or anything . Let 's see how Resolve handles a db with hundreds of hours of footage and a offline/online/proxy workflow @ @ @ @ @ @ @ @ @ @ that pressure . Subscriptions keep going up , and the cloud offering is pretty tremendous . It 's also far from evil to try to actually find a model that makes the hard work of developing software worthwhile . 43971 @qwx453971 <p> 1 . A stock price is not an indicator of customer satisfaction . <p> 2. " ... cloud offering is pretty tremendous " - This is incorrectly assuming that " subscription " always means a full CC suite , when the majority of Adobe subscription numbers are from the $9.99 per month ( Photoshop/Lightroom ) two program subscription . <p> 3 . No one is saying that hard working software developers should not be properly compensated . The evil part happens when Abode holds your intellectual property hostage unless you pay to access your own IP . <p> 4 . Millions are dissatisfied with CC because it 's pay-to-play , which means the moment you stop paying is the moment you stop playing and then you have no software for your source files . You must keep paying Adobe until the end of time . At least with @ @ @ @ @ @ @ @ @ @ do n't want to upgrade you still have software you can use instead of zero software . <p> 5 . People are further dissatisfied the CC suite because they are paying for a lot of programs they do n't use . Right now I need Photoshop , Dreamweaver , and After Effects , so I 'm paying for 23 other programs I do n't need , want , or use . Adobe 's suite offering is the " a la carte " problem . It 's similar to what US cable companies like Comcast and Charter/Spectrum are facing , which is why people are unhappy with cable TV and ditching it . They only watch a few channels and then they end-up paying $100 to $200 a month for 500 to 1,000 other channels they do n't watch or want . <p> 6 . Adobe is running on a limited clock . Subscription sign-ups are estimated to be running thin by 2023 while the churn rate increases . This means they will likely counter by increasing prices before that time . <p> 7 . A subscription does not guarantee that @ @ @ @ @ @ @ @ @ @ ( Avid and Autodesk subscriptions being perfect examples ) . <p> Forbes Magazine : March 27 , 2017 - " While access to the complete Creative Cloud suite costs up to $74.99 per month , access to Photoshop and Lightroom is priced at $9.99 per month . We estimate that the blended ARPS ( Average Revenue Per Subscriber ) for the company was $27.85 in 2016 . The recent trend in subscriptions indicates that users are subscribing to de-itemized versions of the software of CC instead of the full version of Creative Cloud . Accordingly , we expect the ARPS to decline in the coming years to $26 by the end of our forecast period . " <p> This is why I like what Blackmagic is doing . They are offering a very powerful alternative with a perpetual license . In their recent Resolve 14 marketing , they are smart to target people who are unhappy with Adobe and Avid subscriptions . <p> $50 a month for these 3 apps is still pretty damn cheap , historically . And you always have the latest version . 43971 @qwx453971 <p> I @ @ @ @ @ @ @ @ @ @ Premier , inDesign , Media Encoder , and Acrobat Pro ( all of which I use ) , $600 a year for the latest version is not ridiculously expensive . I do agree that their $20-per-app monthly fee is silly . <p> I see Daniel 's point and I wonder how much longer Adobe can keep the subscription model going . There 's a lot of hostility to it out there from the editors I know . I 'm still reluctant to go for the idea of a Swiss Army Knife program that does editing and sound and color and VFX all as one program -- but I would have no problem with four programs you could use on one computer or on four separate computers , all part of a single suite . The danger to me is that Fairlight was always an also-ran among the sound mixers I know , and Pro Tools was always the gold standard there . But maybe that 's an LA thing . <p> The integration is worrying , I hope if it is a problem they will cleave the apps apart again @ @ @ @ @ @ @ @ @ @ at which they integrated Fairlight . There are probably some serious bugs lurking beyond the surface . <p> Also , what sound mixer wants to put a 300 watt GPU in their machine that they do n't need ? That 's a lot of extra heat and noise . <p> As a freelancer , I like the subscription model , because it prevents the situation where I have to shell out an enormous sum to upgrade because one of my many clients did . It keeps costs predictable . If I go though a lean time , I can always cancel it for a while . <p> If I was running a self contained post-production unit that never worked with freelancers , I might prefer the pay once model . I suspect this is a big target market for Resolve : In-house production or small full-service **26;2025;TOOLONG companies . <p> At the same time , in a larger organization , the subscription might be nice because you never have to convince the bean counters to let you upgrade . 
@@44333028 @4333028/ <p> The Northlights are good machines but they are slow and fussy , it is basically the Domino scan updated and it 's not so much the cost of the machine but the upkeep on it . 43971 @qwx453971 <p> No , the Domino scanner was CRT-based , and the Northlight scanner is CCD , with a completely different transport . The Northlight also weighs about ten times more , mostly because of the marble base . I bet the Domino is another one of those early-1990s products that would sell for $49 on eBay . <p> I really , really like the look of the Northlight , and I 've used the Arriscanners , four or five different Spirits , Kodak 's Lightning scanner , the Imagica Scanner , and ILM 's custom-built in-house scanner , but each of them has pros and cons . I think the Spirits are tops for speed , but the Northlight was by far the most reliable and best-looking overall . I hated the look of the Imagica with IP . It was good with negative , but anything print-related is a @ @ @ @ @ @ @ @ @ @ point . There 's a lot of cost-effective CCD scanners out there like the Kinetta and the Image Trends ScanMaster , plus the new Blackmagic Cintel scanner ( $25K ! ) , and all of them can work and do a reasonable job . <p> The Quantel Domino system was a line array CCD scanner ( old 8-Bit Trilinear CCD ) and a CRT Recorder and was build as a cheaper alternative to the Kodak Cineon system I believe . I am pretty sure the guys who built it ended up at Filmlight but I could be mistaken but I think I know who to ask .... <p> The Imagica and Northlight both use the same Kodak trilinear CCD arrays , the sensor for the Northlight-1 is not available anymore but the Northlight-2 sensor is from Trusense ( LONG ... ) both use a hot light source and a linear motor to move the sensor or film , so the difference must be in the sauce . <p> I think both line array ( northlight , spirit , goldeneye ) and flying spot ( Cintel , Rube Goldberg ) are @ @ @ @ @ @ @ @ @ @ All the new scanners like the Kinetta , Blackmagic , ScanMaster , LaserGraphics and the Xena machines ( yes there is another developer of scanners in LA ) we have at Cinelab are based on area sensors ( either B&amp;W or Bayer ) and LED or LED+IR illumination and in our case the Xena machine has a modular software build and interchangeable sensors . We have one machine setup with this : ( http : **31;2053;TOOLONG ) for speed and small gauge ( 8mm , 16mm , 35mm ) and a pin registered machine which I am debating on these two sensors for : ( http : **31;2086;TOOLONG ) for single tap 6K+ or this ( http : **31;2119;TOOLONG ) which has a dual amp system for better DR on print material . <p> I have generally liked the Arriscan frames I have worked with but I think there are better sensors ( like the 6620 in monochrome single tap mode ) than the CMOS chip Arri used in it . <p> I think I might know too much about the lost world of motion picture scanning time for a @ @ @ @ @ @ @ @ @ @ array CCD scanner ( old 8-Bit Trilinear CCD ) and a CRT Recorder ... 43971 @qwx453971 <p> They told us CRT when we had one in for evaluation at Complete Post , sitting in a room for a month down the hall from my bay . Maybe I heard wrong or was drunk and stupid . The results looked like crap -- that much I remember . CFI had some home-brewed thing they used to use for high-res film-outs , and I think it was essentially their imitation of a Domino . God , getting film-space LUTs right in that era -- 1999-2005 -- was an absolute nightmare . <p> I also heard that Quantel lost a mint on the whole Domino debacle . As bad and slow as the Kodak Cineon stuff was , at least the second generation was all solid-state CCD and it was pretty reliable . I occasionally had to use the tube laser ( ! ! ! ) film recorder at Cinesite , and that was no fun , circa 2001-2004 . It was kind of like a " Forrest Gump " box of chocolates @ @ @ @ @ @ @ @ @ @ get . <p> Like I say , all of this stuff is now worth $49.95 on eBay . It 's incredible how worthless this tech is today . My memory is that they quoted us $1.5 million for the system ( circa 1995 ) , and my boss balked at it . <p> The Northlight would at least still be useful and Northlight is still supporting it ... but I tend to doubt they 're selling a lot of new ones . I 'd bet that everybody in the world who wanted a film scanner has already bought one at this point . I 'd be very curious to see how many people buy the BM Cintel for $25K . <p> You could be right , maybe Quantel had a CRT/Photomultiplier based scanner early on , I know the ones I have seen were CCD so maybe they were second generation ? I remember seeing the whole Quantel system at a show when I was in College at NYU in 91-92 ? Super ultra expensive and definitely worth about $49.95 now . <p> As I was product manager for Domino @ @ @ @ @ @ @ @ @ @ based around Domino , I can probably answer accurately <p> The scanner was a 12bit trilinear water cooled CCD array , and was very , very accurate . The Film Recorder was CRT , and based on the Solitare , but heavily modified and with unique and patented flare reduction technology . <p> At the time the whole Domino system produced images that were class leading . <p> I remember doing ' Lost In Space ' with just about every other ' film ' company in Soho , and where it cane to the daily screenings of work-in-progress it was always the Domino based work that got ' OK 'd ' more often first time round . Facilities using other ' digital film ' systems had multiple issues with image quality , etc . ( To be fair CineSite 's shots were alos OK 'd first time more often that not ) . <p> Mind you , the film I 'm personally most proud of from my Men In White Coats days is Shekhar Kapur 's ' Elizabeth ' . Some 100 vfx shots , if I remember correctly , @ @ @ @ @ @ @ @ @ @ Quantel made a mint out of the product , as it did become the standard round the world for independent digital film facilities . And in those days all Quantel kit sold for huge multipliers against build costs ... Kodak were the ones that lost a bundle on Cineon - especially as they gave up on it way too early ! <p> The FilmLight team were nothing to do with Domino , and in fact came out of CFC/FrameStore - although FrameStore were a Domino house . <p> You could be right , maybe Quantel had a CRT/Photomultiplier based scanner early on ... 43971 @qwx453971 <p> That is possible . I 'll take Mr. Shaw 's word if he was working there . <p> This was very early ... possibly as early as 1993 or so . I know our boss was apoplectic at the thought of spending over a million bucks on the thing . Right around this time , they spent a million bucks on an Inferno ( or whatever it cost ) , so I think they were very conscious of what all this cutting-edge tech was @ @ @ @ @ @ @ @ @ @ an investment it took to get into a serious color-correction room in the late 1990s/early 2000s . I think our rooms were at least $2 million each , more if it had a Spirit 4K , and you figure in this era we were also tying up a $100K HD tape machine or a $100K Clipster . <p> Kodak were the ones that lost a bundle on Cineon - especially as they gave up on it way too early ! 43971 @qwx453971 <p> Kodak was kind of forced to give up on it , because of all the lawsuits that happened at the end of the 1990s . Many post houses ( especially in LA ) were unhappy that Kodak manufactured the Cineon compositing software and the scanner &amp; film recording system , but also owned the Cinesite VFX facilities , making them a supplier and a competitor . The customers believed ( wrongly , I think ) that Kodak was holding on to the latest updates and delaying giving them to other VFX companies in order to give themselves an advantage . The truth was that the Cineon software @ @ @ @ @ @ @ @ @ @ find all the bugs and fix them . <p> But the lawsuits continued , and as part of the settlement , my understanding is they refunded a lot of money ( the Cineon software and hardware cost a bundle ) and exited the market rather than create the perception that Kodak was trying to monopolize the market . Interestingly , a decade later , Sony was in a similar position , where they own or control several post houses yet they also manufacture very important industry hardware , also making them both a competitor and a supplier . <p> I was wrong - the CCD was mono-linear , and scanned RGB in three separate passes . Made the quality very good as all pixel colours were perfectly aligned , but slow ... I think they got the speed down to 12 seconds per frame- down from 40 or so ... <p> Cintel had the Millenium II which would do 4K scans ...... if you had a good tube ..... ; -0 ) 43971 @qwx453971 <p> Doh , that was terrible . The C-Reality was n't terrible , but it was @ @ @ @ @ @ @ @ @ @ tube was n't so bad , but the transport crashes would kill you . I much preferred the Thomson/Philips/DFT Spirits , particularly the Spirit 4K , which were incredibly reliable and consistent . <p> I did like the Northlight 2 scanner , which I think is up to less than 1 second a frame in 4K . You figure with 30,000 frames in a 20-minute feature roll ( assuming old-school archival material ) , that 's just 8 hours a reel which is not that bad for pin-registered stuff . On the other hand , the $200,000+ price of the Northlights is tough in the face of the $25,000 from the Blackmagic Cintel 