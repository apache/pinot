
@@105506868 @906868/ <h> New Career Resources Content <p> At our UXPA 2017 International Conference in Toronto , we acknowledged a cosmopolitan group of selfless professionals with the UXPA Volunteer Service Award . We thank this generous team who , over the years , have helped build a UX body of knowledge in five languages and significantly furthered our mission of making UXPA and UX much more international in focus . <p> From the amazing keynotes , to the courses and conference sessions , there was no shortage of great content and knowledgeable presenters . - Networking took shape in many forms " from our receptions , to offsite excursions , to meet and greets in the hotel lounges . - Whether it was catching up with old colleagues , or meeting new friends " the week was full of energy and enthusiasm for all things UX . <p> Randolph Bias , Ph.D , CHFP ! While we do n't have a gold statue to present , we have something better ! Not only will Dr. Bias receive our award at the UXPA 2017 International Conference " he will be onstage , @ @ @ @ @ @ @ @ @ @ the UX Studio series. - <p> Join host Chris Hass , former UXPA International and UXPA Boston President , for a fun-filled Q&amp;A . This is your opportunity to learn all about this year 's winner . <p> At our UXPA 2017 International Conference in Toronto , we acknowledged a cosmopolitan group of selfless professionals with the UXPA Volunteer Service Award . We thank this generous team who , over the years , have helped build a UX body of knowledge in five languages and significantly furthered our mission of making UXPA and UX much more international in focus . <p> From the amazing keynotes , to the courses and conference sessions , there was no shortage of great content and knowledgeable presenters . - Networking took shape in many forms " from our receptions , to offsite excursions , to meet and greets in the hotel lounges . - Whether it was catching up with old colleagues , or meeting new friends ... <p> Randolph Bias , Ph.D , CHFP ! While we do n't have a gold statue to present , we have something better ! Not only @ @ @ @ @ @ @ @ @ @ International Conference " he will be onstage , live , being interviewed during the event for our Inside the UX Studio series. - <p> Join host Chris Hass , former UXPA International and UXPA Boston President , for a fun-filled Q&amp;A . This ... 
@@105506869 @906869/ <h> Menu <h> About UXPA Membership <p> The User Experience Professionals Association ( UXPA ) is an international association devoted to assisting new and established professionals in the user experience disciplines.Through networking , publications , - conferences- and classes , UXPA can help you hone your skills and deepen your involvement in the UX community . <p> Our members are employed in a wide variety of disciplines , including- product and service- strategy , design , research , content and development . We are colleagues in pursuit of a common goal : better products and services , better working , and better living through usability. <h> - <h> Membership Benefits by Level <p> UXPA International offers a variety of membership levels to meet your needs . Global Sustaining Membership- is the appropriate choice for experienced professionals who wish to play a leadership role in the international UX community. - Associate Membership- is intended for less experienced members of the UX professions who are developing their basic skills . Our- Student Membership supports current students and recent grads . <p> - <p> Global Sustaining Membership <p> Associate Membership <p> Student @ @ @ @ @ @ @ @ @ @ for Developing Economies <p> - <p> Access to UXPA International Mentorship Program <p> Support the Publication and Translation of UX Magazine , available free online 
@@105506871 @906871/ <h> Menu <h> About UXPA <h> Supporting those around the world who advocate for user experience <p> The User Experience Professionals Association ( UXPA ) supports people who research , design , and evaluate the user experience of products and services . <p> UXPA- is the organization of choice for user experience professionals worldwide . UXPA was established in 1991 as the UPA ( Usability Professionals ' Association ) when Usability was considered to be an umbrella term for what our members do. - In 2012 the name was changed to UXPA to reflect the change in- members ' job titles to User Experience ( UX ) - and to continue to represent their breadth of work . <p> UXPA holds- yearly international conferences , publishes new findings through both the Journal of Usability Studies ( JUS ) and through User Experience Magazine , and has 50 chapters around the world . In 2004 , UXPA established- World Usability Day , which in 2011 was celebrated in 44 countries . <p> UXPA 's goals are to : <p> Become the authoritative source on the practice of usability , user-centered @ @ @ @ @ @ @ @ @ @ Facilitate professional development and education within the UX field . <p> Promote the business value of user experience , research , design and evaluation to business and other entities . <p> Foster a community of user experience professionals through knowledge sharing and networking . <p> Provide effective UXPA governance . <p> Enhance and provide the value of UXPA membership <h> What is a User Experience Professional ? <p> User experience professionals may- do a broad range of work from interviews and observations to creating wireframes for a product or service . Some have a design background , and some have a library science degree . Many have attended courses towards certification or hold an advanced degree such as one- in- Human-Computer Interaction or- Information Design . <p> Other user experience- professionals focus on one particular area , doing either research , design or evaluation . Some do a combination of activities , but specialize in a specific industry such as health care or education , or perhaps work only with handheld devices . All of them focus on the user , while taking into account the needs of the business @ @ @ @ @ @ @ @ @ @ in UXPA <h> Who are UXPA members ? <p> UXPA is a growing international organization with 50 chapters around the world , and members in 60 countries . UXPA members come from many different backgrounds , sharing the common interest of creating products that meet the needs of the people who work or play with them . Some work full-time as user experience professionals . Others incorporate usability into their work as interface/interaction designers , information architects , developers , and many other fields . 
@@105506872 @906872/ <h> About UX <h> What is User Experience ( UX ) ? <p> User experience ( UX ) - is an approach to product development that incorporates direct user feedback throughout the development cycle ( human-centered design ) - in order to reduce costs and create products and tools that meet user needs and have a high level of usability ( are easy to use ) . There are many- definitions of user experience- by UX professionals . <p> The- business benefits of adding UX to a product development process- include : <p> Increased productivity <p> Increased sales and revenues <p> Decreased training and support costs <p> Reduced development time and costs <p> Reduced maintenance costs <p> Increased customer satisfaction <p> Too often , organizations do n't track total lifecycle costs for a software project . The original development budget and schedule is known , but they do n't typically track how expensive support costs are , what they 're attributable to , how much employee time is wasted on unusable tools , and other post-production factors . Thus , projects that have small up-front costs and short development @ @ @ @ @ @ @ @ @ @ , ineffective , or unusable products . There are many- real world examples- showing how UX can benefit a project and how a lack of UX can add risk . <h> UX in Practice <p> User experience work is based on designing products and services with the- people who will use the product or service- in-mind ( users ) . This practice of focusing on the users is sometimes more specifically- called user-centered design ( UCD ) . In UCD- work the- focus is on the users through the planning , design and development of a product . <h> An International Standard <p> There is an international standard that is the basis for many UX/UCD methodologies . This standard ( ISO 13407 : Human-centred design process ) defines a general process for including human-centered ( user-centered ) - activities throughout a development life-cycle , but does not specify exact methods . <p> In this model , once the need to use a human centered design process has been identified , four activities form the main cycle of work : <p> Specify the context of use Identify the people who will @ @ @ @ @ @ @ @ @ @ , and under what conditions they will use it . <p> Specify requirements Identify any business requirements or user goals that must be met for the product to be successful . <p> Create design solutions This part of the process may be done in stages , building from a rough concept to a complete design . <p> Evaluate designs The most important part of this process is that evaluation - ideally through usability testing with actual users - is as integral as quality testing is to good software development . <p> The process ends - and the product can be released - once the requirements are met . <h> A Typical UX Methodology <p> Most UX methodologies are more detailed in suggesting specific activities , and the time within a process when they should be completed . The UXPA published a poster , - Designing the User Experience , showing a typical UX/UCD process . <p> In this version , the UX activities are broken down into four phases : Analysis , Design , Implementation and Deployment , with suggested activities for each phase . They are : <p> Analysis @ @ @ @ @ @ @ @ @ @ Include usability tasks in the project plan <p> Assemble a multidisciplinary team to ensure complete expertise <p> Develop usability goals and objectives <p> Conduct field studies <p> Look at competitive products <p> Create user profiles <p> Develop a task analysis <p> Document user scenarios <p> Document user performance requirements <p> Design Phase <p> Begin to brainstorm design concepts and metaphors <p> Develop screen flow and navigation model <p> Do walkthroughs of design concepts <p> Begin design with paper and pencil <p> Create low-fidelity prototypes <p> Conduct usability testing on low-fidelity prototypes <p> Create high-fidelity detailed design <p> Do usability testing again <p> Document standards and guidelines <p> Create a design specification <p> Implementation Phase <p> Do ongoing heuristic evaluations <p> Work closely with delivery team as design is implemented <p> Conduct usability testing as soon as possible <p> Deployment Phase <p> Use surveys to get user feedback <p> Conduct field studies to get info about actual use <p> Check objectives using usability testing <p> You may notice that " usability testing " appears several times throughout the process , from the first phase to the last . <p> Providing a @ @ @ @ @ @ @ @ @ @ can find more information about usability and user-centered design- guidelines and methodologies- in the rest of the- resources section- of the UXPA web site . <h> What Is a User Experience Professional ? <p> The UXPA defines a User Experience ( UX ) - professional broadly as people who research , design , and evaluate the user experience of products and services . <p> Some specialize in conducting usability tests or other user research while others practice UX as part of other responsibilities in designing products , services , software applications or web sites . <p> The training and professional background of UX professionals is equally broad . Many have qualifications in closely related fields like human-computer interaction ( HCI ) , information design or psychology . Others have used their backgrounds in computer science , project management , journalism , fine arts , library science , or business as part of their journey towards being a UX professional . <h> " Doing UX " <p> The activities of a UX professional - or the time spent on UX activities for those with other primary responsibilities - are all part @ @ @ @ @ @ @ @ @ @ span the entire product life-cycle from user research during planning and early visioning to the final rollout or release of a product . There is an- international standard , ISO 13407- that is the basis for many user-centered design approaches . <p> Broadly speaking , UX activities can be divided into : <p> Research- - learning about the people who will use a product and the context in which it will be used . <p> Evaluation- - observing ( and learning from ) users as they work with a product before , during and after the design and development process . <p> Design- - whether it is called interface , interaction , information or experience design . <h> Body of Knowledge <p> The UXPA- Body of Knowledge- project is working to define the work of a UX professional . The project includes : <h> How Do I Get Started with UX ? <p> If you are just learning about UX , this site includes some good resources - and links to many others - on UX , whether you like learning by reading , listening or in discussion with others @ @ @ @ @ @ @ @ @ @ Basics " on Usability.gov , the web site from the U.S. Department of Health &amp; Human Services includes an excellent introduction to the field . <h> Ethics Advisory Committee <p> If any member believes that a violation has occurred during the trial period , he or she should report the violation to the Vice President of the UXPA who will call a meeting of the Ethics Advisory Committee. - <p> The Advisory Committee will evaluate the evidence and determine if the Code has been violated . The findings will be reported to the UXPA Board of Directors who have the power to expel the member from the User Experience Professionals Association . <p> " Usability is about human behavior . It recognizes that humans are lazy , get emotional , are not interested in putting a lot of effort into , say , getting a credit card and generally prefer things that are easy to do vs. those that are hard to do . " 
@@105506875 @906875/ <p> To submit a manuscript or contact the editors in chief , send an email to:jus@uxpa.org . <h> Review Process <p> Submission received by the editor in chief . <p> Acknowledgment is sent to the corresponding author within one week . <p> Manuscript is assigned to two editorial board members for blind reviews . <p> Review and acceptance/rejection recommendation is sent to the editor within six weeks . <p> The corresponding author may receive the review outcome as one of the following recommendations : Accepted as is ; Accepted after minor revision ; Accepted conditional upon a major revision ; or rejected . <p> If submission requires revision , the authors will be expected to submit the revised manuscript within a month after receiving the review outcome . <p> A final acceptance/rejection decision will be made no later than one month after reception of revised manuscript . <h> Review Criteria <p> Reviewers will review papers according to the following criteria : <h> Main Article Categories <h> Empirical or analytical study , Methods and Techniques <p> Does it present a well-defined evaluation/testing method ? Does it present a valid evaluation/testing @ @ @ @ @ @ @ @ @ @ use ? Is there a clear description of the measures and their validity ? Does it present appropriate quantitative or qualitative data ? Does it have appropriate descriptive statistics or analysis ? Do the findings have a generalization value to other studies or designs ? Is there a clear discussion of the practical implications ? Is there a clear discussion concerning the impact on the user or the product ? Is there a clear discussion for future , follow-up work ? Is there a clear discussion of the take-aways for other practitioners ? Is there a reference or further readings list that is pertinent to the information reported and is relevant for practitioners ? <h> Education and Training <p> Does it present a clear , valid , well-defined usability training method ? Has the training method been validated empirically ? Is there a clear discussion on when the method is appropriate ? Does it include relevant examples ? <h> Ethics <p> Does it present an ethics problem that has a generalization value ? Does it present a clear a solution or an approach to address the problem ? <h> Opinions @ @ @ @ @ @ @ @ @ @ apply in their work ? Is it an appropriate reply/response to a paper published in JUS ? Can the reply be useful to practitioners ? <h> Logic/Organization <p> Is the objective statement clear ? Is there a logical flow to the information presentation ? Is there a clear progression of ideas building on a central theme ? Is there a clear transition between paragraphs and ideas ? Is there an effective use of transition statements and linking statements ? <h> Structure <p> Is the written submission well-structured ? Does the intro state a clear purpose ? Is the rest of the paper linked clearly to the intro ? Does the body include the test evidence to support the main claims and objectives ? Are there clear conclusion summaries , integration of findings ? <h> Writing style <p> Is the submission free of spelling/grammar mistakes ? Is the submission free of inconsistencies in tense and person ? Does the submission follow the required format and style ? Are all sources , citations , and acknowledgements complete ? <p> - <h> Copyright <p> Once a manuscript was accepted for publication in JUS @ @ @ @ @ @ @ @ @ @ Copyright Permission and Release form : 
@@105506877 @906877/ <h> Menu <h> Sr. UX Researcher -- Financial Products <p> Bloomberg L.P. <p> New York , NY <p> Overview and Description <p> Our UX Research team answers the driving questions posed by designers and product teams . We lead user research initiatives that inform critical design and business decisions about Bloomberg 's Financial Products -- including the iconic Terminal -- on desktop and mobile . Working in a state-of-the-art usability lab and in the field , at all stages of product development , we identify key issues our users may encounter . Collaborating closely with designers , product managers , and engineers , UX research is critical to Bloomberg 's ongoing pursuit of creating intuitive , innovative , and best-in-class financial software . 
@@105506878 @906878/ <h> Menu <h> Sponsorship <p> Your sponsorship tells the world that you are committed to creating and evaluating usable products . It also communicates to your industry , your customers , and potential employees your support of usability and user-centered design . <p> By becoming a UXPA sponsor , you will more easily recruit top usability professionals and further develop your employees through their involvement in the UXPA . Your company will be more visible with your- logo and name on the UXPA website and on UXPA conference materials and on UXPA social media channels ! <h> Sponsorship Opportunities <p> Organizational Sponsorship - Help fund ongoing professional development and outreach ! 
@@105506883 @906883/ <h> Menu <h> UXPA Publications <h> User Experience Magazine <p> User Experience Magazine is a quarterly publication featuring a wide range of - significant and unique articles dealing with the broad field of usability and the user experience . The magazine has been in print since 2002 , and online since 2013 . <h> Journal of Usability Studies ( JUS ) <p> The Journal of Usability Studies ( JUS ) is a peer-reviewed , international , online publication dedicated to promoting and enhancing the practice , research , and education of user experience design and evaluation . 
@@105506887 @906887/ <h> Definitions of User Experience and Usability <h> Definitions of User Experience <p> Every aspect of the user 's interaction with a product , service , or company that make up the user 's perceptions of the whole . User experience design as a discipline is concerned with all the elements that together make up that interface , including layout , visual design , text , brand , sound , and interaction . UE works to coordinate these elements to allow for the best possible interaction by users . - UXPA , Usability Body of Knowledge , Glossary <p> - <p> User Experience is not about good industrial design , multi-touch , or fancy interfaces . It is about transcending the material . It is about creating an experience through a device. - Marc Hassenzahl ( 2013 ) : User Experience and Experience Design . In : Soegaard , Mads and Dam , Rikke Friis ( eds . ) . " The Encyclopedia of Human-Computer Interaction , 2nd Ed . " . Aarhus , Denmark : The Interaction Design Foundation . <p> - <p> User experience ( UX ) @ @ @ @ @ @ @ @ @ @ , system or service . User experience highlights the experiential , affective , meaningful and valuable aspects of human-computer interaction and product ownership . Additionally , it includes a person 's perceptions of the practical aspects such as utility , ease of use and efficiency of the system . User experience is subjective in nature because it is about individual perception and thought with respect to the system . User experience is dynamic as it is constantly modified over time due to changing circumstances and new innovations. - Wikipedia- entry for user experience , page last modified on 24 September 2013 at 23:52 . <p> - <h> Definitions of Usability <p> The extent to which a product can be used by specified users to achieve specified goals with effectiveness , efficiency and satisfaction in a specified context of use . - ISO 9241-11 <p> - <p> Usability means that the- people who use the product- can do so- quickly and easily- to accomplish- their own tasks. - This definition rests on four points : ( 1 ) Usability means focusing on users ; ( 2 ) people use products to @ @ @ @ @ @ @ @ @ @ trying to accomplish tasks ; and ( 4 ) users decide when a product is easy to use . - Janice ( Ginny ) Redish and Joseph Dumas , - A Practical Guide to Usability Testing , 1999 , p. 4 <p> - <p> After all , usability really just means that making sure that something works well : that a person of average ( or even below average ) ability and experience can use the thing - whether it 's a Web site , a fighter jet , or a revolving door - for its intended purpose without getting hopelessly frustrated . - Steve Krug , - Do n't Make Me Think , - 2000 , p. 5 <p> - <p> Usability starts with a philosophy - a belief in designing to meet user needs and to focus on creating an excellent user experience - but it is the specific process and methodology that produce the real goal of usability . A new usability process starts by looking at who uses a product , understanding their goals and needs , and selecting the right techniques to answer the question @ @ @ @ @ @ @ @ @ @ requirements of our users ? " - Whitney Quesenbery <p> - <p> It is important to realize that usability is not a single , one-dimensional property of a user interface . Usability has multiple components and is traditionally associated with these five usability attributes : learnability , efficiency , memorability , errors , satisfaction . - Jakob Nielsen , - Usability Engineering , 1993 , p. 26 <p> - <p> - <p> Usability is a measurable characteristic , that is present to a greater or lesser degree , that describes how effectively a user can interact with a product . It can also be thought of as how easy a product is is to learn and how easy it is to use . -- Jeff Axup , UserDesign <p> - <h> Definition of Human-Centered Design <p> Human-centered design is characterised by : the active involvement of users and a clear understanding of user and task requirements ; an appropriate allocation of function between users and technology ; the iteration of design solutions ; multi-disciplinary design . - ISO 13407 
@@105506890 @906890/ <h> Menu <h> Job Seeker Resources <p> It 's a great time to be working or pursuing a job in UX . Because demand is often outpacing supply when it comes to top UX talent , salaries are rising . According to The Creative Group Salary Guide , average starting salaries for creative positions are projected to increase an average of 3.6 percent in 2017 . Positions requiring digital expertise " like user experience and mobile designers " are seeing even bigger gains . These resources can help you find a job that best fits your skills and experience . <p> The Creative Group ( TCG ) specializes in placing a range of highly skilled interactive , design , marketing , advertising and public relations professionals with a variety of firms on a project , contract-to-hire and full-time basis . Contact your local TCG office at **32;0;TOOLONG or call 1-844-547-7331. 
@@105506891 @906891/ <h> Membership Discounts <h> As a UXPA member you are eligible for significant discounts on useful tools , services and conferences to help you to do your job ! <h> Discount : <p> In honor of World Usability Day , CRC Press is offering UXPA members a 20% discount off of the list price of The UX Careers Handbook by Cory Lebson. - <p> The UX Careers Handbook offers an insider 's look at how to be a successful User Experience ( UX ) professional from comprehensive career pathways to learning , personal branding , networking skills , building of resumes and portfolios , and actually landing a UX job . <p> Features interviews and personal stories from a range of industry-leading UX professionals to show readers how they broke into the industry , stayed with it , and evolved with it . <p> Accompanied by a companion website that provides readers with featured articles and updated resources covering new and changing information to help them stay on top of this fast-paced industry . <p> Provides worksheets and activities to help readers make decisions for their careers and build @ @ @ @ @ @ @ @ @ @ eye-tracking system , Ergoneers provides the best hardware for professional Eye-Tracking studies in real or virtual environments . The products can be purchased globally throughout a wide partner network. - <p> Ergoneers invites you to learn more at www.ergoneers.com , and to enjoy an exclusive UXPA member discount below. - <h> Discount : <p> Focus Suites has been a leading provider of focus group facilities for over 20 years . Our luxurious state-of-the-art facilities are supported by knowledgeable , experienced staff in tune with your project and business needs . Focus Suites versatility and flexibility allows us to efficiently serve client needs while providing unparalleled customer service . <p> Focus Suites is pleased to offer UXPA Members- 20% off of facility and recruitment. - <h> Discount : <p> Front is the product conference for UX designers and product managers . It 's a practitioner 's conference . Attendees come for an education , real world , from-the-trenches case studies , and hands-on training from their peers . It 's four days of learning and growth , providing answers to deep organizational questions and insights into how to validate and solve @ @ @ @ @ @ @ @ @ @ 're thrilled to offer UXPA members $100 off our workshop series in January . During November , we 're also offering combination tickets to both the workshop series and the case study conference for $1,100 ( $300 off ) . <h> Discount : <p> Justinmind is more than a hi-fidelity prototyping platform , it is the best design solution to prototype feature-rich web and mobile applications that are no different from the real app . Justinmind is full of intuitive features to help you build and validate interactive wireframes before moving on to coding . <p> Our drag and drop editor makes it really easy to create content , add events and interactions and , simulate prototypes in real devices or in your web browser . You can get feedback , iterate fast and , collaborate on a single prototype with other people simultaneously . No code needed . Ever . <h> Discount : <p> KNect365 help you to connect with and expand your professional community through the exchange of insights and strategic thinking . KNect365 's Knowledge and Networking Offerings Create Collaborative , Results-Focused Opportunities for Growth . The @ @ @ @ @ @ @ @ @ @ drives your business forward . <h> Discount : <p> The MIT Press is pleased to offer UXPA and its members a standing - While applicable for use on the MIT Press website only , this code will not expire and can be used more than once , for multiple titles , and for both print and MIT Press e-editions where available . Happy reading from The MIT Press ! <h> Discount : <p> Ptype is a full service UX agency in San Francisco . We 're best known for our Axure prototypes and Axure training workshops . Visit- http : //pty.pe- to check out our services , upcoming classes , and video courses . <h> Discount : <p> The Creative Group ( TCG ) specializes in connecting interactive , design , marketing , advertising and public relations talent with the best companies on a project , contract-to-hire and full-time basis . <p> The Creative Group is offering UXPA members a one free day* of creative support . - <p> - <p> *Eight hours free on your first minimum 40-hour assignment @ @ @ @ @ @ @ @ @ @ assignment ( CAN ) . One coupon per order . You must notify The Creative Group Account Executive at the time of order that you are redeeming this coupon . This coupon must accompany the invoice at the time of payment and can not be combined with other promotional offers. - Offer expire 12/31/2016 . <h> Discount : <p> TryMyUI is a crowdsourced , remote unmoderated user testing tool . Unearth , Analyze and Understand what frustrates your users by watching their user experience and hearing their thoughts on it . TryMyUI has a suite of analytical tools in its Team Plan that allows a product team to autonomously and simultaneously benchmark usability , iterate tests and collaboratively annotate and access the video data to create distilled highlight reels . <p> - <p> This World Usability Day , TryMyUI gives you $200 off the first month of your Team Plan <h> Join This List <p> Are you interested in offering a discount that will be promoted to 7,000+ User Experience Professionals ? Have you got a favorite vendor that you 'd love to see offer a discount to UXPA members @ @ @ @ @ @ @ @ @ @ Sponsorship <p> Want to contribute to the UXPA ? Find out about our many sponsorship opprtunities : 
@@105506893 @906893/ <h> Choose your membership level <h> Primary tabs <p> Note to legacy UXPA Members : To renew your membership , we ask that you select one of our new membership tiers : Global Sustaining , Associate , or Student . The new membership will be added to your UXPA Account for the coming year . <h> Step 2 : Add Chapter Dues <p> Adding chapter dues is optional . You can pay dues for as many chapters as needed , or pay later by visiting your local chapter 's homepage . If you do not belong to a local UXPA chapter and do not wish to join one today , please continue to step three . <p> Note : Some UXPA Chapters charge dues themselves . They have been indicated as $0.00 here because UXPA does not collect dues on their behalf . Contact your local chapter for more on chapter dues . <h> Membership Benefits by Level <h> MEMBERSHIP- NOTES <h> Auto-Renewal <p> By default , all membership types- auto-renew until canceled . Users can cancel their membership from the " Change Member Renewal " tab on their @ @ @ @ @ @ @ @ @ @ of your membership expiration date reminding you that your membership is about to auto-renew . <h> Discounted Memberships <p> Associate and Student memberships for those residing in countries with emerging economies ( based on the International Monetary Fund 2012 World Economic Report ) are available at a discount ( $39 for Associate , $25 for Student ) . - Please select your country of residence and add the membership to the cart. - The discounted rate will appear on the Checkout page . <h> Legacy UXPA Member ? <p> If your legacy UPA/UXPA membership expired prior to August 2013 , you will need to set up a new account as old account credentials are no longer active . 
@@105506894 @906894/ <h> Menu <h> Journal of Usability Studies <h> About the Journal <p> The Journal of Usability Studies ( JUS ) is a peer-reviewed , international , online publication dedicated to promoting and enhancing the practice , research , and education of user experience design and evaluation . <p> The journal aims to provide usability practitioners and researchers with a forum to share : <p> Empirical findings and case studies <p> Emerging methods and tools from within the user experience profession and from related disciplines , such as market research and technical communications <p> We invite authors to submit manuscripts addressing various aspects of quantitative and qualitative usability studies that have a strong generalization value to other practitioners working with any human-interactive product . <p> Studies can include ( but are not limited to ) : <p> Empirical findings of usability studies ( but not the full usability reports ) <h> Submission Guidelines <p> While we have no hard rule about the maximum length of manuscripts , the expectation is that an empirical study that includes data from participants will be about 10,000 words , 10-15 journal pages of about @ @ @ @ @ @ @ @ @ @ be longer . The editors-in-chief and the manuscript reviewers reserve the right to determine the appropriate length of a published article . <h> Review Process <p> Submission received by the editor in chief . <p> Acknowledgment is sent to the corresponding author within one week . <p> Manuscript is assigned to two editorial board members for blind reviews . <p> Review and acceptance/rejection recommendation is sent to the editor within six weeks . <p> The corresponding author may receive the review outcome as one of the following recommendations : Accepted as is ; Accepted after minor revision ; Accepted conditional upon a major revision ; or rejected . <p> If submission requires revision , the authors will be expected to submit the revised manuscript within a month after receiving the review outcome . <p> A final acceptance/rejection decision will be made no later than one month after reception of revised manuscript . <h> Review Criteria <p> Reviewers will review papers according to the following criteria : <h> Main Article Categories <p> Empirical or analytical study , Methods and Techniques <p> Does it present a well-defined evaluation/testing method ? Does it @ @ @ @ @ @ @ @ @ @ method other practitioners can use ? Is there a clear description of the measures and their validity ? Does it present appropriate quantitative or qualitative data ? Does it have appropriate descriptive statistics or analysis ? Do the findings have a generalization value to other studies or designs ? Is there a clear discussion of the practical implications ? Is there a clear discussion concerning the impact on the user or the product ? Is there a clear discussion for future , follow-up work ? Is there a clear discussion of the take-aways for other practitioners ? Is there a reference or further readings list that is pertinent to the information reported and is relevant for practitioners ? <p> Education and Training <p> Does it present a clear , valid , well-defined usability training method ? Has the training method been validated empirically ? Is there a clear discussion on when the method is appropriate ? Does it include relevant examples ? <p> Ethics <p> Does it present an ethics problem that has a generalization value ? Does it present a clear a solution or an approach to address the @ @ @ @ @ @ @ @ @ @ opinions that practitioners can apply in their work ? Is it an appropriate reply/response to a paper published in JUS ? Can the reply be useful to practitioners ? <h> Logic/Organization <p> Is the objective statement clear ? Is there a logical flow to the information presentation ? Is there a clear progression of ideas building on a central theme ? Is there a clear transition between paragraphs and ideas ? Is there an effective use of transition statements and linking statements ? <h> Structure <p> Is the written submission well-structured ? Does the intro state a clear purpose ? Is the rest of the paper linked clearly to the intro ? Does the body include the test evidence to support the main claims and objectives ? Are there clear conclusion summaries , integration of findings ? <h> Writing style <p> Is the submission free of spelling/grammar mistakes ? Is the submission free of inconsistencies in tense and person ? Does the submission follow the required format and style ? Are all sources , citations , and acknowledgements complete ? <h> Editorial Assistant <p> Sarah Harris , - Huckleberry Technologies @ @ @ @ @ @ @ @ @ @ , - Microsoft , USA Vynarack Xaykao <h> Graphic Design <p> Donna Lea <p> Aims &amp; Scope <p> The Journal of Usability Studies ( JUS ) is a peer-reviewed , international , online publication dedicated to promote and enhance the practice , research , and education of user experience design and evaluation . <p> Its aim is to provide usability practitioners and researchers with a forum to share : 
@@105506896 @906896/ <h> Chapters &amp; SIGs <h> What is a Local UXPA Chapter ? <p> The goal of Local UXPA Chapters is to offer people with the same interest to come together regularly to share their experience and to encourage each other in the usability profession . The focus is on truly local activities connecting usability professionals with each other . Business meetings with invited speakers , Usability lab tours and Usability roundtables , where you can bring interesting usability problems to be solved jointly , are some examples of what a Local Chapter may offer . Above all ; it provides a handy and convenient network for everyday access . <p> A Local Chapter is a group of UXPA members who live or work in a particular locality and who , as a matter of geographic convenience , organize themselves to promote the goals of the Association cooperatively . It serves as a focal point for its members to share their professional experiences and provide educational opportunities in the advancement research field . Local UXPA Chapters offer a number of benefits to its members . <h> A Platform for Networking @ @ @ @ @ @ @ @ @ @ development is to meet with colleagues in the same field . You gain new information on state-of-the-art technology , access to others ' experiences and knowledge and you will get the chance to broaden your insights . <h> Professional Development <p> Your Chapter meetings and activities allow you to expand your knowledge in the Usability research/development area . Chapters bring continuous education opportunities closer to home and provide a bridge between the annual UXPA conferences . It is also possible to organize Mentor Programs as a way for the members to grow in their profession . <h> Exchange of Services and Contributing to UXPA as an Organization <p> Local UXPA chapters offer the possibilities of exchanging both professional experiences and services , such as reviewing each other 's papers and conference contributions . It will also act as a greenhouse for new ideas that can be implemented globally by the Association , and provide UXPA with potential new members to the Board of Directors . <h> A Creative and Stimulating Environment <p> Design contests and brain-storming exercises are fun and stimulating ways of learning more about usability and the other @ @ @ @ @ @ @ @ @ @ such as usability lab tours and site visits to interesting work places , for example air traffic control centers and high-tech manufacturing plants . Learning more about usability issues in these " extreme " working conditions will often give you good ideas to bring back home . 
@@105506902 @906902/ <h> Menu <h> Quotes on Usability <p> Whether they use the word " usability " or not , these quotes from experts in the technology and other industries are good examples of the problem that usability solves . <p> - <p> " Usability is about human behavior . It recognizes that humans are lazy , get emotional , are not interested in putting a lot of effort into , say , getting a credit card and generally prefer things that are easy to do vs. those that are hard to do . " <p> " Try moving usability thinking into our buildings , call centers , forms and products . It 's common sense : If your business is easier to use than your competitor 's , people will be more likely to do business with you . A usable business is a more competitive business . " <p> " People want less information , they do n't want more information . They want it to be easier for them to use . Easier for them to get what they want . Easier for them to do what they @ @ @ @ @ @ @ @ @ @ my mind , is going to come from the usability standpoint ... A lot of the technology is there right now . The pieces are there ... Technology is ahead of the software curve ... ahead of people 's and companies ' ability to integrate the technology and to support the technology . " <p> " The impact of new software and technologies on employees is rapidly becoming a competitiveness issue to companies ... There 's a lot of traps if we 're not careful as we deploy new technologies . We could field a product that ends up injuring people . That 's why ergonomics has got to become part of a company 's corporate culture . " <p> " One reason so many large , important systems are not being used , or that users have to work around the system to get the work done , is that the programmers did n't understand what the users were doing . They develop the application according to their own interpretation . " <p> --Finn Kensing , Roskilde University , Denmark ( CIO , 10/15/96 ) <p> - <p> The @ @ @ @ @ @ @ @ @ @ not a nice thing to say ... but it is a fundamental truth . Software customers -- you , me , CIOs of multibillion-dollar companies ... have learned to live with mediocre software . We do not count on software to be intuitively easy to understand or to work consistently . Instead , we make do . 
@@105506924 @906924/ <h> Issue Overview 98864 @qwx958864 <p> With this issue , we start the eighth year of JUS . We thought it was time to look back over the first seven years to see if we are meeting the objectives that UXPA established for the journal . We examined the 77 papers that were published between November 2005 and February 2012 . <h> Invited Essay <p> Hollywood " that seller of dreams " has filled our heads with an **29;34;TOOLONG belief that we are entitled , even obligated , to strive and ultimately triumph when we are clearly outmanned , outgunned , and outclassed . Melanie Griffith lands her dream job in Working Girl . 
@@105506928 @906928/ <h> Menu <h> User Research Lead <p> Bose <p> Framingham , MA <p> Job Description User Research Lead " Bose Hear Emerging Business " We are seeking a Principal User Researcher to join our small but growing team at Bose to help us bring new kinds of products to anyone who wants to Hear Better . The Bose Hear Emerging Business is a new , exciting division at Bose where we 've begun selling an innovative product that can help anyone hear conversations better in noisy places ( visit hearphones.bose.com ) . We want to know more about our current and future customers , and learn how we can make even better products that delight more people . This is a unique opportunity to do creative and impactful work . - <p> Qualifications : <p> Requirements : - Technical/Functional Competencies <p> User Research : You have strong user research expertise , including ethnographic , concept testing , and technology assessments with users ; usability testing ; and collecting secondary research relevant to the team 's needs . <p> Analytical Thinking : You have deep critical thinking skills for framing @ @ @ @ @ @ @ @ @ @ trained to deliver thoughtful protocols with the appropriate balance of rigor and speed , and do complex analysis.You can switch between qualitative and quantitative research methodologies with ease . <p> Communication : You have excellent visual , verbal , and written communication and presentation skills , demonstrated in your solid portfolio of past work . You can present complex data results of research in a rich compelling manner verbally and visually . <p> Interpersonal Skills <p> Innovation : You have experience with innovation and new-to-the-world consumer products and businesses . You have an affinity for emerging technology , market trends , and new gadgets . <p> Passion : You are driven " in your personal and professional life " by a burning desire to make a difference and change the world . <p> Curiosity : You wonder about what makes customers happy . You ask many questions about what motivates customers to seek solutions to their needs . <p> Collaboration : You enjoy engaging with multiple team members and value diverse perspectives . You are open to feedback and embrace the exchange of ideas and working together . <p> Education @ @ @ @ @ @ @ @ @ @ , Human-Computer Interaction , Behavior Sciences , Experimental Psychology or a related field , with a minimum of 3 years of related work experience. - Occasional travel , domestic and international ( 10-25% ) . <p> - <p> Responsibilities : <p> As the Principal User Researcher on our team , you will be responsible for a variety of user research plans , analyses and presentations . In some cases , you will execute on the plans , and others you will enlist internal or external partners to carry out the research . You will create a holistic view of the user in all their complexities . You 'll work as a trusted partner with all our disciplines and teams . Your role will be to bring user research insights to the whole team , as a critical input to our learning and decision making . 
@@105506933 @906933/ <h> Choose your membership level <h> Primary tabs <p> Note to legacy UXPA Members : To renew your membership , we ask that you select one of our new membership tiers : Global Sustaining , Associate , or Student . The new membership will be added to your UXPA Account for the coming year . <h> Step 2 : Add Chapter Dues <p> Adding chapter dues is optional . You can pay dues for as many chapters as needed , or pay later by visiting your local chapter 's homepage . If you do not belong to a local UXPA chapter and do not wish to join one today , please continue to step three . <p> Note : Some UXPA Chapters charge dues themselves . They have been indicated as $0.00 here because UXPA does not collect dues on their behalf . Contact your local chapter for more on chapter dues . <h> Membership Benefits by Level <h> MEMBERSHIP- NOTES <h> Auto-Renewal <p> By default , all membership types- auto-renew until canceled . Users can cancel their membership from the " Change Member Renewal " tab on their @ @ @ @ @ @ @ @ @ @ of your membership expiration date reminding you that your membership is about to auto-renew . <h> Discounted Memberships <p> Associate and Student memberships for those residing in countries with emerging economies ( based on the International Monetary Fund 2012 World Economic Report ) are available at a discount ( $39 for Associate , $25 for Student ) . - Please select your country of residence and add the membership to the cart. - The discounted rate will appear on the Checkout page . <h> Legacy UXPA Member ? <p> If your legacy UPA/UXPA membership expired prior to August 2013 , you will need to set up a new account as old account credentials are no longer active . 
@@105506935 @906935/ <h> Conferences &amp; Events <p> UXPA Events provide interactive opportunities for our members and the UX community to learn , have fun , and collaborate. - Events are hosted by UXPA International , by- our amazing chapters all over the world , and in collaboration with- our partners . <h> UXPA Members <p> UXPA invites you to share information about upcoming events or conferences with the UXPA membership . If you are a UXPA member you may add an event to the UXPA Events and Conferences listings for free. - Please contact- brian.mills@uxpa.org . <h> Non-Members <p> Do you have an event or conference you 'd like to share with the UXPA membership ? Non-UXPA members are invited to add a listing to the UXPA Events and Conferences listing for a nominal fee of $100 . Please contact- sara.mastro@uxpa.org. 
@@105506942 @906942/ <h> Menu <h> UX Designer <p> The Federal Reserve Bank of Cleveland <p> Cleveland , OH <p> Overview and Description <p> The Federal Reserve Bank of Cleveland is currently seeking a User Experience ( UX ) Designer in its eGovernment department , a service provider for the U.S. Department of the Treasury in the areas of revenue collections and eCommerce . This position has focus on user research , design , and prototyping , influencing the planning , development , and UX direction of , but not limited to , the Pay.gov website . This website enables individuals and businesses to submit payments to Federal Government Agencies . Our ideal candidate would be an experienced , passionate UX designer who has a proven track record and a wide array of skills in designing an intuitive user experience for large-scale web applications . If you 're a critical thinker with a good design sense , a strong user experience background , an eye for making things better , and the desire to improve the user experience for government services , then this is the position for you . <p> Qualifications @ @ @ @ @ @ @ @ @ @ mobile web , and UX design . Experience with mobile apps is also desired . <p> Generate creative ideas and solutions ; open to challenging current process and procedures <p> Excellent communication , presentation , interpersonal , and analytical skills , which allow you to communicate complex , often abstract , design concepts clearly and persuasively to both designers and non-designers <p> Strong and diverse portfolio of work samples that showcase UX prowess ; that are creative , simple yet elegant , and show a depth of thought ; and use current methodologies , tools , and best practices <p> Thorough understanding of user-centered design theory and practice ; a knack for interpreting loosely structured ideas and evolving them into a clean , cohesive , and captivating UX vision <p> A solid understanding of issue management systems , project management skills , and version control <p> Provide hands-on technical expertise to help solve critical software design and development challenges <p> Identifies , collects , and organizes data for analysis and decision-making 
@@105506944 @906944/ 98864 @qwx958864 <p> I expect Volume 6 , Issue 3 to be a turning point for the journal . Prior to this issue , the journal had not published any articles that straddled the boundary between the user experience literature and the technical communication literature . In this issue 's editorial , Ginny Redish and Carol Barnum describe how the two disciplines are closely intertwined in their scope , methods , and focus on users . They also detail the strong influence that technical communicators had on the founding of UPA . They call for user experience specialists and technical communicators to work more closely together and to share their research at professional meetings and in each other 's journals . <p> By coincidence , the issue includes the first peer-reviewed paper that bridges the two disciplines . Erin Friess , an associate professor of linguistics and technical communication , presents a case study investigating the variations in the language used by participants in usability testing sessions compared to the language used by usability test moderators in their oral reports of the sessions . She examines the mismatches between @ @ @ @ @ @ @ @ @ @ to have been said . <p> In our second article , Constantinos Coursaris and Dan J. Kim present a qualitative meta-analytical review of more than 100 empirical mobile usability studies . It is the first analysis of the contextual factors and measurement dimensions investigated in the published mobile usability literature . The authors also apply an evaluation framework that shows where the gaps are in that literature . <p> The third article , by Justin Owens and colleagues , uses eye tracking along with other measures to show that text advertisement blindness occurs just as banner blindness does on web pages . Their data show that users tend to miss information in text ads on the right side of the page more often than in text ads at the top of the page . 
@@105506946 @906946/ <h> This Issue 98864 @qwx958864 <p> We are closing volume 3 with this special issue devoted to usability with complex systems . For this special issue we have two guest editors : Barbara Mirel and Mike Albers . In addition , both Ginny Redish and Whitney Quesenbery acted as JUS 's associate editors and contributed to putting together this special issue . The background and motivation for this special issue , along with a synthesis of the articles published in this special issue , are described in the editorial written by Barbara Mirel , our co-guest editor for this issue . 
@@105506947 @906947/ <h> Menu <h> Lead Interaction Designer <p> At 3M , we apply science in collaborative ways to improve lives daily . With $30 billion in sales , our 90,000 employees connect with customers all around the world . <p> 3M has a long-standing reputation as a company committed to innovation . We provide the freedom to explore and encourage curiosity and creativity . We gain new insight from diverse thinking , and take risks on new ideas . <p> Here , you can apply your talent in bold ways that matter . <p> Job Description : <p> Introduction <p> 3M is driving creativity and design as a competitive platform for innovation and brand globally to enhance design-driven solutions for people and the world . We are looking for a Lead Interaction Designer for our Industrial and Safety and Graphics Business Groups to enhance the design function at the 3M Design Center , located in St. Paul , MN . <p> The person hired for the position of Lead Interaction Designer will work with cross-functional teams to create user-centered designs and workflows in support of programs of major importance across @ @ @ @ @ @ @ @ @ @ Responsible for overall leadership and coaching of internal and external clients through design direction , improvement initiatives , design-research , and operating plans for design creation . Independently manages and leads multiple major new design programs , redesign programs or continuous improvement projects within the assigned business or function . Manages external design agency resources , build relationships , resolves issues , etc . <p> Does your curiosity inspire you to imagine tomorrow 's solutions to today 's problems ? Do you ever wonder how collaborative creativity can enrich innovation and make progress possible ? Or wonder if design can drive competitive advantage for business , while also having positive impact on the world ? These are just a few of the questions we ask at 3M Design every day . Our diverse , global team not only includes design talent from multiple creative disciplines , but also achievement-oriented professionals who keep the engine going for strong operations to ensure our design organization is world class . We 're looking for creative explorers who are excited to be part of the very inspiring journey of design at an innovation company @ @ @ @ @ @ @ @ @ @ a file or include a link to a non-confidential portfolio of design work along with your application . <p> Location : Maplewood , MN Travel : Up to 20% Relocation : Is Authorized <p> About 3M Design <p> 3M Design is the creative engine driving design for innovation at 3M . As a global team of diverse talent that creates meaningful and consistent brand experiences , we stimulate collaborative creativity to translate insights and technologies into solutions that inspire and positively impact our customers , our employees , and our world . 3M is a global innovation company with $32 billion in sales , employing 90,000 people worldwide with operations in more than 70 countries . For more information about 3M Design , visit www.3M.com/design or follow @3MDesign on Twitter . <p> Must be legally authorized to work in country of employment without sponsorship for employment visa status ( e.g. , H1B status ) <p> Learn more about 3M 's creative solutions to the world 's problems at www.3M.com or on Twitter @3M or @3MNewsroom . <p> 3M is an equal opportunity employer . 3M , age , disability , or veteran status . <p> Please note : your application may not be considered if you do not provide your education and work history , either by : 1 ) uploading a resume , or 2 ) entering the information into the application fields directly . 3M Global Terms of Use and Privacy Statement <p> Carefully read these Terms of Use before using this website . Your access to and use of this website and application for a job at 3M are conditioned on your acceptance and compliance with these terms . Please access the linked document by clicking here , select the country where you are applying for employment , and review . Before submitting your application you will be asked to confirm your agreement with the terms . 
@@105506950 @906950/ <h> Articles by Marilyn Tremaine <p> In this issue , we feature two peer-reviewed articles . Libby Kumin and her colleagues evaluated the ability of adults with Down syndrome to perform tasks with an iPad . Previous studies have shown that adults with Down syndrome can successfully control a cursor with a mouse . <p> In this issue , we feature an editorial by Austin Henderson and Jed Harris who argue that UI designers need to think of themselves as curators " those who maintain order and provide guidance but who ultimately serve users . We believe you will find their ideas stimulating . <p> We are pleased to have an editorial by two of the leaders of the HCI community , Clare-Marie Karat and John Karat . They worked in a research and development group at IBM for many years and have contributed substantially to our literature . 
@@105506954 @906954/ <h> Menu <h> Hiring Manager Resources <p> Recruiting and retaining top UX talent is a growing challenge for companies . According to The Creative Group 's latest hiring survey , 41 percent of advertising and marketing executives said it 's difficult to find skilled creative professionals today . These resources can help . <p> The Creative Group ( TCG ) specializes in placing a range of highly skilled interactive , design , marketing , advertising and public relations professionals with a variety of firms on a project , contract-to-hire and full-time basis . Contact your local TCG office at- **32;65;TOOLONG or call 1-844-547-7331. 
@@105506957 @906957/ <h> Invited Essay <p> I have the luxury of working for a mature user experience ( UX ) organization . One of the benefits of working for such an organization is that we get to tackle a range of challenges not all organizations have the bandwidth to attack . <h> Peer-Reviewed Article <p> In this article , we discuss results of usability evaluations of desktop and web-based email applications used by those who are blind . Email is an important tool for workplace communication , but computer software and websites can present accessibility and usability barriers to blind users who use screen readers to access computers and websites. 
@@105506958 @906958/ <h> This Issue 98864 @qwx958864 <p> The usability practice is still struggling , every day , with basic questions of what and where and how and why Each of us has our own practices and unique expertise and experiences . Are there ways to arrive at shareable , common knowledge and practices ? Usability standards can be one of those ways . Our invited essay author , Nigel Bevan , has been actively engaged in the development of international usability standards . In his essay titled : " International Standards for Usability Should Be More Widely Used " he shares with us his first hand experience in the , sometimes turbulent , history of developing such standards , and the different types of standards that can be used for a variety of purposes and contexts . <p> The use of standard usability questionnaires continues to be a topic of interest . In our first peer-reviewed article titled : " Determining what individual SUS scores mean : Adding an adjective rating scale " , Aaron Bangor , Philip Kortum , and James Miller address the issue of interpreting the results @ @ @ @ @ @ @ @ @ @ scale with adjectives ( as opposed to the numbers ) and found a high correlation between the standard SUS score and the ratings on the adjective-based scale . It is their claim that such an additional scale can help interpret the questionnaire score . <p> What can a usability practitioner do to take advantage of any opportunity to acquire data on the usability of a product ? In our second peer-reviewed article titled : " Extremely rapid usability testing " , Mark Pawson and Saul Greenberg present a case study of running a usability test a part of a trade show . They describe the combination of some known methods in a way that can be deployed and administered within the more constrained circumstances of a trade show . In addition to being a test executed rapidly , it also has the advantage of reaching real-world users of the product . <p> Finally , our third peer-reviewed article addresses another recurring topic of interest : Culture and usability . In their article titled : " The effect of culture on usability : Comparing the perception and performance of Taiwanese and @ @ @ @ @ @ @ @ @ @ Steve Wallace studied the relationship between culture and key usability parameters . They found some interesting associations between two different cultural groups and some of the usability measures . 
@@105506963 @906963/ <h> JUS <h> This Issue 98864 @qwx958864 <p> We are pleased to publish this special issue of the journal . As the guest editors explain in their introduction , the papers were first presented , in shortened form , at The Cambridge Workshop on Universal Access and Assistive Technology ( CWUAAT ) held in 2012 . The papers focus on early stage development of new technologies to assist the disabled and include them in product designs . <p> For user experience practitioners , this issue highlights the barriers the authors faced and overcame in collecting empirical data to assess the effectiveness of the technologies . It typically is a challenge in all empirical studies to recruit target users and set up a procedure to answer the relevant questions . As you will see , with technologies for the disabled there are additional issues and unexpected challenges to face . <p> While there are technical challenges to including the disabled , there are even larger motivational issues . We frequently profess a commitment to increased accessibility . But most of the organizations we work for seldom consider expanding the relevant @ @ @ @ @ @ @ @ @ @ time when we had minimal influence in our organization 's decisions about the scope of the audiences for products . More recently , we often have been present when those important decisions are debated . It is time to put our advocacy into action . 
@@105506966 @906966/ <h> This Issue <h> Clustering for Usability Participant Selection <h> Abstract <p> User satisfaction and usefulness are measured using usability studies that involve real customers . Given the nature of software development and delivery , having to conduct usability studies can become a costly expense in the overall budget . A major part of this expense is the participant costs . Under this condition , it is desirable to reduce the number of participants without sacrificing the quality of the experiment . If a company could use a smaller participant pool and get the same results as the entire pool ; this would result in significant savings . Given a participant pool of size N , is there a subset of N that would yield the same results as the entire population ? This research addresses this question using a data-mining clustering tool called Applications Quest . 98865 @qwx958865 <p> Conducting user studies can be very expensive when selecting several experiment participants . The approach of collecting participant demographics and other information as input into clustering is a mechanism for selecting ideal participants . As a guideline for practitioners @ @ @ @ @ @ @ @ @ @ a survey instrument to collect information about them that you deem relevant to the evaluation task . This may include demographic , system experience , etc . <p> Process the completed surveys in a clustering tool using a K-Means approach where you can specify the number of clusters . Ideally , you could use Applications Quest because it will cluster the applicants and make recommendations on which applicants should be used in your experiments . If you do n't have Applications Quest , there are alternative methods , which are described next . <p> If you do n't have Applications Quest , you can cluster the surveys and then hand pick individuals from each cluster . This could be challenging if the clusters are large , or if you have a large number of clusters . 
@@105506968 @906968/ <h> JUS <h> Issue Overview 98864 @qwx958864 <p> We are pleased to publish this special issue of the journal . As the guest editors explain in their introduction , the papers were first presented , in shortened form , at The Cambridge Workshop on Universal Access and Assistive Technology ( CWUAAT ) held in 2012 . <h> Peer-Reviewed Article <p> This paper describes an exploratory study investigating ways to accommodate inclusive design techniques and tools within industrial design practices . The approach of our research is that by making only small changes in design features , designers end up with more inclusive products . <p> This paper outlines a system for arm rehabilitation for children with upper-limb hemiplegia resulting from cerebral palsy . Our research team designed a two-player , interactive ( competitive or collaborative ) computer play therapy system that provided powered assistance to children while they played specially designed games that promoted arm exercises ... <p> While most aspects of web accessibility are technically easy to solve , providing accessible equivalents of data visualizations for blind users remains a challenging problem . Previous attempts at accessible equivalents @ @ @ @ @ @ @ @ @ @ the creation of two prototypes for providing real-time weather information in a sonified format for blind users . 
@@105506969 @906969/ <h> User Experience Lead <p> The Office of Research Information Services manages a large number of websites and web applications that serve the UW Research community. - <p> Our User Experience Interaction Design Lead is responsible for leading the programmatic components of the User Experience strategy , which guides the execution of all ORIS software product design . This role leads the user experience team that is the voice of the user and works with product owners , business analysts and technical resources to blend business goals , product roadmaps , and user goals to ensure successful outcomes . This role is crucial to our product discovery needs and contributes to shaping the product pipeline . In this capacity the role is to bring simplicity to an institution with a world-leading complex research enterprise . <p> Proficient in a variety of methods to convey ideas and concepts ( e.g. wireframes , prototypes , etc ) <p> Effective communicator , presenter , and negotiator <p> Strong creative and organizational vision for information and product design <p> Creative problem-solver with the ability to work with a blank slate and inspire others @ @ @ @ @ @ @ @ @ @ feasibility of design solutions <p> Understand and use web analytics data , as well as user data from other departments &amp; systems in the organization <p> Knowledge of accessibility guidelines and best practices as they related to user interface design <p> Expert knowledge of HTML , JavaScript , and CSS <p> Proficient with jQuery and other industry leading CSS &amp; JavaScript frameworks <p> Ability to design prototypes in Axure , Photoshop , Illustrator , or Omnigraffle <p> Ability to work collaboratively with technical and non-technical staff to determine and confirm requirements and appropriate solutions <p> Strong portfolio available for viewing <p> Desired Qualifications : <p> Experience working in research administration , especially related to higher education <p> Knowledge of lean/agile software development practices including kanban and scrum <p> Knowledge of resource &amp; service oriented architectures , RESTful web services and data formats such as JSON and XML and how to prototype working solutions with them <p> Knowledge of modern front-engineering building blocks such as HTML5 , CSS3 , Node.js , Angular.js , Foundation , Bootstrap <p> Compensation &amp; Benefits : <p> The University of Washington offers many benefits including medical @ @ @ @ @ @ @ @ @ @ leave , wellness programs , work/life programs , and training and education . <p> How to Apply <p> For complete information and to apply , please visit www.uw.edu/jobs . Click on Find a Job and enter 141000 as the req # . Must apply through the University of Washington employment website. - AA/EOE. - To request disability accommodation in the application process , call 206-543-6450 ( v ) , 206-543-6452 ( tty ) , 206-685-7264 ( fax ) , or email dso@uw.edu. 
@@105506973 @906973/ <h> Peer-Reviewed Article <p> Usability practitioners run the risk of misreading the results of usability evaluations , either identifying false positives when artificial user data interferes with a user 's product experience or overlooking real problems when they use artificial user data . In this paper , we examine a strategy for incorporating users ' real data in usability evaluations . We consider the value and the challenges of this strategy based on the experiences of product teams in a consumer software company . 
@@105506974 @906974/ <h> Choose your membership level <h> Primary tabs <p> Note to legacy UXPA Members : To renew your membership , we ask that you select one of our new membership tiers : Global Sustaining , Associate , or Student . The new membership will be added to your UXPA Account for the coming year . <h> Step 2 : Add Chapter Dues <p> Adding chapter dues is optional . You can pay dues for as many chapters as needed , or pay later by visiting your local chapter 's homepage . If you do not belong to a local UXPA chapter and do not wish to join one today , please continue to step three . <p> Note : Some UXPA Chapters charge dues themselves . They have been indicated as $0.00 here because UXPA does not collect dues on their behalf . Contact your local chapter for more on chapter dues . <h> Membership Benefits by Level <h> MEMBERSHIP- NOTES <h> Auto-Renewal <p> By default , all membership types- auto-renew until canceled . Users can cancel their membership from the " Change Member Renewal " tab on their @ @ @ @ @ @ @ @ @ @ of your membership expiration date reminding you that your membership is about to auto-renew . <h> Discounted Memberships <p> Associate and Student memberships for those residing in countries with emerging economies ( based on the International Monetary Fund 2012 World Economic Report ) are available at a discount ( $39 for Associate , $25 for Student ) . - Please select your country of residence and add the membership to the cart. - The discounted rate will appear on the Checkout page . <h> Legacy UXPA Member ? <p> If your legacy UPA/UXPA membership expired prior to August 2013 , you will need to set up a new account as old account credentials are no longer active . 
@@105506978 @906978/ <h> Menu <h> Glennette Clark <p> Glennette Clark is a Senior- UX Designer with more than 15 years of experience by day and an entrepreneur and unwavering advocate for UX professionals by night . Since founding UXCamp DC and Mobile UXCamp DC in 2010 , she has worked to provide a platform for UX professionals around the world to learn , share , and connect around the design topics they are most passionate about . <p> Before her career in design , she was the Public Relations Director for a small publishing company and Assistant Marketing Director for an international charity . Since then , she handles all of the marketing and communications activities for UXCamp and Mobile UXCamp including the email list , web sites , Twitter and Instagram accounts . <p> She has taken to the stage to discuss issues around design and diversity in technology at the SXSW , Digital Capital Week , and MovDev East to name a few . She is a member of the Steering Committee for DC Web Women and Co-Director of its Girls Rock On the Web ( GROW ) program @ @ @ @ @ @ @ @ @ @ Design MBA program at Philadelphia University and a Senior UX Designer at GEICO. 
@@105506982 @906982/ <h> JUS <h> This Issue <h> A Voyage to Maturing Usability <h> Abstract <p> In this article , the chief editor of the recently published book Maturing Usability : Quality in Software , Interaction and Value reports her experiences , from the very beginning when the book project was conceived to the time when the book was delivered . The two-year process was marked with different problems . It required trust , optimism , patience , and commitment of the contributors to overcome these challenges . <p> The book aims to provide an understanding of how current research and practice have contributed to improving quality from the perspectives of software features , interaction experiences , and achieved value . Specifically , the Q-SIV framework addresses quality in software by looking at how using development tools can enhance usability and other qualities , and how methods and models can be integrated into the development process . <p> The book addresses quality in interaction by applying theoretical frameworks on the nature of interactions and methodologies to evaluate qualities , such as usability , reliability , and pleasure ; It addresses quality @ @ @ @ @ @ @ @ @ @ in the real world , focusing on both increasing value for software development and on increasing value for users and other stakeholders . While the voyage to Maturing Usability has been anchored , other voyages to matured usability are envisioned and will likely be set sail in the near future . 
@@105506986 @906986/ <h> This Issue 98864 @qwx958864 <p> Our invited essay by Dr. Deborah Mayhew adds a new and different perspective on the evolution of the usability practice . Deborah has been around for quite some time , and has had an active and influential role in the shaping of the discipline . In her essay , titled : " User experience design : the evolution of a multi-disciplinary approach " , she outlines some of the interesting milestones in the multi-disciplinary nature of the usability practice . The evolution introduced new collaborative and multi-disciplinary challenges , from working with programmers , through graphic designers and information architects , to persuasion specialists . Dr. Mayhew concludes that the constructive future of the usability discipline may depend on effective alliances with various disciplines and specialties along with the concurrent developments in technology and user needs . <p> Aesthetics has been shown to be related to usability . In their peer-reviewed article titled : " An empirical investigation of color temperature and gender effects on web aesthetics " , Constantinos Coursaris , Sarah Swierenga , and Etan Watrall , report on the impact @ @ @ @ @ @ @ @ @ @ cool colors are more associated with favorable impressions of a web site . In addition , this effect is not confounded by gender . It seems that " cool " is more usable . <p> We tend to think of aspects such as navigation and screen design as typical factors affecting usability . Less often do we consider the physical aspects of the interaction and their impact on usability . Vimala Balakrishnan and Paul Yeow present a " Study of the effect of thumb sizes on mobile phone texting satisfaction " in our second peer-review article . They found that participants ' thumb size was actually associated with texting satisfaction , and suggest that manufacturers consider this factor as influential in the usability of their product . <p> The third peer-reviewed article in this issue offers a practical and evidence-based review of several methods aimed at capturing web information seeking behaviors . William Gibbs , in his article titled : " Examining users on news provider web sites : A review of methodology " surveys several methods used to map behaviors with news web sites and shows that those methods @ @ @ @ @ @ @ @ @ @ scanning , and other browsing behaviors. 
@@105506997 @906997/ <h> JUS <h> This Issue <h> Welcome Bill Albert <p> Dr. Bill Albert has joined the staff of the journal . While he has been on its editorial board since 2009 , Bill and I will now manage the publication of JUS issues and the direction of the journal as joint Editors in Chief . Bill is Executive Director of the Design and Usability Center and Adjunct Professor in the Human Factors in Information Design program at Bentley University . Prior to joining Bentley , he was Director of User Experience at Fidelity Investments , Senior User Interface Researcher at Lycos , and Post-Doctoral Research Scientist at Nissan Cambridge Basic Research . <p> Bill was awarded fellowships through the University of California Santa Barbara and the Japanese Government for his research in human factors and spatial cognition . He received his BA and MA degrees from the University of Washington in Geographic Information Systems and his PhD from Boston University in Geography/Spatial Cognition . He completed a post-doc at Nissan Cambridge Basic Research . <p> Bill 's research interests focus on the science of user experience . He co-authored @ @ @ @ @ @ @ @ @ @ usability metrics , Measuring the User Experience : Collecting , Analyzing , and Presenting Usability Metrics , published by Elsevier/Morgan Kauffman in 2008 ( second edition in 2013 ) . His second book ( with Tom Tullis and Donna Tedesco ) , Beyond the Usability Lab : Conducting Large-Scale User Experience Studies was published in 2010 ( Elsevier/Morgan Kauffman ) . He has presented his research at many national and international conferences , and frequently gives workshops and tutorials on user experience research techniques . 
@@105507004 @907004/ <h> JUS <h> This Issue 98864 @qwx958864 <p> This is the second issue of Volume 6 of the journal . It includes a farewell from Avi Parush , who started the journal and served as editor in chief for its first five years . He set the bar high for both the quality of the papers and for the positive , constructive , and humane way he treated authors , reviewers , and the UPA staff . Thanks again Avi . <p> We are pleased to have a stimulating editorial from Misha Vaughan . In it , she describes her personal journey learning how to communicate the value of user experience to internal sales representatives . Perhaps the most important lesson for user experience professionals is questioning the assumptions we sometimes make about ourselves and about what motivates our colleagues in sales . <p> The first peer-reviewed article , by Carl Turner , focuses on a Balanced Scorecard approach to creating metrics to show the value that user experience provides to an organization . He proposes that user experience professionals go beyond simplified return on investment analyses to quantify @ @ @ @ @ @ @ @ @ @ article , by Jen Hocko , describes a case study showing how user experience professionals can smooth the introduction of a new product in an organization . When an organization has purchased a product and attempts to roll it out without any support from user experience professionals , end users often resist or rebel . Jen describes the steps in learning how we can add value to a team managing the roll out . <p> We are pleased to present the third article , by Brian Wentz and Jonathan Lazar . It is a carefully planned and executed usability comparison of email applications by users who are blind . The test sessions with blind participants provide data showing that they have unique problems with email and calendaring tasks . In addition , there are design flaws that cause minor problems for sighted users but become barriers to task completion for blind users . The authors urge us as a profession to include samples of blind users in our evaluations of products . This paper provides a blueprint for how to include a sample of disabled participants in a product evaluation . 
@@105507006 @907006/ <h> This Issue 98864 @qwx958864 <p> It seems that the field of usability has been around long enough for us to construct some historical perspectives and reflect on the evolvement and future of the field . In her invited essay , Effie Law takes us on such a journey which she titled : " A voyage to maturing usability " . While she tells us the story of a book on maturing usability , one can not but wonder how much it reflects the ongoing evolution of usability as a discipline . Through the structure and models of the book , Effie is presenting us with some interesting and useful " big picture " perspectives on the field . <p> Virtual Reality Environments ( VRE ) develop rapidly and are used in a wide variety of applications . Can we still employ the standard and familiar usability studies approaches to evaluate the usability of VREs ? In their articles titled : " A low-cost test environment for usability studies of head-mounted virtual reality systems " , Ahmed Seffah , Jonathan Benn , and Halima Habieb Mammar introduce some of @ @ @ @ @ @ @ @ @ @ In the article they present their methodological approach to some of those challenges and the technical setup that can able it . <p> While the use of ethnographic research approaches has been discussed in the past , the second peer-review article of this issue provides an excellent summary of such methods and their possible implementation in usability studies . In his article titled : " How may I help you ? An ethnographic view of contact-center HCI " , Howard Kiewe reviews the major ethnographic approaches , and then presents a case study of call centers and how such methods were implemented in order to assess usability of call center applications . <p> Finally , the challenges of evaluating and testing mobile HCI are addressed again in the third article of this issue . Jurgen Kawalek , Annegret Stark , and Marcel Reibeck , in their article : " A new approach to analyse human-mobile computer interaction " , present an approach that is based primarily on objective and non-invasive method of data collection in mobile contexts . No less important is their approach to analyze usability problems based on @ @ @ @ @ @ @ @ @ @ its own for usability practitioners . 
@@105507018 @907018/ <h> This Issue 98864 @qwx958864 <p> This issue is partly devoted to the technique of card sorting . In their invited editorial , Jed Wood and Larry Wood , reflect on the origins of card sorting and how it has evolved to being a popular technique in usability work . They then go on to outline and discuss various current practices with card sorting and provide very useful recommendations of how to approach each practice and avoid pitfalls and fallacies . They also go beyond the current practices to discuss their view on the open issues that require further research and accumulated experience . <p> In the first peer-reviewed article , " A modified Delphi approach to a new card sorting methodology " , Celeste Paul introduces a combination of card sorting and the Delphi forecasting method to produce more useful outcomes for the design of information architecture . Relative to other collaborative techniques , the proposed method utilizes the use of card sorting to elicit knowledge of a group of users or experts and the sequential aspects of the Delphi method resulting in a step-wise consensus building . @ @ @ @ @ @ @ @ @ @ are presented to substantiate the proposed method . <p> The card sorting technique is further addressed in the second peer-reviewed article . In their " The usability of computerized card sorting : A comparison of three applications by researchers and end-users " , Barbara Chaparro , Veronica Hinkle , and Shannon Riley tackle the problem of finding the right computer-based card sorting program that can fit the needs of both end-users and the researchers . While not surprising they find the different applications are preferred for the different groups , they provide some practical advice on what aspects to look for in such programs to fit the needs . <p> The third peer-reviewed article addresses yet again the tough challenges that heuristic evaluation introduces . Shazeeye Kirmani continues her work on the " Heuristic Evaluation Quality Score ( HEQS ) - Defining heuristic expertise " . What is required of someone who performs a heuristic evaluation ? The article discusses factors such age , experience , gender , etc. and how these relate to the quality of problems found . Her findings suggest that experience can be a critical factor @ @ @ @ @ @ @ @ @ @ . 
@@105507019 @907019/ <h> Menu <h> Strategic Partners <p> UXPA seeks to build relationships with other similar types of organizations. - We believe that these partnerships add mutual value at both an organizational level and at an individual member level . <p> Our partnerships often include cross-promotion of activities , joint training , opportunities for speakers and writers , and a forum for our organizations to share new ideas. - We also work to encourage our UXPA chapters to establish partnerships at a local level . - <p> The Society for Technical Communication ( STC ) is a professional association dedicated to the advancement of technical communication . Technical communicators research and create information about technical processes or products directed to a targeted audience through various forms of media . <p> Qualitative Research Consultants Association ( QRCA ) <p> The Qualitative Research Consultants Association ( QRCA ) is a not-for-profit association of consultants involved in the design and implementation of qualitative research " focus groups , in-depth interviews , in-context and observational research , and more . <p> Usability.gov <p> Usability.gov- is- a leading resource- for- user experience ( UX ) best @ @ @ @ @ @ @ @ @ @ US Federal government and private sectors . <p> MRMW is the only global conference series focused on mobile , innovation and high-tech marketing research . Trusted by over 5000 participants around the world , the MRMW conferences are regarded as the industry 's gold standard and a must-attend event for research professionals every year . 
@@105507022 @907022/ <h> JUS <h> This Issue 98864 @qwx958864 <p> We are pleased to have an editorial by two of the leaders of the HCI community , Clare-Marie Karat and John Karat . They worked in a research and development group at IBM for many years and have contributed substantially to our literature . They discuss their experiences over the years as the profession and their role in it evolved . Woven into their story about the changes in their professional lives are their observations about their personal lives , and how they dealt with both being married to each other , often working on the same research team . <p> The first peer-reviewed article is by Jennifer Romano Bergstrom and her colleagues at the US Census Bureau . They report on a project to redesign a website which involved conducting four iterative tests . The concept of iterative testing has been part of user-centered design for more than 20 years . But the literature contains only a few examples of its application . Why ? It 's because there are a variety of challenges to managing and conducting iterative testing @ @ @ @ @ @ @ @ @ @ and we believe that this paper will be a valuable addition to the literature for usability practitioners . <p> Young Sam Ryu and his colleagues compared performance and satisfaction between a standard mouse and a touch-less mouse , which was manipulated by moving a finger without touching any surface . While performance with the touch-less mouse was inferior to that with the standard mouse , the touch-less mouse shows promise for use in situations in which a standard mouse is not practical . <p> Risto Toivonen and his colleagues report on their evaluation of a mobile workstation for use by physicians and nurses during hospital rounds . They used an expert review followed by a usability test of a prototype , as physicians and nurses conducted their rounds . With subjective measures and observations , they evaluated both the ergonomic aspects of the design and its operation . The result was a set of requirements that can be used to support the design of any mobile workstation . 
@@105507024 @907024/ <h> This Issue 98864 @qwx958864 <p> Usability engineering has always been about cost-effectiveness and visible added value . However , with the global economy crisis we are facing now , usability engineering may be one of the first to suffer . In a very timely manner , Tom Tullis shares experience-based and well-informed insights and tips on how to survive this crisis . In his invited editorial " Tips for Usability Professionals in a Down Economy " , Tom presents 10 very practical tips on how to maximize the effectiveness and efficiency of our work , while providing the added value of usability . <p> Technology is introduced to healthcare and medicine at an increasingly rapid pace . This trend presents new challenges to HCI design and usability . One of the technologies is Electronic Medical Records ( EMR ) . Our first peer reviewed article addresses the usability issues of EMRs . In their article entitled simply " Usability of Electronic Medical Records " , John Smelcer , Hal Jacobs-Miller , and Lyle Kantrovich review some of the common issues in EMRs , primarily from the physician 's @ @ @ @ @ @ @ @ @ @ in the " wild " ; i.e. clinics and hospitals . They conclude with some insightful and practical directions to take in order to improve usability of EMRs . <p> One of the common practices in empirical usability testing is to construct user profiles and personas , and recruit participants that can " play the role " of such users . Alex Genov , in his article " Usability Testing with Real Data " is raising the problem that the ' real ' persona of the ' real ' participants in the empirical usability study can have an impact on the findings . Alex Genov proposes some approaches of how to consider such ' real ' data in the analysis and interpretations of usability studies . <p> Finally , we are back to dealing with the ongoing challenges of testing mobile usability . In our third peer reviewed article " Flexible Hardware Configurations for Studying Mobile Usability " , Antti Oulasvirta and Tuomo Nyyss+nen address the specific challenge of having the appropriate setup for capturing the mobile interaction context . They describe a system developed to capture user 's interaction @ @ @ @ @ @ @ @ @ @ is a bit on the technical side , it can be a great reference for all those aspiring to develop and/or use tools for mobile usability testing " in the wild " . 
@@105507025 @907025/ <h> This Issue 98864 @qwx958864 <p> The ability to extract practical information from research articles still remains a challenge for any practitioner , particularly for usability practitioners . Caroline Jarrett , an associate editor in JUS , addresses this problem in her editorial : " On the problems and joys of reading research papers for practitioner purposes " . Caroline offers a personal , yet experienced , perspective on the problems associated with getting some useful and practical material out of articles . Caroline concludes that inapplicable research still has its value and it all depends on what the objective of the research is . <p> The challenge of sifting through the data we collect in usability studies and sorting out what are the issues we need or can deal with has always been a challenge to usability practitioners . In the first peer-reviewed article " A Structured Process for Transforming Usability Data into Usability Information " , Jonathan Howarth , Terence S. Andre , and Rex Hartson suggest that raw usability problems can be viewed as instances of usability problems . They present a structured process to extract @ @ @ @ @ @ @ @ @ @ take the findings of a comparative usability study based on questionnaires and make a decision with respect to the alternatives ? Young Sam Ryu , Kari Babski-Reeves , Tonya L. Smith-Jackson , and Maury A. Nussbaum , in their article : " Decision Models for Comparative Usability Evaluation of Mobile Phones Using the Mobile Phone Usability Questionnaire ( MPUQ ) " suggest that such a decision can be influenced by the questionnaire used . They compared usability of mobile phones and found that the questionnaire used and the method of computing the final scores can influence the decision making among the tested phones . <p> One of the objectives of empirical usability testing with real participants is to be cost-effective . This often entails reducing costs due to the participants . Juan E. Gilbert , Andrea Williams , and Cheryl D. Seals , in their article : " Clustering for Usability Participant Selection " addressed this issue . They suggest a method and describe a tool that can help decide on a smaller sub-set of participants that can still prove to be cost-effective for the objectives of the test . 
@@105507026 @907026/ <h> Articles by Jonathan Lazar <p> The Cambridge Workshop on Universal Access and Assistive Technology ( CWUAAT ) is held every other year at a UK Cambridge University college . CWUAAT ' 12 was part of this series , started in 2002 , that presents research from the international inclusive design community . <p> While most aspects of web accessibility are technically easy to solve , providing accessible equivalents of data visualizations for blind users remains a challenging problem . Previous attempts at accessible equivalents focused on sonification of population data . This paper describes the creation of two prototypes for providing real-time weather information in a sonified format for blind users . <p> In this article , we discuss results of usability evaluations of desktop and web-based email applications used by those who are blind . Email is an important tool for workplace communication , but computer software and websites can present accessibility and usability barriers to blind users who use screen readers to access computers and websites. 
@@105507027 @907027/ <h> JUS <h> This Issue 98864 @qwx958864 <p> Issue 4 is the final one for Volume 6 of the journal . I expect that Dennis Wixon 's invited editorial will generate lively reaction . Dennis argues that the usability profession has largely ignored one of the key components of usability engineering : establishing quantitative usability goals early in product development . Those goals can enhance scenarios and use cases and stimulate development teams to conduct early and iterative testing , which are often cited as essential to creating a successful user experience but seldom practiced . <p> The first peer-reviewed article by Alla Keselman and her colleagues focuses on the usability evaluation of 3-D virtual worlds . Their study includes an analysis of the applicability of some of the traditional evaluation methods to complex , dynamic virtual worlds and of the need to modify or find analogues for other methods . In their empirical pilot study they explore the results of their analysis and show that it is possible to assess the usability of Second Life applications . <p> Daniel Perry and his colleagues conducted a comparison usability test @ @ @ @ @ @ @ @ @ @ was to use the data generated from the comparison to develop standard metrics that Federal agencies such as the EPA and DOE can use to measure the usability of thermostats . The metrics had to be quantitative to be credible with both Federal and industry organizations and directly applicable to any programmable thermostat . <p> Xristine Faulkner and Clive Hayton conducted a test in which web-based navigation menus were placed on either the left or right-hand side of the page . The data show no difference in time to buy between the two locations . The authors challenge the convention that navigation menus should be on the left-hand side of a webpage and suggest that there may be some advantages to placement on the right-hand side . 
@@105507028 @907028/ <h> This Issue <h> User Research of a Voting Machine : Preliminary Findings and Experiences <h> Abstract <p> This paper describes a usability study of the Nedap voting machine in the Netherlands . On the day of the national elections , 566 voters participated in our study immediately after having cast their real vote . The research focused on the correspondence between voter intents and voting results , distinguishing between usability ( correspondence between voter intents and voter input ) and machine reliability ( correspondence between voter input and machine output ) . For the sake of comparison , participants also cast their votes using a paper ballot . <p> The machine reliability appeared to be 100% , indicating that , within our sample , all votes that had been cast were correctly represented in the output of the voting machine . Regarding usability , 1.4% of the participants had cast the wrong vote using the voting machine . This percentage was similar to that of the paper ballot . <p> Practical implications as well as experiences with this type of usability testing are discussed . 98865 @qwx958865 <p> @ @ @ @ @ @ @ @ @ @ . The research may focus on a fine-grained analysis of potential user problems or on an overall assessment of the usability of the voting equipment . It will be hard to serve both goals in one and the same study . <p> User research can effectively be conducted in the vicinity of polling stations . This makes it easy to design an ecologically valid research context . People also appear to be willing to participate in such a study , provided that it does not take very long . <p> It is important to clearly distinguish the test set-up from the real elections that take place in the same building . This can be done by clearly announcing the research with posters , by visually stressing the researchers ' affiliation , and by emphasizing the distinction in the instructions to the participants . <p> The best way to be able to include all voting options in the usability study and to be able to verify the correctness of participants ' votes is by using voting assignments . To avoid memory problems , it may be useful to support a voting assignment with a written note . 
@@105507038 @907038/ <h> Menu <h> Networking <h> Networking- <p> One of the primary goals of UXPA has always been to provide- ongoing networking opportunities- for user experience- professionals to meet and exchange ideas . A few ways to leverage your membership in UXPA include : <p> Attend our annual international conference , where many long-term friendships and business partnerships have been started . Find details about our 2015- conference in California . <p> Explore one or more of the regional conferences offered by UXPA chapters . Some great previous- conferences include : User Friendly ( Shanghai , November 2013 ) and- UX Hong Kong- ( March 2014 ) . <p> Join a- local chapter . UXPA has chapters all over the world . Visit one , or speak with a regional Director about starting a chapter in your area . Begin by- visiting our list of chapters by region . <p> Participate in- our LinkedIn.com- group . Our members include newcomers to the UX field as well as experienced practitioners . Wherever you are , there 's a UXPA member ready to help answer your questions and guide you with practical @ @ @ @ @ @ @ @ @ @ - <p> If you are interested in joining the LinkedIn for UXPA community ( it 's only one step away ) , sign up at the- UXPA invitation on the LinkedIn site . Once you fill in the request , the UXPA office verifies your membership , and the UXPA logo appears on your profile . 
@@105507040 @907040/ <h> Special Interest Groups <h> What is a UXPA Special Interest Group ? <p> UXPA Special Interest Groups ( SIG ) offer people with the same topic interests a place to come together to share their experience and to encourage each other . SIGs provide- a convenient network for everyday access to people with shared interests- to combine their knowledge <h> A Platform for Networking <p> One of the greatest resources for professional and personal development is to meet with colleagues in the same field . You gain new information on state-of-the-art technology , access to others ' experiences and knowledge and you will get the chance to broaden your insights. - SIGs can provide activities that connect- user experience professionals with each other . The SIGs may offer meetings with invited speakers , usability lab tours and roundtables locally and online discussions where- interesting usability problems can be solved jointly . <h> Professional Development <p> Your- meetings and activities allow you to expand your knowledge in user experience . Special Interest Groups bring continuous education opportunities closer to home and provide a bridge between the annual UXPA conferences @ @ @ @ @ @ @ @ @ @ a way for the members to grow in their profession . <h> Exchange of Services and Contributing to UXPA as an Organization <p> UXPA SIGs offer the possibilities of exchanging both professional experiences and services , such as reviewing each other 's papers and conference contributions . It will also act as a greenhouse for new ideas that can be implemented globally by the Association , and provide UXPA with potential new members to the Board of Directors . <h> A Creative and Stimulating Environment <p> Design contests and brain-storming exercises are fun and stimulating ways of learning more about usability and the other members of the Special Interest Group . Special Interest Groups- often organize activities such as usability lab tours and site visits to interesting work places , for example air traffic control centers and high-tech manufacturing plants . Learning more about usability issues in these " extreme " working conditions will often give you good ideas to bring back home . 
@@105507043 @907043/ <h> This Issue <h> A Structured Process for Transforming Usability Data into Usability Information <h> Abstract <p> Much research has been devoted to developing usability evaluation methods that are used in evaluating interaction designs . More recently , however , research has shifted away from evaluation methods and comparisons of evaluation methods to issues of how to use the raw usability data generated by these methods . Associated with this focus is the assumption that the transformation of the raw usability data into usability information is relatively straightforward . We would argue that this assumption is incorrect , especially for novice usability practitioners . In this article , we present a structured process for transforming raw usability data into usability information that is based on a new way of thinking about usability problem data . The results of a study of this structured process indicate that it helps improve the effectiveness of novice usability practitioners . 98865 @qwx958865 <p> There is a need for a more structured approach to transforming raw usability data generated by usability evaluation methods into usability information . Currently , this transformation is more of @ @ @ @ @ @ @ @ @ @ the skill and experience of the usability practitioner . <p> Usability problem instances serve as a bridge between raw usability data and usability problems . Each occurrence of a usability problem as encountered by a participant and observed by the evaluator is a usability problem instance . The same usability problem may be experienced by multiple participants or multiple times by one participant . For transforming usability data into usability information , a structured process based on usability problem instances can improve the effectiveness of novice usability practitioners . <p> Existing usability engineering tool support helps to improve the efficiency of experts , but does little to improve the effectiveness of novices . To continue to grow the usability engineering profession , it is important to improve usability engineering tools , so that they better support novices . 
@@105507044 @907044/ <h> Menu <h> Tenure-Track Assistant or Associate Professors <p> Department of Design and Environmental Analysis , Cornell University <p> Ithaca , NY <p> The Department of Design and Environmental Analysis ( D+EA ) is a uniquely multi-disciplinary department , composed of designers , environmental psychologists , and technologists who are deeply committed to enhancing the quality of life through a human-centered approach to design and research . D+EA addresses problems and opportunities from a systems view " people , products , processes , and places " to create strategic , sustainable , and healthy futures by design . Given its systems view , D+EA is characterized by strong faculty collaboration and interactions both within the department and across the university . For more information about D+EA , see http : //dea.human.cornell.edu/ . <p> Design for Interaction " improving and augmenting the interactions across systems of people , places , and things through design , and recognizing that the strategic process of design impacts individuals and organizations . <p> Design for Sustainability and Resilience " integrating the needs of societies and the integrity of nature , while enhancing quality-of-life and @ @ @ @ @ @ @ @ @ @ redefines interactions between people and the environment as well as between built and natural environments . <p> Emerging Technologies for Design " developing , testing , and understanding the place of new technologies in creative processes , practices , and design outputs ( materials , systems , tools , and means of fabrication and manufacturing ) . Emerging Technologies for Design strives to establish new relationships between people and technology that may help us better understand ourselves . <p> Qualifications : <p> All new faculty members are expected to develop an externally-funded research program of international distinction . Responsibilities include teaching undergraduate and graduate courses based on department needs , advising our B.S. , M.A. , M.S. , and Ph.D . students , and providing service to the department and university community . Preference is given to applicants with a completed Ph.D. , but applicants with a terminal Masters degree with a track record of significant research accomplishment will be considered . Preference is given to applicants who have earned one design-focused degree ( e.g. , in interaction design , user experience design , interior design , industrial design , @ @ @ @ @ @ @ @ @ @ arts ) where there is a robust , human-centered design orientation . <p> To Apply : Please submit the following materials through Academic Jobs Online https : **38;99;TOOLONG and include a cover letter , CV , 3 confidential letters of recommendation , 3-5 selected publications , a portfolio , and evidence of teaching excellence . Applications will be accepted and reviewed until positions are filled . For the portfolio , please submit a single document describing each example and include a link ( URL ) to it ( using YouTube or Vimeo ) . 
@@105507050 @907050/ <h> This Issue <h> New Frontiers in Usability for Users ' Complex Knowledge Work <h> Abstract <p> For usability professionals , one of the top priorities of the coming decades is to assure that products are usable and useful for people 's complex work in complex systems . To meet this challenge we need to better understand the nature and practices of various domain-based complex tasks and the flow of people 's work across tools . This essay gives an overview of articles in this issue that address these challenges and their implications for usability and usefulness . <p> Two years ago , Mike Albers organized and held a workshop on the usability challenges of computer-supported complex work . The enthusiasm generated by that workshop led to this special issue of Journal of Usability Studies ( JUS ) . This special issue explores the problem of how to make human-computer interactions simpler for dynamic knowledge work . Along these lines , many experts in exploratory analysis agree that answering the following questions is a top priority for the coming decades : <p> Why is support for knowledge work so @ @ @ @ @ @ @ @ @ @ high quality design ? <p> How is quality demonstrated and best assessed in regard to usefulness and usability ? <p> The three articles in this issue take an important first step in tackling these questions . All of them strive to clarify why knowledge work is distinctively challenging by understanding how people perform their complex work . The authors of all three articles find that work typically considered well-structured is , in fact , not well-structured . Moreover , mistakenly supporting it as such has the following adverse effects : <p> Users are confident in work that is inaccurate . <p> Users misapply tool capabilities that results in sub-optimal results . <p> Users do not deem a system valuable and therefore stop using it . <p> To identify support that users need but do not get from their technologies , all three articles address certain decision points in users ' work . The articles examine users ' complex tasks and decisions at very different scales , in distinct domains , and with varying attention to levels of expertise . Yet they share the following findings and themes : <p> In @ @ @ @ @ @ @ @ @ @ into knowledge are vital parts of the decision process . <p> During decision episodes , people integrate formal and informal approaches . <p> Technologies and their built-in information and task models are not suitably matched to users ' actual informal ways of knowing , their transitions between formal and informal approaches , or their processes and needs for integrating formal and informal approaches . <p> Successful task performance depends on this integration . Experts expect to do it readily . <p> In the first article , " Creating Effective Decision Aids for Complex Tasks " Caroline Hayes and Farnaz Akhavi look at complex work at an activity level . Examining the activity of engineering design , they analyze choices and choice processes in decision making that designers perform when they winnow down large numbers of possible competing design options to just a few best alternatives . <p> As a classic optimizing task , it would seem that designers should welcome help from deterministic or fuzzy mathematical decision-making models . Yet results of the authors ' studies reveal otherwise . Hayes and Akhavi explain the effects of optimizing models on the @ @ @ @ @ @ @ @ @ @ uses of them , and the misfit between these models ( formalistic tools ) and designers ' actual practices . <p> In their conclusions , Hayes and Akhavi speculate about strategies for incorporating formalistic tools into people 's complex decision making in ways that are sensitive to people 's varying levels of expertise , tasks , and core practices of decisions-in-the-making . One core practice , for example , is decision makers ' common process of moving recursively between information seeking and making comparative judgments about alternatives they are considering . <p> In " Switching Between Tools in Complex Applications , " Will Schroeder looks at complex tasks and decisions at a lower scale . He focuses on choices users make when switching from one tool to another in a complex software application . A better understanding of switching , Schroeder urges , can lead to improved designs of complex applications ( toolkits ) so that users can interact more efficiently and effectively with different components and tools . <p> In Schroeder 's study , " tools " are not different applications but rather various graphical user interfaces or windows @ @ @ @ @ @ @ @ @ @ programming , working with graphics , working with matrices and arrays , or viewing documentation and help ) . Users performed two predefined tasks . In examining the results , Schroeder compares users ' switches in tools , and is able to make and interpret these comparisons across cases because of novel visualizations that he created to display usage log data over time . In addition to these comparisons , Schroeder correlates switches , quality of outcomes , and expertise . <p> Schroeder finds that scattered , frequent switches from tool to tool ( including documentation ) are associated with high task completion , high degrees of expertise , and high user satisfaction . His findings lead to such questions as the following : <p> How might tools cue users about these effective switches - making the right switch at the right time and place - that experts seem to master ? <p> What cues for self-monitoring and encountering barriers might be productive ? <p> Finally in " Unexpected Complexity in a Traditional Usability Study , " Tharon Howard looks at tasks at a fine scale . He gathers and @ @ @ @ @ @ @ @ @ @ formats in handbooks and applying them correctly to texts . Howard 's study shows that one of the " grey areas " making complex tasks complex is a user 's need to make choices when " usage " leaves many options open . Here " usage " relates to language but broadly speaking it is convention and context . <p> Howard recounts the ways in which usability testing , when it is designed to detect complexity in tasks , can prompt clients to reconceive prior notions of formulaic user tasks and redesign products accordingly . One important redesign involves enhancing information seeking through visual communications and scenario based presentations of information . Howard stresses , however , that these techniques must selectively guide attention based on users ' purposes and communities of practice . <p> Implications for usability raised by all the studies commonly highlight the need to further explore the following : <p> Situational awareness : Designing and testing for situational awareness are vital for adequately supporting complex work . Yet current notions in human-computer interaction about what needs to be cued in designs for situational awareness and how @ @ @ @ @ @ @ @ @ @ cues required for complex work . <p> Methodology : A mixed methodology for user studies and usability evaluations is necessary . The mixture may take diverse forms but it must fit the purposes of the research - and be sensitive to demands of numerous stakeholders . <p> Visual communication : Visual communication needs to be exploited more than it is today . <p> Many other issues not raised here are also important for improving usability . The following include some of these issues for further exploration : <p> Cognitive models : We need more domain-based cognitive models of complex work , a better understanding of what they provide that personae can not , and how to better apply them to design . <p> Evaluation : We need more formative testing of actual designs that purposefully aim to support complex work . <p> Rationales : We need specifications , rationales and evidence of core user requirements and design criteria for systems that are useful and usable for complex work . <p> These issues are compelling and far from resolved . They intrigued and engaged the group of us who reviewed the @ @ @ @ @ @ @ @ @ @ Mike Albers , Whitney Quesenbery , Ginny Redish , and myself . The insights presented in the submitted manuscripts sparked many conversations among us about the multiple scales of complexity , about designing for usability , and about strategically evaluating systems . We hope that JUS will consider more special issues dedicated to this fascinating topic and professional challenge . 
@@105507053 @907053/ <h> This Issue <h> Abstract <p> IBM was contracted to provide a new Air Defence Command and Control ( ADCC ) system for the Royal Air Force . The IBM Human Factors ( HF ) team was responsible for the design of the operations room , workstations and the graphical user interfaces . Because the project was safety-related , IBM had to produce a safety case . One aspect of the safety case was a demonstration of the operational effectiveness of the new system . <p> This paper is an in-depth case study of the user testing that was carried out to demonstrate the effectiveness of the system . Due to time constraints the HF team had to observe five participants working simultaneously . Further , to provide a realistic operational environment , up to twenty-eight operators were required for each test . The total effort for this activity was four person-years . The paper will detail the considerations , challenges and lessons learned in the creation and execution of these multi-user user tests . 98865 @qwx958865 <p> Although the client may specify or mandate a UCD approach ( @ @ @ @ @ @ @ @ @ @ 2 ) to be followed , they may not realise how expensive it is to support this approach . The reduction in whole life cycle costs of using a UCD approach are well documented , but the costs of supporting the development from the customer 's perspective are sometimes less explicit . Therefore , it is worthwhile discussing this with the client early in the programme , and if necessary , updating the risk register accordingly . <p> The summative user tests are an excellent mechanism for collecting data and evidence for a safety case . However , to ensure that appropriate safety-related data can be collected on a safety-related system the participants must be placed in an appropriate working environment under a realistic amount of workload . <p> Do not rely solely on a summative user test to produce all the necessary operational safety-related data for a safety case . A good design process that includes activities such as formative user tests and heuristic evaluations , as well as the demonstration of effective training materials can also produce an enormous amount of safety-related data 4 . <p> The production @ @ @ @ @ @ @ @ @ @ tests . The Training team must be notified as early as possible on the required roles and tasks for each user test . This will ensure that the appropriate training materials are available at the right time . Further , Train The Trainer materials are not usually appropriate for a user test . They are often lacking in context and standard operating procedures as these are usually added later by the client 's Trainers once they have been trained . On this project , it took the HF team 60 days to convert the materials into the correct format . Therefore , design the training materials so they can be used effectively for both purposes ( i.e. TTT and user tests ) . <p> Limit the number of observers to only those that are absolutely necessary . When these tests were carried out , the project was nearing completion and everybody wanted to see the system in action . This was a cause of concern for the HF team as the more people that are around the more chance of distraction and skewing of results , albeit accidentally . Therefore @ @ @ @ @ @ @ @ @ @ the user test area . Even attach copies of the instructions to the test room entrance , in the canteen and in congregation areas . 
@@105507056 @907056/ <h> Usability Testing of Mobile Applications : A Comparison between Laboratory and Field Testing <h> Abstract <p> Usability testing a mobile application in the laboratory seems to be sufficient when studying user interface and navigation issues . The usability of a consumer application was tested in two environments : in a laboratory and in a field with a total of 40 test users . The same problems were found in both environments , differences occurred in the frequency of findings between the contexts . Results indicate that conducting a time-consuming field test may not be worthwhile when searching user interface flaws to improve user interaction . In spite of this , it is possible that field testing is worthwhile when combining usability tests with a field pilot or contextual study where user behavior is investigated in a natural context . 98865 @qwx958865 <p> When testing a user interface of a mobile application , field testing may not necessarily be the best place ; mostly because it is more time consuming than the lab test . <p> Testing in the field requires double the time in comparison to the laboratory @ @ @ @ @ @ @ @ @ @ tests per day you run in the laboratory . <p> In a field test , running a pre-test or a pilot is critical : there are so many details that can go wrong , and you really need to check that everything is working correctly . <p> When testing in the field , be prepared that things will not go as planned : there may be interruptions and unexpected events more than in lab. 
@@105507059 @907059/ <h> This Issue <h> Card Sorting : Current Practices and Beyond <h> History and Assumptions <p> Card sorting was originally developed by psychologists as a method to the study of how people organize and categorize their knowledge . As the name implies , the method originally consisted of researchers writing labels representing concepts ( either abstract or concrete ) on cards , and then asking participants to sort ( categorize ) the cards into piles that were similar in some way . After sorting the cards into piles , the participants were then asked to give the piles a name or phrase that would indicate what the concepts in a particular pile had in common . <p> In the world of information technology , information architects and developers of desktop and Web-based software applications are faced with the problem of organizing information items , features , and functions to make it easier for users to find them . Card sorting can be an effective means of discovering the optimal organization of information for potential users ' viewpoint . <p> Unfortunately , the development and practice of card sorting in @ @ @ @ @ @ @ @ @ @ experience , with little influence from systematic research . As a result , many of the decisions faced by developers in conducting a card sort study are based on analogy to seemingly similar arenas ( e.g. , survey administration ) and situational circumstances . As we point out in the following sections , this assumption is often questionable . <h> Number of Participants <h> Current practice : <p> Because of the similarity between card sorting and survey research , it is assumed by many researchers that the more participants that can be recruited for the card sorting task , the more valid the results will be . However , in addition to the increased effort and the logistical issues that might be involved in recruiting a large number of participants , a much greater effort is required to analyze a large body of data . Interestingly , the one study that has been published ( Tullis &amp; Wood , 2004 ) on this question indicates that useful results can be obtained with fewer participants than one might otherwise suppose . <h> Recommendation : <p> As few as 25-30 participants will @ @ @ @ @ @ @ @ @ @ provided these participants are representative of actual users and are familiar with the domain being considered . Thus , researchers could save time and money , not to mention the headache of analyzing all the extra data . <h> Participant Instructions <h> Current practice : <p> When psychologists began to use card sorting as a methodology to study conceptual and category structures , they were careful to give non-directive instructions ( e.g. , " There is no right set of categories . Just group things together that seem to be similar in some way . " ) because they were concerned about biasing participants . Unfortunately , some information technology researchers have carried that practice over into current card sorting work . <h> Recommendation : <p> Avoiding bias is generally a noble goal , but this seems to counter the purpose of gathering information that is relevant to the goal of making it easier for users to navigate through Web pages . Hence , researchers should be explicit about the intended purpose of conducting the card sorting study ( e.g. , " Please group these items as you would expect @ @ @ @ @ @ @ @ @ @ <h> Participants ' Complete Understanding of Items <h> Current practice : <p> Far too many information technology researchers are overly optimistic about their participants ' familiarity with the items . <h> Recommendation : <p> Because it is impossible to guarantee that all participants will have a common understanding of the domain items , it is helpful to provide more detailed descriptions of the items to be sorted . In early research by psychologists , it was a common practice to include the description of an item on the back of the item card . Then participants could simply turn the card over and review the description to make certain they were thinking about the item in the intended manner . We strongly recommend that item descriptions be provided , even in the most promising situations ( e.g. , a corporate intranet with employees as participants ) . <h> Open vs . Closed Sorting Tasks <h> Current practice : <p> Most card sorting projects are open sort , where participants are given a list of content items representative of the content that is being planned ( or already exists ) for @ @ @ @ @ @ @ @ @ @ then asked to categorize the items in a way that represents their best organization for the Web site . Occasionally , researchers have data or experience that appears to justify the use of an existing organization ( i.e. , a pre-existing set of useful categories ) into which participants could be invited to sort the items , resulting in what is termed as a closed card sorting project . The assumption seems to be that the existing structure is close to optimal-it just needs some minor adjustment . <p> Too frequently in our experience , researchers are convinced they have a list of categories that is useful , but just needs some minor adjustment . However , they have little or no evidence related to whether or not potential users are of the same mind . In the extreme , we know of a project in which users refused to participate because they experienced such difficulty in making meaningful sense of the categories in which they were asked to sort a set of content items . <h> Recommendation : <p> Begin research with an open sort . Only after carefully @ @ @ @ @ @ @ @ @ @ validation . If the decision is made to begin with a closed sort , make it simple and be prepared to obtain less-than-optimal results . <h> Data Analyses <h> Current practice : <p> The ability to analyze card sorting data is a " touchy-feely " qualitative experience that ca n't be described beyond " eyeballing " and " looking for trends . " Eyeballing the data is very tedious , time-consuming , and idiosyncratic , which results in a procedure or method that is not reliable . It works well when there are only five participants and a small set of items , but not so well with 30+ participants and a hundred items . While this general method can be useful as an auxiliary aid , we prefer to begin with the more formal cluster analysis and resulting tree diagram . <h> Recommendation : <p> Cluster analysis is a statistical procedure developed to analyze similarities and differences in the ways people categorize sets of domain items . The results can be used to produce a hierarchical tree diagram ( formally called a dendrogram ) , an example of which @ @ @ @ @ @ @ @ @ @ number of cluster algorithms ( Romesburg , 1984 ) , we prefer a simple one based on the frequency with which content items are placed in the same category across all participants in a card sorting project . Essentially , the tree represents an average of the groups or categories produced by each of the card sort participants . It works on the principle of the similarity of each item to every other item , as sorted by the participants . <p> As shown in Figure 1 , the alternate shaded groups represent the highest degree of similarity ( i.e. , items in groups of the same shade are more similar than items in different groups ) . An advantage of a hierarchical tree is that if larger groups than those appearing in the tree are desired for a Web site , two adjacent groups can simply be combined , because adjacent groups are more similar than non-adjacent ones . <p> Figure 1 . Sample hierarchical tree diagram <h> Issues Needing Additional Research <p> There are several other practices that could benefit from additional research . The following sections present @ @ @ @ @ @ @ @ @ @ Sort just a few <h> Issue : <p> Most Web sites have sufficient content to produce very long lists for participants to sort . This , in turn , results in such a long list of domain items that it is questionable to expect participants to reliably perform the task . <h> Proposed research : <p> To she 'd some light on this issue , Tullis and Wood ( 2005 ) reported that useful results can be obtained by having participants sort a random sample of items , so long as enough participants are gathered to sufficiently cover the full set of items . We 're interested in pushing the boundaries of these findings . What if a randomly selected single item is given to thousands of participants in a closed sort ? How might the results differ from those of participants that have the context of a larger set of items ? <h> Finding vs. sorting <h> Issue : <p> There have been some discussions on the Web about whether a sorting task is a sufficiently close analogue to the typical finding task ( when people are browsing the Web @ @ @ @ @ @ @ @ @ @ card sorting as a basis for producing an information architecture . Two obvious differences between the tasks are ( a ) when browsing the Web users typically have a need ( i.e. , a mental description ) they 're trying to fill , and they hope to find something on the Web to fill that need ; and ( b ) participants in a card sorting task have the context of an entire list of items when considering each one . <h> Proposed research : <p> We plan to conduct and compare results of two closed sort studies . One would be a typical study in which each participant is given a list of items to be sorted into a set of pre-defined categories . In the second study , participants would be given the list of categories but only one of the items for which to choose a category . <h> Take your time vs. hurry up <h> Issue : <p> It 's common for us to observe researchers including phrases such as " take your time and carefully consider " in the pre-sorting instructions provided to participants . @ @ @ @ @ @ @ @ @ @ are n't particularly cautious when browsing , which calls into question whether encouraging card sorting participants to be cautious is appropriate . <h> Proposed research : <p> One recent study we conducted encouraged just the opposite of cautious card sorting . We encouraged participants to " sort the items reasonably quickly . " Early results indicate that there is n't a substantial difference . Furthermore , it 's worth considering that " reasonably quickly " is a better match for how most people browse and search on Web sites-clicking the first link that seems like it will yield satisfactory results . We plan to conduct additional studies relevant to this issue . <h> Multi-level sorting <h> Issue : <p> Given that all but the simplest of information hierarchies consist of more than two levels , it seems logical to allow participants to sort items into multiple levels . However , a difficulty arises during analysis . If " eyeball analysis " of single-level sorting is problematic with large numbers , then adding multiple levels exacerbates the problem . Though cluster analysis can be made to work with multiple levels , @ @ @ @ @ @ @ @ @ @ weakness of cluster analysis-the fact that it 's essentially an " average . " <h> Proposed research : <p> Because of the difficulty with data analysis mentioned previously , we have recommended a staged approach for obtaining a multi-level architecture . That is , first conduct a typical single-level sort , analyze to find a preliminary set of categories , and then use those categories as items in a second card sorting study . However , many researchers seem to believe it would be desirable to ask participants to create second level categories as well as first level categories all in one task . Even though we have n't yet found solutions to the analysis problems , we plan to continue to address this issue . <h> Summary <p> Given its origin in the investigation of knowledge organization and category structure , the methodology of card sorting has come a long way in the world of information technology . Card sorting has been a very useful method for gathering useful information from potential Web site users related to the most useful information architecture of a Web site and for the @ @ @ @ @ @ @ @ @ @ resident on a desktop computer . <p> Not surprisingly , a set of common practices in conducting card sort studies and analyzing the resulting data has developed over time . Although the results of these efforts have produced valuable results on balance , the practices are based more on anecdotal evidence than systematic research . We discussed what we consider to be the limitations of current practices and made recommendations based on our own experience ( both as users of the methodology and as vendors of a leading online card sorting tool ) . We also suggested research studies that we believe need to be conducted and that we are in the process of conducting . 
@@105507066 @907066/ <h> Menu <h> User Researcher <p> Hach Company <p> Loveland / Fort Collins , CO <p> Position Summary : - <p> Success in the UX User Research role requires the ability to plan , design , facilitate , analyze and report product usability throughout our development process to provide the user insight needed for product definition and design . The primary focus of this role is on usability testing . We are looking for a candidate that thrives in a collaborative , user-centered and data driven environment . <p> Qualifications : <p> Education , Background and Skill Requirements <p> 3 or more years of experience with User-centered Usability Testing Practices <p> Facilitation experience with examples of techniques and portfolio that includes examples of business outcomes from research and testing 
@@105507067 @907067/ <h> This Issue <h> Creating Effective Decision Aids for Complex Tasks <h> Abstract <p> Engineering design tasks require designers to continually compare , weigh , and choose among many complex alternatives . The quality of these selection decisions directly impacts the quality , cost , and safety of the final product . Because of the high degree of uncertainty in predicting the performance of alternatives while they are still just sketches on the drawing board , and the high cost of poor choices , mathematical decision methods incorporating uncertainty have long held much appeal for product designers , at least from a theoretical standpoint . Yet , such methods have not been widely adopted in practical settings . The goals of this work are to begin understanding why this is so and to identify future questions that may lead to solutions . This paper summarizes the results of several studies by the authors : two laboratory studies in which we asked product designers to use various mathematical models to compare and select design alternatives , and a set of ethnographic studies in which we observed product designers as they @ @ @ @ @ @ @ @ @ @ and needs during decision making . Based on these studies , we concluded that the mathematical models , as formulated , are not well suited to designers ' needs and approaches . We propose a research agenda for developing new approaches that combine decision theoretic and user-centered methods to create tools that can make product designers ' decision making work easier , more systematic , more effective , and more reportable. 98865 @qwx958865 <p> This article looks at some of the issues in designing and developing tools for complex problem solving in work domains such as mechanical design , logistical planning , and medical decision making . It is particularly challenging to develop tools ( software or otherwise ) to assist in these tasks because so much of the work is cognitive . The steps are often internalized , highly nuanced , and dependant on a body of personal experience , rather than well-defined processes . Tools to support decision making must often cater to the needs of a diverse group of users who may range from domain novices to domain experts . Additionally , the tasks themselves and the @ @ @ @ @ @ @ @ @ @ making incorporation of extensive volumes of complex knowledge in a tool impractical . The lessons learned from the work reported in this paper can be applied to many other complex domains . <p> In order to design tools to fit users ' needs in complex domains , it is important to understand ( a ) how they solve problems , ( b ) where their strengths and weaknesses lie , and ( c ) what type of challenges , constraints , and conditions exist in their actual work environments . Observational and measurement techniques for understanding work and problem solving , such as ethnographic studies , protocol analysis , and laboratory studies ( in addition to usability studies of the evolving tool ) , may be even more important in complex , expert domains than they are in other tasks . <p> Domain experts , intermediates , and novices may not all have the same needs , nor do they have the same knowledge , strengths , and weaknesses . While it is desirable to design a tool to assist all of these different types of users , it may @ @ @ @ @ @ @ @ @ @ designers may need to choose to design specifically for users at a specific range of domain expertise levels . <p> Tools must respect users ' problem solving approaches . For example , in the study reported in this paper , users employed flexible approaches , with many short cuts such as only searching for additional information if it was needed to distinguish between two top design alternatives . <p> Users were impatient with having to gather , estimate , or make guesses pertaining to information that might not actually be necessary for a decision and viewed such tasks as unnecessary or " busy work . " ( For example , user may feel , " I can choose the best alternative in my head much more quickly without having to specify all this information for the computer , so why should I bother with the computer tool ? " ) . Unless users perceive that the work of using a tool will directly benefit them , they are likely to view these data entry tasks as unnecessary chores and are unlikely to adopt the tool . <p> Old style " @ @ @ @ @ @ @ @ @ @ to encode complex , highly nuanced , and highly contextual expert knowledge in software . Such systems were expensive to build , as well as brittle and hard to maintain . It can be more cost effective and successful to provide tools that can help problem solvers explore , organize , and visualize problem-relevant data to which they can apply their own knowledge and judgment powers . <p> Similarly , tools should avoid requiring users to articulate and enter large quantities of their knowledge or judgments on a problem-by-problem basis . Unless users perceive this work as directly benefiting them in their current task , they are likely to perceive such data entry tasks as unnecessary chores . 98864 @qwx958864 <p> Creating effective decision aids is not simply a matter of finding a method that computes the most correct answer or the interface that best presents the data , but also of finding the most effective way to integrate tools with human problem solving needs . For example , tools based on mathematically correct and sophisticated models may not actually improve problem solving performance if they frame the problem in @ @ @ @ @ @ @ @ @ @ approaches . <p> In this work , we have chosen to study decision aids that support the product design process , because the ability to compete in the global economy is highly dependant on the ability to rapidly produce high quality , low cost products . However , products such as cell phones , healthcare systems , or space stations are becoming increasingly complex . This means that product designers face greater decision making challenges than ever before . <p> We have further chosen to focus on design decisions made during the very early , conceptual stages of design , because decisions made at this stage have the largest impact on the cost , quality , and success of a product ( Ishii , 2004 ) . By the time one gets to the final stages of the design process , the major decisions have already been made and further choices have relatively little impact . Unfortunately , the conceptual design stage is an inconvenient time at which to make decisions and commitments because choices based on cost and performance must be made between alternatives that are little more than @ @ @ @ @ @ @ @ @ @ assessments of cost and performance during the conceptual design stage . <p> Yet product designers must make choices anyway in order to make the design task manageable . They must because there may be hundreds or even thousands of alternative designs and possible variants for a single complex product . It may require a team of 20 , 100 , or more designers to fully develop even one alternative to a sufficient level of detail that accurate cost and performance estimates can be produced . Thus , it is simply too expensive to develop them all to a level of detail sufficient to allow one to accurately choose the best with certainty . Some may argue that it is inappropriate to apply mathematical methods during conceptual design because of the uncertainty and lack of detail . However , this is where designers face the largest decision making challenges and where improvements can have the largest impact . <p> Mathematical decision making approaches that represent the uncertainty of a situation have long held great theoretical appeal for helping product designers make better design decisions for all the reasons above . However @ @ @ @ @ @ @ @ @ @ help them visualize , analyze , and simulate the performance of products , mathematical decision methods are not used consistently in their daily work . This is not to say that they do not use them . In fact they do . Many companies use a variety of decision making techniques to explain or justify major decisions . However , designers do not tend to use these methods in their day-to-day design decision making to the extent that one might expect . <p> The overarching goals of this work are to begin to develop a better understanding of why mathematical decision methods have not been embraced by designers in the workplace , and how the mathematical decision methods can be made to better support designers ' needs in the workplace . The more immediate goals of this paper are to quantify benefits and costs experienced by designers when using a variety of different mathematically-based decision aids in the laboratory , to better understand how product designers go about decision making in the workplace , and to use this understanding to explain the laboratory study results , and to inform future @ @ @ @ @ @ @ @ @ @ of our model of the product design process that incorporates not only our own data and observations but also unifies a number of models created by other researchers to describe the design process . To study the issues above we used a combination of quantitative and qualitative methods that include the following studies : <p> Two controlled laboratory studies in which we asked product designers to use various computer decision aids . <p> A set of ethnographic studies ( Blomberg et al. , 2002 ) in which we observed designers as they worked on real design tasks ( e.g. , fly-on-the-wall observations of work in a typical setting ) . <p> The research questions that we viewed as relevant to ask evolved during the sequence of studies reported as we gained knowledge of product designers , their approaches to decision making , and the challenges of their work environments . Initially , we set about asking the question : " Which decision aid supports product designers better - one that allows them to express their uncertainty about the price and performance of the design alternatives , or one that does @ @ @ @ @ @ @ @ @ @ with conceptual designs , we assumed that the former would yield the best results . <p> However , much to our surprise , our experiments did not show this to be true . The decision aid that allowed product designers to express uncertainty did not yield significantly better or worse results than the other decision aid . Furthermore , while both decision aids produced better results under certain circumstances than no decision aid , it was not clear that their benefits necessarily outweighed their costs ( time for data entry , software installation , software training , etc . ) . This situation was unexpected and was far more nuanced than we had initially envisioned . <p> At this point , our questions turned towards finding explanations and a deeper understanding : " Why did n't the ' uncertainty ' decision aid yield more benefits than the other ? " " Why did n't either decision aid yield a clearer balance of benefits ? " " How well does the framework imposed by most mathematical decision approaches fit with product designers ' actual approaches to decision making in the workplace @ @ @ @ @ @ @ @ @ @ better support product designers ' actual needs in the workplace ? " These questions form a future research agenda for development of human-centered decision aids to meet the workplace needs of designers working in any complex and uncertain domain . We feel that the results of such a research agenda will apply to electro-mechanical product design and , more generally , to any type of complex design task . <h> Methods <p> We used two methods in this work : ethnographic and laboratory studies . To a lesser extent , we also drew on protocol studies . We surveyed existing models of design with special emphasis on models derived from protocol studies of designers solving actual problems , because the studies provided insights into actual behavior . Aprotocol study is one in which subjects are asked to think aloud as they solve problems . Everything they say is recorded for later analysis ( Ericsson &amp; Simon , 1980 ) . In contrast , ethnographic observations are observations of work as it is carried out in a normal setting ( Bloomberg et al. , 2007 ) . The ethnographic observations differ @ @ @ @ @ @ @ @ @ @ asked to solve specific problems or think aloud . Laboratory studies are more highly controlled than either ethnographic or protocol studies . While the situation in laboratory studies may be somewhat artificial , these studies allow measurement and quantification of phenomena in a way that ethnographic studies can not . Thus , each of these study types , ethnographic , protocol , and laboratory studies , can provide different views of the complex phenomena associated with product design processes . Together they provide a mix of qualitative and quantitative data that allow construction of a richer overall picture than any one method alone . <h> Related Literature <p> The following section presents the mathematical decision making methods . <h> Mathematical decision making methods <p> Complex decision problems require decision-makers to choose from available alternatives characterized by multiple qualitative or quantitative criteria ( Saaty , 1980 ) . Multi-criteria decision making ( MCDM ) techniques ( Klein , 1993 ) are a broad family of mathematical methods that compare alternatives in a set using multiple criteria . For example , a prospective car buyer might compare his or her car choices @ @ @ @ @ @ @ @ @ @ comfort . The criteria used may vary from buyer to buyer depending on what is most important to that particular person . One common MCDM method is the weighted sum method ( Hayes , J. R. , 1981 ) in which each term in the sum represents how well an alternative fulfills a given criterion , and the term 's weight represents that criterion 's importance to the decision maker . Variants of the weighted sum method are popular because they are relatively easy to understand and use . Note that MCDM methods do not automate the decision process , nor do we view that as a desirable goal . Instead , they provide a structured approach through which people arrive at their own decisions by allowing them to specify the criteria they view as important and their judgments of the values associated each alternative . <p> One can further divide MCDM ( and weighted sum ) methods into deterministic and non-deterministic methods . Deterministic decision making methods are those that do not explicitly incorporate a representation of uncertainty , for example , the cost of an option may be @ @ @ @ @ @ @ @ @ @ the decision maker may understand that this number is not exact , the degree to which it is not exact is not represented . In contrast , non-deterministic decision making methods are those that incorporate some explicit representation of uncertainty or unknowns . For example , the uncertainty in the cost of an option may be represented as a range of possible costs or as a function describing the likelihood of various costs . <p> Vagueness and ambiguity can be modeled by many techniques including those based on fuzzy set theory ( Thurston &amp; Carnahan , 1992 ) . The merit of fuzzy techniques is that imprecision ( Bellman &amp; Zadeh , 1970 ) is recognized as an element of the decision model . The drawbacks of such techniques are the relatively high computational effort required for modeling the decision situation and processing the input information ( Law , 1996 ) . However , while much research has focused on the development of formal decision making methods , relatively few studies have assessed their practical utility and impact in complex tasks . In the laboratory study summarized later , deterministic @ @ @ @ @ @ @ @ @ @ against designers ' typical , informal methods . <h> Product Design Processes <p> In the following sections , we will summarize some of the existing literature describing salient properties and structure of design processes . The models described were developed primarily in the context of mechanical design processes . However , the general characteristics of most complex design processes , whether mechanical , software , or systems , are essentially similar . <p> Uncertainty is present in all designs ( Aughernbaugh &amp; Paredis , 2006 ) , from hand-held computer devices to space station systems . Even when a design is considered to be complete , there may still be uncertainty concerning issues such as the performance of the design under all the conditions to which it may be exposed in its working life , its manufacturing feasibility , or the final cost . <p> Uncertainty is most prevalent in the early stages of design , also known as conceptual design , when the alternatives under consideration may be little more than quick sketches or brief outlines . At all stages of the design process , designers must repeatedly choose @ @ @ @ @ @ @ @ @ @ of alternatives is known as the down selection process . Conceptual design , down selection , and their relationship to the overall design process are shown in Figure 1 . The model of the design process shown in this figure represents the authors ' ; integration of their own observations with several other design process models ( described below ) to create a unified design process model . <p> There is an overall progression in the design process from conceptual design to detail design ( Pahl &amp; Beitz , 2006 ) . There is no distinct dividing line between these stages . The design is gradually transformed from a set of sketchy alternatives created during the conceptual design stage into one or more detailed and polished designs that are finalized in the detail design stage . The transformation occurs through many iterations in which they explore , develop , and eliminate many alternatives ( Simon , 1985 ) . Multiple iterations of the design process are represented by the helix in Figure 1 ( Blanchard &amp; Fabrycky , 2006 ) ; each loop of the helix ( depicted as a @ @ @ @ @ @ @ @ @ @ design process . The labels on each loop , such as requirements gathering , design review , and down selection , represent some of the activities performed in each iteration . However , design activities rarely proceed in a precise and orderly progression . <p> Ullman , Dietterrich , and Stauffer ( 1998 ) performed protocol studies of mechanical design processes in which designers were asked to create designs to solve real engineering requirements . They found that , in practice , there is much jumping back and forth between steps . From these studies , they developed the Task/Episode Accumulation ( TEA ) model in which designers incrementally refined and patched design alternatives in a series of design episodes . Each design episode addressed one of six goals : plan , assimilate , specify , repair , document , and verify . These goals can be addressed in almost any order and can be viewed as an alternate way of dividing and describing design activities listed previously . <p> Figure 1 . An iterative model of the design process . <p> The work reported in this article deepens prior @ @ @ @ @ @ @ @ @ @ is the process through which design alternatives are compared and selected for further development . <h> The Design Tasks <p> We studied student designers in the context of two different electro-mechanical design domains : design of a robot arm for a quadriplegic man and design of a manned lunar excursion vehicle . The robot arm for the quadriplegic man had to be capable of manipulating a variety of lightweight objects found in his home and office environment such as paper , small books , compact discs , and soft drink cans . It had to have a control interface that a quadriplegic person could manipulate and be powered by the on-board battery system of his electrically powered wheelchair . Additionally , it had to be simple for an assistant to mount and unmount from the arm of the wheelchair . The electronics and motors had to be reasonably weatherproof , light weight , and inexpensive . To meet the cost goals the students made extensive use of scrap aluminum and junk yard parts such as automobile seat motors . Finally , the students had to build and test their best @ @ @ @ @ @ @ @ @ @ . A wheelchair mounted robot arm created by a student design team . <p> In the second domain , teams of student designers developed designs for manned lunar excursion vehicles , some of which are shown in Figure 3 . All groups were given the same design goals by the NASA Johnson Space Center . They were to design a manned lunar excursion vehicle that would provide occupants with protection , life support , mobility , towing capability , communication , and sufficient power for an average excursion . The excursion vehicle must also fit inside the launch vehicle and deploy successfully at the landing site . Students did not have the tight budgetary restrictions as the students building the robot arm , nor did they have to build the excursion vehicle . <p> Figure 3 . Four design alternatives for a lunar excursion vehicle developed by students . <h> Two Decision Aids <p> In order to study the impact of mathematical decision aids in the design domains described above , the authors implemented two computer decision aids to assist product designers in comparing design alternatives and making down selection @ @ @ @ @ @ @ @ @ @ . They simply provide a structured interface that allowed them to enter their own criteria and judgments and a convenient computational method for systematically combining the components used in their decisions . <h> A deterministic decision aid <p> The deterministic decision aid allowed product designers to compare design concepts using a deterministic weighted sum method . The product designers first entered a list of criteria that they felt were important to the quality of the design . For example , for a manned lunar rover , designers included criteria that a design must include protection for the astronauts riding in the vehicle , life support systems , and structural integrity so that the vehicle will not crumple as it moves over the rough lunar surface and so on . Many more criteria can be entered ; however , the snapshot of the interface shown in Figure 4 shows the interface after only the first three criteria have been entered and the fourth is about to be entered . <p> For each criterion an importance weight must be entered using a slider bar ( as shown on the left side of @ @ @ @ @ @ @ @ @ @ that the criterion is not very important , and a weight of 10 indicates that it is very important . <p> Once criteria and importance weights are entered , then the names of each alternative design must be entered . The product designer entered four alternatives and named them concept x , y , z , and i ( as shown across the top of the interface ) . One could enter more alternatives if desired or use more meaningful names . <p> Figure 4 . The data entry interface for the deterministic decision aid . Decision makers enter values using a single slider bar . <p> Next , a value must be entered to indicate how well each alternative fulfills each criterion . For each alternative , product designers entered a single value for each criterion using a slider bar ( as shown in the central area of Figure 4 ) . A value of 0 indicates that the alternative does not fulfill the criterion well and 10 indicates that it fulfills the criterion very well . <p> Once all values are entered , the Calculate button ( bottom @ @ @ @ @ @ @ @ @ @ each alternative based on the importance weights and values . These scores are displayed near the top of Figure 5 . The data entry interface for the fuzzy decision aid . Decision makers enter value ranges using a pair of slider bars . under each concept name . Higher scores indicate better overall value . These scores can be used to rank the design alternatives from best to worst . <h> A fuzzy decision aid <p> A second version of the decision aid implemented a fuzzy weighted-sum method , based on that described by Bellman and Zadeh ( 1970 ) . The interface was very similar to the deterministic decision aid except that for each alternative concept product designers had to specify a range of values indicating how well each alternative fulfilled each criterion using a pair of sliders to enter an upper and a lower value ( shown in Figure 5 ) . Thus , the product designer has indicated that he or she thinks that concept 1 may be " providing protection " anywhere from very poorly to an average amount . This allows designers to indicate their @ @ @ @ @ @ @ @ @ @ . <h> Laboratory Studies of Decision Aids in Product Design Decision Making <p> This section briefly summarizes two laboratory studies ( Akhavi and Hayes , 2007 ) that used the decision aids described above to investigate the costs and benefits experienced by mechanical designers when using these decision aids verses no aid . <h> Study 1 <p> In the first study , we asked seven student designers ( all at the intermediate level of design expertise ) to rank , from best to worst , four different design alternatives for the elbow joint on the robotic arm and three different design alternatives for a mounting plate ( which would be used to mount the arm on the wheelchair ) . All students had been working on the robot arm design task since the start of the semester , so they were familiar with the task and the criteria for an effective solution . While it might have been desirable in some respects to use subjects who had no prior experience with this particular robot arm design task and the alternatives , it was necessary to use subjects who already had @ @ @ @ @ @ @ @ @ @ understand the criteria and the properties of the alternatives , which were non-trivial to understand . Each student was asked to individually use the two decision aids described earlier to assist with the ranking : the decision aid based on the fuzzy technique and the other based on the deterministic technique . <p> We found that all students produced identical rankings for the solutions regardless of the decision aid used . However , the fuzzy decision aid required significantly more time on average than the deterministic one : 12.5 minutes versus 7 minutes , p-value = 0.02 . This time difference appeared to result directly from the additional data entry required for the fuzzy method , which required entering two values for each alternative and criterion . The deterministic method only required one value . After discussions with the students we concluded that they all produced identical rankings because for both the elbow joint and the mounting plate , the alternatives were clearly very different from each other in quality with obvious winners and losers . In such situations , designers can make choices readily without the assistance or overhead @ @ @ @ @ @ @ @ @ @ comment that they liked the way the tool allowed them to systematically layout the criteria and value judgments for all alternatives . They printed out the decision matrix produced by the tool so they could include it in their final project report as a convenient summary justifying their design decisions . <p> The important lesson learned from this first study was that computer decision aids may not add value for all design decisions , particularly if the top alternative can easily be distinguished from the others . For the next study , we designed a situation where the alternatives were very close in quality so that the top alternative was not easy to identify without careful consideration . <h> Study 2 <p> The second study explored the use of decision aids in the context of the manned lunar excursion vehicle design task . <h> Subjects <p> Twenty-six participants were used in the study . Eighteen were senior undergraduates in a capstone design course , and eight were engineering design professors . The students were considered to be intermediate level designers ( not novices ) and the professors were considered to @ @ @ @ @ @ @ @ @ @ All students in the class were given the same design task : to create a design for a lunar excursion vehicle . A total of 12 designs were created by four teams of students . Each team presented its three designs to the class , and the class decided which was best , average , and worst . Next , all 12 designs were re-sorted into three sets , each containing four designs . Set A contained only the best designs from each team . Set B contained all the average designs from each group , and Set C contained all the worst designs from each group . Thus , each set contained four designs that were similar in quality and would require some thought to rank them from best to worst . Furthermore , because the designs within each set crossed the boundaries of the student teams , none of the students had yet spent time comparing any of the designs with in the new sets . Thus , we created fresh comparison tasks for the students participating in the experiment . <h> Method and tool inputs <p> Each @ @ @ @ @ @ @ @ @ @ a different decision making method to each of the three design sets . The following were the three methods : <p> A fuzzy weighted sum method ( Akhavi , 2006 ) that was incorporated in a computer decision aid . It provided users with two slider bars that allowed them to set the top and bottom of the range bounding the likely values for each alternative and criterion . <p> A deterministic method ( a standard weighted sum ) that was incorporated in a computer decision aid . It provided a single slider bay that allowed users to enter only a single value for each criterion . <p> A no mathematical method that presented subjects with a set of drawings and supporting descriptions , and asked subjects to use whatever their normal method was , which was typically a manual , " seat-of-the-pants " ranking of alternatives . <p> The reader should note that the mathematical methods do not apply any internally encoded design expertise . The design expertise and judgments come entirely from the human participants . The tools simply provide a systematic structure and method to facilitate their @ @ @ @ @ @ @ @ @ @ computed and displayed a single overall value score for each alternative based on a combination of all criteria . However , the fuzzy method actually produced a probability density function for each design alternative describing a distribution of the probable values . Thus , we could have chosen to display the results in many ways , ( e.g. , as a function curve or a range ) but for simplicity we choose to display only the average of each probability density function . <h> Ranking <p> Next , for each method participants were instructed to rank a set of four design alternatives from best to worst . The order and the pairing of methods with design sets were systematically varied . The participants had received instruction in their design class on how to use the weighted sum method to compare and rank design alternatives using a calculator or spreadsheet to perform the calculations . <h> Data recorded <p> The experimenters recorded the following data : <p> Rankings assigned to each alternative in a set . <p> Time required to rank each set . <p> Users ' preferences for one method @ @ @ @ @ @ @ @ @ @ rankings , time , and user preferences . <h> Rankings <p> The rankings for a set of alternatives , ordering them from best to worst , represent a decision ; the top ranked alternative represents the decision maker 's top choice . However , not all decisions are of equal quality . One question that we wished to assess is whether the decision method had an impact on decision quality . <p> Unfortunately , decision quality is difficult to assess directly for many reasons . The knowledge and skill of the decision maker impact the likelihood that the alternative identified as " best " will actually prove to be the best once it is actually built and criteria ( such as cost , performance , reliability , and marketability ) can be tested . We define a high quality decision as one where the decision maker 's rankings accurately reflect the rankings computed from empirically measured data once the design alternatives are actually built and deployed , using the decision maker 's specified criteria . <p> Although alternative prototypes are sometimes built and tested during a design process , the @ @ @ @ @ @ @ @ @ @ for complex devices like lunar exploration vehicles . Thus , in many cases , it is not possible to directly measure decision quality because most of the alternatives are never built . <p> However , there are other measures that one can use as indicators of decision quality when decision quality can not be measured directly . While experts lack perfect judgment , they are far better than others at making judgments in their own area of expertise . Experienced conceptual designers for space missions estimate cost within 10% of the actual cost ( Mark , 2002 ) , which is quite impressive given the novelty of the designs and the number or unknowns they must manage . Additionally , it has been found in domains ranging from manufacturing plans ( Hayes &amp; Parzen , 1997 ; Hayes &amp; Wright , 1989 ) to medical diagnosis ( Aikins et al. , 1983 ) that while experts may sometimes disagree on which is the top alternative , there are high correlations in their rankings even when those rankings are arrived at independently without consultation . In other words , even if @ @ @ @ @ @ @ @ @ @ choices , it is likely that both alternatives will be ranked near the top for most experts . If one assumes that experts are able to judge quality , then one indicator of quality is the correlation of a decision maker 's rankings with those of experts . Figure 6 shows the average rank correlations between the rankings of the expert subjects and between the intermediates and the experts . Thus , the taller the bar is in Figure 6 , the higher the level of agreement with experts ( or of experts with each other ) . <p> Figure 6 . The average correlation of subjects ' rankings to expert rankings , using three different decision methods for intermediate-level and expert designers . <p> These results show that experts were indeed more consistent in their rankings than were the intermediates . The decision aid used also made a difference in average correlation with expert rankings : <p> For expert product designers , ranking correlations between expert designers increased significantly when design experts used either of the decision aids ( fuzzy or deterministic ) then when they ranked alternatives by @ @ @ @ @ @ @ @ @ @ . <p> For expert product designers , there was no significant difference between the fuzzy and deterministic decision aids . This was contrary to our initial expectations that the fuzzy method would be superior , particularly for conceptual design because it allows product designers to express the uncertainty inherently associated with an incomplete design . <p> For intermediate-level designers , there was no significant difference between any of the methods . <p> Each subject 's rankings were produced independently . They were not allowed to discuss the relative merits of the alternatives in a set with other subjects prior to the experiment . Thus , the correlation between subjects is not the result of group discussions . We feel that these results indicate that experts are better at making decisions than intermediates , most likely because their greater experience allowed them to assess the alternatives more accurately . Both decision aids helped the experts to produce better decisions , possibly by encouraging them to think more systematically and carefully about the criteria , the alternatives , and their relative merits . The decision aids did not appear to make a @ @ @ @ @ @ @ @ @ @ intermediate-level designers , possibly because they lacked the experience to make good assessments of the likely cost , performance , etc. of the alternatives . <p> What was surprising was that the fuzzy method did not have a significantly different impact on the consistency of rankings from the deterministic method . In fact , it appears slightly worse than the deterministic method for both groups ( although not significantly ) . There are many possible explanations for this result . Perhaps the subjects were more used to thinking of criteria in terms of single values than ranges ( e.g. , they are more comfortable thinking in terms like " the cost is $10 " as opposed to " the cost is probably between $8 and $14 " ) ; perhaps they were not good at estimating uncertain values ( Tversky , 2003 ) or the display used in the experiment was not supporting this reasoning about uncertain values in a way that fit their internal concepts . <h> Time <p> Figure 7 shows the time required by both intermediates and experts to rank the alternatives , and identify a " @ @ @ @ @ @ @ @ @ @ method required the most time at about 13 minutes on average , the deterministic method required about 9 minutes , and ranking alternatives by hand required only 4 minutes . The differences between all methods were significant ( alpha = 0.05 , p-value = 0.0001 ) . <p> The results for experts were similar . Experts required more than 16 minutes to rank four alternatives using the fuzzy method , 14 minutes using the deterministic method , and less then 5 minutes when ranking the alternatives by hand . All differences were significant ( alpha = 0.05 and p-value = 0.0001 ) except for that between the fuzzy and deterministic methods . <p> Figure 7 . Both intermediate-level and expert designers used more time when ranking alternatives using the computer decision aids . <p> It was evident from observations of the subjects during the experiment that the decision aid required more time than ranking alternatives by hand largely because of the data entry associated with using the decision aids . When ranking alternatives by hand , subjects simply sorted or numbered a stack of drawings that required relatively little time @ @ @ @ @ @ @ @ @ @ alternative when using the decision aid . The fuzzy decision aid required more data entry ( two numbers for each value ) than the deterministic aid ( one number for each value ) which explains why the fuzzy aid required more time . However , spending more time with each alternative may also have encouraged subjects to think more carefully about their relative merits . <p> It may appear counter-intuitive that the experts should spend more time than the intermediates to reach decisions , especially given results such as those reported by J. R. Anderson ( 1980 ) in which he describes experts performing tasks faster than non-experts . However , many of the task domains described by J. R. Anderson , such as cigar rolling and flash arithmetic , while not simple , are less complex than design tasks . For more complex tasks , such as manufacturing , planning ( Hayes &amp; Wright , 1990 ) , military planning ( Marshak , 1999 ) , and equine nutrition ( R. Anderson , 2003 ) , experts have been observed to take more time to complete problem solving tasks @ @ @ @ @ @ @ @ @ @ domains has been that non-experts are simply not completing as many problem solving steps or considering as many issues as experts . The non-experts lacked the experience to know they should be doing these steps or considering these options . Solution quality usually suffered as a result . <h> User preferences <p> A survey given to each subject showed that 63% of the experts preferred the fuzzy decision aid , 38% preferred the deterministic aid , and none preferred to use no aid . In contrast , 50% of the intermediates favored the deterministic decision aid , 39% the fuzzy aid , and 11% no aid . <p> It is interesting that the intermediate-level product designers preferred a decision aid over no aid because it is not clear that the aid provided significant benefits , and it required more time than doing the task by hand . More surprising still is that the experts preferred the fuzzy decision aid over the deterministic aid ; both provided similar benefits in increased consistency in rankings ( which may indicate increased decision quality ) but because the fuzzy aid required more time one @ @ @ @ @ @ @ @ @ @ While users do not always prefer the method that improves performance the most , it is important to understand what users ' preferences are as indicators of what methods they may be willing to adopt and use , given the right conditions . <p> However , it is not uncommon for subjects in a laboratory experiment to express a preference for a technique that does not actually improve their performance ( Morse et al. , 1998 ) . In this case , the subjects ' preferences may have reflected an intellectual appreciation of the mathematical methods encoded in the decision aids . The intermediates may have preferred the deterministic method for its simplicity . This may also reflect students ' lower level of comfort with statistical concepts of uncertainty . The expert designers , however , were more versed in and had a better working appreciation for the uncertainty associated with the cost and performance of design alternatives . <p> Ultimately , despite the preferences expressed during the laboratory study , subjects did not necessarily use the decision aids in subsequent product design work . An intellectual preference expressed during @ @ @ @ @ @ @ @ @ @ typical pressures and deadlines of the workplace , is not the same as a practical preference in the context of a working environment where perceived benefits must outweigh perceived costs . However , we believe it is important to understand users ' preferences because it may impact the ease with which they are willing to accept and use a particular decision aid , given that an appropriate balance of costs and benefits can be achieved . <h> Ethnographic Observations of Design Decision Making <p> The results above provided a very different picture of the usefulness of mathematically-based decision aids than we had predicted . We had expected that the fuzzy method would provide more benefits to both experts and intermediate-level designers than the deterministic method because it would allow the uncertainty of the conceptual design process to be reflected in the decision process . However , we did not find this to be the case . Only expert designers benefited from the decision aids , but only for difficult decision situations . They benefited about equally from both the fuzzy and deterministic aids , but the fuzzy aid required almost @ @ @ @ @ @ @ @ @ @ the added time required for either decision aid made it unclear as to whether the benefits of using such tools would justify the costs . However , we were not yet convinced that mathematical decision aids and fuzzy methods could not provide a satisfactory balance between costs and benefits given the right application , tool design , and sensitivity to users ' needs . <p> The next step was to try to better understand the results above . Were the failures of the tools a matter of improving the information presentation ? Reducing the data entry time ? Increasing the users ' training in the underlying mathematical concepts ? Or , were the problems deeper : was there a mismatch between the way in which designers make decisions and the assumptions and approaches assumed by these methods ? <p> To gain insights into these issues , we turned to the ethnographic observations which we had collected prior to the laboratory experiments . We had studied the four teams that created designs for lunar excursion vehicles and the team that created a wheelchair mounted robot arm . The four lunar excursion @ @ @ @ @ @ @ @ @ @ over the course of a semester . The robot arm team was similarly observed over the course of a different semester . <p> The ethnographic observations revealed that designers continually generated and evaluated alternative designs throughout the design process , some of which were entirely new concepts , while others were minor variants on existing alternatives . To keep the number of alternatives under current consideration manageable , designers continually engaged in down selection to prune out the less promising ones . They appeared to follow at least two different approaches to down selection that we call rapid elimination and considered comparison . Additionally , they were often observed to engage in information seeking if they did not feel comfortable with the amount of information available on each alternative . <h> Rapid elimination <p> In this variant of the down select process , design alternatives were only briefly considered before being discarded on the basis of rapid , informal assessments . Alternatives were often discarded based on a single criterion . For example , a designer might say , " this option is way too expensive for our budget " @ @ @ @ @ @ @ @ @ @ ) than our other options . I do n't see the need to consider it further unless we are desperate . " Options eliminated by this method were usually those that were clearly dominated by others ( e.g. , worse in all major criteria ) . Rapid elimination is by nature imprecise and may sometimes lead to inappropriate elimination of alternatives ( some of which were later revisited ) . However , it is also a very pragmatic approach and probably necessary given the enormous volume of down select decisions that must be made during a typical design process . If designers considered all decisions in depth , design progress would rapidly come to a stand-still . <h> Considered comparison <p> In this variant of down selection , designers were observed to compare several options during group discussions , often comparing all alternatives by one criterion , then another , possibly revisiting one , and so on . They frequently added additional criteria discovered through discussion or revised initial estimates of criteria importance . Thus , unlike formal decision methods in which the criteria and their importance are pre-determined , @ @ @ @ @ @ @ @ @ @ making process . It was only occasionally that designers performed this type of in-depth comparison . It occurred most commonly as a deadline was approaching at which time the team must select and justify a single " best " alternative which they would develop and prototype . In many , but not all cases the alternatives compared were not obviously inferior or superior to each other than those pruned through rapid elimination . <p> Their process resembled a blend of the decision making processes described as the lexicographic method and a weighted sum method ( Hayes , J. R. , 1981 ) . In the lexicographic method , alternatives are first compared by the most important criterion ; if they are equal by that criterion then they are compared by the next most important criterion and so on . This can be contrasted to a weighted sum method in that all major criteria are considered at one time for all alternatives . The considered comparison process differed from both in that designers initially focused a small handful of criteria that they deemed most important . Those criteria were not typically @ @ @ @ @ @ @ @ @ @ which the designers developed for each alternative through discussion of their properties . ( We will later refer to this as a seat-of the-pants judgment . ) If consideration of that small handful of criteria did not produce a clear winner they often considered additional criteria . This process might best be described as a very flexible and very approximate weighted sum method in which criteria , importance weights , and values are continually added , subtracted , or modified . <h> Information seeking <p> Choosing an alternative in the down select process is tightly tied with information seeking . At many points in the design process , designers lacked sufficient information to make informed comparisons between alternatives , particularly during conceptual design . Designers seek information through many methods . Sometimes they create the information themselves by developing more detailed drawings of targeted areas of a design or by building and testing prototypes . For example , information about the likely performance of the Mars Rover on Mars was obtained by building and testing prototypes in the harsh conditions of the Anaconda desert . Sometimes information is produced through @ @ @ @ @ @ @ @ @ @ it is collected from external sources , for example by searching the library and web or by calling multiple venders to gather a range of price quotes . <p> Some information seeking activities require significant effort , knowledge , and cost . Designers must make judgments about when the cost of information seeking is likely to pay-off in the final product . An issue observed in senior undergraduate designer teams was that they did not always know when to seek more information or when to stop . Deadlines were very important in forcing them to think critically about what information was most important and to focus their information seeking efforts . Furthermore , they were far more likely to seek information in areas where they felt knowledgeable and comfortable and to avoid seeking it in areas unfamiliar to them . For example , they were very comfortable elaborating physical , three-dimensional details and conducting mathematical analyses of specific aspects such as stress and torque , but they were far less comfortable developing cost and manufacturability estimates . They did not know where to look for cost information or who to @ @ @ @ @ @ @ @ @ @ by the instructors , but they often grossly underestimated costs and appeared completely unaware of the degree of uncertainty in their estimates . This avoidance is probably not laziness , but simply a hesitancy to engage in an effort of unknown magnitude for an unknown benefit . <p> Bradley and Agogino ( 1994 ) also describe information seeking as an important part of the process through which design alternatives are selected . They studied this process in the context of automated selection of design component choices from a catalog . They describe a mathematical formulation which can be used to decide when it is worthwhile to expend the cost and effort required to gather additional information . However , the method requires the designer to put in effort to collect the input data for the method , which they may not be willing to do if they believe they can fare almost as well without using any special analysis for information seeking decisions . <h> Summary of observations <p> Designers had more than one approach to decision making : they used both rapid elimination and considered comparison for more difficult @ @ @ @ @ @ @ @ @ @ rapid to deliberate focus . Decision aids may not be appropriate and may be viewed as burdensome in rapid elimination tasks , although they provide value in tasks requiring considered comparison . This observation supports the inference drawn from the laboratory studies that decision aids are not necessarily appropriate for all decisions . <p> The tasks of information seeking , comparison of alternatives , and down selection are tightly intertwined . This may suggest that to support designers ' actual work practices , a decision aid may need to support all of these tasks seamlessly . Neither of the mathematical models used in this study , nor the decision aids incorporating them , supported information seeking . This may limit the utility and impact of the decision aids . <p> The process of arriving at a decision is a flexible exploration process . The process of preparing for a decision is really one of exploration to develop a deeper understanding of the design goals and the alternatives and the unexplored possibilities . As Ullman et al . ( 1988 ) also observed designers exhibit great flexibility in this exploration process @ @ @ @ @ @ @ @ @ @ gathering additional information , making comparisons , adding new criteria , adjusting estimates , etc . Ideally , a decision aid should support this flexible exploration process . Most mathematical decision methods assume that this exploration has already been done , and that design goals , relevant criteria , and alternatives have been specified and are now fixed . Such assumptions are likely too rigid for the ill-defined nature of complex design tasks , particularly conceptual design . <p> Precise information or statistical distributions describing likely design performance are often not available in practice . Furthermore , there may be high costs associated with information seeking . Thus , the assumption made by many mathematical models that statistical distributions estimating design performance can be obtained may not be reasonable . Decision aids must not assume it is . <p> Designers can rapidly apply much knowledge and experience in their heads . However , articulating this information and entering it into a tool may be perceived to be a burden . Additionally , the designers are very astute about which information they need most . They do not typically explore all @ @ @ @ @ @ @ @ @ @ criterion that will distinguish top options , skipping many others . This is a time-saving strategy that most mathematical methods do not support . <h> Discussion <p> The following sections discuss some questions that occurred as a result of the study . <h> What may discourage use of mathematical decision aids ? <p> First , it was simply too time consuming to use mathematical methods for all decisions . As mentioned earlier , if these methods were applied to every small design variation , the design process would become exceedingly slow without improvement in most decisions , especially when there is one obvious winner . Users of any tool ( software or otherwise ) are very sensitive to the costs and benefits that they personally derive from the tools , and they may not be willing to use them if they perceive the benefits to be smaller on average than the extra work required ( Grudin , 1988 ) . <p> Second , designers did not have a clear metric or rule of thumb that allowed them to identify situations in which the mathematical tools would be likely to yield @ @ @ @ @ @ @ @ @ @ which they are not sure whether time consuming mathematical methods will provide benefits , it is only natural to chose not to put in the additional work required to use them . <p> Third , the mathematical methods did not allow designers the flexibility to which they were accustomed when comparing design alternatives . We observed that designers would often incrementally consider design criteria , starting with those they considered most important , and conditionally exploring less important criteria as " tie breakers " if a winner did not emerge ( this is the flexible considered comparison strategy which we described earlier ) . Designers can save much time by only considering criteria when they need to and only considering them for specific alternatives . In contrast , the mathematical methods assume that a fixed set of criteria will be compared and users must specify all of them , regardless . <p> Finally , and probably most importantly , most mathematical models assume a rather limited view of decision making ( Klein , 1993 ) . Naturalistic decision making is an approach in which decision making is studied in the @ @ @ @ @ @ @ @ @ @ are typically carried out ( often work environments ) ( Klein , 1993 ; Suchman , 1987 ) . Some of the premises underlying research in naturalistic decision making ( Orasanu &amp; Connolly , 1993 ) are that traditional , mathematically-oriented decision making research focuses on only one part of decision making , the decision event . In a decision event , a single decision maker compares a fixed set of alternatives using a fixed and well defined set of goals . Additionally , if precise information on the performance of each alternative is not available , statistical estimates can be obtained . <p> However , in natural design decision making situations ( Dym , 1994 ; Thomas &amp; Carroll , 1984 ; Ullman et al. , 1988 ) few of these assumptions hold . Design is a good example of such a task ; alternatives are constantly being added , modified , or refined , as are design goals . Much information is simply unknown or hard to obtain . Additionally , there are practical time considerations ; the sheer number of decisions means that most must be made @ @ @ @ @ @ @ @ @ @ not captured by a decision event , for example , the information seeking behaviors that precede the selection of an alternative . <h> Why did n't designers benefit more from the non-deterministic method ? <p> One might expect that the non-deterministic ( fuzzy ) method would produce better results than the deterministic method because the former are based on more information . However , this was not observed to be the case in the experiment and set-up described ; the average " goodness " values produced for each alternative by the fuzzy and the deterministic methods were very similar to each other ( Akhavi , 2006 ) . Thus non-deterministic methods may not provide any direct benefit if applied only to the task of ranking alternatives in a classically framed decision event . Even if the uncertainty of each alternative 's value is displayed , it may not produce a significantly better ranking . <p> Thus , by framing design decisions as " decision events , " one may be asking the wrong question , " Which alternative is best ? " or more accurately , not enough questions . @ @ @ @ @ @ @ @ @ @ alternative is best ? " with " Do I have enough information to decide which is best ? " The uncertainty in the values of the alternatives can be directly used to assist designers in answering this second question , as illustrated in the example in the next section . Thus , by applying non-deterministic MCDM methods to a wider range of tasks ( alternative selection and information seeking decisions ) they may better support a designer 's practical needs . <h> Recommendations <p> The studies described above provide the beginning of an understanding of why product designers do not tend to use formal mathematical methods in their daily work , and what their actual needs are . However , many additional issues need to be explored in order to fully understand the situation , and how to best create human-centered design decision aids . <h> Future research questions <p> The following questions are specific examples that should be explored : <p> Are there ways in which mathematical decision methods can be made more flexible ? <p> Can mathematical methods be adapted to other decision making activities such as information @ @ @ @ @ @ @ @ @ @ relaxed to fit realistic design situations ? <p> Product designers may be more willing to use decision aids if those decisions aids can be better designed to fit the way they actually work . For example , flexible interfaces that allow product designers to switch rapidly between activities such as comparison of alternatives , information seeking , and adding or subtracting criteria may better fit their observed practice of jumping back and forth between these activities . Additionally , information solicited from product designers by the decision aid must consider designers ' willingness to find and enter the data and whether they can realistically obtain it in a timely and cost effective manner . For example , detailed statistical distributions describing cost and performance may not be readily obtainable for novel products , so it may not make sense to use a method that depends on highly accurate distributions . Finally , displays must be designed to present information in ways that facilitate understanding , integration , and navigation of product design information . For example , designers may want to " drill-down " into each of the alternatives to @ @ @ @ @ @ @ @ @ @ 's overall value . <h> Closing Thoughts <p> The contribution of this work is in identifying ( a ) the actual needs and constraints of designers when making design decisions in work contexts , ( b ) the ways in which two decision theoretic methods , when formulated as decision events , fail to meet those needs , and ( c ) a set of future research questions and directions to explore so that user-centered decision support tools can be developed that reflect the way in which designers work , and consider both the costs and benefits of such tools for users . We feel that the major challenges in developing such tools do not necessarily lie in development of new decision theoretic methods , but in gaining a better understanding of how designers work and apply human-centered design principles to existing methods so that they support practical human needs as they exist in the workplace . <p> Additionally , we would like to emphasize that human-centered design does not mean simply design of understandable displays , although displays certainly play an important role . Equally important , if not @ @ @ @ @ @ @ @ @ @ methods are applied and the interactions supported by the tool . All should support the way in which designers understand information , and the processes by which they solve problems . Ideally , displays should be designed so they can be understood with relatively little training by presenting concepts in familiar ways or by using familiar metaphors . <h> Conclusion <p> Laboratory studies , protocol studies , and ethnographic observations suggest there is a mismatch between the classical decision theoretic paradigm that focuses on a highly structured decision event , and the way in which designers actually approach such problems . By paying more attention to human-computer interaction issues associated with MCDM approaches , it may be possible to create mathematically-based tools that designers will actually want to use because the tools respect the constraints and challenges of real design tasks and work environments . This work takes the first steps in that direction by providing a greater understanding of how designers approach decision making tasks , what their needs are , and in what ways traditionally applied MCDM approaches meet and do not meet those needs . Tools that @ @ @ @ @ @ @ @ @ @ greater impact on engineering design than those that mostly sit on the shelf gathering dust . 
@@105507069 @907069/ <h> This Issue <h> Flexible Hardware Configurations for Studying Mobile Usability <h> Abstract <p> The main challenges for mobile usability labs , as measurement instruments , lay not so much on being able to record what happens on the user interface , but capturing the interactional relationship between the user and the environment . An ideal mobile usability lab would enable recording , with sufficient accuracy and reliability , the user 's deployment of gaze , the hands , the near bodyspace , proximate and distant objects of interest , as well as abrupt environmental events . An inherent complication is that the equipment will affect these events and is affected by them . We argue that a balance between coverage and obtrusiveness must be found on a per case basis . <p> We present a modular solution to mobile usability labs , allowing both belt- and backpack-worn configurations and flexible division of equipment between the user , the moderator , and the environment . These benefits were achieved without sacrificing data quality , operational duration , or light weight . We describe system design rationale and report first @ @ @ @ @ @ @ @ @ @ simplifying the system to improve cost-efficiency. 98865 @qwx958865 <p> The swiss army knife approach to mobile usability labs centers around implementing a base system for non-camera equipment that allows enough flexibility to enable belt-worn , backpack-worn , and wireless configurations . For the camera equipment , the following goals are important : <p> Flexibility in camera types , from small to large and from low fidelity to high fidelity <p> Options in camera attachment devices ( a pin , a shallow cell phone shell , a necklace , a pole , and so on ) <p> Options in cables and wireless transfer <p> Offloading cameras to the environment by using signal strength as the switching criterion <p> At the moment the resulting system is somewhat expensive , around 10,000 Euros including hardware and craft , but it is on par with comparable non-mobile usability labs . The most significant challenge is to improve the user experience for the moderator by streamlining the process of using such a setup and by improving the interfaces between parts of the system . 98864 @qwx958864 <p> Science and technology go hand in hand . @ @ @ @ @ @ @ @ @ @ by both conceptual and methodological advances . Of all methodological developments in usability studies , controlled laboratory-based usability evaluation may have had the most wide-spread impact on day-to-day operations . The two-room usability laboratory setup , deploying mirrors and multiple video cameras to record interaction on a desktop PC , was a necessary complement to the novel notions and operationalizations of usability that Jakob Nielsen put forward in his seminal 1993 book . Laboratory-based testing has become the de facto standard of usability practice worldwide . <p> However , nothing similar appears to have taken place on the side of mobile technology , although the research area has existed for over 10 years ( the conference Mobile HCI started a decade ago as a workshop ) . One reason may be technical : no solution has been proposed that is technically reliable , cost-efficient , and flexible enough . In fact , the number of systems presented in the literature is in the order of a mere dozen . Figure 1 presents a sample of four of systems . These previous systems can be divided into three classes : those @ @ @ @ @ @ @ @ @ @ user-worn sources , ( b ) user-worn sources only , or ( c ) device-based sources only . ( Sources are typically cameras and microphones but can also include logs collected on the phone . ) <p> In this paper , we extend on our previous work on mobile usability labs ( Oulasvirta , Tamminen , Roto , &amp; Kuorelahti , 2005 ; Roto et al. , 2004 ) . We present rationale and design of a " swiss army knife approach , " operating with the principle of supporting multiple functions and configurations with one system . The equipment should support any of the abovementioned three configurations if the study at hand so demands . We argue that aiming for ( a ) modularity , ( b ) scalability , and ( c ) flexibility is crucial if the equipment is to be used across many studies . We argue that another reason for the absence of conventions in mobile labs may be that one can not simply transfer the thinking behind laboratory-based setups to mobile conditions . The system should support capturing environmental events and the user 's @ @ @ @ @ @ @ @ @ @ This ability is central in the class of context-aware applications , but also relevant in any use situation where the user 's environment affects interaction . <p> In this paper , we describe our system Attentional Resources in Mobile Interaction version 2 ( ARMIv2 ) . The ARMIv2 system supports both belt-mounted and backpack-mounted configurations of recording devices , as well as totally wireless ones . Wires and wireless transfer can be chosen according to mobility conditions . The system also supports environmental cameras . It integrates all video into a single stream that can be uploaded to a PC for analysis after a trial . In addition , it has the longest operational duration reported and is light weight for the user . The single most important quality , however , is its support for different configurations . <p> In the latter part of the paper , we report first experiences from a real-field deployment . Of the three desirable qualities mentioned above , our approach is targeted towards flexibility in particular . Modularity and flexibility are desirable from the perspective of experimental validity . However , although our @ @ @ @ @ @ @ @ @ @ be cost-efficient as it places heavy demands on researcher training and system maintenance . We nevertheless believe these problems can be overcome and that the general solution represents a promising direction for mobile usability labs . <h> Rationale for field testing <p> We believe that usability studies should be conducted in the field if the key aspects of users ' environment can not be accurately simulated in a lab . For example , a student of mobile input devices may well get along with laboratory-based evaluations for most of the time . Real-world issues like timeout ( due to interruptions ) , one-handed use ( due to reserved modalities ) , application-switching ( due to multitasking ) , or slow use ( due to simultaneous walking ) can be staged in the lab , given that they are known in advance . On the other hand , there are applications where this strategy does not work . For example , one can not ( easily ) stage a whole city for a study of mobile maps or power relationships for a study of organizational use of mobile email . <p> @ @ @ @ @ @ @ @ @ @ present the following three conditions in which one should carry out usability studies in the field : <p> The environment affects the interaction loop through the user . For example , users often interrupt themselves to respond to events perceived in the vicinity . <p> The environment affects the interaction loop through the computer . For example , augmented reality and mixed reality interfaces have sensors that change the state of the interface according to environmental events . <p> The user 's actions are doubly-determined ( i.e. , they transform the environment as well as the interactive state ) . For example , when using location-aware mobile maps , a user 's movement changes the state of the environment as well as the state of the map . <p> There have been discussions around whether " it is worth the hassle " ( Kjeldskov , Skov , Als , &amp; Hoegh , 2004 ) ; that is , whether conducting evaluations in the field pays off in terms of increased ability to capture usability problems . The skeptics have based their arguments on experiments building on contrived operationalizations of mobility @ @ @ @ @ @ @ @ @ @ often operationalized by walking a pre-defined route . This , in effect , reduces environment to the role of a nuisance factor-a source of disruptive events and cognitive resource withdrawals . It is no surprise that the finding has been that field testing is less efficient in capturing usability problems . Environment is not fruitfully operationalized in such a way , rather one must think how environment may support or hinder interaction according to the three abovementioned conditions . Then , one must set up the testing situation so that such events can occur in a natural way and can be captured by the recording equipment . <h> Requirements <p> To demonstrate how varied the requirements for a mobile usability lab can be , and as a further argument for the swiss army knife approach , let us analyze the following three evaluation scenarios : <p> A real-world longitudinal study of a mobile collaborative awareness application . Goal : Understand real-world use of the system in a team of information workers . <p> A comparative study of mobile map representations . Goal : Assess which representation , 3D or 2D @ @ @ @ @ @ @ @ @ @ locating buildings in a city centre . <p> For the first evaluation scenario , a setup that records only the user interface might be enough . The two latter scenarios require at least some track of what happens in the user 's environment . The third scenario is the most demanding in this respect . Ideally , it requires systematic second-by-second analysis of both bodily ( turning of body , deployment of gaze , use of hands ; analyzed post-experimentally from video tapes ) and cognitive ( verbal protocols ) strategies , in addition to analysis of events at the interface and in the environment . Experience with several analogous studies of mobile systems has helped us gain perspective to mobile usability labs and spell out the following general goals : <p> Mobility . The system moves with the user , capturing interaction reliably wherever and whenever it takes place both in indoor and outdoor contexts of use . <p> Captures embodied interaction . The system captures both bodily and virtual components of interaction as well as environmental events that may have an influence on those aspects of interaction that @ @ @ @ @ @ @ @ @ @ not itself bring about direct or indirect changes that would cause bias or distortion in ecological validity . <p> Multi-method support . The system does not limit the researcher to one source of data . <p> Redundancy . The system has multiple data capturing mechanisms , so if one method or source fails then the data is not lost . <p> Quality control . The system allows the experimenter to be aware of the reliability of data captured both during and after an experiment . It can help the experimenter answer questions such as what caused missing data , what situations were data gathered , how reliably the data corresponds to the actual situations it comes from , and so forth . <p> Let us explain the background for these requirements . First , we took as our starting point that the core of mobility is that it foregrounds the changing relationship between the user and the environment . It raises new constraints and resources for interaction from context , and it allows users to involve and utilize new contexts . For the recording system it means that it should @ @ @ @ @ @ @ @ @ @ context . In practice , depending on the situation , this may include near bodyspace of the user , distant and proximate physical objects that a user interacts with , ad hoc environmental events , as well as deployment of gaze . The view that the core of mobile human-computer interaction is in the triad **25;139;TOOLONG demands flexibility in the placement of cameras as well as their division among the environment , the user , and the device . <p> Second , a threat to experimental validity is that the system itself affects the phenomenon under study , for instance due to its ( a ) physical qualities-e.g. , the weight of the system causing fatigue , ( b ) ramifications to the user 's processing of the interface-e.g. , a camera occluding the mobile device , or ( c ) social consequences-e.g. , a hat not being acceptable indoors . Camera types , positioning , and form are crucial qualities that affect these problems . Another factor is the moderator . The presence of a moderator may also affect events and is not always desirable . This is yet @ @ @ @ @ @ @ @ @ @ cameras and their positioning . The cameras should enable researchers to conduct studies without a moderator , preferably without sacrificing the ability to record in an environment . One solution is the utilization of surveillance cameras . <p> Third , the environment is not only of interest as such , but it also introduces noise , unexpected events , and it is the cause of technical unreliabilities . For example , due to a user walking very quickly , it may happen that the moderator is unable to reliably capture events in the front bodyspace of the user . These problems call for ( a ) redundancy in recording and ( b ) online quality control . The former can be addressed if the moderator can place " just in case " cameras to augment and back up the primary ones . The latter can be supported by providing a real-time copy of the A/V stream for the moderator . <h> Flexibility in practice <p> To address these requirements , we aimed for flexibility in the following four qualities of the system : <p> Camera types . At the moment @ @ @ @ @ @ @ @ @ @ ( 17 x 17 x 17 mm ) , but in the future a smaller " minitube " type ( 45 x 7 x 7 mm ) will be explored . <p> Camera attachment devices . The cameras attach using a pin , a shallow cell phone shell ( Figure 4 ) , a necklace ( Figure 2 ) , and a pole ( Figure 2 ) . With a pin we will be able to attach a minitube to a hat as in the LiLiPUT system ( Schusterisch et al. , 2007 ) . <p> Wires . Both wireless and cable transfer are enabled . Furthermore , by providing cables of different types and lengths , we enabled the positioning of user-worn , non-camera components either to a backpack or to a belt . <h> System Description <p> ARMIv2 is based on experiences and technology from ARMIv1 ( Oulasvirta et al . 2005 ) . It highlights the following features : <p> Belt-mounted , backpack-mounted , and wireless configurations are supported . All non-camera equipment , except cables , attach to a belt worn by the user or can @ @ @ @ @ @ @ @ @ @ to use wireless transmitters and let the moderator carry non-camera equipment . However , due to signal interference , this results in poorer video quality especially when outdoors . <p> Environmental cameras are supported . Surveillance cameras can be assembled to different places of the study site and can be switched to on based on signal strength ( Figure 2 ) . These cameras offer a very high image quality and their number is not limited . This enables us to run studies where users can move about autonomously without a shadower . Alternatively , we can also run a study with moderator-controlled cameras ( Figure 4 ) . <p> Wireless and wired transfer are both supported . All cameras can operate either with cables or wirelessly . <p> Figure 2 . From left to right : All non-camera equipment attaches to a black leather belt worn by the user . In this setup the front camera is in a small black box ( a necklace ) . ( b ) A lightweight pole attachable to almost any mobile device can host two minicameras ; one camera capturing the face @ @ @ @ @ @ @ @ @ @ on the display and keyboard . ( c ) Environmental cameras allow for higher quality third-person views . The receiver switches to the nearest camera automatically based on signal strength . <p> To our knowledge , these three are novel features in comparison to previous systems . The following are several other improvements that have been made in comparison to ARMIv1 : <p> Higher quality integrated video . New , compact MPEG-4 recorders provide the advantages of on-the-fly encoding , quick data transfer , and post-trial delivery via USB . Maximum recording times are about ten times longer than a cassette recording . Lower power consumption , price , and smaller size also promote these recorders . ARMIv1 's original video quad was replaced by the smallest digital quad available . It was custom-built into a more compact and usable video hub with a smaller die cast aluminum housing and integrated power and camera signal connectors . With the integrated connectors on the hub and cameras it is easy to use one to four cameras and arrange them into the quad view simply by unplugging and plugging them into different @ @ @ @ @ @ @ @ @ @ results in 234 kbps AVI files , with a resolution of 640 x 544 , a frame rate of about 17 fps , and a MS ADPCM 180 kbps 4 bit audio sound track . <p> Increased operational duration ( optimally up to 4 hours without a battery change ) . In practice we reached about 2.5 to 3.0 hours , which is a significant increase in comparison to previous systems . LiLiPUT ( Schusterisch et al. , 2007 ) and ARMIv1 both achieved about 90 minutes . <p> Lower weight . Due to using hand-crafted casings and generally the smallest available components , the user-worn system part weighs less than 2 kgs . The previous version required the user to carry a backpack of about 4 kgs . LiLiPUT 's user-worn part weighs about the same , but includes less functionality . <p> Figure 3 conveys the system architecture . The stabile part is marked with dark grey coloring ; all receivers for A/V data can be either wireless or wired . <h> Cameras <p> As in ARMIv1 , the cameras are the core of the system . In @ @ @ @ @ @ @ @ @ @ performance , size , cost-efficiency , and versatility . <p> In ARMIv2 , all miniature cameras were equipped with an automatic electric shutter and white balance . Camera elements were originally bought as miniature dome surveillance cameras that were modified for pole attachment . A miniature audio amplifier and a microphone , originally from the same surveillance product , were also re-housed and modified . Images from these cameras on the pole ( see Figures 2 and 6 ) are in the PAL format , still effective image quality is approximately 280 TV-lines or less . The necklace camera element performance is 350 TV-lines with dimensions of 30(H) x 30 ( W ) x 34 ( D ) mm and weighing 30 grams . <p> The environmental cameras provided full quality in PAL resolution , by one chip computer-controlled display ( CCD ) and programmability and versatility for all light conditions . When equipped with wide-angle lenses , the camera view can cover a space size of a meeting room . A practical limitation is posed by the fact that all video streams are integrated into one , which lowers @ @ @ @ @ @ @ @ @ @ experience , environmental cameras work best in small spaces like desktops and hallways . For outdoor experimenting , moderator-controlled camera operation is a better option . <p> Figure 3 . System diagram for the current setup <h> Current setup <p> Our current setup consists of the following three kits ( see Figure 3 for the system diagram ) : <p> User 's kit . The user-worn part ( Figure 2 ) includes the following : <p> One camera holder ( pole ) for attaching two cameras to the participant 's device . One camera shoots the display , the other looks up to the participant 's face . <p> A necklace-camera with a microphone <p> Leather belt , the following are carried on a leather belt : <p> A wireless 2.4 GHz video receiver <p> A video hub that integrates all the signals to the recording device and provides adjustable voltage for the attached cameras . <p> A Video HDD MPEG-4 recorder <p> Three battery packs ( Lithium 12V ) <p> Necessary cables <p> Moderator 's kit . The environmental camera setup consists of the following : <p> Three surveillance @ @ @ @ @ @ @ @ @ @ 110-240 VAC to12 VDC power adapters <p> Video statives and fixing equipment <p> Alternatively , a moderator-carried camera ( a Sony DCR-TRV30E PAL mini-DV digital handycam , equipped with a VCL-ES06 x0.6 wide-angle lens ) can be used . In addition , we have previously explored the option of hiding the moderator-controlled camera in a cell phone shell to make it less visible in public places ( Figure 4 ) . <p> Figure 4 . One of the earlier setups had the moderator-controlled camera built in a fake shell of a mobile phone . The wire goes through the sleeve to a recorder on the waist . This solution allows for quiet , non-visible recording in public places . <p> Maintenance kit . The following are tools for running and preparing the tests and maintaining the setup . <p> Battery rechargers <p> Wireless receiver for setting up the environment cameras <p> Travel cases for the equipment <p> The kits fit into two aluminum briefcases that can be carried to the site of experiment ( Figure 5 ) . The total worth of the current setup , including labor and component @ @ @ @ @ @ @ @ @ @ The whole kit fits into two aluminum cases that can be taken to the site of study . This case includes environmental cameras ; the other kit contains the user-worn components . <h> First Experiences <p> To acquire feedback on the system , we deployed a recent study using the system for mobile maps . In this field experiment , we asked 16 participants to carry out localization tasks in a city center , with either a 2D or 3D mobile map ( Oulasvirta , Estlander , &amp; Nurminen , in press ) . No environmental cameras were used in this ultra-mobile study where 16 different city sites were visited . A moderator-controlled , high definition camera with a wide-angle lens was used instead . <p> Figure 6 depicts output data after integration of the video output ( on the left ) with a reconstruction of the interface we automatically built from the logs . Using custom-made software , the log files from the mobile map software were integrated and then manually synchronised with the four-channel video . <p> Figure 6 . Sample data output from the first field experiment @ @ @ @ @ @ @ @ @ @ results of the experiment showed that 2D was better in both localization and navigation tasks , in all dependent measures we deployed . The mobile lab enabled us to gather unusually rich data from verbal protocols to subjective workload measures and from video observations to integrated interaction logs . These data afforded for going beyond typically measured performance measures , like task completion times and errors . Particularly , we were puzzled by the finding that 2D was associated with not only faster performance overall but lower cognitive load than 3D . The 3D maps were typically thought to lower cognitive demands by making it easier to match what was seen in the surroundings to what was seen on the map ( ego-centric alignment ) . Finally , we came up with the hypothesis that 2D users can better use their body to find alignment between the map and the visual scene . We carried out a second-per-second analysis of the video tape that proved this hypothesis right . We found that 2D users often tilted the device in their hands , used their upper body more , and were @ @ @ @ @ @ @ @ @ @ , they found effective cues quicker ) . Video tapes in combination with interaction logs and verbal protocols thus enabled us to find an explanation that we could have not found otherwise . These results were analyzed to draw implications to a new version of the 3D map that is functionally and representationally closer to the 2D map while retaining some advantages of the 3D map . The full results are presented in Oulasvirta et al . ( in press ) . <p> We were generally happy with the achieved quality . The final integrated video files were 2000 kbps MPEG-1 streams , with a resolution of 520 x 320 , a frame rate of 25 fps and a MPEG-1 Layer 3 64 kbps mono sound track . The sound track was passed through a 3000 Hz low-pass filter to dispose of a high-frequency , whining noise . For this data , we were able to carry out manual coding of events with good inter-coder reliability ( Kappa .75 ) , even for quite subtle behavioral measures ( such as turning of head , turning of device in hand , @ @ @ @ @ @ @ @ @ @ with one second accuracy . The freeware video annotation software InqScribe was used for coding . <p> There were a couple technical difficulties . We had to change recorder and battery type , and consequently re-ran three more subjects . The more persistent problems were due to environmental conditions and accidental events that hampered the use of one or more of the minicameras . Direct sunlight to the face camera , shutter adapting excessively to large contrasts in camera image , necklace camera temporarily occluded by clothes , random compression artifacts , and rain effectively prevented coding of some of the variables-particularly when these effected several cameras at the same time . These problems can be addressed in the future by camera selection , camera placement and attachment , and changing the video recorder . <p> We also noticed effects due to form and design . While our device-mounted parts include only two small minicameras weighing a few grams , they nevertheless affected how the mobile device was held in the user 's hand , which created a bias to keep it more upright than normally . Second , while @ @ @ @ @ @ @ @ @ @ particularly the pole on the device caught the attention of passers-by , although so sporadically that we did not assess this to pose a bias to the validity of the conclusions . <h> Recommendations <p> At the moment , we are focusing on improving and augmenting the system in several respects . We attempt to improve the system using the following approach : <p> Increase the operational duration by exploring different battery and receiver combinations <p> Implement synchronization of logging with recording events <p> Improve technical reliability of the minicameras by trying alternative models <p> Improve the ergonomics of the belt <p> We also plan to implement functionality for remotely triggering recording events via Bluetooth . This solution , enabling us to either start recording remotely or as a function of some context information , could save critical battery life . <p> Sensor extensions are also possible . Our collaborators at Helsinki School of Economics have developed a version based on ARMIv1 that integrated psychophysiological sensors . <p> As we have solved the most pertinent technical problems , the most significant problem at the moment is posed by the complexity @ @ @ @ @ @ @ @ @ @ to support flexibility , training researchers to use it , constructing a new setup for a study , as well as maintenance and repair , pose such an overhead that the system 's adoption even within the group has been slower than hoped . <h> Conclusion <p> This paper has proposed how the varying requirements of mobile usability studies could be addressed by a single mobile usability lab setup . In general , we find the approach of aiming for flexibility the key to the development of mobile usability labs in the future . The key endeavor in aiming for modularity and flexibility is the design the core of the system , in our case the belt-worn part . Flexibility can also be pursued in camera selection , attachment , and positioning , as well as their division between the user , the moderator , and the environment . <p> The system we developed has the following qualities : <p> Support for both belt-mounted and backpack-mounted configurations , as well as totally wireless ones <p> Environmental cameras , not supported by previous systems <p> Wireless and wired transfer both supported @ @ @ @ @ @ @ @ @ @ <p> Higher quality integrated video than our previous systems <p> Increased operational duration , optimally up to 4 hours without battery change , which is more than previously achieved 
@@105507072 @907072/ <h> JUS <h> This Issue <h> Introduction - August 2013 <p> Date : <p> November , 2012 <p> Volume : <p> 8 <p> Issue : <p> 1 <p> With this issue , we start the eighth year of JUS . We thought it was time to look back over the first seven years to see if we are meeting the objectives that UXPA established for the journal . We examined the 77 papers that were published between November 2005 and February 2012 . We looked at where the authors come from in terms of their geographical location and who employs them ; the topics covered and the methods used , and how often the articles were cited in the literature . The analysis shows that the journal has met its primary goals but could have a broader impact with more papers focused on requirements gathering and design rather than its heavy emphasis on usability evaluation . <p> The editorial , by Steve Portigal , addresses the issue of why after more than three decades of user experience practice and research so many products are still so unusable . He @ @ @ @ @ @ @ @ @ @ to improve the interactions between people and technology . As a profession , he believes we have been distracted by projects that address but rarely solve large societal problems causing us to neglect solving the problem of making products usable . <p> Our peer-reviewed article , by Shadi Ghajar-Khosravi and her colleagues , describes a case study of a new method for testing the usability of a product . Instead of having test participants attempt tasks to complete task scenarios , the participants ' task was to reverse engineer a software application while they thought out loud and were observed . The results suggest that having to recreate the application caused participants to pay more attention to its structure , which resulted in uncovering more usability problems than with traditional task scenarios . 
@@105507082 @907082/ <h> This Issue <h> Usability Testing with Real Data <h> Abstract <p> Usability practitioners run the risk of misreading the results of usability evaluations , either identifying false positives when artificial user data interferes with a user 's product experience or overlooking real problems when they use artificial user data . In this paper , we examine a strategy for incorporating users ' real data in usability evaluations . We consider the value and the challenges of this strategy based on the experiences of product teams in a consumer software company . 98865 @qwx958865 <p> Whenever possible , obtain your participants ' real data by having them bring their data with them to the test and enter it themselves . <p> If it 's not possible to have participants either bring or send in their own data in advance , make sure to allow sufficient time to develop and test the technical and logistical process of getting the participants ' data into your test prototype . <p> Involve your company 's privacy and legal stakeholders in decisions of how to handle the participants ' data . Consider encrypting data @ @ @ @ @ @ @ @ @ @ as possible ways to safeguard the privacy of participant data . <p> Prepare a tailored script for each participant by creating task templates ahead of time and then filling them in with each participant 's data as they are received . <p> When analyzing your results , consider constructing average scores for each participant on a per task basis before calculating success metrics across participants . This will help to reveal overall performances when not all participants performed all tasks . 98864 @qwx958864 <p> Usability testing is generally well equipped to help design teams identify issues with a wide range of products and systems , frequently identifying interaction problems that were initially overlooked in even the simplest of proposed design solutions . In certain instances , however , usability testing falls short of producing results that are both reliable and valid . Of particular concern are studies of products in which users heavily interact with their own personal data , for example a user test of an email or calendaring program . In these cases , the fact that data are often fabricated for the test ( and are therefore @ @ @ @ @ @ @ @ @ @ affect users ' abilities to recognize , interpret , and interact with these data in an authentic manner . Thus , usability practitioners run the risk of identifying false positives ( for example , if users are completely confused by a screen because they 're unfamiliar with the data in it , when in the real world they would have been able to use their own familiar data to orient themselves ) or overlooking real problems ( for example , if users answer questions casually about their fake data , but would be much more concerned with the meaning of items when it applies to themselves ) . In this paper , we examine the strategy of incorporating users ' real data into usability testing to avoid these issues and increase the validity of a study . <h> Real Tasks , Real Users But What About Real Data ? <p> In the field of user experience design and research , we routinely speak of the importance of having real users perform real tasks in order to effectively evaluate the usability of our products . The following are just a few @ @ @ @ @ @ @ @ @ @ : " The basic rule for test tasks is that they should be chosen to be as representative as possible of the uses to which the system will eventually be put in the field . " <p> Dumas &amp; Redish ( 1999 , p. 174 ) : " The participants should feel as if the scenario matches what they would have to do and what they would know when they are doing that task in their actual jobs . " <p> What is rarely discussed , however , are the pieces of information or data that participants actually encounter when they interact with our prototypes . In most cases , these data are fictitious , made to resemble the average users ' data in the hopes of having participants successfully immerse themselves in the given scenarios . However , like the proverbial man with his head in the freezer and feet in the fire ( but who 's just fine , on average ) , having all the users interact with an average set of data does not mean that those data will be appropriate for any of them . @ @ @ @ @ @ @ @ @ @ is not a good test for anyone if nine in ten users typically have one stock sale in the list and one in ten has 100 . And even for those users for whom the fake data is typical , the fake data will at the very least be unfamiliar , and will require the participant to assume the role of someone else in order to understand and relate to their experience during the research study . <p> For many products and systems , this may not be a huge problem . For example , participants involved in testing an online retail or travel Web site are likely to be familiar with the scenarios and able to adapt to the data presented . So a task to purchase a particular book or arrange a trip to Miami may be a good test of the interface even if a user has never bought that book before or if the user has never flown to Miami . Data are not critical in these cases , and the user is likely to be able to relate the test scenario to their own life @ @ @ @ @ @ @ @ @ @ these types of tasks . <p> But for some products , the participants ' personal data represent an integral part of the product . For example , imagine testing an interface for setting photo sharing permissions or a system for online bill pay . In such cases , the particular photos that a user wants to share , and friends with whom they want to share them , or the number and timing of the bills to be paid will have a huge effect on how the user interacts with the interface . In these systems , testing with fake data means testing a fake user experience , one that 's potentially quite different from what users will have in the real world . <p> The way to get around this problem is to use the usability participant 's real data in the study , rather than data artificially created to simulate the real thing . In other words , rather than making up reasonable approximations of users ' banking transactions , annual income statements , or medical claim details , a researcher can find ways to incorporate users ' @ @ @ @ @ @ @ @ @ @ examples of research in psychology support the wisdom of this strategy . The first involves the " self-referencing effect " ( Greenwald &amp; Banaji , 1989 ) which suggests that information relevant to the self can be more easily encoded in memory and easily retrieved at a later date . Based on the self-referencing effect , we may hypothesize that by using artificial data rather than users ' real data in a usability test we may impede a user 's ability to perform as efficiently as he or she normally might . <p> A second line of research with similar implications for real data usability evaluations is related to the familiarity or novelty of content . This research has found that reactions to familiar vs. novel content are correlated with changes in different regions of the brain and may reflect different memory processes ( Tulving , Markowitsch , Craik , Habib , &amp; Houle , 1996 ) , again potentially having a negative impact on a user 's efficiency and effectiveness performance when completing desired tasks . <p> Finally , research has shown that a connection exists between personally relevant @ @ @ @ @ @ @ @ @ @ on the topic at hand ( Thomsen , Borgida , &amp; Levine , 1995 ) . In the context of usability studies , users ' real data may motivate users to engage with a product in a more authentic , genuine manner than with artificial data . In the experience of Intuit product teams , this increased level of engagement has produced more accurate and richer findings than otherwise collected with fictitious user data . <h> The Benefits of Real Data Testing <p> While incorporating users ' real data into your usability studies requires significant effort , there are some clear benefits that help make it worthwhile . <h> Ecological Validity <p> The primary benefit of incorporating users ' real data in a usability study is increased ecological validity ( Brewer , 2000 ) -that is , better approximating the real-life situation under study . And , by doing so , real data can also increase the study 's external validity-in other words , the study results are more likely to generalize beyond the lab . <p> For example , consider a real data study done on a product called @ @ @ @ @ @ @ @ @ @ 's Web site to allow the bank 's customers to do certain personal financial management tasks that are more commonly done within a desktop software such as Quicken ( for example , users could enter a check that had not yet cleared to account for that money in their balance ) . The team had run several fake data usability tests in which participants had no trouble with this concept , but in the real data test many were reluctant to even attempt the task . In this more realistic situation , participants assumed that there would be no way to do the task on their bank 's Web site , that they " would n't do that here . " This finding led the team to redesign to increase discoverability and to better educate first-time users about what the product could do . If the team had n't tested with real data , this problem would have gone undiscovered until after launch . <p> Another example is in the user testing of Intuit 's TurboTax tax preparation software . TurboTax has many screens that lead taxpayers through step-by-step explanations @ @ @ @ @ @ @ @ @ @ may not apply to them . For example , based on the amount of income input by the user , the TurboTax software determines if the user is eligible or not for specific tax credits . Based on that determination , the program shows users a certain set of screens that is relevant to that specific tax situation . If , during a usability study , the participant is using made-up data including a specific level of income , the participant will see a set of screens that go with that level of income . These screens may include situations triggered by the pre-determined income level that are not familiar to the participant because that was not his or her actual income level . Such a scenario potentially introduces another source of variability to the usability study , namely the difference between the usability of the software and the usability of the study task and non-real data . In the past , when TurboTax did non-real data usability testing ( which has its own advantages in terms of standardizing measurements of some program-related tasks such as navigation ) , teams @ @ @ @ @ @ @ @ @ @ questions in reference to their actual situations . When they tested with real data , they found that many of these questions were not clear to some users , and so the team rephrased and added explanations to make them easier for customers to understand . <h> Data Issues <p> The second benefit of real data testing is that by testing with a range of real-life data , we can uncover usability issues that would not be produced by a narrower set of representative fake data . For example , at Intuit a team had designed an interface for exporting data from an online payroll system to QuickBooks desktop accounting software . The export software was designed to cancel the export and display an error in certain ( we thought ) very rare exception cases , such as when an employee name in the online payroll system exactly matched something that was not an employee in QuickBooks , such as an item in the vendor list . When we usability tested this system with real data , we found that many users had added their employees to the wrong list @ @ @ @ @ @ @ @ @ @ When we discovered this , we made the matching rules more forgiving , allowing the export to go through in these cases . Without real data testing , we would not have discovered this issue until after launch . <p> In another very simple example , a team was designing a purchase process for a Web site . Instead of having participants enter artificial user data into a credit card field , the team asked users to enter in the actual name and expiration date from their company 's credit card . In one instance , a participant entered in her very long company name ( 35 characters ) that exceeded the field limitations of the design being tested . The most interesting thing was that this aspect of the design was already in production and was not even a priority of the test . By incorporating users ' real data , the team discovered an unknown and previously unencountered design flaw that was easily and immediately fixed . <p> - <p> - <p> - <p> - <p> - <h> Reduced Cognitive Load <p> Another advantage of real data testing @ @ @ @ @ @ @ @ @ @ artificial cognitive load of remembering a fake scenario or recognizing data invented for the purposes of the usability study . <p> The TurboTax team noticed this benefit when they conducted real data testing . Previously , many test participants had asked clarifying questions about the artificial scenarios or data to the extent that the team was beginning to wonder if the test results were reflecting real usability problems or were just a result of the participants ' confusion with the artificial data . When participants used their own data , they were completely immersed in preparing their taxes and did not ask any clarification questions about the scenarios . <p> The reduced cognitive load ( and perhaps also the greater realism ) led to an increased level of cognitive engagement in the interface than in previous studies . For example , when faced with a question about their income on one of the screens , participants made sure they understood the question and answered it correctly , fully aware that a wrong answer by them would lead to inaccuracies in their tax return . In contrast , in studies where @ @ @ @ @ @ @ @ @ @ make up answers and numbers and less motivated to be accurate . <p> So , rather than spending their time and mental energy focused on remembering artificial data , participants in a real data study are more focused on what they 're doing , which is presumably their focus in the real world . <h> Greater Emotional Engagement <p> The final benefit to real data testing is that it makes participants more emotionally engaged in the situation . This often results in a better understanding of the product 's strengths and weaknesses , and richer feedback to the design team . For example , when doing their own taxes , participants were very focused on exactly how much money they were going to get back or have to pay . TurboTax has a refund counter in the upper left of every page that shows the user 's current refund or tax owed . Based on this feedback , the team decided to enhance the refund counter and make it a larger part of the interface . <p> The Quicken Health team , in their real data study of their product @ @ @ @ @ @ @ @ @ @ highly sensitive areas such as insurance claim denials and adjustments . The negative impact associated with these topics ( e.g. , unexpected fees ) was heightened for participants because it was their own medical service that was being denied . This feedback was given to the design team , who considered the implications of this finding for the visual design and language around notification of claim denial . <h> The Challenges of Real Data Testing <p> Incorporating users ' real data into a usability study also poses a number of challenges . Based on the authors ' experiences across multiple product teams at Intuit , there are three primary challenges to incorporating users ' real data into a usability study . <p> The remainder of this paper examines each challenge in turn , drawing on sample projects across product teams . <h> Recruiting Participants and Getting Their Data <p> In order to conduct a real data study , one must first determine how to obtain participants ' real data . Depending on the nature of the product , this can be as straightforward as asking recruited participants to bring their @ @ @ @ @ @ @ @ @ @ partnering with a third party to extract user data files from multiple backend systems prior to the study , followed by mapping and uploading those data into the prototype to be tested . The following are some methods that the authors have used for obtaining real data and the situations where researchers might use them : <p> Have participants login to their own account ( for online applications already in use ) during the study . <p> Have the participants bring their personal data with them to the session in paper or electronic format ( for applications centered around data entry , like TurboTax ) . <p> Have users send in their data in advance of the study so that the researchers can put it into the prototype ( for applications with an easily transferable data file , like QuickBooks , or set of paper data , like Quicken Health ) . <p> Work with a partner organization to collect the data on the backend , then map and upload these data into the prototype ( for hosted applications in which the developer of the application is not the customer @ @ @ @ @ @ @ @ @ @ methods vary widely in difficulty , but each has at least some issues . Having participants login to their own account or bring their own data to the study are logistically quite simple , but involve some recruiting and attrition challenges . People may be reluctant to share their details with others , especially when the product being tested involves sensitive information such as personal financial or medical information . Thus , recruiting becomes more difficult as recruiters have to approach a larger number of people before they acquire a sufficient pool of participants , and the recruitment costs for the study will typically increase . <p> Depending on the application , people may also have trouble finding or collecting the data to bring with them . In the case of studies done with Intuit TurboTax , this was not as much of a problem as it might have been , because even though the program requires user data from multiple sources , the users were already in the habit of pulling their tax data together to either complete their own taxes or to share them with an accountant . @ @ @ @ @ @ @ @ @ @ for Intuit 's payroll software , could not be done in the lab simply because too many participants failed to bring everything they needed and instead had to be done in participants ' offices . <p> Recruiting participants to send their personal data to the research team in advance of the study can prove to be somewhat more challenging than having participants bring their personal data to each session . This approach takes participants ' privacy and security concerns to a new level . With the growing fears of identity theft and phishing scams , people are increasingly wary of sending their information to someone they do not personally know . As a result , research teams need to take extra care and precautions to ensure the security of data transmission and handling ( see the Security and Privacy section ) as well as reassure participants that their information is secure . Another challenge of this method is incorporating the data into the usability study prototype . In a real data study evaluating Quicken Health , participants were asked to mail in their data in advance of the study . @ @ @ @ @ @ @ @ @ @ the prototype to be tested . Additional time was needed to allow for engagement from the Quality Assurance ( QA ) team who helped with ensuring the quality of the data entry prior to testing . <p> In the case of obtaining participant data ahead of the study via a third party partner , the logistical issues grow exponentially . Extra steps required by this strategy include convincing the third party of the cost and benefits associated with using real data , informing potential participants of the real data process and getting their permission to obtain their information , setting security and privacy procedures with the partnering group , and so on . In the case of the FinanceWorks project at Intuit , the application was intended to operate from within a financial institution 's Web site , so the most useful method for obtaining customer 's real data was through the customer 's bank . Finding a bank willing to participate in this test , and then overcoming the legal and organizational hurdles to obtain permission both within Intuit and within the bank to run this study , took @ @ @ @ @ @ @ @ @ @ using the customer 's bank data in the tested prototype made the time spent well worthwhile ( see the Benefits sections ) and the success of the study established procedures and a relationship that opened the door for more real data testing , both with that financial institution and others . <h> Security and Privacy <p> With fictitious data , the concerns about security and privacy of information are usually quite minimal and generally related to permissions for videotaping and subsequent sharing of results . In contrast , when you incorporate users ' real data , you may quickly find yourself dealing with institutional security policies and highly sensitive privacy issues that need to be considered with care . <p> In all the real data studies done at Intuit , extra efforts regarding security and privacy are taken to ensure that participants ' data are not at risk . For the FinanceWorks and Quicken Health studies , these efforts included file encryption to ensure that users ' data was protected at all times and the destruction of all data files within a four week period following the study . Explicitly @ @ @ @ @ @ @ @ @ @ be used in the study was highly effective in setting expectations with participants and ensuring them that the security and privacy of their data was a priority for the research team . <p> Efforts also have to be put in to place to handle any recordings of a real data study . In a real data study on the Intuit FinanceWorks product , the sessions were recorded using Morae . These recordings , in addition to the participants ' banking data , had to be encrypted . The researchers kept records of anyone who had access to or viewed these recordings . In a few cases , the researchers asked for additional consent from the participants after the test was completed to show video clips to a wider audience . Only those video clips for which additional consent was obtained could be shown . <h> Scenarios , Tasks , and Data Analysis <p> With fake data , it 's possible to define the successful path for each task as the same across all participants and to analyze the data accordingly . With real data , things are n't simple . @ @ @ @ @ @ @ @ @ @ different amounts and types of data ; in some cases , the task may not apply at all or may need to be done multiple times . There is no simple solution to this problem . The best way to analyze the data will depend on your particular application and the questions you 're trying to answer . However , there are two techniques that we 've used in multiple real data studies that have been helpful . <p> One is to construct task templates , generic versions of a task that are then filled-in with the specific data for each participant . For example , a task template for a study on an electronic billpay application might be , " Pay your bill that is due on . " The blanks would then be filled in differently for each participant before their test session , based on the data that we had received , and with an attempt to make the data in the blanks relatively similar across participants ( e.g. , selecting bills of similar amounts and due dates as much as possible ) . Thus there is @ @ @ @ @ @ @ @ @ @ paying a bill that is specific and familiar to them . <p> Another technique that is useful specifically for analyzing the data is to construct average scores for each participant in a given task before calculating success metrics across participants . So for example , if a task is to pay all bills that are due in the next week , in a real data study one participant might have five bills due and another might have only one bill . To avoid overweighting the participants with multiple bills , the best way to create an overall score for task success at paying a bill is usually to construct a composite score for each participant , and then average those composite scores . <h> Conclusion <p> In this paper , we have argued that using real data in usability testing is one way to reduce the artificial nature of lab usability testing and to increase the validity of the study results . Accompanying the benefits , however , the use of participants ' real data introduces several methodological and logistic challenges . Our discussion of these challenges centered around recruiting @ @ @ @ @ @ @ @ @ @ , and addressing security and privacy issues associated with users ' real data . <h> Acknowledgements <p> The authors would like to thank Kari Sortland , Sara Cole , and Matt Berent for contributing to the arguments and examples in this paper . 
@@105507085 @907085/ <h> JUS <h> This Issue <h> A Modified Delphi Approach to a New Card Sorting Methodology <p> Date : <p> November , 2008 <p> Volume : <p> 4 <p> Issue : <p> 1 <h> Abstract <p> Open card sorting is used by information architects to gather insights from users to incorporate feedback into an information architecture . In theory , it is one of the more inexpensive , user-centered design methods available to practitioners , but hidden costs make it less likely to be conducted properly and affect the quality of results produced . The following proposes a new card sorting method called the Modified-Delphi card sort to replace the Open card sort . The Modified-Delphi card sort is based on a well-known forecasting technique called the Delphi method . Instead of producing individual models that are then analyzed as a whole , participants work with a single model that is proposed and modified throughout the study . The Modified-Delphi card sorting method produces more useful results to aid in the design of an information architecture than the Open card sorting method . <p> A series of studies were @ @ @ @ @ @ @ @ @ @ methods . First , two parallel studies using both methods were conducted with the same dataset and number of participants . Then , two studies were conducted on the results of the parallel studies : a heuristic review and ranking with information design experts and an Inverse card sort with additional users of the proposed architecture . The Modified-Delphi card sorting method produced results that were found to be at least as good as the Open card sorting method results and in some cases , better . 98865 @qwx958865 <p> The Modified-Delphi card sort is an exciting new method that promises to replace Open card sorting as a pre-design method in information architecture . The following are some of the discussed benefits learned by this study : <p> Get better results for feedback in to the design of an information architecture . <p> Save time in laboratory studies by reducing the number of participants in a study and the amount of data to analyze . <p> Possibly save money in laboratory studies by using fewer participants , fewer days of facilities costs , and fewer hours of analysis time . @ @ @ @ @ @ @ @ @ @ results . <p> A digital camera can help save time in recording data between tightly scheduled participant sessions. 98864 @qwx958864 <p> Card sorting is a participatory , user-centered design activity that information architects use to gain an understanding of how users understand and model information ( Maurer et al. , 2004 ) . This method is used to draw out underlying mental models ( Nielsen et al. , 1995 ; Rosenfeld et al. , 2002 ) that will later aid in the design or validation of an information architecture . A participant in a card sorting study is given a set of cards that each contains a piece of information . The participant sorts the cards into groups and labels each group . These results are then analyzed against a hypothesis by cluster analysis , by affinity mapping , or by simple pattern matching . Specifications on the methodology and what the researcher does with the results depend on the type of card sort conducted . Studies may be conducted in a laboratory setting with a stack of note cards and a table , on a computer in a laboratory @ @ @ @ @ @ @ @ @ @ or over the Internet ( OptimalSort , 2007 ; Socratic Technologies , 2007 ; WebCAT , 2007 ; WebSort , 2007 ) . <p> There are a number of card sorting methods that have been used as research tools by psychologists and information designers ; however , there are two types of card sorts that are used at different stages in the design of an information architecture : pre-design and post-design methods . Pre-design methods are used early in the design process to gather input for creating an information architecture . Post-design methods are used after an information architecture is developed to validate or edit an existing architecture . <p> The Open card sort is a pre-design method where participants sort cards into categories they create themselves . It is one of the earliest design methods information architects employ to aid in creating an information architecture . Participants have very few restrictions on how they can work with the cards ; they can rename cards with better labels , add or remove cards from the final structure , or place the same card in multiple places . This freedom makes @ @ @ @ @ @ @ @ @ @ underlying mental model of the participants . A number of methods for analyzing the results of Open card sorts , with some of the more common metrics being cluster analysis-analyzing the relationship of a card to a category and the relationship of a card with another card ( Rosenfeld et al. , 2002 ; Toro , 2006 ; Tullis et al. , 2004 ) . <p> The Closed card sort is a post-design method where participants sort cards into preexisting categories . Participants do not have as much freedom with the cards as in an Open card sort and must use the categories and labels provided to them by the study administrator . This method can be used in two ways : to add new content to an existing information architecture or to test an information architecture by scoring participant results with the existing structure . <p> The Inverse card sort , also known as reverse card lookup , is another post-design method which is a variation of the Closed card sort . The top levels of the information architecture are provided to the participants , and they are asked @ @ @ @ @ @ @ @ @ @ based on task- or topic-based scenarios ( Classified , 2007 ; Scholtz et al. , 1998 ) . This is a useful method to quantitatively validate or rate an information architecture , similar to administering a quiz or test . <p> A card sort offers a number of benefits that make it attractive in the practice of information architecture ( Maurer et al. , 2004 ) . It is widely practiced and effective for gathering user insights for an information architecture . Card sorting is also simple to conduct and relatively cheap , with the cost of materials being low compared to other user-centered design study methods . However , the method has several weaknesses , some of which negate the proposed benefits . First , the activity of organizing cards is out of the context of the user 's goals and tasks . Organizing a set of data in a laboratory session is much different than wayfinding on a live website . Second , consistency between participants may vary , especially when a dataset has multiple possible organization schemes . Lack of consistency can weaken category and card relationship @ @ @ @ @ @ @ @ @ @ with larger numbers of participants to gather more data . <p> The question of how many participants to include in a card sorting study is under debate , particularly with regard to Open card sorting . Some card sorting guides suggest as few as four to six participants ( Gaffney , 2000 ; Robertson , 2002 ) , others suggest 10 to 15 participants ( Maurer et al. , 2004 ; Nielsen , 2004 ) , while others suggest as many as 20 or more ( McGovern , 2002 ; Tullis et al. , 2004 ) . Tullis and Wood ( 2004 ) have noted a minimum number of 20 to 30 participants are necessary to get meaningful numbers from an Open card sorting study . More participants may help provide more consistent results , but the larger sample size also increases costs and analysis time . In practice , even 10 to 15 participants is a high number of participants for a study . Informal polls conducted at the 2007 Information Architecture Summit and local Usability Professionals Association meetings revealed that many practitioners were conducting card sorting studies with @ @ @ @ @ @ @ @ @ @ participants is practical for practitioners for a number of reasons . A large number of participants mean higher costs of additional participant stipends , moderator fees , facility costs , and analysis hours . For companies who do not have their own facilities , the cost of renting a lab is a significant part of the study costs . Limiting the number of participants in a study to a number that can be scheduled in one day could reduce some of these costs . However , without conducting an Open card sorting study with an adequate number of participants , results from the study may not be reliable or provide the quality of input necessary for designing an information architecture . <p> A strong need exists for a more reliable and less expensive card sorting method that information architects can use early in the design process . This method must have strengths in three areas : results , time , and cost . It must be easy to conduct and not require many participants or a long study period . It must provide results that are both useful and worth @ @ @ @ @ @ @ @ @ @ . It must be low in cost so it can be easily funded and justified . <p> As previously mentioned , there are a number of web-based card sorting tools and services available to conduct an online card sort . OptimalSort ( 2007 ) supports closed card sorting . WebCat ( 2007 ) , Soctratic Online Card Sort ( Socratic Technologies , 2007 ) , and WebSort ( 2007 ) support both open and close card sorting . Web-based card sorting has alleviated some of the expense of conducting an open card sort by turning to the web instead of a laboratory . Because participants can participate online instead of commuting to the testing facility , there are no facility rental fees and participation stipends can be lower . This cost savings can be invested in recruiting more participants to reach the 20 to 30 participant range recommended by Tullis and Wood ( 2004 ) . However , the quality of the results gathered from an Open card sort is still in question . <p> The goal of this research was to develop a new card sorting method that provides @ @ @ @ @ @ @ @ @ @ card sorting . I propose a new methodology similar to the Open card sort that is based on a forecasting technique called the Delphi method . The Delphi method is a moderation process for allowing multiple participants to work together towards a solution , while minimizing collaboration bias . Some of these biases include the bandwagon effect , which is the believing in a certain position because others do ( Nadeau et al. , 1993 ) ; herd behavior , which is a defense mechanism that results in following of the crowd ( Hamilton , 1971 ) ; or the dominance of a strong personality on a group ( Boy , 1997 ) . Instead of producing a number of models created by individual participants that are then combined , averaged , and analyzed to draw a final conclusion , this new method allows participants to work individually on a single model until that model can be accepted without major additional modifications . While Open card sorting records multiple mental models and tries to draw conclusions from the results by analyzing statistical characteristics , Modified-Delphi card sorting proposes a single @ @ @ @ @ @ @ @ @ @ met . <h> Current and Related Work <p> In Wisdom of Crowds ( 2004 ) , Surowiecki discusses how , when accessed collectively , the masses have an intelligence that is rival to none . Four elements are necessary to form a " wise crowd " : diversity of opinion , independence from other people 's opinion , decentralization of knowledge , and a method for gathering the crowd . As an example of the power of collective knowledge , he discusses several kinds of markets , including prediction markets that rely strongly on expressing a position , rather than selecting a choice . According to Wikipedia ( 2007 ) , " A prediction market would ask a question such as , ' Who do you think will win the election ? ' where an opinion poll would ask more specifically , ' Who will you vote for ? ' " <p> Similar to prediction markets , the Delphi method is a forecasting technique used to collect the opinions of a group of people in an objective way . It was developed by the RAND Corporation in the 1950 's @ @ @ @ @ @ @ @ @ @ 1975 ) as a way of gathering a knowledge base of military intelligence and experience without the influence of politics , rank , or other bias . It has since been applied to other domains such as technology , population sciences , and business . <p> The Delphi method is a technique that controls information gathered from participants through the study moderator . There are three key elements to the Delphi method : structure of information flow , feedback to the participants , and anonymity of the participants . Participants are often knowledgeable experts in a field and may have a personal stake in the resulting knowledge base generated from the study . This technique is similar to the Hegelian Principle of thesis , antithesis , and synthesis , where an argument and counter-argument are proposed and discussed until a consensus is reached ( Stuter , 1996 ) . <p> During the Delphi method , each participant was given the following information : <p> Asked to provide an answer to a problem or question <p> Given the combined work of previous participants <p> Allowed to modify their answer after review @ @ @ @ @ @ @ @ @ @ ( the Delphi method ) <p> The moderator combines the previous session results and presents the material to the participants , careful to filter any bias or include personal opinion or experience that may be important . This direct interaction of the moderator is an important communication channel between the participants ; however , it has also been noted as one of the weaknesses in the protocol ( Teijlingen et al. , 2005 ) . Because the moderator has control over the collection and combination of information , they should maintain an objective view of the information and remain neutral on any presented positions . Conflict of interest may arise due to possible personal or business gains based on the results . To avoid this potential problem , unbiased , third-party moderators may be used for critical evaluations . Independent verification and validation is one such method used in the development of mission critical systems ( Whitmore et al. , 1997 ) . <p> The Delphi method has benefits over other group communication methods . Instead of direct collaboration , participants interact with each other through the artifacts they leave @ @ @ @ @ @ @ @ @ @ participant may not have the final answer , but a piece of what they have proposed may be valuable to the next participant . The opinions of others can be influential , valuable , insightful , or useless in a certain context . This anonymous collaboration alleviates peer pressure and other performance anxieties that are common to group collaboration methods and allows participants to focus on the problem . <h> Delphi in user-centered design techniques <p> There are a number of methods currently employed in the practice of user-centered design that have been influenced by or are fundamentally similar to the Delphi technique . Although not strictly limited to user-centered design , the Delphi method of interviewing ( Boucher et al. , 1972 ) is a protocol designed to gather information from multiple experts while limiting the influence or bias of any one expert . For example , when researching user groups of a product , an iterative interviewing model would involve members of the client company who are involved in the development of a product ( see Figure 2 ) . A participant is asked to create a list @ @ @ @ @ @ @ @ @ @ product . Once the participant provides his or her answer , he or she is provided with the combined list of the previous participants answers . The participant is then allowed to modify his or her answer based on this new information . What usually results from this review is one of three things : the participant included a user group that was not previously listed , the participant did not consider a user group from the combined list that he or she thinks is valid , or the participant will notice a combination of groups that include the same information . The participant 's results are combined with the current list by the moderator for use with the next participant . This interviewing process is continued until a consensus has been reached or obvious patterns of conflict and agreement have been identified . <p> Figure 2 . Iterative moderating model <p> Iterative interface design is a development strategy used in user-centered design . This strategy often includes frequent usability testing sessions with a small number of participants over the development of a product ( Krug , 2000 ) . @ @ @ @ @ @ @ @ @ @ incorporated into the testing protocol as a method for gathering feedback and insight ( see Figure 3 ) . Participants are given the prototype design to work with and are asked to provide feedback . They are then presented with alternate design ideas that have been created based on feedback from previous participants and allowed to provide additional feedback , particularly if one design is better than another . Because the goals of the testing sessions are to gather design feedback , and not to validate the prototype design , this method is valuable for trying out design ideas developed during the study . <p> Figure 3 . Linear moderating model <h> Other group collaboration design methods <p> Focus groups are a way to gather a group of people to talk about a topic . The number of people can range from a few to up to 15 , and last from a single one hour session to multiple day sessions . Although good for gathering product experience and preference information ( Ede , 1998 ) , focus groups have a number of drawbacks that make them less desirable as @ @ @ @ @ @ @ @ @ @ It is difficult to understand how participants may use a system without actually observing them using it ( Nielsen , 1997 ) . If asked how they would use a system , what participants may do is much different than what they say they might do . <p> The Collaborative Analysis of Requirements and Design ( CARD ) technique is a task-based game-like procedure that helps direct the design of a task or work flow ( Muller , 2001 ; Tudor et al. , 1993 ) . First , the participants are introduced to each other and examine the materials they will be working with before they begin the work session . The work session consists of three parts : the analysis session , where participants describe their work ; the design session , where participants work with their own work and incorporate other participant 's ideas into their work ; and the evaluation session , where participants comment on each others ' work . It is similar to card sorting in which the cards are used to draw out a participant 's model , but different in the participants @ @ @ @ @ @ @ @ @ @ not topics or labels . <p> Plastic Interface for Collaborative Technology Initiatives through Video Exploration ( PICTIVE ) is a participatory design technique that allows users to modify a proposed interface as a method for gathering design input and feedback ( Muller , 1991 ) . It is similar to paper prototyping where it provides pieces of the interface for participants to interact with ; however , in PICTIVE , the participants are engaged in designing the interface instead of interacting with it . Each participant is given a goal , similar to a job or task scenario . A group of participants interact with pieces of the interface and each other to create an interface that meets all of their goals . <p> The Group Elicitation Method ( GEM ) is a brainstorming technique that provides a decision support system for group design and evaluation ( Boy , 1997 ) . During a half day session , 6 to 10 domain experts brainstorm and collaborate to reach a consensus on an argument . The moderated collaboration model aims to reduce bias often found in other group collaboration methods . @ @ @ @ @ @ @ @ @ @ formulation of problem and selection of participants , ( b ) generation of point of view , ( c ) reformulation of points of view in to concepts , ( d ) generation of relationships between concepts , ( e ) derivation of a consensus via computer scoring , ( f ) analysis of results . It is suggested as a plausible replacement for interviewing , card sorts , or Twenty Questions ( Tognazzini , 1992 ) . <h> The Modified-Delphi card sorting method <p> There are a number of methods that utilize collaboration and iterative information flow in order to gather knowledge ( Boucher et al. , 1972 ; Boy , 1997 ; Ede , 1998 ; Krug , 2000 ; Muller , 1991 ; Tudor et al. , 1993 ) . Of these , the Delphi method is the best method for gathering knowledge from a group of experts . By modifying this method and applying it to card sorting , we can take advantage of its structured information flow that is suitable for linear studies , such as card sorting , and minimizes the bias found in @ @ @ @ @ @ @ @ @ @ for use in card sorting , each participant was given the following information : <p> Given the combined work of previous participants <p> Allowed to modify the work after review <p> The step that asks participants to first provide their answer is omitted for several reasons . Card sorting is a cognitively intense activity . Requiring participants to participate in an open card sort , review work that may be similar or different from their own , and then modify the combined participants ' answers is a lot of work , and may discourage them from making changes to the presented structure and becoming active in the design process . The traditional application of the Delphi method usually involves knowledge experts who have a personal stake in the information and are willing to put forth the necessary amount of work to sufficiently get their point across . Participants recruited for user research studies , including those that would be recruited for the Modified-Delphi card sort , have no personal connections with the product or company . The work they are modifying is that of their peers , therefore there is @ @ @ @ @ @ @ @ @ @ agree , criticize , or make changes ( see Figure 4 ) . <p> The Modified-Delphi card sort can be summarized in the following four steps : <p> The seed participant creates the initial structure from a stack of cards and proposes an information structure model . <p> The following participants comment on the previous participant 's model and make modifications to the proposed model or propose a new model . <p> The card structure changes throughout the study , evolving into a model that incorporates input from all of the participants . <p> A consensus is reached when the information structure stabilizes and there are no more significant changes or obvious patterns of conflict and agreement arise . <p> Figure 4 . Example workflow of a Modified-Delphi card sorting study <p> The seed participant can be selected in a number of ways . The seed participants can be a single participant , a group of participants collaborating , an information architect aiding a participant , or an information architect . A single participant working alone is similar to a participant in an Open card sorting study . Groups of @ @ @ @ @ @ @ @ @ @ and a pair of participants may be a good way to begin a study with a dataset that is difficult to classify or contains information new or unknown to participants . However , a pair of participants working together should only be used as the seed participant . Although the use of groups of participants in card sorting studies is common , using them in applications of the Delphi method is rare . The use or assistance of an information architect may be useful , but should be considered as a last alternative , when information in the dataset is new to participants and the first participant may have a difficult time with organization . <p> The goal of a participatory design method is to gain insight from users of a system , not the designers . Introducing a model influenced by the information architect transforms the method from an information gathering technique to an information validating technique . Using an information architect in the study also presents a social issue . An information architect-because of his or her professional status , knowledge of the information , relationship with the @ @ @ @ @ @ @ @ @ @ higher regard than a peer , and knowledge of an information architect 's influence may intimidate participants from altering the information structure model . Additionally , participants should never be told how many previous participants have worked on the information structure , because the number of participants may be intimidating and prevent a participant from being comfortable making changes . <p> Recruiting for the Modified-Delphi method is similar to recruiting for any other user-centered design study . The Delphi method is traditionally a method of expert opinion , and the users of a product could be considered an " expert " on the product . Approximately 8 to 10 participants have been the typical number of experts recruited in traditional Delphi studies . Depending on the goals or needs of the product design , you may recruit participants who are the target users and mixed user types , a single user group of particular interest , or the primary user group . However , if the participant types are very different and propose very different models to work with , this instability may prevent the study from reaching a consensus @ @ @ @ @ @ @ @ @ @ be the existence of conflict cards . These are topic cards that do not stabilize in a category and participants can not reach a consensus by the end of the study . There are a number of reasons why there may be conflict cards including : <p> A topic card 's label is incorrect , misleading , or ambiguous . <p> The categories that participants are presented with do not capture a location suitable for a specific topic card , but fit with the rest of the data and so is left unchanged . <p> Multiple user groups may have different opinions on where a specific topic card should be . <p> Patterns of conflict may be identified during the study or during analysis . It is useful to look back at participants ' comments from the study sessions to see if there were any indications as to why a topic card may have had issues . Conflict cards are not necessarily a bad thing ; they identify weak points in the information dataset so the information architect can pay special attention to them when designing the information architecture . @ @ @ @ @ @ @ @ @ @ sort using affinity mapping or another pattern matching technique ; however , special attention should be paid to the final participant 's work . The final participant has had the influence of all the previous participants and should have had the fewest significant changes . In a model study , their work will be very similar to the final results analysis and can be used as a preliminary result or a metric to compare against the final analysis . <p> The goal of the Delphi method is to reach a consensus in a body of knowledge . In information architecture , there is rarely a single correct answer , but usually a few suitable answers that will accommodate most of the audience . If a proposed model is not agreeable to a participant , the participant is free to propose a new model by scooping up the cards and beginning from scratch . During a Modified-Delphi card sorting study , it is possible that many models are proposed and no consensus can be reached ; in that case , variables such as homogeny of the dataset , participant experience with @ @ @ @ @ @ @ @ @ @ <p> There may be several logical structures for the information , and it is up to the information architect to choose which one is the most appropriate . In this case , the Modified-Delphi card sort may be too relaxed an information gathering tool , and more directed studies after a model is selected may be more useful . The Modified-Delphi card sort is meant to be a more practical pre-design activity to aid in the design of an information architecture . Improving the quality of results from each participant , reducing the time to conduct the study and analyze the results , and lowering the costs of conducting a study and possibly cognitive costs to the participants are all potential and expected benefits . <h> Methodology <p> Using the Modified-Delphi card sort may reduce the financial and cognitive costs of a study . I have investigated if the results from this method are at least as good as results generated from Open card sorting study . If the results can not provide the same value to the researcher , the cost benefit is lost . <p> This brings me @ @ @ @ @ @ @ @ @ @ the results generated by the Modified-Delphi card sort and Open card sort . <p> The Modified-Delphi card sort generates better results than the Open card sort for aiding in the design of an information architecture . <p> In the approach I have chosen to answer my research question , I have directly compared the Modified-Delphi card sort and Open card sort by conducting parallel studies in a laboratory environment using the same user configuration and card collection to generate result structures . I use the term information structure , rather than information architecture , because these are generated from the results of the studies without any modifications based on heuristics , logic , or experience . The information structures are a representation of the results without the assistance of an expert . Two independent studies were conducted : ( a ) an expert heuristic review and ranking by information design experts ( b ) and an Inverse card sort with the website 's target user groups . The results of these studies have provided data for directly comparing the Modified-Delphi card sort versus the Open card sort . <p> The @ @ @ @ @ @ @ @ @ @ a new website design for the University of Baltimore School of Law in the Fall of 2006 . Results from the usability study revealed issues relating to the information architecture , and consultation with an information architect was recommended ( Nomiyama , Abela , &amp; Summers , unpublished ) . This presented an opportunity to conduct parallel studies using the Modified-Delphi card sort and Open card sort for direct comparison and to test the new method with a real-world problem . Using a dataset such as weather or food may not have produced realistic results . These kinds of information have strong preexisting social constructs that define how they are categorized , conventions that are learned early and are hard to break . <h> Modified-Delphi and Open card sorting studies <p> Eighteen participants of the University of Baltimore School of Law website 's target user groups were recruited to participate in one of two card sorting studies . These user groups included : law students , both current law students and undergraduate students interested in law school ; law school staff , which included administration support staff , law professors @ @ @ @ @ @ @ @ @ @ attorneys who may or may not be university alumni and support specialists such as paralegals , records managers , and so on . Students and staff are the most frequent users of the website , but it was important to support professionals seeking information about clinics and seminars , especially alumni who are interested in donating to the school . The Modified-Delphi card sort was conducted with eight participants : one undergraduate interested in law school , two current law school students , one school administrator , one faculty member , two law professionals , and one attorney . The Open card sort was conducted with 10 participants : one undergraduate interested in law school , three current law school students , one school administrator , two faculty members , two law professionals , and one attorney . <p> Sessions for both the Modified-Delphi and Open card sorting studies lasted no longer than 60 minutes per participant . Before the session began , all participants were asked to complete a study consent form and answer additional background information questions about their experience using law school websites . <p> Participants of @ @ @ @ @ @ @ @ @ @ , a pen , and extra cards to use for naming groups , renaming card titles , or adding missing content . All of the Open card sort participants and Participant 1 of the Modified-Delphi card sorting study were given a set of 90 index cards containing high level topics from the current University of Baltimore School of Law website ( http : //law.ubalt.edu/ ) . Participants 2 through 8 of the Modified-Delphi study were given the previous participant 's results to work with . Participants of both studies were asked to create an organization for the content provided that made the most sense to them . They were permitted to change labels , remove cards that did not seem to fit , and add missing information . It should be noted that 90 cards were not enough to represent all of the content in the website and comments about missing content were expected . <p> There were two major differences in the instructions given to the participants of the Modified-Delphi card sort versus the Open card sort : <p> Modified-Delphi card sort participants were asked to review the work @ @ @ @ @ @ @ @ @ @ began with an empty table and a stack of cards . <p> Each Open card sort participant began with the original set of 90 cards while the Modified-Delphi card sort participants began with a modified collection based on the changes previous participants made . <p> After the participants of both studies were satisfied with their work and declared themselves to be finished , a review of their work was conducted to clarify grouping and labeling decisions . A final questionnaire was administered that asked participants to select 10 of the most important topics from the original set of 90 information cards . The results of the questionnaire helped formulate questions for the Inverse card sort and to ensure that the possibility that unfamiliar cards were presented to participants of the Modified-Delphi card sorting study did not affect the final comparison of the two methods . Results of each participant session were recorded by taking a series of digital photographs of the participant 's work so they could be recorded and analyzed at a later time . <h> Generating information structures from card sorting study results <p> Once the two studies @ @ @ @ @ @ @ @ @ @ guidelines that guided objective creation and limited expert interaction . The same guidelines and analysis methods were used for the results of the Modified-Delphi and Open card sorting studies . <p> The results were recorded in a spreadsheet , similar to the popular spreadsheet template published by Joe Lamantia ( 2003 ) . For each card sorting study , a separate spreadsheet was created to record the results . Categories generated in a study were given a separate column and similar categories labels were combined . For each participant , the card topics were listed in the category it was placed by the participant . When the card topics from all of the participant sessions were placed , an agreement weight ( see Formula 1 ) was calculated for each card in each category . The agreement weight for a card topic is calculated by dividing the number of occurrences of a single card topic in a category by the total number of topic cards . <p> An agreement weight is a way to describe the strength of a card in a single category . This calculation is used instead @ @ @ @ @ @ @ @ @ @ multiple variables . For example , a correlation would be used to find the relationship of a card between two specific categories ( two variables ) . The agreement weight finds the single strongest category ( one variable ) for a card . <p> Formula 1 . Calculating agreement weight <p> Result structures were created by organizing cards with greater than 50% agreement weight into categories in the final information structure . Greater than 50% agreement weight means that more than half of the participants agreed on the location of the card . These high percentage agreement cards also helped determine the strongest categories in the information structure . Cards with less than or equal to 50% agreement weight were organized based on their highest agreement . Additional categories were created as cards with lower agreement percentage that did not fit in to existing categories were added as needed . Agreement weight ties between categories were decided based on one of the following heuristics , in order of importance : <p> If the category to be organized in was created based on inclusion of cards with greater than 50% agreement @ @ @ @ @ @ @ @ @ @ selection of a category would result in the creation of a new category that was not popular with the results , that choice was omitted . <p> If the organization of a card in a specific category was obviously illogical , that choice was omitted . <p> The third heuristic was rarely relied on because of its need for expert opinion , but necessary in order to prevent an anomaly that may affect the later studies . The goal of these guidelines was to create information structures that explicitly represented the results of the card sorting study and not the expertise or opinion of the analyst . <h> Expert review <p> Fifteen information design experts were recruited to participate in a heuristic evaluation and ranking of the resulting information structures from the Modified-Delphi and Open card sorting studies . On average , the participants had 3 or more years of professional experience as an information expert and spent most of their time at work on information architecture-related activities . The study was conducted online via a browser-based form that provided instructions and a method for answering a series of questions @ @ @ @ @ @ @ @ @ @ goals of the website and a downloadable copy of the results of the Modified-Delphi and Open card sorting studies ( Appendix 1 , Appendix 2 ) . They were then asked to provide a score from 1 ( very poor ) to 5 ( very good ) for a series of information architecture heuristics based on industry best practices and Rosenfeld 's ( 2004 ) information architecture heuristics : <p> Breadth and depth are balanced . <p> Labels are clear and meaningful . <p> Data is of similar granularity and dimension . <p> Naming scheme is consistent and logical . <p> Visual hierarchy is clear . <p> Organization fits users ' needs . <p> In addition to the heuristics , the participants were asked for their overall impression of the information structure and asked to rank the two information structures . This was a within subject study with all the participants rating both information structures from the Modified-Delphi and Open card sorting studies . The information structures were anonymized and counterbalanced to prevent bias . For half of the expert reviewers , the Modified-Delphi information structure was labeled as Information @ @ @ @ @ @ @ @ @ @ structure was labeled as Information Structure B and presented second . For the other half of the expert reviewers , the Open information structure was labeled as Information Structure A and presented first , and the Modified-Delphi information structure was labeled as Information Structure B and presented second . Participants were informed that the two information structures were generated from card sorting studies , but not that the card sorting studies employed different methodologies . <p> After both of the information structures were reviewed , the participants were asked to rank which structure was better , or if they were the same . <h> Inverse card sort <p> Seven participants were recruited for an Inverse card sort of the resulting information structures from the Modified-Delphi and Open card sorting studies . The recruiting was based on the same criteria as the card sorting studies : four current law students , two law professionals , and one undergraduate pre-law student . The study was conducted online via a browser-based form that provided instructions and a method for answering a series of questions . Participants were asked to select the category where @ @ @ @ @ @ @ @ @ @ . The questions were derived from the results of the exit questionnaire administered during the Modified-Delphi and Open card sorting studies . The categories were the top level categories from the information structures generated from the Modified-Delphi and Open card sorting studies . <p> The Inverse card sort was also a within subject study , with all the participants answering the same set of questions for the anonymized Modified-Delphi and Open information structures . The order in which the questions appeared was counterbalanced to prevent a learning bias . Half of the participants were asked to answer questions for the Modified-Delphi information structure first and the Open information structure second , and the other half of the participants were asked to answer questions in the reverse order . <h> Information structures <p> The information structures generated were based on guidelines specified in the Methodology section . Guidelines for the Modified-Delphi and Open card sorting studies can be found in Appendix 1 and Appendix 2 , respectively . <p> In the results from the Modified-Delphi card sorting study , 66 out of 90 of the original set of cards had a @ @ @ @ @ @ @ @ @ @ ) . More than half of the participants agreed on the location of 73% of the cards . Also , nine out of the final 10 categories were represented in these cards . The final participant 's work was also very similar to the combined information structure . Eight out of the final 10 categories were represented by participant 's raw results ; an additional category from this participant had been merged with another that made 9 out of 10 in the information structure . Seven cards did not match with the final information structure ; they were " floating " cards of special interest and were not directly organized . Fewer than 90 of the original cards were represented in the final participant 's work . There were a total of 75 cards represented in their structure including several " grouped " cards ( bound by paper clips ) that were counted as a single card and nine new cards were added through the study . In the results from the Open card sorting study , 19 out of 90 of the original set of cards had a greater @ @ @ @ @ @ @ @ @ @ participants agreed on the location of 21% of the cards . Eight of the final 11 categories were represented in these cards . <p> Although the number of categories represented by the high agreement cards in the Open card sorting study is very close to the number represented by the Modified-Delphi card sorting study , there is a much greater difference between the two studies in the number of high agreement cards in general . The Modified-Delphi card sort provided 47 more high agreement cards ( with a total of 66 out of 90 cards ) than the Open card sort ( with a total of 19 out of 90 cards ) . <p> Table 1 . Summary of Agreement Weight Comparisons <p> Comparison <p> Modified-Delphi <p> Open <p> Number of cards with &gt; 50% agreement weight <p> 66/90 ( 73% ) <p> 19/90 ( 21% ) <p> Number of categories represented by cards with &gt; 50% agreement weight <p> 9/10 <p> 8/11 <h> Exit questionnaire <p> An exit questionnaire was administered to the 8 Modified-Delphi card sort participants and 10 Open card sort participants for a total of 18 @ @ @ @ @ @ @ @ @ @ felt were 10 of the most important cards to them . Their selections were based on their perspective of their individual audience needs , but because of the diversity of participants involved in the two card sorting studies , the selections were deemed to provide insight as to the most important topics of the dataset . Results of this questionnaire also helped determine questions for the Inverse card sort ( see Table 2 ) . <p> Fifty-seven out of 90 cards were selected by at least one participant with a median number of votes of 2 ( excludes 0 vote cards ) . Given that half of the participants were current law students or undergraduate students interested in law school , it follows that the top selected cards relate to topics students are interested in : course listings and schedules , costs and ways to pay for school , and finding a job . <p> Table 2 . Top 15 Responses to the Exit Questionnaire <p> Card Title <p> Number of Votes <p> ( 18 Participants ) <p> 1 . Catalog/Course Listings <p> 9 <p> 2 . Tuition <p> 8 @ @ @ @ @ @ @ @ @ @ Financial Aid Information <p> 7 <p> 5 . About the School <p> 7 <p> 6 . Academic Calendar <p> 7 <p> 7 . Scholarships &amp; Loans <p> 6 <p> 8 . How to Apply <p> 6 <p> 9 . Academic Requirements <p> 6 <p> 10 . Course Descriptions <p> 5 <p> 11 . Library Services <p> 5 <p> 12 . Centers &amp; Programs <p> 5 <p> 13 . Admissions Process <p> 5 <p> 14 . Faculty Profiles <p> 5 <p> 15 . University Facilities &amp; Services <p> 5 <h> Heuristic review results <p> Fifteen information architects completed the heuristic review that asked them to rank each of the information structures on a scale from 1 ( very poor ) to 3 ( average ) to 5 ( very good ) ( see Table 3 ) . In addition to six heuristics , they were asked to provide an overall rating of the information structure on the 1 to 5 scale , independent of the other information structure ( see Table 4 ) . <p> In each heuristic , the average score of the Modified-Delphi information structure was greater than the @ @ @ @ @ @ @ @ @ @ is used when there are small numbers of matched samples ( NIST , 2007 ) and does not require a normally distributed sample like other tests . The differences are calculated and then ranked by magnitude . ( One participant did not fully complete the heuristic review so only 14 pairs were calculated . ) The nominal alpha criteria level ( + ) and the limit for the probability ( p ) of the outcome under the null hypothesis is set at 0.05 . Probability is the measurement of the likelihood of an event . <p> According to the expert heuristic review responses gathered , the Modified-Delphi information structure is better than the Open information structure in granularity and dimension , consistent and logical naming , and overall , significant at the + = 0.05 level . The comparison of other heuristic responses gathered are not statistically significant based on response distribution and sample size . <p> Table 3 . Average of All Results from Heuristic Review <p> Heuristic <p> Modified-Delphi <p> Open <p> Breadth and depth are balanced <p> 4.0 <p> 3.4 <p> Labels are clear and meaningful <p> @ @ @ @ @ @ @ @ @ @ dimension <p> 4.1 <p> 2.9 <p> Naming scheme is consistent and logical <p> 4.0 <p> 3.1 <p> Visual hierarchy is clear <p> 4.1 <p> 3.6 <p> Organization fits users ' needs <p> 4.1 <p> 3.4 <p> Overall rating Individual question , not an average of the heuristic score <p> Overall rating ( individual question , not an average of the heuristic score ) <p> 3.9 <p> 3.2 <p> 0.023 <p> * not statistically significant at the + = 0.05 level <h> Independent information structure ranking <p> Fourteen participants completed paired responses to the question of usefulness of an information structure . Participants were asked , independent of the other information structure , if the presented information structure would be useful in aiding in the design of an information architecture ( see Table 5 ) . <p> A binomial test is a one-tail test that determines the significance of the deviation from an expected distribution ( an equal number of yes and no responses ) . The tail is the part of the bell-shaped normal distribution curve that is far from the center . Determining that there is no significant difference @ @ @ @ @ @ @ @ @ @ is a high probability that both scores came from the same distribution . In this case , the scores would fall under the bell part of the distribution and not in the low probability tail . <p> In a completely random sample , the expected response would be an equal number of yes and no votes for a particular information structure . If the structure is truly helpful in designing an information architecture , the responses would lean towards yes . <p> response yes = response no There is no difference in the helpfulness of the information structure . <p> The binary test confirms the chance of observing 14 or more yes votes for Modified-Delphi in 14 trials is significant at the + = 0.01 level where p = 0.0001 . The chance of observing 12 or more yes votes or 2 or fewer no votes for Open in 14 trials is significant at the + = 0.05 level where p = 0.0065 . According to the expert responses gathered , both the Modified-Delphi and Open information structure are helpful in designing an information architecture , significant at the + @ @ @ @ @ @ @ @ @ @ Independent Structure Ranking <p> Would you find this structure helpful in designing an information architecture ? <p> Yes <p> No <p> Modified-Delphi information structure <p> 14 <p> 0 <p> Open information structure <p> 12 <p> 2 <h> Dependent information structure ranking <p> Fourteen participants completed the information structure ranking . Participants were asked to rank which information structure was better to aid in the design of an information architecture ( see Table 6 ) . They were presented with two information structures representing results from the Modified-Delphi and Open card sorting studies that were anonymized and order counterbalanced . They were asked to rank the first information structure as being better , the same , or worse than the second information structure . Ten participants responded that the Modified-Delphi information structure was better than the Open information structure . Two participants responded that the Open information structure was better than the Modified-Delphi information structure . Two participants responded that the information structures were the same . <p> The sign test is a two-tail test based on the assumption that two cases have equal probabilities of occurring . The sign test @ @ @ @ @ @ @ @ @ @ a consistent direction . This test is ideal for small samples as these differences may prove to be significant even if the magnitudes of the differences are small . The binomial test is a one-tail test based on the probability of observing each case in a series of trials where one answer or the other is selected . All equal observations ( where Modified-Delphi information structure = Open information structure ) are ignored . <p> The sign test confirms that the chance of observing 10 or more votes for the Modified-Delphi ( or two or fewer votes for Open ) in 12 trials is significant at the + = 0.05 level where p = 0.0386 . The binomial test confirms the chance of observing 10 or more votes for the Modified-Delphi in 12 trials is significant at the + = 0.05 level where p = 0.0193 . According to the information expert ranking , the Modified-Delphi information structure is not equivalent to the Open information structure ; the Modified-Delphi information structure is better than the Open information structure , significant at the + = 0.05 level . <p> Table 6 @ @ @ @ @ @ @ @ @ @ Number of Votes <p> Open is better than Modified-Delphi <p> 2 <p> Modified-Delphi and Open are about the same <p> 2 <p> Modified-Delphi is better than Open <p> 10 <p> - <h> Inverse card sort results <p> Seven participants completed the Inverse card sort . Participants were asked to select the category where they would be able to find the information asked for in a question ( see Table 7 ) . <p> Both the Modified-Delphi and Open information structures did poorly in the Inverse card sort with very close cumulative scores of 33 and 32 total correct out of 70 possible answers , respectively . Although the Modified-Delphi had more high scores ( 5 out of 7 or greater ) than the Open information structure , it also had the most low scores ( 1 out of 7 or fewer ) . <p> Questions where both the Modified-Delphi and Open information structures did well ( 5 out of 7 or better ) were the following : where to find the final class drop date , the Dean 's Notes Newsletter , and school computing services information . Additionally , @ @ @ @ @ @ @ @ @ @ where to find the Tax Law clinic application , and the Open information structure had a good score on where to find information on the Attorney Practice internship . <p> Questions where both the Modified-Delphi and Open information structures did very poorly ( 1 out of 7 or worse ) were the following : where to find the Judge Solomon Liss visiting scholars program , a copy of your transcripts , and information about the Student Council . Additionally , the Modified-Delphi information structure also had a very poor score on where to find forms to register for classes at another institution . <p> There were an insufficient number of responses to conduct a statistical test on the results . <p> Table 7 . Results from Inverse Card Sort <p> Question <p> # Correct/7 Participants <p> Modified-Delphi <p> Open <p> 1 . Where would you find information about the last day to drop or add a class for the semester ? <p> 7 . Where would you find help with connecting your computer to the school network ? <p> 7 <p> 5 <p> 8 . Where would you find the @ @ @ @ @ @ @ @ @ @ 1 <p> 4 <p> 9 . Where would you find information about the Attorney Practice Internship ? <p> 4 <p> 5 <p> 10 . Where would you find a copy of the Honor Code ? <p> 3 <p> 3 <p> Total # correct out of all answered questions <p> 33/70 <p> 32/70 <h> Discussion <p> In the real world , the results of a pre-design card sort should not be converted to an information architecture or be used to verify the final information architecture without the influence or input from an information design expert . However , for the purpose of comparing two pre-design methods without the bias and experience of an information design expert , this was the best way obtain accurate results for this study . Pre-design card sorting methods are designed to gather insights on how users model information , but do not take in to account user tasks , context of use , or client goals . They offer a useful way to gain design insight to aid in the design of an information architecture and involve users in the design process . <p> During both @ @ @ @ @ @ @ @ @ @ between participants of the Modified-Delphi card sorting study and the Open cart sorting study . Most of the Modified-Delphi study participants were very talkative during their sessions , asking questions about cards and the instructions , talking about the decisions they were making as they organized the cards , their experiences they have had on law school websites , or about their lives in general . Participants of the Open study were quite the opposite , with only a few participants asking questions about the cards or the instructions , and no one talking aloud during their process or sharing any experiential or personal information . Although this behavior is reported anecdotally , this may be an indication of how high the cognitive load is for the Open card sort in comparison to the Modified-Delphi card sort . Each Open card sort participant must review all of the cards , create a model , and then refine that model . The Modified-Delphi participants had help with the creation of a model by only having to make modifications to other participants ' work that they review . The cards were presented @ @ @ @ @ @ @ @ @ @ had to form an opinion about the model and modify it , rather than creating a model from scratch and refining it . Besides being more participant-friendly , the possibility of a lower cognitive load may make the Modified-Delphi method appropriate for studying larger or more difficult datasets that are usually avoided in Open card sorting because of the cognitive costs to participants . <p> During the Modified-Delphi card sorting study , three of the participants ( not including the seed participant ) decided to start from scratch and scooped up all the cards in a pile before beginning their work , destroying the previous participants ' work . This was expected to create a noticeable change in the evolving structure ; however , during analysis , there was still a great similarity between the participants who based their work off of others and the participants who started from scratch . During the study instructions , participants were asked to review the provided structure before beginning their work . This priming ( Tulving , 1990 ) of the previously proposed model may have influenced them on some scale . Their @ @ @ @ @ @ @ @ @ @ the dataset and felt more comfortable organizing it on their own terms . <p> During the analysis phase of the parallel card sorting studies , there was a noticeable difference in the difficulty analyzing results between the two methods . It took more than twice as long to analyze the Open card sort results than the Modified-Delphi card sort . In addition , there were two additional participants in the Open card sorting study than the Modified-Delphi study , but contrary to expectation , it was not during the card entry in to the spreadsheet where the time was lost . This difference in analysis time may be attributed to the Modified-Delphi card sorting study results having a much higher agreement weight than the Open card sorting study . Seventy-three percent of the cards in the Modified-Delphi card sorting study had a greater than 50% agreement weight that made the decision on where to place card in the final information structure category very quick and easy . However , only 21% of the cards in the Open card sorting study had greater than 50% agreement weight . This meant that @ @ @ @ @ @ @ @ @ @ the placement heuristics described in the Methodology section had to be employed . This process was very time consuming and essentially made the analysis of the Open card sorting study more expensive . <p> The two studies had very similar user group combinations , which strengthens the comparison between the two methods ( see Table 8 ) . The Modified-Delphi method did have a disproportionate number of women over men , but I do not think this had an impact on the study . The Inverse card sort had a small number of participants ( seven ) ; however , this number is typical in most user-centered design studies . There were also a disproportionate number of students ( four current law students , one undergraduate pre-law out of seven participants ) who participated compared to the other study participant makeups . This may have had an effect on the Inverse card sort study results . <p> Table 8 . Summary of Study Participant Groups <p> User Group <p> Modified-Delphi Card Sort <p> Open Card Sort <p> Undergraduate Pre-Law Students <p> 2 <p> 1 <p> Current Law Students <p> 2 @ @ @ @ @ @ @ @ @ @ <p> Law School Administration <p> 1 <p> 1 <p> Law-Related Professional ( non-Attorney ) <p> 1 <p> 2 <p> Law-Related Professionals ( Attorney ) <p> 1 <p> 1 <p> Total number of participants <p> 8 <p> 10 <h> Summary of results <p> The results from the studies presented in this thesis provide compelling evidence that as a laboratory method , the Modified-Delphi card sort provides an alternative that is better than the Open card sort for gathering input for an information architecture early in the design process ( see Table 9 ) . <p> The independent information structure ranking suggests that information design experts thought both the Modified-Delphi and Open information structures would be useful for aiding in the design of an information architecture . However , the overall rating and the dependent information structure ranking both offer statistically significant evidence that information design experts thought the Modified-Delphi information structure was better than the Open information structure in terms of heuristics and more useful for aiding in the design of an information architecture . Although not every heuristic ranking was found statistically significant ( 3 out of 4 rankings were @ @ @ @ @ @ @ @ @ @ Modified-Delphi information structure was ranked higher than the Open information structure in all counts . <p> The results from the Inverse card sort were surprisingly very poor . Low results were expected because the tested information structures were not refined information architectures , however a performance of 46% was well below the expected score . Although only seven participants were tested , a number too small for statistical tests , this number of participants is typical to how a usability study would recruit . Interestingly , the information structures from the Modified-Delphi and Open studies performed similarly in most of the questions and high or low scores , even though the questions were selected because of the differences in the information structures . This suggests that the topic questions that had the poorest scores may be sensitive to the types of participants ( who were overwhelmingly students ) and must be given special consideration when refining the final information architecture . <p> Table 9 . Summary of Results from Method Comparison Studies <p> Comparison of Sorting Method <p> Heuristic Review <p> Overall Rating <p> Independent Ranking <p> Dependent Ranking <p> @ @ @ @ @ @ @ @ @ @ - <p> - <p> - <p> - <p> Modified-Delphi = Open <p> - <p> - <p> X <p> - <p> X** <p> Modified-Delphi &gt; Open <p> X* <p> X <p> - <p> X <p> - <p> * not all heuristic scores were statistically significant at the + = 0.05 level ** not found to be statistically significant at the + = 0.05 level due to small sample size <h> Conclusion <p> When I proposed the Modified-Delphi card sort , there were three goals I wanted to meet in order to have successfully proposed a pre-design method to replace the Open card sort : ( a ) improve the quality of results from each participant , ( b ) reduce the time to conduct the study and analyze the results , ( c ) lower the costs of conducting a study and possibly the cognitive costs to participants . <p> Results . There is a proven benefit of participants working with a single model , rather than many . The information evolutionary model influenced by the Delphi method helps control randomness and outliers that are commonly encountered when analyzing multiple models @ @ @ @ @ @ @ @ @ @ studies conducted also suggest the superiority of results generated from the Modified-Delphi card sort over the Open card sort . <p> Time . By reducing the recommended number of participants to complete a successful study , the amount of time required to both conduct a pre-design card sorting study and analyze the results has been reduced . The amount of time saved during analysis may be even greater due to the quality of results gathered , as discussed in the Discussion section . The weak agreement between the Open card sorting results required extra analysis time to follow heuristics for card placement , thus increasing the difficulty in analysis and the time for analysis . <p> Costs . Time is money ( Walker et al. , 2003 ) , and by reducing the overall time of a study , the costs are also reduced . Combined with fewer participant stipends and fewer days of facility costs , the overall return on investment of the Modified-Delphi card sort is significantly higher over the Open card sort . While not monetary in nature , there is also a savings of cognitive costs @ @ @ @ @ @ @ @ @ @ the Modified-Delphi card sorting study were much more talkative than the participants of the Open card sorting study . It is possible that the method reduced cognitive cost and helped the participant to be more engaged in the problem solving parts of the task . <p> There is strong statistically significant evidence that as a laboratory method , the Modified-Delphi card sorting method is better than the Open card sorting method . Results from the heuristic review and ranking by information experts show the overall rating of the results from the Modified-Delphi card sort were better than the results from the Open card sort . The information structure generated from the Modified-Delphi card sort was also considered to be more helpful for aiding in the design of an information architecture than the information structure generated from the Open card sort . Also , many of the expert heuristic review scores from the Modified-Delphi card sort were significantly better than the results from the Open card sort ; those that were not statistically significant still provide evidence that the two methods are at least equivalent . <p> The Modified-Delphi method is @ @ @ @ @ @ @ @ @ @ study and provides better results than the traditional Open card sort . Additionally , results from the study suggest a savings of time and costs from using the Modified-Delphi card sort over the Open card sort . A parallel study of the two methods has provided statistically significant evidence that results from the Modified-Delphi card sort are at least as good as the results from the Open card sort , and in some cases , better . <h> Next Steps <p> The Modified-Delphi cart sort provides a promising alternative to Open card sorting for use as a pre-design method . However , it is a new method and must be further researched in order to refine the methodology and maximize the return of investment it requires . Questions about the method surfaced during the design of method and analysis and preliminary reporting of results . Some of these questions included the following : <p> Does the number of 8 to 10 participants hold up for all datasets and participant diversity cases ? <p> How do characteristics of the dataset affect the validity of the method ? <p> How much does @ @ @ @ @ @ @ @ @ @ user group versus a sampling of all user groups ) ? <p> How much influence does the selection of the seed participant have on the study ? <p> How does the method hold up with groups of participants per session , rather than single participant sessions ? <p> Does the Modified-Delphi card sorting method provide as good of results executed as a web-based method as it does in the laboratory ? <p> The answers to these questions can only be obtained by conducting additional Modified-Delphi card sorting studies . Parallel Open and Modified-Delphi studies are not cost effective and only make sense for research purposes . Modified-Delphi card sorting has already been proven to be as at least as good and very likely better than Open card sorting . It would be interesting to conduct this method as a web-based method to see if it offers benefits to web-based methods as it has to laboratory-based methods . I encourage practitioners to utilize this method early in the design process and share their results with the rest of the community . <h> Acknowledgments <p> This research was partially funded by the @ @ @ @ @ @ @ @ @ @ ) and introduced at the 2007 Information Architecture Summit in Las Vegas , Nevada . I would also like to thank Kathryn Summers of the University of Baltimore and my colleagues Bill Killam and Marguerite Autry of User-Centered Design , Inc. for their guidance and support during the project . 
@@105507088 @907088/ <h> Articles by Patrick Langdon <p> The Cambridge Workshop on Universal Access and Assistive Technology ( CWUAAT ) is held every other year at a UK Cambridge University college . CWUAAT ' 12 was part of this series , started in 2002 , that presents research from the international inclusive design community . <p> This paper describes an exploratory study investigating ways to accommodate inclusive design techniques and tools within industrial design practices . The approach of our research is that by making only small changes in design features , designers end up with more inclusive products . 
@@105507096 @907096/ <h> Menu <h> Letter from the President <p> In 2016 , we launched our monthly webinar program ( sometimes more than one webinar a month ) , and we had amazing attendance . I am proud to announce that we will continue our monthly webinar program , and we are making it even better in 2017 . We are kicking off the year with a webinar by Rajesh Kalidindi about Accessibility ( this Friday ! ) Register here . This year , we will also bring you webinars in other languages ! The first will be in Spanish on Feb 27 . Stay tuned for more info and for upcoming webinars in other languages too ! <p> We also plan on bringing more educational resources to UX professionals this year . We will try out a " short course " program , and results of the bi-annual UX Salary Survey will be published . At the annual UXPA conference , we will host a Student Design Competition . Speaking of students , we launched our first student chapter this month ! This is really exciting for the organization , @ @ @ @ @ @ @ @ @ @ up and running this year . <p> In 2017 , you will also be hearing from the Board of Directors more . We want to be more transparent in our activities and organizational structure . Each month , a Director will share things that are going on and that they are working on . If you have questions , please reach out . Our goal is to be transparent and engage more with members . <p> Personally , I am also looking forward to many changes in 2017 . My husband and I recently bought a house in Cold Spring , NY , and I will be transitioning from Facebook to Instagram in February . I will lead UX research for Instagram in emerging markets . I am looking forward to the shorter commute into the city ( it used to be 2+ hours , not to mention that I spent much of 2016 traveling around the world ) , and I am looking forward to working alongside my team more often . In 2016 , I co-authored a book , Usability Testing for Survey Research , and it will @ @ @ @ @ @ @ @ @ @ forward to attending more UX meetups in NYC . Have a favorite ? Message me , and I will see you there ! <p> Once again , happy new year . I am looking forward to accomplishing great things this year with you all . See you soon ! 
@@105507097 @907097/ <h> This Issue <h> Culture and Usability Evaluation : The Effects of Culture in Structured Interviews <h> Abstract <p> A major impediment in global user interface development is that there is inadequate empirical evidence for the effects of culture in the usability engineering methods used for developing these global user interfaces . This paper presents a controlled study investigating the effects of culture on the effectiveness of structured interviews in international usability evaluation . The experiment consisted of a usability evaluation of a website with two independent groups of Indian participants . Each group had a different interviewer ; one belonging to the Indian culture and the other to the Anglo-American culture . The results show that participants found more usability problems and made more suggestions to an interviewer who was a member of the same ( Indian ) culture than to the foreign ( Anglo-American ) interviewer . The results of the study empirically establish that culture significantly affects the efficacy of structured interviews during international user testing . The implications of this work for usability engineering are discussed . 98865 @qwx958865 <p> Cultural mismatch between the interviewer @ @ @ @ @ @ @ @ @ @ interviews . <p> Interviewers from the same culture might be more effective in eliciting usability problems in when users come from hierarchical cultures . <p> Hofstede 's cultural dimensions model can be used to inform the selection of usability assessment techniques cross-cultural user testing . <p> Culture might influence the efficacy of a usability method that involves high degree of social interaction . <p> Considering culture in the usability evaluation process will lead to better informed products and services . 
@@105507100 @907100/ <h> This Issue <h> Unexpected Complexity in a Traditional Usability Study <h> Abstract <p> This article is a case study of a demonstration project intended to prove the value of usability testing to a large textbook publishing house . In working with a new client , however , the research team discovered that what our client thought were simple problems for their users were actually complex problems that required the users to evaluate potential solutions in a surprisingly complex context of use . As Redish ( 2007 ) predicted , traditional ease of use measures were " not sufficient " indicators and failed to reveal the complex nature of the tasks . Users reported high levels of satisfaction with products being tested and believed they had successfully completed tasks which they judged as easy to complete when , in fact , they unknowingly suffered failure rates as high as 100% . The study recommends that usability specialists expand our definition of traditional usability measures so that measures include external assessment by content experts of the completeness and correctness of users ' performance . The study also found that it @ @ @ @ @ @ @ @ @ @ end of complexity in their products because doing so creates a new space for product innovation . In this case , improving our clients ' understanding of complexity enabled them to perceive and to take advantage of a new market niche that had been unrealized for decades . 98865 @qwx958865 <p> With new clients usability specialists need to accommodate the likelihood of encountering complex problems masquerading as simple ones and ensure that the studies we design use sound methodological triangulation techniques , including content experts ' assessment of the quality of the users ' performance and , when possible , head-to-head comparisons with competing products . <p> Users in this study assumed that they were dealing with a simple problem , and once they found what they thought was the simple solution , they did n't look any further for more complex answers . <p> Users gave a positive evaluation of the ease of use for products that they believed had helped them complete a task when , in fact , the information products misled them , allowing them to believe that they had finished tasks that were only partially @ @ @ @ @ @ @ @ @ @ of success that can ultimately lead to poor user performance , visuals must signal to users that they are dealing with a complex problem . <p> Maintaining a singular purpose in visuals is important for success , even though complex information needs to be delivered . Use several visuals to convey different purposes . <p> Users need tasks that help them understand where to begin the decision-making process . When users understand the role they are supposed to play , they also understand the logic they 're supposed to follow in order to make the decisions necessary to complete the tasks at hand . <p> Modeling complex problem solving behaviors through the use of scenarios may lead to more effective performance from users when it 's simply not possible to capture or replicate all of the potential variables in a situational context ( Flowers , Hayes , and Swarts 1983 ) . <p> Helping new clients understand the upper end of complexity in their products can make it possible for designers to perceive new opportunities for product innovation and can help them create new strategies for market success . In @ @ @ @ @ @ @ @ @ @ had failed to recognize the complexity involved in citing MLA sources and in responding to commas also helped them find innovative ways of entering a market that had been dominated by one product design approach for decades . 98864 @qwx958864 <p> In " Expanding Usability Testing to Evaluate Complex Systems , " Redish ( 2007 ) observed , " Ease of use-what we typically focus on in usability testing-is critical but not sufficient for any product . Usefulness ( utility ) is as important as ease-of-use . If the product does not match the work that real people do in their real environment , it may be an easy-to-use solution to the wrong set of requirements " ( p. 104 ) . This study provides empirical evidence that validates Redish 's observation . The traditional ease of use indicators in this study , both quantitative and qualitative , suggested that users found the prototype handbook my colleagues and I were testing to be an " easy-to-use solution " to their perceived needs . What 's more , our client was entirely satisfied with the ease of use data we provided @ @ @ @ @ @ @ @ @ @ in the study . As is often the case with clients new to usability studies , the clients were more concerned with " whether users liked using the product " than they were with what the study might be able to tell them about accuracy and functionality . This common attitude among clients who are new to usability studies can make it difficult for usability professionals to justify the additional expense and complexity of research designs that collect data on " utility " as well as ease of use ( Redish 2007 ) . However , had this study 's design solely focused on what the client told us they wanted and had we only used traditional measures , we would have failed both our client and the users . Consequently , this case will hopefully serve as an argument that can be used to show clients why we need to go to the additional expense of methodological triangulation . What 's more , this case is also a " success story " that can hopefully be used to illustrate how helping clients understand complexity they did n't know existed @ @ @ @ @ @ @ @ @ @ for innovation and creative thinking . In other words , this is a story about how discovering unexpected complexity enabled our client to release what has become a tremendously successful product into a market that had been dominated for decades by a traditional type of product design . <p> Definitions of complex problems vary , but according to Michael Albers ( 2003 ) , " In complex problem solving , rather than simply completing a task , the user needs to be aware of the entire situational context in order to make good decisions " ( p. 263 ) . In doing a comparative usability test of two handbooks for students in freshman composition classes , this study provided a very interesting case in point of Albers ' statement . Students thought the writing handbooks were easy to use , but they chose incorrect solutions to problems , because the handbooks did not help them understand the complexity of their task and did not provide guidance on how to choose wisely within that complexity . <p> This case study is also particularly interesting because we worked with textbook publishers . @ @ @ @ @ @ @ @ @ @ but the textbook industry has traditionally relied on reviews by experts . They rarely take time to gather user experience information from the actual users of their products , i.e. , the students . In this project , my colleagues and I worked with the Director of Research and Development for a major textbook publisher , Allyn &amp; Bacon/Longman , to design a study that sought to demonstrate that usability testing of textbooks with actual users could be used both ( a ) as a means of helping authors to produce more usable books and ( b ) as a means of helping acquisition editors and senior management evaluate whether or not to invest the resources needed to take the product to market . <p> Initially , the study had three goals : <p> Convince textbook publishers of the value of usability testing . <p> Compare a new visual type of handbook against a traditional version . <p> Help find and fix problems in the new type of handbook . <p> However , as the study progressed and we learned more about our new client 's understanding of the complexity @ @ @ @ @ @ @ @ @ @ , a fourth goal emerged . We were able to demonstrate for the client that aggressively seeking to understand complexity in a product that they had n't known was there simultaneously enabled them to see new market opportunities and helped create a space for creative thinking about a product that had been on the market for decades . <h> Visual versus Verbal Designs <p> One of the principal ways which Longman wished to distinguish its new handbook from the long-time market leader was to attempt to primarily deliver the content for the book visually rather than verbally . Our particular study compared the usability of a brand new grammar handbook designed entirely around the visual delivery of grammar and other writing conventions with a grammar handbook that has been the market leader and that is extremely verbal in its delivery of content . <p> As Figure 1 illustrates , Diana Hacker 's A Writer 's Reference ( 2006 , p. 236 ) depends mainly on prose discussion to deliver the content it covers . Grammatical rules and conventions are stated in the traditional vocabulary of grammarians . Examples of the @ @ @ @ @ @ @ @ @ @ set off from the prose discussion by maroon-colored bullets and boldfaced font . Occasionally , a maroon caret symbol or is used beneath a line of text to call attention to an important punctuation . Alternatively , an italics font is used to call attention to key words in the example . <p> Figure 1 . Sample page from market leading handbook <p> The Longman prototype handbook , however , seeks to minimize the amount of prose used to deliver content as well as grammatical terminology . As Figure 2 shows , the Longman prototype uses a variety of visual techniques to deliver content . Color coding is used to call attention to the differences in locations where phrases may be added in a basic sentence and then to show what those phrases look like and how they are punctuated . Also , rather than using terminology like " adding a medial modifier to an independent clause , " the language here is much simpler and accessible to a non-specialist audience . <p> Figure 2 . Sample page from prototype handbook <p> In terms of its approach to MLA documentation @ @ @ @ @ @ @ @ @ @ same fashion as was illustrated in Figure 1 , using the same page layout and conventions to deliver its content . Because of its visual orientation , however , the Longman prototype used a different visual to illustrate to students and users how to prepare a works cited entry ( see Figure 3 ) . <p> Figure 3 . MLA works cited visual in prototype <p> Our study sought to determine whether the traditional verbal or the newer visual approach was more usable . <h> Methodology <p> The procedure used in this study was a traditional think-aloud protocol analysis . There were three major parts to the study : <p> Pre-test interview : where demographic and background data about users were collected . <p> Think-aloud protocols : where users were presented with scenarios requiring that they complete tasks and say aloud what they are thinking and doing as they perform the tasks . <p> Post-test interview : where users were asked to reflect critically on their experiences and to compare the texts examined . <p> Pilot testing was conducted on the instruments to ensure that the questions , instructions , @ @ @ @ @ @ @ @ @ @ collection instruments functioned properly , but data from the pilot testing are not used in this article . <h> Pre-test interview and subject profiles <p> After going through the informed consent statement agreement and giving their permission to be videotaped , participants in the study were asked to participate in a pre-scenario interview . This interview collected basic demographic information about participants ' experience with high school English classrooms and helped us gauge whether or not the participants were representative of typical freshman composition students . <p> Table 1 provides a breakdown of the 12 participants who were recruited from 6 different composition classes and were paid $75 for their participation . Because grammar handbooks are used in both 2-year and 4-year colleges , 6 of the participants ( 4 males and 2 females ) were from a 2-year community college , and 6 of the participants ( 3 males and 3 females ) were from a 4-year university . All of the participants were either 18 or 19 , all 12 were in their first semester of college , and most importantly , all 12 participants were currently enrolled @ @ @ @ @ @ @ @ @ @ . Personal Background Information <p> User # <p> Age <p> Gender <p> Race <p> Major <p> High School GPA <p> 4.0 Scale <p> 1 <p> 18 <p> M <p> A <p> Industrial Engineering <p> 4.2/5.0 <p> 3.36 <p> 2 <p> 18 <p> F <p> C <p> Communication <p> 4.8/5.0 <p> 3.84 <p> 3 <p> 19 <p> M <p> C <p> Undeclared <p> 4.02/5.0 <p> 3.22 <p> 4 <p> 18 <p> F <p> C <p> Spanish &amp; International Trade <p> 4.24/5.0 <p> 3.4 <p> 5 <p> 18 <p> M <p> A <p> Computer Science <p> 3.7/4.0 <p> 3.7 <p> 6 <p> 18 <p> F <p> A <p> Chemistry <p> 3.5/4.0 <p> 3.5 <p> 7 <p> 19 <p> M <p> C <p> Univ . Transfer-Electrical Engineering <p> 3.84/4.0 <p> 3.84 <p> 8 <p> 19 <p> M <p> C <p> Univ . Transfer-Electrical Engineering <p> 3.6/4.0 <p> 3.6 <p> 9 <p> 19 <p> M <p> C <p> Undeclared <p> 3.4/4.0 <p> 3.4 <p> 10 <p> 18 <p> M <p> C <p> Univ . Transfer-Business Management <p> 2.8/4.0 <p> 2.8 <p> 11 <p> 18 <p> F <p> C <p> Univ . Transfer-Business Management <p> @ @ @ @ @ @ @ @ @ @ C <p> Univ . Transfer-General Studies ( wants to go to Medical School ) <p> 3.2/4.0 <p> 3.2 <p> TOTALS <p> 18.3 <p> - <p> - <p> - <p> - <p> 3.42 <p> A=African American C=Caucasian <p> We also collected data about grades , SAT scores , majors , and other information in order to show that users were fairly representative of freshman composition users in the Southeast . <h> Scenarios and tasks for think-aloud protocols <p> After collecting this basic information about the participants , we introduced participants to the scenarios . Naturally , participants were instructed to talk out loud and to verbalize their thoughts as they attempted to use the textbook to perform the tasks provided . To help us track what they were observing on the pages , participants were instructed to point at the text as they moved through it and to read aloud when they were reading text . If users did not speak for more than 5 seconds , they were prompted by the test administrator and asked to explain what they were thinking . Also , active intervention protocol techniques were used @ @ @ @ @ @ @ @ @ @ what they were seeing , and how they felt about the material . Participants ' comments were videotaped and coded , helping us to identify whether or not the page layout techniques , navigation systems , and other features of the text assisted or impeded users ' ability to perform the tasks . <p> Evaluating the acceptability of sources based on information about a specific assignment and the audience for the piece they would be writing ( the evaluating sources scenario ) <p> Each scenario increases the complexity of the task to be performed . The citing sources scenario essentially asked users to follow a model in order to complete a task . Identifying non-trivial comma errors was slightly more complex because it asked users to apply rules to a situation and to make a judgment . And the final task , evaluating possible sources for a library research paper , was the most complex because it required an understanding of the rhetorical situation in which the sources would be used . Users had to make a judgment about the appropriateness of a source based on the exigency for the @ @ @ @ @ @ @ @ @ @ wide variety of other environmental factors . <p> Participants used both handbooks for each scenario so they could compare the two handbooks . However , in order to control for first-use bias , the research team alternated which handbook they used first in each scenario . This ensured that both handbooks were used first an equal number of times . <h> Citing sources scenario <p> In the first scenario , users were asked to assume that they were working in their current composition class on a research paper . The scenario required that they create a works cited entry using MLA style . The researchers provided books with passages marked in them that users had quoted in the hypothetical research paper they had written . <p> During the pilot testing , we discovered that users would not actually use the handbooks thoroughly if the works cited entries were simple , single authored books . Because of their previous experiences writing research papers and creating works cited entries , it was necessary to challenge users with difficult and unusual citation tasks that actually required them to use the handbooks to find @ @ @ @ @ @ @ @ @ @ one of the citing sources scenario , the following text was used for the works cited entry : <p> At the end of each part , users were asked to rate the ease of use for the handbook using the following scale : <p> Very useful , Useful , Rarely useful , Not useful <p> Users were then asked to explain their rating . <h> Using punctuation scenario <p> In this scenario , users were asked to identify comma errors in a paragraph and to provide the page numbers from the handbooks that provided information about the correct comma usage . We presented users with a sample student essay pregnant with comma errors that were based on the 20 most common errors found in 3,000 college essays by Connors and Lunsford ( 1992 ; see also Smith , 2006 ) . We identified four potential comma problems for the users and then asked them to locate information in the handbooks that told them : <p> if a comma was required at the location indicated , <p> if no comma was required , or <p> if the comma was optional . @ @ @ @ @ @ @ @ @ @ it was for users to locate specific comma usage information . <p> In part one , users were asked about comma usage in the places indicated by the four numbered circles in the following paragraph : <p> In America1 it is quite possible to live in a cocoon2 oblivious to the world around you . Confined living situations and close-knit social structures can prevent an individual from ever experiencing a reality , 3 outside of his or her own . Through an artistic medium such as photography one can get a glimpse of a world far removed . Gordon Parks photographer , artist4 and writer , was a liaison between those Americans in one world and their fellow citizens who subsisted in a completely different one . <p> In part two , a similar paragraph was used for the second handbook . After completing each part of the scenario , users were asked to rate the ease of use for each handbook using the same ease of use scale as the citing sources scenario . <h> Evaluating sources scenario <p> The third and final scenario gave users a research paper @ @ @ @ @ @ @ @ @ @ audience for the paper . Users were then given possible sources for the research paper and asked to use the handbooks to indicate if the source was acceptable , unacceptable , or if more information was required . Users were again instructed to provide the page numbers from the handbook that enabled them to make their determinations . And once again , we collected ease of use data on this task . However , while the data from this third scenario were of interest to the client in terms of making recommendations on how to improve their textbook , the focus of this article is on complexity , and most of the complexity issues resulted from the citing sources and comma usage scenarios . <h> Controlling for print quality bias <p> Because the scenarios required that the students use and compare both handbooks and because one text was an unfinished draft , care was taken to ensure that texts used were of comparable finished quality . The materials used in the study were color copies of Longman 's forthcoming DK Handbook and color copies of excerpts from Diana Hacker 's @ @ @ @ @ @ @ @ @ @ made to provide equivalent sections of Hacker 's text so that neither text users received was complete . Both texts were cut to size , printed on facing pages , and plastic-comb bound . The texts were divided into the three sections , separated by Post-it- note tabs that were labeled Commas , Eval . Sources , and MLA . Throughout the study , the test administrator referred to both of the texts as " prototypes " under development and did not reveal to participants that the Hacker text had already been previously published . To further disguise the fact that one handbook was an unfinished draft , while the other represented excerpts from a published text , the test administrator labeled each copy with initials only . Throughout the study , the Hacker copy was referred to as the HC handbook or the HC prototype . The DK Handbook was only referred to as the LP handbook or the LP prototype . <h> Post-test interview <p> Having used both handbooks to complete tasks involving MLA documentation , comma usage , and evaluating sources , participants were asked a series @ @ @ @ @ @ @ @ @ @ experiences . Users were asked to compare the handbooks , to make recommendations for improving the handbooks , and to indicate which of the handbooks they would recommend to their teachers and why . Data obtained from the post-test interview as well as the scenarios are discussed below . <h> Findings <p> This section discusses the general findings and observations of the study . <h> Users preferred the visual approach <p> As Redish has suggested , traditional ease of use measures alone gave us no real sense of the complexity involved in the tasks . Overall , users reported that they preferred the DK prototype 's visual ease of use to the more verbal approach used in Hacker . When asked to rank the " overall " ease of use for the two texts after they had actually used both handbooks , 9 of the 12 users preferred the DK prototype , and 9 of the 12 indicated that they would recommend it to their teachers for their entire class . Users recommending the DK prototype also appeared to have a stronger preference for their recommendations than the 3 recommending @ @ @ @ @ @ @ @ @ @ their preference on a scale from 1 to 10 where 1 indicated that they thought the text was " slightly better " and 10 indicated " vastly superior . " The 9 users recommending DK averaged 7.44 ( standard deviation was 2.35 ) , and the 3 recommending Hacker averaged 6.00 ( standard deviation was 1.0 ) . See Table 2 for the range of scores users gave . <p> Table 2 . Strength of Preference <p> - <p> Strength of Choice <p> Strength of Choice <p> - <p> DK <p> Hacker <p> - <p> 7 <p> 6 <p> - <p> 5 <p> 5 <p> - <p> 3 <p> 7 <p> - <p> 10 <p> - <p> - <p> 9 <p> - <p> - <p> 9 <p> - <p> - <p> 9 <p> - <p> - <p> 9 <p> - <p> - <p> 6 <p> - <p> Avg . <p> 7.44 <p> 6.00 <p> Std . Dev . <p> 2.35 <p> 1.00 <p> During the post-test , users were also asked to give their overall ranking of the ease of use for both texts in terms of finding information ( @ @ @ @ @ @ @ @ @ @ 4 was " Easy " and 1 was " Difficult , " the DK text received an average score of 3.33 ( with a standard deviation of 0.78 ) , and the Hacker excerpt received an average score of 2.33 ( with a standard deviation of 0.89 ) . And once again , the low standard deviation for the DK scores here are noteworthy since only 2 of the 12 users gave the DK text a score of 2 ( or " somewhat difficult " ) , while the remaining users either gave the text a 3 ( " somewhat easy " ) or a 4 ( " easy " ) . Users ' evaluations of the Hacker excerpts were slightly more varied , resulting in the larger standard deviation . However , the difference here is notable when one considers that only 5 users gave Hacker a score of either " easy " or " somewhat easy , " and the remaining 7 users gave Hacker a score on the " difficult " side of the scale . <p> Table 3 . Overall Ease of Use for Finding Info @ @ @ @ @ @ @ @ @ @ <p> Hacker <p> - <p> 4 <p> 3 <p> - <p> 4 <p> 2 <p> - <p> 3 <p> 3 <p> - <p> 2 <p> 3 <p> - <p> 4 <p> 3 <p> - <p> 3 <p> 2 <p> - <p> 4 <p> 2 <p> - <p> 4 <p> 2 <p> - <p> 3 <p> 1 <p> - <p> 2 <p> 4 <p> - <p> 3 <p> 2 <p> - <p> 4 <p> 1 <p> Avg . <p> 3.33 <p> 2.33 <p> Std . Dev . <p> 0.78 <p> 0.89 <h> Users failed at tasks , but did n't realize it <p> Yet , while users ' clear preference for the DK text and their overall " ease of use " scores are suggestive , it would be an error to conclude that the DK prototype 's visual approach was more " usable " than a verbal approach . The ease of use evaluations above do not give a complete picture of the usability of the texts because the users ' evaluations must also be considered in light of the question of whether or not users were actually able to @ @ @ @ @ @ @ @ @ @ other words , users may initially give a positive evaluation of the ease of use for a product that they thought had helped them complete a task , but if a text misled them by allowing them to believe that they had finished the task when , in reality , the task was only partially completed , then the users ' initial assessments are less valuable as a measure of usability . It is at this point that the issue of complex problem solving manifested itself in our study , and it is by means of content experts ' assessment of the quality of the users ' performance that researchers who are working with new clients can observe when complex problems may be disrupting the findings in a traditional usability study . <p> In this study , both the DK prototype and the Hacker excerpts failed the users when it came to successfully completing acceptable works cited entries for the works provided . All 12 users failed to provide a works cited entry that would have been judged satisfactory by college-level composition instructors . Even if one takes into account @ @ @ @ @ @ @ @ @ @ the use of " et al " for multiple authors or editors , the omission of words like " Press " and " Inc. " from publishers ' names , or the decision of whether or not to include the initials for the state after giving the city 's name-even without these , users in this study omitted critical information necessary for a complete , acceptable citation . For example , users failed to list the authors of an article in an anthology , they failed to list a title of the essay , they failed to include the number of an edition , they failed to provide the page numbers for articles , and so on . <h> Users failed to recognize the complexity of their situations <p> Admittedly , the books that users were tasked with citing were challenging , but a strength of the study was that the task was also realistic . One of the works users had to cite was a corporate author where the corporation was also the publisher . The other was a book chapter with three authors published in the second edition @ @ @ @ @ @ @ @ @ @ necessary to challenge the users so that they would actually need to use the handbooks to complete the tasks , and it is the case that all the information needed to cite both of the texts is provided in both handbooks . The books were intended to support precisely this sort of challenging citation , so if the handbooks were to be considered truly usable , it seems legitimate to have expected that users should have experienced more success than the total failure we observed . <p> Furthermore , we saw similar performance issues in the responses to the punctuation scenarios . Although the findings were less problematic from a performance perspective , our study found that users consistently failed to correctly indicate when the use of commas was required , not required , or optional , and they also failed to provide the correct page number from the texts where they obtained the information . For example , 11 of the 12 users incorrectly stated that a comma was required rather than optional after short , 2-word introductory clauses , and once again , this finding was observed for @ @ @ @ @ @ @ @ @ @ of this deficiency in their performance and did not consider this factor when they assessed the " usability " of the handbooks . It was the discovery of these extremely poor performance indicators for both texts that initially led us to question what might have been at issue . Had only one text failed , then this might have suggested that it was the delivery technique used by that product that was at issue . However , the failure of both products was the first real clue that complex problems were at issue . <h> Why did users fail ? <p> Unfortunately there is no single , obvious , one-size-fits-all explanation that describes why some users struggled with the comma sections in the handbooks or that can adequately illustrate why all 12 users failed to produce appropriate works cited entries . Several factors contributed to users ' problems : <p> Users scanned pages for examples that matched the mental models they had for patterns and only stopped to read material when they found patterns that matched those models . <p> They thought the problem was simple and did n't look @ @ @ @ @ @ @ @ @ @ enough . <p> They relied on bold headings and skipped the paragraphs . <p> The visual manual tried to combine too much information in one graphic . <p> The authors of the manuals did n't understand their users ' mental models . <p> To illustrate the difficulties here , it may be worthwhile to examine the ways users attempted to address the question of whether or not a comma is required or optional after the phrase " In America " in the following sentence : <p> In America it is quite possible to live in a cocoon . <p> The correct answer to this question is that the comma is optional , which is explained on page 433 in the DK prototype and on page 236 in Hacker . However , only one DK user correctly gave " optional " as a response , and this user incorrectly identified the pages where the information could be found . All the other DK users incorrectly stated that a comma was required , but only 2 of those 11 gave page 432 as the page that indicated that a comma was required @ @ @ @ @ @ @ @ @ @ The other DK users gave page 428 as the page that contained the information because page 428 had examples of sentences that looked like the pattern . By contrast , all of the Hacker users correctly identified page 236 as the page with the information they needed , but only because the only examples available were on page 236 . <h> Users were satisfied by the first simple solution <p> A closer examination of the process users followed may help to account for some of this difference , but the principal point to be made here is , users assumed that they were dealing with a simple problem , so once the handbooks suggested to them that they had found the simple solution , they did n't look any further for more complex answers . Yet , deciding whether or not a comma is required after the introductory clause " In America " required that users needed " to be aware of the entire situational context in order to make good decisions " ( Albers , p. 263 ) . Whether or not the comma is required depends on the @ @ @ @ @ @ @ @ @ @ required in a formal piece of discourse such as a business proposal and may be omitted in an informal medium such as a letter to a friend ) . However , because the handbooks functioned acontextually , they never signaled to users that context might be a factor in their decision-making processes . <p> For example , the DK users would read the sentence in question and identify the introductory clause pattern ( a few users actually called it an " intro clause " ) . They would then quickly decide that they were dealing with a question about commas and they were very successful at identifying where they should go in the text to locate more information . Because printed pages were used in this study , eye-tracking systems that could have confirmed where users were looking on the page were not available to the researchers . Nevertheless , all of the users appeared to read the large bold heads that said , for example , " Use commas to make numbers , place names , and dates clear . " Often these headers were all that users needed @ @ @ @ @ @ @ @ @ @ were seeking . In the main , users skipped prose passages . Once users found headers that suggested they might be close to the type of comma use they were seeking , they only scanned the examples on the page . They looked at examples in a very specific way-i.e. , to decide if they could match the syntactic pattern they were seeking to the examples . Once they found the example that matched the pattern , the use of commas provided by the example was the only answer they felt they needed , and they rarely read any prose text to confirm the accuracy of their decisions . Unless there were additional examples or some other visual clues to suggest that the decision might be more complex , users assumed it was simple and read no further . <h> Users scan headers and examples and skip text <p> Although this scanning behavior was new to our clients , it will come as no surprise to usability professionals who have observed users of computer or software user guides . Given that users read headers and scanned for examples in order @ @ @ @ @ @ @ @ @ @ a section , it might be easy to assume that the visual approach used by the DK prototype would be far easier for users to scan ( see Figure 2 ) . However , it should be observed that the logic of Hacker 's pages also enabled users to aggressively seek out examples on a page and then slavishly follow those examples . Hacker 's pages use a maroon colored header to state the comma usage ( e.g. , " Use a comma after an introductory word group " ) , and the pages set off examples from the rest of the text with a maroon bullet , a different font , indentation , and double-spacing . Users complained , sometimes vociferously , about how they " hated " Hacker 's small fonts and the fact that the page design made them flip around and read too much . But they were also careful to qualify this by saying that they were , ultimately , able to find the information they wanted . However , their performance on the question of whether introductory commas were required or optional was essentially @ @ @ @ @ @ @ @ @ @ because , even though Hacker states on the same page that there are exceptions when an introductory comma may be omitted ( see Figure 1 ) , these users did not read the exception to the rule . They read the header , they looked at the example , and they decided that they had all the information they needed in order to make a judgment . <h> The visual handbook tried to combine too much in one graphic <p> Another observation that appeared to have contributed to the problems users had with inappropriate citations and with users ' problems with correct comma usage had to do with the overuse of a single graphic or visual . It may be that the authors of the DK manuscript were limited to a strict page count , or they may have been restricted in the number of graphics they could use . However , the authors appeared to be attempting to force as much information as possible into graphics and visual elements like those illustrated in Figures 2 and 3 . This led to visuals <p> that actually produced errors , <p> @ @ @ @ @ @ @ @ @ @ <p> that led users to complain that the book was " tangled up " and visually " messy . " <p> For example , user 11 complained that the visual shown in Figure 2 " threw " her because it was trying to do too much , and she went on to explain that the information around the base sentence " I make time to play outside " is trying to illustrate five different syntactic structures in the same visual . Similarly , user 10 complained that the " pattern pages were distracting " because the " pattern " pages were those that used visuals like those shown in Figure 2 and 3 . <p> Because of its complexity , the graphic in Figure 2 appeared to contribute significantly to the performance errors and decisions not to read the text described in the previous section . In fact , it should be observed that few users actually looked at or commented on the graphic during the actual think-aloud protocol . The test administrator often had to take users to the pages in the post-test interview and ask them about the @ @ @ @ @ @ @ @ @ @ they did , actually , glance at the visual ; however , they complained that they decided that the graphic on 432 was " too busy " and would require too much effort to understand . Consequently , they skipped it . This was an unfortunate decision since six of the eight comma uses from the punctuation scenario needed information described on page 432 . <p> It should be noted that this was not the case for the visual shown in Figure 4 . Users observed the pattern , understood it , and commented favorably on its clarity . This led us to question why Figure 2 was ignored and Figure 4 was success . <p> Figure 4 . Successful single-purpose visual <p> Part of the answer we believe lies in the scanning behavior described in the previous section . Users scanned almost exclusively to match syntactic patterns in the examples . In Figure 2 , the most prominent feature of the visual is the base clause or main idea , " I make time to play outside . " The clauses or additional information that are added to the @ @ @ @ @ @ @ @ @ @ uncolored , and uncluttered main idea . Yet , it is precisely this subordinated material which users needed in order to match the syntactic structures they were seeking . Additionally , instead of having to examine one structure and then decide if it matched the pattern they were seeking , Figure 2 required that they consider at least five patterns in the same visual . Conversely , the more successful Figure 4 does not subordinate elements , and it does not attempt to combine multiple patterns into one visual . <p> The visual as shown in Figure 3 was even more problematic than Figure 2 , however . This is ironic because users were nearly unanimous in their positive comments about Figure 3 and how effectively they thought that the visual would help them to produce a properly formatted MLA works cited entry . However , this visual actually produced errors because it failed entirely to alert users to the problems faced when citing an article from a book , an edition , a corporate author , an edited collection , etc . Users were overwhelmed by the large font @ @ @ @ @ @ @ @ @ @ and failed to recognize that additional information would be required . Although once again the information they needed was provided , the treatment of the material in the visual misled users into thinking that they had adequately responded to the task . Users believed that , if they supplied the " Author 's Name , " " Title of the Book , " " Place of Publication , " " Publisher , " and " Year , " then they had provided all the data needed for the works cited entry , when , in fact , they had not . As a result , users commented positively on the visual ; yet , every single user failed the task . It would be unfair to assert that the visual in Figure 3 was entirely responsible for this failure , and the next section discusses other factors that appeared to lead users of both the Hacker and DK texts to produce incomplete works cited entries . However , asking the visual in Figure 3 to accomplish too much certainly appeared to contribute to the problem , and once again , @ @ @ @ @ @ @ @ @ @ signal to users that they were dealing with a complex problem resulted in positive user evaluations and poor user performance . <h> Users ' task environments can be improved with scenarios <p> The failure of both the Hacker excerpt and DK prototype on the works cited task , and users ' difficulties with commas , can all be partially attributed to incomplete understandings of users ' goals and task environment . It is obvious that improving an author 's understanding of the users ' needs is likely to result in more usable information products , hence the mantra " Know Thy User . " This is well understood . <p> What is often less well-understood is the role that authors play in actually constructing the use or task environment for users . All too frequently , we tend to think in terms of accommodation of users ' needs , and we tend to overlook the important role that authors and designers play in the construction of users ' task environment . Indeed , the movement away from theories of " user-centered design " in the 1990 's toward " user-experience @ @ @ @ @ @ @ @ @ @ between accommodation of users on the one hand and creation of user-experiences on the other . Successful texts and information products create roles and provide interpretive frameworks that users can deploy in order to successfully complete tasks and achieve their goals . <p> The evaluating source section of the DK prototype did this fairly successfully , and it did so mainly by using " If , Then " scenarios that users could play in order to understand some of the criteria that need to be taken into consideration when deciding whether or not a source is credible and relevant ( note : see Flowers , Hayes , and Swarts 's 1983 article " The Scenario Principle " for more details about this approach ) . Rather than attempting to provide linear , step-by-step procedures for judging the value of a source , the DK prototype used mini-stories to exemplify how the complex decision of whether a source is relevant can be made . In one of these " stories , " students named Pedro and Aaliyah are described . Pedro is writing a research paper for his class about social @ @ @ @ @ @ @ @ @ @ Aaliyah is researching viral marketing techniques for a marketing company . Pedro and Aaliyah are both asked to judge whether an online news story about how viruses spread electronically . The handbook guides users through the characters ' decision making processes to show why the same text is not appropriate for Pedro 's situational context on the one hand , and completely relevant to Aaliyah 's on the other . This section creates a clear role for users to play as they use the text and it also acknowledges the importance of context on decision-making , which the other sections of the handbooks did not adequately address or address at all . And because the users understand the role they are supposed to play , they also understand the logic they 're supposed to follow in order to make the decisions necessary to complete the task at hand . <p> This did not pertain , however , for either handbook in the MLA sections . Both texts failed because both asked users to play roles they could not adopt . Both texts assumed , erroneously , that users begin the @ @ @ @ @ @ @ @ @ @ works being cited and thus had a simple problem of choosing the format that matched the type of work . However , the users in this study did not understand the context and so did n't differentiate between types of works . Generally speaking , the users in this study did n't know , for example , what a corporate author was , and two of the users decided that a collection of readings was " a reference book . " If you , as a user , do n't know what a corporate author is , you 're not going to be able to use the information on Figure 3 of the DK prototype or the tables on pages 341 and 349 in Hacker 's handbook . <p> When users do n't know what decisions they need to make ( e.g. , how do I decide if I have a corporate author , government author , no author , etc. ) , then constructing a task environment like the one created in the evaluating sources section is a positive approach . The presentation of the information in terms of @ @ @ @ @ @ @ @ @ @ identify helps users understand where to begin the decision-making process . The authors should also ask questions as headers , such as " How do I quote or paraphrase in my text ? " or " How do I format an entry for a works cited , reference list , or bibliography ? " This would create the same " context of use " found in the evaluating sources section , and it allows the authors to build a decision matrix or some other tree-type structure that could be used by students and teachers to make decisions about what type of text they have so that they can make an informed judgment about the appropriate works cited pattern to use . <h> Conclusion <p> This study provides empirical evidence that validates Redish 's observation that " Ease of use-what we typically focus on in usability testing-is critical but not sufficient for any product " ( 2007 , p. 104 ) . The ease of use indicators in this study , both quantitative and qualitative , suggested that our users found the prototype handbook and its visuals to be an " @ @ @ @ @ @ @ @ @ @ client would very likely have been entirely satisfied with the ease of use data we provided . However , had the study 's design solely focused on what the client told us they wanted , we would have failed both our client and the users . The true complexity of the tasks would not have been revealed , the lack of " utility " in the handbooks would not have manifested itself ( Redish 2007 ) , and our clients would never have seen the need to " think outside the box " of traditional handbook design . They would have lacked the cognitive dissonance that motivated at least some of the creative thinking that ultimately led to the successful handbook they ultimately published . <p> Fortunately , because we were working with clients who trusted us not to run up the cost of our study needlessly , we introduced some additional elements to the research methodology used . As a result of these additional elements , we observed the following points : <p> It is important not to be swayed by clients and to ignore methodological triangulation . The @ @ @ @ @ @ @ @ @ @ ' performance was fortunate . Had it not been for the observation that users failed to have acceptable works cited entries , we would not have suspected that the problems were more complex than we had originally considered . <p> Testing the market-leader and the prototype with the same scenarios helped the researchers to distinguish problems with information delivery techniques from confusion over the complexity of the tasks . <p> Maintaining a singular purpose in visuals is important for success , even though complex information needs to be delivered . Use several visuals to convey different purposes . <p> Users need to be made aware of the complexity of a problem before any potential solutions are introduced so they can look for additional contextual factors and use a recursive decision-making process . <p> Modeling complex problem solving behaviors through the use of scenarios may lead to more effective performances from users when it 's simply not possible to capture or replicate all of the potential variables in a situational context ( Flowers , Hayes , and Swarts , 1983 ) . <p> As the field of usability studies grows and @ @ @ @ @ @ @ @ @ @ the needs of new clients in new fields , the likelihood of encountering complex problems masquerading as simple ones will increase . Our research methods need to accommodate this and ensure that the studies we design for new clients include content experts ' assessment of the quality of the users ' performance and , when possible , head-to-head comparisons with competing products . <h> Acknowledgements <p> The author would like to thank Wendy Howard who collaborated on the data collection and analysis portions of this study . Thanks also belong to Alicia Hatter , Ginny Redish , and Barbara Mirel for the insightful feedback that was critical in producing the final version of this article . 
@@105507102 @907102/ <h> Menu <h> International Standards <p> There are several useful international standards on UX-related topics . <h> Why are standards important ? <p> Standards are a formal agreement on a specific , detailed topic . They allow us to codify best practices or a set of requirements , and share them across industries , national boundaries and disciplines . <p> Some standards focus on processes , describing principles and making recommendations for how to achieve a results . Others are detailed specifications , and contain requirements that must be met . Both are useful in establishing a user-centered design process or in evaluating the usability of a product . <h> Getting copies of standards <p> Standards are generally sold by the organizations that publish them , or by a national standards organization . <p> The easiest way to locate a standards document is to look it up by number ( for example , ISO 13407 ) . <p> Human-Centred Design Process- ( ISO 13407 ) This standard is the basis for many UCD methodologies . It defines a general process for including human-centered activities throughout a development life-cycle . @ @ @ @ @ @ @ @ @ @ ( ISO 9241 ) This standard is the source of one of the commonly cited definitions of usability. 
@@105507106 @907106/ <h> This Issue <h> An Empirical Investigation of Color Temperature and Gender Effects on Web Aesthetics <h> Abstract <p> Limited research exists on the relevance of hedonic dimensions of human-computer interaction to usability , with only a small set of this research being empirical in nature . Furthermore , previous research has obtained mixed support for gender differences regarding perceptions of attractiveness and usability in Web site design . This empirical research addresses the above gap by studying the effects of color temperature and gender on perceptions of Web site aesthetics . A 2 x 2 between-subject research design manipulates the temperature of a Web site 's primary and secondary colors . Each color pair consists of adjacent hues and is categorized as either warm or cool . <p> Findings include significantly more favorable perceptions regarding a Web site design 's aesthetics when cool color combinations ( blue-light blue ) , as opposed to warm color combinations ( red-orange ) , are used ; direct effects of classical aesthetic dimensions ( e.g. , cleanliness ) on expressive aesthetics items ( e.g. , creativity ) ; and no effects of @ @ @ @ @ @ @ @ @ @ The following list summarizes practical take-aways that practitioners can get from this article : <p> Choose a Web site 's colors wisely as they will impact visitors ' impressions of its order and creativity . <p> The safest split-complementary color schemes ( in terms of influencing users ' impressions of a Web site 's aesthetic appeal ) are those with a cool primary color ( e.g. , blue ) for the top or global part of the page . Similarly , a cool secondary color is safer than a warm one . It should be noted that context is an overriding factor ; in certain cases warm color combinations may result in higher aesthetic appeal . <p> Using cool color schemes will create favorable impressions about the Web site 's design , which in turn may translate in building credibility and trust . <p> Web site designs that appear orderly are more likely to be also perceived as aesthetically pleasing and in turn more usable . <p> Even if designers are interested in producing creative , fascinating Web sites , they would be wise to consider orderly presentation given its @ @ @ @ @ @ @ @ @ @ . 98864 @qwx958864 <p> As the World Wide Web continues to grow in popularity , currently estimated to exceed 1.2 billion users ( Nielsen , 2005 ) , Web sites have become core extensions of a business practice rather than a consideration of a new channel ( Seethamraju , 2005 ) . Companies seek new insights on how to create more effective Web sites and entice online customers . Extensive literature exists on the acceptance of a new technology , but the former has centered on utility-related dimensions that drive this acceptance . Although research has been conducted on the aesthetic dimensions of interfaces since the mid 1980s , limited research exists on the explicit relevance of hedonic dimensions of human-computer interaction ( HCI ) to usability . An even smaller set of this research is empirical in nature ( Hoffman &amp; Krauss , 2004 ; Schenkman &amp; Jonsson , 2000 ; Zhang &amp; Li , 2005 ) . Current findings highlight the importance of aesthetics and specifically color in the scope of Web experience . For example , Tractinsky et al . ( 2000 ) empirically studied and showed @ @ @ @ @ @ @ @ @ @ interaction process , and that those evaluations are carried forward and may influence later perceptions of usability . Additionally , a few other studies have shown that the cool blue color schemes are associated with higher perceived credibility and trust levels ( Fogg et . al , 2001 ; Lee &amp; See , 2004 ; Zhang &amp; Li , 2005 ) . Furthermore , it appears that there are gender differences regarding perceptions of attractiveness , usability , and the consequent affective state of satisfaction in Web site design . Cyr and Bonnani ( 2005 ) studied the differences between genders ' preferences in the context of e-commerce . According to their study , the two genders experience e-commerce differently especially in terms of design . Other factors such as trust and satisfaction seemed to be equally assessed . Furthermore , Simon ( 2001 ) explored the differences among genders and cultures . Simon 's findings indicated that , in most cases , the perception of Web site attractiveness across genders was significantly different , especially in individualistic ( e.g. , North American ) cultures . However , more research @ @ @ @ @ @ @ @ @ @ and their effects on Web site aesthetics . Such an understanding can contribute to further insight regarding Web site usability and in turn goodness ( Hassenzahl , 2004 ) . Thus , this research aims to address the above need by studying the effects of color temperature and gender on the perceptions of Web sites aesthetics . <h> Literature Review <p> The following sections present aesthetics and gender differences in hedonic effects . <h> Aesthetics <p> Aesthetics is a subset of value theory that studies values , sometimes called judgments of sentiment or taste . Aesthetics is interlinked with the philosophy of art . It is considered to be a particular theory of the conception of beauty ; a particular approach to what is pleasing to the senses ( Hoffman &amp; Krauss , 2004 ; Kripintiris &amp; Coursaris , 2007 ) . People throughout the centuries have been highly interested in aesthetics . The appreciation of beauty is a classical quality that is applied to many aspects of life , such as senses , imagination , and understanding ( Lavie &amp; Tractinsky , 2004 ) . Aesthetics have been a @ @ @ @ @ @ @ @ @ @ schools of thought . They have been approached from many different angles and points of view . Aesthetics possess multiple meanings ( Lavie &amp; Tractinsky , 2004 ) . A commonality among aesthetics across the centuries is their dynamic nature . Beauty has been reformulated to address and reflect the propensities of the era to which it belongs . It has been observed and studied that aesthetic preferences of the present come to replace those of the past and so forth ( Lavie &amp; Tractinsky , 2004 ) . Tarasewich ( 2001 ) cites Eysenck ( 1983 ) who addresses two conflicting points on aesthetics . The first considers aesthetics as an objective quality that can be understood and shown to people . The second point of view sees aesthetics as something completely subjective and that beauty is a quality unable to be shown . However , Eysenck supports the concept that there is objectivity in aesthetic considerations . Also , some of his experiments provide insight regarding simple stimuli like shapes and color that may influence aesthetic judgments ( Tarasewich et al. , 2001 ; Eysenck , 1983 ) @ @ @ @ @ @ @ @ @ @ in the respective works of DeAngeli et al . ( 2006 ) and Lavie and Tractinsky . Lavie and Tractinsky describe this dyad as two distinct approaches of understanding aesthetics described as classical aesthetics and expressive aesthetics . Classical aesthetics are defined as aesthetic notions that " presided from antiquity until the 18th century " and " emphasize orderly and clear design . " Expressive aesthetics are defined as aesthetic notions that reflect a designer 's creativity and originality ( Lavie &amp; Tractinsky , 2004 ) . Nasar ( 1999 ) offers support for these two dimensions , but labels them visual clarity and visual richness respectively . <p> The relationship between classical and expressive aesthetics has received extremely limited attention . Lavie and Tractinsky ( 2004 ) point out that such a relationship is not predefined and that good design " should strive to balance their degrees given the design context . " On the other hand , the Bayesian model presented by Papachristos et al . ( 2005 ) offers support for the following relationships between Web site design attributes : A " pleasant " design affects perceptions @ @ @ @ @ @ @ @ @ @ and " modern " design , while an " attractive " design has a mediated effect on how " sophisticated " it is perceived to be . Several of these dimensions had been used in the operationalization of aesthetics by Lavie and Tractinsky , with " pleasant " falling under " classical " and " modern " measuring " expressive " aesthetics . Papachristos et al. , based on a similar study from Kim et al . ( 2003 ) , define their chromatic schemes after requesting actual users to characterize color combinations according to a set of emotional descriptors . Consequently , they gather the 12 most distinctive characterizations and they " formally " select ( as they mention ) a dominant and a secondary color scheme . Kim et al. , following a similar pattern , first brainstorm with " design experts , " and then survey Web users in order to identify 13 emotional dimensions . These attributes are found in the operationalization of the two aesthetics constructs by Lavie and Tractinsky , suggesting that classical aesthetics impact expressive aesthetics . Therefore , and with the caveat @ @ @ @ @ @ @ @ @ @ : <p> H1 . Higher levels of classical aesthetics will have a more positive effect on expressive aesthetics . <h> Aesthetics <p> Hedonic , derived from Greek where hedonism means pleasure , dimensions include factors such as color , graphics , animation , and other design elements that either implicitly or explicitly cause an affective state of pleasure . Zhang and Li ( 2005 ) argue that the more pleasing or attractive a Web site is , the easier it will be for the individual to learn how to use it and the more likely that this individual will continue to use it . Past studies have primarily looked at Web site design as the aggregate product of these hedonic dimensions and the users ' consequent affect . However , a closer look at the impact of each hedonic dimension on affect is warranted . <p> Empirical studies on the impact of color on the perceived attractiveness and usability of Web sites are extremely limited . Kim et al . ( 2003 ) reference Liu ( 2001 ) in their claim that " prior studies did not identify any quantitative @ @ @ @ @ @ @ @ @ @ studies focus on the role that aesthetics play in usability and treat color in an overly subjective and qualitative manner ( Brady &amp; Philips , 2003 ; Dittmar , 2001 ; Kim et . al , 2003 ; Papachristos et al. , 2005 ) . However , based on the limited number of empirical studies on the subject , it appears that color ( and more specifically color combinations ) has a significant effect on the perceived attractiveness and aesthetic appeal of a Web site . Brady and Philips ( 2003 ) suggest that users found a site with a triadic color scheme more usable and more aesthetically pleasing than a site with a non-standard color scheme ( note : a review of the color scheme used by Brady and Philips suggests their design to be split-complementary instead of triadic ; however , both are proximal techniques in combining colors ) . Their study was limited by its design in that it did not differentiate between color properties ( e.g. , hue , saturation , and temperature ) and their respective effects on users ' perceptions of usability and attractiveness @ @ @ @ @ @ @ @ @ @ color combinations and schemes resonate with users in a particularly emotional manner . Their research shows that users tend to predictably attach specific emotional descriptors , such as fresh , modern , friendly , and aggressive , to specific color schemes and color combinations . Results of their research further suggest that the design attribute with the strongest effect on the Web site 's perceived attractiveness is the brightness of the dominant color , followed by the brightness of the secondary color and its temperature ( warm or cool ) , the number of colors , and the contrast between hues . As a rough point of reference to color theory , warm colors include those that fall in the spectrum between red and yellow ( with orange as the secondary by-product ) , whereas cool colors encompass those that center around blue ( with green and purple as the secondary by-products ) ( Ohta &amp; Robertson , 2006 ) . <p> Based on the limited past empirical research , it is plausible to suggest that color , color schemes , and color combinations are variables dependant on other areas @ @ @ @ @ @ @ @ @ @ also possible to suggest , based on the work by Papachristos et al. , where cooler colors were found to be preferred over warmer colors , that the perceived temperature of a color impacts a Web site 's aesthetics . Thus , the following hypotheses are proposed for this study : <p> H2 . Increasing the color temperature of a Web site design will negatively impact users ' perceptions of its classical aesthetics . <p> H3 . Increasing the color temperature of a Web site design will negatively impact users ' perceptions of its expressive aesthetics . <h> Gender Differences in Hedonic Effects <p> Effects of Web design on affect have also been studied in the context of the users ' gender . While several studies have explored the relationships between trust , satisfaction , and the consequent loyalty to a Web site ( Anderson &amp; Srinivasan , 2003 ; Cyr &amp; Bonnani , 2005 ; Devaraj , Fan , &amp; Kohli , 2002 ; Szymanski &amp; Hise , 2000 ; Yoon , 2000 ) , very few studies have focused on the relationship between gender and Web site design @ @ @ @ @ @ @ @ @ @ 2001 ) . In the realm of visual design , men had more favorable impressions of how product information was presented . Women were more attracted by the colors on the site , and men by animations and the interactive , " flashy " aspects of the site ( Cyr &amp; Bonanni , 2005 ) . Simon ( 2001 ) found that women preferred sites that were less cluttered , having few graphics , as well as sites that avoid multiple levels of sub-pages to drill through . Men liked sites that used extensive graphics and animation . Additionally , in a study of gender and Web usage among college students , significant gender differences emerged with respect to evaluative criteria and use patterns , with men liking some of the " bells and whistles " and women using academic Web sites more ( Mitra et . al , 2005 ) . <p> It appears that there are gender differences regarding perceptions of aesthetics , usability , and the consequent affective state of satisfaction in Web site design , but more research is needed to understand the nature of such @ @ @ @ @ @ @ @ @ @ usability and acceptance , gender differences were explored in terms of Web site designs as an aggregate of multiple design elements instead of a more controlled design regarding these aesthetic factors . There is limited research that investigates the effects of gender on color preferences . Studies in various contexts have found that both men and women prefer the same color temperature ( i.e. , cool blue ) ( Dittmar , 2001 ; Guegen , 2003 ; Silver &amp; Ferrante , 1995 ) , but significant differences arose regarding their least preferred colors , where men " stated more often yellow and less often red as least preferred than women did " ( Dittmar , 2001 ) . Thus , the following hypothesis is proposed for this study : <p> H4 . Increasing the color temperature of a Web site design will have a more negative impact on women 's perceptions of its aesthetics than those of men . <h> Methods <p> The following sections present the experiment design and procedure , the subjects , and the instrument scales and validity . <h> Experiment Design and Procedure <p> This study @ @ @ @ @ @ @ @ @ @ is manipulated at four levels , each representing an increased level of overall color temperature for the Web site . To assist in operationalizing color temperature increases , a Web site 's design elements ( e.g. , logo , navigation bar ) were placed in two groups , herein referred to as primary and secondary . Primary layout elements refer to areas of the site which are specifically designed ( by the site designer ) to immediately attract the focus or attention of the user upon visiting the site or to contain primary information or content ( such as body text relating to the site 's intended purpose ) . These include , but are no means limited to , primary branding ( i.e. , logos , etc. ) , top level navigational elements ( i.e. , horizontal navigation bars ) , or primary content bearing columns or containers . Secondary layout elements refer to areas of the site which are not designed to convey vital information and include , but are no means limited to , hyperlinks , secondary or tertiary level text , form styling , and secondary @ @ @ @ @ @ @ @ @ @ the primary ( more dominant ) elements , the secondary elements , or both , the Web site 's overall color temperature would either increase or decrease . It is important to note that the terms primary and secondary do not refer to primary and secondary colors . Instead , the terms primary and secondary refer to design or layout elements of the site to which the specific color is applied . <p> Regarding color temperature levels , two sets of colors are selected from the color wheel , each being categorized as either warm or cool . A warm color refers to colors ranging between yellow to red-violet on the chromatic wheel . It is important to note that interaction between colors may cause a hue such as red-violet to appear warmer if it is placed next to a cold color , such as green , or colder if it is placed next to a warm color , such as orange . A cool or cold color refers to colors ranging between blue-violet and yellow-green on the chromatic circle . As with warm colors , interaction between colors may @ @ @ @ @ @ @ @ @ @ it is placed next to a warm color , such as red , or warmer if it is placed next to a cold color , such as blue . <p> A Web site was developed by the authors for the purpose of this study and represented the digital storefront of a fictional hotel . Four versions of the identical Web site design were produced with color ( temperature ) combination being the only varying design element ( see Figure 2 for corresponding screenshots ) . All other design elements ( e.g. , text , images , background ) were held constant across the four designs . Implementation of this design resulted in the following four split-complementary treatments or color combinations for the test Web site : ( a ) Warm Primary-Warm Secondary ( i.e. , #FF0000/#FF7F02 or 255.0.0/ 255.127.2 or Red/Orange ) , ( b ) Warm Primary-Cool Secondary ( i.e. , #FF0000/879ADC or 255.0.0/135.154.220 or Red/Light Blue ) , ( c ) Cool Primary-Warm Secondary ( i.e. , #3C4360/#FF7F02 or 60.67.96/255.127.2 or Blue/Orange ) , and ( d ) Cool Primary-Cool Secondary ( i.e. , #3C4360/#879ADC or 60.67.96/135.154.220 or @ @ @ @ @ @ @ @ @ @ , the logo and main navigation bar were designated as primary elements , while the form styling , secondary navigation ( hyperlinks ) , and image borders were designated as secondary elements . All other design elements ( e.g. , text , images , background ) were held constant across the four designs . This research design allowed for any differences found among the four groups of subjects to be attributed to the decreased levels of color warmth as a result of color choices for the primary and secondary colors of the Web site . <p> Tasks invoked participants to browse through a Web site developed for the purpose of this study in search of specific information . Participants were informed that the tasks were only meant to offer them an opportunity to explore the Web site and its design , instead of measuring their performance with it . Having evaluated the Web site design randomly assigned to them , participants were then asked to rank four different Web site designs in terms of their respectively perceived aesthetics . <p> A Structural Equations Modeling ( SEM ) technique , Partial @ @ @ @ @ @ @ @ @ @ the validity of both the structural and measurement model . Data analysis will speak to the four aforementioned hypotheses . <p> Figure 2 . Screenshots of the Web site 's designs from coolest to warmest ( clockwise , from top left ) <h> Subjects <p> A total of 356 subjects were recruited for this Web-based voluntary study via email announcements on various databases and electronic mailing lists . Of the 356 participants recruited , 328 usable data sets were collected , with a minimum of 72 subjects per group . All participants used the same Web site , but each treatment involved the use of a discriminant color design described above . The minimum sample size for the selected method , PLS , is 10 times the number of the most complex construct ( Chin , 1998 ) . In this study endogenous constructs consist of five items each , thus our sample size far exceeded the needed 50 cases . Each subject participated in only one treatment group , and assignment of subjects to groups was fully randomized to control for confounding effects due to differences in subject characteristics @ @ @ @ @ @ @ @ @ @ and females ( 170 males to 158 females ) . The average age was 34 ( ranging from 18 to 70 ) , and 83% described themselves as Caucasian/White ( while another 7% as Asian/Pacific Islander and another 10% fell under remaining categories ) . The participants were almost entirely college-educated and had an average experience of 17 years with computers and 10 years with the World Wide Web , respectively . ANOVA tests found no significant differences for subjects in the various treatment groups in terms of these control variables , thereby ensuring the successful randomization of assignment across groups . <h> Instrument Scales and Validity <p> The questionnaire used for data collection contains scales that measure the various constructs shown in the research model and are provided in Table 1 . All scales were adapted from a prior study ( Lavie &amp; Tractinsky , 2004 ) , which had established their reliability and validity , thereby satisfying content validity . These scales were used to measure the users ' perceived attractiveness of Web sites through assessments of classical aesthetics and expressive aesthetics . These 7-point Likert scales ( @ @ @ @ @ @ @ @ @ @ shared question " My perception of this Web site is that it is " for each of the following items : clean , clear , symmetric , aesthetic , pleasant for classical aesthetics , original , creative , fascinating , sophisticated , and uses special effects for expressive aesthetics . <p> When the questionnaire was conducted , items within the same construct group were randomized to prevent systemic response bias . Upon further testing it was shown that non-response , temporal , and common method biases were not present in our data set . The factor loadings for the total set of items used in this study are summarized in Table 1 . Shimp and Sharma ( 1987 ) , Carmines and Zeller ( 1979 ) , and Hulland ( 1999 ) suggest that an item is significant if its factor loading is greater than 0.7 to ensure construct validity . Adherence to this criterion required the modification of only one scale ( classical aesthetics ) through the removal of two items : ClasAes1 ( or clean ) and ClasAes2 ( or symmetrical ) . After the removal of the @ @ @ @ @ @ @ @ @ @ item-to-total correlation measure , where all items had higher measures than the 0.35 threshold suggested by Saxe and Weitz ( 1982 ) . <p> The square root of the variance shared between a construct and its items was greater than the correlations between the construct and any other construct in the model ( see Table 3 ) suggesting discriminant validity ( Fornell &amp; Larker , 1981 ) . Discriminant validity was confirmed by verifying that all items load highly on their corresponding factors and load less on other factors ( see Table 4 ) . Although the correlation between the two aesthetics constructs was quite high ( i.e. , 0.622 ) , a phenomenon also observed in the work by Lavie and Tractinsky ( 2004 ) , it is not exceedingly high according to Kline 's ( 1998 ) suggestion that correlations between factors should not be greater than 0.85 , thus further supporting the discriminant validity of the two aesthetic factors . <p> Table 3 . Correlation matrix and discriminant validity assessment <p> ITEM <p> ClasAes <p> ExprAes <p> ClasAes <p> 0.9851 <p> - <p> ExprAes <p> 0.622 <p> @ @ @ @ @ @ @ @ @ @ discriminant validity , which is the square root of the average variance extracted compared to the construct correlations . Bold values are supposed to be greater than those in corresponding rows and columns . <p> Table 4 . Matrix of loadings and cross-loadings <p> ITEM <p> ClasAes <p> ExprAes <p> ClasAes2 <p> 0.746 <p> 0.455 <p> ClasAes3 <p> 0.863 <p> 0.547 <p> ClasAes4 <p> 0.895 <p> 0.554 <p> ExprAes1 <p> 0.474 <p> 0.847 <p> ExprAes2 <p> 0.638 <p> 0.849 <p> ExprAes3 <p> 0.571 <p> 0.896 <p> ExprAes4 <p> 0.524 <p> 0.885 <p> ExprAes5 <p> 0.391 <p> 0.779 <h> Results <p> Following from the earlier discussion on the instrument 's validity , statistics regarding significant items and construct are reported in Table 5 ( on subsequent page ) . <p> The proposed structural model shown earlier in Figure 1 was tested by Jackknifing in PLS . This resampling procedure assesses the significance of PLS parameter estimates ( Chin , 1998 ) . Jackknifing is just one of several PLS techniques that may be used in evaluating a research model . For example , Bootstrapping is another common PLS approach , but @ @ @ @ @ @ @ @ @ @ ( Chin , 1998 ) . All three of the original hypotheses were supported as shown in Figure 3 , while Table 6 presents the validation of these hypotheses in more detail . Furthermore , the structural model tested using PLS demonstrated mixed explanatory power for perceived Web site aesthetics . With an R-square of 0.45 , 45% of the variance in expressive aesthetics was explained by both the color temperature effects and the classical aesthetics ( more heavily so ) in this study . Only 3.2% of the variance for classical aesthetics was explained by this manipulation , suggesting that there are other dimensions not captured by the scale ( in part explained after the removal of two items ) , by the exogenous construct 's effects , or both . <p> The next measurement pertains to the ranking of the different Web site designs . Rankings were significantly different ( one-sample T-test ) suggesting a preference for blues or the cool-cool color design ( see Table 7 ) . <p> Table 7 . Web site rankings ( of perceived aesthetics ) and one-sample of comparison of means ( @ @ @ @ @ @ @ @ @ @ aesthetic and 4 or least aesthetic ) <p> Color temperature ( primary-secondary ) <p> N <p> Mean <p> Std . deviation <p> Std . error mean <p> t <p> Df <p> Sig. ( 2-tailed ) <p> Warm-warm <p> 328 <p> 2.97 <p> 1.171 <p> 0.065 <p> 45.956 <p> 327 <p> 0.000 <p> Warm-cool <p> 328 <p> 2.62 <p> 0.914 <p> 0.050 <p> 51.959 <p> 327 <p> 0.000 <p> Cool-warm <p> 328 <p> 2.34 <p> 0.980 <p> 0.054 <p> 43.326 <p> 327 <p> 0.000 <p> Cool-cool <p> 328 <p> 2.00 <p> 1.135 <p> 0.063 <p> 31.971 <p> 327 <p> 0.000 <p> The fourth hypothesis stated that " increasing the color temperature of a Web site design will have a more negative impact on women 's perceptions of its aesthetics than those of men . " ANOVA test results suggested that there were no significant differences in the reporting of both classical and expressive aesthetics scale items ( see Table 8 ) , as well as in the ranking of the four Web site designs ( see Table 9 ) . Thus , gender does not appear to be related to users @ @ @ @ @ @ @ @ @ @ combinations in the context of hotel Web sites . However , the hypothesized directionality becomes apparent when contrasting the p-values for the two cool Web site designs ( i.e. , third and fourth design with p-values above 0.88 ) with the two warm Web site designs ( i.e. , first and second design with p-values below 0.30 ) . Thus , gender differences regarding color preferences at the warmer end of the spectrum may occur , although the findings of this study do not offer such support . <p> Table 8 . ANOVA for relationships between gender and aesthetics ( classical and expressive ) <p> Item <p> - <p> Sum of squares <p> df <p> Mean square <p> f <p> Sig . <p> ClassAes2 <p> Between groups <p> 1.451 <p> 1 <p> 1.451 <p> .707 <p> .401 <p> - <p> Within groups <p> 669.573 <p> 326 <p> 2.054 <p> - <p> - <p> - <p> Total <p> 671.024 <p> 327 <p> - <p> - <p> - <p> ClassAes3 <p> Between groups <p> 3.280 <p> 1 <p> 3.280 <p> 2.178 <p> .141 <p> - <p> Within groups <p> 491.110 <p> 326 @ @ @ @ @ @ @ @ @ @ <p> 494.390 <p> 327 <p> - <p> - <p> - <p> ClassAes4 <p> Between groups <p> .233 <p> 1 <p> .233 <p> .175 <p> .676 <p> - <p> Within groups <p> 435.617 <p> 326 <p> 1.336 <p> - <p> - <p> - <p> Total <p> 435.851 <p> 327 <p> - <p> - <p> - <p> ExprAes1 <p> Between groups <p> .435 <p> 1 <p> .435 <p> .179 <p> .673 <p> - <p> Within groups <p> 793.001 <p> 326 <p> 2.433 <p> - <p> - <p> - <p> Total <p> 793.436 <p> 327 <p> - <p> - <p> - <p> ExprAes2 <p> Between groups <p> 4.632 <p> 1 <p> 4.632 <p> 1.899 <p> .169 <p> - <p> Within groups <p> 795.307 <p> 326 <p> 2.440 <p> - <p> - <p> - <p> Total <p> 799.939 <p> 327 <p> - <p> - <p> - <p> ExprAes3 <p> Between groups <p> .027 <p> 1 <p> .027 <p> .011 <p> .916 <p> - <p> Within groups <p> 773.912 <p> 326 <p> 2.374 <p> - <p> - <p> - <p> Total <p> 773.939 <p> 327 <p> - <p> - <p> - <p> ExprAes4 <p> Between @ @ @ @ @ @ @ @ @ @ .852 <p> - <p> Within groups <p> 790.696 <p> 326 <p> 2.425 <p> - <p> - <p> - <p> Total <p> 790.780 <p> 327 <p> - <p> - <p> - <p> ExprAes5 <p> Between groups <p> 1.286 <p> 1 <p> 1.286 <p> .674 <p> .412 <p> - <p> Within groups <p> 622.199 <p> 326 <p> 1.909 <p> - <p> - <p> - <p> Total <p> 623.485 <p> 327 <p> - <p> - <p> - <p> - <p> Table 9 . ANOVA for relationships between gender and aesthetics rankings of four Web site designs ( i.e. , color temperature combinations ) <p> Item <p> - <p> Sum of squares <p> df <p> Mean square <p> F <p> Sig . <p> Rank of site 1 <p> Between groups <p> 2.280 <p> 1 <p> 2.280 <p> 1.665 <p> .198 <p> - <p> Within groups <p> 446.473 <p> 326 <p> 1.370 <p> - <p> - <p> - <p> Total <p> 448.753 <p> 327 <p> - <p> - <p> - <p> Rank of site 2 <p> Between groups <p> .931 <p> 1 <p> .931 <p> 1.115 <p> .292 <p> - <p> Within groups <p> 272.191 @ @ @ @ @ @ @ @ @ @ <p> Total <p> 273.122 <p> 327 <p> - <p> - <p> - <p> Rank of site 3 <p> Between groups <p> .004 <p> 1 <p> .004 <p> .004 <p> .949 <p> - <p> Within groups <p> 314.066 <p> 326 <p> .963 <p> - <p> - <p> - <p> Total <p> 314.070 <p> 327 <p> - <p> - <p> - <p> Rank of site 4 <p> Between groups <p> .027 <p> 1 <p> .027 <p> .021 <p> .886 <p> - <p> Within groups <p> 420.970 <p> 326 <p> 1.291 <p> - <p> - <p> - <p> Total <p> 420.997 <p> 327 <p> - <p> - <p> - <h> Conclusion <p> The findings of the present study support and extend prior research regarding the effect of color combinations on aesthetics ( Brady &amp; Philips , 2003 ; Papachristos et al. , 2005 ) . First , color temperature variations on Web site designs appeared to impact both sets of aesthetic dimensions ( i.e. , classical and expressive ) . Second , the split-complementary color schemes that utilized a cool primary color ( blue ) for the top or global part of the @ @ @ @ @ @ @ @ @ @ blue ) or a warm color ( orange ) for the secondary page components provided the color balance that users found most aesthetically pleasing . In contrast , the site that combined both a warm primary color ( red ) and a warm secondary color ( orange ) was the least aesthetically pleasing . The current results suggest that designers need to carefully consider color choice as the combinations will convey information about the quality of the site that may not be intended . <p> Furthermore , while there was limited literature regarding the hypothesis that classical aesthetics directly impact expressive aesthetics , this study offers strong support for this relationship . A significant implication to management arises : by ensuring Web site designs adhere to fundamental design principles and guidelines , thus satisfying more objective aesthetic dimensions , such favorable impressions will also influence perceptions of more affective aesthetic dimensions ( e.g. , originality , creativity ) . Consequently , this is a research area that warrants further investigation . Therefore , we can extend Tractinsky 's et al . ( 2000 ) suggestion that " what is beautiful @ @ @ @ @ @ @ @ @ @ is beautiful and in turn usable " bearing in mind that context of use is the overriding factor that influences perceptions of order and beauty . <p> While other research has found gender effects in several computer-related contexts ( Cyr &amp; Bonanni , 2005 ; Simon , 2001 ) , the current study did not indicate that gender impacted perceptions of Web site aesthetics . One plausible explanation for this observation is that women tend to employ more exhaustive information processing strategies than men do , which means that gender differences may have been masked by the lack of detailed content in the prototype Web site ; the content was not as extensive as users expected from a travel Web site ( Meyers-Levy &amp; Maheswaran , 1991 ; Simon , 2001 ) . We plan to expand and hone the Web site content to create a more realistic level of detail on each page , as well as have more content pages , which would enable users to better assess perceived usability within the context of the multiple color schemes . Additionally , future research efforts will seek to broaden @ @ @ @ @ @ @ @ @ @ of Web site aesthetics through a global multi-country study . <p> In closing , this study aimed at extending the limited body of research ( DeAngeli et . al , 2006 ; Lavie &amp; Tractinsky , 2004 ; Tarasewich , 2001 ) in the area of Web site aesthetics and our understanding of how the manipulation of design elements ( here , color temperature and color scheme ) may impact users ' perceptions of a Web site 's aesthetics . <h> Recommendations for Future Research and Conceptual Development <p> The next step on this research agenda is to study the effects of Web aesthetics on perceived usability . The authors will specifically seek to identify the relative importance of classical versus expressive aesthetics on the perceived efficiency and effectiveness of a Web site 's design . The second step will be to engage in a multi-country , cross-cultural study that will attempt to gain support for the generalizability of these relationships beyond a U.S. audience . <p> At a higher level , this research stream will expand on currently limited insight on the relationship between aesthetics and usability , i.e. @ @ @ @ @ @ @ @ @ @ design choices . <h> Acknowledgements <p> The authors would like to thank the reviewers for their insightful feedback that was critical in producing the final version of this article . 
@@105507108 @907108/ <h> This Issue <h> Metaphor-Based Design of High-Throughput Screening Process Interfaces <h> Abstract <p> This paper describes work on developing usable interfaces for creating and editing methods for high-throughput screening of chemical and biological compounds in the domain of life sciences automation . A modified approach to metaphor-based interface design was used as a framework for developing a screening method editor prototype analogous to the presentation of a recipe in a cookbook . The prototype was compared to an existing screening method editor application in terms of effectiveness , efficiency , and satisfaction of novice users and was found to be superior . 98865 @qwx958865 <p> Metaphors can be a powerful tool for guiding interface design in specific domains . They enable users to map knowledge from a familiar source domain to an unfamiliar target domain , thus they are particularly useful for novices . <p> The combination of formalized CTA methods with existing frameworks for developing metaphor-based interfaces can be helpful in developing , evaluating , and refining an appropriate metaphor . <p> Mismatches between software environments and the real-world analog defining a design metaphor can be effectively @ @ @ @ @ @ @ @ @ @ references . 
@@105507113 @907113/ <h> This Issue <h> Switching Between Tools in Complex Applications <h> Abstract <p> Large software applications are made up of many specialized tools . In Microsoft Word the document editor is supported by tools to create and fix drawings and tables . Programming environments have custom views ( difference editors ) and analyses ( performance reports ) to help developers make robust code . Every application has tools to help users sift the documentation . <p> In usability , we usually test a tool at a time , yet complex work requires many tools , and this brings a new set of issues . How do I know when I should be using a different tool ? What tool do I need when the one I am using is not working ? How do I get to it ? How quickly can I start using it ? <p> In complex or creative work , our observations show that users seldom choose the correct tool as soon as work progress dictates . This erodes productivity and creativity and is a prime target for improved designs . <p> Usability practice needs @ @ @ @ @ @ @ @ @ @ highlight tool switch events for study . This paper describes one that supports the trained observers on which User-Centered Design relies to detect problems and causes , and evaluate design changes . 98865 @qwx958865 <p> Methods presented in this paper serve as a foundation for the following : <p> Comparison of users ' performance in complex tasks by their choice of workflow path elements ( tool use ) and strategies ( tool choice and decision timing ) . <p> A set of baseline data against which design changes in a specific area of MATLAB- , or any other complex software toolset , can be evaluated for their impact on tool switching behavior , alongside their effect based on standard usability metrics. 98864 @qwx958864 <p> User-Centered Design for tool switching is a big topic , and we 're just beginning to work out a technique for gathering , examining , and evaluating data . A study of users moving from tool to tool ( and the design changes intended to improve it ) will not fit easily into a usability practitioner 's normal practice . This work generates large amounts of @ @ @ @ @ @ @ @ @ @ reduce . Tool switching appears in time patterns that the facilitator is too close to the process to observe . Critical tool changes ( those that merit immediate analysis ) ca n't always be identified at the moment they happen , and when they are identified , retrospective analysis ( of a recording ) is necessary to evaluate them . To improve design in this area , new techniques based on an understanding of the problems tool switching gives users are needed . <p> Conventional usability practice is organized around tasks , because a task framework supports observation , testing , and measurement . Work is sectioned into testable units that are concrete steps with quantifiable goals . This tacitly assumes that chaining the right steps in the proper order adequately models how work is actually done . In fact , only simple and repetitive work can be reduced to such a step-by-step model . The production line model is not adequate for complex or creative work . These are not production tasks . When starting them , users have only the most general idea what the workflow will be @ @ @ @ @ @ @ @ @ @ work . <p> Individual tools in Visual Studio , Photoshop , and Word ( toolkits ) may work well on their own , but not in concert or in sequence . The following inefficiencies arise when users move between tools : <p> Time and effort involved in transition ( switching ) does not advance the task . <p> Moving work from tool to tool dissipates focus . <p> Information is difficult to transfer when focus changes . <p> The right tools are n't used if they ca n't be smoothly integrated into users ' work . <p> Tool switching is the task of choosing and changing from one tool to another . Tool switching work is not productive-it does not alter the product . Users switch to increase productivity by applying whatever is , at the time , the most productive tool . But gains from using the new tool must offset losses from the work of switching . <p> Switching between tools adds users ' responsibilities to the production-line , step-by-step task model : monitoring task progress , deciding when to change tools , and choosing the best next @ @ @ @ @ @ @ @ @ @ successfully applying these functions as well as usable individual tools and planned workflows . Proficient toolkit users apply tools in different ways , with different frequency , in a different sequence , at different times . Some must be better than others . Their switching patterns should be reinforced with appropriate improvements in design . <p> Usability and productivity of software toolkits peak when users switch to the right tool at the right time , every time and steadily focus on the work , not the tools . <h> The problem <p> For a complex application ( toolkit ) , the best design results in timely application of correctly chosen tools , each for as long as it remains the best choice . This means not only that we optimize individual tools , but also ( a ) minimize the labor of switching , ( b ) facilitate correct tool selection , and ( c ) arrange switching at the right time . Options a and b imply either planning of tool use , monitoring of task progress , or both to ensure that correct and timely choices be made @ @ @ @ @ @ @ @ @ @ neither option b nor c . <h> Long term goal <p> Tool switching has the following three aspects we would like to improve with better design : <p> Accomplish each switch and choice smoothly and efficiently . Software 's effectiveness suffers when users move between tools . Time is lost . Energy is wasted . Focus is dissipated . Information is difficult to transport from one tool to another . <p> Encourage tool change at the right point in the task . Transitions between tools can occur too soon ( resulting in inefficient extra usage of the subsidiary tool ) or too late ( time and effort ineffectively spent before transition ) . Users need design support to make these decisions more effectively . <p> But we must walk before we can run . First we must be able to compare and evaluate patterns of tool choice and switch timing and determine critical incidents for study in realistic ( complex ) usability tasks or in actual work . Then we will know where to focus design effort and be able to measure how well our new designs foster more effective @ @ @ @ @ @ @ @ @ @ Before we get into analysis , and then into design ( UCD practitioner ) or experimentation ( academic ) , we must ask , can we compare tool switching and patterns of switching effectively ? This takes a lot of specific data and fresh ways of looking at it . The amount of raw data needed to study the subtasks and transitions that make up a complex " real " task can overload normal usability techniques . Test observations and facilitator 's notes do not suffice . We need " unattended data capture for portions of a long-term evaluation , used along with observations and interviews " ( Redish , 2007 , p. 107 ) . This data is useless without effective tools and metrics that make it possible to study and to compare the patterns of use and prioritize the design effort . <h> Simple and complex tasks <p> In simple tasks , work steps and tool choices are known beforehand . Deviations from expected path and usage are very likely to be errors or inefficiencies . In a complex task , the workflow is not known ahead of @ @ @ @ @ @ @ @ @ @ driven by unique or unfamiliar problems users encounter . In response , users apply tools chosen from the set they are aware of as best they can . To pose a complex task , we either ( a ) give users a problem without a formulaic solution ( e.g. , a programming problem ) or ( b ) give users a simple task amid a large array of possible tools ( e.g. , plotting and formatting data ) . Complex applications evolve to facilitate complex tasks . <h> Background <p> Kuutti says , " The focus of mainstream HCI research is narrow , covering most adequately the area of error-free execution of predetermined sequences of actions " ( 1996 , p. 37 ) . Theoretical formulations that describe users ' engagement with complex tasks have not been coupled with workable evaluative techniques . <h> Usability testing limitations <p> Literature on testing of tool switching , because of its specific focus ( multi-tasking ) and limited scope ( complex software ) , is sparse . To one side is conventional usability work ( Dumas &amp; Parsons , 1995 ; Rubin , @ @ @ @ @ @ @ @ @ @ , 1999 ; Card and Henderson , 1987 ) , in which the analysis of simple tasks remains within the production line paradigm . This serves because workflows are known and tool options limited or equivocal . To the other side we find ambitious projects ( Kline &amp; Saffeh , 2005 ; Singer , Lethbridge , Vinson , &amp; Anquetil , 1997 ; Weiderman , Habermann , Borger , &amp; Klein , 1986 ) proposing to manage the entire software engineering life cycle . These papers are non-starters . They recognize the need for this kind of study without proposing strategies , let alone techniques , to gather and manage the enormous amount of data and observations that User-Centered Design requires . <p> Studying user-driven workflows is daunting . " Real " multi-tool tasks have an untestably large number of valid workflows , branching with every opportunity for tool change . Complexity of creative tasks and idiosyncratic work practices pull together the components of integrated development environments ( IDEs ) into what are , in effect , unique configurations each time they are used ( Sy , 2006 ; Redish @ @ @ @ @ @ @ @ @ @ , 1985 ; Leich , Apel , Marnitz , &amp; Saake , 2005 ) treat IDEs as single entities with fixed workflows by evaluating them with simple tasks despite the fact that each user applies the components differently . <p> Tool choice and switch timing , when they are considered at all , are taken as either an outcome of training or acquired expertise ( Card , Moran , &amp; Newell , 1983 ; Kuutti , 1996 ) , or the result of the user 's awareness of the task and the environment ( Engestrom , 1999 ; Suchman , 2007 ) . To date , neither approach has produced results that nourish effective design strategies for toolkits , which is what we are after . <p> Theories ( B++dker , 1996 ; Engestrom , 1999 Green &amp; Petre , 1996 ; Kuutti , 1996 ; Suchman , 2007 ) attempt to clarify the problem by more subtle description of what we observe without sufficient data and concrete examples to make them usable for the practitioner . This body of work sustains itself on elaborate schemas and thoughtfully studied examples @ @ @ @ @ @ @ @ @ @ when schemas conflict , debate collapses into whose example is more relevant . <p> Activity theory maintains that experienced users move unconsciously from step to step in a learned process until a problem ( contradiction ) intrudes , at which point the user is open to change and improvement of the process . Suchman and others suggest that step choices are " situated " , that is , based on factors in the current problem state . Tension between these views remains because neither has been tested to determine the extent and actual conditions of its validity . Klein ( 1999 ) , in a series of detailed interview studies of situated decisions and actions , gives evidence that the situation driven thinking described by Suchman is typically evoked by emergency or crisis situations ( contradictions that cause process breakdowns ) . Klein 's detailed study , drawing from many subjects engaged in widely different complex tasks , suggests that these two stances may be the same underlying behavior observed under different conditions . <p> The question of whether the " layered task execution choices " ( Wild , Johnson @ @ @ @ @ @ @ @ @ @ monitoring , and decision making work can be supported , at least in part , by a user interface has also not been systematically tested . If transitions are marked and measured , their effects can be separated from the effects of the tools , and both can be studied in the context of complex tasks . The framework and vocabulary used by activity theory to describe how workflows change over time with experience provides a workable starting place . The issue of tool switching needs this depth of information to drive the design of better tools and toolkits . <h> Theoretical formulations of complex tasks <p> Activity theory calls the use of a tool an operation ( Card et al. , 1983 ; Kuutti , 1996 ) . For the novice , an operation is a discrete series of actions , each of which must be , at first , chosen and planned . With practice , the need for planning and choice diminishes , and a chain of actions becomes one smooth operation . A tool works well if it allows productive work on an object . Two @ @ @ @ @ @ @ @ @ @ productive work : <p> Breakdowns occur when work is interrupted by something ; perhaps the tool behaves differently than was anticipated , thus causing the triggering of inappropriate operations or not triggering any at all . In these situations the tool as such , or part of it , becomes the object of our actions . A focus shift is a change of focus or object of the actions or activity that is more deliberate than those caused by breakdowns . Now the operations that she normally does become actions to her ( B++dker , 1996 , p.150 ) . <p> Breakdowns and focus shifts , although different in character , result from contradictions : <p> Activity theory uses the term contradiction to indicate a misfit within elements , between them , between different activities , or between different development phases of a single activity . Activity theory sees contradictions as sources of development ( Kuutti , 1996 , p. 34 ) . <p> These are opportunities for changing tools and opportunities for learning and , potentially , opportunities for reworking and improving the chain of actions that activity theory @ @ @ @ @ @ @ @ @ @ to activity theory , an expert beginning an operation initiates a preset chain of actions . The operation 's internalized chain of action is a form of planning . Contradictions result when these plans fail . In activity theory terms a contradiction signaling a breakdown or focus shift should occur at any point where the tool is not " doing its best . " The contradiction alerts the user that the current tool or its usage is ( at the moment , at least ) suboptimal . The user must then decide whether to change tools or persevere . <p> These plans ( chains of actions ) may develop from experience ( trial and error ) or they may result from training , or simply arise from users striving to implement best practices . The result is the same . Actions proceed without evaluation until a contradiction intrudes . <h> The importance of tool switching <p> As soon as task success includes an aesthetic evaluation or quality goal , Kuutti 's definition of success as " error-free " must be replaced by " optimal " or at least " satisfactory @ @ @ @ @ @ @ @ @ @ of adequate quality , performed at an acceptable rate . <p> Having assembled actions into an operation that may be carried out without considering what to do next , the expert must remain aware of the quality of the work and the rate of progress in order to detect problems and act on them . These problems include not only contradictions , but also inefficiency from using the wrong tool or missing information . Any or all of these problems may require modification of the sequence of actions or a search for a more effective tool . It is precisely in this area that theories clash . Suchman warns that the planned selection of actions of an expert 's operation do not account for choices and decisions made in specific situations that advance the work , but that they result from the user 's engagement with the task(s) and could not be planned in detail in advance . Activity theory holds that operations disintegrate into actions separated by evaluation and choice when the operation encounters a contradiction , problem , or difficulty . <p> Unfortunately , " expert users , @ @ @ @ @ @ @ @ @ @ of breakdown of their expectations , because they happen in the unconscious middle phase of an action " ( Raeithel &amp; Velichovsky , 1996 , p. 201 ) . In other words , the expert is unaware of any monitoring process until the operation breaks down , so the expert feedback reports only on the problem solving activity that it collapses into . We have no cognitive description of " checking for breakdown . " As a practical matter we are left with the following : <p> Operations are carried out with ( novices ) or without ( experts ) deliberation between actions until users detect contradictions ( flaws in the state of the work ) . At this branch-point adjustments may or may not be made , depending on what tactics appear to be available or applicable . <p> The signals that users detect may originate in the workflow , or from the user 's monitoring of the work , or from a plan , or from an interruption by a co-worker or software agent . Detect means that users take themselves out of flow and choose the next @ @ @ @ @ @ @ @ @ @ , users ' experience with the task , users ' experience with the tools , and the user interface ( UI ) . <p> Tool switching is the only contradiction-generated events that can be reliably detected . We know that a contradiction has been considered and a choice made . <h> Contradictions and interruptions <p> Activity theory is a theory of learning . Contradictions " are not the same as problems or conflicts . Contradictions are historically accumulating structural tensions within and between activity systems " ( Engestrom , 1999 , p. 3 ) . For our purposes it is not important , at least initially , to distinguish between contradictions whose resolution generates a new form of operation and interruptions that cause the user to notice that the workflow has bogged down . Our interest begins when any event initiates consideration of a tool change tactic . <p> Interruptions have received more concrete study than contradictions . Mark , Gonzales , and Harris ( 2005 ) call interruptions from within the working sphere " interactions , " those from outside the working sphere are " disruptions " that reflects @ @ @ @ @ @ @ @ @ @ ( 2004 ) show that the cognitive impact of interruption varies according to which stage of the task ( beginning , middle , end ) it occurs . Cutrell , Czerwinski , and Horvitz ( 2000 ) attribute more impact to interruption of chunking behaviors ( highly focused subtasks ) than to interruptions happening between them . Speier , Valacich , and Vessey ( 1997 ) demonstrate that the disruptive impact of an interruption increases with the complexity of the task . These definitions and characterizations are useful in describing what is observed in the tool switching process . <h> Methods of observation <p> Studies of multi-tasking and task switching in the literature provide a framework for categorizing data and controlling the test environment . A general taxonomy for modeling multitasking ( Wild et al. , 2004 ) enumerates possible sources of error and distortion in our test environment and protocol . The test environment provides that test participants do not inhabit different roles ( developer , manager , leader , or contributor ) , that there are no interruptions from outside the task , that there are no proximate @ @ @ @ @ @ @ @ @ @ there are no events hindering one tool in favor of another , and that there is no change in work environment . The following are the most important points to consider : <p> No distinct goals separate operations and actions into different but interwoven , workflows . Tasks in this study have a single goal . No other tasks compete for attention . <p> The character of the goal and how it affects activity does not differ from participant to participant . The goal in this study " to finish as many subtasks as possible in the allotted time " engages all equally . <p> Compared to most multi-tasking studies , usability testing is clean-room control . <p> Learning about tool switching begins with an accurate picture of what actually happens . So , to perform the necessary basic studies , we need a technique that extracts and summarizes pertinent ( to tool changes ) data and metrics from testing and focuses on critical incidents with little or no " dog work " to encumber the practitioner 's day job . <h> Logging and reporting <p> Microsoft used silent logging @ @ @ @ @ @ @ @ @ @ Harris , 2008 ) . A web site , Hackystat , provides an open source framework that enables anyone to log their own activity on a remote server and retrieve regular reports . <p> Recording operating system focus and user input captures time spent with each tool and an indication of the activity . The log record 's time signal indexes into a recording of screen activity and users ' commentary ( if any ) . Additionally , success in tasks and subtask steps can be measured using completion , time to completion , user satisfaction , and the System Usability Scale ( Brooke , 1996 ; Tullis &amp; Stetson , 2004 ) . We use Techsmith 's Morae- , which captures some tool switching information , to record screen and voice . <p> A Java applet logs window activity and user input from within MATLAB- capturing all activity that can be precisely distinguished . A Visual Basic for Applications ( VBA ) script on the back end of the logger produces timeline pictures of tool use and changes , and statistical descriptions ( counts and charts ) that summarize @ @ @ @ @ @ @ @ @ @ paper . <h> How to read the charts <p> Task 1 is a problem without a formulaic solution ( e.g. , a programming problem ) . Task 2 is simple plotting task amid a large array of possible tools ( e.g. , plotting and formatting data ) . In the programming Task 1 , users create , step-by-step , a function or script that can find palindromes in text . They employ very few of the available tools . The complexity in this case lies in deciding when to switch from one tool to another . In the plotting Task 2 users load data from a file and create plots for export or printing . In this case the complexity is tool choice ; there are many ways to accomplish Task 2 in MATLAB- . <p> We recorded 14 users doing each of the two tasks . This gave a feeling for the variation in tool use across users ( see below ) and a baseline against that we can measure our next set of design changes . The charts show the user 's progress through the task in terms @ @ @ @ @ @ @ @ @ @ of tool changes so as to give a quick , overall picture of what tools were applied , when the tools were applied , and how long the tools were applied . The following is a description of how to read each line in the figures that follow : <p> The x-axis is a timeline in minutes . Each plot begins when the user finishes the first reading of the task and begins to work with the software . <p> Row 1 ( the bottom row ) contains the helper tools that support both the command window and the editor ( file browser , several ways of looking at variables , and a command history ) . It 's a measure of the simplicity of the programming task that they get so little use . <p> Rows 2 and 3 ( from the bottom ) of the timeline charts show the ( legacy ) Command Window ( row 2 ) , from which MATLAB- was originally run , and the Editor ( row 3 ) , where users write more elaborate code to manipulate the same component mathematical functions . @ @ @ @ @ @ @ @ @ @ 's capability . Working between them and using each most effectively has been difficult for users from the start . <p> Row 4 is Help and Documentation . Switching to help is present ( in varying degrees and with different severity ) at all levels of expertise , and in every task . Users ' aversion to this instance of tool switching is well documented ( Grayling , 2002 ) . We anticipate new design work in this area . There is no problem accumulating large amounts of data . <p> Success of the help-seeking step is ( relatively ) easy to measure . ( The user 's evaluation is likely to be accurate . ) <p> The contradiction that provokes switching from a work-piece to help is comparatively easy to observe " users discover a need for information that they must then search for " and therefore to measure and evaluate . <p> Row 5 ( the top row ) shows a group of GUI tools used for plotting , including the window where the actual plot appears . <p> This visual overview of test progress allows us to @ @ @ @ @ @ @ @ @ @ a description of tool use that we can compare with tests of the same task using other designs . <p> Figure 1 . User 6 working through the programming task ( top ) and the plotting task ( bottom ) . <p> Figure 1 shows User 6 doing Task 1 and then Task 2 , using different tools for each task . In Figure 2 , user 2 plots with the same toolset user 6 used for programming . User 2 's style ( in the period from 10:00 to 50:00 ) demonstrates that user 6 might have done the plotting task with the same tool use tactics used to complete the programming task , but chose not to . <h> Interpreting the timelines <p> With practice , we can pick out problem areas that might have been overlooked as the test was running . For example , the brief visit to the documentation that user 6 made at about 22:30 into the plotting task ( see Figure 1 ) resulted in misleading information , requiring another visit to the documentation a minute later . Of course , the cause @ @ @ @ @ @ @ @ @ @ . But investigating brief visits to documentation is almost always valuable from a usability viewpoint . When the trip is successful you can take the time to figure out what worked . We see in the timeline when they must repeat the visit ( usually because they have not been able to bring back enough information ) . <p> Long visits to helper tools ( row 1 ) may also be of interest . For example , why is user 6 spending so much time in the Array Editor ? In addition , the statistical data on which this picture is based will quickly show whether other users also spend large amounts of time with a relatively non-productive tool . But the main value of the timelines here is their depiction of different users ' tool strategies . <p> All of this paper 's key questions appear in Figures 1 and 2 . The tools ( level 5 in the timeline ) user 6 applied to the plotting task are designed for plotting . User 6 completed 10 of 13 segments of the task with them . User 2 applied @ @ @ @ @ @ @ @ @ @ segments in about the same length of time . <p> We might say that user 6 was where we wanted him to be ( in terms of tools ) , and user 2 was not . Yet user 2 's Standard Usability Scale ( SUS ) score was almost twice that of user 6 ( 72 to 40 ) . In relative terms , then , user 2 was comparatively unaware of how poorly the tools he chose were performing . To compound the problem , user 6 did much less well on the programming task than on the plotting task , finishing only one segment , yet his SUS score for that task was 50% higher than for the plotting task ( 57.5 to 40 ) . <p> The timeline view enables flagging of critical points in the testing ( previous section ) , and grouping of workflows by patterns or styles , as shown here and in the discussion . The following sections present the limitations of the current version . <h> Drawbacks and advantages of the logged data <p> This automatically captured data provides us with information @ @ @ @ @ @ @ @ @ @ other way , but it is neither perfect nor complete . We can make most effective use of it by remaining alert to the following limitations : <p> We can not identify and measure periods of inactivity . We ca n't say whether users are puzzled by the work , waiting for the software , or sipping coffee . We ca n't distinguish waiting and thinking from just waiting . <p> Window focus ( which is what we detect ) is not always equivalent to user focus , although users must act to change focus . Activity in a period of focus without logged user input may be of little or no significance to the work . <p> While these limitations may lead to misinterpretation of individual usages and switches , they are more likely to be associated with statistical outliers-overlong dwells in the first case , and very rapid changes in the second . <p> These problems should be seen against the main advantage of the method , which is to gather and manage large amounts of data during conventional usability testing with little or no additional effort . @ @ @ @ @ @ @ @ @ @ order to identify and compare patterns of behavior that are , even without the noise generated by inactive periods and rapid focus shifts , statistical . This logging , counting , and plotting extend conventional usability practice by enabling consideration of new problem areas . <h> Rating <p> We rate tool switching and tool switch patterns in two ways . First , switches that statistically dominate successful task execution ( defined both as task completion and satisfaction with the task experience , as measured by SUS ) are rated as preferred by users . Secondly , however , the choices made in the course of the work can be evaluated as regards timing ( by observing whether task progress before the switch point was satisfactory ) and in terms of choice ( by observing whether users choose the most effective tool ) . <p> These ratings are concrete data descriptive of users ' cognitive behavior , based on observation , consistent with other summative work in usability studies . Their quantity and quality are made possible by automated detections and pre-processing of all events in this category , and artificially @ @ @ @ @ @ @ @ @ @ through the use of recordings and flagging of events . The technique remains qualitative ; its foundation is the experienced observer 's analysis of critical incidents . However , quantitative methods ( plots and statistics ) identify patterns and clustering , which suggest where those critical incidents lie . <h> Analysis <p> The analysis has four steps : choice of event types ( or perhaps design changes ) for study , parsing and processing of logs into statistics or graphics that arrange the data into useful views , pairing events and patterns with their metrics , and checking conclusions against test recordings . The final step connects the statistical abstraction with actual test observations , and also warrants that conclusions are not distorted by outliers . <p> We have already observed problem patterns prior to the study that stands out in the timelines and data . We expect to discover further orderings of subtasks ( groupings or effective sequences ) that , although they alter the na+ve flow of the task , are so evidently more productive that design should reinforce them . One example of such a technique is @ @ @ @ @ @ @ @ @ @ as a block , and then updating the contents of the new block . Speed , accuracy , and neatness are achieved by " going out of order . " These sometimes unexpected strategies are fruitful because of dependencies between subtasks that users discover and exploit ( Card et al. , 1983 ) . Graphical presentation of workflows ( Figures 3 and 4 ) help us detect patterns and dependencies . <h> Results <p> At this time , we have refined generation of the timeline and automated some charts that summarize and compare data . However , the data so far is only a baseline sample . The acid test of the technique and approach will come when we run the same tasks under the same conditions with improved designs . We expect that differences in tool usage will be easy to study and evaluate using this approach . Meanwhile , the results to date do support some observations of interest and potential value . ( This discussion is limited to the programming task , which Users 3 and 5 did not attempt . ) <p> We asked each user @ @ @ @ @ @ @ @ @ @ after the task , and we recorded the number of task steps each user finished . The users also supplied information about themselves , rating themselves expert ( E ) or novice ( N ) , on level of skill and comfort with MATLAB ( 1-5 low to high ) , and whether they thought of themselves as programmers or not . They also reported the number of files they wrote per month , and years they had used the product . <p> Table 1 . User Data from the Programming Task <p> User <p> SUS Score <p> Completed <p> Self-Rating <p> Experience <p> Editor-Command Window Switches <p> Expert ? <p> Skill with MATLAB <p> Comfort with MATLAB <p> Program- mer <p> Files/ Month <p> Years Using MATLAB <p> 1 <p> 60 <p> 2 <p> E <p> 3 <p> 4 <p> Y <p> 1 <p> 6 <p> 61 ( High ) <p> 2 <p> 82.5 <p> 1 <p> N <p> 2 <p> 4 <p> N <p> 7 <p> 0.5 <p> 17 ( Middle ) <p> 4 <p> 87.5 <p> 5 <p> E <p> 4 <p> 5 <p> Y <p> 24 @ @ @ @ @ @ @ @ @ @ 57.5 <p> 1 <p> N <p> 2 <p> 3 <p> N <p> 9 <p> 2 <p> 4 ( Low ) <p> 7 <p> 45 <p> 0 <p> N <p> 2 <p> 3 <p> - <p> 1 <p> 8 <p> 6 ( Low ) <p> 8 <p> 40 <p> 1 <p> N <p> 1 <p> 2 <p> Y <p> 2 <p> 0.5 <p> 9 ( Low ) <p> 9 <p> 70 <p> 1 <p> N <p> 1 <p> 3 <p> N <p> 4 <p> 5 <p> 8 ( Low ) <p> 10 <p> 67.5 <p> 3 <p> E <p> 3 <p> 3 <p> Y <p> 12 <p> 3 <p> 29 ( Middle ) <p> 11 <p> 90 <p> 3 <p> E <p> 1 <p> 3 <p> Y <p> 0 <p> 4 <p> 0 ( Low ) <p> 12 <p> 100 <p> 1 <p> N <p> 3 <p> 4 <p> Y <p> 15 <p> 1 <p> 28 ( Middle ) <p> 13 <p> 70 <p> 2 <p> N <p> 3 <p> 5 <p> N <p> 5 <p> 1.5 <p> 18 ( Middle ) <p> 14 <p> 80 <p> 2 <p> E <p> @ @ @ @ @ @ @ @ @ @ 26 ( Middle ) <p> No correlation between user scores and their self-descriptions was apparent , nor was it expected . However , it was possible to group users meaningfully by their tool switching behavior , which is the content of the last column . How this was done appears in Figure 3 . <p> Tool switching styles of 11 users who completed the programming task are presented as point data connected by lines in Figure 3 . This view accentuates user differences and similarities around individual transitions . We can immediately pick out users 6 and 13 moving between the Editor and the Documentation , user 1 's frequent use of the Array Editor , and so forth . We can also separate the users into three groups based on their frequent switching in and out of the Editor and the Command Window ( the first two data on the x-axis ) . We can see what this grouping implies by comparing the " styles " depicted by the users ' timeline diagrams . ( This grouping is analyzed in Table 2 . ) <p> Figure 3 . Transition @ @ @ @ @ @ @ @ @ @ of switching from Editor to Command Window suggested the division of users into three groups : High &gt; 50 , Middle 18-30 , and Low <p> Figure 4 shows a timeline from each of the three groups . There are other visible differences , but sample sizes are too small for anything but speculation at this point . <h> Discussion <p> We revisited whether some of the factors given in Table 1 contribute to differences in tool switching behavior such as those shown in Figures 3 and 4 . Table 2 collapses Table 1 into three groups , either by averaging individual results or by expressing class ( expert or novice ) representation as ratios . These are the same groups that appear in the last column of Table 1 . <p> Before examining Table 2 , the experimenter naively assumed that the Low group had the best tool switching strategy and the High group the worst , based on a gut feeling that switching should be minimized . He could not have been more wrong . Not only did the Low group perform the worst ( fewest segment completions @ @ @ @ @ @ @ @ @ @ ) , but they were the most unhappy with the tools ( SUS ) , and had the lowest in exposure to the product ( 3.2 files written per month ) . The High group , whose timelines appear almost dysfunctionally scattered , were in fact all-expert , and they proved it by completing by far the most task segments . Not only did they score the best , they also rated themselves the best and most comfortable with the software . <p> Table 2 . Metrics by Style Grouping of Switching Statistics <p> Group <p> Expert : Novice <p> SUS <p> Finished ( of 5 ) <p> Skill ( MATLAB ) <p> Comfort ( MATLAB ) <p> Years <p> Programmer : Plotter <p> Files/ Month <p> High <p> 2:0 <p> 73.75 <p> 3.5 <p> 3.5 <p> 4.5 <p> 5.5 <p> 2:0 <p> 12 <p> Mid . <p> 2:3 <p> 80 <p> 1.8 <p> 2.4 <p> 4 <p> 1.24 <p> 2:3 <p> 11.8 <p> Low <p> 1:4 <p> 60.5 <p> 1.2 <p> 1.4 <p> 2.8 <p> 3.9 <p> 2:2 <p> 3.2 <p> The manner in which this grouping of users @ @ @ @ @ @ @ @ @ @ MATLAB- ( SUS ) , and users ' self-ratings and experience is the most important finding in the study thus far . It counter-intuitively suggests that more keystrokes and more switching " work " can be better , which goes to show how much we have yet to learn about tool switching . <h> Recommendations ( What 's Next ) <p> Making tool switching painless and quick may not be enough , and perhaps not even the highest design priority . Experienced users employ techniques-and choose tools-that appear rapid and efficient because using them has come to seem easy . Some are effective and some are not . For example , every user has his or her way to access help and support information . These patterns ca n't all be optimal , even if users think they are . Some users get better results than others ; their tool choices and switching strategies will be the ones to reinforce in design . <p> Development of new designs to support tool switching highlights some areas of further investigation that are likely targets for work carried out with the techniques that @ @ @ @ @ @ @ @ @ @ <p> The notion of best practices and much of the content of training hinge at least in part on the assumption that users can monitor the quality of their work . In terms of this study , users discover through evaluation of work progress that they need to change tools . Our more narrow interest would be whether design could support this monitoring activity or not . <p> Studies of pair programming shed interesting light on this area : Experimental work pairing experts with novices shows experts ' awareness of task barriers ( contradictions ) when observing novices encountering them ( Raeithel &amp; Velichovsky , 1996 ) . It is not clear , however , that these experts monitor their own activity with the same care . Pair programming , in which one user codes while a teammate takes a supervisory view and ( theoretically ) makes good switching decisions , should model any system that notifies users of contradictions in their ongoing work ( Domino , Collins , Hevner , &amp; Cohen , 2003 ) . An ethnographic study of pair programming in practice suggests that this is not @ @ @ @ @ @ @ @ @ @ would even be possible for two people working at different levels of abstraction to successfully sustain a conversation at all " ( Chong &amp; Hurlbutt , 2007 , p. 2 ) . Also , the authors repeatedly observed that the best-functioning pairs are also peers at skill level and experience ( Chong &amp; Hurlbutt , 2007 ) . Advice , however good , administered injudiciously , can be counterproductive . Experiments harnessing the joint attention of two users ( Raeithel &amp; Velichovsky , 1996 ) appear to be more encouraging , suggesting that appropriately designed software agents might support the monitoring task better than peers hampered by their social baggage . <h> Task specificity of tools <p> Nardi ( 1996 ) suggests classification of tools as more or less task specific according to the variety of tasks that a task is suited for . As an example , the task of comparing two files can be carried out in most editors , but awkwardly ; the same task can be handled much more quickly and efficiently in a difference editor . It 's plausible that users resist switching to a @ @ @ @ @ @ @ @ @ @ small part of the overall task , and the difference editor is not hospitable to most of what the user is trying to accomplish . The effect of task specificity in tool choice appears in frequency of switching in tasks where use of the tool is appropriate , and in delay ( inefficient attempts to complete the work ) before moving to the tool , and then moving back . <h> A more productive understanding of expertise <p> Although complex software applications must be accessible to novices , for users to remain novices is undesirable . Yet the definition of expert remains elusive . Since user 4 scored as an expert , and rated himself as an expert , I reviewed his test recording to see if there was anything concrete I could say about his expertise . What I saw was an expert as defined by activity theory . No time was spent deciding whether or not to change tools , or which tool to change to . And with each tool his actions were swift and sure , at times almost too fast for me to follow . @ @ @ @ @ @ @ @ @ @ the steps of his workflow , and expert ( economical and effective ) with the tools he chose . <p> But I could not rate his workflow or his choice of tools so highly . Having watched many programmers try this task , I know that there are better orderings of steps , and better choices of tools than the ones that this user had internalized . Coleridge ( 1827 ) defined prose as " words in their best order " and poetry as " the best words in their best order . " User 4 produced eloquent prose . The highest level of expertise must be more like poetry . <p> Activity theory describes the process by which novices learn to produce satisfactory prose . When a workflow is derailed by one of Engestrom 's contradictions the same process heals and improves it . But users with operations that employ less effective tools , or switch between tools at less than optimal times , do n't run into contradictions . And yet a new and better tool ( within the application ) that users believing themselves to be functioning @ @ @ @ @ @ @ @ @ @ , in some important sense , less than expert . <p> Adding tools to an application 's mix will not likely help users . Even if they ( a ) become aware of the new features ( not easy ) , and ( b ) try to weave the new tool into their workflow ( lots of extra work ) . Determining which tool ( old or new ) works better is neither straightforward nor foolproof . <p> Extensions of Engestrom 's " contradictions " are more likely to succeed if users are made aware of more effective alternative tools at the moment of choice , or notified of ineffective performance at a point when they feel the need to " upgrade " their process . The trick , of course , is making users aware of the information at that pivotal moment . <h> Interruptions by design <p> Because of their focus on task work , users sometimes miss points where they ought to be branching . This is why study of interruptions is vital ; if users are to make the best choices at the best points in @ @ @ @ @ @ @ @ @ @ ? <p> We want to be able to inject task interruptions in the form of alerts and reminders to influence better tool switching and to improve the timing of tool switching by helping users decide when to switch . The most interesting studies on interruptions deal with interruptions that are distractions from the task ( Cutrell et al. , 2000 ; Mark et al. , 2005 ; Speier , Valacich , &amp; Vessey , 1997 ) . <p> In our testing , interruptions originate either with the participants when the task flow breaks because questions they ca n't answer come up in the progress of the work ( " How do I finish this for loop ? " ) , when the computer throws an error or performs an unexpected action , or when reaching the end of a subtask . Interactions occur at natural activity boundaries and are adequately characterized in our data by the before and after activities and time measurements , frequency , and positioning along the length of subtask and task . <p> Interruption statistics , within-task completion times , and users ' satisfaction with their @ @ @ @ @ @ @ @ @ @ position in the task in user input . We can also observe whether interruptions are preceded by a tapering off of productivity as Speier ( 1997 ) predicts . <p> Design for more timely tool changes requires affordances in the user interface that alert users to the pace and effectiveness of their work . <h> Conclusion <p> Tasks that are the work of large software applications are typically complex . Neither the user nor the usability tester knows either the path the workflow will take or which tools will be brought to bear . Users ' awareness of points where they change tools , and the choice of tools they make at those points , directly affects productivity and creativity . Training and the inculcation of best practices may well improve this awareness and decision making ability , but most users never get either . Improved design for making these choices and transitions , on the other hand , reaches every user every time a tool needs changing . <p> A model of user action made up of steps or segments that smoothly merge into an efficient operation as expertise @ @ @ @ @ @ @ @ @ @ because it omits the following : <p> Switching from one tool to another at times that depend uniquely on task content and progress . Users must become or be made aware that the work is not advancing as well as it might be , and that a tool change is the right next move . <p> Consideration and study of complex , " real " tasks wherein the user discovers or invents the workflow . The switch from one optimal tool to another at the appropriate time is a key problem in this discovery and invention of a workflow . <p> Discovery and adoption by experts of better , more appropriate tools-simply tools that are newly available . As soon as the task is fresh , or in any way different , the expert 's polished workflow , however impressively effective , is potentially ripe for improvement . Experts must cope with this possibility without losing their effectiveness . <p> We have only vague notions of how design may help users to use the best tools in the best order , and how to test proposed improvements . We need @ @ @ @ @ @ @ @ @ @ Acknowledgements <p> Andrew Wirtanen and Mike Ryan , who facilitated the testing , Amy Kidd , who helped me plan it , Jared Spool , who taught me how to design real tasks , Donna Cooper , the MATLAB LCT Usability team , Chauncey Wilson , the excellent peer reviewers , and others who grappled so supportively with ideas out of order , and , of course , the participants . 
@@105507116 @907116/ <h> This Issue <h> Usability Studies and the Hawthorne Effect <h> Abstract <p> This paper provides a brief review of the Hawthorne effect , a discussion of how this effect relates to usability studies and help for practitioners in defending their studies against criticisms made on the basis of this effect . 98865 @qwx958865 <p> The Hawthorne effect can be ( mis ) used as a basis on which to criticize the validity of human-centered studies , including usability studies . Therefore , it is important that practitioners are able to defend themselves against such criticism . A wide variety of defenses are possible ; depending on which interpretation of the Hawthorne effect is adopted . To make an informed decision as to which interpretation to adopt , practitioners should be aware of the whole story regarding this effect . <p> A precursor to any defense should be pointing out that there are many significant differences between the studies carried out at the Hawthorne works and typical usability studies . Therefore , care must be taken when relating any interpretation of this effect to our discipline . <p> Most @ @ @ @ @ @ @ @ @ @ proposed by Mayo ( 1933 ) . However , despite its popularity , this interpretation has been largely debunked over the last few decades . At worst , it can be considered as nothing more than a popular myth that has no place in any serious research thinking . At best , it can be considered as a controversial idea that has highly questionable relatability to our discipline . Therefore , a defense against this interpretation is not likely to be required once all the stakeholders in a study understand this fact . However , should such a defense be required , the study should maximize the use of ( blind ) controls and gather evidence of the causation mechanisms that resulted in any significant findings ( benefits ) . This is probably best achieved through the use of qualitative techniques such as verbal protocols and pre- and post-test semi-structured interviewing . <p> If the interpretation of the Hawthorne effect proposed by Parsons ( 1974 ) is adopted , then it should be insured that extrinsic performance feedback to participants in a study is eliminated , or minimized as far @ @ @ @ @ @ @ @ @ @ the study design and its execution , it is clearly important that the study team agree with the client what position will be taken in relation to the Hawthorne effect in advance of the study . This position should be then be published in an appendix to the study report . 
@@105507117 @907117/ <h> This Issue <h> Towards the Design of Effective Formative Test Reports <h> Abstract <p> Many usability practitioners conduct most of their usability evaluations to improve a product during its design and development . We call these " formative " evaluations to distinguish them from " summative " ( validation ) usability tests at the end of development . A standard for reporting summative usability test results has been adopted by international standards organizations . But that standard is not intended for the broader range of techniques and business contexts in formative work . This paper reports on a new industry project to identify best practices in reports of formative usability evaluations . The initial work focused on gathering examples of reports used in a variety of business contexts . We define elements in these reports and present some early guidelines on making design decisions for a formative report . These guidelines are based on considerations of the business context , the relationship between author and audience , the questions that the evaluation is trying to answer , and the techniques used in the evaluation . Future work will @ @ @ @ @ @ @ @ @ @ guidelines or templates. 98865 @qwx958865 <p> There is little guidance in the literature for the good design of a report on a formative evaluation . This is in contrast to summative evaluation reports , for which there is an international standard . <p> There is wide variation in reporting on formative usability evaluations . <p> The audience for a formative usability report should be carefully considered in designing the report format . The content , presentation or writing style , and level of detail can all be affected by differences in business context , evaluation method , and the relationship of the author to the audience . <p> The IUSR project seeks to provide tools such as best practice guidelines and sample templates to help practitioners communicate formative evaluation results more effectively . To join the IUSR community , visit http : //www.nist.gov/iusr/ 
@@105507120 @907120/ <h> This Issue <h> Abstract <p> Small sample sizes are a fact of life for most usability practitioners . This can lead to serious measurement problems , especially when making binary measurements such as successful task completion rates ( p ) . The computation of confidence intervals helps by establishing the likely boundaries of measurement , but there is still a question of how to compute the best point estimate , especially for extreme outcomes . In this paper , we report the results of investigations of the accuracy of different estimation methods for two hypothetical distributions and one empirical distribution of p . If a practitioner has no expectation about the value of p , then the Laplace method ( ( x+1 ) / ( n+2 ) ) is the best estimator . If practitioners are reasonably sure that p will range between .5 and 1.0 , then they should use the Wilson method if the observed value of p is less than .5 , Laplace when p is greater than .9 , and maximum likelihood ( x/n ) otherwise . 98865 @qwx958865 <p> Always compute a confidence @ @ @ @ @ @ @ @ @ @ estimate . For most usability work , we recommend a 95% adjusted-Wald interval ( Sauro &amp; Lewis , 2005 ) . <p> If you conduct usability tests in which your task completion rates typically take a wide range of values , uniformly distributed between 0 and 1 , then you should use the LaPlace method . The smaller your sample size and the farther your initial estimate of p is from .5 , the more you will improve your estimate of p . <p> If you conduct usability tests in which your task completion rates are roughly restricted to the range of .5 to 1.0 , then the best estimation method depends on the value of x/n. ( 3a ) Ifx/n = .5 , use the Wilson method ( which you get as part of the process of computing an adjusted-Wald binomial confidence interval ) . ( 3b ) If x/n is between .5 and .9 , use the MLE . Any attempt to improve on it is as likely to decrease as to increase the estimate 's accuracy . ( 3c ) If x/n = .9 , but less @ @ @ @ @ @ @ @ @ @ . DO NOT use Wilson in this range to estimatep , even if you have computed a 95% adjusted-Wald confidence interval ! ( 3d ) If x/n = 1.0 , use the Laplace method . <p> Always use an adjustment when sample sizes are small ( n&lt;20 ) . ( It does no harm to use an adjustment when sample sizes are larger. ) . 
@@105507121 @907121/ <h> This Issue <h> Abstract <p> A usability assessment entailing a paper prototype was conducted to examine menu selection theories on a small screen device by determining the effectiveness , efficiency , and user satisfaction of a popular cellular phone 's menu system . Outcomes of this study suggest that users prefer a less extensive menu structure on a small screen device . The investigation also covered factors of category classification and item labeling influencing user performance in menu selection . Research findings suggest that proper modifications in these areas could significantly enhance the system 's usability and demonstrate the validity of paper-prototyping which is capable of detecting significant differences in usability measures among various model designs . 98865 @qwx958865 <p> The effect of breadth/depth tradeoffs in navigation is much more obvious and important in a small screen device . Findings suggest ( 1 ) reduce both breadth and depth of the menu and ( 2 ) display more menu items and options in one page so that users can avoid extra scrolling actions on a level . <p> Ill-categorized and ill-labeled menu items have strong impacts on user @ @ @ @ @ @ @ @ @ @ Norman 's ( 1983 ) suggestion of using longer ( and more complete ) descriptions in naming is more useful to users learning the system ; and ( 2 ) Shneiderman 's ( 1998 ) recommendation of forming distinctive menu categories based on users ' tasks . <p> This study demonstrates the efficacy of a paper prototyping variation as a way of testing the usability of an information architecture , which shows that an evaluation of a user 's path-finding activities in a map of menu hierarchy can detect significant differences in user performance among a variety of models . 
@@105507131 @907131/ <h> This Issue <h> Abstract <p> Small sample sizes are a fact of life for most usability practitioners . This can lead to serious measurement problems , especially when making binary measurements such as successful task completion rates ( p ) . The computation of confidence intervals helps by establishing the likely boundaries of measurement , but there is still a question of how to compute the best point estimate , especially for extreme outcomes . In this paper , we report the results of investigations of the accuracy of different estimation methods for two hypothetical distributions and one empirical distribution of p . If a practitioner has no expectation about the value of p , then the Laplace method ( ( x+1 ) / ( n+2 ) ) is the best estimator . If practitioners are reasonably sure that p will range between .5 and 1.0 , then they should use the Wilson method if the observed value of p is less than .5 , Laplace when p is greater than .9 , and maximum likelihood ( x/n ) otherwise . 98865 @qwx958865 <p> Always compute a confidence @ @ @ @ @ @ @ @ @ @ estimate . For most usability work , we recommend a 95% adjusted-Wald interval ( Sauro &amp; Lewis , 2005 ) . <p> If you conduct usability tests in which your task completion rates typically take a wide range of values , uniformly distributed between 0 and 1 , then you should use the LaPlace method . The smaller your sample size and the farther your initial estimate of p is from .5 , the more you will improve your estimate of p . <p> If you conduct usability tests in which your task completion rates are roughly restricted to the range of .5 to 1.0 , then the best estimation method depends on the value of x/n. ( 3a ) Ifx/n = .5 , use the Wilson method ( which you get as part of the process of computing an adjusted-Wald binomial confidence interval ) . ( 3b ) If x/n is between .5 and .9 , use the MLE . Any attempt to improve on it is as likely to decrease as to increase the estimate 's accuracy . ( 3c ) If x/n = .9 , but less @ @ @ @ @ @ @ @ @ @ . DO NOT use Wilson in this range to estimatep , even if you have computed a 95% adjusted-Wald confidence interval ! ( 3d ) If x/n = 1.0 , use the Laplace method . <p> Always use an adjustment when sample sizes are small ( n&lt;20 ) . ( It does no harm to use an adjustment when sample sizes are larger. ) . 
@@105507134 @907134/ <h> This Issue <h> Abstract <p> A usability assessment entailing a paper prototype was conducted to examine menu selection theories on a small screen device by determining the effectiveness , efficiency , and user satisfaction of a popular cellular phone 's menu system . Outcomes of this study suggest that users prefer a less extensive menu structure on a small screen device . The investigation also covered factors of category classification and item labeling influencing user performance in menu selection . Research findings suggest that proper modifications in these areas could significantly enhance the system 's usability and demonstrate the validity of paper-prototyping which is capable of detecting significant differences in usability measures among various model designs . 98865 @qwx958865 <p> The effect of breadth/depth tradeoffs in navigation is much more obvious and important in a small screen device . Findings suggest ( 1 ) reduce both breadth and depth of the menu and ( 2 ) display more menu items and options in one page so that users can avoid extra scrolling actions on a level . <p> Ill-categorized and ill-labeled menu items have strong impacts on user @ @ @ @ @ @ @ @ @ @ Norman 's ( 1983 ) suggestion of using longer ( and more complete ) descriptions in naming is more useful to users learning the system ; and ( 2 ) Shneiderman 's ( 1998 ) recommendation of forming distinctive menu categories based on users ' tasks . <p> This study demonstrates the efficacy of a paper prototyping variation as a way of testing the usability of an information architecture , which shows that an evaluation of a user 's path-finding activities in a map of menu hierarchy can detect significant differences in user performance among a variety of models . 
@@105507141 @907141/ <h> This Issue <h> Expanding Usability Testing to Evaluate Complex Systems <h> Abstract <p> This essay discusses ways that usability professionals can expand usability testing to evaluate complex systems , such as intelligence gathering and medical decision-making , that do not lend themselves to more traditional laboratory-based usability testing . In the essay , Redish explains what complex systems are , why they do n't lend themselves to traditional usability test methodologies , and what other techniques are available for gathering and analyzing the data . The essay also discusses the importance of involving domain experts in the design of the test to ensure that both the components and the system as a whole are being adequately tested . 
@@105507146 @907146/ <h> This Issue <h> Extremely Rapid Usability Testing <h> Abstract <p> The trade show booth on the exhibit floor of a conference is traditionally used for company representatives to sell their products and services . However , the trade booth environment also creates an opportunity , for it can give the development team easy access to many varied participants for usability testing . The question is can we adapt usability testing methods to work in such an environment ? Extremely rapid usability testing ( ERUT ) does just this , where we deploy a combination of questionnaires , interviews , storyboarding , co-discovery , and usability testing in a trade show booth environment . We illustrate ERUT in actual use during a busy photographic trade show . It proved effective for actively gathering real-world user feedback in a rapid paced environment where time is of the essence . 98865 @qwx958865 <p> The following are advantages and disadvantages of performing extremely rapid usability testing ( ERUT ) at trade shows . <h> Advantages <p> The testing provides for a light weight , rapid gathering of good quality user feedback without @ @ @ @ @ @ @ @ @ @ . <p> There is a narrow focus on business goals and core functionality that produces valuable insights . <p> There is easy access to a broad brush of credible users . <p> Access to domain experts is easy . <p> There are no " no show " participants . <p> The data can be easily collected in a user database for future tests . <p> Participants are in the booth for their benefit first , which yields rich customer input . This could also be a disadvantage if it generates false excitement . <p> The method is very fluid . Company representatives must adapt to change to suit the situation . <h> Disadvantages <p> The focus tends to be narrow . <p> This type of on-the-fly usability testing does not look at all of the product 's capabilities . Because of the time constraints , only a few aspects of the product can be evaluated . <p> Core tasks tested in isolation may not represent what happens if that task were performed in the context of a complete application workflow . <p> The trade show environment is rapid and hectic @ @ @ @ @ @ @ @ @ @ . <p> Questionnaires and storyboards can be reduced to scribbles because of the time constraints and the desire to quickly capture as much data as possible . <p> Participants are not in their natural environment where they would use the product . <p> Observations are not made in context of real work . <p> Participants are in a trade show frame of mind . They could be affected by the excitement in the booth . 98864 @qwx958864 <p> Traditional usability testing typically occurs in a laboratory-like setting . Participants are brought into the test environment , a tester provides tasks to the participants , and the participants are instructed to " think aloud " by verbalizing their thoughts as they perform the tasks ( e.g. , Dumas &amp; Reddish , 1999 ; Nielsen , 1996 ) . Observers watch how the participants interact with the product under test , noting both problems and successes . While a typical usability test normally takes at least one hour to run through several key tasks , it can take many days or weeks to set up ( e.g. , lab and equipment set @ @ @ @ @ @ @ @ @ @ with no-shows , etc . ) . The key problem is that it may be quite difficult and/or expensive to motivate people-particularly domain experts-to participate in such a study . While this can be mitigated by running the test in the domain expert 's workplace , this introduces other significant problems , such as disruptions to the expert 's actual work . <p> Another possibility is to use a trade show as a place for conducting usability tests , especially for new versions of a product that would naturally fit a trade show theme . We can consider the benefits of a trade show in light of Dumas and Reddish 's ( 1999 ) following five characteristics of usability testing : <p> The primary goal to improve the usability of a product <p> Participants represent real users , <p> Participants do real tasks , <p> You observe and record what participants do and say and <p> You analyze the data and recommend changes <p> A trade show emphasizes characteristics 1 , 2 , and 3 . Characteristic 2 is the one that is maximized : there is a plethora of @ @ @ @ @ @ @ @ @ @ , not only present but likely willing to participate in the usability test . They should be highly motivated to try out , and thus test , new product versions . Their attendance means they have a large block of time for doing so . Next , a trade show setting sets the scene for characteristic 1 because trade shows largely concern advertising , familiarizing , and ultimately selling a product to potential customers . Product features , usefulness , and usability dominate discussions between participants and those manning the booth . For characteristic 3 , participants are engaged by the theme of a trade show , they could easily reflect upon the actual tasks that they would want to perform on a product or critique the tasks they are being asked to do . In turn , the feedback gained is likely highly relevant to real-world use . <p> Yet there are issues . A trade show is not a laboratory , nor is it a workplace . Trade shows are crowded and bustling venues , where vendors compete with others to attract people to their booths . A @ @ @ @ @ @ @ @ @ @ cramped space that exists for three days and could be visited by 500 people or more . Booth visitors can be users , competitors , students , or future customers . Each visitor may spend anywhere from one minute to 60 minutes in a booth . Distractions are rampant . This is not a typical usability test environment ! This makes characteristics 4 ( observe and record ) and 5 ( analyze ) more problematic for the evaluator and constrains the kinds and number of tasks ( characteristic 3 ) that can be done . Yet for companies with limited time and resources to get their product to market , a trade show could offer a realistic way to gather a broad brush of domain experts in one place for product testing . <p> Of course , there are evaluations methods within human computer interaction ( HCI ) that others developed for time and resource limited environments ( e.g. , Bauersfeld &amp; Halgren , 1996 ; Gould , 1996 ; Marty &amp; Twidale , 2005 ; Millen , 2000 ; Thomas , 1996 ) , but none specifically address the @ @ @ @ @ @ @ @ @ @ the earliest advocate of rapid testing . He describes a plethora of highly pragmatic methods that let interface designers quickly gather feedback in various circumstances . His examples include placing interface mockups in an organization 's hallway as a means to gather comments from those passing by and continually demonstrating working pieces of the system to anyone who will take the time to watch . The advent of quick and dirty usability testing methods in the mid 90s formalized many of these processes . Each method was an attempt to decrease the cost of the test ( time , dollars , resources , etc. ) while maximizing the benefit gained ( e.g. , identifying large problems and effects , critical events , and interface compliance to usability guidelines , etc . ) ( Nielsen , 1994 ; Thomas 1996 ) . Other methods were developed to specific contexts . For example , Marty and Twidale ( 2005 ) described a high-speed ( 30 minute ) user testing method for teaching , where the audience can " understand the value of user testing quickly , yet without sacrificing the inherent realism @ @ @ @ @ @ @ @ @ @ Millen ( 2000 ) discussed rapid ethnography , a collection of field methods tailored to gain a limited understanding of users and their activities within the constraints of limited time pressures in the field . <p> No method specifically addressed running rapid usability tests in a busy trade show or conference exhibit hall booth . The question remained , how can we use the trade show as a place for conducting usability tests ? Consequently , our goal was to see if we could adapt and modify existing usability testing methods to the trade show context , which we called extremely rapid usability testing ( ERUT ) . Our experiences with ERUT involved a pragmatic combination of HCI evaluation techniques : questionnaires , co-discovery , storyboarding , and observational think-aloud tests . It was an example of taking formative testing methods and applying it to a particular context of use . We wanted to exploit the " best " of each method , i.e. , the portion that delivers the maximum amount of information within the severe limitations of the trade show . ERUT is not a formal or exhaustive @ @ @ @ @ @ @ @ @ @ other methods . Rather , ERUT applies and mixes various informal discount methods to provide insights into the usefulness and usability of primary product features . <p> ERUT developed opportunistically . This paper 's author , Mark Pawson , and another colleague were invited by Athentech Inc. of Calgary Alberta to attend the PDN PhotoPlus show in New York to perform rapid usability tests on the Perfectly Clear- digital imaging enhancement software . Pawson already worked as a usability evaluator , and both he and his colleague were experienced in working trade booths from a marketing perspective . We developed ERUT to quickly gather real-world feedback about the usability and usefulness of this product and to she 'd significant light on whether Athentech 's unique selling proposition resonated with the customer . <p> In the remainder of this paper , we describe our experiences developing and using ERUT to evaluate Perfectly Clear- at the PDN PhotoPlus trade show . We caution that ERUT as described here is a case study of our experiences and the lessons we learnt , rather than a rigid prescription of how to do usability testing in @ @ @ @ @ @ @ @ @ @ be seen as a starting point for practitioners to adapt usability testing to their own trade show settings . <h> The Product and Context <p> Athentech states that Perfectly Clear Pro- is a digital image enhancement software designed to correct a digital photo to match what the human eye sees when the picture was taken . Without getting into technical details , Athentech developed a process that overcomes camera limitations and produces photos that yield what the photographer saw when capturing the image . <p> Athentech licenses this technology to photographic labs and to industry leaders such as Fuji , Blacks , Ritz , and Walgreens for use in kiosks and mini-labs . They also wanted to enter the professional consumer market . To this end , Athentech regularly attended trade shows to understand the problems photographers face with digital imaging and with existing software tools on the market . They then developed Perfectly Clear Pro- as their first venture into developing a product for the professional and serious amateur photographers . <p> In our specific case , Athentech was keen to take an alpha version of Perfectly Clear Pro- @ @ @ @ @ @ @ @ @ @ tag line is " to be on the cutting edge of what 's happening in photography and imaging " ( http : //www.photoplusexpo.com ) . However , Athentech had not yet performed any usability evaluations . They believed the show represented a tremendous opportunity not only to get their product in front of many potential customers in a very short time but to try to understand where the alpha version succeeded and failed . <p> From prior experiences , we knew that running usability tests in a booth would be quite different from the usual evaluation setting . <p> The trade show had strict daily closing times , which meant testing after show hours would not be possible . <p> The environment was noisy . While Athentech had chosen a closed booth with a section cordoned off for testing , cordoning was done via curtains . <p> Participant selection would be haphazard , as it depended on who we could attract from the general conference milieu . <p> Testing time was very limited . Past experience with booth visitors indicated that having 15-20 minutes of a participant 's time would @ @ @ @ @ @ @ @ @ @ , most would fit this in-between talks and visits to other booths . <p> Time to immediately reflect on particular study results was limited due to the need to process as many people as possible within the three day duration of the show . <p> From the participant 's perspective , usability testing was only one purpose-the lesser purpose-of the booth . When a visitor stressed business needs over a desire to be a test participant , the tester would have to rapidly switch from wearing a usability testing hat to a sales hat . <p> A testing regime has to be fluid in order to respond to these constraints . Consequently we designed ERUT to focus on the following two primary objectives : <p> Assess the usefulness of the core functionality of a product , i.e. , was the product 's unique selling proposition solving a problem that a majority of customers wanted solved ? <p> Find major usability problems in the core functionality . <p> While this meant that some aspects of the software would be ignored , we hoped that ERUT could determine the usefulness and usability @ @ @ @ @ @ @ @ @ @ trade booth doubles as both a marketing venue and the usability testing area . While it is possible to have two separate booths , we believe a single one is best as it is the product marketing that attracts the participants ( discussed shortly ) . Still , it is important to isolate the testing area from the direct flow of the convention crowd , perhaps by partitioning the booth into two areas : an outer booth for marketing and an inner booth for testing . Without an isolated quieter area , the evaluator runs considerable risk of introducing interactions and distractions in the booth between test participants and those wandering in and out of the booth ( IXDA 2007 ) . <p> In our case , PhotoPlus attracted huge crowds with over 27,000 registered participants . To adjust the flow of potential participants and to isolate the test area , we set the booth walls up around the outside perimeter of the allotted booth area assigned to PhotoPlus . The outside of the booth walls were hung with promotional posters and sample pictures of Perfectly Clear- technology , as @ @ @ @ @ @ @ @ @ @ into the inner booth , which became the test area as illustrated in Figure 2 . As discussed below , the Athentech marketing representative would then feed participants through this doorway when we were able to receive them . <p> Figure 1 . The booth 's exterior , used for product promotion and marketing . Note the doorway to the interior testing area on the right . <p> Figure 2 . The booth 's interior , used as a testing area . <h> Recruiting Participants <p> The trade show offered ease of access to a large variety of domain experts and potential customers in one place . The question was how do we recruit these people given the large number of other booths competing for their attention ? <p> In our case , the attractant was the pictures that hung on the booth wall exteriors that displayed the before and after effects of the Perfectly Clear- technology ( Figure 1 ) , and the unique selling proposition delivered by the Athentech representative working the front of the booth . The Athentech representative served as our gatekeeper . He invited interested @ @ @ @ @ @ @ @ @ @ flow into the testing area . <p> Interested attendees typically asked a booth representative for a demonstration . While many booths provided such demonstrations , our representative explained that the product was still in its early stages and that only those people willing to participate in usability test could try it . Those who volunteered to participate in usability testing were then invited into the booth on a first come , first served basis . Participants felt that they were in control of this process , for it came out of their desire to try the system . To make this work , much of the preliminary process that precedes a usability study was discarded . For example , we did not use written consent forms , nor did we offer incentives to have people participate in the usability testing ( although we did give participants gifts of all-natural chocolate from the Amazon rainforest ) . Certainly , the issue of consent has to be revisited both to inform the participant more clearly and for organizational liability ; the question is how to do such consent effectively within this context @ @ @ @ @ @ @ @ @ @ possible participants due to time constraints . Yet those who could not participate were not necessarily lost opportunities . We scanned in contact information from the badges of several hundred trade show attendees who were interested in trialing ( and thus evaluating ) a beta copy of the product at a future time . <h> Questionnaires <p> We originally planned on a short pre-questionnaire and an optional post questionnaire ( e.g. , a satisfaction or a desirability survey ) . We knew that time would be short in the booth and that participants would be eager to get to the product , so we wanted the questionnaire to be equally short . Thus we focused only on a few key questions that the company considered critical . <p> Athentech 's previous research had already validated that Perfectly Clear- was aligned with customer goals . Their concern was with the offerings of recent competitive products on the market . Athentech felt that those products offered a different workflow and unnecessary functionality . Athentech also thought other vendors had understated the limitations of the digital camera in capturing true images . Given @ @ @ @ @ @ @ @ @ @ if participants were professional or serious amateur photographers ) , what software tools they were currently using for their work , and what they were using these tools for . <p> However , there were tradeoffs . Athentech also wanted to collect additional user feedback on various topics that would help guide their future software development . This would have dramatically increased the size of the questionnaire . We were concerned that customers would be turned off ; they were drawn to the usability test ( which was in the spirit of trade show demonstrations ) but not to the barrage of questions both before and after the test . We found it challenging to balance the questionnaire so that it met both business and testing needs while respecting the customers ' short timelines and interests . As discussed later in our " Lessons Learnt " section , flexibility was the best approach . Instead of requesting this extra information as part of the written questionnaire , we worked the questions into our conversation with participants while they were doing the task . We were opportunistic : we asked questions @ @ @ @ @ @ @ @ @ @ in the interest of time not all questions were asked . <p> We also found that our post-test survey questionnaire did not work in the context of the booth . The questionnaire did not fit with the natural rapport of a trade show booth . As one participant said " everything you have done up to now has been great , but this just turns me off . " <h> Choice of Tasks <p> We develop three tasks ahead of time that were both unique and representative of problems we believed that potential customers wanted solved and that incorporated the unique selling proposition of Perfectly Clear- . This was a modification of an idea used by Chauncey Wilson for testing in a trade show booth ( personal communication , 2007 ) . We had planned to let participants select the most personally interesting one of these three tasks to do . We thought the choices made would give us insight on what parts of the product the participant perceived as the most useful . <p> However , we decided that this approach was not the best one . First , @ @ @ @ @ @ @ @ @ @ to allow people to actually do some of these independent tasks . More importantly , Perfectly Clear- was targeting a specific task workflow , cull and image correction of photos . Athentech was in part positioning itself against its competitors who ( Athentech believed ) had lost sight of this basic customer need by adding layers of complexity and functionality . Consequently , we decided to concentrate only on a core task that addressed this specific workflow . If that could not be done by people to their satisfaction , then it would n't really matter how well they could do other tasks with the system . Therefore we spent time with Athentech learning about the specific problems photographers faced with image enhancement and how this was addressed by Perfectly Clear's- workflow . From this we created four interrelated scenarios in a photographer 's language that we felt were both representational and motivational . These tasks were originally written down on 4 x 6 cards and were to be given to the participants as they completed each task . However , as in the questionnaire , we found the best @ @ @ @ @ @ @ @ @ @ informal conversation rather than by script . Hence the exact language used to introduce each of the four tasks varied between participants . <p> While the above may sound like normal task selection and debugging , we want to stress that the short time line forced us to reconsider our tasks . We would likely have time for people to do only a single task , and we needed to ensure that the results were extremely practical . <h> Co-discovery , Think Aloud , and Active Intervention <p> We were concerned that the trade booth could create an intimidating atmosphere for usability testing . We did not know ahead of time how the booth layout would affect participant privacy and distraction , which in turn would hamper the concentration of a single participant asked to " think aloud " while completing a task . We decided to use co-discovery , where two participants work together to complete a task . Co-discovery yields higher quality verbal communication between paired participants than single participants . The pair typically converse for their own benefit to complete the task , as opposed to a @ @ @ @ @ @ @ @ @ @ 's benefit . <p> In the trade show context , we felt it unreasonable to pair strangers . Instead , we looked for people who visited the booth with a friend or associates and encouraged them to be our participants . Still , we did use single participants if no pair was around at the moment . In these instances , and given the predicted short test cycles , we used active intervention in order to elicit high quality think aloud comments . Active intervention was also advocated in a Web discussion forum on usability testing at conferences ( IXDA , 2007 ) . We were somewhat surprised at how well this worked . Only once did we have to ask a participant what they were thinking , all others proved textbook examples of the think aloud technique . We surmise that this is the result of the informality of our private testing area , the relaxed trade show atmosphere of the attendees , and participants ' keen interest in the product . <p> In practice , we gleaned equally high quality think-aloud and co-discovery comments from both individual and @ @ @ @ @ @ @ @ @ @ participants with each other as the research has reported . However , we also found that it was quite common for one participant to break off his conversation and attention to the task . The participant would explain her thoughts to us or ask a question , while the partner carried on alone . We used active intervention on both the single and paired participants to work in guiding questions at appropriate times . <h> Storyboards for Recording Results <p> Recording test results in the fast pace , noisy atmosphere of the trade show raises other challenges . We used a modification of an HCI discovery technique described by McQuaid , Goel , and McManus ( 2003 ) to shadow and record the " story " of library visitors . They took pictures of the visitors as they pursued their activities . They printed these pictures and overlaid acetate sheets to record their notes of what they observed . Then , they compiled these into storyboards that they hung on a wall and displayed to stakeholders . <p> In a similar way , we used hardcopy screen shots of Perfectly @ @ @ @ @ @ @ @ @ @ took in exploring the task . To clarify , storyboarding is a prototyping technique usually used to describe an interface sequence to others . Instead , we used storyboarding for note-taking , where the visuals and annotations described the primary actions a person actually did . We did not use videotapes or screen-capture software for recording the usability results , as we would not have had the time to revisit , analyze , and reflect on these recordings . As well , we were looking for high-level vs. detailed effects . It was unclear if video analysis was worth the effort . The advantages of paper storyboards are the ease in taking notes by simply circling or numbering areas visited , adding annotations as needed , and-perhaps most importantly-the immediacy of the result . The storyboards helped us collate our notes at the end of the day and perform our analysis without having to wade through hours of video tape . However , the storyboards are by no means neat , as annotations were made in a rushed pace . Notes on interesting observations , comments made by the participants @ @ @ @ @ @ @ @ @ @ storyboard and these could be hard to decipher days later . Ours had to be looked at on the same day while our memories were still fresh in our mind . Also , unlike McQuaid , Goel , and McManus ' storyboards , ours were far too messy to show to stakeholders . <h> Lessons Learnt <p> While every trade show and usability testing needs differ , we offer the following lessons learned for others to consider within their context . <p> Easy access to domain experts and potential customers . Perhaps the biggest advantage of ERUT over a standard usability testing methodology is the ease of access to a large variety of domain experts and potential customers in one place . There is no time spent recruiting participants , dealing with the logistics of scheduling , or losing time due to no-show participants . These issues simply do not exist . A trade booth , if designed well , is a natural attractant for people . People are at a trade show because they want to be , and they come into a booth because they are interested in @ @ @ @ @ @ @ @ @ @ just a matter of suggestion . <p> Business comes first . In a trade show environment , the business need comes first . Most companies enter trade shows for marketing , not for testing . More importantly , trade show participants are there to see products , not to test them . Thus one should not expect to do rigorous usability testing in such an environment ; incomplete questionnaires and tasks are the norm , and participants may shift their attention to their personal needs vs. keeping strictly to the test regime . Yet this shift of attention is also an opportunity , as it creates a type of contextual interview around the topic of user and business needs while running the test task ( it is contextual in the sense that the trade show offerings are often part of the conversation ) . In fact , our experience from this was that a trade show booth might be the next best thing to observing users in the context of their real environment because they are there for themselves , seeking real solutions to problems they have , and they @ @ @ @ @ @ @ @ @ @ . The result is very rich customer input on their needs . <p> Casual conversation over scripts and questionnaires . The best way to engage participants was to drop the usability script and questionnaires ; we used casual conversation instead . In our case , participants had a real need for automatic batch correction of their photos . Event photographers in particular were in the booth because they wanted to know how Perfectly Clear- would save them time doing hundreds of image corrections and allow them to get back to their jobs-shooting photos . They were captivated by the message that they had heard from the Athentech representative and were keen to see the software . Introducing ourselves by giving the standard " thank you for participating in our usability test " patter and then presenting them with consent forms and a pre-test questionnaire was cold and robotic and did not fit the pace of action . Instead we worked both the business questions and the task into an exploratory conversation . This immediately engaged them , showed respect for their time , and worked with the natural flow of @ @ @ @ @ @ @ @ @ @ , not be treated as a test subject . They were there to get answers , not to be asked questions . By being very familiar with the questions we wanted to ask , we looked for opportunities to introduce them as part of a conversation during the testing . This was probably the greatest value of the questionnaires-they became our talking points . The questionnaires helped us pick up on important points made by the participant that otherwise could have gone unnoticed unless one is a domain expert in photography . Of course , this comes at a cost , the loss of a script means that the process is not as repeatable . Different words ( and different evaluators ) may motivate people differently and large chunks of the script may be omitted . This also implies that the collected data is better seen as samples rather than a consistent outcome based on repeatable instructions and tasks . <p> Tasks need to be meaningful . The actual tasks done by participants and how they are introduced may also deviate from the script . The trade show setting meant @ @ @ @ @ @ @ @ @ @ that was meaningful to the participant . In one case , we had a pair of participants who were looking in detail at a photograph and expressed a desire to make the red colors " pop out . " Perfectly Clear- corrects photos back to the true colors ; artistically enhancing colors ( typically done using other products on the market ) is not a feature . However , the software does offer an export function . Thus we changed our task on the fly to fit the participant 's expectations and workflow . Originally , our final task read , " Now you have completed your enhancements , pick your three best photos and store them as high quality JPEGs in a folder of your choice on your computer . " We turned their comment around and simply asked them , " How would you get that photo into the software of your choice to pop out that red ? " <p> The test requires a narrow focus on core issues . Focusing on core issues is critical , not only because time is short ( Bauersfeld &amp; Halgren @ @ @ @ @ @ @ @ @ @ ) , but because it is likely those core issues will engage participants . Another advantage of the narrow focus is that it requires all stakeholders to define what the core functionality of the product is and what they hope to gain from usability testing in such an environment . <p> Interruptions are the norm . Even though participants were in a screened inner booth , interruptions happened and had to be accommodated . An example includes participants answering their cell phone . As well , some participants had to leave partway through the test due to conference talks or catching the last train home . Unlike a normal usability test , we could not expect people to set aside a fixed block of time solely for our purposes . <p> Participants perceive the test primarily as a demonstration . The trade show is a place to gather materials and see demonstrations . Even though we told people they were in a usability test , they still thought of it as an opportunity to try out the system , i.e. , they did not really dwell on the fact that @ @ @ @ @ @ @ @ @ @ , a participant responded to a cell phone call from a colleague by saying " Yeah , I 'm in a demo right now . I want to buy this software , ok bye . " To keep in this spirit , our final question was " would you buy this software ? " As well , participants had the opportunity to sign up to get beta-releases of the system . <p> Tag teaming and active intervention . We found the best sessions were when the two experimenters were able to tag team each other rather than working alone . Although we tried working alone , there were times when note-taking disrupted the natural conversation with the participant . Key observations could have been missed , and the participant ( whose time is precious ) had to wait for the note-taker to catch up . Tag teaming allowed us to engage and disengage with the participant . One of us would write notes while another would pick up with a thread of interest . Tag teaming was a better fit to the trade show atmosphere , where we could engage @ @ @ @ @ @ @ @ @ @ watching . This active intervention by a team meant that participants were always being observed , that notes were being taken , and that they could talk to us any time . <p> Test time is variable . We originally felt that 20 minutes was the maximum time that we could expect from any participant . In practice , and somewhat surprisingly , most participants stayed much longer than that because they became engaged with the system . We allowed people to stay longer than planned when this happened . This also meant that strict scheduling could not be done . Instead , our " gatekeeper " would feed us participants as we were able to receive them . <p> Participant flow must be regulated . Because no scheduling is done , we needed some way to control the flow of participants into the test area . In practice , there were times in the booth where participants were let in too soon after a test has been completed , leaving us scrambling to get prepared ( we needed about ten minutes between each test to collate our results , @ @ @ @ @ @ @ @ @ @ for the next test ) . The problem was that the gatekeeper was busy with his own needs ( marketing ) and sometimes used the departure of a participant as an ( incorrect ) cue that we were ready for the next one . It would have been helpful to have had a green and a red flag by the doorway for the gatekeeper 's benefit ( red meant busy , green is ready for more test participants ) . <h> Conclusion <p> ERUT is a valuable adaption and combination existing methodologies to use in public trade show situations where a company exhibits its products . A wide array of actual and potential customers are coming to these exhibits of their own accord . Being able to get a product in front of them for their evaluation is very attractive , and for some companies may be the only chance to run usability tests with true domain experts . ERUT can be both effective and inexpensive . It can provide guidance to what product features really matter to customers and where major usability ( and usefulness ) problems exist . @ @ @ @ @ @ @ @ @ @ i.e. , the validity of the selling proposition ) , software development ( i.e. , features to include , exclude , refine ) , and-most importantly as usability practitioners-those key areas of the product that should be evaluated using more formal HCI techniques . ERUT can also validate learning gained from rapid field methods such as contextual interviews , or from other methods such as heuristic reviews ( Thomas , 1996 ) , or even the external validity of laboratory-based usability test results . <p> When working a trade booth the participant is in control of the time and its use . Expect interruptions and be fluid enough to change from a usability tester 's hat to a business hat . Remember that participants are in the booth for their benefit first , so your rapport with them in regards to questionnaires and test tasks must engage them on their level . When this is done well our experience is that extremely rapid usability testing can be an effective way of gathering user feedback in a trade show environment . As a rapid method to get in front of customers @ @ @ @ @ @ @ @ @ @ . <p> There are cautions . As Thomas ( 1996 , p.112 ) notes , results from quick and dirty methods are " illustrative rather than definitive . " This method can provide insights only into usability issues . The results are not gospel and thus one must guard against the project stakeholders who treat this as the only evaluation procedure ( especially if the results are very positive ) . Similarly a trade booth environment can generate its own excitement and could give a false sense of product success . There are also valid arguments against discount usability methods ( Cockton &amp; Woolrych , 2002 ) . Certainly , we need more experiences and debate within HCI regarding collecting user feedback in such environments . <h> Acknowledgements <p> Mark Pawson thanks his colleague and friend Marc Shandro in challenging him to join him at PhotoPlus . Most of all , thanks to Athentech Technology Inc. ' s president , and Mr. Pawson 's former mentor , Jim Malcolm and vice president Brad Malcolm for giving him the opportunity to be engaged in such a unique project . 
@@105507148 @907148/ <h> President 's Corner ( Jan , 2014 ) : Looking forward to an amazing year <p> First off , a big thanks to our immediate past president , Rich Gunther , for leading UXPA for these past two years and who will provide continuity to the UXPA board as secretary in 2014 . <p> UXPA has meant a lot to me and has been a professional home to me for the past ten years , first at a local chapter level , and shortly after , at the international level as well . I am very much looking forward to leading UXPA this year , helping to grow an organization that in turn has so much ability to help other user experience professionals along their career journey . <p> This will be a year of partnerships : a year when we seek to work with other organizations for mutual benefit . If you are involved with leading another organization and we can come together " to be stronger together , please let me know . <p> This will be a year of learning : a year where we @ @ @ @ @ @ @ @ @ @ both those who are new to the field and those who are established in their careers . <p> This will be a year of building a global community . There are strong UXPA chapters around the globe , and by coming together , we become a part of something big- something not bound by geography . <p> This will be a year of connections , of providing opportunities for networking and ways to grow our professional networks . 
@@105507154 @907154/ <h> UXPA2014 is 21-24 July , 2014 . Be part of it . <p> A robust program with five tracks branching from the conference theme - Motivation ; <p> Inspiring keynotes with compelling stories and insights ; <p> Exhibits from UX vendors and recruiters of all stripes ; <p> Networking on a truly international scale ; <p> Case studies from top brands and organizations ; <p> The return of the Hero Scholarship program ; <p> Fun London excursions ; <p> And a few surprises we 're not quite ready to share ! <p> - <p> How can you be part of it ? - <p> Submit ! - No matter where you 're from , who you are , or how long you 've been practicing or studying UX , we want to know about all of the cool stuff you 've been working on and thinking about . ( Watch this space for announcements ! ) <p> Review ! - This is a GREAT way to get a sneak peek at the newest stuff happening in the field , months before the conference itself . It 's also @ @ @ @ @ @ @ @ @ @ that is the UXPA International Conference . <p> Attend ! - It wo n't be the same without you. - London is an incredibly easy city to get to and navigate , and the UXPA-discounted Park Plaza room block will soon be taking reservations . - <p> Sponsor ! - What better way to bring your message to hundreds of the smartest , most dedicated UX professionals in the world ? Whether your goal is selling , informing , or recruiting , UXPA attendees are eager to listen and learn . Email- conf2014@uxpa.org- to get started on your custom sponsorship package 