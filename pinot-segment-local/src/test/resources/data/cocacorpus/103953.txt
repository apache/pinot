
@@97506068 @1706068/ <h> UniProtKB Accession Number(s) <p> Search for structures by UniProtKB Accession Number , the unique identifier of an entry in the UniProtKB sequence database . <p> The UniProt Knowledgebase ( UniProtKB ) is the central database for sequences and functional annotations of proteins . The UniProtKB accession number is the unique alphanumeric identifier of each entry in UniProtKB . <p> A PDB structure may contain several chains , i.e. several separate macromolecules , and UniProtKB annotations in the PDB are provided at the chain level . The vast majority of protein chains in the PDB are annotated with a UniProtKB accession number . A search of the PDB by UniProtKB accession numbers is a query for structures that contain a chain that has been annotated with one of the accession numbers . <p> One or more UniProtKB accession numbers can be typed or copied and pasted in the search box . Multiple accession numbers can be separated by commas or white space , including line breaks . <h> Examples <p> Enter P69905 into the text box . Press " Result Count " to preview the number of chains in @ @ @ @ @ @ @ @ @ @ number . Press " Submit Query " to retrieve the results . On the Query Results Browser page , click on the PDB I 'd , thumbnail image , or title of any structure to load its Structure Summary page . The UniProtKB accession number can be found on the " Sequence " tab . 
@@97506070 @1706070/ <p> The Biology WorkBench is a web-based tool for biologists . The WorkBench allows biologists to search many popular protein and nucleic acid sequence databases . Database searching is integrated with access to a wide variety of analysis and modeling tools , all within a point and click interface that eliminates file format compatibility problems . <p> Forgotten Pasword : we 've noticed that most people that forget their password are actually using the incorrect user name . Our user names are case sensitive , so " JohnDoe " , " johndoe " , and " JOHNDOE " are all different names . If you still can not log in , there are two ways you can get at your old data , once we verify you own the account in question . One option is for you to register for a new user name , and we can transfer the data from your old account to your new account once you mail us your old and new user names . The other option is for us to remove the password to your old account , which allows you @ @ @ @ @ @ @ @ @ @ show up once you log in again . Please mail bwbhelp@sdsc.edu and let us know which option you prefer . <p> The Biology Student Workbench group at the University of Illinois National Center for Supercomputing Applications has developed a number of lessons which use the Biology Workbench , as well as a How To tutorial . Also , we have written a Frequently Asked Questions document for our users , and a list of recent updates . If these documents do not help you with your problem , please send a message to bwbhelp@sdsc.edu . <p> Suggested Web Browser : the Biology Workbench was originally developed for Netscape Communicator or Navigator , up through version 4.7x . Microsoft Internet Explorer ( especially older versions ) can be unpredictable when loading the Biology Workbench , but the latest versions of Explorer seem to work fine . Because we are unable to force Internet Explorer to open seconary windows with our software , showing database records and reading help pages can be a bit clumsy . Nonetheless , most Biology Workbench operations *should* work within Internet Explorer , Firefox , or other @ @ @ @ @ @ @ @ @ @ go away when one clears the disk cache , and turning off the disk cache altogether when using the Biology Workbench might be a good idea . Also , your memory cache should be set as high as comfortable , as some of our pages can take up quite a bit of space in your browser . We suggest a minimum value of 10 megabytes for your memory cache , if possible . <p> Structure Viewing : PDB structures can be viewed for PDBFinder records that are returned from a database search . One way to do this is to use the Rasmol program . The Chime plugin is another option for viewing structures on Windows and Macintosh machines , and we may eventually provide a Java-based structure viewer . For molecules with PDB structures , we also provide links to the PDB Structure Explorer page for that particule molecule , and to the Protein Explorer display for that particule molecule . <p> Collaboration within a former NCSA AT team has led to the inclusion of structure-based tools within the Biology Workbench . Some of these tools can be found @ @ @ @ @ @ @ @ @ @ on the Biology Workbench . These tools have been given the " Alpha " designation for two reasons : they were developed by a separate team , meaning we generally can not provide full support on them , and they are not fully integrated with the other Biology Workbench tools . 
@@97506071 @1706071/ <p> Here are some basic Unix commands for navigating and exploring data . Since Unix is often run in a command-line environment , you will have to type commands to see , edit and change the files and move between them . Here are the very basic commands you will need to know to run DEX with confidence . <p> Listing files and folders , type : <p> ls - Lists the files and folders in the current directory <p> ls -lt - Lists more information about all the files/folders in the current directory , one file/folder per line <p> Pattern matching <p> * - Example : " ls time* " will list every file that starts with the word " time " while the command <p> " ls time*.dat " will match everything in the folder that begins with " time " and ends with " . dat " . This is a very powerful feature of Unix and should be used to reduce the number of commands are need to execute a program . <p> Tab - The " Tab " button will complete the name of @ @ @ @ @ @ @ @ @ @ example , if there is only one file that begins with " time-2 " then simply typing " ls time-2 ( Tab ) " will complete the file without having to type the rest of the letters . This saves a lot of time when most of the files are unique . <p> Ctrl d - This is typed if you have finished typing part of a filename and you need to find out which files have that same beginning part in their names . For example , typing " ls time-2 ( Ctrl d ) " will display all the files that have that beginning of the filename " time-2 " . After typing this , you can see what the exact name of the file you wish is named and type in the rest of the name . When you combine " Ctrl d " with the Tab button , you typically only need to type a few letter per name , even if it is long . This saves a considerable amount of time . <p> Up arrow - Typing the up arrow on the command line @ @ @ @ @ @ @ @ @ @ You can just modify this one or run it again to see the results of the same or new file . <p> Changing folders/manipulation of folders <p> cd folder-name - folder-name is the name of the folder you want to move to <p> pwd - Displays what folder you are currently in <p> mkdir - Creates a folder ( directory ) <p> rmdir - Removes a folder ( directory ) but it must be empty <p> Changing/moving file/filenames <p> cp old-filename new-filename - Copies the old file to the new filename without erasing the old file <p> mv old-filename new-filename - Changes the old file to the new filename and erases the old file <p> rm filename - Removes the filename you type <p> Printing a file <p> lpr Pprinter-name - printer-name is the name of the Unix networked printer . If you do not have a printer setup with the Unix commands , you will have to find the file in the window system and print how you would normally print it . Cygwin users : You can open regular text files in MS Word and save and print @ @ @ @ @ @ @ @ @ @ vi filename - vi is a visual program to view the text in a file such as the results from a program . <p> head filename - Shows the first 10 lines of a file <p> tail filename - Shows the last 10 lines of a file . <p> Executing/Stopping programs <p> To execute a program , you only need to type its name . You may need to type " . / " before the name , if your system does not check for executables in that file . <p> Ctrl c - This command will cancel a program that is running or wont automatically quite . It will return you to the command line so you can run something else . <p> vi program <p> Once you type the command above , the window will display the text . Unlike a regular document program , vi does not let you just type using the keyboard . Instead , each key has a function , such as move the cursor left or down . Here are the basic commands you can use to edit and save the commands . @ @ @ @ @ @ @ @ @ @ and must be used to move the cursor to the place you want to start typing/editing . The mouse will not work in the standard " vi " program , only the keyboard is used . This takes some getting used to . <p> i - insert is similar to " a " but is to the left of where the cursor is . <p> a - appends to the data , after typing " a " you can use the keyboard to type in letter/numbers like a normal word processing program . When you are done typing what you want at that location , you need to hit the " Esc " button before you can return to the regular mode . <p> Esc - escape ends the current mode you are in ( i.e. typing mode ) and takes you back to the original . <p> x - Typing " x " lowercase will erase the character on the cursor , like the " Backspace " key does in a normal word processing program . The " Delete " button will do the same thing in the regular @ @ @ @ @ @ @ @ @ @ you made to a existing file name . <p> : w new-filename - saves all the changes you made to a new filename you specify . <p> ZZ - Capital ZZ . This will save and quit the " vi " program and bring you back to the command line . <p> : q ! - This will quite the " vi " program without saving the last changes you made . <p> u - This is undo , and will undo the last action you have completed . <p> dd - This will erase the entire line . Tying a number before typing " dd " will remove the number of lines chosen . <p> : se nu - This is to " set number lines " which will temporarily number the lines of the document for you . 
@@97506072 @1706072/ <h> Ligand Explorer <p> A viewer that visualizes the interactions of bound ligands in protein and nucleic acids structures . Ligand Explorer has options to turn on the display of interactions including hydrogen bonds , hydrophobic contacts , water mediated hydrogen bonds , and metal interactions . <h> How do I launch Ligand Explorer ? <p> A link to Ligand Explorer is available in the Ligand Chemical Component widget on the structure summary page . <h> How do I rotate or move the Structures ? <p> To quickly get information on structural details you may mouse over structural features in the molecular viewer window as well as the sequence viewer at the top . Information regarding structural details such as residue number and element type is displayed in the status bar at the bottom of the viewer . <p> Basic Commands <p> Rotate <p> Click ( windows : left click ) in the 3D window and drag the mouse up , down or left and right . <p> Zoom <p> Hold down the shift key while dragging the mouse up and down . <p> Translate <p> Hold down the @ @ @ @ @ @ @ @ @ @ window . <h> How do I display Interactions with the Ligand ? <p> First , select a ligand from the left hand menu . If you launched Ligand Explorer from the Ligand Chemical Component widget , a ligand is already selected . Second , click the check boxes next to the list of interactions to turn on the interactions . <p> Interacting residues and the distance of the interactions are automatically displayed , as shown in the example below . Interacting residues are also highlighted with grey boxes in the sequence display . <h> What types of Interactions are available ? <p> Hydrogen bonds ( pink lines ) <p> Hydrophobic interactions ( green lines ) <p> Bridged hydrogen bonds ( water mediated hydrogen bonds ) ( blue lines ) <p> Metal interactions ( gray lines ) <p> Neighbor residues ( interactions with surrounding residues ) <p> Each interaction type has a default distance threshold , i.e. 3.3 A for hydrogen bonds . You can change these values by typing in a new value and hitting the Enter key . 
@@97506074 @1706074/ <h> MyPDB Login <h> HTTP and HTTPS Services <p> PDB entry files , chemical component files , and other data files are available for Display and/or Download via http and https . These URLs are useful with scripted downloads using utilities such as wget . <p> Options are available to view data in a web browser and to allow the web browser to perform a background download of the data without showing the data in a web browser and requiring a save page as action . <h> Differences <p> View : The HTTP/HTTPS response headers to the client are set with : Content-Type : text/plainDownload : The HTTP/HTTPS response headers to the client are set with : Content-Type : application/octet-stream and **25;0;TOOLONG : binary . <h> Similarities <p> Identical data <p> No difference in **29;27;TOOLONG over the network <p> Both work over HTTP and HTTPS <h> URLs for File Types <h> PDB Entry Files <p> PDB entry files are available in several file formats ( PDB , PDBx/mmCIF , XML ) , compressed or uncompressed , and with an option to download a file containing only " header " @ @ @ @ @ @ @ @ @ @ . <h> Small Molecule Files <p> Small molecule files , including the ligands/chemical components maintained in the Chemical Component Dictionary and the Biologically Interesting Molecule Reference Dictionary ( BIRD ) are available in multiple formats . 
@@97506075 @1706075/ <h> Recent Changes/Updates <h> 05/11/2011 - SwissProt divisions <p> In order to make our popular database searches more efficient , we have split SwissProt into ten taxonomic divisions defined by the UniProt group . The content of each database should be self explanatory . There are no overlaps between divisions , so human and rodent database records are not in the mammal or vertebrate divisions , and the mammal database records are not in the vertebrate division . <h> 01/03/2011 - Genbank PAT removal <p> We have removed the GenbankPAT ( Patent Sequnces ) database . <h> 9/3/08 - RPS BLAST removed <p> RPS Blast has been removed from the Biology Workbench , as it is no longer working with our system and we do n't have the personnel to get it working again . Many other programs ( for example HMMPFAM ) can be used to compare a sequence to domain or motif databases . <h> 9/3/08 - BLAST updated <p> The Blast programs from NCBI has been updated to version 2.2.18. <h> 09/01/2008 - Genpept TrEMBL removal <p> The Genpept and TrEMBL ( UniProt ) databases will @ @ @ @ @ @ @ @ @ @ they are mostly redundant to the protein translations available in the Genbank divisions , and our disk space is at a premium . The SwissProt portion of UniProt is still available . <h> 08/29/2006 - Genbank ENV addtion <p> By request , we have added the Genbank ENV ( Environmental Sample Sequences ) database . <h> 07/01/2006 - Genbank EST removal <p> We have removed the Genbank EST division from our databases - both the flat files and the Blastable databases . The EST division uses large amount of our resources , but does not get a lot of usage . Furthermore , because of the limitations of the Biology Workbench , we can not provide the ability to do a lot of meaningful analysis with the EST database ( maybe a reason for its limited usage ) . Workbench users that want to do work with the EST sequences can search or Blast against the EST databases at NCBI or EBI , and then import the Fasta format EST sequences to the Biology Workbench. <h> 05/12/2006 - Genbank Refseq changes <p> The complete Refseq database has grown too large @ @ @ @ @ @ @ @ @ @ We have replaced it with with the individual divisions ( for example , mammalian organisms , plasmids , etc. ) , much like the regular Genbank Releases . Also , like in Genbank , the Refseq updates for all divisions since the last full release are all stored in one database . <h> 12/05/2005 - PIR protein database removal <p> We have removed the PIR protein database from our system . PIR is now part of the UniProt consortium , and it is no longer being maintained as a separate database . The SwissProt and TrEMBL databases should be used as a replacement for PIR. <h> 09/23/2005 - Genbank GSS and HTG removal <p> We have removed the GenbankGSS ( Genomic Survey Sequnces ) and GenbankHTG ( High Ghroughput Genomic ) databases . Very few people used these databases and sequences , and we are running out of available disk space for our local databases . <h> 03/24/2005 - FUNDING UPDATE : <p> The Biology Workbench Team learned this week that our proposal for new development of the Biology Workbench has been approved , and funding will begin around 4/1/2005 @ @ @ @ @ @ @ @ @ @ use and who wrote letters of support for the Workbench ; your help and support was instrumental in our success . We encourage you to flood us with input and suggestions as we develop the Next Generation Biology Workbench . Watch this space for news and updates surrounding our local development and beta releases of the new site . <h> 02/01/05 - New Genomic Databases were added : <p> honeybee ( Apis mellifera ) , fly ( Drosophila melanogaster ) <h> 02/01/05 - Most Genbank Bacterial Genomes databases were removed <p> databases , due to the large number of them ( we lack the dataspace and the manpower to keep up ) . If you wish to work with a certain bacterial genome database that is no longer on the database list , please contact us : bwbhelp@sdsc.edu <h> 01/10/05 - Restore Session tool <p> We have added a tool which restores session files from the last backup of a user 's sessions . Users that have their data accidentally deleted can use this tool to get a copy of their session files as of the previous evening . Users @ @ @ @ @ @ @ @ @ @ should be be able to restore their work as of their last Workbench visit . <h> 07/10/03 - HMMPFAM update <p> The HMMER package ( of which HMMPFAM is part ) has been updated from version 2.2 to version 2.3.1 . The new version of HMMPFAM is significantly faster than the old one . <h> 05/28/03 MitoProteome Protein List database <p> We have added a simplified version of the MitoProteome ( mithochondrial protein database ) ( http : //www.mitoproteome.org/ ) - the Protein List . It contains almost 1000 human protein sequences , found experimentally and via public database searches . Our version contains only the name , function , public database IDs and protein sequence information . This database allows protein sequence comparisons with Blast , FastA and other tools . The records are linked to the MitoProteome website ( http : //www.mitoproteome.org ) for more information on the protein ( disease , gene information , domains , interactions ) . <h> 03/24/03 - EXTCOEF - extinction coefficient at 280 nm calculator <p> This tool calculates the extinction coefficient and absorption of a protein at 280 nm , based @ @ @ @ @ @ @ @ @ @ Tyr and Cys residues ) . The calculations are done based on the original formula by Gill and von Hipple ( Anal . Biochem. 182 , 319-326 ; 1989 ) : e = 5690 x ( #W ) + 1280 x ( #Y ) + 60 x ( #C ) . This formula assumes that ALL Cys residues appear as half cystines ( i.e involved in S-S bridges ) . Cysteine residues do not absorb appreciably at wavelengths &gt;260 nm , while cystine does . A second formula is also used , in which there is no contribution from the Cys residues , equivalent to the fact that none of the Cys residues appear as half cystines . ( Conditions : 6.0 M guanidium hydrochloride ; 0.02 M phosphate buffer ; pH 6.5 ) <h> 01/10/03 - Alliance for Cellular Signalling ( AfCS ) protein database <p> We have added a simplified version of the AfCS Molecule Page database ( http : **28;58;TOOLONG ) . Our version contains only the AfCS I 'd , synonyms , category and protein sequence information for all the AfCS proteins that have a protein sequence @ @ @ @ @ @ @ @ @ @ are mouse , human if mouse is n't defined , rat if neither mouse nor human are defined , and in a few cases something else ( e.g. cow , Drosophila ) when the molecule could n't be defined with mouse , human , or rat . There is a Blastable component to this database , for proteins -- so it can be accessed with BlastP or BlastX . The records are linked to the AfCS-Nature Signaling Gateway Molecule Page website for more information on the protein . One needs to have an account in order to access the Molecule Pages on that website . <p> We have added as blastable dabases subsets of **26;88;TOOLONG databases , that contain only human or only mouse protein sequences . The combination of Swissprot , TrEMBL and TrEMBLnew human/mouse only subsets would contain the vast majority of human/mouse protein sequences publicly available . This is useful when one wants to blast against human protein sequences only , for example . The Swissprot and TrEMBL subsets have very little overlap in sequences . <h> 12/18/02 - PRIMERCHECK and PRIMERTM - new primer tools <p> @ @ @ @ @ @ @ @ @ @ of a given short nucleic sequence , in particular , a primer . GC content for any nucleic sequence can also be calculated with NASTATS tool . PRIMERTM designs primers of minimum length , that start at the ends of the two strands and that have a melting point above a minimum desired temperature Tm . In both tools the sequence has to be selected from the user 's data and should not contain characters other than A , C , T , G. Salt and oligos/DNA concentrations WILL affect the calculations . In both , the calculations are done essentially as described by Breslauer et al. , in P.N.A.S. , 1986 and by Rychlik et al. , N.A.R. , 1990 . For a more complex primer program see PRIMER3. <h> 12/16/02 - BLAST updated <p> The Blast package from NCBI has been updated from version 2.2.2 to version 2.2.5 . <p> We have removed the sequences that contained both nucleic and amino-acids from the blastable databases PDBFINDER and PDBSEQRES , such as some ribosome and tRNA sequences , or DNA comlexed with an amino acid or enzyme complexed with @ @ @ @ @ @ @ @ @ @ contained odd characters , like " ? " . The " mixed " records and sequences are still available for a NDJINN search however . Attention should be paid when importing complex/hetero- sequences that contain both Amino-acids and Nucleic-acids ( such as 1TTTD , E , F ; 1RGA ; 2EDA ; 2ARG ; 1FFZB ; 1OLD ; 1M905 ; 1B23R ; 2FMTC , D ; 1C95A , B ; 1FG0B ; 1FIRA ; 1KQS4 etc . ) . Note that many protein sequences from PDBFINDER ( and PDBSEQRES ) contain the character " X " , and that many nucleic characters have meaning as protein characters and vice-versa . <h> 10/09/02 - Bugs fixed in PATTERNMATCH tools <p> We have fixed a bug in PATTERNMATCH and PATTERNMATCHDB tools that allow now the use of the negation character " X " . We have added to the Help page the note that Perl regular expression " A+ " does not work ( the " + " character does not work ) - " A1 , " should be used instead . We have also limited the length of the regular @ @ @ @ @ @ @ @ @ @ or protein sequences . Those searches can be done with other tools , like Blast , and they are not the purpose of PATTERNMATCHDB. <h> 09/15/02 - PRODOM consensus sequences are no longer in the Non-Redundant database <p> The PRODOM consensus sequences are no longer included in the SDSC Non-Redundant Database since they are not " real " protein sequences . We continue to offer PRODOM as a blastable protein database though . The only other blastable protein databases not included in the Non-Redundant Database are the Dictyostelium ORFs sequences ( from Genomes databases ) . <h> 07/19/02 - Easier sequence download added to " View " Tool <p> We now have made it easier to download sequences from within the View tool ( previously , one could only do that from Netscape , or they had to use the Download function ) . This will allow the user to download any sequence(s) they want , in all the formats the Biology Workbench can interpret . This is quite useful for porting sequences to other applications , or for backing up data . The link to get all the sequences @ @ @ @ @ @ @ @ @ @ of the page . To get a text file of the sequences you are viewing , right-click on the link , or save the page that opens up when you click on the link . <p> A separator line ( a line full of the equal sign : " = " ) is used to separate multiple alignments that are downloaded . This is necessary , because otherwise there is no easy way to designate separate alignments . Note : the Biology Workbench can not read in more than one alignment at a time ; trying to do so will lead to an error . If you are saving alignments to be reloaded to the Biology Workbench later on , you will want to save each one to an individual file . <h> 6/28/02 - Genbank Mus musculus Genomic database <p> The Genbank Mus musculus Genome database has been added . It has two components -- one for the nucleic contigs , and one for the mRNA sequences ( and their translated proteins ) . <h> 04/10/02 PI - isoelectric point calculation tool <p> A tool to calculate the isoelectric @ @ @ @ @ @ @ @ @ @ developed at EMBL WWW Isoelectric Point Service . Molecular weight can be calculated using AASTATS tool . <h> 04/08/02 Genomic databases updates <p> Fission yeast Schizosaccharomyces pombe and parasite Encephalitozoon cuniculi genomes and CDS/proteins from NCBI genomes have been added . As usual , we keep adding new bacterial genomes as they become available from NCBI-Genbank. <h> 04/02/02 FASTA scoring matrices change <p> Because of format problems we now offer only the scoring matrices that were distributed with the original FASTA package ( versions 2.0 and 3 ) ( including SSEARCH , ALIGN , LALIGN , LFASTA ) . <h> 02/22/02 HMMPFAM update <p> Pfam has now two different types of models , the glocal models ( " ls " mode , in the Pfamls HMM database ) and Smith/Waterman models ( " fs " mode , in the Pfamfs HMM database ) . In glocal mode , only full-length complete domains are found . In Smith/Waterman mode , fragmentary domains can also be found , because fully local alignments are allowed . " ls " mode is much more sensitive than " fs " mode , but only if @ @ @ @ @ @ @ @ @ @ deleted fragment is present , " fs " mode will be needed . The two modes have different curated cutoffs . The old flatfiles called Pfam ( standard ) and PfamFrag have now been deprecated . HMMER version used has been updated to 2.2. <h> 12/06/01 MVIEW -added choosing reference option <p> We have added an option to choose the sequence to be used as reference when computing identities - by the place in the alignment , counting from top . <h> 12/05/01 Genomic databases updates <p> Arabidopsis thaliana genome and proteins have been added . Most bacterial genomes are now mirrored as the NCBI RefSeq complete genomes . More information can be found at the current ftp site ftp : **26;116;TOOLONG in the README files . We continue to add new bacterial genomes as they are released by Genbank. <h> 12/04/01 SIXFRAME - added alignment option <p> We have added the option of displaying the nucleic sequence which is being translated aligned with its corresponding translation . We have highlighted the methionine ( M ) residues as well as the " stars " which correspond to stop codons . @ @ @ @ @ @ @ @ @ @ 5,6 first one , or two , nucleotides , respectively are ommitted in the alignment . This is still experimental so feel free to send us your suggestions . <h> 11/30/01 FASTA - changes in display of the results <p> We have changed the display of results of all fasta programs , from a scroll-down list to a table with checkboxes . This should make it easier to select the sequences . We have also provided links between the sequence in the table and the corresponding alignment , making it easier to get to the desired alignment down the page and back . <p> We have created a database based on the PDBFINDER database records , in which the sequences are derived from PDB . The database contains only those records from PDBFINDER for which there is a sequence with the same I 'd in PDB . The sequences for those records are then taken from the updated listing of all PDB sequences in FASTA format ( therefore there may be slight differences betweeen sequences with the same I 'd in PDBSEQRES and in PDBFINDER ) . <h> 10/22/01 - Ndjinn - @ @ @ @ @ @ @ @ @ @ the display of Ndjinn giving the user the choice of using either a full display mode with checkboxes or a compact display mode with a scroll-down list . The default is now full display mode with checkboxes and none of the matches are selected . This should make it easier to view/identify and select the sequences since in a scroll-down list the names of the sequences are sometimes truncated . The compact display could be more useful when one needs to select a lot of sequences or for a quick view of all the hits . <h> 10/12/01 - BLAST and CLUSTALW - changes in display of the results <p> We have changed the display of results of all blast programs , including RPSBLAST , from a scroll-down list to a table with checkboxes . This should make it easier to select the sequences . <p> We have also provided links between the sequence in the table and the corresponding alignment , making it easier to get to the desired alignment down the page and back . In the case of PSI-BLAST , the links are provided only for the @ @ @ @ @ @ @ @ @ @ hit summary table and the different iterations resuts . For RPSBLAST we have provided links between the domains in the results table and the corresponding NCBI Conserved Domain Database entries . <p> For CLUSTALW we have changed only the display of the selected sequences , providing both FASTA and Workbench labels . This should make it easier to identify the sequences in the alignment . <h> 9/07/01 - RPS-BLAST addition <p> The RPSBLAST tool has been added to the Protein Tools . RPS-BLAST ( Reverse PSI-BLAST ) searches a query protein sequence against a database of profiles . Select All or one search database of the Conserved Domain Databases which currently contains domains derived from Smart and Pfam , and alignments from the LOAD-database ( Library Of Ancient Domains ) . <h> 8/25/01 - ProDom addition <p> The ProDom protein domain database has been added as a searchable and Blastable database . Select " ProDom " from the list of protein databases in any tool that compares a sequence to protein databases ( for example , BlastP ) to do a sequence search . The ProDom Blastable database is made @ @ @ @ @ @ @ @ @ @ <h> 8/15/01 - ClustalW optimization <p> ClustalW has been optimized , and you may notice faster performance , especially for those alignments that take a longer time to complete . <h> 8/13/01 - Genomic databases <p> We have converted our genomic databases into a more usable format . These genomic databases have two components : " genome " and " CDS and proteins " . The genome component is the entire Genbank record pertaining to a large sequence fragment - usually a chromosome . The CDS and proteins database contains all the protein sequences identified in the genome , and all the CDS regions that code for those proteins . The CDS and proteins database is of more use to most people using the Biology Workbench . Importing the large nucleic sequences from the " genome " database is quite dangerous , and often can lead to a crashed session - users should avoid doing this ( see the FAQ for more details ) . <h> 7/31/01 - TMAP <p> TMAP has been updated to edition 55 , and a single-sequence version of TMAP edition 52 has been added . @ @ @ @ @ @ @ @ @ @ , though the author still recommends using alignments when possible to get more accurate results . <h> 7/17/01 - Blast update <p> The Blast programs have been updated to version 2.2.1 <h> 6/22/01 - Account time limit implemented <p> Due to extreme growth , it has become necessary to implement an account lifetime . If an account has not been accessed in 6 months , all of its data may be deleted . The username/password combination will still be reserved , though . <h> 6/22/01 - Genpept split into 2 databases <p> Genpept has been split into 2 databases : the full release and updates . The full release contains the gene products in the last full Genbank release , and " updates " contains the gene products in the Genbank updates ( Genbank New ) . <h> 6/14/01 - Non-redundant database changes <p> The Biology Workbench non-redundant protein database has had a few changes . The Blastable file now contains text information , which in most cases should give an idea of what the sequence represents . Also , we no longer display it as a separate choice from @ @ @ @ @ @ @ @ @ @ , so we consider it a separate database . <p> Selecting the non-redundant database *and* any other database would be pointless , as the non-redundant database contains information from all the Blastable databases . <h> 4/1/01 - Genbank Indexing <p> All the Genbank and Genpept databases have been reindexed by GI number . Though this does n't effect any data on a cosmetic level , if you try to use " View Database Records of Imported Sequences " on any sequences from those databases that were imported before this change , the function will not work . <h> 3/17/01 - Genbank Homo Sapiens Genome <p> The Genbank Homo Sapiens Genome database has been reformatted . It now has two components -- one for the nucleic contigs , and one for the mRNA sequences ( and their translated proteins ) . <h> MVIEW <p> We have added Mview , a program for producing color-coded HTML views of sequence alignments . <h> TMHMM <p> We have added TMHMM , a program for finding transmembrane regions in single protein sequences . Previously , one could only do this with TMAP -- a program @ @ @ @ @ @ @ @ @ @ number of Blastable databases in Genbank has been greatly reduced , by combining the smaller files into larger ones . This should make it considerably more convenient for the users to Blast against Genbank ( for examples , the 89 EST sections are now represented by only 4 Blastable databases ) . <p> All of the Blast programs have been updated to version 2.1.2 . The problem with PSIBLAST crashing when multiple databases are selected seems to have gone away . <h> Clustal W <p> We updated ClustalW to version 1.81 , and now use Phylip to display the guide trees as images ( if desired ) . <h> Fasta <p> The major Fasta tools have been updated to version 3.307b . This includes FASTA , TFASTA , TFASTX , TFASTY , FASTX , FASTY , and SSEARCH . <h> Internet Explorer bug fixed <p> Some of our HTML pages were causing Microsoft Internet Explorer version 5.5 to crash . We were able to find the errors in our HTML that led to the crash , and they have been fixed . Internet Explorer should n't be so sensitive that @ @ @ @ @ @ @ @ @ @ <h> HMMPFAM and BLIMPS have been added <p> Two utilities for comparing protein sequences to motif databases have been added to the Biology Workbench . HMMPFAM compares a seqeunce to the Pfam motif databases , and BLIMPS compares a sequence to the BLOCKS motif databases . <h> PSIBLAST <p> PSIBLAST has been updated with a few new output features . Users can choose to view 1-line descriptions and alignments from every iteration or just the last iteration . By default , 1-line descriptions are shown from every iteration , and alignments are show for the last iteration . <p> Before , the users could only see the results from the last iteration . <h> SENSEI <p> Sensei has been removed from the Biology Workbench , due to extensive memory use by the program . We will give the location of the source code to those interested in running it themselves . <h> PROSITE <p> PPSEARCH has been added as another regex-based ( i.e. text comparison ) search tool for the Prosite motif database . The " PROSITE " script for searching the PROSITE database for a particular sequence has been @ @ @ @ @ @ @ @ @ @ The PROSEARCH and PPSEARCH programs accomplish the same task , and gives correct results . <h> TeXshade <p> We have added a new program for coloring alignments called " TEXSHADE " . This script uses Eric Beitz 's TeXshade programs ( specifically , LaTeX style sheet ) to colorize alignments . It offers many additional options to those offered by BOXSHADE , and since the postscript output is produced by LaTeX and dvips , it is likely to be more compatible across all systems and postscript viewers and interpretors . We suggest those that have used BOXSHADE give this program a try . <p> Note that the colored alignment might not look nearly as " crisp " on the screen as it does on paper , as the conversion of postscript files to gif images can blur the fonts . We do think you will notice the difference on paper , though . <h> Clustal W <p> Profile alignments are now available on the Biology Workbench , via the CLUSTALWPROF tool . In the Alignment tools , this allows one to align two alignments of the same type . In @ @ @ @ @ @ @ @ @ @ or more protein sequences to an existing protein alignment within the current session , and in the Nucleic tools align one or more nucleic sequences to an existing nucleic alignment within the current session . <p> This will be very useful for people that want to align a few sequences to an alignment they had already made , or to construct " super " alignments out of smaller alignments . <p> Also , we now have implemented a limit on Clustal W , so that sequences or alignments with lengths over 5,000 will not be allowed . This was added because we have had a large number of people try to use Clustal W to align sequence fragments to entire genomes -- an application for which Clustal W was not intended . These jobs used a lot of our resources , and led to alignments which for all intenets and purposes were meaningless . <h> FingerPRINTScan <p> We have added FINGERPRINTSCAN , a program which compares a protein sequence to Fingerprints within the PRINTS motif database . <h> Databases <p> Many databases have been added to the Biology Workbench . @ @ @ @ @ @ @ @ @ @ DBCAT database is a database of databases , and can give more detailed information on a particular database . <h> Non-redundant protein database <p> We have added a non-redundant protein database that includes all of the protein databases within the Biology Workbench . This database uses a simple string comparison to catch redundancies , so it may not be appropriate for certain statistical calculations , but it does greatly enhance the ability to search . 
@@97506076 @1706076/ <p> This page contains links to sequence and annotation data downloads for the genome assemblies featured in the UCSC Genome Browser . Table downloads are also available via the Genome Browser FTP server . For quick access to the most recent assembly of each genome , see the current genomes directory . This directory may be useful to individuals with automated scripts that must always reference the most recent assembly . <p> To view the current descriptions and formats of the tables in the annotation database , use the " describe table schema " button in the Table Browser . <p> All tables in the Genome Browser are freely usable for any purpose except as indicated in the README.txt files in the download directories . To view restrictions specific to a particular data set , click on the corresponding download link and review the README text . These data were contributed by many researchers , as listed on the Genome Browser credits page . Please acknowledge the contributor(s) of the data you use . <p> The Genome Browser , Blat , and liftOver source are freely downloadable for academic @ @ @ @ @ @ @ @ @ @ commercial licensing , please see the Genome Browser and Blat licensing requirements . <p> Genome Browser Mirror The Genome Browser Mirror is available for installing a local copy of the Genome Browser website onto your web server . We provide precompiled executable binaries for this purpose , so you do not have to compile the whole source code . <p> Troubleshooting : For slow download speeds , try using UDR ( UDT Enabled Rsync ) , which improves throughput of large data transfers over long distances ( 64/32-bit also available ) . <p> UtilitiesPlease review the userApps README for a summary of how to fetch selected directories from the kent source tree , or download userApps.src.tgz , to install all kent utilities . The utilities directory offers downloads of pre-compiled standalone binaries : <p> The /gbdb fileserver holds all files referenced by the genome browser tables . Some files in the browser are hosted in binary files such as a bigBed file that can be downloaded from our download server from the /gbdb location . <p> Examples for the hg38 database are the crispr.bb and crisprDetails.tab files for the CRISPR @ @ @ @ @ @ @ @ @ @ or the whole genome annotation from such binary files can be obtained using tools like bigBedToBed which can be compiled from the source code or downloaded as a precompiled binary for your system . Instructions for downloading source code and binaries can be found here . <p> The bigBedToBed tool can also be used to obtain only features within a given range , e.g. bigBedToBed LONG ... -chrom=chr21 -start=25000000 -end=30000000 stdout <p> Sequence files for assembly sequences used in alignment tracks , such as in the 100-species conservation track are also found in the /gbdb location . <p> The liftOver links have been moved to the corresponding assembly sections ( for example , the link for the mm5-to-mm6 over.chain file is now located in the mm5 downloads section ) . The liftOver source download link is now located in the source downloads section . 
@@97506077 @1706077/ <h> Systematic Pre-calculated Protein Structure Alignments <h> Introduction <p> Proteins can have various degrees of similarity . If two proteins show high similarity in their amino acid sequence , it is generally assumed that they are closely evolutionary related . With increasing evolutionary distance the degree of similarity usually drops . Even if the sequence similarity is low , proteins can still show similar function and have an overall similar 3D structure . The detection of such remote similarities is important in order to infer functional and evolutionary relationships between protein families and is a core technique used in structural bioinformatics . The goal is to establish regions of structural similarity between two or more molecules . <p> While protein sequence comparisons can be computed quickly , the calculation of protein structure alignments is much more time consuming . The RCSB PDB offers tools , that allow users to quickly identify protein sequence neighbors and run pairwise protein structure comparisons . To help identify more distant 3D relationships , a pre-calculated set of 3D protein structure alignments is available through the 3D similarity tab . <p> Screenshot of a @ @ @ @ @ @ @ @ @ @ the jCE algorithm , available through the Protein Comparison Tool at the RCSB web site . <h> Domain-split Representatives <h> Sequence representatives <p> Representative protein domains are being used since calculation of a real all vs. all comparison would require too much CPU time . The procedure to come up with the domain-split representative is an extension of our protein-chain sequence clustering approach . In order to remove redundancy , we start with a 40% sequence identity clustering procedure . All sequences in a cluster are sorted and are being represented by the protein chain on rank #1 . This is usually the chain with the highest resolution and has been determined by X-ray Crystallography . <h> Multiple domains <p> In case the representative chain consists out of multiple domains , each of those domains are included in database searches . If available , the domain assignment as provided by SCOP 1.75 is used . Otherwise algorithmic domain assignments are computed , using the ProteinDomainParser software . <p> Chains that are grouped together in a cluster of chains with 40% sequence similarity and then ranked , are being represented by @ @ @ @ @ @ @ @ @ @ the chain with the highest resolution and has been determined by X-ray Crystallography . If a PDB chain is accessed , that is not the representative , the results for the representative chain are loaded automatically . <p> At the present systematic comparisons contain about 1 billion pairwise alignments . These bulk of these have been calculated on the Open Science Grid . A technical report describing the details of how this calculations were run is available from renci.org . At the present weekly updates for new structures are calculated using RCSB servers . <p> The screenshot above shows the summary results that are available for a database search . In order to obtain a detailed view of the results , click on the PDB I 'd in a row . Each column in this table can be sorted . The results can be filtered based on various criteria . <p> Meaning of the column labels : <p> Rank : current row position . Changes with different sorting orders and filter rules <p> %ID : % sequence identity in the alignment . A 40% sequence identity filter is applied before the @ @ @ @ @ @ @ @ @ @ similarity . If the sequences are of vastly different lengths , the clustering procedure will group them in different clusters , even if they share a region of high sequence similarity . <p> %Cov1 : The coverage , or % , of aligned residues in chain 1 <p> %Cov2 : The coverage , or % , of aligned residues in chain 2 <p> The table is sorting is by P-value by default . Clicking on the column header will change the sort order . Select the Filter Results icon to apply other filtering criteria . <h> Interactive Structure Alignment Display <p> The pairwise view of a structure alignment can be used to investigate protein sequence and structure relationships between the sequence-representation of the alignment and the 3D display in Jmol . Regions can be selected in the bottom sequence display to see where they are in the 3D Jmol display . <h> XML Download <p> The all vs. all structural similarity results table for a representative chain can be downloaded in XML . For example , this returns Rank , PDB.Chain , Description , P-value , Score , RMSD , @ @ @ @ @ @ @ @ @ @ LONG ... Note : A maximum 2000 rows can be returned through this URL . To fetch all approx. 17,000 results for a chain , you need to slice through the results using the page parameter . <h> Algorithms <p> A number of algorithms are provided for structural comparison . The precalculated results are based on FATCAT-rigid . The downloadable Protein Comparison Tool can use CE , CE-CP , FATCAT-rigid , and FATCAT-flexible for structural comparisons , as well as the Smith-Waterman algorithm for sequence alignment . The website offers additional services for pairwise alignments , including TM-align , TopMatch , and Dali through external servers . <h> FATCAT <p> The all vs. all comparisons are based on jFATCAT , a Java port of the original FATCAT algorithm see : <p> Two flavors of jFATCAT are available . FATCAT-rigid uses a rigid-body superposition to align the two structures . FATCAT-flexible introduces ' twists ' between different parts of the proteins which are superimposed independently . This is ideal for proteins which undergo large conformational shifts , where a global superposition can not capture the underlying similarity between domains . For @ @ @ @ @ @ @ @ @ @ bound can be much better aligned with FATCAT-flexible than with one of the rigid alignment algorithms . The downside of this is that it can lead to additional false positives in unrelated structures . <p> CE performs a rigid-body superposition of the proteins , similar to FATCAT-rigid . <p> The tool also provides CE with Circular Permutations ( CE-CP ) . CE and FATCAT both assume that aligned residues occur in the same order in both proteins ( e.g. they are both sequence-order dependent algorithms ) . In proteins related by a circular permutation , the N-terminal part of one protein is related to the C-terminal part of the other , and vice versa . CE-CP allows circularly permuted proteins to be compared . For more information on circular permutations , see the Wikipedia or Molecule of the Month articles . <h> Expert Mode for RCSB PDB Protein Comparison Tool <p> Want more control in using structure alignment algorithms ? Would you like to better understand how the algorithms work by trying different parameter sets ? The new jCE/jFATCAT user interface supports manipulation of low-level alignment parameters . This option @ @ @ @ @ @ @ @ @ @ alignment algorithms . <p> As an example is the maximum gap size parameter G during the extension of Aligned Fragment Pairs of the CE algorithm . The parameter is by default set to 30 , a trade-off for performance vs. result accuracy . For the protein pair 1CDG.A and 1TIM.A , the default parameters ca n't identify the whole TIM barrel that is in common between the two chains . Removing the restriction on the parameter G ( by setting it to 0 ) increases the calculation time , but gives an alignment that is 25 residues longer . <h> PDB wide structure alignments <p> The Structure Alignment Tool also provides functionality for PDB-wide structural searches . This systematically compares a query structure against all representative structures in the PDB . Comparisons can be made to either representative chains or domains , as described above . <p> To do a PDB-wide structure alignment , use the ' Database Search ' panel of the Structure Alignment Tool . The selected output directory will be used to store results . These consist of individual alignments in compressed XML format , as well @ @ @ @ @ @ @ @ @ @ The statistics are displayed in an interactive results table , which allows the alignments to be sorted . The ' Align ' column allows individual alignments to be visualized with the alignment GUI. 
@@97506079 @1706079/ <h> Protein Workshop <p> A viewer that visualizes 3D protein and nucleic acid structures in a high-quality ribbon style . Protein Workshop offers the same default styles and colors as Simple Viewer , but has options to change styles , colors , and visibility , and add labels to atoms and residues . Protein Workshop also supports molecular surfaces to aid in the display of quaternary structure , protein-protein interactions , and binding sites . Surfaces are created for all macromolecule chains in a PDB entry using the algorithm from D. Xu , Y. Zhang ( 2009 ) Generating Triangulated Macromolecular Surfaces by Euclidean Distance Transform . PLoS ONE 4(12) : e8140 <h> How do I launch Protein Workshop ? <p> A link to Protein Workshop is available on any Structure Summary page in the Biological Assembly and Asymmetric Unit widget . <h> How do I change the Style and Colors of the Structure ? <p> Choose either " Atom and Bonds " or " Ribbons " depending on what you want to change . <p> Select an additional option , depending on the tool . <p> Choose an @ @ @ @ @ @ @ @ @ @ ) from the tree viewer or by clicking on the item in the 3D viewer window . <p> More advanced color options are available from the Shortcuts tab . <h> How do I use the Surfaces feature ? <p> To turn on surfaces , move the surface transparency slider all the way to the right to display an opaque surface . Move the slider to the left to increase the transparency . Surfaces can be turned off by moving the slider to the off position . <h> Coloring a surface <p> Surfaces can be colored by four properties : <p> Chain : each macromolecule chain is rendered in a different color or shade of color <p> Entity : unique macromolecule chains with identical sequence are colored the same <p> Single Color : all chains are colored by the same color <p> Hydrophobicity : residues are color coded according to hydrophobicity <h> Toggling transparency and changing the color of a selected surface <p> The visibility and color surfaces for selected chains can be adjusted using the Visibility and Colors tools from the toolbar after surfaces have been generated for all chains @ @ @ @ @ @ @ @ @ @ of a specific surface <p> Choose Visiblity <p> Choose Surfaces <p> Choose a Chain <p> To change the color of a specific surface <p> Select the Color tool <p> Choose Surfaces <p> Choose a Chain <h> Coloring by chain and entity <p> Chains and entities are colored by color schemes , which are carefully chosen combinations of colors or shades of color . <p> There are three types of color schemes available : sequential , diverging , and qualitatitive . The default color palette ( 5 colors ) will be adjusted by the number of chains or number of entities to render the surface . In case there is only a single chain or entity , try the Single color option instead . All colors of the sequential color scheme are colorblind safe , however , not all colors schemes from the diverging and qualitative colors are colorblind safe . A description for each color scheme is available as a mouse-over . These color schemes have been adopted from ColorBrewer developed by C. Brewer , The Pennsylvania State University . <h> Surfaces for large biological assemblies <p> The surface generation @ @ @ @ @ @ @ @ @ @ as virus capids . <h> Tips for manipulating surfaces <p> When displaying a large assembly such as a virus capsid , turn visibility of the ribbons off . This will significantly decrease memory requirements and will increase the responsiveness of the application . Parts of ribbons occasionally protrude through the molecular surface . Again , turning off ribbons will solve this problem . 
@@97506083 @1706083/ <h> Visualization of Electron Density Maps around Ligands <p> Sigma-weighted 2mFo-dFc electron density " mini-maps " for ligands are available from the JSmol 3D View . This option is available for ligands with more than one atom ( ions excluded ) in PDB entries with structure factor data . Different sigma values can be selected . <p> Electron density mini-map for Retinoic Acid in PDB I 'd 1CBS at sigma level 1 . In this example , the ligand fits well into the density . <p> Electron density mini-map for Thymidine-5 ' -Phosphate in PDB I 'd 3HW4 at sigma level 2 . Electron density is only available for part of the ligand . <p> To access this feature , select the Electron Density ( JSmol ) button from the " Small Molecules &gt; Ligands " table on a Structure Summary page . <h> Wild Type Search <p> A new Advanced Search option for Wild Type Protein selects protein sequences that do not contain mutations in comparison with the reference UniProt sequence . <p> Some PDB entries include expression tags that were was added during the experiment . Select " No @ @ @ @ @ @ @ @ @ @ filter out sequences with expression tags . <p> PDB entries may also contain only a portion of the referenced UniProt sequence . The " Percent coverage of UniProt sequence " option defines how much of a UniProt sequence needs to be contained in the PDB entry . <h> Redesigned Search Results Page <p> The layout of Search Results has been extensively redesigned to simplify browsing and improve the usability on both desktop and mobile devices . The left column provides options to refine the search results , while the right column displays the structures matching the search . <h> New Page Organization <p> Structures : List of search results <p> Unreleased Structures : List of structures that are on hold for release ( if present ) 
@@97506085 @1706085/ <h> Born : Clifton Forge , Virginia , 1899 <h> Died : New Orleans , November 9 , 1964 . <h> Lifelong Struggle of a Zoologist <p> Roger Arliner Young was the first African-American woman to receive a doctorate in zoology , after years of juggling research and teaching with the burden of caring for her invalid mother . Her story is one of grit and perseverance . <p> Roger Arliner Young grew up in Burgettstown , Pennsylvania . In 1916 , she entered Howard University . In 1921 , she took her first science course , under Ernest Everett Just , a prominent black biologist and head of the zoology department at Howard . Although her grades were poor , Just saw some promise and started mentoring Young . She graduated with a bachelor 's degree in 1923 . <p> Her relationship with Just improved her skills , and he continued working with her . According to his biographer , Just probably chose a woman protTgT because he thought men more likely to pursue lucrative careers in medicine than to remain in academe. * Just helped Young find @ @ @ @ @ @ @ @ @ @ entered the University of Chicago part-time . Her grades improved dramatically . She was asked to join Sigma Xi , an unusual honor for a master 's student . She also began publishing her research . Her first article , " On the Excretory Apparatus in Paramecium , " appeared in Science in September 1924 . She obtained her master 's degree in 1926 . <p> Just invited Young to work with him during the summers at the Marine Biological Laboratory , Woods Hole , Massachusetts , starting in 1927 . Young assisted him with research on the fertilization process in marine organisms . She also worked on the processes of hydration and dehydration in living cells . Her expertise grew , and Just called her a " real genius in zoology . " <p> Early in 1929 , Young stood in for Just as head of the Howard zoology department while Just worked on a grant project in Europe . It was the first of many trips to Europe for Just and the first of many stand-in appointments for Young . In the fall of that year , Young @ @ @ @ @ @ @ @ @ @ direction of Frank Lillie , the embryologist who had been Just 's mentor at Woods Hole . But she failed her qualifying exams in January 1930 . <p> She had given little indication of stress , but the failure to qualify was devastating . She was broke and still had to care for her mother . She left and told no one her whereabouts . Lillie , deeply concerned , wrote the president of Howard about her mental condition . She eventually returned to Howard to teach and continued working at Woods Hole in the summers , but her relationship with Just cooled considerably . <p> Just started easing her out of her position in 1933 . There had been rumors about romance between Just and Young . Various accusations were exchanged . They had a confrontation in 1935 , and in 1936 she was fired , ostensibly for missing classes and mistreating lab equipment . <p> She took her firing as an opportunity . In June 1937 , she went to the University of Pennsylvania to begin a doctorate under L. V. Heilbrunn , who had befriended her at @ @ @ @ @ @ @ @ @ @ continue . She earned her Ph.D . in 1940 . <p> She took an assistant professorship at the North Carolina College for Negroes in Raleigh . Unfortunately , her mental health failed again . She worked short contracts in Texas and at Jackson State College in Mississippi . While in Mississippi in the late 1950s , she was hospitalized at the State Mental Asylum . She was discharged in 1962 and she went to Southern University in New Orleans . She died , poor and alone , on November 9 , 1964. 
@@97506086 @1706086/ <h> Born : Vienna , Austria , November 7 , 1878 <h> Died : Cambridge , England , October 27 , 1968 <h> A Battle for Ultimate Truth <p> In 1945 , the Royal Swedish Academy of Sciences awarded the Nobel Prize in Chemistry to Otto Hahn for the discovery of nuclear fission , overlooking the physicist Lise Meitner , who collaborated with him in the discovery and gave the first theoretical explanation of the fission process . <p> While Meitner was celebrated after World War II as " the mother of the atomic bomb , " she had no role in it , and her true scientific contribution became , if anything , more obscure in subsequent years . A new biography by Ruth Lewin Sime* tells Meitner 's often paradoxical story and sets forth the daily sequence of events that constituted the discovery of fission and , subsequently , the " forgetting " of the role of one discoverer . <p> Lise Meitner was the third of eight children of a Viennese Jewish family . In 1908 , two of Lise 's sisters became Catholics and she @ @ @ @ @ @ @ @ @ @ counted for nothing after Hitler came to power . Owing to Austrian restrictions on female education , Lise Meitner only entered the University of Vienna in 1901 . With Ludwig Boltzmann as her teacher , she learned quickly that physics was her calling . Years later , Meitner 's nephew , Otto Robert Frisch , wrote that " Boltzmann gave her the vision of physics as a battle for ultimate truth , a vision she never lost . " * <p> Doctorate in hand , she went to Berlin in 1907 to study with Max Planck . She began to work with a chemist , Otto Hahn , she doing the physics and he the chemistry of radioactive substances . The collaboration continued for 30 years , each heading a section in Berlin 's Kaiser Wilhelm Institute for Chemistry . Together and independently they achieved important results in the new field of nuclear physics , competing with IrFne Curie , FrTdTric Joliot , and other foreign groups . <p> In 1934 , Enrico Fermi produced radioactive isotopes by neutron bombardment , coming to a puzzle only with uranium . There @ @ @ @ @ @ @ @ @ @ ? Meitner drew Hahn and also Fritz Strassmann into a new collaboration to probe the possibilities . By 1938 , the puzzle had only grown . <p> After the Anschluss ( German annexation of Austria in March 1938 ) , Lise Meitner had to emigrate . In the summer of 1938 , she went to Manne Siegbahn 's institute in Stockholm . As Sime writes , " Neither asked to join Siegbahn 's group nor given the resources to form her own , she had laboratory space but no collaborators , equipment , or technical support , not even her own set of keys ... " ? She corresponded with Hahn as he and Strassmann tried to identify their " transuranes . " <p> On November 13 , 1938 , Hahn met secretly with Meitner in Copenhagen . At her suggestion , Hahn and Strassmann performed further tests on a uranium product they thought was radium . When they found that it was in fact barium , they published their results in Naturwissenschaften ( January 6 , 1939 ) . Simultaneously , Meitner and Frisch explained ( and named ) @ @ @ @ @ @ @ @ @ @ model of the nucleus ; their paper appeared in Nature ( February 11 , 1939 ) . The proof of fission required Meitner 's and Frisch 's physical insight as much as the chemical findings of Hahn and Strassmann . <p> But the separation of the former collaborators and Lise 's scientific and actual exile led to the Nobel committee 's failure to understand her part in the work . Later Hahn rationalized her exclusion and others buried her role ever deeper . The Nobel " mistake , " never acknowledged , was partly rectified in 1966 , when Hahn , Meitner , and Strassmann were awarded the U.S. Fermi Prize . 
@@97506087 @1706087/ <p> Almost all features of the RCSB PDB web site require a modern web browser with JavaScript and cookies enabled . To browse the database by exploring a hierarchy ( for example , the taxonomy tree or the SCOP tree ) , pop-ups must not be blocked . <p> A . gz file is a compressed file similar to a . zip . To open a . gz file , simply double click it to open it in your default archiving utility . If you do not have one , free programs like 7-Zip are available which will uncompress the file for you . <p> The PDB archive is updated each week on or about Wednesday 00:00 UTC ( Coordinated Universal Time ) with new entries , modified entries , and updated status information . <p> Updates are prepared on the previous Friday . Citation updates and release requests should be sent to deposit@deposit.rcsb.org by noon ET on the preceding Thursday to be included in an update ; changes made after an update has been packaged will appear with the following update . <p> The files in the FTP @ @ @ @ @ @ @ @ @ @ . <p> From the RCSB PDB site , the most recent release is timestamped and linked on every page from the top right header . <p> Users can maintain their own local copy of all PDB files using rsync . Example scripts are available in the section on " Automated Download of Data from the RCSB PDB FTP Archive " on the FTP Services page . <p> Additional information on obtaining and maintaining copies of the entire PDB archive or certain portions of it is available from this README file in the FTP archive . <p> PDB-101 is a view of the RCSB PDB that places educational materials front and center . It packages together the resources of interest to teachers , students , and the general public to promote exploration in the world of proteins and nucleic acids . <p> Clicking on the blackboard PDB-101 logo or its related widget in the left-hand menu takes the user to PDB-101 . This view offers easy navigation : select any Molecule of the Month article from the top bar menu or mouse over the PDB-101 pulldown to jump to other sections @ @ @ @ @ @ @ @ @ @ top left at any time to access RCSB PDB deposition and query services from the main website . <p> The RCSB PDB combines the primary data from PDB archival files with data from external resources to enhance the query and display functionality on the RCSB PDB website . A listing of such external data is provided in the table at LONG ... 
@@97506089 @1706089/ <h> Protein Symmetry View <h> Introduction <h> Protein Symmetry <p> Protein symmetry refers to point group or helical symmetry of identical subunits ( &gt;= 95% sequence identity over 90% of the length of two proteins ) . While a single protein chain with L-amino acids can not be symmetric ( point group C1 ) , protein complexes with quaternary structure can have rotational and helical symmetry . <p> Complexes are considered symmetric if identical subunits superpose with their symmetry related copies within &lt;= 7 + Ca RMSD . Protein subunits are considered identical if their pairwise sequence identity is &gt;= 95% over 90% of the length of both sequences , to account for minor sequence variations such as point mutations and truncated or disordered N- and C-terminal segments . Protein chains with less than 20 residues are excluded , unless at least half of the chains are shorter than 20 residues . Nucleic acids and carbohydrate chains , as well as ligands are excluded . Split entries ( entries divided between multiple coordinate files due to the limitations of the PDB file format ) are currently excluded from the @ @ @ @ @ @ @ @ @ @ <p> Pseudosymmetry refers to symmetry of homologous protein subunits . Protein complexes with pseudostoichiometry may have a higher structural symmetry than the symmetry calculated based on sequence identity . If we consider hemoglobin again , at a 95% sequence identity threshold the alpha and beta subunits are considered different , which correspond to an A2B2 stoichiometry and a C2 point group . At the structural similarity level , all four chains are considered homologous ( 45% sequence identity ) with an A4 pseudostoichiometry and D2 pseudosymmetry . <p> Hemoglobin with 4 homologous subunits ( Stoichiometry A4 ) has D2 pseudosymmetry . In addition to the C2 symmetry ( red axis ) , there are two perpendicular C2 axes ( blue ) . <h> Global Symmetry <p> Global symmetry refers to the symmetry of the entire complex . Protein complexes may be symmetric , pseudosymmetric , or asymmetric . <p> Examples of global protein symmetry . <h> Local Symmetry <p> Asymmetric protein complexes may have local symmetry . Similar to global symmetry , we distinguish local symmetry of identical subunits and local pseudosymmetry of homologous subunits . <p> Examples of local @ @ @ @ @ @ @ @ @ @ <p> Protein symmetry can be viewed in 3D using Jmol ( select the " 3D View " link or " 3D View " tab on an entry 's Structure Summary page ) . Protein symmetry is calculated for all entries containing at least one protein chain , including asymmetric units and all biological assemblies ( except for entries split among several PDB files due to their size ) . <p> To facilitate the exploration of symmetry , several options are available : <h> Default Orientation <p> Protein complexes are aligned along the highest-order symmetry axis , helix axis , or along the principal axes on inertia for asymmetric cases . Several default orientations of the structure can be toggled using the &lt; and &gt; buttons . The default orientations are canonical views : sides and back , and along unique symmetry axes . <h> Symmetry polyhedra and axes <p> A polyhedron and symmetry axes can be displayed to facilitate symmetry analysis of symmetry . A complex is enclosed in a polyhedron that matches its symmetry . All symmetry axes and their icons representing the fold ( ellipsis for 2-fold , @ @ @ @ @ @ @ @ @ @ for n-fold axis ) can be displayed . <p> For helical symmetry , a helix axis is displayed and for asymmetric cases , the 3 axes of inertia are displayed . Polyhedron and Axes can be toggled on/off using the check boxes in the right panel . <h> Color by Symmetry <p> Two examples of structures colored by symmetry are shown below . <h> Example 1 <p> For Cn symmetry ( see example on left ) , the color scheme start at the 12 o'clock position , and the color gradient ( light to dark ) increases in a clockwise direction . The polyhedron , a pentagonal prism is displayed in a color complementary to the symmetry color scheme . The principal rotation axis is rendered in red with a pentagon representing the 5-fold rotation . <h> Example 2 <p> In all cubic systems ( T , O , I ) , different layers of the subunit are colored along a gradient ( light to dark ) from the plus and minus z-axis towards the origin ( see example on left ) . The 4-fold axes are rendered in red @ @ @ @ @ @ @ @ @ @ axes in blue . Using the toggle option underneath the Jmol applet , 3 views along these 3 different axes are available . 
@@97506091 @1706091/ <p> Appearance of a tool in the OpenTopography Tool Registry does not imply endorsement , recommendation , or support , by the NSF OpenTopography Facility and is meant simply as a service to our users . OpenTopography does not guarantee the completeness or accessibility of specific content and links contributed by users . If you have been directly involved with the development of a registered tool and are not the original contributor of the tool to the registry , please email info@opentopography.org to supply updates or modifications to its entry . <p> Description : This is a GUI enabled version of the PDAL ground classification tool which supports both SMRF and PMF ground classification methods . It provides for automated ground return classification and supports both LAS and LAZ files . Code base is PDAL version 1.5 and has been compiled for 64-bit editions of Windows 7 or greater . All necessary exe 's and DLL 's are included in the installer . <p> Description : This is a GUI version of the MCC LIDAR ground classification tool . It has been compiled for 64-bit editions of Windows and @ @ @ @ @ @ @ @ @ @ successfully . This version includes all the necessary binaries and DLL files . The original MCC LIDAR homepage ( non-gui ) can be found here : https : **36;144;TOOLONG The installer is 880KB in size . Installed binaries are 2.29MB in size . <p> Description : planlauf/TERRAIN is an easy-to-use application for Windows 7/8/10 to visualize Digital Elevation Models . It uses state-of-the-art techniques from the gaming industry like ' Mesh Decimation ' , ' Normal Mapping ' and ' Level of Detail ' to reduce the size of the models significantly while preserving the visual details of the original model . A 30 day trial version without any restrictions is available . <p> Description : A collection of scripts developed by the Natural Environment Research Council ( NERC ) Airborne Research Facility Data Analysis Node ( NERC-ARF-DAN ; formerly ARSF ) for generating DEMs from point clouds . Through command line tools or Python functions the ARSF DEM Scripts provide a common interface to generate DSMs or DTMs from LAS files using GRASS , SPDLib , points2grid , FUSION or LAStools . When using SPDLib , FUSION or LAStools @ @ @ @ @ @ @ @ @ @ license required to produce a DTM using LAStools ) . Additional utilities are available for manipulating DEMs ( e.g. , patching with a courser resolution DEM to fill in gaps ) using GRASS . <p> Description : terrainShadingMask is a free collection of terrain masks for various locations . They are given in geometrical ( . obj files ) and textual ( . hor files ) forms . A terrain shading mask is a diagram which maps the silhouette of the surrounding terrain ( hills , valleys , mountains , tree tops ... ) around the chosen location . Building simulation softwares ( IES VE , Trnsys 3D , Design Builder ... ) can use the . obj geometry files , while **26;182;TOOLONG software ( PV*SOL , PVsyst ... ) can use . hor files . <p> Description : Landlab is a Python-based modeling environment that allows scientists and students to build numerical landscape models . Landlab was designed for disciplines that quantify earth surface dynamics such as geomorphology , hydrology , glaciology , and stratigraphy , but can also be used in related fields . <p> Description : The @ @ @ @ @ @ @ @ @ @ , FUSION and LDV ( LIDAR data viewer ) , and a collection of task-specific command line programs . The primary interface , provided by FUSION , consists of a graphical display window and a control window . The FUSION display presents all project data using a 2D display typical of geographic information systems . It supports a variety of data types and formats including shapefiles , images , digital terrain models , canopy surface models , and LIDAR return data . LDV provides the 3D visualization environment for the examination and measurement of spatially-explicit data subsets . Command line programs provide specific analysis and data processing capabilities designed to make FUSION suitable for processing large LIDAR acquisitions . <p> Command line utilities and processing programs , called the FUSION LIDAR Toolkit or FUSION-LTK , provide extensive processing capabilities including bare-earth point filtering , surface fitting , data conversion , and quality assessment for large LIDAR acquisitions . These programs are designed to run from a command prompt or using batch programs . <p> FUSION runs on all current versions of Windows and has been successfully used on LINUX systems @ @ @ @ @ @ @ @ @ @ . The command line tools require the use of batch files to be most effective . <p> Description : The lidar2dems project is a collection open-source ( FreeBSD license ) command line utilities for supporting the easy creation of Digital Elevation Models ( DEMs ) from LiDAR data . lidar2dems uses the PDAL library ( and associated dependencies ) for doing the actual point processing and gridding of point clouds into raster data . <p> Description : TopoToolbox is a suite of MATLAB functions for topographic analysis . The major aim of TopoToolbox is to offer helpful analytical GIS utilities in a non-GIS environment in order to support the simultaneous application of GIS-specific and other quantitative methods . Key methods include advanced methods of flow direction derivation , stream network extraction and manipulation , slope-area analysis , chiplots , and swath profiles . <p> Description : LiVoxGen is a C++ program that voxelizes LiDAR data . It outputs a flat file containing voxel ( volumetric pixel ) information based on the user 's parameters and may also output a flat file of metric information for the voxel column centroids . @ @ @ @ @ @ @ @ @ @ structure by dividing LiDAR data into voxels . <p> Description : Civil Maps allows users to upload their survey data , then specify the assets of interest and mapping specification . Upon upload , Civil Maps indexes all of the spatial information as defined in the mapping specification into a query-able format . The maps can then be dynamically generated on demand and exported to various tools such as AutoDesk Map3D from Civil Maps , which is useful for integrating into the customers workflow . The biggest pain point in the industry is the time to annotate 3D scans . Currently , processing huge 3D survey datasets is limited by the point and click annotation speed of the user and the limited resources of the users computer ( I/O , CPU , Network , Memory ) . <p> By circumventing these bottlenecks , Civil Maps is introducing a paradigm shift in the workflow of annotating 3D survey data . Advancements in parallel computing and deep learning allows Civil Maps to reduce 2 years of manual annotation work down to 2 days of processing using our cloud infrastructure <p> Description : @ @ @ @ @ @ @ @ @ @ ) is a professional software that is highly automatic , very fast , and accurate in generating bare earth models , digital surface models , digital height models , and extracting forest information from individual trees to regional scales . <p> Tiffs was featured in the cover of the 2007 February issue of Photogrammetric Engineering &amp; Remote Sensing ( PE&amp;RS ) , the official journal of photogrammetry and geospatial society in the U.S. The core algorithms in Tiffs have won a couple of prestigious awards including the Best Paper Award in the Remote Sensing Specialty Group of Association of American Geographer in 2005 . Most of the algorithms are published in the peer-reviewed journal PE&amp;RS . The filtering algorithm was validated with the benchmark dataset provided by ISPRS ( the International Society of Photogrammetry and Remote Sensing ) . The comparison with the other well-known algorithms showed that our algorithm achieved the best overall accuracy . The tree isolation method used a conquer-and-divided strategy and has the advantages of minimizing both omission ( missing trees ) and commission ( overcounting trees ) errors . The canopy structure estimation is based @ @ @ @ @ @ @ @ @ @ has solid scientific roots in plant allometry . <p> Description : The Geographic Calculator is THE Global Coordinate Transformation tool for any coordinate or geomatic challenge . A powerful windows solution with particular strength in survey , seismic and energy exploration , it is available in 32 or 64 bit versions . This highly accurate transformation software includes tools such as Canadian DLS ( Dominion Land Survey ) Land Grid tools , Seismic Survey Conversion tools , Area of Use tools for guiding users , and much more . The Geographic Calculator supports a wide range of file formats with support from the largest geodetic parameter database available anywhere . Display and reproject your DEMs into the right coordinate systems . <p> Description : The Global Mapper LiDAR Module is an optional enhancement to the software that provides numerous advanced LiDAR processing tools , including automatic point cloud classification , feature extraction , cross-sectional viewing and editing , dramatically faster surface generation , and much more . At a fraction of the cost of comparable applications , it is a must-have for anyone using or managing LiDAR data . <p> @ @ @ @ @ @ @ @ @ @ application that offers access to an unparalleled variety of spatial datasets and provides just the right level of functionality to satisfy both experienced GIS professionals and beginning users . Equally well suited as a standalone spatial data management tool and as an integral component of an enterprise-wide GIS , Global Mapper is a must-have for anyone who deals with maps or spatial data . Global Mapper is more than just a utility ; it has built in functionality for distance and area calculations , raster blending , feathering , spectral analysis , elevation querying , line of sight calculations , cut-and-fill volume calculations , as well as advanced capabilities like image rectification , contour generation from surface data , view shed analysis , watershed delineation , sea level rise modeling , terrain layer comparison , and triangulation and gridding of 3D point data.Users can now simulate fly-through recordings in high-definition with various sky models in the 3D viewer . This new Skybox tool enables users to choose from a handful of existing sky templates or custom designs . Support for 3D textures and meshes have also been added to give @ @ @ @ @ @ @ @ @ @ host of new 3D formats . Live GPS data can also now be viewed and tracked in the 3D Viewer . Version 16 features dramatically faster processing speeds for analyzing large amounts of data . <p> Description : Potree is a free and open source WebGL based viewer for large point clouds . ( 20 billion and more ) Some of its features include distance &amp; area measurements , height profiles , clip volumes , various point rendering qualities ( square , circle , interpolation , splats ) and different types of materials . <p> Description : PointCloudViz is a free 3D LiDAR display and processing tool supporting input LAS , LAZ and ASCII formats . It provides very interactive visualization and attribute symbology without data size limits , as well as single or multiple orthoimage draping and grid DEM export . LiDAR data can be imported from multiple files , and also multiple processed datasets can be displayed simultaneously . PointCloudViz FE is available for Windows , Mac and Linux operating systems . Version 2.0 includes linear measurement and information tools , Web Map Service image draping and connection @ @ @ @ @ @ @ @ @ @ The fleurdelas class is an IDL/ENVI implementation of a LAS file reader/writer . The class object can read , write and manipulate points and/or full waveforms contained in the binary LAS file format . It can handle any LAS 1.0 , 1.1 , 1.2 , 1.3 and 1.4 with point format from 0 to 10 . This class object therefore is very well suited to access the point structure information or the waveform information for further process . It can also filter the point cloud and generate new LAS file . However , if you need to manipulate ; the Variable Length Records or the Coordinates System we will recommend you to use LAStools as it is more adapted for this purpose . <p> Description : The TerEx Tool automates the process of mapping terraces and floodplains from high resolution topography data ( works best on &lt;3m grid resolution ) . The tool maps terraces and floodplains from user-defined parameters including , a local-relief threshold selected by a variable-size moving window , minimum area threshold , and maximum distance from the channel to identify and map discrete terrace and floodplain @ @ @ @ @ @ @ @ @ @ , absolute elevation , and height relative to the local river channel for each terrace polygon . TerEx can be run in a Python environment or as a GUI plugin for ArcMap . See Stout and Belmont , 2014 and the users manual for explanations of tool functionality and several test cases that provide guidance on parameter values for a wide range of landscapes . <p> Description : The CHaMP Topo Processing Toolbar exists to take raw data from CHaMP ground-based topographic surveys and run these through steps of 1 ) survey data evaluation , 2 ) generating topographic surfaces , 3 ) DEM derived products and metrics ( including , detrending , derivation of cross sections , profiles , thalwegs , bankfull , etc. ) , 4 ) QA/QC . Although the overall workflow of the tool is focused on CHaMP topographic data , many of the individual commands and steps will be more generically useful to some . <p> Description : The GCD software was developed primarily for morphological sediment budgeting in rivers . The volumetric change in storage is calculated from the difference in surface elevations from @ @ @ @ @ @ @ @ @ @ surveys . As each DEM has an uncertain surface representation ( which might vary in space and time ) , our ability to detect changes between surveys is highly dependent on surface representation uncertainties inherent in the individual DEMs . The fundamental problem is separating out the changes between the surveys that are due to geomorphic change as opposed to noise in the survey data . GCD provides a suite of tools for quantifying those uncertainties independently in each DEM and propagating them through to the DEM of difference . The program also provides ways for segregating the best estimates of change spatially using different types of masks . The overall suite of tools is more generically applicable to many different spatial raster-based change detection problems . <p> Description : The Ecogemorphology &amp; Topographic Analysis Lab ( ET-AL ) and the RSGIS Laboratory developed this ArcGIS Add-in for the transformation and projection of CHaMP field survey data ( Bouwes et al . 2011 ) into real world coordinates given hand-held GPS coordinates from just three benchmarks . The CTT will work with any unprojected data in a file geodatabase , @ @ @ @ @ @ @ @ @ @ based on a simple shift and rotation . Although other transformation tools exist in ArcGIS , there is not a simple sequence of geoprocessing tools , which allow you to transform vector data in a manner that preserves the high relative accuracy of a total station survey . See Wheaton et al . ( 2012 ) and the ReadMe page for more background . <p> Description : LIDAR Analyst software is the premier feature extraction solution for airborne LIDAR data , allowing geospatial analysts to unlock the value of LIDAR by automatically extracting 3D objects such as bare earth , trees or buildings . Now the software has been upgraded with a major enhancement : an integrated 3D Viewer that delivers mission-critical high-resolution 3D exploitation and further establishes LIDAR Analyst as the Gold Standard . Enjoy the performance of viewing and manipulating LIDAR data containing over a billion points . LIDAR Analyst has eight years of proven heritage as one of the first commercially released LIDAR applications . The power , speed , accuracy and broad selection of tools enable users to get real work accomplished . That 's why LIDAR @ @ @ @ @ @ @ @ @ @ forces , federal agencies , local governments , academic universities and GIS services providers around the globe . Since LIDAR Analyst is an extension for Esri ArcGIS , the products familiar environment makes it easy to install and learn . Use the 3D Viewer to manipulate , filter and edit LIDAR point clouds displayed in stunning color . Rapidly inventory all the buildings across a city or area of interest by using LIDAR Analysts advanced algorithms to detect , recognize , and extract complex 3D building shapes . Count the trees in a forest or scattered throughout a neighborhood . Strip away the buildings and shrubbery to reveal the true ground level for detailed terrain . <p> Description : RiverTools is a user-friendly GIS application that contains a wide variety of tools specifically designed for terrain and watershed analysis and visualization . Version 4.0 supports the latest versions of Windows , Mac OS X and Linux . <p> Description : LP360 for ArcGIS is an extension to ArcMap that allows visualizing and processing of very large point clouds ( LIDAR and dense image matching ) in a familiar GIS desktop @ @ @ @ @ @ @ @ @ @ provides tools from rapid visualization and derived product generation through advanced features such as automatic ground classification and building footprint extraction . <p> LP360 adds to the native viewing capabilities of ArcGIS 10.1 by providing the capability to work with very large projects such as county-wide aerial LIDAR data sets . From Quality Check to advanced feature extractions , LP360 is the tool of choice for LIDAR professionals throughout the world . <p> Description : ArcGIS ( version 10.1 ) geoprocessing script tools for removing pits ( or sinks ) from Digital Elevation Models using a combination of cut and fill . This alternative to the standard Fill tool provides more realistic flow paths with less required manual adjustment . Ideal for high-resolution datasets such as LiDAR . <p> The tool also allows users to mark specific depressions to be left unmodified by setting the lowest cell to have a value of No Data . This feature can be used to establish reservoirs as well as known drainage features such as storm sewer inlets . <p> Also contains a C++ executable that can be run via command line inputs independent @ @ @ @ @ @ @ @ @ @ . <p> Description : ENVI LiDAR is an interactive geospatial software environment that allows you to create realistic 3D visualizations and easily extract important features ( trees , powerlines , buildings ) from LiDAR point cloud data . Elevation information contained within LiDAR can be included in your geospatial analysis projects such as viewshed and line-of-sight studies , forest inventories , right-of-way analyses , and urban planning applications . With ENVI LiDAR , you have the software tool to quickly prepare LiDAR data , accurately extract 3D features , fine-tune results , and export your results to your existing tools , such as ENVI or ArcMap , for further analysis or inclusion in your geospatial products . <p> Description : SPDLib is a set of open source software tools for processing laser scanning data ( i.e. , LiDAR ) , including both discrete return and waveform data captured from airborne and terrestrial platforms . The aim of the software is provide you , the user , with the tools you need to process these data . The software has grown from research carried out at Aberystwyth University and Queensland University @ @ @ @ @ @ @ @ @ @ has continued to grow . One of the key features which differentiates this software from other LiDAR software is the ability to process and store full waveform datasets alongside traditional discrete return data <p> Description : FME empowers users at leading organizations worldwide to transform spatial data so that they can use and share it wherever , whenever , and however its needed . FMEs unmatched capabilities and support for 250+ spatial and non-spatial formats enable you to quickly overcome any challenge related to the use and sharing of spatial data , so you can focus your energies on your objectives , instead . <p> Description : CloudCompare is a 3D point cloud and triangular mesh processing software . It has been originally designed to perform fast cloud/cloud or cloud/mesh comparison on huge datasets such as those acquired by laser scanners ) . Afterwards , it has been extended to a more generic 3D data editing and processing software . <p> Description : Makai Voyager is an advanced 3D/4D visualization platform that allows you to fly anwhere in the world to view geospatial data , including : terrain imagery , @ @ @ @ @ @ @ @ @ @ more . <p> PCL is released under the terms of the BSD license and is open source software . It is free for commercial and research use . The project is financially supported by multiple companies , including : Willow Garage , NVidia , Google , and Toyota <p> Description : Points2Grid is a robust and scalable tool for gridding LIDAR point cloud data to generate Digital Elevation Models ( DEMs ) . Points2Grid uses a local gridding method to compute grid cell elevation using a neighborhood defined around each cell based on a search radius provided by the user ( see image below ) . Points2Grid offers two processing modes - in-core and out-of-core - to allow it to handle generation of rasters larger than available memory . <p> Description : DielmoOpenLiDAR is an open source software with GNU GPL license based in gvSIG for the management of LiDAR data . It allows access , visualization and analysis of the original LiDAR data , toguether with the possibility of visualizing at the same time big volumes ( hundreds of GigaBytes ) of original LiDAR data ( irregular points in @ @ @ @ @ @ @ @ @ @ . In the near future , it will also generate basic final products such as ( DSM , DTM , intensity images , etc ) and new added value final products . <p> Developing open source software for the management of LiDAR data , we hope to make this technology more accessible to standard GIS users and scientific communities , having as final objective increase the use of LiDAR data . <p> For developing this application , we had the collaboration of the Regional Valencia Council for Infrastructures and Transportation - CIT and National Geographic Institute of Spain - IGN <p> Description : SAGA GIS is a FOSS which is , besides common GIS tasks , specialized on digital terrain analysis . The software provides a lot of analysis tools which you will not find in any other software package , e.g. for morphometric or hydrologic analysis . SAGA supports various raster and vector ( including LAS and SAGA point clouds ) formats , DEM generation , analysis and visualization ( e.g. 3D point cloud viewer , bare earth extraction ) . One of SAGA 's main objectives is to @ @ @ @ @ @ @ @ @ @ algorithms ( C++ ) but it is also used in commercial environments . SAGA provides a GUI and can be scripted in various ways ( e.g. batch/bash , python ) . SAGA runs on Windows , Linux and FreeBSD , both 32 and 64bit . The software is licensed under the GPL ( GUI , most of the modules ) and the LGPL ( API ) . <p> Description : Vrmesh Survey is an intelligent solution for automatic point cloud classification and bare-earth extraction . It automatically and accurately classifies LIDAR point clouds into ground , vegetation , building , and others . More than 90% identification jobs will be done in a one-click process . Manual adjustments can be easily performed in a cross section view . It also provides comprehensive functionalities to generate accurate triangle meshes . <p> Description : The ( RBT ) is available for free and is under active development . Tools exist for cutting cross sections and longitudinal profiles into high resolution DEMs to extract hydrologic parameters such as wetted area , bankfull width , hydraulic radius , gradient and sinuosity . It is @ @ @ @ @ @ @ @ @ @ and then add them to a map . Using an automated detrending algorithm we are able to remove the overall valley slope . Tools are being created that use the detrended raster to investigate flooding outside a main channel at any prescribed discharge or flow stage . <p> Description : BCAL LiDAR Tools are open-source tools developed by Idaho State University , Boise Center Aerospace Laboratory ( BCAL ) . These tools can be used for processing , analyzing and visualizing LiDAR data . They are written in IDL programming language and is intended to be used as add-on in the ENVI remote sensing software package . <p> Description : GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation . As a library , it presents a single abstract data model to the calling application for all supported formats . It also comes with a variety of useful commandline utilities for data translation and processing . <p> GDAL is a powerful tool for converting digital elevation model ( DEM ) formats , performing @ @ @ @ @ @ @ @ @ @ processing functionality such as generation of hillshades and slope maps . <p> Description : A Linux ( Korn ) shell script that takes an unreferenced 3-column xyz dataset ( e.g. , a TLS point cloud ) and places it in a Cartesian global coordinate system ( e.g. , UTM ) given at least one known global point ( e.g. from a hand-held GPS unit ) . <p> Description : A MATLAB script that takes an unreferenced 3-column xyz dataset ( e.g. , a TLS point cloud ) and places it in a Cartesian global coordinate system ( e.g. , UTM ) given at least one known global point ( e.g. from a hand-held GPS unit ) . <p> Description : GRASS is free Geographic Information System ( GIS ) software used for geospatial data management and analysis , image processing , graphics/maps production , spatial modeling , and visualization . GRASS is currently used in academic and commercial settings around the world , as well as by many governmental agencies and environmental consulting companies . GRASS is an official project of the Open Source Geospatial Foundation . <p> GRASS has @ @ @ @ @ @ @ @ @ @ processing and analysis . lidar specific elements of GRASS are discussed here : http : **28;210;TOOLONG <p> Description : libLAS is a C/C++ library for reading and writing the very common LAS LiDAR format . The ASPRS LAS format is a sequential binary format used to store data from LiDAR sensors and by LiDAR processing software for data interchange and archival . First , libLAS focuses almost completely on providing an easy-to-program-with library for software developers wishing to implement the LAS specification in their own software . Second , libLAS exists to provide a truly open source library . Third , libLAS exists to provide advanced functionality and concentrate almost solely on the specification not LiDAR data processing in general . libLAS a building block for developers to use to implement their own LiDAR data processing when working with ASPRS LAS data . <p> Description : The Points2Grid Utility is a simple tool for the generation of Digital Elevation Models ( DEMs ) from LiDAR point cloud data . The Points2Grid Utility is a Windows application that utilizes the same local binning algorithm deployed in the OpenTopography LiDAR system to @ @ @ @ @ @ @ @ @ @ point return data . <p> Description : LViz is a tool designed for 3D visualization of LiDAR point cloud and interpolated data , the tool offers import of LiDAR point cloud data ( delimited text file ) or interpolated surfaces ( in ascii or arc ascii grid formats ) . LViz also offers texture mapping and user control over display settings such as data and background color . LViz was originally written for visualization of outputs from the GEON LiDAR Workflow system and is designed to run on Windows platforms . Although originally conceived for the 3D visualization of LiDAR data , LViz can be used to view any 3D point cloud data such as seismicity or gravity . 
@@97506092 @1706092/ <h> Tutorials <h> TSCC Quick Start Guide <h> Technical Summary <p> The Triton Shared Computing Cluster ( TSCC ) provides central colocation and systems administration for your purchased cluster nodes ( " condo cluster " ) , as well as providing a " hotel " service for those with temporary or bursty high-performance computing ( HPC ) needs . TSCC provides different kinds of compute nodes in the cluster , including General Computing Nodes and Graphics Processing Unit ( GPU ) Nodes . <p> TSCC is now open to new purchases with refreshed technology and special vendor pricing . <p> More information about Secure Shell may be found in the New User guide . SDSC security policy may be found at the SDSC Security site . Download the TSCC Quick Reference Guide PDF . <h> Important Guidelines for Running Jobs <p> Please do not write job output to your home directory ( /home/$USER ) . NFS filesystems have a single server which handles all the metadata and storage requirements . This means that if a job writes from multiple compute nodes and cores , the load is focused on @ @ @ @ @ @ @ @ @ @ /oasis/tscc/scratch ) is optimized for efficient handling of large files , however it does n't work nearly as well when writing many small files . We recommend using this filesystem only if your metadata load is modest i.e. , you have O(10)-O(200) files open simultaneously . <p> Use local scratch ( **33;240;TOOLONG ) if your job writes a lot of files from each task . The local scratch filesystem is purged at the end of each job , so you will need to copy out files that you want to retain after the job completes . <h> Running Jobs with TORQUE <p> TSCC uses the TORQUE Resource Manager ( also known by its historical name Portable Batch System , or PBS ) with the Maui Cluster Scheduler to define and manage job queues . TORQUE allows the user to submit one or more jobs for execution , using parameters specified in a job script . <h> Job Queue Summary Descriptions <p> The intended uses for the submit queues are as follows : <p> hotel The hotel queue supports all non-contributor users of TSCC . Jobs submitted to this queue will @ @ @ @ @ @ @ @ @ @ cluster . As such , the total number of cores running all hotel jobs is limited to the total number of nodes in that cluster ( currently 640 ) . <p> home <p> This is a routing queue intended for all submissions to group-specific clusters ; if you intend for your job to run only within the nodes you have contributed , submit to this queue . Some users may belong to more than one home group ; in this case , a default will be in effect , and using a non-default group will be specified in the job submission . <p> condo The condo queue is exclusive to contributors , but allows jobs to run on nodes in addition to those purchased . This means that more cores can be in use than were contributed by the project , but it also limits the run time to eight hours to allow the node owners to have access per their contracted agreement . <p> glean The glean queue will allow jobs to run free of charge on any idle condo nodes . These jobs will be terminated whenever the @ @ @ @ @ @ @ @ @ @ nodes . This queue is exclusive to condo participants . <h> Job Queue Characteristics <h> Default Walltimes Changed <p> The default walltime for all queues is now one hour . Max cores has been updated on some queues as well . Max walltimes are still in force per the below list . <p> For the hotel , condo , pdafm and home queues , jobs charges are based on the number of cores allocated . Memory is allocated in proportion to the number of cores on the node . <p> Memory per Allocated Core by Queue Type <p> Queue <p> # Cores <p> Memory ( GB ) <p> GB Memory per Core <p> hotel <p> 16 <p> 64 <p> 4 <p> condo <p> 16 <p> 64 or 128 <p> 4 or 8 <p> pdafm <p> 32 <p> 512 <p> 16 <p> home <p> 16 <p> 64 or 128 <p> 4 or 8 <h> Queue Usage Policies and Restrictions <p> All nodes in the system are shared , and up to 16 jobs can run on each . <p> Anyone can submit to the hotel queue . The total number of @ @ @ @ @ @ @ @ @ @ is capped at 640 . Jobs submitted to this queue run only on machines with 64GB memory . ( Some nodes have 128GB ; hotel nodes should n't run there ) . <p> The home queue is available to groups that have contributed nodes to the TSCC . Usage limits for those queues are equal to the number of cores contributed . Similarly , the condo queue is also restricted to contributors , so that sharing access to nodes in this queue becomes a benefit of contributing nodes to the cluster . <p> The glean queue is available only to node contributors of the condo cluster . Jobs are not charged but must run on idle cores and will be canceled immediately when the core is needed for a regular condo job . <p> Only members of Unix groups defined for node contributors are allowed to submit to the home queue . The home queue will route jobs to specific queues on the submitter 's group membership , so the specific queue name is not used in the job submission . The total number of processors in use by all @ @ @ @ @ @ @ @ @ @ to the number of cores they contributed to the condo cluster . <p> Only members of Unix home groups are allowed to submit to condo ( i.e. , no hotel users ) . There is no total processor limit for the condo queue . If the system is sufficiently busy that all available processors are in use and both the hotel and condo queues have jobs waiting , the hotel jobs will run first as long as the total processors used by hotel jobs does n't exceed the 640-processor limit . Condo jobs do not run on hotel nodes . <h> Note ! <p> All TSCC nodes practically have slightly less than the nominal amount of memory available , due to system overhead . Jobs that attempt to use more than the specified proportion will be killed . <p> To submit a job for the PDAFM nodes , specify the pdafm queue . For example : <p> #PBS -q pdafm #PBS -l nodes=2:np=20 <p> To reduce email load on the mailservers , please specify an email address in your TORQUE script . For example : <p> Set of conditions under @ @ @ @ @ @ @ @ @ @ : ( a ) bort , ( b ) egin , ( e ) nd . <p> #PBS -A &lt;account&gt; <p> Specify account to be charged for running the job ; optional if user has only one account . If more than one account is available and this line is omitted , job will be charged to default account . <p> To ensure the correct account is charged , it is recommended that the -A option always be used . <p> cd **28;275;TOOLONG name&gt; <p> Change to user 's working directory in the Lustre filesystem . <p> mpirun -v -machinefile $PBSNODEFILE -np 20 &lt;. /mpi.out&gt; <p> Run as a parallel job , in verbose output mode , using 20 processors , on the nodes specified by the list contained in the file referenced by $PBSNODEFILE , and send the output to file mpi.out in current working directory . <h> GPU Queue Details <p> Because of the confusion caused by keeping the hotel general computing nodes and GPU nodes in a single poolthe GPU nodes have no IB connection , so multi-node jobs can fail unless you 're careful with the @ @ @ @ @ @ @ @ @ @ and gpu-condo . To run on a general computing GPU , use a command similar to : <p> # qsub -I -q gpu-hotel -l nodes=1:ppn=3 <p> This command will run your job on one of the three hotel GPU nodes and allocate a GPU to your job . Because the GPU nodes contain 12 cores and 4 GPUs each , one GPU is allocated to the job per every three cores requested . Allocated GPUs are referenced by the CUDAVISIBLEDEVICES environment variable . Applications using the CUDA libraries will discover GPU allocations through that variable . <p> Similarly , condo-based GPUs are accessible through the gpu-condo queue , which provides users who have contributed GPU nodes to the cluster with access to each other 's nodes . Like the general computing condo queue , jobs submitted to this queue have an 8-hour time limit . <p> Condo owners can glean cycles on condo GPU nodes via the general glean queue . To do so , just add : gpu to the node resource specification . For example : <h> Obtaining Support for TSCC Jobs <h> TSCC Software <h> Installed and @ @ @ @ @ @ @ @ @ @ PGI and Intel compilers are available , as are mvapich2 and openmpi . Over 50 additional software applications and libraries are installed on the system , and system administrators regularly work with researchers to extend this set as time/costs allow . <h> Applications Software <p> This list is subject to change . Specifics of installed location , version and other details may change as the packages are updated . Please contact tscc-support@ucsd.edu for details . <h> Requesting Additional Software <p> Users can install software in their home directories . If interest is shared with other users , requested installations can become part of the core software repository . Please submit new software requests to tscc-support@ucsd.edu . <h> System Information <h> Hardware Specifications <p> There are three kinds of compute nodes in the cluster : General Computing Nodes , GPU Nodes , and Petascale Data Analysis Facility ( PDAF ) Nodes . The current specifications for each type of node are as follows : <p> General Computing Nodes <p> Processors <p> Dual-socket , 8-core , 2.6GHz Intel Xeon E5-2670 ( Sandy Bridge ) <p> Memory <p> 64GB ( 4GB/core ) ( @ @ @ @ @ @ @ @ @ @ InfiniBand optional ) <p> Hard Drive <p> 500GB onboard ( second hard drive or SSD optional ) <p> Warranty <p> 3-years <p> GPU Nodes <p> Host Processors <p> Dual-socket , 6-core , 2.3GHz Intel Xeon E5-2630 ( Sandy Bridge ) <p> GPUs <p> 4 NVIDIA GeForce GTX 680 ( GTX Titan upgrade available ) <p> Memory <p> 32GB ( 64GB/128GB memory optional ) <p> Network <p> 10GbE ( QDR InfiniBand optional ) <p> Hard Drive <p> 500GB + 240GB SSD <p> Warranty <p> 3-years <p> PDAF ( shared resource ; pay-as-you-go only ) <p> Processors <p> 8-socket , 4-core AMD Shanghai Opteron <p> Memory <p> 512 GB <p> Network <p> 10 GbE <p> ( IDI will annually update the hardware choices for general computing and GPU condo purchasers , to stay abreast of technology/cost advances . ) <h> Network <p> TSCC nodes with the QDR InfiniBand ( IB ) option connect to 32port IB switches , allowing up to 512 cores to communicate at full bisection bandwidth for low latency parallel computing . <h> Storage <p> TSCC users will receive 100GB of backed-up home file storage , and shared access @ @ @ @ @ @ @ @ @ @ file system subject to individual user quota . ( There is a 90day purge policy on Data Oasis , and this storage is not backed up . ) <p> To check your own usage , use the command : <p> lfs quota -u $USER /oasis/tscc/scratch <p> Note <p> Additional persistent storage can be mounted from lab file servers over the campus network or purchased from SDSC. 
@@97506093 @1706093/ <p> Sherlock Launches Secure , Compliant Cloud Services in Amazon Web ServicesWith this new capability SDSC becomes one of the few academic institutions to offer secure , compliant , managed services through a Hybrid Cloud leveraging both a private Cloud ( @SDSC ) and a public Cloud ( @AWS ) platformLearn More <p> Novel Molecular Dynamics Captures Atomic-level Detail of CRISPR-Cas9 ActivityUsing a novel method capable of capturing the motion of gyrating proteins , researchers have identified the myriad structural changes that activate and drive the innovative gene-splicing technology that 's transforming the field of genetic engineeringLearn More <p> SDSCs Comet is a Key Resource in New Global Dark Matter ExperimentSDSC 's Comet supercomputer has emerged as a key resource in what is considered to be the most advanced dark matter research quest to-date , with promising results after only one month of operation with a new detectorLearn More <p> Shape and Size of DNA Lesions Caused by Toxic Agents Affects Repair of DNAEvery day our bodies come under a barrage of toxic agents that create damaging lesions in our DNA that can initiate cancer and other diseases . Fortunately @ @ @ @ @ @ @ @ @ @ such lesions.Learn More 
@@97506099 @1706099/ <h> Directions to SDSC <p> Exit Genesee , turn West off exit . ( From the south , turn left . From the north , turn right. ) * Note : intersection is currently under a long term construction project . Please allow extra time . <p> At the top of the hill , turn left at North Torrey Pines Road . <p> At the first light , turn left on Northpoint Drive into the UC San Diego campus . <p> Follow Northpoint Drive past the campus Information Booth . <p> Continue on Northpoint as it turns right and becomes Hopkins Drive . <p> Turn right on Voigt Drive ( South of RIMAC and SDSC ) . <p> Enter the Hopkins Parking Structure at the top of the hill ( Level 6 ) . <p> From San Diego International Airport : From the airport parking lot , take the Harbor Island , Downtown Exit . Follow Harbor Drive ( which leads to downtown San Diego ) to Laurel Street ( on the left ) . Follow Laurel to India Street to Interstate 5 ( I-5 ) North . Signs @ @ @ @ @ @ @ @ @ @ 
@@97506100 @1706100/ <h> Born : Lyme Regis , England , May 21 , 1799 <h> Died : Lyme Regis , England , March 9 , 1847 <h> Finder of Fossils <p> Mary Anning lived through a life of privation and hardship to become what one source called " the greatest fossilist the world ever knew . " * Anning is credited with finding the first specimen of Ichthyosaurus acknowledged by the Geological Society in London . She also discovered the first nearly complete example of the Plesiosaurus ; the first British Pterodactylus macronyx , a fossil flying reptile ; the Squaloraja fossil fish , a transitional link between sharks and rays ; and finally the Plesiosaurus macrocephalus . <p> Her history is incomplete and contradictory . Some accounts of her life have been fictionalized , and her childhood discoveries have been mythologized . She was a curiosity in her own time , bringing tourism to her home town of Lyme Regis . Only her personal qualities and her long experience brought her any recognition at all , since she was a woman , of a lower social class , and from @ @ @ @ @ @ @ @ @ @ , gentlemanly scholars , received the bulk of the credit for geological discoveries . <p> Anning learned to collect fossils from her father , Richard , a cabinet maker by trade and a fossil collector by avocation . But he died at 44 in 1810 , leaving his family destitute . They relied on charity to survive . <p> Fossil collecting was a dangerous business in the seaside town . Anning walked and waded under unstable cliffs at low tide , looking for specimens dislodged from the rocks . During her teenage years , the family built both a reputation and a business as fossil hunters . In 1817 they met Lieutenant-Colonel Thomas Birch , a well-to-do fossil collector who became a supporter of the family . He attributed major discoveries in the area to them , and he arranged to sell his personal collection of fossils for the family 's benefit . Most of Anning 's fossils were sold to institutions and private collectors , but museums tended to credit only people who donated the fossils to the institution . Therefore , it has been difficult for historians to @ @ @ @ @ @ @ @ @ @ known are a small Ichthyosaurus discovered in 1821 and the first Plesiosaurus , unearthed in 1823 . <p> Mary had some recognition for her intellectual mastery of the anatomy of her subjects , from Lady Harriet Silvester , who visited Anning in 1824 and recorded in her diary : <p> the extraordinary thing in this young woman is that she had made herself so thoroughly acquainted with the science that the moment she finds any bones she knows to what tribe they belong . . . . by reading and application she has arrived to that greater degree of knowledge as to be in the habit of writing and talking with professors and other clever men on the subject , and they all acknowledge that she understands more of the science than anyone else in this kingdom. ** <p> Visitors to Lyme increased as Anning won the respect of contemporary scientists . In the last decade of her life she received an annuity from the British Association for the Advancement of Science ( 1838 ) . The Geological Society of London collected a stipend for her and she was named @ @ @ @ @ @ @ @ @ @ , one year before her death from breast cancer . Her obituary was published in the Quarterly Journal of the Geological Society--an organization that would not admit women until 1904 . <p> * Annotation on an undated letter from Mary Anning to one of the Misses Philpot of Lyme , in the collection of the American Philosophical Society , Philadelphia , cited in Torrens , Hugh : " Mary Anning ( 1799-1847 of Lyme : ' the greatest fossilist the world ever knew , ' British Journal for the History of Science , 25 : 257-84 , 1995. 
@@97506101 @1706101/ <h> Born : Monmouth , Illinois , October 7 , 1858 <h> Died : San Diego , California , January 12 , 1947 <h> " First Woman Ichthyologist of Any Accomplishments " <p> In the dark , rocky caves beneath San Diego 's Point Loma Peninsula live schools of little , pink , blind fish , six or seven inches long . They were discovered and later described and classified by a young woman named Rosa Smith . The blind goby , Typhologobius californiensis ( now Othonops eos ) inaugurated her career . According to famed marine biologist Carl L. Hubbs , " Rosa Smith was indeed the first woman ichthyologist of any accomplishments. " * <p> Smith was the last of nine children . Her parents had come from California to Illinois to launch a newspaper , but they returned when their frail , tubercular youngest was advised to seek a warmer climate . Rosa finished her secondary schooling at Point Loma Seminary , taking a lively interest in the natural history of the region . She joined the San Diego Society of Natural History and began , @ @ @ @ @ @ @ @ @ @ identify local species of animals and plants . <p> In 1879 , the noted ichthyologist David Starr Jordan came to San Diego . One of Rosa Smith 's daughters wrote that Jordan met Rosa Smith while renting a horse and buggy from her father , but another daughter believed they met at the Society of Natural History . There , the story went , Jordan heard Smith read a paper on a new species of fish ( very likely the blind goby ) , was deeply impressed , and urged her to study with him at Indiana University . <p> Rosa spent the summer of 1880 on a natural history tour in Europe with Jordan and his students , then attended Indiana University for two years , but was called home owing to illness in her family and did not graduate . Before she left , Jordan introduced her to a young German student of his named Carl H. Eigenmann , who was in the process of obtaining a doctorate in ichthyology . <p> Back in San Diego , Rosa Smith undertook the formal description and publication of the various @ @ @ @ @ @ @ @ @ @ kept up an exchange of papers and correspondence with Carl Eigenmann . Before they married on August 20 , 1887 , she had published nearly 20 papers on her own . They collaborated first on a study of South American freshwater fishes in the collections at Harvard , and Rosa Eigenmann was the first woman allowed to attend graduate-level classes there . <p> In 1891 , Jordan became chancellor of Stanford University , and Carl Eigenmann was left to head the zoology department at Indiana University . He ultimately became department chair and , later , Dean of the Graduate School . The five Eigenmann children included a disabled daughter and a son who was eventually institutionalized , and the burden of child care fell heavily on Rosa Eigenmann . Nevertheless , she managed to collaborate with her husband on 15 more papers . Eigenmann and Eigenmann were first to describe some 150 species of fish . <p> When Carl Eigenmann had a stroke in 1927 , Rosa returned with him to San Diego , where he died on April 24 . She stayed in San Diego with her children @ @ @ @ @ @ @ @ @ @ career had been pursued in spite of all obstacles , and she once wrote , " in science as everywhere else in the domain of thought woman should be judged by the same standard as her brother . Her work must not simply be well done for a woman . " 
@@97506105 @1706105/ 1450 @qwx861450 <p> The San Diego Supercomputer Center ( SDSC ) at UC San Diego has been awarded a National Science Foundation ( NSF ) grant that will augment its campus computing cluster with targeted capabilities for bioinformatics analyses to support researchers across campus and their collaborators including the ability to conduct de-multiplexing , mapping , and variant calling of a single human genome in less than one hour . <p> The grant is part of the NSFs Campus Cyberinfrastructure ( CC* ) program , which invests in coordinated campus-level cyberinfrastructure ( CI ) components of data , networking , computing infrastructure , capabilities , and integrated services that lead to higher levels of performance , reliability , and predictability for science applications and distributed research projects . Learning and workforce development in CI is explicitly addressed in the program , and science-driven requirements are the primary motivation for any proposed activity . <p> " This new award illustrates SDSCs increasing role in providing high-performance campus cyberinfrastructure in addition to its ongoing national supercomputing role , " said SDSC Director Michael Norman . " Through the capabilities enabled by @ @ @ @ @ @ @ @ @ @ productivity that should be of benefit to many UC San Diego researchers using bioinformatics tools and techniques for life sciences research . " <p> " A key objective of this project is to leverage new technology to provide accelerated computing capacity so that researchers can conduct approximately 8,000 whole-genome analyses per year , plus the ability to conduct quick turnaround single-genome analyses in about one hour , " said Ron Hawkins , SDSCs Industry Relations director and the principal investigator for the project . " The latter capability could be particularly useful for precision medicine and emerging clinical applications of genomics . " <p> " The project will enable analysis and re-analysis of existing genome data in the context of the new genomes that will be sequenced over the coming years , " said Terry Gaasterland , a co-investigator in the project as well as a UC San Diego professor of computational biology and genomics , and director of the Scripps Genome Center . " This ability will bring new value to genome information and will accelerate how we tie genome variants to diagnosis and prediction of progression , onset @ @ @ @ @ @ @ @ @ @ Cluster Upgrades , BioBurst <p> The NSF award , valued at almost half a million dollars and slated to run through January 2018 , provides funding for new hardware for UC San Diegos Triton Shared Computing Cluster , or TSCC , a " condo computing " program established in 2013 that has seen strong growth over the last two years . Condo computing is a form of shared ownership model in which researchers use funds from grants or other sources to purchase and contribute compute " nodes " ( computer servers ) to the system . The result is a researcher-owned , shared computing resource of medium- to large-proportions and much larger than could typically be afforded by the typical principal investigator for dedicated use . The already large and growing life sciences research enterprise at UC San Diego is an increasing consumer of computing capacity on TSCC . <p> Under the NSF award , SDSC will implement a separately scheduled partition of TSCC with technology designed to address key areas of bioinformatics computing including genomics , transcriptomics , and immune receptor repertoire analysis . Called BioBurst , the system @ @ @ @ @ @ @ @ @ @ ( I/O ) accelerator appliance with 40 terabytes of non-volatile memory and software designed to improve network throughput by alleviating the small-block/small-file I/O problem characteristic of many bioinformatics codes ; <p> A field programmable gate array ( FPGA ) -based computational accelerator system that has been demonstrated to perform de-multiplexing , read mapping , and variant calling of complete human genomes in about 22 minutes ; <p> Integration with a large-scale , Lustre parallel file system which supports streaming I/O and has the capacity to stage large amounts of data associated with many bioinformatics studies ; and <p> Customization of the job scheduler to accommodate bioinformatics workflows , which can consist of hundreds to thousands of jobs submitted by a single user at one time . <p> " UC San Diego is grateful for this new support from the National Science Foundations CC* program to enhance the power of TSCC , " said Valerie Polichar , Director of Research IT Services at UC San Diego . " This award extends our overall research capabilities to innovatively enable our biomedical and life sciences researchers to push the boundaries of science through @ @ @ @ @ @ @ @ @ @ Director Shawn Strande and Robert Sinkovits , director of the Centers scientific computing applications , are co-PIs of the project in addition to Gaasterland . The NSF CC* award number is #ACI-1659104 . <p> Technology providers for the BioBurst system include Advanced HPC , Bird Rock Systems , DDN , and Edico Genome . <p> About SDSC <p> As an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community , including industry and academia . Cyberinfrastructure refers to an accessible , integrated network of computer-based resources and expertise , focused on , and health IT . SDSCs petascale Comet supercomputer continues to be a key resource within the National Science Foundations XSEDE ( Extreme Science and Engineering Discovery Environment ) program . 
@@97506107 @1706107/ <h> Where do trees get their water ? <p> Maintaining moisture is a daily challenge for plants that live on land . Unlike animals , plants do not have the capability to run to the nearest stream or lake for rehydration . Desert plants are specifically adapted to withstand harsh conditions to stay wet . For example , the Welwitschia mirabilis is a true survivor of the desert and has been a constant resident in the Namib Desert for millions of years ( Henschel and Seely , 2000 ) . This " ugly " plant has evolved to live on less than 6 inches ( 15 cm ) of rain and tolerate weeks at temperatures &gt; 100 f ( &gt; 35 C ) in a nearly dormant state . Welwitschia and other desert plants share a common desert strategy : shut down to prevent drying and wait for rain . And when it does rain , desert plants use their shallow root systems to absorb as much water as possible . But not all plants are adapted to drought conditions . Without a constant supply of water , many @ @ @ @ @ @ @ @ @ @ she 'd their leaves , especially during hot , dry weather . That brings us to the question of interest : Where do trees get their water ? <p> Image 1 . Depending on their environment , plants get water in different ways . <p> Compared to desert survivors , trees are seemingly less prepared to ward off the effects of a drought . Each of their leaves needs a constant supply of water for producing sugars via photosynthesis , to regulate their cellular chemistry , and to transport mineral nutrients and sugars with the rest of the tree . Trees and forests generally occur in temperate and tropical climates where precipitation exceeds 100 cm per year ( 40 inches per year ) , although forests can grow in drier climates . Trees obtain their water from their extensive root systems . In forests across the world , the majority of tree roots are within the first 20 inches ( 50 cm ) of the soil ( Jackson et al. , 1996 ) . The reason behind this phenomenon is simple : unless there is a nearby stream or groundwater is @ @ @ @ @ @ @ @ @ @ roots , precipitation from above is the easier to access . Thus , rain and melting snow is the major source of water used by trees . We know this is true beyond just reasoning , it is actually possible to determine where trees are getting their water by the molecular nature of the water in their leaves and vascular tissues . <p> Measuring stable isotopes in water has been a very effective technique for learning where water moves in the environment . The basis of water stable isotopes is that its components , hydrogen and oxygen , do not all weigh the same . Hydrogen is generally a lone proton , but it has an isotope , deuterium is a proton and neutron . Similarly for oxygen , instead of only having 8 protons and 8 neutrons , oxygen can have 9 or 10 neutrons . The reason these are considered stable isotopes rather than radioactive isotopes , is that they do not lose their extra neutrons with time . The combination of hydrogen and oxygen creates water with different weights , but they behave the same and plants @ @ @ @ @ @ @ @ @ @ this technique is that different waters can have different isotope compositions , which allows researchers to determine the source of water . <p> Dr. Katie Gaines , a recent PhD recipient from the Pennsylvania State University , studied the source of waters in northern hardwood tree species at the Shale Hills Critical Zone Observatory using stable isotope ratios of water . In her study , Dr. Gaines observed oak ( Quercus spp. ) , hickory ( Carya spp ) , and maple ( Acer spp. ) were primarily utilizing water held in the top 12 inches ( 30 cm ) of the soil ( Gaines et al. , 2015 ) . In addition , the isotopic signature of the water within the trees closely resembled that of summer precipitation . However , Dr. Gaines also found that oak and hickory were using a much deeper soil water and water tightly-bound to surfaces that require more effort to access . <p> Although trees generally use water near the soil surface , tree roots are able to grow deeper into soil and even weathered bedrock ( regolith ) in their search for @ @ @ @ @ @ @ @ @ @ study conducted in the Amazon forest of Brazil , tree and grass roots were abundantly found 26 feet ( 8 meters ) into the deeply weathered soil profile ( Nepstad et al. , 1994 ) . What is more surprising was the abundance of tree roots down to 60 feet ( 18 meters ) ! Of course , this depth of rooting varies with the physiology of the tree and the depth of the soil and regolith . <p> You can learn more about the movement of water into plants at the USGS website : <p> LONG ... <p> Have any questions swirling in your noodle about the rock , soil , water , fauna , or flora of the critical zone ? Send them our way at **25;305;TOOLONG . <h> COMMENT ON " Adventures in the Critical Zone " <p> All comments are moderated . If you want to comment without logging in , select either the " Start/Join the discussion " box or a " Reply " link , then " Name " , and finally , " I 'd rather post as a guest " checkbox . @ @ @ @ @ @ @ @ @ @ , findings , conclusions or recommendations presented in the above blog post are only those of the blog author and do not necessarily reflect the views of the U.S. CZO National Program or the National Science Foundation . For official information about NSF , visit www.nsf.gov. 
@@97506108 @1706108/ <h> Sequence Motif <p> Searches protein and nucleic acid sequences that match a sequence motif . A Sequence Motif can be an exact sequence or a sequence pattern expressed by regular expression syntax . Regular expressions are powerful notations for defining complex sequence patterns . Click on the sequence to run the example queries below . <h> Examples <p> The motif search supports wildcard queries by placing an ' X ' at the variable residue position . A query for SH3 domains using the consequence sequence -X-P-P-X-P ( where X is a variable residue and P is Proline ) can be expressed as : <p> Ranges of variable residues are specified by the n notation , where n is the number of variable residues . To query a motif with seven variables between residues W and G and twenty variable residues between G and L use the following notation : <p> Variable ranges are expressed by the n , m notation , where n is the minimum and m the maximum number of repetitions . For example the zinc finger motif that binds Zn in a DNA-binding domain can be expressed as : 
@@97506114 @1706114/ <p> An Introduction to Fuzzy Logic Controllers in the LabView Environment . <p> William Green <p> Argonne National Laboratory <p> Mathematics and Computer Science Division <p> I:Background <p> Fuzzy Logic is a variation on set theory where a variable can partially be an element of a set.Fuzzy Logic was first developed in the state we know of now by LoftiZadeh in the 1960s.Fuzzy Logic Controllers are intended to " think " like humans do , in this way they are helpful for problems that can not easily be set up mathematically , but can easily be expressed in words . <p> In traditional set theory , a variable is either an element of a set or it is n't , essentially a Boolean 1/0 classification.However , in Fuzzy Logic , a variable can partially exist in sets.For instance , in the following picture , x is wholly in the sets A , B , and C in the traditional set theory.However , in Fuzzy Logic , we can say x is in A to a degree of 0.7 , 0.4 in B and 0.5 in C. ( Or essentially an @ @ @ @ @ @ @ @ @ @ a degree of set membership.For instance , say we have created a feedback controller that is based on the error of control.An error value of +8mm can exist partially in the set " Over Target " , partially in the set " Close to target " , and partially in the set " Slightly Over Target . " <p> This should suffice to introduce the ideas behind Fuzzy Logic necessary to writing a LabView Fuzzy Logic Controller.For more information on Fuzzy Logic see : <p> First , one must determine the input variables and the desired output.The input variables are often referred to by " linguistic terms , " a way in which to describe the meaning of the variables.For instance , Error and Actual Position are two easily defined linguistic terms for use as input variables.The fuzzy output must then be utilized to control the system . <p> Now , we go to the LabView Fuzzy Logic Toolkit which is located in the Tools menu of the LabView programming environment.In true LabView style , the Fuzzy Logic Toolkit is a GUI , a Graphical User Interface.We must first open @ @ @ @ @ @ @ @ @ @ name and define our variables by going to the Edit menu and selecting Set Editor . <p> For example , the first Fuzzy Logic Controller I designed was a simple controller to trace the data I collected of an analog channel on the DAQ box connected to my computer.I read the voltage off of a function generator that was generating either sine , square or triangle waveforms . <p> The output of the Fuzzy Controller I named " Fuzzy Position " and plotted versus " Wave Position , " the analog input.I then took the difference of the two and named it " Error " and made it the first Fuzzy Controller input variable.This controller would essentially only consider the Error term in calculating its output , though considering multiple terms is possible . <p> Next I had to define my Fuzzy Sets.You can define a maximum of 9 sets for each variable.For Error , I designated the sets " Far Under , " " Under , " " Mid Under , " " Slightly Under , " " Close , " " Slightly Over , " " Mid Over @ @ @ @ @ @ @ @ @ @ . " Now , I had to decide the range of these variables.Since I was using a function generator whose waveforms spanned the range of 1 volt , the maximum magnitude of error would be 2 . Thus , I made the range of the error variable to be 2 . <p> Similarly , I defined the response variable.The sets were named " Down Hard , " " Down , " " Slight Down , " " No Move , " " Slight Up , " " Up , " and " Up Hard . " I also used 2 as the range of this variable as any correction to the error would have to lie in this range . <p> Next , I had to designate the rules by which the Fuzzy Logic Controller would take in inputs " fuzzify " them , determine their set memberships , " defuzzify " them and determine the appropriate response.Complete step-by-step details for the LabView Fuzzy Logic Controller Design environment can be found in 1 Chapter 5 . <p> Essentially , you pick a response for each combination of inputs.Here , Wave @ @ @ @ @ @ @ @ @ @ to the waveform.Notice the column labeled " DoS , " this is the weighting factor.This will modify how the Defuzzification process will work , it will weight responses to give you slightly better results if used properly . <p> The Fuzzy Logic Controller will then take in the input variables match them up with your linguistic variables , and determine how the input should be converted to the appropriate output.It can do this by one of three " Defuzzification " methods , Center-of-Gravity , Center-of-Maximum or Mean-of-Maximum . <p> The different Defuzzification methods produce different results , but for now , we do n't  need to worry about these differences.It suffices to say that for simple controllers the outputs from the different controllers are similar enough that simply trying out the three methods and determining which one best suits your needs is the most effective way to choose a Defuzzification method . <p> Next I set up the VI to properly handle the data and the built-in capabilities of LabView did the rest of the work.The following trace is the result of the Fuzzy Controller responding to an inputted analog @ @ @ @ @ @ @ @ @ @ red is the fuzzy logic output . <p> III:Designing a More Complicted Fuzzy Logic Controller <p> To design a more complicated Fuzzy Logic Controller , one simply has to generalize the ideas from setting up a simple controller to a more complicated system.Probably the most difficult part will be incorporating multiple variables.There is a wonderful example of a 2 variable Fuzzy Logic Controller that is included with the PID control toolset of LabView , for more details see 1 Chapter 5 and the VI included in the PID toolset . <p> When controlling actual instruments , one must first test how the Fuzzy Logic Response generalizes to your specific equipment.For instance , a fuzzy logic response of 20 is how much you want to change a given parameter , not necessarily the voltage or amperage you want to output to the instrument.Test the response to a small test step increase in voltage and determine the necessary output levels and avoid instrument damage.It is always a good idea to have a simulation to test your controller and controller software on before using your actual equipment.Errors in software are easier to rectify @ @ @ @ @ @ @ @ @ @ instrument . <p> When creating more complicated VIs that utilize the Fuzzy Logic Controller be sure to have routines that will stop the VI if an error occurs.The Fuzzy Logic Controller does not always respond well to errors in other parts of the VI , this can easily cause instrument damage . <p> IV:Summary <p> The Fuzzy Logic Controller is a powerful tool in controlling instruments and equipment.Using Fuzzy Logic can quickly lead to more efficient , precise and accurate controls.For instance , the Fuzzy Logic Waveform Tracer described in Section II has since been modified to create a control for an ANCO shake table located in Argonne National Laboratory.The Fuzzy Logic Controllers are easily adapted , a few slight changes in range of the variables and finding a conversion factor appropriate for the equipment and I was ready to write software to control the table . <p> The benefits of Fuzzy Logic Controllers are simple ; they can be more accurate and more precise than their PID counterparts.For example , the Fuzzy Logic controller on the ANCO shake table was used in a program to simulate a trace of @ @ @ @ @ @ @ @ @ @ cut down the effective error by nearly half compared to the PID control , but more impressively cut down the standard deviation of the error by 60-70% , thus creating a far more precise control.Further , there is better control on the overshoot that often occurs on large jumps in the earthquake trace as well as considerably less " ringing . " <p> LabView provides a powerful toolkit for setting up these Fuzzy Logic Controllers.Essentially all the hard work of Fuzzy Logic is done internally by the LabView program , the user merely needs to go through a Graphical Interface and set up a few picture guided screens to create a powerful , highly effective tool for controlling instruments.The Fuzzy Logic Controllers are more precise and accurate than their PID predecessors , and are actually easier to learn to a beginner . 
@@97506116 @1706116/ <h> Research Overview <p> Technology is driving revolutionary changes in biology . What began as the sequencing era has led to methods for deducing the network architecture that defines gene regulation and cellular signaling . Systems Biology can be viewed as the generation of such networks , along with the development of computational models to describe how they mitigate cellular behavior . Likewise , DNA synthesis technologies are driving the development of Synthetic Biology , whereby genomes can be reconstituted from chemical building blocks . This could lead to cells with highly reduced genomic complexity , as genes that govern the ability to adapt to multiple environments are eliminated to construct specialized organisms for biotechnology and basic research . Finally , imaging technologies that span many length scales , from tissues to single molecules , are catalyzing the development of Quantitative Biology . A central goal of Q-Bio is the deduction of the fundamental equations that can be used to describe biology . <p> The Biodynamics Laboratory ( BDL ) seeks to understand the network interactions that mediate gene regulation and cellular signaling . Since behavior arising from these @ @ @ @ @ @ @ @ @ @ we employ experimentally validated computational modeling approaches . We design and construct de novo synthetic gene circuits , which provide a natural framework for reducing the complexity of gene regulatory networks . We use tools from physics and engineering to study such simplified systems and to dissect , analyze , and control the modular components that govern the dynamics of gene regulation and cellular signaling . <h> Synthetic Biology <p> Quorum-triggered oscillations in a growing population <p> The engineering of genetic circuits with predictive functionality in living cells is a defining focus of synthetic biology . What started with the design and construction of a genetic toggle switch and an oscillator a decade ago , has led to circuits capable of pattern generation , edge detection and event counting . We have sucessfully engineered a gene network with global intercellular coupling which can generate synchronized oscillations in a growing population of cells- using microfluidic devices tailored for cellular populations at differing length scales , we investigate synchronization properties and spatiotemporal waves . Our synchronized genetic clock sets the stage for the use of microbes in the creation of a macroscopic @ @ @ @ @ @ @ @ @ @ model system for the generation of a mechanistic description of emergent coordinated behaviour at the colony level . <h> Systems Biology <p> Single-cell gene response in a changing environment <p> Natural selection dictates that cells respond optimally to environmental changes . The nature of such an optimal response is likely to depend on the specifics of the dynamically changing environment . For example , a rapid response may be optimal when environmental changes are slow , whereas a slow response may be necessary to effectively filter undesirable environmental fluctuations . While this assumption of a context-dependent cellular response is perhaps natural , the current understanding of the regulation of most cellular processes has relied on data generated from static or semi-static environments . We focus on systems biology approaches aimed at transforming our understanding of cellular response from the realm of tightly controlled batch experiments to that of dynamically changing environmental conditions . <h> Mammalian Cell Signaling <p> Dynamic macrophage response to lipopolysaccharide ( 24hrs ) <p> The tools we use to construct and explore synthetic systems can also be used in larger-scale networks . Mammalian cells are capable of @ @ @ @ @ @ @ @ @ @ single signals at nanomolar concentrations . Intrinsic and extrinsic noise play a large role in this process- cells exposed to the same signal behave extremely heterogeneously . As a result , many large-scale responses have been notoriously difficult to even define with traditional static and population-level studies . We take a new approach to these systems , describing changes in expression and behavior using newly-developed automated tracking techniques . We use this approach to study responses in dynamic signaling environments which more accurately replicate in vivo conditions . <h> Rational Design and Evolution <p> A competitive growth advantage in a natural environment is a key to survival , as the ability to consume available nutrients and grow faster than competing organisms ensures the propagation of a species . Therefore , organisms have evolved complex metabolic networks to confer the optimal growth characteristics in their most common environments . We combine microfluidic technology and synthetic biology with quantitative modeling to engineer competitive growth advantages in dynamic environments through both a natural evolutionary process as well as synthetic network construction . <h> Microfluidics Design <p> Microfluidics chip loaded with E. Coli <p> @ @ @ @ @ @ @ @ @ @ the field of synthetic biology and beyond . Microfluidics facilitates the study of cellular behavior because it provides the necessary tools for recreating in vivo-like cellular microenvironments . We use microfluidic devices to observe cellular development within dynamic microenvironments , and design devices to generate thermal or chemical gradients or incorporate large-scale networks of fluidic channels for high-throughput cellular analysis . With these devices , we can generate single-cell expression profiles for a large number of cells , which is essential to understanding the roles of regulatory motifs within native and synthetic gene networks . 
@@97506119 @1706119/ <p> This page provides a guide to waxing and caring for your skis or snowboard at home . It details prepare your skis or snowboard for waxing , how to apply hotwax and how to remove the wax so you are ready to hit the slopes . <p> Before Getting Started <p> I recommend that you use a universal wax as the primary wax since it has a wide range of temperature applications . This means that it will work in all snow conditions , both warm and cold snow . In this way you will have a general purpose base coat that if necessary you can always re-coat with a specific temperature wax such as a fluorowax . <p> Highly flourinated waxes as the second coat help with swing and speed , but wear down faster than universal wax . Hence having a good universal base coat is essential . <p> Do <p> 1 . Make sure you purchase the proper equipment ( or the equivalent ) : <p> waxing iron ( a simple cheap electric iron will do for this . Just do n't expect it to @ @ @ @ @ @ @ @ @ @ do not use your mothers ultraglide super expensive iron - you have been warned ) <p> universal hot wax <p> temp specific/flourinated wax ( only needed for a second coat if you so desire ) <p> a large plastic scraper ( must be wider than the ski edges ) <p> a nylon structure brush ( there are many types of brushes - brass , nylon , horse hair , etc. ) unless you have a different preference . <p> 2 . Consider purchasing a ski / board vise - a moving ski or board is always hard to wax . You can also use a simple G-Clamp - just be sure to put a suitable block of wood under the ski and some material around the clamp to protect the ski surface . <p> 5 . Avoid burning the wax - it will smoke if it is burning . You only want to melt the wax - however , a small amount of smoke is okay . But if you can smell burning that is bad . <p> 6 . Scrape excess wax off , but be careful not to @ @ @ @ @ @ @ @ @ @ 7 . Buff the scraped layer of wax to put some structure to it . This breaks up surface suction . Useful when skiing on wet snow . <p> You can search for wax and waxing equipment sellers below : <p> Enter your search termsSubmit search form <p> Do n't <p> 1 . Do n't overheat the wax . This can destroy the effectiveness of the wax . If you smell burning or smoky smells then the temperature of the iron is too high . <p> 2 . Never overheat the ski or snowboard . You need to use quick constant motion at all times . Never pause while the iron is in contact with the skis surface . SERIOUS DAMAGE can occur to the ski or board both internally and externally if you overheat it . <p> 3 . Do not leave too thick a layer of wax on the base of the ski . If you do not remove enough of the wax you will find the skis are very slow and turning is much more difficult . You do n't need to worry too much about this @ @ @ @ @ @ @ @ @ @ couple of runs is normally sufficient to clean it off . <p> 4 . Never use a metal scraper . This will damage both the ski edge and the base . Always use an acrylic scraper . <p> With this in mind you are ready to learn how to wax your skis or snowboard : 
@@97506124 @1706124/ <h> CSE 151 Lecture Notes INTRODUCTION TO ARTIFICIAL INTELLIGENCE <h> Searching for Solutions <h> Administrivia : <p> The second homework assignment is due today at the start of class . <p> The third homework assignment will be handed out at the end of class . <p> I have decided to discuss the very important topic of heuristic search , in particular the A* algorithm . This should be review for most people since it was discussed in CSE150 . Please read sections 4.1 and 4.2 in the book . Understanding this material will be helpful in doing the first programming assignment . <p> A few clarifications have been made to the first programming assignment . I will hand out the updated assignment at the end of class . <h> Overview <p> Problem-solving agents : find sequence of actions that achieve goals . <p> The general search algorithm <p> Operators expand a state : generate new states from present ones . <p> search strategy : tells which state to expand next . <p> fringe or frontier : discovered states to be expanded <p> Some search strategies : <p> Evaluation criteria @ @ @ @ @ @ @ @ @ @ one exists ? <p> time efficiency : <p> space efficiency : <p> optimality of solution : <p> Uninformed or blind searches : <p> Breadth-first : ( FIFO queuing strategy ) <p> complete : yes <p> time and space : O(branchfactordepth) ( empirically space a larger problem than time ) <p> optimality : yes , if path cost non-decreasing with depth . <p> Uniform Cost ( expand lo cost fringe node ) <p> optimal : yes , if no negative costs <p> a generalization of Breadth-first . <p> Depth-first ( uses a stack ) <p> completeness : no ! may go down and not come back <p> time : O(branchfactordepth) <p> space : O(depth) <p> optimality : no ! returns first found , not necessarily ideal <p> depth-limited ( DFS down to some cutoff ) <p> completeness : yes , provided solution exists before cutoff <p> time : **25;332;TOOLONG <p> space : O(depthlimit) <p> optimality : no ! <p> Iterative Deepening ( successively increasing depth-limited ) <p> diameter of state space : max anticipated path length for most problems . <h> Searching for solutions <p> Instead the agent knows the initial @ @ @ @ @ @ @ @ @ @ a function which " expands " a node . <p> " Expanding " a node means computing the node that the agent could move to using the operator . <p> With this available knowledge , the general search algorithm that the agent can use is : <p> The Romanian travel example <p> Imagine using a map to plan your trip from Arad to Bucharest . Start with your finger on Arad . <p> Arad is not a goal node so expand it to find all its successors : Sibiu , Timisoara , and Zerind . <p> Now make a choice of one of these to investigate next , say Sibiu . <p> Sibiu is not a goal node so expand it to get its successors : Arada , Fagaras , Oradea , and Rimnicu Vilcea . <p> Now make a choice of any one of the current leaf nodes to expand next , i.e. one of Arada , Fagaras , Oradea , Rimnicu Vilcea , Timisoara , and Zerind . <p> And so on , until the node you choose is a goal node . <p> The search tree <p> @ @ @ @ @ @ @ @ @ @ the search space directly . <p> Notice that expanding the node Sibiu gives Arad , which has already been visited . <p> In general the search algorithm does not " know " this . <p> In some domains it is easy to test whether multiple nodes correspond to the same state . In other domains it is impossible . <p> Consider the 8-queens problem . If we define states to be " any arrangement of 8 queens on the board " , then it becomes very difficult to determine if we have already visited a state during search because there are 648 = 264 possible states . <p> We could try to keep track of the states we have already visited ( say , with a has table ) , but this would require 264 entries ! <p> But this problem can be handled in another way--by making the operators avoid states that have already been visited . For example , we could design an operator for the 8-queens problem that always places a queen in the leftmost unoccupied column . This insures that no state is ever revisited without @ @ @ @ @ @ @ @ @ @ ? ) <p> Representing the search tree <p> What is called a " representation " in AI is often called a data structure elsewhere in computer science . <p> We will represent a node as a list ( technically a tuple ) with the following components : <p> information about the corresponding state e.g. the string " Arad " <p> the parent node ( actually , a pointer to the parent node ) <p> the operator that was applied to the parent and which generated this node ( actually , just its name or a pointer ) <p> the depth of this node <p> the path cost from the initial state to this node <p> The depth of a node is the number of nodes on the path to this node from the root . <p> The fringe of the tree is the set of leaf nodes of the tree . A leaf node is a node that is NOT the parent of any other node . <p> The search algorithm needs an efficient representation of the fringe . We shall use a queue , i.e. a special list where @ @ @ @ @ @ @ @ @ @ insert new elements into the middle . <p> The path cost of a node is the cost of the path from the root to this node . This is normally the sum of the costs of the operators used along the path to the node . <p> The general search algorithm ( again ) <p> The variable nodes is the queue . <p> The well-formulated search problem includes the following functions : <p> MAKE-QUEUE creates a queue with the given elements <p> EMPTY ? tests whether the queue is empty <p> REMOVE-FRONT removes the first element of the queue <p> EXPAND applies all the operators of the problem to a node and returns all the successors of this node <p> QUEUING-FN inserts elements into a queue <p> The key point is that we can change the behavior of the search algorithm by changing the internals of the queuing function . <p> Search strategies <p> Different algorithms use different " insert " functions . <p> We can evaluate algorithms according to : <p> completeness : does it always find a solution , if one exists ? <p> time complexity ( = @ @ @ @ @ @ @ @ @ @ optimality : does it find the highest-quality solution ? <p> If a search strategy has no idea of the path cost or search cost from the current node to the goal , it is referred to as an uninformed or blind strategy . <p> A strategy which uses such knowledge is called informed or heuristic search . <p> Blind search is less effective than heuristic search , but it forms the basis of many heuristic search techniques . <p> Uninformed search strategies <p> Breadth-first search <p> BFS is the general search algorithm where the " insert " function is " enqueue-at-end " . This means that newly generated nodes are added to the fringe at the end , so they are expanded last . <p> BFS first considers all paths of length 1 , then all paths of length 2 , and so on . This is why it is called " breadth-first " . <p> The number of nodes at depth d or less is N = 1 + b + b2 + ... + bd . <p> Somewhat surprisingly , N = O(bd) . <p> Intuitively , almost @ @ @ @ @ @ @ @ @ @ the number of shallower nodes is negligible . <p> For a problem with branching factor b where the first solution is at depth d , the time complexity of BFS is O(bd) . <p> The space complexity of BFS is also O(bd) . <p> The big problem with depth-first search is that it uses about as much space as it uses time . Any real computer will run out of space before it runs out of time , at this rate . <p> Depth-first search <p> DFS is the general search algorithm where the " insert " function is " enqueue-at-front " . This means that newly generated nodes are added to the fringe at the beginning , so they are expanded immediately . <p> DFS goes down a path until it reaches a node that has no children . Then DFS " backtracks " and expands a sibling of the node that had no children . If this node has no siblings , then DFS looks for a sibling of the grandparent , and so on . <p> See the picture below for an illustration of DFS . <p> @ @ @ @ @ @ @ @ @ @ will always go deeper if it has a child . <p> The major weakness of DFS is that it will fail to terminate if there is an infinite path " to the left of " the path to the first solution . <p> In other words , for many problems DFS is not complete : a solution exists but DFS can not find it . <p> The major advantage of DFS is that it only uses O(bm) space if the branching factor is b and the maximum depth is m . <p> ( Explanation : there are m nodes on the longest path , and for each of these b-1 siblings must be stored . ) <p> Iterative deepening <p> Iterative deepening is a very simple , very good , but counter-intuitive idea that was not discovered until the mid 1970s . Then it was invented by many people simultaneously . <p> The idea is to do depth-limited DFS repeatedly , with an increasing depth limit , until a solution is found . <p> Intuitively , this is a dubious idea because each repetition of depth-limited DFS will repeat uselessly @ @ @ @ @ @ @ @ @ @ , this useless repetition is not significant because a branching factor b &gt; 1 implies that <p> # nodes at depth k &gt;&gt; # nodes at depth k-1 or less <p> Iterative deepening simulates BFS with linear space complexity . <p> For a problem with branching factor b where the first solution is at depth d , the time complexity of iterative deepening is O(bd) , and its space complexity is O(bd) . <p> Informed search strategies <p> Heuristic " best-first " search <p> " Knowledge is power . " How can we use knowledge to improve our search algorithm ? <p> One common type of knowledge is a scoring function that estimates how good a node is as the next node to expand . <p> Note the word " estimates " . If the scoring function was perfect , then we would n't need any search at all . <p> The function " eval " scores nodes according to how promising they are for further search ( lower is better ) <p> The QUEUING-FN places new nodes into the queue according to their scores . <p> This type of @ @ @ @ @ @ @ @ @ @ designers . <p> The idea is that the next node to be taken from the queue and expanded will be the one with the highest priority , i.e. the lowest score . <p> Minimizing cost to goal : Greedy search <p> A heuristic scoring function , h , is defined as follows : <p> h(n) = estimated cost of the cheapest path from state(n) to a goal <p> Greedy search is best-first search with h as its " eval " function . <p> For each particular problem , the designer must choose an appropriate h function . <p> For example , for the Romanian travel problem let h(n) be the straight line " as the crow flys " distance from state(n) to Bucharest . <p> Greedy search is called greedy because it always tries to make the biggest jump possible towards the goal , without " thinking " further ahead about what will happen from that point . <p> See the figure below for an example of how greedy search does not find the optimal path . <p> In the worst-case , the time and space complexity of greedy search @ @ @ @ @ @ @ @ @ @ the " average " case having a good heuristic function h should make greedy search much better . <p> Proving that greedy search is better is very difficult mathematically . It is common in AI that a heuristic idea is good but proving that it is good is very difficult--precisely because heuristics are not guaranteed . <p> Minimizing total path cost : A* search <p> A* search is best-first search with an admissible heuristic . <p> An admissible heuristic is an optimistic heuristic--one that never overestimates the remaining cost to the goal . <p> The idea is that if we want to find the optimal path to the goal , i.e. the shortest or cheapest path , then the next node to explore should be the one that looks like it will yield the OVERALL cheapest path . <p> The best estimate of the total cost of the path through a candidate node is <p> f(n) = g(n) + h(n) <p> where h is the same estimator function of remaining distance as above and <p> g(n) = cost of path from the initial state to state(n) . <p> Note that @ @ @ @ @ @ @ @ @ @ are standard : you should remember to use this notation . <p> Study the example in the figure below carefully . <p> The importance of the A* algorithm is that it can be proven to be : <p> optimal--the first path to a goal found by A* has lowest path cost of all possible paths , <p> complete--always finds a solution if one exists and the branching factor is finite ( although this may take exponential time ! ) , <p> optimally efficient--no other optimal and complete algorithm that only has g and h available as knowledge always expands fewer nodes than A* . <p> Finding heuristic functions <p> The practical usefulness of A* depends entirely on having a good heuristic function . <p> Consider the 8 puzzle and two heuristic functions ; <p> h1 = number of misplaced tiles h2 = sum of distances of tiles from goal position <p> Note that h1 and h2 both always underestimate the number of tiles that must be moved to reach a solution : they are both admissible . <p> Note also that h1(n) &lt;= h2(n) for all states of the 8 @ @ @ @ @ @ @ @ @ @ h1 . <p> To invent an admissible heuristic , take the original problem and relax some of the demands so that it is possible to solve the easier problem without search . <p> Use the cost of the solution to the relaxed problem as the underestimate of the actual solution cost . 
@@97506126 @1706126/ <h> Transition to Comet <p> Please read this important information for users transitioning from Gordon . The SDSC Gordon cluster is scheduled to be decommissioned at the end of March , 2017 . To minimize the impact of this change and allow you to continue computing without interruption , we will be transitioning Gordon users to Comet . More details are provided in the Transitioning from Gordon section of the Comet User Guide . <h> System Access <p> As an XSEDE computing resource , Gordon is accessible to XSEDE users who are given time on the system . In order to get an account , users will need to submit a proposal through the XSEDE Allocation Request System . <p> Interested users may contact SDSC User Support for assistance with applying for time on Gordon ( see sidebar for contact information ) . <p> Logging in to Gordon <p> Gordon supports Single Sign On through the XSEDE User Portal and from the command line using an XSEDE-wide password . To login to Gordon from the command line , use the hostname : <p> gordon.sdsc.edu <p> The following are examples @ @ @ @ @ @ @ @ @ @ used to log in to Gordon : <h> Notes and hints <p> When you login to gordon.sdsc.edu , you will be assigned one of the four login nodes gordon-ln1-4.sdsc.edu . These nodes are identical in both architecture and software environment . Users should normally login to gordon.sdsc.edu , but can directly access one of the four nodes directly if they see poor performance . <p> Please feel free to append your public RSA key to your /. ssh/authorizedkeys file to enable access from authorized hosts without having to enter your password . Make sure you have a password on the private key on your local machine . You can use ssh-agent or keychain to avoid repeatedly typing the private key password . <p> Do not use the login nodes for computationally intensive processes . These nodes are meant for compilation , file editing , simple data analysis , and other tasks that use minimal compute resources . All computationally demanding jobs should be submitted and run through the batch queuing system . <h> Modules <p> The Environment Modules package provides for dynamic modification of your shell environment . Module commands @ @ @ @ @ @ @ @ @ @ in support of a particular application . They also let the user choose between different versions of the same software or different combinations of related codes . <p> For example , if the Intel module and mvapich2ib module are loaded and the user compiles with mpif90 , the generated code is compiled with the Intel Fortran 90 compiler and linked with the mvapich2ib MPI libraries . <p> Several modules that determine the default Gordon environment are loaded at login time . These include the MVAPICH implementation of the MPI library and the Intel compilers . We strongly suggest that you use this combination whenever possible to get the best performance . <p> Useful Modules Commands <p> Here are some common module commands and their descriptions : <p> Command <p> Description <p> module list <p> List the modules that are currently loaded <p> module avail <p> List the modules that are available <p> module display &lt;modulename&gt; <p> Show the environment variables used by &lt;module name&gt; and how they are affected <p> module unload &lt;module name&gt; <p> Remove &lt;module name&gt; from the environment <p> module load &lt;module name&gt; <p> Load &lt;module name&gt; @ @ @ @ @ @ @ @ @ @ <p> Replace &lt;module one&gt; with &lt;module two&gt; in the environment <p> Loading and unloading modules <p> You must remove some modules before loading others . <p> Some modules depend on others , so they may be loaded or unloaded as a consequence of another module command . For example , if intel and mvapich2ib are both loaded , running the command module unload intel will automatically unload mvapich2ib . Subsequently issuing the module load intel command does not automatically reload mvapich2ib . <p> If you find yourself regularly using a set of module commands , you may want to add these to your configuration files ( . bashrc for bash users , . cshrc for C shell users ) . Complete documentation is available in the module(1) and modulefile(4) manpages . <p> Module : command not found <p> The error message module : command not found is sometimes encountered when switching from one shell to another or attempting to run the module command from within a shell script or batch job . The reason that the module command may not be inherited as expected is that it is defined as @ @ @ @ @ @ @ @ @ @ this error execute the following from the command line ( interactive shells ) or add to your shell script ( including Torque batch scripts ) <p> source **25;359;TOOLONG <h> Software Packages <p> Sample scripts using some common applications and libraries are provided in **27;386;TOOLONG . If you have any questions regarding setting up run scripts for your application , please email help@xsede.org . <h> Software Package Descriptions <p> AMBER <p> AMBER is package of molecular simulation programs including SANDER ( Simulated Annealing with NMR-Derived Energy Restraints ) and a modified version PMEME ( Particle Mesh Ewald Molecular Dynamics ) that is faster and more scalable . <p> CP2K is a program to perform simulations of molecular systems . It provides a general framework for different methods such as Density Functional Theory ( DFT ) using a mixed Gaussian and plane waves approach ( GPW ) and classical pair and many-body potentials . <p> Parallel Three-Dimensional Fast Fourier Transforms is a library for large-scale computer simulations on parallel platforms . 3D FFT is an important algorithm for simulations in a wide range of fields , including studies of turbulence , climatology @ @ @ @ @ @ @ @ @ @ platform to support users that have different environmental needs then what is provided by the resource or service provider . While the high level perspective of other container solutions seems to fill this niche very well , the current implementations are focused on network service virtualization rather than application level virtualization focused on the HPC space . Because of this , Singularity leverages a workflow and security model that makes it a very reasonable candidate for shared or multi-tenant HPC resources like Comet without requiring any modifications to the scheduler or system architecture . Additionally , all typical HPC functions can be leveraged within a Singularity container ( e.g. InfiniBand , high performance file systems , GPUs , etc . ) . While Singularity supports MPI running in a hybrid model where you invoke MPI outside the container and it runs the MPI programs inside the container , we have not yet tested this . <p> " Examples for various modes of usage are available in **34;415;TOOLONG . Please email help@xsede.org ( reference Gordon as the machine , and SDSC as the site ) if you have any further questions @ @ @ @ @ @ @ @ @ @ are available at http : **27;451;TOOLONG . <p> VisIt Visualization Package <p> The VisIt visualization package supports remote submission of parallel jobs and includes a Python interface that provides bindings to all of its plots and operators so they may be controlled by scripting . <p> To charge your job to one of these projects replace &lt;&lt; project &gt;&gt; with one from the list and put this PBS directive in your job script : <p> #PBS -A &lt;&lt; project &gt;&gt; <p> Many users will have access to multiple accounts ( e.g. an allocation for a research project and a separate allocation for classroom or educational use ) . On some systems a default account is assumed , but please get in the habit of explicitly setting an account for all batch jobs . Awards are normally made for a specific purpose and should not be used for other projects . <p> Adding users to an account <p> Project PIs and co-PIs can add or remove users from an account . To do this , log in to your XSEDE portal account and go to the add user page . <p> @ @ @ @ @ @ @ @ @ @ including Gordon , is the SU ( service unit ) and corresponds to the use of one compute core for one hour . Keep in mind that your charges are based on the resources that are tied up by your job and do n't  necessarily reflect how the resources are used . <p> Unlike some of SDSCs other major compute resources , Gordon does not provide for shared use of a compute node . A serial job that requests a compute node for one hour will be charged 16 SUs ( 16 cores x 1 hour ) , regardless of how the processors-per-node parameter is set in the batch script . <p> The large memory vSMP nodes , on the other hand , will be charged according to the number of cores requested . Be sure to request as many cores as your job needs , but be aware that if you request all 256 cores within a vSMP node , your job will be charged at the rate of 256 SUs per hour . <h> Compiling <p> Gordon provides the Intel , Portland Group ( PGI ) , and @ @ @ @ @ @ @ @ @ @ MPICH2 , OpenMPI ) . Most applications will achieve the best performance on Gordon using the Intel compilers and MVAPICH2 and the majority of libraries installed on Gordon have been built using this combination . Although other compilers and MPI implementations are available , we suggest using these only for compatibility purposes . <p> All three compilers now support the Advanced Vector Extensions ( AVX ) . Using AVX , up to eight floating point operations can be executed per cycle per core , potentially doubling the performance relative to non-AVX processors running at the same clock speed . Note that AVX support is not enabled by default and compiler flags must be set as described below . <h> Using the Intel compilers ( Default/Suggested ) <p> The Intel compilers and the MVAPICH2 MPI implementation will be loaded by default . If you have modified your environment , you can reload by executing the following commands at the Linux prompt or placing in your startup file ( /. cshrc or /. bashrc ) <p> module purge module load intel mvapich2ib <p> For AVX support , compile with the -xHOST option @ @ @ @ @ @ @ @ @ @ , so compilation with -O3 is also suggested . The -fast flag invokes -xHOST , but should be avoided since it also turns on interprocedural optimization ( -ipo ) , which may cause problems in some instances . <p> Intel MKL libraries are available as part of the " intel " modules on Gordon . Once this module is loaded , the environment variable MKLROOT points to the location of the mkl libraries . The MKL link advisor can be used to ascertain the link line ( change the MKLROOT aspect appropriately ) . <p> For example to compile a C program statically linking 64 bit scalapack libraries on Trestles : <p> For more information on the Intel compilers : ifort icc icpc -help <p> Serial <p> MPI <p> OpenMP <p> MPI+OpenMP <p> Fortran <p> ifort <p> mpif90 <p> ifort -openmp <p> mpif90 -openmp <p> C <p> icc <p> mpicc <p> icc -openmp <p> mpicc -openmp <p> C++ <p> icpc <p> mpicxx <p> icpc -openmp <p> mpicxx -openmp <p> Note for vSMP users on Gordon MPI applications intended for use on the large memory vSMP nodes should use MPICH2 instead @ @ @ @ @ @ @ @ @ @ tuned for optimal vSMP message passing performance . <p> Note for C/C++ users : compiler warning - feupdateenv is not implemented and will always fail . For most users , this error can safely be ignored . By default , the Intel C/C++ compilers only link against Intel 's optimized version of the C standard math library ( libmf ) . The error stems from the fact that several of the newer C99 library functions related to floating point rounding and exception handling have not been implemented . <h> Using the PGI compilers <p> The PGI compilers can be loaded by executing the following commands at the Linux prompt or placing in your startup file ( /. cshrc or /. bashrc ) <p> module purge module load pgi mvapich2ib <p> For AVX support , compile with -fast <p> For more information on the PGI compilers : man pgf90 pgcc pgCC <p> Serial <p> MPI <p> OpenMP <p> MPI+OpenMP <p> Fortran <p> pgf90 <p> mpif90 <p> pgf90 -mp <p> mpif90 -mp <p> C <p> pgcc <p> mpicc <p> pgcc -mp <p> mpicc -mp <p> C++ <p> pgCC <p> mpicxx <p> pgCC @ @ @ @ @ @ @ @ @ @ The GNU compilers can be loaded by executing the following commands at the Linux prompt or placing in your startup files ( /. cshrc or /. bashrc ) <p> module purge module load gnu openmpi <p> For AVX support , compile with -mavx . Note that AVX support is only available in version 4.6 or later , so it is necessary to explicitly load the gnu/4.6.1 module until such time that it becomes the default . <p> For more information on the GNU compilers : man gfortran gcc g++ <p> Serial <p> MPI <p> OpenMP <p> MPI+OpenMP <p> Fortran <p> gfortran <p> mpif90 <p> gfortran -fopenmp <p> mpif90 -fopenmp <p> C <p> gcc <p> mpicc <p> gcc -fopenmp <p> mpicc -fopenmp <p> C++ <p> g++ <p> mpicxx <p> g++ -fopenmp <p> mpicxx -fopenmp <h> Notes and Hints <p> The mpif90 , mpicc , and mpicxx commands are actually wrappers that call the appropriate serial compilers and load the correct MPI libraries . While the same names are used for the Intel , PGI and GNU compilers , keep in mind that these are completely independent scripts . <p> If you @ @ @ @ @ @ @ @ @ @ for different applications , make sure that you load the appropriate modules before running your executables . <p> When building OpenMP applications and moving between different compilers , one of the most common errors is to use the wrong flag to enable handling of OpenMP directives . Note that Intel , PGI , and GNU compilers use the -openmp , -mp , and -fopenmp flags , respectively . <p> Explicitly set the optimization level in your makefiles or compilation scripts . Most well written codes can safely use the highest optimization level ( -O3 ) , but many compilers set lower default levels ( e.g. GNU compilers use the default -O0 , which turns off all optimizations ) . <p> Turn off debugging , profiling , and bounds checking when building executables intended for production runs as these can seriously impact performance . These options are all disabled by default . The flag used for bounds checking is compiler dependent , but the debugging ( -g ) and profiling ( -pg ) flags tend to be the same for all major compilers . <h> Running Jobs on Regular Compute Nodes @ @ @ @ @ @ @ @ @ @ the Catalina Scheduler , to manage user jobs . If you are familiar with PBS , note that TORQUE is based on the original PBS project and shares most of its syntax and user interface . Whether you run in batch mode or interactively , you will access the compute nodes using the qsub command as described below . Remember that computationally intensive jobs should be run only on the compute nodes and not the login nodes . Gordon has two queues available : <p> Queue Name <p> Max Walltime <p> Max Nodes <p> Comments <p> normal <p> 48 hrs <p> 64 <p> Used for exclusive access to regular ( non-vsmp ) compute nodes <p> vsmp <p> 48 hrs <p> 1 <p> Used for shared access to vamp node <p> Submitting jobs <p> A job can be submitted using the qsub command , with the job parameters either specified on the command line or in a batch script . Except for simple interactive jobs , most users will find it more convenient to use batch scripts . <p> TORQUE batch scripts consist of two main sections . The top section @ @ @ @ @ @ @ @ @ @ notification details ) while the bottom section contains user commands . A sample job script that can serve as a starting point for most users is shown below . Content that should be modified as necessary is between angled brackets ( the brackets are not part of the code ) . <p> The first line indicates that the file is a bash script and can therefore contain any valid bash code . The lines starting with " #PBS " are special comments that are interpreted by TORQUE and must appear before any user commands . <p> The second line states that we want to run in the queue named " normal " . Lines three and four define the resources being requested : 2 nodes with 16 processors per node , for one hour ( 1:00:00 ) . The next three lines ( 5-7 ) are not essential , but using them will make it easier for you to monitor your job and keep track of your output . In this case , the job will be appear as " jobname " in the queue ; stdout and stderr will @ @ @ @ @ @ @ @ @ @ , respectively . The next line specifies that the usage should be charged to account abc123 . Lines 9 and 10 control email notification : notices should be sent to " username@domain.edu " when the job aborts ( a ) , begins ( b ) , or ends ( e ) . <p> Finally , " #PBS V " specifies that your current environment variables should be exported to the job . For example , if the path to your executable is found in your PATH variable and your script contains the line " #PBS V " , then the path will also be known to the batch job . <p> The statement " cd $PBSOWORKDIR " changes the working directory to the directory where the job was submitted . This should always be done unless you provide full paths to all executables and input files . The remainder of the script is normally used to run your application . <p> Interactive jobs <p> There are several small but important differences between running batch and interactive jobs : <p> Interactive jobs require the -I flag <p> All node resource requests @ @ @ @ @ @ @ @ @ @ , comma-separated list <p> To ensure obtaining nodes with flash storage , the " : flash " specifier must be included in the node properties <p> The following command shows how to get interactive use of one node , with flash , for 30 minutes ( note the comma between " flash " and " walltime " ) : <p> If neither : flash or : noflash is requested , the interactive job may land on a mix of nodes with and without SSDs . Be reminded that this only applies to jobs submitted without a batch script , and jobs submitted via a submission script automatically receive the : flash property . <p> Monitoring and deleting jobs <p> Use the qstat command to monitor your jobs and qdel to delete a job . Some useful options are described below . For a more detailed understanding of the queues see the User Guide section Torque in Depth . <p> Running MPI jobs regular compute nodes <p> MPI jobs are run using the mpirunrsh command . When your job starts , TORQUE will create a PBSNODEFILE listing the nodes that had @ @ @ @ @ @ @ @ @ @ replicated ppn times . Typically ppn will be set equal to the number of physical cores on a node ( 16 for Gordon ) and the number of MPI processes will be set equal to nodes x ppn . Relevant lines from a batch script are shown below . <p> Sometimes you may want to run fewer than 16 MPI processes per node ; for instance , if the per-process memory footprint is too large ( &gt; 4 GB on Gordon ) to execute one process per core or you are running a hybrid application that had been developed using MPI and OpenMP . In these cases you should us the ibrun command and the -npernode option instead : <p> For hybrid parallel applications ( MPI+OpenMP ) , use ibrun to launch the job and specify both the number of MPI processes and the number of threads per process . To pass environment variables through ibrun , list the key/value pairs before the ibrun command . Ideally , the product of the MPI process count and the threads per process should equal the number of physical cores on the nodes @ @ @ @ @ @ @ @ @ @ MPI process per node with the threads per node equal to the number of physical cores per node , but as the examples below show this is not required . <p> The Gordon network topology is a 4x4x4 3D torus of switches with 16 compute nodes attached to each switch . For applications where the locality of the compute nodes is important , the user can control the layout of the job . Note that requesting a smaller number of maximum switch hops rather than relying on the default behavior may increase the length of time that a job waits in the queue . The maxhops option is specified in the job script as follows : <p> #PBS -v Catalinamaxhops=HOPS <p> Where HOPS is set to the integer values 0-3 or the string None . The behavior is described below : <p> 0 : only nodes connected to the same switch will be used 1 : only nodes connected by at most one inter-switch link will be used 2 : only nodes connected by at most two inter-switch links will be used 3 : only nodes connected by at most @ @ @ @ @ @ @ @ @ @ may be used , regardless of switch proximity <p> To get nodes spanning the minimum number of hops for jobs of different sizes , use the following guidelines : <p> If you do not specify a Catalinamaxhops value for jobs larger than 16 nodes , your job will use the default of Catalinamaxhops=None and scatter your job across all of Gordon 's 4x4x4 torus . The maximum distance between any two node pairs is 6 hops . <p> Note and hints <p> Try to provide a realistic upper estimate for the wall time required by your job . This will often improve your turnaround time , especially on a heavily loaded machine . <h> Storage Overview <h> SSD Scratch Space <p> Both the native ( non-vSMP ) compute nodes and the vSMP nodes on Gordon have access to fast flash storage . Most of the native compute nodes mount a single 300 GB SSD ( 280 GB usable space ) , but some of the native nodes , labeled with the " bigflash " property , have access to a 4.4 TB SSD filesystem . The vSMP nodes have access @ @ @ @ @ @ @ @ @ @ sixteen 300 GB SSDs ( 4.4 TB usable space ) . <p> The latency to the SSDs is several orders of magnitude lower than that for spinning disk ( &lt;100 microseconds vs. milliseconds ) making them ideal for user-level check pointing and applications that need fast random I/O to large scratch files . Users can access the SSDs only during job execution under the following directories : <p> Regular native compute nodes : <p> /scratch/$USER/$PBSJOBID <p> Note that the regular compute nodes have 280GB of usable space in the scratch SSD location . For jobs needing larger scratch space , the options are " bigflash " compute nodes and the vSMP nodes . <p> Bigflash native compute nodes : <p> /scratch/$USER/$PBSJOBID <p> Additionally , users will have to change the PBS node request line : <p> #PBS l **30;480;TOOLONG <p> If more than one bigflash node is needed for a single job , the following line also needs to be added since not all bigflash nodes share the same switch : <p> For the vSMP nodes , /scratch is a symbolic link to /scratch1 and is provided to maintain a @ @ @ @ @ @ @ @ @ @ contain flash drives ( Gordon and Trestles ) . <p> These directories are purged immediately after the job terminates , therefore any desired files must be copied to the parallel file system before the end of the job . <p> Noflash Nodes : <p> A small subset of Gordon nodes do not have a flash filesystem . If your job does not require the use of SSDs , you may wish to explicit request the " noflash " property when submitting your job ( e.g. , #PBS -l **29;512;TOOLONG ) to prevent it from having to wait for flash-equipped compute nodes to become available . By default , all non-interactive jobs go to nodes with SSD scratch space , and the noflash nodes are only allocated when explicitly requested . <h> Parallel Lustre File System <p> SDSC provides several Lustre-based parallel file systems . These include a shared Projects Storage area that is mounted on both Gordon and Trestles plus separate scratch file systems . Collectively known as Data Oasis , these file systems are accessed through 64 Object Storage Servers ( OSSs ) and have a capacity of 4 @ @ @ @ @ @ @ @ @ @ has 1.4 PB of capacity and is available to all allocated users of Trestles and Gordon . It is accessible at : <p> LONG ... <p> where &lt;allocation&gt; is your 6-character allocation name , found by running showaccounts . The default allocation is 500 GB of Project Storage to be shared among all users of a project . Projects that require more than 500 GB of Project Storage must request additional space via the XSEDE POPS system in the form of a storage allocation request ( for new allocations ) or supplement ( for existing allocations ) . <p> Project Storage is not backed up and users should ensure that critical data are duplicated elsewhere . Space is provided on a per-project basis and is available for the duration of the associated compute allocation period . Data will be retained for 3 months beyond the end of the project , by which time data must be migrated elsewhere . <p> Lustre Scratch Space <p> The Gordon Scratch space has a capacity of 1.6 PB and is currently configured as follows . <p> **30;543;TOOLONG **32;575;TOOLONG <p> Users can not write directly @ @ @ @ @ @ @ @ @ @ of the two subdirectories listed above . <p> The former is created at the start of a job and should be used for applications that require a shared scratch space or need more storage than can be provided by the flash disks . Because users can only access this scratch space after the job starts , executables and other input data must be copied here from the user 's home directory from within a job 's batch script . Unlike SSD scratch storage , the data stored in Lustre scratch is not purged immediately after job completion . Instead , users will have time after the job completes to copy back data they wish to retain to their projects directories , home directories , or to their home institution . <p> The latter is intended for medium-term storage outside of running jobs , but is subject to purge with a minimum of five days notice if it begins to approach capacity . <p> Note that both directories ( $PBSJOBID and tempproject ) are part of the same file system and therefore served by the same set of OSSs . User @ @ @ @ @ @ @ @ @ @ same performance . To avoid the overhead of unnecessary data movement , read directly from tempproject rather than copying to the $PBSJOBID directory . Similarly , files that should be retained after completion of a job should be written directly to tempproject . <h> Home File System <p> After logging in , users are placed in their home directory , /home , also referenced by the environment variable $HOME . The home directory is limited in space and should be used only for source code storage . Jobs should never be run from the home file system , as it is not set up for high performance throughput . Users should keep usage on $HOME under 100GB . Backups are currently being stored on a rolling 8-week period . In case of file corruption/data loss , please contact us at help@xsede.org to retrieve the requested files . <h> Using the vSMP Nodes <p> Most applications will be run on the regular compute nodes , but in certain instances you will want to use the large-memory vSMP nodes : <p> Serial or threaded applications requiring more than 64 GB of memory @ @ @ @ @ @ @ @ @ @ <p> Applications that can take advantage of a RAM file system ( ramfs ) <p> Before reading this section , we suggest that you first become familiar with the user guide sections that cover compiling and running jobs . <p> Compilation instructions are the same for the regular compute nodes and the vSMP nodes , although ScaleMPs specially tuned version of MPICH2 should be used when building MPI applications . <p> Jobs are submitted to the vSMP nodes using TORQUE and , with the exception of specifying a different queue name , all of the instructions given in the user guide sections describing the batch submission system still apply . In the remainder of this section , we describe the additional steps needed to make effective use of the vSMP nodes . Note that we do not provide complete scripts below , but rather just the critical content needed to run under vSMP . <p> Memory and core usage <p> Since multiple user jobs can be run simultaneously on a single vSMP node , there has to be a mechanism in place to ensure that jobs do not use more @ @ @ @ @ @ @ @ @ @ , jobs should run on distinct sets of physical nodes to prevent contention for resources . Our approach is to implement the following policies : <p> Cores are requested in units of 16 <p> Memory is allocated in proportion to number of cores requested <p> Each physical node has 64 GB of memory , but after accounting for the vSMP overhead the amount available for user jobs is closer to 60 GB . A serial job requiring 120 GB of memory should request 32 cores . <p> #PBS -l nodes=1:ppn=32:vsmp <p> OpenMP jobs <p> The example below shows a partial batch script for an OpenMP job that will use 16 threads . Note that the queue name is set to vsmp . <p> The first export statement preloads the vSMP libraries needed for an application to run on the vSMP nodes . The second export statement is not strictly required , but is suggested since using the Hoard library can lead to improved performance particularly when multiple threads participate in dynamic memory management . The third export statement prepends the PATH variable with the location of the numabind command , @ @ @ @ @ @ @ @ @ @ <p> The final export statement requires a more in depth explanation and contains the " magic " for running on a vSMP node . Setting the KMPAFFINITY variable determines how the threads will be mapped to compute cores . Note that it is used only for OpenMP jobs and will not affect the behavior of pThreads codes . <p> The assignment maps the threads compactly to cores , as opposed to spreading out across cores , provides verbose output ( will appear in the jobs stderr file ) , and starts the mapping with the first core in the set . The numabind statement looks for an optimal set of 16 contiguous cores , where optimal typically means that the cores span the smallest number of physical nodes and have the lightest load . Enclosing within the back ticks causes the numabind output to appear in place as the final argument in the definition . <p> For most users , this KMPAFFINITY assignment will be adequate , but be aware that there are many options for controlling the placement of OpenMP threads . For example , to run on 16 @ @ @ @ @ @ @ @ @ @ set the ppn and offset values to 32 , the number of threads to 16 , and the affinity type to scatter . Relevant lines are shown below . <p> As mentioned in the previous section , setting the KMPAFFINITY variable has no effect on pThreads jobs . Instead you will need to create an additional configuration file with all of the contents appearing on a single line . It is absolutely critical that the executable name assigned to the pattern is the same as that used in the script . <p> Serial job submission is very similar to that for OpenMP jobs , except that the taskset command is used together with numabind to bind the process to a core . Even though a single core will be used for the computations , remember to request a sufficient number of cores to obtain the memory required ( 60 GB per 16 cores ) . <p> If you require less than 64 GB of memory per MPI process , we suggest that you run on the regular compute nodes . While ScaleMP does provide a specially tuned version of the @ @ @ @ @ @ @ @ @ @ the vSMP foundation software does add a small amount of unavoidable communications latency ( applies not just to vSMP , but any hypervisor layer ) . <p> Before compiling your MPI executable to run under vSMP , unload the mvapich module and replace with mpich2 <p> $ module swap mvapich2ib mpich2ib <p> To run an MPI job , where the number of processes equals the number of compute cores requested , use the following options . Note that VSMPPLACEMENT is set to PACKED , indicating that the MPI processes will be mapped to a set of contiguous compute cores . <p> The situation is slightly more complicated if the number of MPI processes is smaller than the number of requested cores . Typically you will want to spread the MPI processes evenly across the requested cores ( e.g. when running 2 MPI processes across 64 cores , the processes will be mapped to cores 1 and 33 ) . In the example below the placement has been changed from " PACKED " to " SPREAD232 " . The SPREAD keyword indicates that the processes should be spread across the cores @ @ @ @ @ @ @ @ @ @ 2 nodes with 32 cores per node . <h> Torque in Depth <p> This section takes a closer look at the some of the more useful TORQUE commands . Basic information on job submission can be found in the User Guide section Running Jobs . For a comprehensive treatment , please see the official TORQUE documentation and the man pages for qstat , qmgr , qalter , and pbsnodes . We also describe several commands that are specific to the Catalina scheduler and not part of the TORQUE distribution . <h> Listing jobs ( qstat -a ) <p> Running the qstat command without any arguments shows the status of all batch jobs . The a flag is suggested since this provides additional information such as the number of nodes being used and the required time . The following output from qstat a has been edited slightly for clarity . <p> The output is mostly self-explanatory , but a few points are worth mentioning . The Job I 'd listed in the first column will be needed if you want to alter , delete , or obtain more information about a job @ @ @ @ @ @ @ @ @ @ the Job I 'd is needed . The queue , number of nodes , wall time , and required memory specified in your batch script are also listed . For jobs that have started running , the elapsed time is shown . The column labeled " S " lists the job status . <p> R = running Q = queued H = held C = completed after having run E = exiting after having run <p> Jobs can be put into a held state for a number of reasons including job dependencies ( e.g. task2 can not start until task1 completes ) or a user exceeding the number of jobs that can be in a queued state . <p> On a busy system , the qstat output can get to be quite long . To limit the output to just your own jobs , use the u $USER option <h> Detailed information for a job ( qstat -f jobid ) <p> Running qstat f jobid provides the full status for a job . In addition to the basic information listed by qstat a , this includes the jobs start time , @ @ @ @ @ @ @ @ @ @ and account being charged . <h> Nodes allocated to a job ( qstat -n jobid ) <p> To see the list of nodes allocated to a job , use qstat n . Note that this output does n't  reflect actual usage , but rather the resources that had been requested . Knowing where your job is running is valuable information since you 'll be able to access those nodes for the duration of your job to monitor processes , threads , and resource utilization . <h> Altering job properties ( qalter ) <p> The qalter command can be used to modify the properties of a job . Note that the modifiable attributes will depend on the job state ( e.g. number of nodes requested can not be changed after a job starts running ) . See the qalter man page for more details . <h> Specifying job dependencies <p> In some cases , you may wish to submit multiple jobs that have dependencies . For example , a number of mutually independent tasks must be completed before a final application is run . This can be easily accomplished using the depend @ @ @ @ @ @ @ @ @ @ job described in pbsscriptd is not allowed to start until jobs 273 , 274 and 275 have terminated , with or without errors . <p> qsub W **27;609;TOOLONG pbsscriptd <p> While this is useful , it requires the user to manually submit the jobs and construct the job list . To easily automate this process , capture the jobids using back ticks when the earlier jobs are submitted ( recall that qsub writes the jobid to stdout ) . Using the previous example and assuming that the earlier jobs were launched using the scripts pbsscriptabc , we can do the following : <p> This is only a very brief introduction to controlling job dependencies . The depend attribute provides a rich variety of options to launch jobs before or after other jobs have started or terminated in a specific way . For more details , see the additional attributes section of the qsub User Guide . <h> Node attributes ( pbsnodes -a ) <p> The full set of attributes for the nodes can be listed using pbsnodes a . To limit output to a single node , provide the node @ @ @ @ @ @ @ @ @ @ Running pbsnodes l lists nodes that are unavailable to the batch systems ( e.g. have a status of " down " , " offline " , or " unknown " ) . To see the status of all nodes , use pbsnodes l all . Note that nodes with a " free " status are not necessarily available to run jobs . This status applies both to nodes that are idle and to nodes that are running jobs using a ppn value smaller than the number of physical cores . <h> Monitoring Your Job <p> In this section , we describe some standard tools that you can use to monitor your batch jobs . We suggest that you at least familiarize yourself with the section of the user guide that deals with running jobs to get a deeper understanding of the batch queuing system before starting this section . <p> Figuring out where your is job running <p> Using the qstat n jobid command , you 'll be able to see a list of nodes allocated to your job . Note that this output does n't  reflect actual usage , but @ @ @ @ @ @ @ @ @ @ your job is running is valuable information since you 'll be able to access those nodes for the duration of your job to monitor processes , threads , and resource utilization . <p> The output from qstat n shown below lists the nodes that have been allocated for job 362256 . Note that we have 16 replicates of the node gcn-5-22 , each followed by a number corresponding to a compute core on the node . <p> Under normal circumstances , users can not login directly to the compute nodes . This is done to prevent users from bypassing the batch scheduler and possibly interfering with other jobs that may be running on the nodes . <p> $ ssh gcn-5-22 Connection closed by 10.1.254.187 <p> Access is controlled by the contents of the **25;638;TOOLONG file , which for idle nodes denies access to everyone except root and other administrative users . <p> $ cat **25;665;TOOLONG -:ALL EXCEPT root diag : ALL <p> The situation is different though once a batch job transitions from the queued state to the run state . A special script known as the prologue makes a number @ @ @ @ @ @ @ @ @ @ access.conf file so that the owner of the job running on the node can login . <p> $ cat **25;692;TOOLONG -:ALL EXCEPT root diag username:ALL <p> Once the job terminates , a corresponding script known as the epilogue undoes the changes made by the prologue . If you happen to be logged in to a compute node when your job terminates , your session will be terminated . <p> Static view of threads and processes with ps <p> Once you login into a compute node , the ps command can provide information about current processes . This is useful if you want to confirm that you are running the expected number of MPI processes , that all MPI processes are in the run state , and that processes are consuming relatively equal amounts of CPU time . The ps command has a large number of options , but two that are suggested are l ( long output ) and u user ( restrict output to processes owned by username or user I 'd ) . <p> In the following example , we see that the user is running 16 MPI processes @ @ @ @ @ @ @ @ @ @ compute node gcn-5-22 . The TIME column shows that the processes are all using very nearly the same amount of CPU time and the S ( state ) column shows that these are all in the run ( R ) state . Not all applications will exhibit such ideal load balancing and processes may sometimes go into a sleep state ( D or S ) while performing I/O or waiting for an event to complete . <p> The ps command with the m ( not m ) option gives thread-level information , with the summed CPU usage for the threads in each process followed by per-thread usage . In the examples below , we first run ps to see the five MPI processes , then use the m option to see the six threads per process . <p> While ps gives a static view of the processes , top can be used to provide a dynamic view . By default the output of top is updated every three seconds and lists processes in order of decreasing CPU utilization . Another advantage of top is that it displays the per-process memory @ @ @ @ @ @ @ @ @ @ your job is not exceeding the available physical memory on a node . <p> In the first example below , the CPU usage is around 600% for all five processes , indicating that each process had spawned multiple threads . We can confirm this by enabling top to display thread-level information with the H option . ( This option can either be specified on the top command line or entered after top is already running ) <p> We have not covered all of the options and capabilities of qstat , ps , and top . For more information consult the man pages for these commands . <h> Dedicated I/O Nodes ( Gordon ION ) <p> A limited number of Gordons 64 flash-based I/O nodes are available as dedicated resources . The PI and designated users of a dedicated I/O node have exclusive use of the resource at any time for the duration of the award without having to go through the batch scheduler . Any data stored on the nodes is persistent ( but not backed up ) and can only be removed by the users of the project . @ @ @ @ @ @ @ @ @ @ " and must be requested separately from the regular compute cluster . <p> Each I/O node contains two 6-core 2.66 GHz Intel Westmere processors , 48 GB of DDR3-1333 memory and sixteen 300 GB Intel 710-series solid-state drives . The drives will typically be configured as a single RAID 0 device with 4.4 TB of usable space . However , this default configuration can be changed based on the needs of the project and in collaboration with SDSC staff . Each of the I/O nodes can mount both the NFS home directories and Data Oasis , SDSCs 4 PB Lustre-based parallel file system , via two 10 GbE connections . The aggregate sequential bandwidth for the 16 SSDs within an I/O node was measured to be 4.3 GB/s and 3.4 GB/s for reads and writes , respectively . The corresponding random performance is 600 KIOPS for reads and 37 KIOPS for writes . <p> Recommended Use <p> Gordon ION awards are intended for projects that can specifically benefit from persistent , dedicated access to the flash storage . Applications that only need temporary access to the flash storage ( e.g. @ @ @ @ @ @ @ @ @ @ be accessed multiple times within a batch job ) can do so via a regular Gordon compute cluster award . <p> Gordon ION is particularly well suited to database and data-mining applications where high levels of I/O concurrency exist , or where the I/O is dominated by random access data patterns . The resource should be of interest to those who , for example , want to provide a high-performance query engine for scientific or other community databases . Consequently , Gordon ION allocations are for long-term , exclusive , and dedicated use of the awardee . <p> SDSC staff can help you configure the I/O node based on your requirements . This includes the installation of relational databases ( MySQL , PostgreSQL , DB2 ) and Apache web services . <p> Allocations <p> Dedicated Gordon I/O nodes are only available through the XSEDE startup allocations process . Normally a single I/O node will be awarded , but two I/O nodes can be awarded for exceptionally well-justified requests . Projects that also have accompanying heavy computational requirements can ask for up to 16 compute nodes for each I/O node . @ @ @ @ @ @ @ @ @ @ use of the I/O nodes . This should include relevant benchmarks on spinning disks , with projections of how the applications will scale when using flash drives . Additionally , the request should include a strong justification for why these should be provided as a dedicated resourcefor example , providing long-term access to data for a large community via a Science Gateway <p> Gordons flash-based I/O nodes are a unique resource in XSEDE and we encourage you to contact SDSC staff to discuss the use of this resource before you submit your allocation request . SDSC applications staff will be able to help you understand if this resource is a good match for your project , and subsequently , provide assistance in getting started with your project . 
@@97506127 @1706127/ <h> MyPDB Login <h> Visualize Options <h> NGL <p> NGL is a fast &amp; interactive web-based tool for 3D visualization of large structures . For X-ray structures , crystallographic unit cells can be displayed , as well as " supercells " comprising a set of adjacent unit cells . <h> Hemoglobin <h> Zika Virus <h> Unitcell Representation <h> Supercell Representation <h> Jsmol and Jmol <p> JSmol , the JavaScript version of Jmol offers several options for display and analysis . Launch the viewer from any entry 's Structure Summary page or by entering an I 'd below . The Jmol Applet remains available from the 3D View page , and will continue to work in most browsers . Learn more about Jsmol/Jmol . <h> Protein Feature View <p> Provides a graphical summary of a full-length protein sequence from UniProt and how it corresponds to PDB entries . It also loads annotations from external databases ( such as Pfam ) and homology models information from the Protein Model Portal . Annotations visualizing predicted regions of protein disorder and hydrophobic regions are displayed . <h> Human Gene View <p> Illustrates the correspondences @ @ @ @ @ @ @ @ @ @ genes have been mapped to representative PDB structure protein chains ( selected from sequence clusters at 40% sequence identity ) to show which regions of a gene are available in PDB coordinates . 
@@97506128 @1706128/ <h> Welcome to Hi-C project at Ren Lab ! <p> Here you can download various aspects of the data , including the interaction matrices for each chromosome , and the positions of the domains identified by our analysis . To download the raw sequencing data and the mapped reads , please visit the GEO Database under the GEO accession number GSE35156 <p> We have interaction matrices for each of the four cell types analysis ( mouse ES cell , mouse cortex , human ES cell ( H1 ) , and IMR90 fibroblasts ) . The interaction matrices are created using either a 40kb bin size throughout the genome . Here , you can download both the raw interaction matrices and the normalized matrices ( normalized according to the method described by Yaffe and Tanay ) . Each of the datasets can downloaded here as a tarball containing 40kb raw(uij's) or **25;719;TOOLONG interaction matrices . We only have the intrachromosomal interaction matrices available here . <p> We also have the locations of each of the topological domains that we call in the data . The method used for called the @ @ @ @ @ @ @ @ @ @ for each dataset are provided as bed files , with each line containing the chromosome , domain start , and domain end . The domains were called using the normalized data with a bin size of 40kb , so each domain start and end will be a multiple of 40000. 
@@97506129 @1706129/ <h> Born : London , England , December 10 , 1815 <h> Died : London , England , November 27 , 1852 <h> Analyst , Metaphysician , and Founder of Scientific Computing <p> Ada Byron was the daughter of a brief marriage between the Romantic poet Lord Byron and Anne Isabelle Milbanke , who separated from Byron just a month after Ada was born . Four months later , Byron left England forever . Ada never met her father ( who died in Greece in 1823 ) and was raised by her mother , Lady Byron . Her life was an apotheosis of struggle between emotion and reason , subjectivism and objectivism , poetics and mathematics , i 'll health and bursts of energy . <p> Lady Byron wished her daughter to be unlike her poetical father , and she saw to it that Ada received tutoring in mathematics and music , as disciplines to counter dangerous poetic tendencies . But Ada 's complex inheritance became apparent as early as 1828 , when she produced the design for a flying machine . It was mathematics that gave her life its @ @ @ @ @ @ @ @ @ @ elite London society , one in which gentlemen not members of the clergy or occupied with politics or the affairs of a regiment were quite likely to spend their time and fortunes pursuing botany , geology , or astronomy . In the early nineteenth century there were no " professional " scientists ( indeed , the word " scientist " was only coined by William Whewell in 1836 ) --but the participation of noblewomen in intellectual pursuits was not widely encouraged . <p> One of the gentlemanly scientists of the era was to become Ada 's lifelong friend . Charles Babbage , Lucasian professor of mathematics at Cambridge , was known as the inventor of the Difference Engine , an elaborate calculating machine that operated by the method of finite differences . Ada met Babbage in 1833 , when she was just 17 , and they began a voluminous correspondence on the topics of mathematics , logic , and ultimately all subjects . <p> In 1835 , Ada married William King , ten years her senior , and when King inherited a noble title in 1838 , they became the @ @ @ @ @ @ @ @ @ @ . The family and its fortunes were very much directed by Lady Byron , whose domineering was rarely opposed by King . <p> Babbage had made plans in 1834 for a new kind of calculating machine ( although the Difference Engine was not finished ) , an Analytical Engine . His Parliamentary sponsors refused to support a second machine with the first unfinished , but Babbage found sympathy for his new project abroad . In 1842 , an Italian mathematician , Louis Menebrea , published a memoir in French on the subject of the Analytical Engine . Babbage enlisted Ada as translator for the memoir , and during a nine-month period in 1842-43 , she worked feverishly on the article and a set of Notes she appended to it . These are the source of her enduring fame . <p> Ada called herself " an Analyst ( &amp; Metaphysician ) , " and the combination was put to use in the Notes . She understood the plans for the device as well as Babbage but was better at articulating its promise . She rightly saw it as what we would @ @ @ @ @ @ @ @ @ @ developping sic and tabulating any function whatever . . . the engine is the material expression of any indefinite function of any degree of generality and complexity . " Her Notes anticipate future developments , including computer-generated music . <p> Ada died of cancer in 1852 , at the age of 37 , and was buried beside the father she never knew . Her contributions to science were resurrected only recently , but many new biographies* attest to the fascination of Babbage 's " Enchantress of Numbers . " 
@@97506130 @1706130/ <h> Born : Dover , Delaware , December 11 , 1863 <h> Died : Cambridge , Massachusetts , April 13 , 1941 <h> Theorist of Star Spectra <p> Oh , Be A Fine Girl--Kiss Me ! This phrase has helped several generations of astronomers to learn the spectral classifications of stars . Ironically , this mnemonic device , still used today , refers to a scheme developed by a woman . <p> Annie Jump Cannon was the eldest of three daughters of Wilson Cannon , a Delaware shipbuilder and state senator , and his second wife , Mary Jump . Annie 's mother taught her the constellations and stimulated her interest in astronomy . At Wellesley , Annie studied physics and astronomy and learned to make spectroscopic measurements . On her graduation in 1884 , she returned to Delaware for a decade , but became impatient to get back to astronomy . After the death of her mother in 1894 , Cannon worked at Wellesley as a junior physics teacher and became a " special student " of astronomy at Radcliffe . <p> In 1896 , she became a @ @ @ @ @ @ @ @ @ @ " Pickering 's Women , " women hired by Harvard College Observatory director Edward Pickering to reduce data and carry out astronomical calculations . Pickering 's approach to science was thoroughly Baconian : " the first step is to accumulate the facts . " * The accumulating was supported by a fund set up in 1886 by Anna Draper , widow of Henry Draper , a wealthy physician and amateur astronomer . <p> Pickering conceived the Henry Draper Memorial as a long-term project to obtain optical spectra of as many stars as possible and to index and classify stars by their spectra . While the measurements were difficult enough , the development of a reasonable classification scheme proved as much a problem in " theory " ( which Pickering was slow to recognize ) as " fact accumulation . " <p> The analysis was begun in 1886 by Nettie Farrar , who left after a few months to be married . Her place was taken by Williamina Fleming , the first of Pickering 's female crew to be recognized in the astronomical community at large . Fleming examined the spectra @ @ @ @ @ @ @ @ @ @ containing 22 classes . The work was carried further by Antonia Maury , who developed her own classification system . The system was cumbersome by comparison with Fleming 's , and Pickering could not sympathize with Maury 's insistence on theoretical ( what we would today call astrophysical ) concerns that underlay her scheme . <p> It was left to Annie Jump Cannon to continue , beginning with an examination of bright southern hemisphere stars . To these she applied yet a third scheme , derived from Fleming 's and Maury 's , an " arbitrary " division of stars into the spectral classes O , B , A , F , G , K , M , and so on . It was as " theory-laden " as Maury 's ordering , but greatly simplified . Her " eye " for stellar spectra was phenomenal , and her Draper catalogs ( which ultimately listed nearly 400,000 stars ) were valued as the work of a single observer . <p> Cannon also published catalogs of variable stars ( including 300 she discovered ) . Her career spanned more than forty @ @ @ @ @ @ @ @ @ @ . She received many " firsts " ( first recipient of an honorary doctorate from Oxford , first woman elected an officer of the American Astronomical Society , etc . ) . At Harvard she was named Curator of Astronomical Photographs , but it was only in 1938 , two years before her retirement , that she obtained a regular Harvard appointment as William C. Bond Astronomer . <p> * Quoted by Pamela Mack in her article , " Straying from their orbits : Women in astronomy in America , " in G. Kass-Simon , P. Farnes , and D. Nash , 1990 : Women of Science : Righting the Record ( Bloomington , Indiana University Press ) , p. 91. 
@@97506133 @1706133/ <h> What is happening to coastal groundwater ? <p> As a Delawarean , it is nearly tradition for friends and colleagues to spend their summer weekends and vacation at the beach , to enjoy the toasty sun and warm ocean water . It is a dream for many people to have the ocean as a neighbor , living on a calm sandy beach or rocky shoreline . Unfortunately for many parts of the United States , seawater is becoming an uninvited guest , intruding on belowground water sources many people depend on . It is unclear if this problem will spread or can be prevented . So , is it worth investing large amounts of federal and state funds for well installation and instrument measurement ? What is happening to coastal groundwater ? <p> To guarantee continuous fresh tap water , it is important for engineering and geoscientists to monitor the intrusion of seawater . The Delaware Geological Survey has developed an observation network generating approximately 52 , 600 daily temperature and 5700 specific conductance records , which monitors groundwater quality . Delawares groundwater resource is threatened by increasing @ @ @ @ @ @ @ @ @ @ in Delaware are vulnerable to salinization due to sea-level rise and ocean surges . Increasing evidences have shown that saltwater has crept into groundwater supplies along Delaware coast . <p> Not only in Delaware , many coastal groundwater resources around the world are adversely affected by sea-level rise and increases in the frequency and intensity of ocean surges . Both lateral seawater intrusion ( caused by sea-level rise ) and vertical infiltration ( caused by ocean surges ) could introduce significant amount of saltwater into freshwater aquifers . <p> As an example , many supply wells in Cape May , NJ , have been abandoned due to excessive chloride contamination from seawater ( Chesler , 2005 ) . At Cape Canaveral Barrier Island Complex in Florida , model simulations have shown that 18 69 % of aquifers will be salinized by seawater in 2050 due to sea-level rise ( Xiao et al . 2016 ) . Thinking globally , this is also a growing problem in Asia . A studying using field measurements and model simulations suggested that inundated salt water by the Supertyphoons could contaminate a major aquifer in @ @ @ @ @ @ @ @ @ @ Cardenas et al. , 2015 ) . This could be major problem for coastal population , who rely on groundwater source for drinking , cleaning , and cooking ! <p> Figure 1 : Illustration of Coastal Critical Zone processes <p> Movement of water near the grounds surface will affect other coastal Critical Zone processes . For example , infiltrating seawater raises the chances of chemical exchange between land and ocean . Trace elements present in the soil may be mobilized to the surface due to rising water table or be released to the ocean . To ensure this process does not and cause a public health emergency , scientists monitor the chemical parameters and composition of groundwater . <p> In addition to people , the plants living in coastal environment can also be affected . The vegetated coastal Critical Zone plays an important role in carbon dynamics and erosion control ( Leonardi et al. , 2015 ) . Salt marsh vegetation fixes atmospheric carbon , and a portion of that carbon is eventually buried in underlying sediments . Unfortunately , salt marshes are globally in decline , and their carbon @ @ @ @ @ @ @ @ @ @ vegetation can enhance erosion and further contribute to climate change ( Mudd et al. , 2009 ) . Coastal Critical Zone scientists have invested efforts to understand physical , chemical , and biological processes to better predict the impacts of environmental change on the coastal Critical Zone ( Sawyer et al. , 2016 ) . In our recent study found that coastal landscape ( i.g. ponds , dunes , barrier islands , and channels ) has a strong impact on overwash and salinization processes , which should be considered in coastal land management practices ( Yu et al. , 2016 ) . In near future , advance models and field monitoring network should be developed to improve our ability to predict , mitigate , and prevent the adverse effects of climate change on coastal groundwater quality . <p> You can learn more about Coastal Critical Zone processes at LONG ... <h> COMMENT ON " Adventures in the Critical Zone " <p> All comments are moderated . If you want to comment without logging in , select either the " Start/Join the discussion " box or a " Reply " link @ @ @ @ @ @ @ @ @ @ I 'd rather post as a guest " checkbox . <h> ABOUT THIS BLOG <p> General Disclaimer : Any opinions , findings , conclusions or recommendations presented in the above blog post are only those of the blog author and do not necessarily reflect the views of the U.S. CZO National Program or the National Science Foundation . For official information about NSF , visit www.nsf.gov. 
@@97506137 @1706137/ 1450 @qwx861450 <h> Teaching Computers to Recognize Unhealthy Guts <h> SDSCs Gordon Supercomputer Assists in New Microbiome Study <p> A visualization of protein families is projected onto researchers Mehrdad Yazdani and Bryn Taylor in the UC San Diego Center for Microbiome Innovation . Selfie by Bryn Taylor . <p> By Tiffany Fox <p> A new proof-of-concept study by researchers from the University of California San Diego has succeeded in training computers to " learn " what a healthy versus an unhealthy gut microbiome looks like based on its genetic makeup . Since this can be done by genetically sequencing fecal samples , the research suggests there is great promise for new diagnostic tools that are , unlike blood draws , non-invasive . <p> As recent advances in scientific understanding of Parkinsons disease and cancer immunotherapy have shown , our gut microbiomes the trillions of bacteria , viruses and other microbes that live within us are emerging as one of the richest untapped sources of insight into human health . <p> The problem is these microbes live in a very dense ecology of up to one billion microbes per gram @ @ @ @ @ @ @ @ @ @ all the different animals and plants in a complex ecology like a rain forest or coral reef and then imagine trying to do this in the gut microbiome , where each creature is microscopic and identified by its DNA sequence . <p> Determining the state of that ecology is a classic Big Data problem , where the data is provided by a powerful combination of genetic sequencing techniques and supercomputing software tools . The challenge then becomes how to mine this data to obtain new insights into the causes of diseases , as well as novel therapies to treat them . <p> The new paper , titled " Using Machine Learning to Identify Major Shifts in Human Gut Microbiome Protein Family Abundance in Disease , " was presented last month at the IEEE International Conference on Big Data . It was written by a joint research team from UC San Diego and the J. Craig Venter Institute ( JCVI ) . At UC San Diego , it included Mehrdad Yazdani , a machine learning and data scientist at the California Institute for Telecommunications and Information Technologys ( Calit2 ) Qualcomm @ @ @ @ @ @ @ @ @ @ Pediatrics Postdoctoral Scholar Justine Debelius ; Rob Knight , a professor in the UC San Diego School of Medicine 's Pediatrics Department as well as the Computer Science and Engineering Department and director of the Center for Microbiome Innovation ; and Larry Smarr , Director of Calit2 and a professor of Computer Science and Engineering . The UC San Diego team also collaborated with Weizhong Li , an associate professor at JCVI . <p> Metagenomics and Machine Learning <p> The software to carry out the study was developed by Li and run on the data-intensive Gordon supercomputer at the San Diego Supercomputer Center ( SDSC ) , an Organized Research Unit of UC San Diego , using 180,000 core-hours . That 's equivalent to running a personal computer 24 hours a day for about 20 years . <p> The work began with a genetic sequencing technique known as " metagenomics , " which breaks up the DNA of the hundreds of species of microbes that live in the human large intestine ( our " gut " ) . The technique was applied to 30 healthy people ( using sequencing data from @ @ @ @ @ @ @ @ @ @ together with 30 samples from people suffering from the autoimmune Inflammatory Bowel Disease ( IBD ) , including those with ulcerative colitis and with ileal or colonic Crohns disease . This resulted in sequencing around 600 billion DNA bases , which were then fed into the Gordon supercomputer to reconstruct the relative abundance of these species ; for instance , how many E. coli are present compared to other bacterial species . <p> Since each bacteriums genome contains thousands of genes and each gene can express a protein , this technique made it possible to translate the reconstructed DNA of the microbial community into hundreds of thousands of proteins , which are then grouped into about 10,000 protein families . <p> To discover the patterns hidden in this huge pile of numbers , the researchers harnessed what they refer to as " fairly out-of-the-bag " machine-learning techniques originally developed for spam filters and other data mining applications . Their goal was to use these algorithms to classify major changes in the protein families found in the gut bacteria of both healthy subjects and those with IBD , based on the @ @ @ @ @ @ @ @ @ @ first used standard biostatistics routines to identify the 100 most statistically significant protein families that differentiate health and disease states . These 100 protein families were then used as a " training set " to build a machine learning classifier that could classify the remaining 9,900 protein families in diseased versus healthy states . The goal was to find a " signature " for which protein families were elevated or suppressed in disease versus healthy states . <p> In this figure from the paper , some representative protein families are on the left-hand side of each graph . Each colored dot represents the abundance of that protein family ( measured on a logarithmic scale ) for a particular fecal sample taken from a healthy person ( green dots ) or a person with IBD ( red , blue , purple dots ) . Selected for the top two graphs was a " training set " of 100 protein families that statistically differentiated healthy from disease states , which shows that for patients with IBD , certain protein families are either over-abundant ( first graph ) or under-abundant ( second graph @ @ @ @ @ @ @ @ @ @ are the results from the machine learning algorithm , which discovered the protein families that had similar patterns in the remaining 9,900 protein families . Note that since the scale is logarithmic , these differences in abundance are often 100 to 1 or more . <p> The process is akin to training a computer to recognize the different flavors of fruit juices something a human toddler could do intuitively , albeit from a limited perspective . <p> " From your past experiences drinking juice , you know the difference between orange , apple , and cranberry juice , " Taylor noted . " Your future decision about what juice you are drinking will be based on your past preferences . But its really hard to figure out what apple juice tastes like without experiencing it first . " <p> They have to train the computer , in other words , to recognize what apple juice tastes like or in this case , what a " healthy " microbiome looks like by clustering data according to bacteria . <p> " You can try to categorize healthy and sick people by looking @ @ @ @ @ @ @ @ @ @ " but the differences are not always clear . Instead , when we categorize by the bacterial protein family levels , we see a distinct difference between healthy and sick people . This is because proteins are the workhorses of biology , and by analyzing the proteins produced by these bacteria , we can get an idea of what the bacteria are doing in your gut . " <p> The machine-learning approach is effective , notes Yazdani , precisely because its statistically based . " The rules are not set in stone , " he said . " What you need is past data and past experiences from patients , and then based on statistics or distribution you make your decisions . You let the data speak for itself . " <p> Since Smarr suffers from Crohns disease , he has been working with Knights Center for Microbiome Innovation to advance research in this area . " Because of the exponential increase in the data on your daily changing gut microbiome , it will be essential to develop new machine-learning approaches to bring the biomedically important facets to light , @ @ @ @ @ @ @ @ @ @ researchers hope to expand their analysis , using SDSCs new Comet supercomputer , from 10,000 protein families to one million individual genes , each of which codes for a protein which can be expressed in the gut microbiome . " Scalable methods for quickly identifying such anomalies between health and disease states will be increasingly valuable for biological interpretation of sequence data , " researchers wrote in the paper . <p> Following peer review , the paper was one of only 20 percent of the 423 submissions that were accepted as regular papers for the 2016 IEEE International Conference on Big Data . The conference was held in Washington , D.C. on Dec. 5-8 , 2016 . Yazdani and Taylor attended and presented the paper . <p> About SDSC <p> As an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community , including industry and academia . Cyberinfrastructure refers to an accessible , integrated network of computer-based resources and expertise , focused on , and health IT . SDSCs Comet joins the Centers data-intensive Gordon cluster , and are both part of the National Science Foundations XSEDE ( Extreme Science and Engineering Discovery Environment ) program . <h> Media Contacts <p> Tiffany FoxQualcomm Institute at the UC San Diego division of Calit2 ( 858 ) 246-0353 
@@97506138 @1706138/ <h> 3D View ( JSmol and Jmol ) <p> The 3D View page utilizes Jsmol ( the JavaScript version of Jmol , an open-source Java viewer for 3D chemical structures ) for display and analysis . Options are available to render structures in different styles and colors . The Jmol Applet remains available from the 3D View page , and will continue to work in most browsers . Hover over the red circles in the screenshot below to explore different options . <h> Four Distinct Display Modes <p> Select from four modes of predefined style and color settings for Jmol . The default mode colors residues by ' Secondary Structure ' . The ' Subunit ' mode renders all protein subunits in different colors , and the ' Symmetry ' mode emphasizes the point group symmetry or helical symmetry of a protein complex . The custom view can be used to access additional options for style , color , surface rendering , and more . <h> Secondary Structure <p> Renders structure in Cartoon style and colors by Secondary Structure <h> Subunit <p> Renders structure in Cartoon style and colors @ @ @ @ @ @ @ @ @ @ for visualizing quaternary structure . <h> Symmetry <p> Renders structure in CPK style and colors subunits to emphasize symmetry of a protein complex ( read more about color schemes ) . The structure is enclosed by a polyhedron and symmetry axes are drawn to elucidate the symmetry of a complex . For asymmetric structure , a rectangular box and the three axes of inertia are drawn . <h> Custom View <p> Provides full access to all styles and color schemes , options for surface and hydrogen bond display , change of background color , and toggling on/off autorotation . <h> Interactions <p> Basic Commands <p> Open Jmol Menu <p> Right Click Ctrl + Left Click Left Click on Logo <p> Rotate Around X , Y <p> Left Click and Drag <p> Move along X , Y ( = translate ) <p> Shift + Left Double-Click and Drag Middle Double-Click and Drag Ctrl + Right Click and Drag * Works both when clicking on the molecule or away from it . 
@@97506141 @1706141/ <p> Note if you are looking to replace a FAULTY DRIVE in an existing Tivo , expanded or not , then please see the end of this article . Although I recommend you read through the entire article first to get an understanding of the type of replacement drive needed etc . <p> This approach might seem daunting at first but it is actually a very simple procedure that has been done by thousands of people , many without any major computer experience . So please do n't be put off by the length of this document . Just read it through once , order your replacement drive and then go through the steps for the method that matches your Tivo and computer setup and you 'll have a new expanded tivo running in no time . <p> Unlike a number of sites that discuss upgrades I have personally tried these methods myself without problems on my series 2 , TiVo HD , TiVo Premiere , TiVo Roamio and TiVo BOLT . Many users have also reported success using these methods for TiVo Series 3 and Virgin Media ( @ @ @ @ @ @ @ @ @ @ have a basic knowledge of PC computer hardware and that you are comfortable plugging in hard drives , cdrom drives and/or USB pen drives . You should be comfortable executing a few simple command line instructions in Linux ( or Windows if you choose to use the optional Windows method discussed below ) although all commands will be given to you so no knowledge of Linux itself is required . If you are n't happy making your own hard drive upgrade a number of companies sell preconfigured drives . Several of them are probably featured on the banner ads on this site . <p> If you buy a TiVo Premiere or Roamio HD you can now add support for multi-room and remote viewing , by including the new TiVo Stream that has full iPad , iPhone/iPod and Android remote control and streaming support ( the Bolt , Roamio Plus and Pro include it internally ) . I highly recommend it . <p> While I have used this procedure numerous times without problems to upgrade all versions of TiVo hardware you use these instructions at your own risk ! <p> No @ @ @ @ @ @ @ @ @ @ these instructions or procedures . <p> Opening your TiVo to upgrade the hard disk will likely void your warranty . You have been warned ! However , a note from my own experience : TiVo thankfully do not include warranty void if opened stickers on their DVRs . At least my TiVo Premiere , TiVo HD and TiVo Series 2 did n't have them . The UK Virgin Media TiVo has a sticker over one of the screws but I am reliably informed that this can be easily removed intact ( and later replaced ) via the careful use of a hairdryer and scalpel . Hence if you keep the old drive somewhere safe if anything goes wrong they need never know that you opened your TiVo. : - ) <p> Do NOT touch the power supply - you run the risk of an electric shock . <p> What you need before you get started <p> TiVo Bolt <p> 1 ) A Torx T-10 &amp; T-9 screwdriver . You can find these at Home Depot for about a buck or two or you can order a kit of all sizes @ @ @ @ @ @ @ @ @ @ Disk to use to upgrade your TiVo. ( see below ) . Note unlike all previous TiVo models which use desktop sized 3.5 " hard drives the TiVo BOLT uses a laptop sized 2.5 " hard disk . At present 2TB , 3TB and 4TB hard drives have been tested . Currently the most cost effective approach is to purchase a 2.5 " 4TB external backup drive and remove the case to expose the bare hard drive inside ( although this could change so I recommend looking on amazon.com for 2.5 " laptop hard drives ) . 3.5 " drives also work but you need to mount the drive externally and be prepared to cut a slot in the case for the SATA cable . You will also need access to a Windows PC with SATA capability ( USB SATA adapters should also work but I have not personally tested them ) if you plan to use a drive &gt; 3TB since you will need to reformat the drive using MFSR as described after initially placing it in your Tivo BOLT . I believe a Mac running Windows via Parallels @ @ @ @ @ @ @ @ @ @ have not tested this . There is also a Windows free option specifically for 4TB described later if you do not have access to a Windows system . <p> TiVo Roamio / New UK Virgin Media TiVo ( Samsung ) <p> 1 ) A Torx T-10 screwdriver ( T-8 is required for the base Roamio models ) . You can find these at Home Depot for about a buck or two or you can order a kit of all sizes online from Amazon . For some models you will also need a Torx T-15 screwdriver since these TiVo 's use T-15 screws for the Hard Drive . It has been reported that some UK TiVos have tamper resistant versions of the Torx T-10 screw . These type of screws have a little ' pin ' in the middle of the star . For these you need a Tamper Resistant Torx T-10 screwdriver . Here 's an example from Amazon . <p> 2 ) A suitable Hard Disk to use to upgrade your TiVo. ( see below ) . When considering which drive to purchase note that those labeled OEM are @ @ @ @ @ @ @ @ @ @ you plan to use a drive bigger than 3TB ( currently a max of 8TB is supported ) with a Tivo Roamio you will also need access to a PC with SATA capability ( USB SATA adapters should also work but I have not personally tested them ) to either reformat the drive using MFSR ( Windows ) or ' bless ' it ( Linux 4TB only ) drive as described later . This is not required for drives &lt; 3TB . <p> 1 ) A Torx T-10 screwdriver . You can find these at Home Depot for about a buck or two or you can order online from Amazon . For some TiVo HD 's and Premiere 's you will also need a Torx T-15 screwdriver since these TiVo 's use T-15 screws for the Hard Drive . It has been reported that some UK TiVo 's have tamper resistant versions of the Torx T-10 screw . These type of screws have a little ' pin ' in the middle of the star . For these you need a Tamper Resistant Torx T-10 screwdriver . Here 's an example @ @ @ @ @ @ @ @ @ @ . <p> 2 ) A PC with SATA capability and a CDROM/DVD drive . ( CDROM/DVD drive not necessarily required for the Windows Method or the Premiere USB boot method ) - If you are upgrading a TiVo Series 2 then you will need an 80 pin parallel ATA cable . While the HD inside a series 2 is connected with a 40 pin cable when you plug it into your PC it needs to be connected with an 80 pin IDE cable to work properly . Note if you are not happy reconfiguring the insides of your PC you can also just use two USB adapters . This will mean the process takes slightly longer but it wo n't be a massive difference . You can also get such adapters very cheaply , often less than $10 each . An example that is reported to work great is : SATA/PATA/IDE Drive to USB 2.0 Adapter Converter Cable for 2.5 / 3.5 Inch Hard Drive / Optical Drive with External AC Power Adapter <p> 3 ) A suitable Hard Disk to use to upgrade your TiVo. ( see below ) @ @ @ @ @ @ @ @ @ @ buying an OEM drive . When considering which drive to purchase note that those labeled OEM are often cheaper but do not include a SATA cable while those labeled as boxed or retail should include the SATA cable . Note the maximum hard drive size for a TiVo HD or Series 3 is 2TB and for the Premiere is 4TB although only 2.3TB will actually be used . <p> 4 ) The free upgrade software : <p> TiVo Series 2 / 3 and HD <p> Windows Method : A copy of WinMFS v9.3f or laterYou can download it directly from here 0.1 MB . <p> Linux Boot CD Method : A copy of the MFSLive Boot CD ISO . ( thanks to www.mfslive.org for this awesome software ) Download the ISO and burn it to CD . ( MFSLive v1.4 ISO 7.3 MB ) Remember to choose Burn Image <p> WDIDLE Software : If you are using a Western Digital WDxxEURS drive manufactured before Jan 2013 you will need this for all TiVo versions , otherwise you will just need it for TiVo Series 3 with a Western Digital Drive @ @ @ @ @ @ @ @ @ @ CD. ( wdidle.iso 0.9 MB ) Remember to choose Burn Image - hint : you can also use the USB method described for the Premiere below . <p> TiVo Premiere &amp; Original Cisco Virgin Media ( UK ) TiVo <p> Linux Boot CD Method : A copy of the Premiere JMFS Linux Boot CD ISO . ( thanks to Comer for this Slax based compilation ) Download the ISO and burn it to CD. ( **40;746;TOOLONG 82.3 MB ) Remember to choose Burn Image <p> Linux Bootable USB Pen Drive Method : You need a USB pen drive large enough to hold the ISO image . A 256MB or larger USB pen drive should be fine . Assuming you are using Windows first download the the ISO and save it to your desktop . ( **40;788;TOOLONG 82.3 MB ) . You also need to download a program called Universal USB Installer that we will use to create the bootable USB pen drive . Save this on your desktop as well . ( **31;830;TOOLONG 862 KB ) . <p> Next insert your USB pen drive drive into an available USB slot @ @ @ @ @ @ @ @ @ @ . Select SLAX 6.1.2 as the Linux Distribution under step 1 . Under step 2 browse for the iso file you saved on the desktop above . Finally select the drive letter for your USB pen drive ( Make sure you get this correct ) and tick the box that states you want to format the drive . Your screen should look something like this : <p> Finally click Create . The process will take approximately 1 minute . Once done , hit Close and unplug the USB pen drive and keep it safe until you are read to boot off it when upgrading your TiVo Premiere 's hard drive as described below . <p> WDIDLE Software : If you are using a Western Digital WDxxEURS drive manufactured before Jan 2013 you will need this for all TiVo versions , otherwise you will just need it for TiVo Series 3 with a Western Digital Drive . Download the ISO . ( wdidle.iso 0.9 MB ) . Then either make a second bootable USB pen drive for this ISO using the method described above or burn it to a CD Remember @ @ @ @ @ @ @ @ @ @ Tivo , purchased in the last 3 or 4 years and a new hard drive you do not need to worry about this . <p> Replacement Hard Disk <p> If you are upgrading a Roamio , Premiere , Virgin Media ( UK ) , Series 3 or HD TiVo then you will need a 3.5 " SATA hard drive . For the TiVo BOLT you will need a 2.5 " laptop sized hard drive . <p> In a TiVo HD the original hard disk is 160GB for around 20 hours of HD video . In a Series 3 it is 250GB for around 25-35 hours of HD video while in a Premiere it is 320GB ( some are 500GB ) for around 35 - 45 hours of HD video , an Original Virgin Media ( UK ) TiVo it is 500GB and a TiVo BOLT is 500GB ( 1TB also available ) for around 75 HD hours varies depending on input signal . Hence you will need something bigger than this . Pretty much any SATA drive will work , however , some models are more suitable based on their @ @ @ @ @ @ @ @ @ @ the best hard drives for upgrading an HD compatible TiVo are the Western Digital SATA drives , particularly the Green models ( although this has now been discontinued ) or Blue which are low power and low spin speed which makes them quiet or the AV-GP models which are specifically designed for DVR use ( but cost slightly more than the Green models ) including : <p> 1TB TiVo Series 3 / HD or Premiere ( or Green Version ) <p> 2 TB TiVo Series 3 / HD / Premiere / Roamio ( or Green Version ) <p> 3 TB Premiere / Roamio only* ( or Green Version ) <p> 4 TB Premiere / Roamio only* ( or Green Version ) <p> 6TB Roamio only* <p> 2TB TiVo BOLT only <p> 3TB TiVo BOLT only <p> 4 TB TiVo BOLT only* <p> 4 TB TiVo BOLT only ( will need to remove case ) * <p> Note : TiVo Series 3 and TiVo HD are limited to 2TB as the maximum disk size . TiVo Premiere appears to currently have a limit of 4TB as the maximum disk size although @ @ @ @ @ @ @ @ @ @ is supported on the Roamio and BOLT but requires either reformatting the drive or pre-blessing it before upgrading if the drive is &gt; 3 TB . Both procedures are described in the relevant section later . Tivo BOLT 's need 2.5 " ( laptop format ) hard drives . Also note that there have been reports that the 4TB Seagate 2.5 " drive ( STDR4000100 ) has long term reliability issues so you may want to consider options from other manufacturers . <p> I highly recommend the Western Digital drives since they are optimized for DVRs , especially the AV-GP Series or Green series shown above although the other versions should also work fine . Drives from other manufacturers will work fine as well . The great thing about the Western Digital AV-GP and Green series drives are that they are optimized for low power consumption and low noise . They automatically slow to 5400 rpm or less when not being used flat out . While not necessarily optimum for a desktop PC for a TiVo these properties are perfect since TiVo never really stresses the drives performance . The @ @ @ @ @ @ @ @ @ @ Note : If you are upgrading a TiVo series 2 single or dual tuner model then you need a Parallel ATA Drive ( UDMA 133 is best although UDMA 100 drives should also work ) . <p> Once you have everything you need you are ready to begin the upgrade process and unlock the true power of your TiVo . <p> The price of external eSATA drives for upgrading TiVo Premiere , HD and Series 3 's has recently dropped to the point where you might want to consider this instead of upgrading the internal hard disk . The internal drive upgrade procedure is simple but you may be more comfortable just plugging in an external drive . Of course you do not gain any of the power saving advantages of replacing the internal disk and you have to hide the external drive somewhere but this is an easy way to give you approximately 150 hours additional HD recording capacity . <p> A note of caution : If you are planning to upgrade a TiVo Bolt , Roamio , Premiere , HD or Series 3 that currently has an external @ @ @ @ @ @ @ @ @ @ before attempting the upgrade . This will result in you loosing the shows on your TiVo so I would suggest using TiVo Desktop to copy shows you want to keep onto your computer before starting . You can always copy them back once you complete the upgrade although if your cable company sucks , like mine ( Time Warner ) and illegally marks most shows as copy prohibited you will not be able to do this - probably the best justification there is for using Bittorrent . You should then power down your TiVo , disconnect the external drive and power it up again . It will complain about the external drive not being connected . Let TiVo rebuild itself to be a single drive TiVo again . Then perform the drive upgrade as detailed below . Unfortunately at present it is not possible to connect an external drive to an expanded TiVo . <p> Select your Preferred Upgrade Method <p> While either the Linux or Windows approach should work for TiVo HD , Series 3 or Series 2 you should select whichever you feel more comfortable with although I @ @ @ @ @ @ @ @ @ @ be sure . For TiVo Premiere the Linux approach is the only one currently available . <p> Upgrade procedures for the Tivo BOLT are still evolving although the approaches described below have been confirmed to work on drives up to 8TB ( although at the time of writing 4TB is the biggest 2.5 " drive available ) . This requires purchase of a 2.5 " hard drive . The current recommended model , in terms of cost , is a Seagate HN-M201RAD for 2TB and a Seagate STDR4000100 for 4TB . Note this will not preserve any shows and you will be required to repeat guided setup . A 4TB drive yields a TiVo BOLT with 630 HD hours ( firmware as of mid 2016 , varies by input signal ) . Thanks go to Michael Boulanger here for testing things and for providing the photos . <p> Warning : The BOLT upgrade method is similar to the Roamio and it is refreshing to see that TiVo persisted with this allowing this simple procedure . However there are a couple of things you need to be aware of . Firstly @ @ @ @ @ @ @ @ @ @ recordings and season passes so I suggest copying those off first to another TiVo or Tablet if your evil cable company let 's you and using TiVo online to backup your season passes . Secondly if you use a cable card with your Tivo you may need to call your cable company to re-pair these depending on how aggressive your cable operator is with authentication rarely required . <p> The procedure for the BOLT depends on whether the new drive you are using is larger than 3TB . <p> New Drives of 3TB or Less in Size . <p> At the time of the Tivo BOLT 's release the maximum it would auto format to was limited to 2.2TB but it would appear that Tivo BOLT 's with current firmware ( as of mid 2016 ) can auto format all the way up to 3TB . Thus if your new drive is 3TB in size or less then the procedure is extremely simple and makes use of the BOLT 's autoformat facilities . The procedure is as follows : <p> 1 ) If you purchased a cased 2.5 " drive such @ @ @ @ @ @ @ @ @ @ the case . This can be a little tricky . The following video may help . <p> 2 ) Open the TiVo BOLT . There are two screws under the cable card cover and one on the back consisting of T-9 and T-10 star screws . Once removed you should be able to carefully pry open the side of the case on the bend . Remove the stickies holding the drive cable in place , remove the single drive mount screw and you can remove the existing hard drive . Place it somewhere safe for storage . <p> 3 ) Insert the new blank hard drive ( if you are using a drive that was previously used in a computer you will need to blank it first by deleting the partitions in your OS ) in the TiVo using the mount from the original drive . Reattach the cables , place the lid back on and secure the 3 case screws . <p> 4 ) Power on the TiVo BOLT . The TiVo should boot and after several minutes will begin guided setup ( if you get a message stating @ @ @ @ @ @ @ @ @ @ needs to be reformatted just press select to confirm ) . Go through guided setup and enjoy your new TiVo BOLT XL . : - ) . You can check your new recording capacity in the System Information menu . <p> New Drives of 4TB or Greater in Size . <p> If your new drive is greater than 4TB in size the TiVo BOLT 's auto format facility will unfortunately only format the drive to a maximum size of 3TB as of mid 2016 . To access the full capacity of the new drive it is necessary to either reformat it ( requires access to a machine running Windows and supports up to 8TB drives ) or bless it ( Windows free method but limited to 4TB drives . Read warning before using this approach ) . <p> BOLT Windows Method ( Recommended ) <p> 1 ) Be sure to power on your Tivo , complete guided setup and registration and perform software updates to ensure you have the latest software version . Although not required it is best to run your new Tivo for a day or so just @ @ @ @ @ @ @ @ @ @ the upgrade . <p> 2 ) This method makes use of the the awesome MFS Reformatter ( MFSR ) software from Ggieseke . You can obtain the latest version from this thread ( registration required ) . If that does not work you can also get the BOLT enabled version here , but I encourage you to try to register to download it from the TiVo community thread first in case there have been any updates . Save it on your Windows Desktop or somewhere convenient . <p> 3 ) If you purchased a cased 2.5 " drive such as the STDR4000100 then first remove the hard drive from the case . This can be a little tricky . The following video may help . <p> 4 ) Open the TiVo BOLT . There are two screws under the cable card cover and one on the back consisting of T-9 and T-10 star screws . It is not as easy to open as previous Tivos but is possible with some patience . You may find the following video useful . Once removed you should be able to carefully pry open @ @ @ @ @ @ @ @ @ @ careful not to disturb any of the wires inside the case that are unrelated to the hard drive . Remove the stickies holding the drive cable in place , remove the single drive mount screw and you can remove the existing hard drive . Place it somewhere safe for storage . <p> 5 ) Insert the new blank hard drive ( if you are using a drive that was previously used in a computer you will need to blank it first by deleting the partitions in your OS ) in the TiVo using the mount from the original drive . Reattach the cables , place the lid back but do not snap it in place . <p> 6 ) Power on the TiVo BOLT . The TiVo should boot and after several minutes will begin guided setup ( if you get a message stating that there is a problem with the drive and it needs to be reformatted just press select to confirm ) . Do NOT start guided setup just pull the power from your TiVo BOLT . <p> 7 ) Remove the new hard drive from the BOLT @ @ @ @ @ @ @ @ @ @ connect the hard drive to your Windows computer using either a direct SATA connection or a USB 3.0 to SATA adapter . ( USB 2.0 SATA adapters should also work but will be slow . ) . A MAC running Windows via Parallels should also work but I have not tested this , <p> 9 ) Power on your Windows machine . If you are asked if you want to partition or initialize the hard drive select no ( or cancel ) . <p> 10 ) Right click on the mfsr.exe file you downloaded above and under the Compatibility tab select ' Run as Administrator ' . ( If you are using Windows 10 also tick the " Run this program in compatibility mode for : " box and select Windows 7 . If you do not do this on Windows 10 MFSR will likely report that no Tivo drives are present ) . <p> 11 ) Follow the instructions given by MFSR to reformat the drive to use its full capacity . The procedure is self explanatory . <p> 12 ) When done close MFSR and shutdown your Windows @ @ @ @ @ @ @ @ @ @ from your Windows computer , place it in your TiVo BOLT using the mount from the original drive . Reattach the cables , place the lid back on and secure the 3 case screws . <p> 14 ) Power on your TiVo BOLT . Go through guided setup and once finished check that the various Apps ( Netflix , Amazon , Hulu etc ) are working . If they are then you are all done , enjoy your new TiVo BOLT XL . : - ) If you get a V312 error rare with the latest MFSR then go to TiVo Central - Settings &amp; Messages &gt; Help &gt; Restart or Reset &gt; Clear and Delete Everything . And then follow the instructions . <p> After rebooting your TiVo , going through guided setup again you should find the apps are fully working . <p> At the end of Guided Setup you can check the capacity in the system settings . It should be &gt; 488 HD Hours for a 4TB disk depends on your cable company 's compression rate , some people have reported up to 650 HD hours @ @ @ @ @ @ @ @ @ @ . : - ) <p> 15 ) Finally if you found this howto useful then please , as a token of your gratitude , consider using one of the Amazon or NewEgg links on this website , such as the one below , next time you purchase something from Amazon ( anything works , maybe a new TiVo Roamio even ! ) . Just follow the link below to get to amazon.com ( or newegg.com ) before adding items to your shopping cart . Alternatively please consider a small donation via paypal to help cover my server costs - even a dollar can help . <p> Also if you have any comments , suggestions , come across any problems or have any questions please feel free to contact me at the email address above . Good luck and enjoy your new TiVo . <p> BOLT Windows Free Method <p> If you do not have access to a machine running Windows then you can use the following method that involves ' blessing ' your new hard drive prior to inserting it in the TiVo BOLT using a bootable CD or USB @ @ @ @ @ @ @ @ @ @ with 4TB drives and you should read the warning below before proceeding . <p> 4TB WARNINGThe image described here for 4TB drives is based on that developed for the Roamio Tivos . It has been noted that there are definite differences in the disk layout between the Roamio and the BOLT . One key item being the size of the swap partition . The method described below has been tested and at the present time confirmed to work . However , if you choose to use this to bless a 4TB drive for a BOLT you should be aware that there is no guarantee that it will continue working down the road when Tivo releases various software updates . This might necessitate you needing to go back to your original drive and using whatever update procedures have been developed at that time . This will mean you will lose the recordings on your 4TB drive . If you understand this and are happy to accept this risk then proceed . Otherwise you are advised to stick to drives of 2TB or less for now or use the Windows method above @ @ @ @ @ @ @ @ @ @ Tivo , complete guided setup and registration and perform software updates to ensure you have the latest software version . Although not required it is best to run your new Tivo for a day or so just to make sure everything is working okay before proceeding with the upgrade . <p> 2 ) If you purchased a cased 2.5 " drive such as the STDR4000100 then first remove the hard drive from the case . This can be a little tricky . The following video may help . <p> 7 ) When the system finishes booting and you are given a command line enter : <p> sudo bash <p> wget LONG ... <p> tar xvzf bolt4tb.tar.gz <p> cd bolt4tb <p> . /apply4tbboltimage.sh /dev/sdaor sdb if sda does not work <p> 8 ) Allow approximately 20 mins for the ' blessing ' to complete - it may finish sooner . Then shutdown your PC and remove the drive . <p> 9 ) Open the TiVo BOLT . There are two screws under the cable card cover and one on the back consisting of T-9 and T-10 star screws . Once removed @ @ @ @ @ @ @ @ @ @ of the case on the bend . Remove the stickies holding the drive cable in place , remove the single drive mount screw and you can remove the existing hard drive . Place it somewhere safe for storage . <p> 10 ) Insert the new hard drive that you just blessed in the TiVo using the mount from the original drive . Reattach the cables , place the lid back but on and screw it in place . <p> 11 ) Power on the TiVo BOLT . The TiVo should boot directly into guided setup . Go through the guided setup - you skip as much as you want here since this is just to get to the point where you can reset everything as described in step 11 . <p> 13 ) When your TiVo BOLT reboots go through guided setup a second time , this time taking the time to set channels correctly etc . At the end of Guided Setup check that the apps such as NetFlix , Amazon , Hulu load okay . If you get a V312 error then repeat step 11 above . You @ @ @ @ @ @ @ @ @ @ should be &gt; 488 HD Hours for a 4TB disk depends on your cable company 's compression rate , some people have reported up to 650 HD hours at 4 TB . Enjoy your new TiVo BOLT XL . : - ) <p> 14 ) Finally if you found this howto useful then please , as a token of your gratitude , consider using one of the Amazon or NewEgg links on this website , such as the one below , next time you purchase something from Amazon ( anything works , maybe a new TiVo Roamio even ! ) . Just follow the link below to get to amazon.com ( or newegg.com ) before adding items to your shopping cart . Alternatively please consider a small donation via paypal to help cover my server costs - even a dollar can help . <p> Also if you have any comments , suggestions , come across any problems or have any questions please feel free to contact me at the email address above . Good luck and enjoy your new TiVo . <p> Tivo did a WONDERFUL thing with the TiVo @ @ @ @ @ @ @ @ @ @ which is a cut down Roamio based system ) and continued this with the TiVo BOLT that makes upgrading the hard drive space , or replacing a broken hard drive a piece of cake . No longer is it necessary to ' bless ' a new drive for use in a TiVo Roamio like it was in previous Tivos ( unless the drive is &gt;3TB in size ) . The new Roamio can accept a blank drive and during the booting process will automatically install the Tivo software on this drive including any hard drive space updates . This means that if you are happy losing your current recordings and season passes then all you need to do is purchase a new hard drive , such as a WD 2TB drive , open up your Tivo , remove the stock 500GB , drive , insert the new 1 , 1.5 or 2 TB drive see below if you want to add a &gt;2 TB drive , power on and wait while Tivo initializes the drive . You should then be good to go . <p> Warning : The Roamio upgrade @ @ @ @ @ @ @ @ @ @ coming , however there are a couple of things you need to be aware of . Firstly this will result in you losing all of your current recordings and season passes so I suggest copying those off first with Tivo to Go if your evil cable company let 's you . Secondly if you use cable cards with your Tivo you may need to re-pair these depending on how aggressive your cable operator is with authentication rarely required . Typically this is a simple procedure of calling your cable company - although note with some , like Time Warner Cable , talking with their customer service reps about cable cards can be a bit like pulling teeth so you may need to escalate the call to get to talk to the correct person . Reports for Samsung Virgin Media customers in the UK is that the auto hard drive upgrade process works flawlessly without requiring any new pairing . <p> If you have your new drive and are ready to go then proceed as follows : <p> New Drives of 3TB or Less in Size . <p> 1 ) Use the @ @ @ @ @ @ @ @ @ @ that the base model Roamio and Roamio OTA have a T-8 rather than T-10 so you may want to have a torx set to use here ) and then remove the hard drive that is inside , note recent TiVo models require a Torx T-15 Screwdriver to unscrew the hard drive from the bracket . Be careful not to touch the power supply . Note , the only screws holding the TiVo lid are those on the back of the case . Sometimes the top can be a little stiff so you may have to slowly work it loose . <p> 2 ) Carefully remove the existing tivo drive after disconnecting the cable connecting it to the main Roamio circuit board . Place it on an anti-static surface such as a piece of aluminum foil or an anti-static bag . <p> 3 ) Insert the new hard drive in your Tivo Roamio and connect the cable , secure it in place . <p> 5 ) Power on and wait approximately 30 mins for the TiVo Roamio to work its magic . If you have a UK Virgin Media Tivo then @ @ @ @ @ @ @ @ @ @ if you find that after 30 seconds all 4 lights on the front of the Tivo are flashing this likely means you are using the wrong power supply ( likely you switched with a Tivo Mini ) , switch back to the original Roamio power supply . Repeat guided setup and you should then be good to go . You can check the new capacity in system settings . <p> New Drives of 4TB or Greater in Size . <p> If your new drive is greater than 3TB in size the TiVo Roamio 's auto format facility will unfortunately only format the drive to a maximum size of 3TB . To access the full capacity of the new drive it is necessary to either reformat it ( requires access to a machine running Windows ) or bless it ( Windows free method but limited to 4TB drives ) . <p> Roamio Windows Method ( Recommended ) <p> 1 ) This method makes use of the the awesome MFS Reformatter ( MFSR ) software from Ggiesekei and supports up to 8TB drives . You can obtain the latest version from this @ @ @ @ @ @ @ @ @ @ work you can also get it here , but I encourage you to try to register to download it from the TiVo community thread first in case there have been any updates . Save it on your Windows Desktop or somewhere convenient . <p> 2 ) Use the Torx T-10 Screwdriver to open up your TiVo ( Note that the base model Roamio and Roamio OTA have a T-8 rather than T-10 so you may want to have a torx set to use here ) and then remove the hard drive that is inside , note recent TiVo models require a Torx T-15 Screwdriver to unscrew the hard drive from the bracket . Be careful not to touch the power supply . Note , the only screws holding the TiVo lid are those on the back of the case . Sometimes the top can be a little stiff so you may have to slowly work it loose . <p> 2 ) Carefully remove the existing tivo drive after disconnecting the cable connecting it to the main Roamio circuit board . Place it on an anti-static surface such as a piece @ @ @ @ @ @ @ @ @ @ ) Insert the new hard drive in your Tivo Roamio and connect the cable , optionally secure it in place since we will remove it again shortly . <p> 4 ) Power on and wait approximately 30 mins for the TiVo Roamio to work its magic . If you have a UK Virgin Media Tivo then you need to hold TV+Down+Record during boot . Note , if you find that after 30 seconds all 4 lights on the front of the Tivo are flashing this likely means you are using the wrong power supply ( likely you switched with a Tivo Mini ) , switch back to the original Roamio power supply . When you get to the first step of guided setup unplug your Tivo Roamio . <p> 5 ) Remove the new hard drive from the Roamio . <p> 6 ) Power off your Windows machine and connect the hard drive to your Windows computer using either a direct SATA connection or a USB 3.0 to SATA adapter . ( USB 2.0 SATA adapters should also work but will be slow . ) . A MAC running Windows @ @ @ @ @ @ @ @ @ @ this , <p> 7 ) Power on your Windows machine . If you are asked if you want to partition or initialize the hard drive select no ( or cancel ) . <p> 8 ) Right click on the mfsr.exe file you downloaded above and under the Compatibility tab select ' Run as Administrator ' . ( If you are using Windows 10 also tick the " Run this program in compatibility mode for : " box and select Windows 7 . If you do not do this on Windows 10 MFSR will likely report that no Tivo drives are present . ) <p> 9 ) Follow the instructions given by MFSR to reformat the drive to use its full capacity . The procedure is self explanatory . <p> 10 ) When done close MFSR and shutdown your Windows computer . <p> 11 ) Remove the new hard drive from your Windows computer , place it in your TiVo Roamio and secure it in place using the mount from the original drive . Reattach the cables , place the lid back on and secure the case screws . <p> 12 @ @ @ @ @ @ @ @ @ @ guided setup . Go through guided setup and you should then be good to go . You can check the new capacity in system settings . It should be around 640 HD hours for a 4TB drive . <p> Tivo Roamio with 4TB drive . <p> 13 ) Finally if you found this howto useful then please , as a token of your gratitude , consider using one of the Amazon or NewEgg links on this website , such as the one below , next time you purchase something from Amazon ( anything works , maybe a new TiVo Roamio even ! ) . Just follow the link below to get to amazon.com ( or newegg.com ) before adding items to your shopping cart . Alternatively please consider a small donation via paypal to help cover my server costs - even a dollar can help . <p> Also if you have any comments , suggestions , come across any problems or have any questions please feel free to contact me at the email address above . Good luck and enjoy your new TiVo . <p> Roamio Windows Free Method <p> @ @ @ @ @ @ @ @ @ @ Windows then you can use the following method that involves ' blessing ' your new hard drive prior to inserting it in the TiVo Roamio using a bootable CD or USB pen drive Image . Note that this approach only works with 4TB drives ( e.g. WD40EURX ) . <p> 4 ) When the system finishes booting and you are given a command line enter : <p> sudo bash <p> wget LONG ... <p> tar xvzf roamio4tb.tar.gz <p> cd roamio4tb <p> . /apply4tbroamioimage.sh /dev/sda or sdb if sda does not work <p> 5 ) Allow approximately 20 mins for the ' blessing ' to complete - it may finish sooner . Then shutdown your PC and remove the drive . <p> 6 ) Use the Torx T-10 Screwdriver to open up your TiVo ( Note that the base model Roamio and Roamio OTA have a T-8 rather than T-10 so you may want to have a torx set to use here ) and then remove the hard drive that is inside , note recent TiVo models require a Torx T-15 Screwdriver to unscrew the hard drive from the bracket . @ @ @ @ @ @ @ @ @ @ , the only screws holding the TiVo lid are those on the back of the case . Sometimes the top can be a little stiff so you may have to slowly work it loose . <p> 7 ) Carefully remove the existing tivo drive after disconnecting the cable connecting it to the main Roamio circuit board . Place it on an anti-static surface such as a piece of aluminum foil or an anti-static bag . <p> 8 ) Insert the new blessed hard drive in your Tivo Roamio and connect the cable , secure it in place . <p> 10 ) Power on and wait for Roamio to boot to guided setup . Go through guided setup and you should then be good to go . You can check the new capacity in system settings . It should be around 640 HD hours for a 4TB drive . <p> Tivo Roamio with 4TB drive . <p> 11 ) Finally if you found this howto useful then please , as a token of your gratitude , consider using one of the Amazon or NewEgg links on this website , such as @ @ @ @ @ @ @ @ @ @ Amazon ( anything works , maybe a new TiVo Roamio even ! ) . Just follow the link below to get to amazon.com ( or newegg.com ) before adding items to your shopping cart . Alternatively please consider a small donation via paypal to help cover my server costs - even a dollar can help . <p> Also if you have any comments , suggestions , come across any problems or have any questions please feel free to contact me at the email address above . Good luck and enjoy your new TiVo . <p> Warning : Do NOT use this method for a TiVo Premiere , Roamio or BOLT . Use the Premiere method described above for the Premiere or the Hard Disk Replacement Method for the Roamio or BOLT <p> 1 ) Use the Torx T-10 Screwdriver to open up your TiVo and then remove the hard drive that is inside , note recent TiVo models require a Torx T-15 Screwdriver to unscrew the hard drive from the bracket . Be careful not to touch the power supply . Note , the only screws holding the TiVo lid @ @ @ @ @ @ @ @ @ @ the top can be a little stiff so you may have to slowly work it loose . <p> 2 ) Check the BIOS settings of your PC to make sure it is configured to boot from the CDROM drive then power off your PC and open the case . You should also check that your SATA ports are enabled in the bios . Some old PCs ship with these turned off . <p> 3 ) Disconnect all of the hard drives in your PC but leave the CDROM drive attached . For this process the only hard drives you will need connected to your PC are the original TiVo hard disk and the disk you plan to upgrade to . We will be booting off of a CDROM so you do n't need your PC 's own operating system . <p> 4 ) Check that the jumper settings on your new drive ( if applicable ) are set to 3.0 Gb/sec and NOT 1.5 Gb/sec . Then attach the original TiVo drive to the first SATA port and the new drive to the second SATA port . Also connect the @ @ @ @ @ @ @ @ @ @ your PC , insert the MFSLive boot cd in the drive and allow your machine to boot from it . If you are prompted with a boot menu during the boot process simply press enter to boot with the default options . <p> 6 ) For me this brought up the text based terminal ( even though I just let it boot the graphic mode by default ) . If you find yourself in some weird graphics interface then try rebooting and selecting text mode ( option 3 ) . <p> 7 ) Next we need to identify the names of the two hard drives , your TiVo original drive and your new ' upgrade ' drive . To do this type ' cat /proc/partitions ' and hit enter . You should see a screen similar to that below , ( if you do not see the drives listed then it likely means that your motherboard 's SATA controller is not supported by drivers included in the MFSLive boot CD . You could try a newer version of the CD or alternatively try the Windows method below which should ALWAYS @ @ @ @ @ @ @ @ @ @ should mean that the SATA controller is supported ) : <p> Make a note of the name of each drive . Make sure you get this part correct because if you mix the two up you could end up blanking your original TiVo drive and preventing your TiVo from working . In this case the 250GB drive , the TiVo original , is drive sda while the upgrade drive ( a 500GB drive in this example ) is sdb . ( If this is a series 2 TiVo with UDMA drives then you will be looking for hda and hdb or hdc . ) Note , depending on the number of shows recorded on your original TiVo drive you may see lots of partitions listed here ( such as sda1 , sda2 etc . ) . You can safely ignore these since the commands below operate on the complete physical disk ( in this case the letters before the number , so sda , sdb etc . ) <p> 8 ) The next step is to copy over the contents of the original TiVo drive to the new drive and @ @ @ @ @ @ @ @ @ @ At the prompt this type the command : <p> backup -qTao - /dev/sda restore -s 128 -xzpi - /dev/sdb <p> where the first /dev entry points to your source drive , the original TiVo drive , in this case sda and the second /dev entry points to the destination drive , your upgrade drive , in this case sdb . This step can take upwards of an hour on a dual core 2.4GHz AMD machine . On older machines it can take 4+ hours to compete . <p> The options specified above are as follows : <p> -qTao = Sets the backup command to preserve all your settings and recordings. /dev/sda = TiVo Source Disk ( Original ) = the pipe symbol ( normally shift-&gt;backslash ) means redirect the output of the backup command to input for the restore command . -s 128 = Sets the new TiVo disk to contain 128 MB of swap ( more than enough ) -xzpi = Sets the restore command to restore all your settings and recordings and then expand the free space to fill the new disk. /dev/sdb = TiVo Destination Disk ( @ @ @ @ @ @ @ @ @ @ source drive and destination drive after in the correct order . If you copy from the blank drive to the original drive you will wreck your TiVo. ( although I believe the above command will give you an error if you do this - but I have never been brave enough ( or foolish enough ? ) to try it . ) <p> If you do n't want to wait several hours and do n't mind loosing all your current recordings then you can use the following command which will take only a few minutes : <p> backup -f 9999 -qso - /dev/sda restore -s 128 -xzpi - /dev/sdb <p> 9 ) At this point you are done as far as the update goes although I highly recommend you carry out the next ( optional ) step which will set your drive to use aggressive power management ( saving you money ) and maximum acoustic silencing to keep your TiVo as quiet as possible . In my experience this command has no negatives and does not appear to impact TiVo 's performance in any way . Issue the command : @ @ @ @ @ @ @ @ @ @ where /dev/sdb is the new TiVo drive . The options here have the following effect : <p> -k 1 = keep settings after drive is powered off. -B 1 = set most aggressive power management mode . -M 128 = set acoustic management to as quiet as possible . <p> Note you may get one or more errors from this command if the new drive you have selected does not support power management or acoustic management ( or in the case of most new drives controls it dynamically such as new EADS , EVCS , EVDS and EURS series WD drives ) . In either case you can safely ignore any warnings here . If you do get an error ( invalid command ) you can try the following to just set the acoustic management ( which may also fail on the latest drives and is nothing to be concerned about ) : <p> hdparm -k 1 -M 128 /dev/sdb <p> 10 ) Run wdidle3 to reset the idle time on Western Digital drives ( Required for TiVo Series 3 with any WD drive . Required for ALL TiVos using @ @ @ @ @ @ @ @ @ @ the majority of people this step is unneeded and can be safely skipped . <p> Switch off your PC and turn it back on with the WDIDLE boot CD ( or USB pen drive attached ) and just the new TiVo Western Digital disk attached to any SATA port ( other disks should be disconnected ) . Note:If you have a newer PC , you may need to enter your PC bios and temporarily switch your SATA controller to EIDE mode from AHCI/RAID . This change is needed before a bootable DOS CD can detect the drive on some newer PCs . <p> Boot from the above CD or USB - hit " Enter " within 15 seconds to boot to DOS and then run " wdidle3 /S0 " to disable IntelliPark on the WD drive . <p> A message should indicate " Idle3 Timer is disabled . " That means you were successful and you will no longer face what is known as the ' soft reboot ' problem where some TiVos would hang on restart with newer Western Digital Hard Drives . <p> 11 ) With that you @ @ @ @ @ @ @ @ @ @ power button of your PC so that it powers off . Typically you need to hold it in for 4+ seconds or the PC will simply reboot . Then disconnect the old TiVo hard disk and put it somewhere safe in case you ever need to put it back in for a warranty call . Then take the new TiVo hard disk and mount it in your TiVo . Screw the cover back on , plug it all in and power it up . Once it starts simply go to the settings -&gt; System Information menu and you should see the new increased capacity of your TiVo . <p> ( TiVo HD with 1TB disk and supersized with WinMFS . Default capacity from the procedure above will be about 136 HD hours or so ) <p> 12 ) Finally if you found this how to useful then please as a token of your gratitude please consider using one of the Amazon or NewEgg links on this website , such as the one below , next time you purchase something from Amazon ( anything works , maybe a new TiVo Roamio @ @ @ @ @ @ @ @ @ @ get to amazon.com ( or newegg.com ) before adding items to your shopping cart . Alternatively please consider a small donation via paypal to help cover my server costs - even a dollar can help . <p> Finally if you have any comments , suggestions , come across any problems or have any questions please feel free to contact me at the email address above . Good luck and enjoy your new TiVo . <p> Warning : Do NOT use this method for a TiVo Premiere . Use the Premiere method described below . <p> If you are more comfortable using Windows than Linux then you may prefer to use this method in place of the Linux boot CD method described above . However , be aware that you need to be careful here to make sure you DO NOT INITIALIZE your TiVo disk under Windows . Doing so will blank your TiVo disk . You also need to make sure you are logged in with an account that has administrator privileges . <p> 1 ) Download the WinMFS software here ( if that link does not work it is @ @ @ @ @ @ @ @ @ @ so does not need to be installed . Fire it up now just to make sure it is working . You should see the following screen . <p> at the moment no TiVo drives will be recognized since they are not connected to your PC . <p> 2 ) Use the Torx T-10 Screwdriver to open up your TiVo and then remove the hard drive that is inside , note recent TiVo models require a Torx T-15 Screwdriver to unscrew the hard drive mounting and then the T-10 again to unscrew the Hard Drive from the bracket . Be careful not to touch the power supply . Note , the only screws holding the TiVo lid are those on the back of the case . Sometimes the top can be a little stiff so you may have to slowly work it loose . <p> 3 ) Check the BIOS settings of your PC to make sure it is set up to recognize all SATA drives that are connected . You should check that your SATA ports are enabled in the bios since some old PCs ship with these turned off @ @ @ @ @ @ @ @ @ @ your new drive ( if applicable ) are set to 3.0 Gb/sec and NOT 1.5 Gb/sec . Then attach the original TiVo drive to the first available SATA port and the new drive to the second available SATA port . Also connect the power to these drives . Do NOT disconnect your windows Boot Drive . <p> 5 ) Power on your PC , make sure the two new drives are recognized by the bios and then let it boot into Windows . If you get any messages about initializing the new drives make sure you select No or Cancel . <p> 6 ) Once Windows has finished booting run the WinMFS.exe program . Select File -&gt; Select Drive . This will bring up the following window : <p> Here you need to select your original TiVo drive ( under the A drive box ) - make sure you highlight the correct one - for a standard TiVo HD this should be the 160GB drive . Then hit the Select button . <p> 7 ) Backup your original TiVo drive . One of the neat things about the WinMFS tools @ @ @ @ @ @ @ @ @ @ original TiVo drive to an image file that you can store on your PC or later burn to a CD/DVD for safe keeping . This allows you to reuse your original TiVo HD for something else and not have to worry if your new ' upgrade ' drive should fail at some point . Hit File-&gt;TiVo Drive ( Truncated ) - This will backup everything except for you actual saved videos meaning the image will be small enough to fit on a CD . <p> This will bring up a backup options window . <p> Here you just need to select the name of the file you want to backup to ad then hit start . This takes about 5 to 10 minutes depending on your hardware . <p> 8 ) Next comes the actual drive upgrade . Close the backup window and hit Tools-&gt;Mfscopy . As the source drive select your original TiVo drive and under target select the new ' larger ' TiVo drive . <p> 9 ) Hit Start and wait . A note of caution here , avoid running anything else why this copy is running @ @ @ @ @ @ @ @ @ @ you drag something in front of the MFScopy window it will cease to update and you wo n't know how far it has got until it finishes and updates the window once more . <p> 10 ) Once done WinMFS will ask you if you 'd like to increase the TiVo image to make use of the extra capacity of your new drive . I assume you do so hit yes . <p> 11 ) Optional . If you want you can enable the Supersize option under the tools menu . This supposedly takes space that would be reserved for downloading commercials and sponsored programs and instead makes it available for recordings further increasing your capacity . <p> 12 ) Run wdidle3 to reset the idle time on Western Digital drives ( Required for TiVo Series 3 with any WD drive . Required for ALL TiVos using WDxxEURS drives manufactured prior to Jan 2013 ) . For the majority of people this step is unneeded and can be safely skipped . <p> Switch off your PC and turn it back on with the WDIDLE boot CD ( or USB pen @ @ @ @ @ @ @ @ @ @ disk attached to any SATA port ( other disks should be disconnected ) . Note:If you have a newer PC , you may need to enter your PC bios and temporarily switch your SATA controller to EIDE mode from AHCI/RAID . This change is needed before a bootable DOS CD can detect the drive on some newer PCs . <p> Boot from the above CD or USB - hit " Enter " within 15 seconds to boot to DOS and then run " wdidle3 /S0 " to disable IntelliPark on the WD drive . <p> A message should indicate " Idle3 Timer is disabled . " That means you were successful and you will no longer face what is known as the ' soft reboot ' problem where some TiVos would hang on restart with newer Western Digital Hard Drives . <p> 13 ) With that you are done with the upgrade . Shut down Windows and power off your PC . Then disconnect the old TiVo hard disk and put it somewhere safe in case you ever need to put it back in for a warranty call . Then @ @ @ @ @ @ @ @ @ @ your TiVo . Screw the cover back on , plug it all in and power it up . Once it starts simply go to the settings -&gt; System Information menu and you should see the new increased capacity of your TiVo . <p> ( TiVo HD with 1TB disk and supersized with WinMFS . Default capacity from the procedure above will be about 136 HD hours or so ) <p> 14 ) Finally if you found this how to useful then please as a token of your gratitude consider using one of the Amazon or NewEgg links on this website , such as the one below , next time you purchase something from Amazon ( anything works , maybe a new TiVo Roamio even ! ) . Just follow the link below to get to amazon.com ( or newegg.com ) before adding items to your shopping cart . Alternatively please consider a small donation via paypal to help cover my server costs - even a dollar can help . <p> Finally if you have any comments , suggestions , come across any problems or have any questions please feel free @ @ @ @ @ @ @ @ @ @ luck and enjoy your new TiVo . <p> WARNING : This approach has ONLY been tested on a TiVo Premiere , Premiere 4 and Original Virgin Media ( UK ) TiVo <p> This approach will also NOT work for upgrading a WeakKnees or previously expanded drive . e.g. If you purchased a 1TB WeakKnees drive for your Premiere or previously upgrade your TiVo yourself to an expanded drive you will need to use the original 320GB hard drive to perform the upgrade to your new drive <p> Virgin Media Tivo update Nov 11th 2015 : I have now received multiple independent reports that this method no longer works for some Cisco based Virgin Media Tivos . The common denominator appears to be that they are running software version 20.4.3.1 . RC4-VMC-2-C00 . With this version ( and perhaps newer versions ? ) Virgin media appear to have encrypted the MFS partitions and so JMFS can not recognize the drives . If and when a fix becomes available I will post it here . If your software version is older you may be in luck but I do not have the @ @ @ @ @ @ @ @ @ @ present . <p> 1 ) Use the Torx T-10 Screwdriver to open up your TiVo and then remove the hard drive that is inside , note recent TiVo models require a Torx T-15 Screwdriver to unscrew the hard drive from the bracket . Be careful not to touch the power supply . Note , the only screws holding the TiVo lid are those on the back of the case . Sometimes the top can be a little stiff so you may have to slowly work it loose . ( Note the US version of the Premiere has no stickers or warranty void stickers preventing you opening the case . The Original Virgin Media ( UK ) branded TiVo Premiere includes a small sticker over one of the screw holes . This can be fairly easily removed using a sharp knife and a hairdryer . If you do it slowly enough you will not damage the sticker and can replace it should you need to . <p> Image of the rear of the TiVo Premiere with the 4 case screws numbered . <p> Image showing the inside of the TiVo Premiere @ @ @ @ @ @ @ @ @ @ NOT touch it . You can see the hard disk on the bottom left . Interestingly TiVo mount this upside down . You can do either with your new disk but for consistency you should probably mount it upside down as well . <p> Image showing the original TiVo premiere hard drive removed . As you can see TiVo use Western Digital drives like those recommended above . <p> Image showing the inside of the Virgin Media TiVo from the United Kingdom . The haddrive was mounted on the left and was removed for this photograph . The power supply is on the far right and you should be careful not to touch this . <p> 2 ) Check the BIOS settings of your PC to make sure it is configured to boot from the CDROM drive / or USB if you are using the USB pen drive method then power off your PC and open the case . The precise layout and menu structure of your bios will differ but mine is show below for reference . Note you often need to press ' Del ' or F2 at @ @ @ @ @ @ @ @ @ @ settings . You should also check that your SATA ports are enabled in the bios . Some old PCs ship with these turned off . <p> 3 ) Open your PC and disconnect all of the hard drives but leave the CDROM drive attached ( if you plan on booting from CD ) . For this process the only hard drives you will need connected to your PC are the original TiVo hard disk and the disk you plan to upgrade to . We will be booting off of a CDROM / USB pen drive so you do n't need your PC 's own operating system . It is not strictly critical that you disconnect the other drives but it will prevent you from accidentally erasing them . <p> 4 ) Check that the jumper settings on your new drive ( if applicable ) are set to 3.0 Gb/sec and NOT 1.5 Gb/sec . The Western Digital drives are all 3.0 Gb/sec ( or better ) by default so you do not need to do anything to these . Next attach the original TiVo drive to the first SATA port @ @ @ @ @ @ @ @ @ @ It is not critical that they be on these specific ports since we will be given a menu to choose from in the drive upgrade program ) . Also connect the power to these drives . ( Note : One could also in principal use USB to SATA connectors here and plug the drives into USB ports but I have NOT tested this . ) <p> 5 ) Power on your PC , insert the Linux boot cd you made above in the drive ( or plug in the USB pen drive ) and allow your machine to boot from it . The first screen that should come will ask you about video modes . You can either wait 30 seconds for things to continue or press the space bar to continue . <p> 6 ) The Linux operating system should then boot , with a bunch of text scrolling past of the screen . Do n't worry about this . If the system locks up or fails to recognize the drives then it is possible that your SATA controller is not recognized . Updating your machine 's bios may @ @ @ @ @ @ @ @ @ @ know the specs of your motherboard and I will see if Linux support is available and can be added to the image . If you have an alternative machine you can always try this one instead . <p> After about 20 to 30 seconds you should get presented with the main TiVo Premiere Upgrade command menu . <p> We will ultimately go through all 3 options in the following order . Copy , Expand and then Supersize . The Copy option simply copies our original TiVo hard disk to the new hard disk . Expand then expands that copy on the new disk to fill all of the available space so giving us the larger recording capacity . Finally the Supersize option modifies our new disk to have less space reserved for commercials and more for programs . This is GOOD in my opinion and gives almost a 10% boost in capacity for free . Do you really want TiVo recording 20 hours of HD infomercials ? <p> Step one is to start the copy so press ' c ' and then the enter key . <p> 7 ) Next @ @ @ @ @ @ @ @ @ @ Original TiVo drive . <p> Hopefully it will only show a single drive as above . You should check this has the capacity you expect for the original TiVo . In the case of the Premiere this is 320.0 GB . Press ' Y ' then the enter key to accept this drive as the source drive . If you get presented with multiple drives be sure to choose the correct on . <p> 8 ) Next we need to select the target drive . At this point the program will present you with a list of available drives in the computer . This will include the USB pen drive if you booted off of USB . <p> In the example above you see two drives listed . My 1TB Western Digital drive and the 1 GB USB pen drive . In this case I want to have the 1TB Western Digital drive as my new TiVo drive . Make a note of the ' device ' identifier of the drive . This is the /dev/xxx text since you will need it for the optional step 12 below where you @ @ @ @ @ @ @ @ @ @ . In this case it is /dev/sda . The drive to be copied to is clearly drive 1 here so I press ' 1 ' and then enter . <p> 9 ) Next the program will present you with a summary screen confirming the source and destination drives . Double check this and if all is good press ' y ' and then Enter . <p> The program will then begin copying your original TiVo drive to the new drive . You will see a screen similar to below showing you the progress . <p> Be patient this step can take up to 3 hours . You can get an estimate of how long it will take by looking at the ' average rate ' on the right of the screen . You are copying 320GB = 320,000MB = 320,000,000 kB . The rate shows the copy rate in kB per sec . So in the example above 37,650 kB / s . Hence it will take 320,000,000 / 37,650 = 8499 seconds = 2.36 Hours . <p> 10 ) Once the copy completes the program should then ask you @ @ @ @ @ @ @ @ @ @ ( If the software fails to identify the new drive this may be an issue with the way your motherboards hard drive controller functions . The simplest approach is to reboot and then skip the above steps and go straight to this expand option ) . <p> Double check that the drive report is the correct ' NEW ' drive and if it is press ' y ' and then enter . Then press ' y ' and enter again at the confirmation page to begin the expansion process . This should complete in a matter of seconds showing you the approximate recording space on the new drive . In this case 143 HD hours ( for a 1TB drive ) . <p> 11 ) The next step is optional but recommended . Press ' r ' and then enter to choose another operation . This will take you back to the main menu where you can press ' z ' and then enter to Supersize your disk . This converts excess space that your Tivo reserves for downloading commercials and other sponsored junk and make it available for recording @ @ @ @ @ @ @ @ @ @ both are now TiVo drives . Make sure you enter the number for the ' NEW ' drive and then press enter . Confirm again with ' y ' and enter . The process will take a couple of seconds and then should complete successfully . Congratulations you are now the owner of an expanded and blessed TiVo hard drive . You can now either press ' s ' enter to shutdown your machine or , if you want to carry out the optional step 12 below and turn on the power saving and acoustic management settings on the new drive press ' x ' and then enter . <p> 12 ) At this point you are done as far as the update goes although I highly recommend you carry out the next ( optional ) step which will set your drive to use aggressive power management ( saving you money ) and maximum acoustic silencing to keep your TiVo as quiet as possible . In my experience this command has no negatives and does not appear to impact TiVo 's performance in any way . Issue the command : @ @ @ @ @ @ @ @ @ @ where /dev/sda is the new TiVo drive identified in step 8 above . The options here have the following effect : <p> -k 1 = keep settings after drive is powered off. -B 1 = set most aggressive power management mode . -M 128 = set acoustic management to as quiet as possible . <p> Note you may get one or more errors from this command if the new drive you have selected does not support power management or acoustic management ( or in the case of most new drives controls it dynamically such as new EADS , EVCS , EVDS and EURS series WD drives ) . In either case you can safely ignore any warnings here . If you do get an error ( invalid command ) you can try the following to just set the acoustic management ( which may also fail on the latest drives and is nothing to be concerned about ) : <p> hdparm -k 1 -M 128 /dev/sda <p> 13 ) Run wdidle3 to reset the idle time on Western Digital drives ( Only required for TiVo Series 3 with WD drive or @ @ @ @ @ @ @ @ @ @ 2013 ) . For the majority of people this step is unneeded and can be safely skipped . <p> Switch off your PC and turn it back on with the WDIDLE boot CD ( or USB pen drive attached ) and just the new TiVo Western Digital disk attached to any SATA port ( other disks should be disconnected ) . Note:If you have a newer PC , you may need to enter your PC bios and temporarily switch your SATA controller to EIDE mode from AHCI/RAID . This change is needed before a bootable DOS CD can detect the drive on some newer PCs . <p> Boot from the above CD or USB - hit " Enter " within 15 seconds to boot to DOS and then run " wdidle3 /S0 " to disable IntelliPark on the WD drive . <p> A message should indicate " Idle3 Timer is disabled . " That means you were successful and you will no longer face what is known as the ' soft reboot ' problem where some TiVos would hang on restart with newer Western Digital Hard Drives . <p> 14 @ @ @ @ @ @ @ @ @ @ Type ' halt ' and press enter and within a few seconds your desktop should power down . You can then disconnect the old TiVo hard disk and put it somewhere safe in case you ever need to put it back in for a warranty call or your new drive fails for some reason and you have to use the original drive to copy onto a new disk . Then take the new TiVo hard disk and mount it in your TiVo . Screw the cover back on , plug it all in and power it up . Your TiVo Premiere should start to boot and after about 3 to 4 minutes display the initial lead in video . It ' may ' then go into Standby mode with a blank screen . Do not panic , just press the TiVo button on your remote to bring your new TiVo alive . Then simply go to the settings -&gt; System Information menu and you should see the new increased capacity of your TiVo Premiere . <p> ( TiVo Premiere with 1TB disk and supersized . ) <p> ( Virgin Media @ @ @ @ @ @ @ @ @ @ . ) <p> There you have it . As easy as pie you are now the proud owner of a massive TiVo Premiere or Virgin Media TiVo .. <p> 15 ) Finally if you found this howto useful then please , as a token of your gratitude , consider using one of the Amazon or NewEgg links on this website , such as the one below , next time you purchase something from Amazon ( anything works , maybe a new TiVo Roamio even ! ) . Just follow the link below to get to amazon.com ( or newegg.com ) before adding items to your shopping cart . Alternatively please consider a small donation via paypal to help cover my server costs - even a dollar can help . <p> Finally if you have any comments , suggestions , come across any problems or have any questions please feel free to contact me at the email address above . Good luck and enjoy your new TiVo . <p> Fixing a Broken TiVo - Do n't panic - it is simple . <p> Has your beloved TiVo been freezing lately or @ @ @ @ @ @ @ @ @ @ ? Are you pulling your hair out in frustration ? - DO NOT PANIC - the fix is actually fairly cheap and easy to complete and could save you hundreds in repair costs for an out of warranty TiVo . For those with expensive unit lifetime subscriptions this will help you breathe even better . <p> In 99.9% of cases the cause of a freezing or rebooting TiVo is a faulty hard drive . Often the drive is failing , taking too long to read a sector for example and this freaks the TiVo out . In most cases though the drive is not entirely past redemption and can still be read inside a computer in order to copy to a new drive . There are a number of ways you accomplish this . First you need to purchase a new Hard Drive of equal or greater capacity that the current drive you have in the TiVo . <p> Once you have that go ahead and open up your TiVo and remove the existing drive using the method appropriate for your model as described above . <p> Series 2 , @ @ @ @ @ @ @ @ @ @ PC using the Linux method described above . However , when you get to step 8 replace the command with the following : <p> backup -qTao - /dev/sda restore -s 128 -zpi - /dev/sdb <p> where the first /dev entry points to your source drive , the original TiVo drive , in this case sda and the second /dev entry points to the destination drive , your upgrade drive , in this case sdb . The difference is the removal of the -x ( expand ) option here since trying to expand an already expanded drive will give you an error . This could take between 1 and 12 hours depending on how bad your existing TiVo drive it . Hopefully though it completes successfully and you can the new drive back into your TiVo and it should magically start working again . <p> Premiere , Roamio and BOLT ( and series 2,3 and HD that do not work with the method above ) <p> For the latest TiVo models the best approach is to simply attempt a raw sector by sector copy of the old hard drive to the @ @ @ @ @ @ @ @ @ @ identical models but can also work if they are simply the same size or the new disk is larger in size than the original disk . Most disk copies fail when they encounter a bad sector so it is critical to do this in a way that ignores and unreadable sectors . We are essentially crossing our fingers here that any bad sectors on the failing disk are inside recorded shows rather than the TiVo 's actual system software - this the most likely case since the shows take up 99.99% or more of the disk . <p> The procedure involves using the Ubuntu Rescue Remix Linux boot CD and the disk copy program ddrescue . The procedure is as follows : <p> 2 ) Remove all drives from your PC and hook up the failing TiVo drive and your new drive using the same procedure described earlier on this page . I suggest connecting the source ( failing ) drive to the first SATA connector and the new drive to the second SATA connector such that the original TiVo drive will be sda and the new TiVo drive will @ @ @ @ @ @ @ @ @ @ booting and you are given a command line enter : <p> sudo bash <p> 5 ) Check your source and destination drives by running <p> hdparm -i /dev/sda <p> hdparm -i /dev/sdb <p> Double check that the drive models and capacities are those you expect for the source and destination drive . <p> 6 ) Begin the copying process by running the following command : <p> ddrescue -f -n /dev/sda /dev/sdb <p> Let this run to completion . It can take between 4 and 24 hours depending on how bad your drive is . <p> 7 ) Once the copy process is complete place the new drive in your TiVo and then go through the following procedure ( not always required , if your TiVo appears to be working fine after the recovery you can skip this step ) : <p> TiVo Premier , TiVo HD , Series 2 and Series 3 <p> Switch on your TiVo and wait for the green light on the front to appear . <p> When you see AMBER lights starting to flash hold down the pause button on the remote and keep it held @ @ @ @ @ @ @ @ @ @ and red lights are both on release the pause button . <p> Type in the number ' 57 ' using the remote ( there is about a 10 second window to do this ) . <p> If the above procedure worked the TiVo will then reboot into a green screen saying " TiVo has detected a serious problem and is attempting to fix it " - Leave this to run , around 30 minutes or so is typical . Once complete the TiVo will reboot automatically and hopefully you will be greeted with a fully working TiVo . <p> TiVo Roamio , Roamio Plus , Roamio Pro , Roamio OTA and BOLT <p> If your Roamio remote control is in RF mode , you will need to put it in IR mode before running the kickstart procedure . Press and hold TiVo + C until the activity indicator on the remote lights up to enter IR mode . <p> As the TiVo DVR restarts , the green LED light on the front bezel of the box will be lit . <p> When you see the yellow/amber light begin to flash @ @ @ @ @ @ @ @ @ @ for two seconds , then release it . <p> If successful , the green and amber lights will blink in alternating patterns to indicate the code has been accepted and the TiVo will reboot showing a recovery screen . Leave your TiVo on and it should reboot normally once the recovery is complete , typically 30 mins to several hours . <p> A common error that occurs when trying to fix a broken Tivo is that it displays Error #51 on startup . This normally occurs if you used the hard drive from a different ( but same model ) Tivo to do the copy . The drive needs to be correctly linked to the serial number of the Tivo it is in . Fortunately fixing this is easy . You simply need to tell the Tivo to clear and delete everything and then reboot it so it comes on as if it was a factory fresh Tivo . <p> If you find that your TiVo is giving you a V312 error when you try to access apps like NetFlix or Amazon do not panic . Well panic a little @ @ @ @ @ @ @ @ @ @ your TiVo but it is fixable . This errors seems to occur most often on TiVo BOLTs that have had the hard drive upgraded . The solution is to : <p> After rebooting your TiVo , going through guided setup again you should find the apps are fully working . <p> Authentication Error for APPs <p> It has been reported that some apps ( particularly VUDU ) use the system I 'd from the motherboard of the Tivo . This can cause problems if you move a hard drive from one Tivo to another . For example if you fix a broken Tivo by copying the drive of another identical Tivo you will likely find that a number of the Apps give authentication errors . It is unclear if all Apps are fixed by doing a Clear and Delete Everything . As such if you plan on moving a drive or drive image from one Tivo to another it is advisable to deactivate all the various apps before doing the copy . One can also simply format the drive on a PC or MAC and then repeat the copy - this @ @ @ @ @ @ @ @ @ @ with App authentication . <p> Finally if you found this howto useful then please , as a token of your gratitude , consider using one of the Amazon or NewEgg links on this website , such as the one below , next time you purchase something from Amazon ( anything works , maybe a new TiVo Roamio or Bolt even ! ) . Just follow the link below to get to amazon.com ( or newegg.com ) before adding items to your shopping cart . Alternatively please consider a small donation via paypal to help cover my server costs - even a dollar can help . 
@@97506142 @1706142/ <p> Notice that there is another type of question : " Do you know what time it is ? " . <p> This could be encoded as a sentence in FOL as follows : <p> Exists t Knows ( You , t ) AND CurrentTime(t) <p> The answer to this question could still be " True " , " False " or " do n't know " , but , if it is " True " , it would be nice to know what the value of " t " ( the current time ) is ! <p> Just like the previous question , this question can be answered by constructing a proof that either proves that " t1 " , say , is the current time , or proving that there is no current time . <p> To summarize then , reasoning in FOL involves finding the answers to questions by constructing proofs . If the questions have variables , then the values of the variables , not just the answer , are important . <h> Inference rules for FOL <p> Now we must look at how to @ @ @ @ @ @ @ @ @ @ , i.e. an AI agent , can not do proofs by somehow magically " understanding " sentences and " thinking " about their meanings . <p> A proof must always be built from a fixed set of inference rules . <p> The propositional logic inference rules <p> Remember the inference rules for propositional logic : <p> Modus Ponens or Implication-Elimination : <p> A =&gt; B , A --------- B <p> And-Elimination : <p> A1 and A2 and ... An -------------------- Ai <p> And-Introduction : <p> A1 , A2 , ... An -------------------- A1 and A2 and ... An <p> Or-Introduction : <p> Ai ------------------ A1 or A2 or ... An <p> Double-Negation Elimination : <p> NOT ( NOT ( A ) ) ----------- A <p> Unit Resolution : <p> A or B , NOT(B) --------------- A <p> Resolution : <p> A or B , NOT(B) or C ------------------- A or C <p> These rules are still valid for FOL . <p> A , B , and C are now atomic sentences ( ie , a predicate or Term=Term , no negation ) . <p> Substitution <p> We need some way @ @ @ @ @ @ @ @ @ @ and " FORALL " . <p> To describe inference rules involving variables and quantifiers , we need the notion of " substitution " . <p> Let alpha be any sentence and let theta be a substitution list : <p> theta = x/fred , y/z , ... <p> The notation SUBST ( theta , alpha ) will denote the result of applying the substitution theta to the sentence alpha Intuitively subst ( x : = g , alpha ) is alpha with every appearance of x replaced by g . <p> Notice that this is a syntactic operation on sentences . Any operation involved in a proof has to be syntactic : it has to just manipulate and transform sentences . <p> Elimination and Introduction <p> Many inference rules tell how how to get rid of ( eliminate ) or introduce a connective or quantifier into a formula . <p> The " universal elimination " rule let 's us use a universally quantified sentence to reach a specific conclusion , i.e. to obtain a concrete fact . <p> FORALL x alpha --------------------- SUBST ( x/g alpha ) <p> Here g may be @ @ @ @ @ @ @ @ @ @ in the knowledge base . <p> The " existential elimination " rule let 's us convert an existentially quantified sentence into a form without the quantifier . <p> EXISTS x alpha --------------------- SUBST ( x/k alpha ) <p> Here k must be a new constant symbol that does not appear anywhere else in the database . It is serving as a new name for something we know must exist , but whose name we do not yet know . <p> The " existential introduction " rule let 's us use a specific fact to obtain an existentially quantified sentence : <p> alpha **30;863;TOOLONG EXISTS y SUBST ( g/y , alpha ) <p> This rule can be viewed as " leaving out detail " : it omits the name of the actual entity that satisfies the sentence alpha , and just says that some such entity exists . <p> Skolemization <p> The " existential elimination " rule is also called the Skolemization rule , after a mathematician named Thoralf Skolem. 
@@97506143 @1706143/ <h> Born : London , England , July 25 , 1920 <h> Died : London , England , April 16 , 1958 <h> Pioneer Molecular Biologist <p> There is probably no other woman scientist with as much controversy surrounding her life and work as Rosalind Franklin . Franklin was responsible for much of the research and discovery work that led to the understanding of the structure of deoxyribonucleic acid , DNA . The story of DNA is a tale of competition and intrigue , told one way in James Watson 's book The Double Helix , and quite another in Anne Sayre 's study , Rosalind Franklin and DNA . James Watson , Francis Crick , and Maurice Wilkins received a Nobel Prize for the double-helix model of DNA in 1962 , four years after Franklin 's death at age 37 from ovarian cancer . <p> Franklin excelled at science and attended one of the few girls ' schools in London that taught physics and chemistry . When she was 15 , she decided to become a scientist . Her father was decidedly against higher education for women and @ @ @ @ @ @ @ @ @ @ relented , and in 1938 she enrolled at Newnham College , Cambridge , graduating in 1941 . She held a graduate fellowship for a year , but quit in 1942 to work at the British Coal Utilization Research Association , where she made fundamental studies of carbon and graphite microstructures . This work was the basis of her doctorate in physical chemistry , which she earned from Cambridge University in 1945 . <p> After Cambridge , she spent three productive years ( 1947-1950 ) in Paris at the Laboratoire Central des Services Chimiques de L'Etat , where she learned X-ray diffraction techniques . In 1951 , she returned to England as a research associate in John Randall 's laboratory at King 's College , London . <p> It was in Randall 's lab that she crossed paths with Maurice Wilkins . She and Wilkins led separate research groups and had separate projects , although both were concerned with DNA . When Randall gave Franklin responsibility for her DNA project , no one had worked on it for months . Wilkins was away at the time , and when he returned @ @ @ @ @ @ @ @ @ @ a technical assistant . Both scientists were actually peers . His mistake , acknowledged but never overcome , was not surprising given the climate for women at the university then . Only males were allowed in the university dining rooms , and after hours Franklin 's colleagues went to men-only pubs . <p> But Franklin persisted on the DNA project . J. D. Bernal called her X-ray photographs of DNA , " the most beautiful X-ray photographs of any substance ever taken . " Between 1951 and 1953 Rosalind Franklin came very close to solving the DNA structure . She was beaten to publication by Crick and Watson in part because of the friction between Wilkins and herself . At one point , Wilkins showed Watson one of Franklin 's crystallographic portraits of DNA . When he saw the picture , the solution became apparent to him , and the results went into an article in Nature almost immediately . Franklin 's work did appear as a supporting article in the same issue of the journal . <p> A debate about the amount of credit due to Franklin continues . @ @ @ @ @ @ @ @ @ @ role in learning the structure of DNA and that she was a scientist of the first rank . Franklin moved to J. D. Bernal 's lab at Birkbeck College , where she did very fruitful work on the tobacco mosaic virus . She also began work on the polio virus . In the summer of 1956 , Rosalind Franklin became i 'll with cancer . She died less than two years later . 
@@97506144 @1706144/ <p> The SCOP Browser permits the user to navigate through the SCOP hierarchy to arrive at the subset of particular interest . The browser opens up the top level in the hierarchy . Clicking on the arrow/folder icons expands the respective nodes . Clicking on the name of the node will retrieve all PDB IDs assigned to that node . <h> Examples : <p> Search for a term <p> TIM barrel <p> Type TIM barrel in the text box above the tree and click the " Find in Tree " button . The first hit is to " PHP domain " which mentions " some similarity to ... TIM-barrel fold " . Scroll back to the top , and click " Next " . You will find the term TIM beta/alpha-barrel . Click on the name to retrieve a list of these structures . <p> Browse the SCOP Classification Browser <p> Ferritin-like <p> Mouse over All alpha proteins to see the number of structures that belong to this category . Expand this node by clicking on the arrow and scroll down to Ferritin-like . Mouse over to see how @ @ @ @ @ @ @ @ @ @ on the name to retrieve this set of structures . 
@@97506145 @1706145/ <h> Data Query &amp; Download <h> You are here <p> The TEAM network is very proud to provide open access to all data collected through the network . The data you are about to download has been collected by many individual scientists and partners throughout the world and involved an enormous amount of work. - These data do NOT belong to TEAM , but the partners who collected it . - In order to ensure proper credit is given to each and every person responsible for collecting these data , please follow these guidelines : <p> Visit the data set creators page for contact information of the people that need to be consulted- BEFORE you intend to use their data in an analysis towards a publication . As a courtesy , you should offer co-authorship to each data set contact for that particulat data set . If they decline to be a co-author please remember to include them in the acknowledgements of your paper . <p> Ask each data set creator to send you names of other people/institutions who might have been involved in data collection that- should be @ @ @ @ @ @ @ @ @ @ to have the following text included in the acknowledgements section : " All data in this publication were provided by the Tropical Ecology Assessment and Monitoring ( TEAM ) Network , a collaboration between Conservation International , the Smithsonian Institution , and the Wildlife Conservation Society , and partially funded by these institutions , the Gordon and Betty Moore Foundation , and other donors " . <p> Before- downloading the data , you will be asked to agree to TEAM 's Data Use Policy . Please review it carefully to ensure you understand- your responsibilities and rights as a data user . <p> For security reasons , - access to data for sensitive species ( either listed as Endangered or Critically Endangered by IUCN 's Red List ) needs to be approved by TEAM technical staff before it can be downloaded . Please describe as clearly as possible the intended use of the data , the name of the project , and the names of scientists and institutions involved . TEAM will process these requests within 24 hours of reception. - <h> TEAM Protocol Data <p> Data Query and Download @ @ @ @ @ @ @ @ @ @ butterfly , climate , primate , vegetation and terrestrial vertebrate data and images . Click on the image to the right to use this application or review the query and download tutorial to get started . For sites with slow network connectivity , the Data Download ( Lite ) application is a non-map based alternative version of the Data Query and Download application . Please note that endangered species records are filtered out and are not available for download without completing a request form . <h> Lidar Data <p> There is a separate page to download small-footprint , discrete-return lidar data that were collected over the La Selva protected area in Costa Rica in 2009 . The data includes coverage of the lowest elevation TEAM vegetation plot at the Volcan Barva TEAM site as well as low-biomass secondary forests and plantations . <h> Historic Camera Trap Data <p> We also have a separate page for downloading historic camera trap data . This data contains the initial pilot data for the Camera Trapping Protocol which is no longer being implemented . The active Terrestrial Vertebrate Protocol utilizes camera traps to detect @ @ @ @ @ @ @ @ @ @ are available for download in the Data Query and Download application . 
@@97506146 @1706146/ <h> The Critical Zone <h> Earth 's permeable near-surface layer ... from the tops of the trees to the bottom of the groundwater . <p> It is a living , breathing , constantly evolving boundary layer where rock , soil , water , air , and living organisms interact . These complex interactions regulate the natural habitat and determine the availability of life-sustaining resources , including our food production and water quality . <h> Explore the Critical Zone <h> The Critical Zone is Earth 's outer skin <p> A permeable layer from the tops of the trees to the bottom of the groundwater . <p> An enviro ? nment where rock , soil , water , air , and living organisms interact and shape the Earth 's surface . <p> Water and atmospheric gases move through the porous Critical Zone , and living systems thrive in its surface and subsurface environments , shaped over time by biota , geology , and climate . <p> All this activity transforms rock and biomass into the central component of the Critical Zone - soil ; it also creates one of the most @ @ @ @ @ @ @ @ @ @ processes operate on second-to-eon timescales <p> The Critical Zone is imprinted by important events over seconds , hours , years , millenia , and geologic time . The present structure and functioning of the Critical Zone reflects : <p> short-term responses to events like rainfall and human activities like land-use changes <p> long-term responses to climatic and tectonic changes over geologic time <h> Critical Zone processes sustain life on Earth <p> The Critical Zone supports all terrestrial life . Its complex interactions regulate the natural habitat and determine the availability of life-sustaining resources , such as <p> food production <p> water quality <p> These are but two of the many benefits or services provided by the Critical Zone . Such " Critical-Zone Services " expand upon the benefits provided by ecosystems to also include the coupled hydrologic , geochemical , and geomorphic processes that underpin those ecosystems . <h> Human impacts on the Critical Zone are large , and vice versa <p> The Critical Zone and human society are closely intertwined , impacting each other in myriad ways . Two particularly important issues are <p> climate change <p> land use @ @ @ @ @ @ @ @ @ @ <p> soil quality <p> stream flow and runoff <p> contaminant transport <p> the carbon cycle <p> Humans clearly affect the Critical Zone . Some examples : <p> 30-50% of global land surface and 50% of freshwater has been used by humans . <p> Croplands and pastures now rival forest cover as the major biome on Earth . <p> Soil loss on U.S. croplands and pastures exceeds 1cm/yr at present . <p> Contaminants have been documented in 80% of representative streams in the U.S. <p> More specifically , too little is known about how physical , chemical , and biological processes in the Critical Zone are coupled and at what spatial and temporal scales . Many of these processes are highly nonlinear and can range across scales from atomic to global , and from seconds to aeons. 
@@97506151 @1706151/ <p> The steps for waxing a ski or a snowboard are essentially one and the same . The ski or board should first be cleaned ( rubbing alcohol is good for this if you have it available . Orange or lemon peel is also good at removing old wav ) . The wax should then be heated , dripped evenly onto the ski or snowboard and then the hot ironed into the base . The wax should then be allowed to cool before being is scraped off , leaving a thin layer on the base . The wax should then be ' buffed ' with a structure brush to allow it to even out and assume the structure of the base . <p> Step by Step Guide <p> Step <p> Procedure Description <p> Ski Illustration <p> Snowboard Illustration <p> 1 <p> Clean the ski or board 's base , using a citrus based wax remover/ski base cleaner . You can also use rubbing alcohol or lemon/orange peel . Just remember to also wash down the base afterwards if you use peel . <p> 2 <p> Heat the wax stick @ @ @ @ @ @ @ @ @ @ slowly move along with the iron a few inches above the base dripping the wax evenly onto the base . You want a drop of wax every 2 cm or so . <p> Next smooth the wax onto the base with the iron - allow it to penetrate while keeping the iron moving . If you have n't waxed your skis or board in ages , or it is brand new you may find that the base absorbs almost all the wax . In this case you should drip more wax on and repeat the process . <p> 5 <p> Add more wax by dripping extra where necessary . Do not apply heat directly to the base for too long . This is especially true for unwaxed parts of the base . Make sure you cover entire base . <p> 6 <p> Next turn off waxing iron and allow the wax to cool and solidify to become part of the base . <p> 7 <p> Scrape off any excess wax with a plastic scraper - most of the wax will appear to come off ; this is fine since what @ @ @ @ @ @ @ @ @ @ has soaked into the base . Most of the wax is has actually been absorbed into the base material . <p> 8 <p> Once you have removed the excess wax buff the base with a structure brush to remove uneven areas and provide structure ( it breaks up suction ) . If you do n't have a buffing brush a soft toothbrush ( one you do n't plan to use to brush teeth anytime soon ) works well although it can take a while . <p> 9 <p> If a second wax coat of a temp specific or flourinated is desired , wait until you scrape off excess from first coat , then apply the second coat . 
@@97506152 @1706152/ <h> MyPDB Login <h> RCSB PDB Policies &amp; References <h> Usage Policies <p> Data files contained in the PDB archive ( ftp : //ftp.wwpdb.org ) are free of all copyright restrictions and made fully and freely available for both non-commercial and commercial use . Users of the data should attribute the original authors of that structural data . By using the materials available in the PDB archive , the user agrees to abide by the conditions described in the PDB Advisory Notice . <p> Molecular images from RCSB PDB Structure Summary pages are available under the same conditions . The authors of the structural data producing the image and the RCSB PDB should be cited . <p> Molecule of the Monthillustrations are available under a CC-BY-4.0 license . Attribution should be given to David S. Goodsell and the RCSB PDB . Molecule of the Montharticles are copyrighted by the RCSB PDB and the authors of the article . Text can only be reprinted with permission , with attribution , and without the right to manipulate or change its content . To request permission , please contact info@rcsb.org . <p> @ @ @ @ @ @ @ @ @ @ are copyrighted . Permission to use these materials may be requested by contacting info@rcsb.org . <h> Privacy Policy <p> Standard server log information , such as IP address , time spent on the site , browser type , etc. , is collected and assessed on an aggregate , rather than individual , basis in order to track site statistics , data that are most popular to our visitors , performance monitoring , and troubleshooting . Studying access statistics and usage patterns helps to project future hardware needs , and aids in the design of new functionality . We also use this information for site and system security . When you sign up for a MyPDB account , we ask you for personal information ( such as your name , email address , account password , country , user type ) . This information is stored on secure RCSB servers . We do not share server log and MyPDB account information with third parties for marketing or other purposes . <p> An entry may also be referenced using its Digital Object Identifier ( DOI ) . The DOIs for PDB entries @ @ @ @ @ @ @ @ @ @ is replaced with the PDB I 'd ( e.g. , 10.2210/pdb4hhb/pdb ) <p> New website features and resources are also described in the articles listed on our Publications page and in regular contributions to the Nucleic Acids Research Database Issue , including the most recent article : <p> The RCSB PDB is a member of the worldwide PDB ( wwPDB ) . The wwPDB should be cited with the URL www.wwpdb.org and the following citation : H.M. Berman , K. Henrick , H. Nakamura ( 2003 ) Announcing the worldwide Protein Data Bank Nature Structural Biology 10 ( 12 ) : 980 . <p> Individual Molecule of the Month articles and images may be referenced using its Digital Object Identifier ( DOI ) and the author/s of the article . The DOIs for these features all have the same format : **25;895;TOOLONG , where YYYY should be replaced with the year , and MM with the number of the month ( one or two digits ) . The DOI can be used as part of a URL ( e.g. , http : **37;922;TOOLONG or entered in a DOI resolver ( http : //www.crossref.org/ ) . 
@@97506153 @1706153/ <p> This form allows TCDB users to submit their own sequenced proteins and descriptions for inclusion into the database . When you submit a sequence , it will be reviewed before it is added to the database . If you have information related to multiple transport families , then you should fill out this form multiple times . The form is divided into sections , e.g. user information , protein information , transport system information , etc . If you have information relating to a multicomponent transport system , you may add as many " protein information " sections as you like . There is a button below the last protein section to add more . When you enter certain data , some information can be imported automatically by clicking the corresponding Update Form button . If these data are available , they will automatically appear in the appropriate text boxes . Not all fields are required , but please fill out the form as completely as possible . Thank you for your contribution . <h> User Information <p> Your Email : <p> Your Name : <p> Your Affiliation @ @ @ @ @ @ @ @ @ @ : <p> Sequence ( FASTA format ) : <p> Protein Name : <p> Alternate protein name(s) : <p> Gene Name : <p> Alternate gene name(s) : <p> Number of transmembrane segments ( either predicted or known ) . If the number of TMS is predicted automatically , please list the method ( e.g. TMHMM ) : <p> Subcellular Location : <p> Substrate(s) ( separated by commas ) : <p> Substrate binding constants ( affinity for substrates ) : <p> Mode of energy coupling : <p> Polarity ( transport direction ) : <p> Mechanism ( e.g. symport , antiport ) : <p> Text Description : <p> If you have information about more proteins that are associated with this transport family , you may create another section on this form for it.Click the button to reload the page with one additional section , " Protein Information #2 " <h> Transporter Information <p> Organism : <p> Suggested Transport Classification ( TC ) Number : <p> When you enter a sequence , we compare it to proteins in TCDB . These are the TCDB-BLAST results : <p> ( No TCDB-BLAST hits ) <p> Current @ @ @ @ @ @ @ @ @ @ : <h> Reference Information <h> Comments <p> Any other Comments : <p> If you know of a WWW link relevant to this transport family , you may add a section on this form for it.Click the button to reload the page with one additional section , " Voluntary Link to External Database #1 " <p> Please contact us to report problems with , or ask questions about this page . Your entry will be reviewed before it is entered into TCDB . We will use the email address you supply to contact you if we have questions about your entry . 
@@97506158 @1706158/ <p> How does one display vector field ? How would one visualize a time evolution of ground motion for an earthquake ? Vector visualization is an important area to investigate scientific data in many domains . Prior visualization strategies have primarily focused on low resolution and discrete glyph plots or volumetric rendering of scalar fields , which do not permit a full characterization of underlying phenomena . We have developed an interactive glyph visualization application that allows scientists to explore vector fields . We implement a novel technique of procedural dipole texturing to encode and display vector data which shows magnitude and direction . We employ a novel lattice method to show neighborhood , which also enables to distinguish glyphs . We further enhance visualization by using screen space ambient occlusion , jitter , outline halos and displacement . By combining contextual information with glyphs , such as geographic maps and representations of crustal structure with three-dimensional isosurfaces , scientists are able examine vector fields in a rich and interactive multivariate environment . We demonstrate the results for time-dependent vector data from a three different simulations including the " @ @ @ @ @ @ @ @ @ @ . These new visualizations clearly depict the obvious features as well those missed in previous investigation . 
@@97506160 @1706160/ <p> ( b ) Once the command in ( a ) runs you will be placed on a compute node . Keep this window open and then from a different terminal window directly ssh to the compute node . For example if you step ( a ) put you on comet-29-01 , you can do : <p> ( Assumes that LONG ... in your submit directory - can be copied from the examples directory ) <p> Once the job runs ( use the squeue command to check on its status ) , the output file will be created in the submit directory . <h> Running Parallel MATLAB Jobs <h> A Tutorial Guide for Comet and Gordon-Simons <p> Both Comet and Gordon support Parallel MATLAB , a development environment from The MathWorks . Here you will find instructions and examples for running jobs with the MATLAB Parallel Computing Toolbox on a desktop and submitting them to the MATLAB Distributed Computing Server ( MDCS ) . <p> Note : MATLAB configurations are very similar on both Comet and Gordon-Simons . Where this documentation refers to Gordon-Simons , you may substitute Comet @ @ @ @ @ @ @ @ @ @ MATLAB on Comet and Gordon-Simons is limited to users from degree-granting educational institutions . To use MATLAB on Comet or Gordon-Simons , request to have your account added to the matlab UNIX group by sending an email , submitting an XSEDE Help Desk ticket . <h> How to Use MATLAB on Comet and Gordon-Simons <h> MATLAB Architecture <p> Parallel MATLAB consists of two parts : the PCT and the MDCS . <h> Features of the Parallel Computing Toolbox <p> The PCT is a module that runs on the MATLAB client . It contains a number of useful capabilities , including : <p> Parallel for-Loops ( parfor ) <p> Distributed Arrays ( arrays spread among several worker processes ) <p> spmd blocks ( single program , multiple data ) which execute code in a manner similar to mpi runs ; MATLAB code placed within an spmd block executes simultaneously on the pool of MATLAB processes that the user has allocated ; each process may be identified by a labindex variable <p> The ability to define several independent tasks to run simultaneously within a single , embarrassingly parallel MATLAB job <h> Usage @ @ @ @ @ @ @ @ @ @ with MATLAB Parallel Computing Toolbox <p> MATLAB has unique setup requirements for users with a Parallel Computing Toolbox on their Windows desktop , compared with other desktop platforms . A secure shell is required to access the Comet or Gordon-Simons MATLAB server , and this must be installed if not already available . For Linux and Mac OS X , the default system shell will suffice . <p> When using the Comet- or Gordon-Simons-based toolbox and client , this desktop configuration is not required . <h> Configuring MATLAB for Use with a Desktop Client <p> To access the Distributed Computing server on Comet or Gordon-Simons from a desktop system ( in other words , to use the Parallel Computing Toolbox with a client not installed on Comet or Gordon-Simons ) , you must have a secure shell installed on the desktop . The setup process is different for Windows and Linux/Mac &lt;#unix&gt; . <p> Copy these files to the toolbox/local directory of your local MATLAB installation . These are modified versions of the MATLAB files that come with the MATLAB release . These files allow you to specify several job @ @ @ @ @ @ @ @ @ @ TSCC , Gordon-Simons or Trestles and are planning to run it on Comet , please update the files in your MATLAB installation under toolbox/local by downloading the new archive linked above . The new files also work with both TORQUE and SLURM . <p> Using these modified files , you may also set : <p> number of processors per node <p> account name <p> queue name <p> wall clock time <p> Other parameters that need to be set include <p> name of the remote cluster ( comet.sdsc.edu or gordon.sdsc.edu ) <p> directory where temporary data will be stored on Comet or Gordon-Simons <p> In this example , a cluster object ( cluster ) is returned by getCluster() , which is passed to createCommunicatingJob() , which returns a job object . The files that are required on the cluster are defined , as well as the number of processors ( 32 in this case ) . A task is created that will call the function testparfor2() which has one output argument and one input argument with the value 32 . The job is then submitted , and the output is stored @ @ @ @ @ @ @ @ @ @ example , an array of dimension N is initialized with zeros , and the process I 'd is written to each array element . Since in this case we have passed the number 32 to N and we have asked for 32 processors , we might expect to get 32 different processes to run the tasks . <p> we can see that only 31 worker processes were used to perform the job . Two of the loop passes were performed by the same process ( 27880 ) . The reason is that MATLAB uses one worker to run the serial code , allocating the remaining workers to perform the parallel functions . Since this only leaves 31 available workers , one of them must handle two loop iterations . 
@@97506161 @1706161/ <p> Lipopolysaccharides ( LPSs ) coat the outer surfaces of the outer membranes of Gram-negative bacteria . LPS is synthesized in the bacterial inner membrane . The lipid A-core region and the O-antigen side chain are synthesized and transported separately to the outer periplasmic surface of the inner membrane where the O-antigen chain is ligated to the lipid A core . The fully assembled LPS is then transported to the outer surface of the outer membrane . The proteins that appears to catalyze this transport process includes a large , essential , outer membrane protein which when defective results in formation of aberrant membranes . It has been called ' increased membrane permeability ' ( Imp ) or ' organic solvent tolerance protein ' ( OstA ) . Mutants defective for this protein in Neisseria meningitidis are viable but produce reduced amounts of LPS . That which is produced is restricted to the periplasmic space . This protein is highly conserved among Gram-negative bacteria . <p> The LptB2FG proteins form an ATP-binding cassette ( ABC ) transporter that uses energy from ATP hydrolysis in the cytoplasm to facilitate extraction @ @ @ @ @ @ @ @ @ @ prior to transport to the cell surface . How ATP hydrolysis is coupled with LPS release from the membrane is poorly understood . Simpson et al . 2016 identified residues at the interface between the ATPase and the transmembrane domains of this heteromeric ABC complex that are important for LPS transport . <p> The Lpt system consists of seven known LPS transport proteins ( LptA-G ) spanning from the cytoplasm to the cell surface . Imp ( OstA or LptD ) of E. coli forms a complex with another essential protein , RlpB ( LptE ) ( Takase et al. , 1987 ) . LptD contains a soluble N-terminal domain and a C-terminal transmembrane domain . LptE stabilizes LptD by interacting strongly with the C-terminal domain of LptD . LptE binds LPS specifically and may serve as a substrate recognition site at the OM ( Chng et al. , 2010 ) . RlpB is a rare lipoprotein that is essential for viability and for transport of LPS to the outer surface of the outer membrane ( Wu et al. , 2006 ) . The seven proteins required for LPS export @ @ @ @ @ @ @ @ @ @ peptidoglycan layer as well as the two membranes of the Gram-negative envelope ( Chng et al. , 2010b ) . The complex that inserts lipopolysaccharide into the bacterial outer membrane forms a two-protein plug-and-barrel , where LptD is the outer membrane barrel , and LptE is the plug ( Freinkman et al. , 2011 ) . The periplasmic component , LptA is able to form a stable complex with the inner membrane anchored LptC but does not interact with the outer membrane anchored LptE ( Bowyer et al. , 2011 ) . The LptC component of the LptBFGC complex may act as a dock for LptA , allowing it to bind LPS after it has been assembled at the inner membrane . That no interaction between LptA and LptE has been observed supports the theory that LptA binds LptD in the LptDE homodimeric complex at the outer membrane . <p> LptD shows cation selectivity and has an estimated pore diameter of 1.8 nm . Addition of Lipid A induces a transition of the open state to a sub-conductance state with two independent off-rates , which suggests that LptD is able @ @ @ @ @ @ @ @ @ @ bacteria all have periplasmic N-terminal domains and C-terminal barrel regions . The latter show distinct sequence properties , particularly in LptD proteins of cyanobacteria , and this specific domain can be found in plant proteins as well . LptD from Anabaena sp . PCC 7120 can also transport Lipid A ( Haarmann et al. , 2010 ) . <p> To date , the only proteins implicated in LPS transport are MsbA ( TC# 3 . A.1.106 ) , responsible for LPS flipping across the inner membrane , and the Imp/RlpB complex , involved in LPS targeting to the OM . Two additional Escherichia coli essential genes , yhbN and yhbG , renamed lptA and lptB , respectively , participate in LPS biogenesis ( Sperandeo et al. , 2007 ) . Mutants depleted of LptA and/or LptB not only produce an anomalous LPS form , but also are defective in LPS transport to the OM and accumulate de novo-synthesized LPS in a novel membrane fraction of intermediate density between the inner membrane ( IM ) and the OM . LptA is located in the periplasm , and expression of the lptA-lptB @ @ @ @ @ @ @ @ @ @ LptA and LptB are implicated in the transport of LPS from the IM to the OM of E. coli , possibly together with Imp/RlpB . A unique LptA structure reported by Suits et al ( 2008 ) represents a novel fold , consisting of 16 consecutive antiparallel beta-strands , folded to resemble a slightly twisted -jellyroll . Each LptA molecule interacts with an adjacent LptA molecule in a head-to-tail fashion to resemble long fibers . <p> OstA ( Imp ) homologues have been shown to play a role in outer membrane biogenesis . Bioinformatic analyses of these proteins in organisms with fully sequenced genomes reveal that these proteins occur only in bacteria with two membranes . Two OstA types were identified , large OstAs ( L ; 812 94 residues ) and small OstAs ( S ; 181 25 residues ) ( Hu and Saier , 2006 ) . S possesses only the OstA domain while L has this domain plus a larger nonhomologous OstA-C domain . Bacteria lacking both S and L proteins were primarily restricted to reduced genome size pathogens and symbionts . Several of these bacteria appear @ @ @ @ @ @ @ @ @ @ biosynthesis of typical Gram-negative bacterial lipopolysaccharide ( LPS ) . Phylogenetic analyses of both S and L homologues showed that they generally follow the phylogenies of the 16S rRNAs from the same organisms with few exceptions . They may comprise two orthologous sets of proteins that together facilitate a single unified function . While most organisms possess a single L and a single S , those lacking S but possessing L are more numerous than those lacking L but possessing S. Based on these findings , is was suggested that the L and S proteins normally act together in macromolecular insertion , ( 1 ) they are important for proper LPS assembly in the outer leaflet of the outer membrane , ( 2 ) they function specifically to export LPS to the outer leaflet , and ( 3 ) L provides a primary function while S provides an important auxiliary function ( Hu and Saier , 2006 ) . <p> There are at least 5 proteins that may make up the machinery for transport from the periplasmic surface of the inner membrane to the outer surface of the outer membrane @ @ @ @ @ @ @ @ @ @ were called ( 1 ) OstA(L) or Imp ( Hu and Saier , 2006 ) ( an outer membrane protein ) , ( 2 ) RlpB , a LPS-assembly lipoprotein discussed above , ( 3 ) OstA(S) or LptA ( a periplasmic protein showing homology to part of OstA(L) ) , and ( 4 ) LptB , a presumed cytoplasmic ATPase , homologous to ATPases of the ABC superfamily ( Hu and Saier , 2006 ; Sperandeo et al. , 2006 ) . Sperandeo et al . ( 2008 ) have identified a fifth component , and have provided a uniform nomenclature of the complex . While the inner membrane ( IM ) transport protein MsbA , is responsible for flipping LPS across the IM , five components of the LPS transport machinery downstream of MsbA have been identified . These are : The OM protein complex LptD/LptE ( formerly Imp/RlpB , respectively ) , the periplasmic LptA protein , the IM associated cytoplasmic ABC protein LptB , and LptC ( formerly YrbK ) , an essential IM component of the LPS transport machinery . Depletion of any of the @ @ @ @ @ @ @ @ @ @ abnormal membrane structures in the periplasm ; ii ) accumulation of de novo synthesized LPS in two membrane fractions with lower density than the OM ; iii ) accumulation of a modified LPS , which is ligated to repeating units of colanic acid in the outer leaflet of IM . Thus , LptA , LptB , LptC , LptD and LptE operate in the LPS assembly pathway ( Sperandeo et al. , 2008 ) . The LptA protein of Escherichia coli is a periplasmic lipid A binding protein involved in the lipopolysaccharide export pathway ( Tran et al. , 2008 ) . <p> E. coli contains two proteins , previously of unknown function called YjgP and YjgQ . They are about 350aas long and have 6TMSs in a 3+3 arrangement , where the two 3TMS units are separated by a large hydrophilic domain ( 150 residues , including an OstA domain ) . They are found mainly in Gram-negative bacteria and are annotated as ' putative permease protein ' . The YjgPQ proteins have over a thousand sequenced homologues present in prokaryotes . The first transmembrane domain is COG0795 . @ @ @ @ @ @ @ @ @ @ are substantially sequence divergent . <p> The two essential Escherichia coli IM proteins , YjgP and YjgQ , are required for the transport of LPS to the cell surface ( Ruiz et al. , 2008 ) . These two proteins , have been renamed LptF and LptG , respectively . They are the missing transmembrane components of the ABC transporter that , together with LptB , functions to extract LPS from the IM en route to the OM ( Ruiz et al. , 2008 ) . However Narita and Tokuda ( 2009 ) have concluded that LptBFGC comprise an ABC transpoter ( 3 . A.1 ) that is required in outer membrance lipopolysaccharide sorting . While LptB is homologous to the ATP hydrolyzing subunits of ABC transporters , LptF ( COG0795 ) , LptG ( COG0795 ) and LptC ( DUF1239 ) are not demonstrably homologous to ABC type subunits . This raises some doubt but does not negate the evidence . <p> LptA-G ( 1 . B.42.1.2 ) form a bridge between the inner and outer membranes of gram-negative bacteria . Freinkman et al . ( 2012 ) used @ @ @ @ @ @ @ @ @ @ that give rise to the Lpt bridge and also showed that the formation of this transenvelope bridge can not proceed before the correct assembly of the LPS translocon in the OM . This ordered sequence of events may ensure that LPS is never transported to the OM if it can not be translocated across it to the cell surface . <p> LptD translocates LPS from the periplasm across the outer membrane ( OM ) . In E. coli , this protein contains two disulfide bonds and forms the OM LPS translocon with the lipoprotein LptE . Chng et al . ( 2012 ) identified seven in vivo states on the oxidative folding pathway of LptD . Proper assembly involved a nonfunctional intermediate containing nonnative disulfides . Intermediate formation required the oxidase DsbA , and subsequent maturation to the active form with native disulfides was triggered by LptE . Thus , disulfide bond-dependent protein folding of LptD requires the proper assembly of a two-protein complex in order to promote disulfide bond rearrangement ( Chng et al. , 2012 ) . 
@@97506162 @1706162/ 1450 @qwx861450 <p> Click to enlarge Overview of biomimetic neuroprosthetic system . Left to right : Information about what target to reach can be gathered from electrodes in the brain . This modulates ongoing activity in the biomimetic cortical and spinal cord models which then drives the virtual arm , which is then mirrored by the robot arm . Right to left : haptic feedback ( touch sensation ) could then be delivered back in the other direction so that the user could feel what is being touched . Reproduced with permission from Dura-Bernal et al . 2017 ( IBM Journal of Research and Development ) <p> By applying a novel computer algorithm to mimic how the brain learns , a team of researchers with the aid of the Comet supercomputer based at the San Diego Supercomputer Center ( SDSC ) at UC San Diego and the Centers Neuroscience Gateway has identified and replicated neural circuitry that resembles the way an unimpaired brain controls limb movement . <p> The research , published in the March-May 2017 issue of the IBM Journal of Research and Development , lays the groundwork @ @ @ @ @ @ @ @ @ @ that replicate brain circuits and their function -- that one day could replace lost or damaged brain cells or tissue from tumors , stroke , or other diseases . <p> " In patients with motor paralysis , the biomimetic neuroprosthetic could be used to replace the deteriorated motor cortex where it could interact directly with healthy brain pre-motor regions , and send commands and receive feedback via the spinal cord to a prosthetic arm , " said W.W. Lytton , a professor of physiology and pharmacology at State University of New York ( SUNY ) Downstate Medical Center in Brooklyn , N.Y. , and the studys principal investigator . <p> This scenario , portrayed in the IBM paper titled " Evolutionary algorithm optimization of biological learning parameters in a biomimetic neuroprosthesis " , required high-performance computing and expertise to simulate and evaluate potential computer models in an automated way , along with the Neuroscience Gateway ( NSG ) based at SDSC , which provided an entrance to these resources . <p> " The increasing complexity of the virtual arm , which included many realistic biomechanical processes , and the more @ @ @ @ @ @ @ @ @ @ sophisticated methods and highly parallel computing in a system such as Comet to tackle thousands of model possibilities , " said Amit Majumdar , director of the Data Enabled Scientific Computing division at SDSC , principal investigator of the NSG , and co-author of the IBM Journal paper . <p> " Combining these computational advantages can be an effective approach to build even more realistic biomimetic neuroprostheses for future clinical applications , " he added . <p> Fusing Computational and Biological Principles <p> Over the past decade or so , researchers have been trying to fuse computational and biological principles to create realistic computer models that would form the basis for silicon-based neural circuits or implants that would replace damaged brain tissue . In this emerging field , a primary goal has been the decoding of electrical signals recorded from the brain to move , for example , a prosthetic arm . In scenarios once considered science fiction , techniques that encode neural signals from a prosthetic virtual arm to the brain are now allowing users to feel what they are touching . <p> To get closer to this goal @ @ @ @ @ @ @ @ @ @ inspired by biology to create a more realistic artificial neural network that allows the motor cortex to learn to direct a virtual arm consisting of eight bones , seven joints and 14 muscle branches to a specified target.ere , H <p> The biomimetic model in question involved more than 8,000 spiking neurons and about 500,000 synaptic connections . The main component consisted of primary motor cortex microcircuits based on brain activity mapping , connected to a circuitry model of the spinal cord and the virtual arm . <p> " We argue that for the model to respond in a biophysiologically realistic manner to ongoing dynamic inputs from the real brain , it needs to reproduce as closely as possible the structure and function or actual cortical cells and microcircuits , " said Salvador Dura-Bernal , a research assistant professor in physiology and pharmacology with Downstate , and the papers first author . <p> As outlined , the researchers trained their model using spike-timing dependent plasticity ( STDP ) and reinforcement learning , believed to be the basis for memory and learning in mammalian brains . Briefly , the process refers @ @ @ @ @ @ @ @ @ @ on when they are activated in relation to each other , meshed with a system of biochemical rewards or punishments that are tied to correct or incorrect decisions . <p> In this case , reward signal is based on the ability of the computer model to control how close a virtual hand comes to a target . If the hand got close to the target , synapses generating that movement were rewarded ; if the hand was further away , those synapses were punished . <p> Identifying the best reinforcement learning model required the identification of an optimal set of characteristics or parameters ; among others , these include learning and exploratory movement rates , duration of training , and motor command threshold measured in spikes . <p> " Evolutionary " Algorithm Employed <p> To isolate reinforcement learning parameters that yielded the best control over a virtual arm , the researchers turned to " evolutionary algorithms . " The methodology follows the principles of biological evolution , where a population of individuals , each representing a set of genes or parameters , evolves over generations until one of them reaches @ @ @ @ @ @ @ @ @ @ are evaluated and selected for reproduction , produce new offspring by crossing their genes and applying random mutations , and subsequently are replaced by fitter offspring . <p> " Only the fittest individuals remain , " said Dura-Bernal , " those models that are able to learn better , survive and propagate their genes . " <p> Applied to this system , each individual gene represents a model with a particular set of learning parameters . The fitness of the individual is the ability of the model to learn to control the virtual arm based on real brain signals . <p> For their study , the researchers evolved a population of 60 individuals ( models with different learning parameters ) over 1,000 generations , where at every generation a new set of characteristics was evaluated to measure its fitness . Measuring the fitness required training the whole system ( pre-motor input , motor cortex , spinal cord , virtual arm ) for a period of time and then testing each of the two directions ( left and right ) to check how it performs . The process needed to be @ @ @ @ @ @ @ @ @ @ Years Using a Single Processor <p> To further increase the chances of finding an optimal solution , the researchers turned to an " island model " approach , which divided the total population of 60 individuals into sub-groups , each of which evolved independently . Again , following principles of biological evolution , an individual periodically would be migrated to a different island , introducing new genes to increase the chances of finding a new combination of genes with better fitness or performance . <p> Parallel HPC implementation of the evolutionary algorithm . A population of 60 individuals is divided into 6 islands and evolved independently . Each individual represents an instance of the neuroprosthetic system with a set of learning metaparameters . Individuals are periodically migrated between islands to increase diversity . Individuals are selected for reproduction and their genes are mutated to create new offspring . New individuals are evaluated to obtain their fitness values . Evaluation of fitness functions occurs in parallel in the HPC using PBS/SLURM , with each evaluation consisting of training the motor system model via reinforcement learning ( RL ) , and testing @ @ @ @ @ @ @ @ @ @ fittest individuals are allowed to survive after each generation , which results in the optimization of genes ( model metaparameters ) . Reproduced with permission from Dura-Bernal et al . 2017 ( IBM Journal of Research and Development ) <p> " Integrating the parallel island model on Comet required some work , but in the end we made it work to further speed the process , " said Subhashini Sivagnanam , a senior scientific computing specialist at SDSC , co-PI of the NSF-funded NSG project , and a co-author of the IBM Journal paper . <p> " Since thousands of parameter combinations need to be evaluated , this is only possible by running the simulations using HPC resources such as those provided by SDSC , " Dura-Bernal said . " We estimated that using a single processor instead of the Comet system would have taken almost six years to obtain the same results . " <p> Future studies will focus on developing even more realistic models of the primary motor cortex microcircuits to help understand and decipher the neural code how information is encoded and transmitted in the brain . @ @ @ @ @ @ @ @ @ @ , from the molecular , through the cellular , up to the network level , " said Dura-Bernal . " This will be instrumental in understanding and treating brain disorders such as epilepsy , schizophrenia , Parkinsons , motor paralysis , depression , or amnesia . " <p> Also participating in the research were : Samuel A. Neymotin , from Brown University ; Cliff Kerr , from the University of Sydney , Australia ; and Joseph T. Francis , from the University of Houston . <p> As an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community , including industry and academia . Cyberinfrastructure refers to an accessible , integrated network of computer-based resources and expertise , focused on , and health IT . SDSCs petascale Comet supercomputer continues to be a key resource within the National Science Foundations XSEDE @ @ @ @ @ @ @ @ @ @ 
@@97506163 @1706163/ <h> How do I learn about the Biology Workbench ? <p> Many helpful tutorials were developed by the Biology Student Workbench group , an EOT-PACI team that focused on biology and bioinformatics education . They also have a How To tutorial . If these documents do not help you with your problem , please send a message to bwbhelp@sdsc.edu . <p> To help reduce the chance of these errors occurring or affecting you , try the following : <p> Avoid large sessions : sessions that are larger , that is sessions with a lot of sequences , or with larger sequences in them . If a session is too large , the chances of an error occurring increase quite a bit . If you notice that your session is getting big , then it is time to delete unneeded sequences , or to create a new session . You can break up a larger session by repeatedly copying the larger session , then deleting what you do n't need from the copy . We realize this is inconvenient , but unfortunately it is the only way this can be @ @ @ @ @ @ @ @ @ @ used the " Back " button on your browser : the Biology Workbench stores information in the web pages themselves , so if you have added or deleted sequences , what is listed an old copy of that web page ( one you have reached by using " Back " ) is not going to be correct . The Workbench , in its confusion , may just end up deleting everything in the session . To avoid this , ALWAYS hit the " Refresh " button before you select any sequences , or toggle the tool type ( for example , if you are in Protein Tools go to Nucleic Tools , and then back to Protein Tools ) . <p> Copy your critical data : if you have recently ( that is , within the same day ) done a lot of work , copy the session to a backup . Yes , this does waste a little space , but it adds a lot of safety for you . Just do n't keep around too many copies -- when you make an updated copy please delete the last @ @ @ @ @ @ @ @ @ @ fight a slow network : if the network is very slow , data errors likely will occur . If you can not wait , be sure you have copies of your data . <p> Try to keep your Default Session very small ( or empty ) , and use alternate sessions for doing your work . If any session other than the Default Session is corrupt , you merely can not access that session . If the Default Session is corrupt , you can not log in to your account at all . <h> How safe is my data ? <p> Generally , pretty safe . Our disk is backed up every day or two to tape , to make restorations from catastrophic disk failures . In addition , we make local backups of the user session data . Sessions which have been changed are backed up nightly , and every month all sessions are backed up . This is why we can restore corrupted sessions in a relatively short amount of time . Data that has been added since the prior backup can not be restored , unfortunately . @ @ @ @ @ @ @ @ @ @ , we suggest that Biology Workbench users make local copies of their data . Because our Session files are not in a portable format , we do not suggest using " Download Session " for this purpose . Rather , saving the sequences should be done with the " Download " function or with the " Text format " link from within the View tool output . <h> Why is n't TEXSHADE returning a colored alignment ? <p> TEXSHADE crashes quite often , especially for large alignments . Try Boxshade if you can not get TEXSHADE to work . <h> Why is n't the Biology Workbench being improved ? <p> The Biology Workbench currently is not being funded ; it is being maintained by a very small portion of one person 's time as a service to the community . There is a proposal in its initial stages to obtain funding to completely rewrite the Workbench and use it as a framework to do many more things than the current Biology Workbench , but even if funding is obtained it will be a while before it is ready for public @ @ @ @ @ @ @ @ @ @ import large sequences , or a large number of smaller sequences ? <p> This is because the Biology Workbench was originally designed for work with protein sequences , and the large ( &gt;100K bp ) nucleotides create problems for the Biology Workbench core . An import of a large number of sequences can also cause the same problems . <p> Even if the Biology Workbench program could handle large sequences , few of the tools in the Biology Workbench are useful for genomic analysis , so in general one should be doing genomic analysis with other services . <h> How can I draw bootstrap values on the phylogenetic tree drawn in " CLUSTALTREE " ? <p> Unfortunately , the Drawtree and Drawgram tools from Phylip do not allow for bootstrap values to be printed on the branches of the trees . One needs to use their own picture/graphic editor to do this task . <h> What sort of phylogenetic analysis is done with the Biology Workbench tools ? <p> The answer is not a lot - implementing all of the Phylip tools would be a rather big undertaking for us @ @ @ @ @ @ @ @ @ @ the Phylip tools to display the neighbor-joining guide trees generated in the Clustal W multiple sequence alignment program . We offer this functionality as a service to our users , but suggest that people in need of detailed phylogenetic analysis look at other services or packages ( for example , installing Phylip on their local machines ) . <p> Occasionally we reindex our databases on different keys , and when that happens the key that is tied to the sequence in your session is not correct . Also , sometimes the databases can become corrupt . Try searching for that sequence and reimporting it if you really need this feature to work . If it still is failing after you try that , please submit a bug report . <h> Why is the Biology Workbench very slow ? <p> This could be due to many factors , but the most likely explanation is that there is a problem ( bottleneck ) in the Internet between your site and the San Diego Supercomputer Center . If you notice this happening quite often , it probably is worth sending a bug report @ @ @ @ @ @ @ @ @ @ on our end or on your end . <h> Why were the sequences I selected not the sequences that were used by the program I chose ? <p> In most cases , the way around this is to try not using the back button on your web browser when in the Biology Workbench . Be sure you always get back to the tool menus by hitting the " Return " , " Main " , or " Abort " buttons , and NOT by using the " back " function on your browser . <p> We store state variables in the web pages , and the location of those state variables may change . For example , if you import or add a sequence or alignment , and then use the back button to get back to the tool menu , the sequences you select on the screen may not be the sequences that actually are being selected . Using delete in this case can be disasterous . <p> There are cases where using the " back " function can be convenient . For example , if you are running @ @ @ @ @ @ @ @ @ @ of the settings from what you had before , you can do this relatively safely ( as long as you do not add , delete , or import during while doing this ) . We do have a " Change Settings " button which is a safer way of performing the same task , though . 
@@97506165 @1706165/ <h> Main menu <h> Welcome to the SyGMA lab webpage <p> The Synchrophasor Grid Monitoring and Automation ( SyGMA ) laboratory is a new facility located at the San Diego Supercomputer Center ( SDSC ) at the University of California San Diego ( UCSD ) . The SyGMA lab is a key player in the emerging technology on electric grid instrumentation and the research in this lab develops new data processing , modeling and model validation applications based on synchrophasor measurements for- advanced grid monitoring and automatic control of - electric networks . <h> The need for speed in phasor data processing <p> The intensification of distributed electric energy resources ( renewable energy ) and- energy storage systems ( such as batteries ) - motivates the need to monitor power flow and power quality more accurately in the electricity grid . Complementary to the traditional Supervisory Control And Data Acquisition ( SCADA ) systems , synchrophasor vector processing systems implemented in ( protection ) relays , digital fault recorders and specialized Phasor Measurement Units ( PMUs ) can produce distributed and time synchronized measurements of the power flow in @ @ @ @ @ @ @ @ @ @ sampling rates that follow the the main AC frequency . The enormous volumes of synchronized time stamped data produced at 60Hz sampling by PMUs provides a clear challenge for data management. - The SyGMA lab addresses these needs by the partnering with- OSIsoft- to use of dedicated data servers- to provide real-time data storage capabilities . <h> Partners for advanced grid monitoring and automation <p> Although the concept of a " smart grid " is often linked to the electical infrastructure , a true smart grid is only obtained when- the subsystems of the grid can work together . Subsystems may include distributed energy resources , loads ( appliances ) - and protection equipment that have to coordinate to provide a resilient power grid . Such resilence can be implemented- from the small scale of individual households to the larger scale of - microgrids to- the main grid by monitoring the performance and faults of the grid . <p> Phasor Measurement Units that produce- high sampling rates provides new opportunities for automatic power systems protection and currently the SyGMA lab partners with- National Instruments- to develop new testing , measurement @ @ @ @ @ @ @ @ @ @ lab also has acces to- microPMUs- for evaluation of event detection and real-time control algorithms . Current projects with- SDG&amp;E- on microgrid control and the opportunity to collaborate on real-time , dynamic modeling and testing of power systems using the Real Time Digital Simulator from- RTDS Technologies- in partnership with SDG&amp;E , further boost the capabilities of the SyGMA lab. 
@@97506166 @1706166/ <h> HPC Systems <p> For almost 30 years , SDSC has led the way in deploying and supporting cutting-edge high performance computing systems for a wide range of users , from the campus to the national research community . From the earliest Cray systems to todays data-intensive systems , SDSC has focused on providing innovative architectures designed to keep pace with the changing needs of science and engineering . <p> Whether you 're looking to expand computing beyond your lab or a business looking for that competitive advantage , SDSCs HPC experts will guide potential users in selecting the right resource , thereby reducing time to solution while taking your science to the next level . <p> Take a look at what SDSC has to offer and let us help you discover your computing potential . <h> Comet <h> Trial Accounts <p> Trial Accounts give potential users rapid access to Comet for the purpose of evaluating Comet for their research . This can be a useful step in accessing the usefulness of the system by allowing them to compile , run , and do initial benchmarking of their application prior @ @ @ @ @ @ @ @ @ @ Accounts are for 1000 core-hours , and requests are fulfilled within 1 working day . <h> HPC for the 99 Percent <p> Comet is SDSCs newest HPC resource , a petascale supercomputer designed to transform advanced scientific computing by expanding access and capacity among traditional as well as non-traditional research domains . The result of a National Science Foundation award currently valued at $21.6 million including hardware and operating funds , Comet is capable of an overall peak performance of two petaflops , or two quadrillion operations per second . <p> " Comet is really all about providing high-performance computing to a much larger research community what we call HPC for the 99 percent and serving as a gateway to discovery , " said Norman , the projects principal investigator . " Comet has been specifically configured to meet the needs of underserved researchers in domains that have not traditionally relied on supercomputers to help solve problems , as opposed to the way such systems have historically been used . " <p> Comet is configured to provide a solution for emerging research requirements often referred to as the long tail @ @ @ @ @ @ @ @ @ @ number of modest-sized , computationally-based research projects still represents , in aggregate , a tremendous amount of research and resulting scientific impact and advance . Comet can support modest-scale users across the entire spectrum of NSF communities while also welcoming research communities that are not typically users of more traditional HPC systems , such as genomics , the social sciences , and economics . <p> Comet is a Dell-integrated cluster using Intels Xeon Processor E5-2600 v3 family , with two processors per node and 12 cores per processor running at 2.5GHz . Each compute node has 128 GB ( gigabytes ) of traditional DRAM and 320 GB of local flash memory . Since Comet is designed to optimize capacity for modest-scale jobs , each rack of 72 nodes ( 1,728 cores ) has a full bisection InfiniBand FDR interconnect from Mellanox , with a 4:1 over-subscription across the racks . There are 27 racks of these compute nodes , totaling 1,944 nodes or 46,656 cores . <p> In addition , Comet has four large-memory nodes , each with four 16-core sockets and 1.5 TB of memory , as well as @ @ @ @ @ @ @ @ @ @ graphic processing units ) . The GPUs and large-memory nodes are for specific applications such as visualizations , molecular dynamics simulations , or de novo genome assembly . <p> Comet users will have access to 7.6 PB of storage as SDSCs Data Oasis parallel file storage system is substantially upgraded . The system is being configured with 100 Gbps ( Gigabits per second ) connectivity to Internet2 and ESNet , allowing users to rapidly move data to SDSC for analysis and data sharing , and to return data to their institutions for local use . By the summer of 2015 , Comet will be the first XSEDE production system to support high-performance virtualization at the multi-node cluster level . Comets use of Single Root I/O Virtualization ( SR-IOV ) means researchers can use their own software environment as they do with cloud computing , but can achieve the high performance they expect from a supercomputer . <p> Comet replaces Trestles , which entered production in early 2011 to provide researchers not only significant computing capabilities , but to allow them to be more computationally productive . <h> Gordon-Simons <h> Meeting @ @ @ @ @ @ @ @ @ @ HPC system built specifically for the challenges data-intensive computing . Debuting in early 2012 as one of the 50 fastest supercomputers in the world , it was the first HPC system to use massive amounts of flash-based memory . Gordon-Simons contains 300TB ( terabytes ) of flash-based storage along with large-memory " supernodes " based on ScaleMPs vSMP Foundation software . The standard supernode has approximately 1TB of DRAM , but larger memory configurations can be deployed as needed . <p> As a well-balanced resource between speed and large memory capabilities , Gordon-Simons is an ideal platform for tackling data-intensive problems , and designed to help advance science in domains such as genomics , graph analysis , computational chemistry , structural mechanics , image processing , geophysics , and data mining applications . The systems supernodes are ideal for users with serial or threaded applications that require significantly more memory than is available on a single node of most other HPC systems . Gordon-Simons flash-based I/O nodes offer significant performance improvement for applications that exhibit random access data patterns or require fast access to significant amounts of scratch space . @ @ @ @ @ @ @ @ @ @ storage system , providing researchers with a complete array of compute and storage resources . XSEDE Allocations on Gordon-Simons are no longer available . <h> ? TSCC Computing " Condo " <h> Affordable Computing for Campus &amp; Corporate <p> In mid-2013 SDSC launched the Triton Shared Computing Cluster ( TSCC ) after recognizing that UC San Diego investigators could benefit from an HPC system dedicated to their needs and with near-immediate access and reasonable wait times instead of accessing a national system entailing competitive proposals and often longer wait times . Following an extensive study of successful research computing programs across the country , SDSC selected the " condo computing " model as the main business model for TSCC . Condo computing is a shared ownership model in which researchers use equipment purchase funds from grants or other sources to purchase and contribute compute " nodes " ( servers ) to the system . The result is a researcher-owned computing resource of medium to large proportions . <p> By mid-2016 TSCC had 365 users across 27 labs/groups , with a total of 230 nodes ( approximately 4,000 processors ) and @ @ @ @ @ @ @ @ @ @ wide diversity of domains , including engineering , computational chemistry , genomics , oceanography , high-energy physics , and others . <h> TSCC Participant Info <h> Condo plan summary <p> The condo plan gives participants access to additional computing capability through the pooling of computing resources , offering participants significantly greater computational power and higher core counts than if limited to their own hardware or individual laboratory cluster . Researchers who contribute to the TSCC cluster have priority access to the nodes that they contribute ( via the home queue ) . In addition , they can run jobs on any available nodes , including hotel and other condo nodes ( via the condo queue ) . This effectively increases their computing capability and flexibility , which can be extremely valuable during times of peak research needs . <p> Condo participants may purchase general computing nodes , Graphics Processing Unit ( GPU ) nodes , or both . See the current price structure in the TSCC Node Expense Table ( costs and configurations are subject to change annually ) . The operations fee is supplemented for UCSD participants by the @ @ @ @ @ @ @ @ @ @ , administration hardware , and colocation fees . <p> Condo participants may take possession of their nodes and remove them from the cluster at any time ; however , once equipment is removed it can not be reinstalled in the TSCC . <p> Condo nodes come with a three-year warranty . After expiration , participants may continue to run their operational nodes in the TSCC for an additional year ( equipment failing in the fourth year may be idled without repair ) . At the end of four years , participants must take possession of or surplus their equipment . <h> Usage model <p> Charges for computing time are calculated on a Service Unit ( SU ) basis . One SU = 1 corehour of computing time on all queues for which charges are calculated . The cost in SUs is the same regardless of which actual nodes a job runs on . <p> Most of the system administration , user support , software licensing , and other operating costs are supplemented by the UCSD Administration . The system is housed at the San Diego Supercomputer Center on the UCSD @ @ @ @ @ @ @ @ @ @ Participation Status <p> Organization / Node Type <p> Hotel <p> Condo <p> UCSD Users <p> $0.025/SU <p> $0.015/SU ( equivalent ) <p> Other UC Campuses <p> $0.03/SU <p> Please inquire <p> Public <p> $0.06/SU <p> Please inquire <h> Condo <p> Each year , condo cluster participants will receive an allocation of cluster computing time proportional to the capacity of their purchased nodes . For example , a participant that purchases eight general computing nodes will receive just under 1.6 million SUs . This amount is based on 24x365 usage of 8 x 24 = 192 cores , allowing for 3% maintenance downtime . These corehours can be used any time during the year on any of the computing nodes . Unused corehours by condo participants expire at the end of each year . Note : heavy computing workloads may exhaust the annual allocation in less than one year . <h> Time-sharing on condo nodes <p> Condo jobs that require a number of cores less than or equal to their purchased nodes are guaranteed to start within eight hours of submission and can run for an unlimited amount of time . @ @ @ @ @ @ @ @ @ @ limit , while jobs that extend to other participants condo nodes have an eighthour time limit . See the Jobs section for submission details and examples . <p> Condo participants may submit " glean " jobs ( via the glean queue ) to run on idle computing nodes . These jobs are not charged against the submitter 's SU balance , but they may be terminated at any time by the scheduler if the nodes where they are running are needed to run higherpriority jobs . <p> Because the capabilities and purpose of the GPU nodes differ significantly from the general computing nodes , SUs received for contributed GPU nodes and general computing nodes can not be interchanged . <h> Terms of Participation <p> Participants in the Condo Program are requested to sign a Memorandum of Understanding ( MOU ) containing the basic terms of their participation in the program . <h> Hotel <p> Payasyougo jobs can only run on the hotel nodes . Currently , there are 48 general computing nodes ( 768 cores ) ; additional nodes may be added based on demand . Hotel nodes are configured @ @ @ @ @ @ @ @ @ @ The general computing nodes are allocated per-core , allowing up to 16 jobs to run on each node simultaneously . <h> Acceptable Use Policy <p> Download the Acceptable Use Policy . When using the Triton Shared Computing Cluster and associated resources , you agree to comply with these conditions . <h> Condo/Hotel Cost Details <h> Condo computing <p> The TSCC condo cost structure is based on condo participants purchasing their nodes , paying a onetime fee for their pro rata share of the common networking and storage infrastructure , and a modest annual operating expense that is subsidized by the UCSD Administration . <h> Hotel computing <p> Pay-as-you-go hotel users purchase cycles that reflect the total cost-of-ownership , albeit leveraging the economies of scale afforded by TSCC . For UCSD affiliates , the cost for the general computing hotel nodes is $0.025 per SU . The minimum hotel purchase is $250 ( 10,000 SUs ) . <h> Additional UCSD/Non-UCSD Cost Details <h> Cost for UCSD condo users <p> For condo participants , the primary cost is purchasing the computing nodes , plus a one-time fee of $939 per node to @ @ @ @ @ @ @ @ @ @ home file systems and the parallel file system . In addition , there is a modest IDC-bearing operations fee of $495/node/year , which will allow for ongoing operations , user services support , and expansion of the cluster . <h> Cost for non-UCSD hotel users <p> The TSCC is available to researchers from other UC campuses , other educational institutions and industry . Costs are competitive but higher than those cited above for UCSD researchers because the UCSD Administration is supplementing the program . Please get in touch with the TSCC Participant Contact for information on the rate structure for your organization . <h> TSCC User Documentation <p> The TSCC User Guide has complete information on accessing and running jobs on TSCC. 
@@97506167 @1706167/ <p> AbstractBACKGROUND : It has been well documented that the level of serum/plasma free triiodothyronine ( fT3 ) falls rapidly following brain death or during certain surgical procedures , for example , heart surgery carried out on cardiopulmonary bypass . The level in patients following cardiopulmonary bypass usually recovers within 2 days . <p> RESULTS : The mean level of fT3 in healthy na+ve baboons was 3.1 - 0.9 pg/ml and in healthy na+ve monkeys was 2.6 - 0.3 pg/ml . Following pig heart , liver , and artery patch xenoTx and monkey heart alloTx , there was an immediate rapid fall in fT3 level . Recovery of fT3 was more rapid in Groups 3 and 4 than in Groups 1 and 2 . In Group 1 , within 4 days fT3 had recovered , but only to the lower limit of normal range , where it remained throughout follow-up ( for up to 42 days ) . In Group 2 , no recovery was seen during the 7 days of follow-up . In immunosuppressed baboons with pig patch grafts that received IL-6R blockade ( n = 2 ) @ @ @ @ @ @ @ @ @ @ that received no IL-6R blockade ( n = 6 ) . <p> CONCLUSIONS : Following operative procedures , there is a dramatic fall in serum fT3 levels . The persistent low level of fT3 after pig heart and liver xenoTx may be associated with a continuing inflammatory state . We suggest that consideration should be given to the replacement of T3 therapy to maintain normal fT3 levels , particularly in nonhuman primates undergoing orthotopic pig heart or liver xenoTx. 
@@97506169 @1706169/ <p> AbstractResistance to insulin action is a key cause of diabetic complications , yet much remains unknown about the molecular mechanisms that contribute to the defect . Glucose-induced insulin resistance in peripheral tissues such as the retina is mediated in part by the hexosamine biosynthetic pathway ( HBP ) . Glucosamine ( GAM ) , a leading dietary supplement marketed to relieve the discomfort of osteoarthritis , is metabolized by the HBP , and in doing so bypasses the rate-limiting enzyme of the pathway . Thus , exogenous GAM consumption potentially exacerbates the resistance to insulin action observed with diabetes-induced hyperglycemia . In the present study , we evaluated the effect of GAM on insulin action in retinal M++ller cells in culture . Addition of GAM to M++ller cell culture repressed insulin-induced activation of the Akt/mTORC1 signaling pathway . However , the effect was not recapitulated by chemical inhibition to promote protein O-GlcNAcylation , nor was blockade of O-GlcNAcylation sufficient to prevent the effects of GAM . Instead , GAM induced ER stress and subsequent expression of the protein Regulated in DNA Damage and Development ( REDD1 ) , @ @ @ @ @ @ @ @ @ @ Akt on Thr308 . Overall , the findings support a model whereby GAM promotes ER stress in retinal M++ller cells , resulting in elevated REDD1 expression and thus resistance to insulin action . 
@@97506170 @1706170/ <h> CSE 151 Lecture Notes INTRODUCTION TO ARTIFICIAL INTELLIGENCE <h> AI : A Modern Approach , Chapter 2 : Intelligent Agents <h> Administrivia : <p> The lecturer 's office hours have changed to Wednesdays 2-4 PM . Please check the " What 's New ? " page on the CSE151 website frequently for news . <p> Last week we talked about definitions of Artificial Intelligence and decided to view the primary goal of AI as being to develop systems which act rationally . We call this the intelligent agent approach . <p> Today we will discuss how one can go about designing an intelligent agent . <h> Intelligent Agents <p> An agent perceives and acts in an environment . <p> An ideal agent takes action to maximize its expected performance given the percept sequence so far . <p> An autonomous agent is one which learns from its environment rather than relying completely on built in knowledge . <p> An agent program maps a sequence of percepts to a sequence of actions based on its internal model of the world . <p> PAGE is an intelligent agent design methodology that @ @ @ @ @ @ @ @ @ @ . <p> We discuss designing four types of intelligent agents : <p> table-driven agents which respond to the sequence of percepts seen so far , <p> reflex agents which respond immediately to single percepts , <p> goal-based agents which act to achieve goals , and <p> utility-based agents which act to maximize their " happiness " . <p> Environments can be classified as : <p> easier <p> harder <p> accessible <p> inaccessible <p> deterministic <p> nondeterministic <p> episodic <p> nonepisodic <p> static <p> dynamic <p> discrete <p> continuous <h> Intelligent Agent = architecture + program <p> An agent program is a function from percepts to actions . <p> The architecture is the hardware upon which the program runs . This architecture may be physical , or may be virtual as in the case of software agents ( softbots ) which live in a world of software . <p> The key advantage of this architecture is that the UPDATE-STATE function identifies " equivalence classes " of percepts : many different percepts correspond to the same environmental situation , from the point of view of what the agent should do . <p> @ @ @ @ @ @ @ @ @ @ to EVERY aspect of the environment . <p> The table of rules can be much smaller than the lookup table above . <p> " Goal-based " agent architecture <p> With reflex agents , the goal is implicit in the condition-action rules . With goal-based agents , we make the goal explicit . This allows us to <p> change the goal easily , and <p> use search and planning in deciding the best action at any given time . <p> Consider the following architecture for an intelligent agent : <p> function GOAL-BASED-AGENT ( percept , goal ) returnsaction <p> static:s , an action sequence , initially empty static:state , a description of the current world state <p> state state , percept ) ifs is empty then <p> s state , goal ) <p> action s , state ) s s ) returnaction <p> The heart of the goal based agent is the search function . <p> It returns a path from the current state to the goal--a plan . <p> The agent then returns the first action in the plan . <p> A simple enhancement would allow the agent to have @ @ @ @ @ @ @ @ @ @ Sometimes agents will have multiple conflicting goals . In this case , a utility function is more appropriate . <p> A utility function assigns a number proportional to the " happiness " of the agent to each state . <p> An ideal agent will attempt to maximize the expected value of the utility function , so it must know the probability of each of the possible outcomes of an action in a given state . Consider the following architecture for an intelligent agent : <p> function **27;961;TOOLONG , utility-fn ) returnsaction <p> static:s , an action sequence , initially empty static:state , a description of the current world state <p> state state , percept ) ifs is empty then <p> s state , utility-fn ) <p> action s , state ) s s ) returnaction <p> The SEARCH function differs from that of a goal-based agent in that it attempts to find the optimal , reachable state in terms of the expected value of the utility function . <p> Learning in intelligent agents <p> With the reflex architecture , if the table of rules prescribes the wrong action , and @ @ @ @ @ @ @ @ @ @ has automatically generalized from its specific experience . <p> All complex intelligent agents will have a lot of background knowledge preprogrammed , because they do not have the time to receive enough experience and feedback from the environment to allow them to learn to behave correctly starting from scratch . <p> In linguistics this is called the " poverty of stimulus " argument . If you calculate how many sentences a young child hears before it starts to speak correct English , the number is too few to allow it to " guess " the grammar of English . Therefore the baby must have a so-called universal natural language grammar preprogrammed into it by its genes . This argument is controversial , but there is scientific agreement that background knowledge of some sort ( often very hidden and implicit ) is necessary for learning in humans and AI systems . <p> Classification of environments We can classify the difficulty of dealing with an environment by asking whether it is : <p> accessible : is the full environment state directly observable ? <p> deterministic : does the same agent action in @ @ @ @ @ @ @ @ @ @ 
@@97506171 @1706171/ 1450 @qwx861450 <p> The Center for Applied Internet Data Analysis ( CAIDA ) at the San Diego Supercomputer Center ( SDSC ) has been awarded a $1.4 million grant from the U.S. Department of Homeland Security to demonstrate and illuminate structural and dynamic aspects of the internet infrastructure relevant to cybersecurity vulnerabilities , including macroscopic stability and resiliency analyses , grey markets for IPv4 addressing resources , and on-demand router-level topology inference . <p> " ( This ) project will deliver groundbreaking capabilities in Internet measurement and identification of infrastructure vulnerabilities , " said Ann Cox , program manager for Internet Measurement and Attack Modeling , a DHS program that supports researchers in academia and the cybersecurity community developing solutions in areas of resilient systems , modeling of internet attacks and network mapping and measurement . <p> " The internet has grown organically and there are many things yet to be discovered , " Dr. Cox added in a statement . " The knowledge developed in this project will enable better defenses for critical infrastructure that is tied to the internet . " <p> Specifically , the grant , @ @ @ @ @ @ @ @ @ @ Research , " will provide funds to CAIDA to expand the scale and capabilities of its secure measurement platform Archipelago ( Ark ) , which supports large-scale active measurement studies of the global internet . Among other outcomes , the project will achieve a systematic , continuous and complete knowledge base needed to monitor and analyze inter-domain route hijacking and other security-related phenomenon . SDSC is an Organized Research Unit of the University of California San Diego . <p> " We are honored to continue our support for DHS 's Cybersecurity Program under this new project , " said kc claffy , the grants principal investigator and founding director of CAIDA , a collaboration started in 1997 among commercial , government , and academic research sectors to promote greater cooperation in the engineering and maintenance of a robust , scalable global internet infrastructure . " DHS 's Science and Technology Directorate has consistently demonstrated the ability to identify and advance progress in areas of cybersecurity research that industry can not solve on its own . " <p> About CAIDA <p> The Center for Applied Internet Data Analysis ( CAIDA ) @ @ @ @ @ @ @ @ @ @ a collaborative undertaking among organizations in the commercial , government , and research sectors aimed at promoting greater cooperation in the engineering and maintenance of a robust , scalable global Internet infrastructure . <p> About SDSC <p> The San Diego Supercomputer Center ( SDSC ) is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community , including industry and academia . Cyberinfrastructure refers to an accessible , integrated network of computer-based resources and expertise , focused on , and health IT . SDSCs Comet joins the Centers data-intensive Gordon cluster , and are both part of the National Science Foundations XSEDE ( eXtreme Science and Engineering Discovery Environment ) program , the most advanced collection of integrated digital resources and services in the world . 
@@97506172 @1706172/ <h> Step 1 <p> The Download Tool can download coordinate and experimental data files , FASTA sequence files , and ligand data files for one or many PDB entries . After entering the IDs of interest , select the " Launch Download " button and you will be prompted to download and save locally a file called downloadrcsb.jnlp . For Chrome , the file must be downloaded and then opened . <h> Step 2 <p> Choose the option " Open with Java Web Start ( default ) " and check the box " Do this automatically for files like this from now on " . Then click OK . Occassionally , in order to get past this step , you may need to add a Security Exception to your Java settings . Read more info here . <h> Step 6 <h> Step 7 <h> Security Settings in System Preferences ( Mac ) <p> Based on the security settings on your machine , the following dialogue may appear . Please change the settings accordingly . The next images will guide you through changing the Security &amp; Privacy settings and launching @ @ @ @ @ @ @ @ @ @ By default , Mac only allows apps downloaded from Mac App Store . Click the lock icon at the bottom of left-hand-side corner . You need to enter the password to make changes . <h> Step 2 <p> Choose the option " Anywhere " under " Allow apps downloaded from " . Then click the button Allow From Anywhere to confirm the change . You may also want to click the lock icon again to prevent further changes . <h> Step 3 <p> Locate downloadrcsb.jnlp and double click it to launch the application . When you launch the RCSB Download Application for the first time , you will see the following message . Click Open to run the download application . 
@@97506173 @1706173/ 1450 @qwx861450 <h> UC San Diego to Develop Cyberinfrastructure for NASAs ICESat-2 Data <h> SDSCs OpenTopography Project Serving as Model <p> Published March 7 , 2017 <p> Enlarge ( Top left ) ICESat 91-day tracks across newly discovered subglacial Lake Engelhardt in West Antarctica . Tracks are color-coded by elevation changes between October 2003 and November 2005 . White asterisks locate tide-induced ice-flexure limits for the grounding line derived from ICESat repeat-track analysis . ( Top right/bottom left ) Repeat ICESat profiles along two tracks across the lake ( see top left panel for locations ) . Track 206 was an almost exact repeat of an 8-day track . ( Bottom right ) ICESat elevations against time at three orbit crossovers in the center of the lake including the 8-day data at crossover 1 in February/March 2003 . Image courtesy of Fricker , Helen Amanda ; Ted Scambos ; Robert Bindschadler ; Laurie Padman : " An Active Subglacial Water System in West Antarctica Mapped from Space . " Science 315 , no. 5818 ( 2007 ) : 1544-1548 . <p> The San Diego Supercomputer Center ( SDSC ) @ @ @ @ @ @ @ @ @ @ San Diego have been awarded a NASA ACCESS grant to develop a cyberinfrastructure platform for discovery , access , and visualization of data from NASAs ICESat and upcoming ICESat-2 laser altimeter missions . <p> ICESat and ICESat-2 ( scheduled for launch in 2018 ) measure changes in the volume of Earths ice sheets , sea-ice thickness , sea-level height , the structure of forest and brushland canopies , and the distribution of clouds and aerosols . <p> The new project , dubbed " OpenAltimetry " ( www.openaltimetry.org ) , will build upon technology that SDSC developed for its NSF-funded OpenTopography facility , which provides web-based access to high-resolution topographic data and processing tools for a broad spectrum of research communities . <p> OpenAltimetry , which includes the Boulder , CO-based National Snow and Ice Data Center ( NSIDC ) and UNAVCO as collaborators , also incorporates lessons learned from a prototype data discovery interface that was developed under NASAs Lidar Access System project , a collaboration between UNAVCO , SDSC , NASAs Goddard Space Flight Center , and NSIDC . <p> OpenAltimetrywill enable researchers unfamiliar with ICESat/ICESat-2 data to easily @ @ @ @ @ @ @ @ @ @ any area of interest . These capabilities will be heavily used for assessing the quality of data in regions of interest , and for exploratory analysis of areas of potential but unconfirmed surface change . <p> Possible use cases include the identification of subglacial lakes in the Antarctic , and the documentation of deforestation via observations of forest canopy height and density changes . <p> " The unique data generated by ICESat and the upcoming ICESat-2 mission require a new paradigm for data access , both to serve the needs of expert users as well as to increase the accessibility and utility of this data for new users , " said Adrian Borsa , an assistant professor at Scripps Institute of Geophysics and Planetary Physics and principal investigator for the OpenAltimetry project . " We envision a data access system that will broaden the use of the ICESat dataset well beyond its core cryosphere community , and will be ready to serve the upcoming ICESat-2 mission when it begins to return data in 2018 , " added Borsa . " Ultimately , we hope that OpenAltimetry will be the platform @ @ @ @ @ @ @ @ @ @ . " <p> This animation helps explain the dynamics of subglacial water exchange and how it is observed from space . It begins with a satellite image of the study area , followed by an artists concept of the Antarctic surface . The animation then moves down to a cross section of the ice sheet with lakes hidden deep beneath . As pressure is exerted on one lake , its water is forced to an adjacent lake , resulting in elevation changes at the surface over both lakes . Animation courtesy of NASA . <p> " OpenTopography has demonstrated that enabling online access to data and processing tools via easy-to-use interfaces can significantly increase data use across a wide range of communities in academia and industry , and can facilitate new research breakthroughs , " said Viswanath Nandigam , associate director for SDSCs Advanced Cyberinfrastructure Development Lab . Nandigam also is the principal investigator for the OpenTopography project and co-PI of OpenAltimetry . <p> On a broader scale , the OpenAltimetry project addresses the primary objective of NASAs ACCESS ( Advancing Collaborative Connections for Earth System Science ) program , @ @ @ @ @ @ @ @ @ @ usability of NASA 's earth science data using mature technologies and practices , with the goal of advancing Earth science research through increasing efficiencies for current users and enabling access for new users . <p> The project leadership team includes Co-I Siri Jodha Singh Khalsa from NSIDC , Co-I Christopher Crosby from UNAVCO , and Co-I Helen Fricker from Scripps . Additional SDSC staff supporting the project include Kai Lin , a senior research programmer ; and Minh Phan , a software developer . The OpenAltimetry project is funded under NASA ACCESS grant number NNX16AL89A until June 22 , 2018 . <p> About SDSC <p> As an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community , including industry and academia . Cyberinfrastructure refers to an accessible , integrated network of computer-based resources and expertise , focused on , and @ @ @ @ @ @ @ @ @ @ cluster , and are both part of the National Science Foundations XSEDE ( Extreme Science and Engineering Discovery Environment ) program . <p> About Scripps Institution of Oceanography <p> Scripps Institution of Oceanography at the University of California San Diego , is one of the oldest , largest , and most important centers for global science research and education in the world . Now in its second century of discovery , the scientific scope of the institution has grown to include biological , physical , chemical , geological , geophysical , and atmospheric studies of the earth as a system . Hundreds of research programs covering a wide range of scientific areas are under way today on every continent and in every ocean . The institution has a staff of more than 1,400 and annual expenditures of approximately $195 million from federal , state , and private sources . Learn more at scripps.ucsd.edu. 
@@97506174 @1706174/ <h> Resource to be used for Astrophysics , Biology , Materials Research <p> The San Diego Supercomputer Center ( SDSC ) at the University of California San Diego and the Simons Foundations Flatiron Institute in New York have reached an agreement under which the majority of SDSCs data-intensive Gordon supercomputer will be used by Simons for ongoing research following completion of the systems tenure as a National Science Foundation ( NSF ) resource on March 31 . <p> Under the agreement , SDSC will provide high-performance computing ( HPC ) resources and services on Gordon for the Flatiron Institute to conduct computationally-based research in astrophysics , biology , condensed matter physics , materials science , and other domains . The two-year agreement , with an option to renew for a third year , takes effect April 1 , 2017 . <p> Under the agreement , the Flatiron Institute will have annual access to at least 90 percent of Gordons system capacity . SDSC will retain the rest for use by other organizations including UC San Diego 's Center for Astrophysics &amp; Space Sciences ( CASS ) , as well @ @ @ @ @ @ @ @ @ @ for Applied Internet Data Analysis ( CAIDA ) , which is based at SDSC . <p> " We are delighted that the Simons Foundation has given Gordon a new lease on life after five years of service as a highly sought after XSEDE resource , " said SDSC Director Michael Norman , who also served as the principal investigator for Gordon . " We welcome the Foundation as a new partner and consider this to be a solid testimony regarding Gordons data-intensive capabilities and its myriad contributions to advancing scientific discovery . " <p> " We are excited to have a big boost to the processing capacity for our researchers and to work with the strong team from San Diego , " said Ian Fisk , co-director of the Scientific Computing Core ( SCC ) , which is part of the Flatiron Institute . <p> David Spergel , director of the Flatiron Institutes Center for Computational Astrophysics ( CCA ) said , " CCA researchers will use Gordon both for simulating the evolution and growth of galaxies , as well as for the analysis of large astronomical data sets . @ @ @ @ @ @ @ @ @ @ computational problems . " <p> Simons Array and Simons Observatory <p> The POLARBEAR project and successor project called The Simons Array , led by UC Berkeley and funded first by the Simons Foundation and then in 2015 by the NSF under a five-year , $5 million grant , will continue to use Gordon as a key resource . <p> " POLARBEAR and The Simons Array , which will deploy the most powerful CMB ( Cosmic Microwave Background ) radiation telescope and detector system ever made , are two NSF supported astronomical telescopes that observe CMB , in essence the leftover heat from the Big Bang in the form of microwave radiation , " said Brian Keating , a professor of physics at UC San Diegos Center for Astrophysics &amp; Space Sciences and a co-PI for the POLARBEAR/Simons Array project . <p> " The POLARBEAR experiment alone collects nearly one gigabyte of data every day that must be analyzed in real time , " added Keating . " This is an intensive process that requires dozens of sophisticated tests to assure the quality of the data . Only by leveraging resources @ @ @ @ @ @ @ @ @ @ legacy of success . " <p> Gordon also will be used in conjunction with the Simons Observatory , a 5-year $40 million project awarded by the Foundation in May 2016 to a consortium of universities led by UC San Diego , UC Berkeley , Princeton University , and the University of Pennsylvania . In the Simons Observatory , new telescopes will join the existing POLARBEAR/Simons Array and Atacama Cosmology Telescopes to produce an order of magnitude more data than the current POLARBEAR experiment . An all-hands meeting for the new project will take place at SDSC this summer . A video describing the project can be viewed by clicking the image below . <p> Click image to play video <p> Delivering the Data <p> The result of a five-year , $20 million NSF grant awarded in late 2009 , Gordon entered production in early 2012 as one of the 50 fastest supercomputers in the world , and the first one to use massive amounts of flash-based memory . That made it many times faster than conventional HPC systems , while having enough bandwidth to help researchers sift through tremendous amounts @ @ @ @ @ @ @ @ @ @ within NSFs XSEDE ( Extreme Science and Engineering Discovery Environment ) project . The system will officially end its NSF duties on March 31 following two extensions from the agency . <p> By the end of February 2017 , Gordon had supported research and education by more than 2,000 command-line users and over 7,000 gateway users , primarily through resource allocations from XSEDE . One of Gordons most data-intensive tasks was to rapidly process raw data from almost one billion particle collisions as part of a project to help define the future research agenda for the Large Hadron Collider ( LHC ) . Gordon provided auxiliary computing capacity by processing massive data sets generated by one of the LHCs two large general-purpose particle detectors used to find the elusive Higgs particle . The around-the-clock data processing run on Gordon was completed in about four weeks time , making the data available for analysis several months ahead of schedule . <p> About the Simons Foundation <p> The Simons Foundations mission is to advance the frontiers of research in mathematics and the basic sciences , supporting discovery-driven scientific research . Co-founded in @ @ @ @ @ @ @ @ @ @ foundation celebrated its 20th anniversary in 2014 . The Foundation makes grants in four program areas : mathematics and physical sciences , life sciences , autism research , and education and outreach . In 2016 the Foundation launched an internal research division called the Flatiron Institute , a multidisciplinary institute focused on computational science . <p> About SDSC <p> As an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community , including industry and academia . Cyberinfrastructure refers to an accessible , integrated network of computer-based resources and expertise , focused on , and health IT . SDSCs petascale Comet supercomputer continues to be a key resource within the National Science Foundations XSEDE ( Extreme Science and Engineering Discovery Environment ) program . 
@@97506179 @1706179/ 1450 @qwx861450 <h> SDSC to Double Comet Supercomputers Graphic Processor Count <h> Resource to be a Top Provider of GPU Computing to National Research Community <p> Published May 2 , 2017 <p> SDSCs Petascale Comet Supercomputer . Credit : Ben Tolo , SDSC <p> The San Diego Supercomputer Center ( SDSC ) at the University of California San Diego has been granted a supplemental award from the National Science Foundation ( NSF ) to double the number of graphic processing units , or GPUs , on its petascale-level Comet supercomputer in direct response to growing demand for GPU computing across a wide range of research domains . <p> Under the supplemental NSF award , valued at just over $900,000 , SDSC is expanding the high-performance computing resource with the addition of 36 GPU nodes , each with four NVIDIA P100s , for a total of 144 GPUs . This will double the number of GPUs on Comet from the current 144 to 288 . The nodes are being provided by Dell , the vendor and co-design partner for Comet . They are expected to be in production by early @ @ @ @ @ @ @ @ @ @ provider of GPU resources available to the NSF-funded Extreme Science and Engineering Discovery Environment ( XSEDE ) , a national partnership of institutions that provides academic researchers with the most advanced collection of digital resources and services in the world . Prior to this award , the NSF granted SDSC a total of $24 million to develop and operate Comet , which went into production in mid-2015 . <p> Once used primarily for video game display graphics , todays much more powerful GPUs have been developed that have more accuracy , speed , and accessible memory for more scientific applications that range from phylogenetics and molecular dynamics to creating some the most detailed seismic simulations ever made to better predict ground motions to save lives and minimize property damage . <p> " This expansion is reflective of a wider adoption of GPUs throughout the scientific community , which is being driven in large part by the availability of community-developed applications that have been ported to and optimized for GPUs , " said SDSC Director Michael Norman , who is also the principal investigator for the Comet program . <p> Francis @ @ @ @ @ @ @ @ @ @ the first detector of its kind designed to observe the cosmos from deep within the South Pole ice , welcomes the new GPU addition . <p> " The IceCube neutrino detector transforms natural Antarctic ice at the South Pole into a particle detector , " said Halzen , also a physics professor at the University of Wisconsin-Madison . " Progress in understanding the precise optical properties of the ice leads to increasing complexity in simulating the propagation of photons in the instrument and to a better overall performance of the detector . " <p> " This expansion will help the XSEDE organization meet increased demand for GPU resources from these areas , as well as prepare for research in new areas , such as machine learning , which has become increasingly important for a wide range of research in areas including image processing , bioinformatics , linguistics , and others , " said SDSC Director of Scientific Applications and Comet Co-PI Bob Sinkovits . <p> The P100 is NVIDIAs newest GPU . SDSC benchmarking tests show that for some applications the GPUs achieve speed-ups of two times over the K80 @ @ @ @ @ @ @ @ @ @ be added to the existing Comet GPU nodes and become a separately allocable resource . <p> The amended NSF award number for Comet , including the GPU additions , is FAIN 1341698 . The award is estimated to run until March 30 , 2020 . <p> About SDSC <p> As an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community , including industry and academia . Cyberinfrastructure refers to an accessible , integrated network of computer-based resources and expertise , focused on , and health IT . SDSCs petascale Comet supercomputer continues to be a key resource within the National Science Foundations XSEDE ( Extreme Science and Engineering Discovery Environment ) program . 
@@97506182 @1706182/ <h> Welcome to Mouse Encode Project at Ren Lab ! <p> Data can be downloaded and viewed in ENCODE website . The raw data has also been deposited to GEO with the GEO accession I 'd GSE29184 . <p> Below are the links to the predicted cis-regulatory elements in each tissue/cell type . To map promoters , we relied exclusively on the presence of H3K4me3 . To identify enhancers , we took advantage of the chromatin signature pattern that they share , i.e. the presence of H3K4me1 but absence of H3K4me3 , and developed a chromatin-signature based enhancer predictor using the distal P300 binding sites in mESCs as training data set . To identify potential insulator elements , we determined the binding sites of CTCF in each tissue . For details , please refer to the the methods section of the paper . <p> We also determined the transcriptome in each tissue or cell type through RNA-Seq experiments using a protocol that permits the detection of both abundance and strandedness of RNA transcripts . We mapped raw reads in FASTQ format to the mouse genome with TopHat software and the @ @ @ @ @ @ @ @ @ @ expression value(FPKM) for each gene in RefSeq with Cufflinks software . Note , if you want to remap the RNA-Seq reads by yourself , please remove the first 6 bases ( sequencing barcode ) from the FASTQ files for samples in GSM8509XX series ( SRR392604-SRR392619 ) . <p> Listed below are the tissue-specific enhancers used in Figure 4D and the downstream motif/Go term analysis . To cluster the enhancers , we first combined the predicted enhancers from all the tissue/cell types , and the progressively merged enhancers within 1.5 kb . The values in each bin in Figure 4D is the normalized H3K4me1 intensity for +-1.5 kb around the center of the predicted enhancers . Cluster 3.0 was used to generated the final heatmap. 
@@97506184 @1706184/ <h> Poseview Image of NDG in 1OQO <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NAG in 1OQO <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MAN in 1OQO <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of BMA in 1OQO <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of FUC in 1OQO <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of FUL in 1OQO <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506186 @1706186/ <h> Abstract <p> A former marijuana cultivation site adjacent to the Shasta/Trinity National Forest . The site was illegally clear cut and graded to create the garden site , causing significant erosion as well as fragmentation of natural habitat . Photo by Jennifer Carah , TNC . <p> A former marijuana cultivation site adjacent to the Shasta/Trinity National Forest . The site was illegally clear cut and graded to create the garden site , causing significant erosion as well as fragmentation of natural habitat . Photo by Jennifer Carah , TNC . <p> The liberalization of marijuana policies , including the legalization of medical and recreational marijuana , is sweeping the United States and other countries . Marijuana cultivation can have significant negative collateral effects on the environment that are often unknown or overlooked . Focusing on the state of California , where , by some estimates , 60%70% of the marijuana consumed in the United States is grown , we argue that ( a ) the environmental harm caused by marijuana cultivation merits a direct policy response , ( b ) current approaches to governing the environmental effects @ @ @ @ @ @ @ @ @ @ the environmental impacts of cultivation when shaping future marijuana use and possession policies represents a missed opportunity to reduce , regulate , and mitigate environmental harm . 
@@97506187 @1706187/ <h> longtable example <p> When you have a table that spans more than one page , the longtable package can help you out . It allows you to specify the column headings such that it prints on each page . Also , you can add a caption on each continued page to indicate that it the table is continued from the previous page . Similarly , you can add a footer to indicate that a table will be continued on the following page . Other than that , the longtable syntax is identical to the regular table environment . The following table spans more than one page : 
@@97506189 @1706189/ 1450 @qwx861450 <h> SDSC to Hold Annual Winter Forum on Data <p> The Center for Large-Scale Data Systems ( CLDS ) at the San Diego Supercomputer Center ( SDSC ) will hold its annual Winter Forum on Data next month , bringing together a wide range of experts to discuss emerging opportunities for businesses involved in all aspects of data governance . <p> The December 14-15 Data West forum , to be held on the UC San Diego campus in the Robinson Auditorium at the School of Global Policy and Strategy ( GPS ) and SDSC , will cover the latest advances in data analytics , data risks and liabilities , cyber-physical systems and large-scale data system agility and performance . Early-bird registration has been extended through Sunday , December 4 . Conference topics include : <p> Valuing data ? <p> Data entrepreneurship <p> IoT and the next-generation internet <p> Smart cities <p> Democratizing Data Science <p> Trust , Risk and Cybersecurity <p> The Data West forum will include presentations from Robert Foster , CIO of the Department of the Navy , Paul Barth , Founder and CEO of @ @ @ @ @ @ @ @ @ @ Ampool ; Avi Kalderon , Managing Partner with New Vantage Partners ; and Marco Pacelli , Founder and CEO of Clickfox . <p> As a center of excellence based at the San Diego Supercomputer Center on the UC San Diego campus , CLDS conducts field-based research into the technologies and management practices defining success in the digital economy . CLDS researchers study a wide range of end-to-end problems in large-scale data , from data creation to ingesting , processing and storing data effectively , to understanding the value , management and economic benefits of data . CLDS research is conducted in active participation with partners in government , industry , and scientific organizations . <p> About SDSC <p> As an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community , including industry and academia . Cyberinfrastructure refers to an accessible , integrated network of computer-based resources and expertise , focused on , and health IT . SDSCs Comet joins the Centers data-intensive Gordon cluster , and are both part of the National Science Foundations XSEDE ( Extreme Science and Engineering Discovery Environment ) program . 
@@97506190 @1706190/ <h> Survey Analysis via Visual Exploration <p> SuAVE proved to be a useful tool teaching undergraduate research methods in sociology at UCSD for the last two years , as many students came to statistics classes with little math background and a lot of anxiety . SuAVE 's user-friendly , visual approach to statistical analysis enabled students to explore simple social science questions with survey data in an intuitive way , without having to master first the mathematical tools of statistics . Its unique capacity to animate transitions between the big picture and individual cases with great ease was critical to explaining statistical approaches to students , especially in presenting the relationship between small N , case based , idiographic , comparative arguments , and large N , variable based , nomothetic explanations . <p> The visual approach implemented in SuAVE allows students to scrutinize cases by looking up all relevant information about a specific case . This proves to be helpful in three ways : <p> it allows the student to look more closely at unusual values in univariate distributions , to find out whether these are true outliers @ @ @ @ @ @ @ @ @ @ outliers in bivariate distributions of correlated indicators of the same concept to better understand the ability of individual indicators to capture underlying concepts . <p> using deviant case analysis to discover causal explanations with SuAVE , students learn that , rather than be treated as random noise , exceptions to patterns can teach us a lot about the social world . <p> Here are selected student comments from the first two classes taught by Prof. Akos Rona-Tas in 2015 and 2016 ( SOCIOLOGY 103M " Computer Applications to Data Management in Sociology " ) : <p> I really liked used SuAVE . I think the visualization portion of it was extremely useful and I learned a lot from using it . In addition , it was very user friendly and easy to navigate . <p> It was cool to see how you can find specific information about one respondent . The way to categorize respondents and understand odd findings was interesting because you would sometimes expect the opposite . <p> I really enjoyed SuAVE , it made the analyzing procedure easier by allowing us to look at multiple variables at once . 
@@97506192 @1706192/ <h> PmagPy Cookbook <p> June 13 , 2017 <p> Dear Reader , <p> This documentation is updated from that in the book entitled Essentials ofPaleomagnetism by Tauxe et al. , ( 2010 ) . This cookbook was designed as a companion website to the book Essentials of Paleomagnetism , 4th Web Edition . Chapter references to this companion book are , for example , " Essentials Chapter 1 " . <p> There are many chefs who contributed to this work , in particular , the MagIC Database Team ( Cathy Constable , Anthony Koppers , Rupert Minnett , Nick Jarboe , Ron Shaar , and Lori Jonestrask ) . Nick Swanson-Hysell ( UC Berkeley ) contributed the demaggui and Jupyter notebook documentation . The PmagPy project is supported by grants from the National Science Foundation . <p> If you only want to use the GUIs , you can download them as standalone programs . The standalone install does n't  require that you have Python , but is more limited than the full PmagPy install . <p> If you want access to all of PmagPys functionality , you must first @ @ @ @ @ @ @ @ @ @ pip utility to download and install the PmagPy function library and the PmagPy command line programs or manually download and install by setting environment variables yourself . <p> If you want to actively participate in developing and modifying PmagPy , you should do a developer install . <p> If you do not need the full PmagPy functionality , and you only want to use Pmag GUI , MagIC GUI , Thellier GUI , and Demag GUI , there is now a standalone download for you . You wont need to install Python for this . <p> Follow the download instructions in the provided links , and then you can skip the " Full install " section and go straight to using Pmag GUI or MagIC GUI . The standalone versions of these applications are still in development ; please report problems to the PmagPy team by creating an issue on Github . <p> To get started , download the zip file and put the resulting folder on your desktop . Inside the PmagPy-Standalone folder you will have one folder each for Pmag GUI and MagIC GUI . Open the appropriate @ @ @ @ @ @ @ @ @ @ security settings , you may have to right click the icon and then select " ok " the first time you open it ) . <p> Get started by downloading the zip file ( see links below ) and put the resulting folder wherever you wish . You will need to extract all files . Inside the PmagPy-Standalone folder you will find icons for Pmag GUI and MagIC GUI . Double click the program you wish to use and you should be able to get started . <p> This binary has only been tested on a Ubuntu 14.04 ( Trusty ) distribution and might experience problems on other distributions . You can simply clone the standalone repository or download and unzip . The GUIs should run when you double click the executable , but will take time to start up ( anywhere from 5 to 30 seconds ) please be patient . <p> To get the full use of PmagPy functionality , you will first have to install Python . <p> If you have already installed Canopy Python or Anaconda Python , you can skip this section and go straight @ @ @ @ @ @ @ @ @ @ be aware that they come with a version of Python already installed ; but this pre-installed version does NOT have everything you will need to run PmagPy , so you will still need to download a scientific Python distribution . PmagPy can be run with either Python 2 or Python 3 , but if starting from scratch , we recommend that you install Anaconda Python 3 . <p> Download and install Anaconda Python 3 . During the install , you will see Advanced Options . Select both , including : " Add Anaconda to my PATH environment variable " ( this is listed as not recommended , but is useful ) . <p> Anaconda provides a handy cheat sheet with information about how to install and update Python packages , as well as create custom Python environments and more . <p> To customize your Python ( and use PmagPy programs ) , you will need to find your command line . This is Terminal for OS X and Command Prompt for Windows . ( Note for Windows users : if you did not add Anaconda Python to your PATH as @ @ @ @ @ @ @ @ @ @ Anaconda Prompt " instead ) . <p> To get started , you will need to download a few non-default Python 3 packages . Open your command line and run the following commands : <p> Here we provide instructions for using pip to install PmagPy from the command line . " Pip " is a package manager that comes standard with any modern Python distribution ( NB : you will have to update it to the most recent version ) . Pip allows you to install and uninstall pmagpy , the package with all the low-level functionality of PmagPy and pmagpy-cli which provides the PmagPy command-line-interface and has all the packages you may use to download/upload , visualize , and manipulate data . <p> Now your installation should be complete . There are a few more details : <p> Testing your installation : <p> To make sure everything is working , return to your command line . OSX users , run the command eqarea.py -h . Windows users : you will need to use eqarea -h In fact , you will drop the " . py " anytime you are calling @ @ @ @ @ @ @ @ @ @ the help message , check the Trouble Shooting section . If you do n't  find a solution there , please report the error message and the Python distribution you are using , including the version number , on the PmagPy Github page . <p> Next , make sure the GUIs are working . <p> For Windows users : pmaggui <p> For Mac users with Anaconda Python : pmagguianaconda . <p> For Mac users with a different Python : pmaggui.py <p> If you do n't  see the Pmag GUI window , make sure you have followed all the install directions correctly , and check the Trouble Shooting section . If you are stumped , please create a Github issue . <p> Accessing example data files : <p> There are many data files used in the examples of programs and for use with the textbook Essentials of Paleomagnetism . You may want to copy the data files to your Desktop or another convenient location . To do this , navigate on the command line to your destination folder ( for help , see this section ) . Then , use the command @ @ @ @ @ @ @ @ @ @ of the PmagPy example files to your current directory ( represented by " . " ) . <p> We have attempted to support both Canopy and Anaconda Python . However , there are some differences in the way Anaconda Python interacts with the screen . Because of this , Anaconda users will need to use a different command to access the PmagPy GUIs . All four GUIs Pmag GUI , MagIC GUI , Demag GUI , and Thellier GUI can be invoked with on a Mac with Anaconda python . However , instead of calling them in the standard way on the command line : <p> pmaggui.py <p> You will use : <p> pmagguianaconda <p> If you try to use the first syntax , you will see an error message like this : <p> This program needs access to the screen . Please run with a Framework build of python , and only when you are logged in on the main display of your Mac . <p> If you are a developer , and wish to download PmagPy from the Github repository , you may want to add PmagPy directly @ @ @ @ @ @ @ @ @ @ If you 're wondering whether you should do a pip install or a developer install , it depends on what you 're hoping to do with PmagPy . The benefit of a regular pip installation is that you will get a tested , stable version of PmagPy . You can install everything you need quickly and easily , and you 'll be able to use the full functionality . The benefit of a developer installation is that you will be able to stay up-to-date with what other developers are working on . You will also be able to make changes and use those changes immediately . The downside is that the developer install may be buggier and requires to edit your $PATH . <p> If you want to contribute to PmagPy or modify PmagPy programs for your own use , you should do a developer install , otherwise go with the regular pip install . The developer install follows these steps : <p> devsetup.py edits your $PATH and $PYTHONPATH variables . If you want to look at or edit those variables directly , see this information about setting your PATH . <p> After @ @ @ @ @ @ @ @ @ @ your command line . After restarting , you should be able to use all PmagPy functionality as if you had done a full pip install . The only difference is that you will be able to stay up-to-date with PmagPy development and make edits in PmagPy code which will be immediately available in your system . <p> Run python devsetup.py install For more information on how to install , run python devsetup.py -h <p> After completing the developer install , you will need to restart your command line . After restarting , you should be able to use all PmagPy functionality as if you had done a full pip install . The only difference is that you will be able to stay up-to-date with PmagPy development and make edits in PmagPy code which will be immediately available in your system . <p> If you care about the details , here is what the developer install script actually does . It adds these lines editing $PATH to your . bashrc file : <p> Pmag GUI is a Graphical User Interface ( GUI ) written by Lori Jonestrask and Ron Shaar that @ @ @ @ @ @ @ @ @ @ The work flow is illustrated schematically here : <p> Note : Pmag GUI is available for use with both 3.0 and 2.5 data . All new contributions should be built up using 3.0 , but 2.5 may be used for legacy data sets and then later upgraded using on the MagIC site . You may specify which data model you want to use on the command-line using the switch " -DM " , or in the window that pops up when you first open PmagGUI . <p> The current tutorial is for the 2.5 format , but the usage is very similar . We will update with new , 3.0 pictures soon ! <p> When you invoke Pmag GUI , the first step is to change directories into a Project Directory . For each study , create a directory with a name that relates to that study . Here we will call it ThisProject . This is where you will collect and process all the rock and paleomagnetic data for a given study , usually a publication . The project directory name should have NO SPACES and be placed on @ @ @ @ @ @ @ @ @ @ in the path . Under certain Windows versions , this means you should not use your home directory , but create a directory called for example : D : MyPmagProjects and put ThisProject there . <p> copy example files : In the datafiles folder you will find a subfolder named PmagGUI . Copy the contents of the ThisProject directory into your own Project Directory folder . <p> Start Pmag GUI either by typing pmaggui.py on your command line , or ( if you have the standalone ) , by double-clicking the Pmag GUI icon on your Desktop . <p> Change directories by clicking on the " change directory " button and select your own ThisProject directory . <p> Pmag GUI allows for converting many different laboratory formats . For the complete set of instructions , see the section on Supported Rock Magnetometer files . For this example , we will use a generic file format . An example for a generic file format is shown here : <p> To learn what all the column headers mean look at the documentation for genericmagic.py . <p> In your ThisProject directory there are @ @ @ @ @ @ @ @ @ @ data of specimens from site sr01 ( a lava flow site ) of Tauxe et al . ( 2004 ) . There is also a file with sample orientation , location and other metadata typical for a paleomagnetic study . <p> Press the Convert magnetometer files to MagIC format button . If no menu pops up or the window is blank , click on the Python icon ( the little space ship ) on your dock . You should see a dialog window will appear with different file formats : <p> Click on generic format and press the import file button . A new dialog box will appear . For more details click on the help button . <p> In the dialog box , click on the Add button and choose one of the measurements files . <p> Optional : Insert your EarthRef user name . <p> Choose your experiment type from the dropdown list . <p> Choose specimen-sample naming convention . In this example , specimen sr01a1 belongs to sample sr01a so the specimen-sample naming convention is no. of terminal characters and the delimiter/number field is 1 ) . @ @ @ @ @ @ @ @ @ @ sample sr01a belongs to site sr01 so the sample-site naming convention is no. of terminal characters with a delimiter/number of 1 . <p> Note : a location is a stratigraphic section , a sampling region , an drill core , and so on . MagIC does n't  really care what your location name is , but use the same location name every time you are asked for it , because it really ties your dataset together and is required within the data model . <p> Your dialog boxes should look like this for the AF and thermal data : <p> and like this for the paleointensity data . For paleointensity data , you must also supply the lab field in micro tesla ( 40 ) and orientation relative to samples X direction : 0 90 . <p> Press OK to create a new MagIC measurement file , which is saved in your ThisProject directory . <p> After converting all files to MagIC format , press the Next Step button in the convert magnetometer files dialog box . All files with the . magic suffix will be added to the list @ @ @ @ @ @ @ @ @ @ or adding additional MagIC formatted files . You should see a list of the three magic files : <p> Click the OK button . The three files will be combined to a single file , named magicmeasurements.txt and also stored in your ThisProject directory . <p> Pmag GUI provides an optional tool for calculating geographic and tilt-corrected directions if those directions were not part of the original data files . To use this tool click on the button labeled calculate **25;990;TOOLONG directions . <p> An empty template of a file named SrExampleorient.txt was created in the MagIC Project Directory . This file is displayed in a Python window . You can fill in this file manually using the Python window or with with a spreadsheet program ( recommended ) . The empty template has now only the sample names derived from your measurement files , using the naming rules that you chose . <p> To see an example of a filled in one , pull down the menu bar file ? open orientation file and choose the file SrExampleorient.txt from MyFiles folder . <p> Click the button ? Save orientation @ @ @ @ @ @ @ @ @ @ . <p> Fill in the dialog box like this : <p> Orientation convention : Pomeroy . <p> Declination correction : Use the IGRF . <p> Orientation priority : #1 . <p> Put in the number of hours to SUBTRACT from the local time to get to Greenwich Mean Time : -6 . Local time was 6 hours behind GMT for this example . <p> Filling in the Earth Ref data is a critical part of building a MagIC Project Directory . The Earth-Ref data relevant to this example are arranged in five er* tables : erspecimens , ersamples , ersites , erlocations , erages . To complete the ER data , click the button , and follow the directions in the help window in order : <p> Step 0 : Choose the appropriate headers for each of your er* tables . Required headers ( or headers already present in your er* tables ) show up in the top box , optional headers in the bottom . In our example , the only header you should add is " agesigma " in the age table . Once you have added all @ @ @ @ @ @ @ @ @ @ the next step . <p> The next six steps contain editable grids . For many columns , you can edit using drop-down menus that provide controlled vocabularies . For others , you must manually enter data into the cell . A single left click will bring up a drop-down menu ; a double-left click will call up the cell editor . With both types of data entry , it is possible to select a single value for the entire column . Simply click on the column label . If that column has a drop-down menu , you can then click on any cell in the column , the appropriate menu will pop up , and the value you select will propagate throughout the column . Edits in that column will continue to be global until you select another column to edit or you de-select the column by clicking on the column label again . If you select a column that does not have a drop-down menu , a text entry dialog will pop up , and the value you provide will be applied to the column . <p> Step 1 @ @ @ @ @ @ @ @ @ @ may not rename specimens , but you may reassign them to a different sample . It is also possible to add additional samples , using the Add new sample button . Note that specimentype , specimenlithology , and specimenclass columns do not show up . They will propagate down when you select sampletype , samplelithology , and sampleclass in step 4 . <p> Step 2 : Update the relationship between samples and sites . You may rename samples , or you can reassign them to a different site . You can also add additional sites using the " Add a new site " button . You will be able to update other columns in the ersamples file in step 4 . <p> Step 3 : Update the relationship between sites and locations . You may rename sites , or you can reassign them to a different location . You can add additional locations using the " Add a new location " button . You must choose a legal entry from the controlled vocabularies for the following headers : " siteclass " , " sitelithology " , " sitetype " . @ @ @ @ @ @ @ @ @ @ them a colon delimited list . If you select a value in these categories : class , lithology , type , longitude , or latitude , the values will propagate to the samples table . In this example , **27;1017;TOOLONG ; sitelithology=Basaltic Lava ; sitetype=Lava Flow <p> Step 4 : Some data from sites in Step 3 have propagated into the samples table . You may update and fill in all columns in the samples table here . <p> Step 5 : Fill in information for locations . If you have provided site latitudes and longitudes , the columns for beginning and ending latitudes and longitudes should be filled in already . You must choose a legal entry from the controlled vocabularies for the " locationtype " . In this example : locationtype = outcrop . <p> Note that if you re-run this sequence , certain values may be changed or overwritten . For example , if you set samplelithology to be something different from sitelithology , re-running ErMagic will cause samplelithology to revert back to being the same as sitelithology . <p> Use of the Demag GUI is described @ @ @ @ @ @ @ @ @ @ Here are a few instructions that can be used as a quick start for using Demag GUI . Note : If at any point you require help with a particular aspect of Demag GUI clicking on Help ? Usage and Tips ( hotkey : ctrl-h ) then clicking on the item you wish to know more about will provide a pop-up box with information for most all aspects of the GUI and Interpretation Editor , see additional help for details . <p> To analyze the data in the example , follow these steps for each specimen : <p> From the coordinate system drop-down menu you can choose the coordinate system in which you wish to view the data ( e.g. geographic ) . <p> Click add a fit ( hotkey : ctrl-n ) and choose the temperature/AF bounds for the fit by double clicking on the measurement lines , by double clicking on the measurement points in the zijderveld plot , or by choosing from the bounds dropdown menu . <p> Click the next button to analyze the next specimen ( hotkey : ctrl-right ) . <p> To calculate Fisher @ @ @ @ @ @ @ @ @ @ drop-down menus : component=All ; mean=Fisher . <p> All of the fits can be viewed and modified using the Interpretation Editor which can be selected from the Tools menu in the top menubar. ( hotkey : ctrl-e ) <p> To save all of the specimen interpretations , choose from the menubar File ? Save MagIC pmag tables . This saves all the interpretations in MagIC formatted pmag* files in your ThisProject directory . Fits that are saved this way will be loaded into demaggui the next time it is launched . To save temporary analysis data use File ? Save interpretations to a redo file . This saves all interpretation data to a redo file which will not load immediately when the GUI starts , but preserves ascetic aspects of interpretations such as color as well as the specimen you were on when you saved to keep your place in analysis and allows rebooting of session without full export of MagIC tables . <p> Click through the dialog boxes and fill out choices for the MagIC results table . <p> Use of the Thellier GUI is described in more detail @ @ @ @ @ @ @ @ @ @ document : Thellier GUI manual . Here are a few instructions that can be used as a quick start for using Thellier GUI . <p> To analyze the data in the example , follow these steps for each specimen : <p> Choose temperature bounds from the temperatures dropdown menus . <p> Click save so the program remembers the interpretations ( the interpretation is not saved to a file yet ! ) . <p> Click the next button to analyze the next specimen . <p> The default of the program is to calculate sample means . To change it to site level mean , choose from the menubar : Analysis ? Acceptance criteria ? Change acceptance criteria . Find the average by sample/site dropdown menu in the third row and change it to site . Click OK . The site mean will appear in the sample/site results box ( top right ) . <p> After all specimen interpretations are saved in memory , choose from the menubar File ? Save MagIC pmag tables . This save all the interpretations in MagIC formatted pmag* files in the MagIC Project Directory . <p> @ @ @ @ @ @ @ @ @ @ upload txt file button on the main page of Pmag GUI . A file named upload.txt will be created in your ThisProject directory . Now , go to the MagIC search interface . Click on the create button and find or create your reference . <p> Click on Upload in the Actions column and drag and drop the upload.txt file onto the Drop a MagIC Text File window . Congratulations . Your data are now in the database . But , they are not yet activated which you can do by clicking on the Activate button in the Actions column . Once you activate an uploaded dataset ( only for published papers ) , it will be publicly available . <p> Data can be downloaded from the MagIC database and examined with PmagPy tools . The MagIC search interface provides a rich variety of search filters available by clicking on the Filter the MagIC Search Results text box . To locate data from a particular reference , simply substitute the digital object identifier ( DOI ) in your browser window : <p> The above DOI will find the data for @ @ @ @ @ @ @ @ @ @ . This may fail in Safari ; if so , use an alternative browser like Firefox or Chrome . To download the data , simply click on the file icon below the column labeled data . This will save a file to your downloads folder . To unpack this file after downloading it from the database click the " unpack downloaded txt file button " in the Pmag GUI panel . <p> There is an astounding number of different ways that paleomagnetists document data in the field and in the lab . This variety is met with a large number of method codes that describe sampling and orientation procedures ( see http : **32;1046;TOOLONG for a complete description ) . The MagIC database expects sample orientations to be the azimuth and plunge of the fiducial arrow used for measurement ( see Essentials , Chapter 9 ) and the orientation of the bedding to be dip direction and downward dip so no matter what your own preference is , it must be translated into the standard MagIC convention for use with the PmagPy programs and with Pmag GUI . <p> Pmag @ @ @ @ @ @ @ @ @ @ sampling related information into a MagIC usable format . The first way is through step 2 on the GUI front panel and filling in the data from within the GUI . That way will work for many applications , but it may be desirable to fill the spreadsheet in separately from the GUI by using a tab delimited file ( orient.txt format ) . By clicking on step 2 on the GUI front panel you create a file named demagorient.txt which has all of your sample names in it . Each orient.txt file should have all the information for a single location sensuMagIC . <p> The next row has the names of the columns . The required columns are : samplename , magazimuth , fielddip , date , lat , long , samplelithology , sampletype , sampleclass ) but there are a number of other possible columns ( e.g. , Optional Fields in orient.txt formatted files are : date , shadowangle , hhmm , date , stratigraphicheight , beddingdipdirection , beddingdip , imagename , imagelook , imagephotographer , participants , methodcodes , sitename , and sitedescription , GPSAz ) . @ @ @ @ @ @ @ @ @ @ data for stratigraphicheight are in meters . Also note that if these are unoriented samples , just set magazimuth and fielddip to 0 . <p> It is handy to document the lithology , type and material classification information required by MagIC . These are all controlled vocabularies listed at http : **35;1080;TOOLONG . For archaeological materials , set the lithology to " Not Specified " . <p> Put in stratigraphic height , sun compass , differential GPS orientation information under the appropriate column headings . You can also flag a particular sample orientation as suspect , by having a column sampleflag and setting it to either g for good or b for bad . Other options include documenting digital field photograph names and who was involved with the sampling . <p> For Sun Compass measurements , supply the shadowangle , date and time . The date must be in mm/dd/yy format . If you enter the time in local time , be sure you know the offset to Universal Time as you will have to supply that when you import the file . Also , only put data from one @ @ @ @ @ @ @ @ @ @ should follow the convention shown in this figure ( from Tauxe et al. , 2010 ) : <p> Supported sample orientation schemes : <p> There are options for different orientation conventions ( drill direction with the Pomeroy orientation device drill azimuth and hade is the default ) , different naming conventions and a choice of whether to automatically calculate the IGRF value for magnetic declination correction , supply your own or ignore the correction . The program generates ersamples.txt and ersites.txt files . Be warned that existing files with these names will be overwritten . <p> All images , for example outcrop photos are supplied as a separate zip file . imagename is the name of the picture you will import , imagelook is the " look direction " and imagephotographer is the person who took the picture . This information will be put in a file named erimages.txt and will ultimately be read into the erimage table in the console where additional information must be entered ( keywords , etc . ) . <p> Often , paleomagnetists note when a sample orientation is suspect in the field . To @ @ @ @ @ @ @ @ @ @ its orientation that is greater than about 5 , enter SO-GT5 in the methodcodes column and any other special codes pertaining to a particular sample from the method codes table . Other general method codes can be entered later . Note that unlike date and sampleclass , the method codes entered in orient.txt pertain only to the sample on the same line . <p> Samples are oriented in the field with a " field arrow " and measured in the laboratory with a " lab arrow " . The lab arrow is the positive X direction of the right handed coordinate system of the specimen measurements . The lab and field arrows may not be the same . In the MagIC database , we require the orientation ( azimuth and plunge ) of the X direction of the measurements ( lab arrow ) . Here are some popular conventions that convert the field arrow azimuth ( magazimuth in the orient.txt file ) and dip ( fielddip in orient.txt ) to the azimuth and plunge of the laboratory arrow ( sampleazimuth and sampledip in ersamples.txt ) . The two angles , @ @ @ @ @ @ @ @ @ @ arrow is the strike of the plane orthogonal to the drill direction , Field dip is the hade of the drill direction . Lab arrow azimuth = magazimuth-90 ; Lab arrow dip = -fielddip <p> 3 Lab arrow is the same as the drill direction ; hade was measured in the field . Lab arrow azimuth = magazimuth ; Lab arrow dip = 90-fielddip . <p> 4 Lab arrow orientation same as magazimuth and fielddip . <p> 5 Lab arrow azimuth is magazimuth and lab arrow dip is the fielddip-90 <p> 6 Lab arrow azimuth is magazimuth-90 , Lab arrow dip is 90-fielddip , i.e. , the field arrow was strike and dip of orthogonal face : <p> Structural correction conventions : <p> Because of the ambiguity of strike and dip , the MagIC database uses the dip direction and dip where dip is positive from 0 ? 180 . Dips&gt;90 are overturned beds . <p> Supported sample naming schemes : <p> 1 XXXXY : where XXXX is an arbitrary length site designation and Y is the single character sample designation . e.g. , TG001a is the first sample from @ @ @ @ @ @ @ @ @ @ XXXX ( XXX , YY of arbitary length ) 3 XXXX.YY : YY sample from site XXXX ( XXX , YY of arbitary length ) 4-Z XXXXYYY : YYY is sample designation with Z characters from site XXX 5 site name = sample name 6 site name entered in sitename column in the orient.txt format input file 7-Z XXXYYY : XXX is site designation with Z characters from samples XXXYYY <p> There are two types of files that help in plotting of IODP paleomagnetic data sets : the core summaries with depth to core top information and the sample information that contains lists of samples taken . Visiting the IODP science query website at LONG ... allows you to select SRM - Remanence of magnetization under the Analysis scroll down menu . By picking the expedition , site , hole , etc. you can download a . csv format ( comma separated values ) for the expedition data . ( Be aware that this is the rawest form of the data , including disturbed intervals , bad measurements , core ends , etc. and may not be exactly what ended @ @ @ @ @ @ @ @ @ @ " Show Report " button , then , " Expand Table " , then " Get File " : <p> This can take a very long time , so get yourself a cup of tea . <p> You can also ( while you 're at it ) click on the Summaries tab and download the coring summaries : <p> The MagIC database is designed to accept data from a wide variety of paleomagnetic and rock magnetic experiments . Because of this the magicmeasurements table is complicated . Each measurement only makes sense in the context of what happened to the specimen before measurement and under what conditions the measurement was made ( temperature , frequency , applied field , specimen orientation , etc ) . Also , there are many different kinds of instruments in common use , including rock magnetometers , susceptibility meters , Curie balances , vibrating sample and alternating gradient force magnetometers , and so on . We have made an effort to write translation programs for the most popular instrument and file formats and continue to add new supported formats as the opportunity arises . Here we @ @ @ @ @ @ @ @ @ @ to prepare your files for importing . In general , all files for importing should be placed in the MyFiles directory or in subdirectories therein as needed . If you do n't  see your data type in this list , please send an example file and a request to : ltauxe@ucsd.edu and well get it in there for you . <p> Pmag GUI will import hysteresis data from room temperature Micromag alternating gradient magnetometers ( AGM ) in several different ways . You can import either hysteresis loops or backfield curves , or you can import whole directories of the same . In the latter case , the file endings must be either . agm ( . AGM ) or . irm ( . IRM ) and the first part of the file must be the specimen name . See the documentation for agmmagic.py for examples . <p> Now you 've collected together all the files you need , we can start importing them into MagIC directory with Step 1 in Pmag GUI . <p> MagIC GUI is a Graphical User Interface written by Lori Jonestrask . It is meant for @ @ @ @ @ @ @ @ @ @ . MagIC GUI is specifically designed for making a contribution without measurement data ; if you ARE including measurement data , we recommend that you use Pmag GUI instead . MagIC GUI allows you to add data at the location , site , sample , and specimen levels . You can also add results and ages . The program uses an Excel-like grid interface for entering and editing data . Useful features include : pop-up menus with controlled vocabularies , multi-cell pasting from external spreadsheets , and built-in validations for MagIC database requirements . Note : magicgui.py uses the current MagIC data model , 3.0 . For working with legacy data in the 2.5 format , you should instead use magicgui2.py . The tutorial below was written specifically for the older magicgui2.py but will soon be updated with illustrations and instructions from magicgui.py . The two programs do work almost identically , so the tutorial should still point you in the right direction even if you are working with 3.0 data . <p> Start up MagIC GUI . Youll do this by clicking on the icon ( if you downloaded @ @ @ @ @ @ @ @ @ @ line ( if you downloaded the full PmagPy installation ) . If you are using Anaconda Python , you will type magicguianaconda on your command line instead . <p> When you first start magicgui.py , you will change directories into a Project Directory . For each study , create a directory with a name that relates to that study . Here we will call it MyProject . This is where you will collect and process all the rock and paleomagnetic data for a given study , usually a publication . The project directory name should have NO SPACES and be placed on the hard drive in a place that has NO spaces in the path . Under certain Windows versions , this means you should not use your home directory , but create a directory called for example : D : MyPmagProjects and put MyProject there . <p> Start by clicking on 1 . add location data . Enter location data as seen below . Notice use of the context menu : in most instances , these will provide controlled vocabularies . <p> You 'll notice that some information is not @ @ @ @ @ @ @ @ @ @ citations , you can leave ercitationnames blank and it will be automatically filled in with This study after you save this data . Second , if you will be providing latitude/longitude for sites , then you do n't  need to provide it at the location level . The program will extract start and end latitude and longitude from your site data and apply it to each location . <p> If you want to provide more than the default required information , you may want to add more columns to your grid . In our case , well add in Country . To do this , just click the button Add additional columns . Choose whatever headers you want to add and select Ok . <p> Now you 'll have a new column with a new controlled vocabulary . Find and select U.S.A.. When you 're done entering location data , click Save and close grid . <p> Next , well add in sites , so start by opening the site grid . We will have two sites in this dataset , so click the Add row(s) button once . Then , fill @ @ @ @ @ @ @ @ @ @ both sites will have the same value . To provide this data more efficiently , click on the column label to edit all values in that column . If that column has a menu , you can then click anywhere in that column and make a selection for every cell in the column . Once you 're done editing the column , click the header again to exit the multiple-cell-edit mode . <p> For controlled vocabulary columns , like type , it is possible that you might not be able to find an appropriate value , or you might not know the appropriate value . In that case , most controlled vocabularies provide a Not Specified option , which we will use here . <p> Once you 're done adding sites , save and close the site grid , then open the sample grid . Enter data as you see below : <p> All other required data will fill in automatically . If you do n't  provide latitude and longitude data for a sample , it will propagate down from the site after you save and close the grid . <p> For @ @ @ @ @ @ @ @ @ @ Add additional columns and select sampleintsigma from the Headers for interpretation data column . Select ok and have a look at your new grid . You 'll notice that the column you selected has been added , as well as two additional columns : magicinstrumentcodes and magicmethodcodes++ . These two columns are required if you are including any interpretation data at the sample level . Fill in the added columns . You 'll notice that magicmethodcodes++ has a drop-down menu . If you are n't  familiar with the MagIC codes system , click Show method codes which provides brief explanations for each possible code . <p> Save and close the sample grid , then re-open it to see how the data have propagated ( re-opening the grid is n't necessary for the program , this is just for you to see what MagIC GUI does automatically ) . <p> In this example , we wont add data at the specimen level , so we will be skipping step 4 . Adding specimens works just the same as adding samples . <p> Now open the age grid . You can assign ages at any @ @ @ @ @ @ @ @ @ @ . For this study , we will add ages at the sample level . Choose sample as the age level . Next , you 'll need to add in the age column . Fill in the grid as below : <p> You 'll notice that you cant add or remove rows in this grid . If you want to add an additional sample , you would need to go back to the sample grid to do so . <p> The last data step is to put in result information . You 'll need to open the result grid and , for this example , add just one statistic : vadm . Then , fill out the grid : <p> For each result , you will add one or more items that the result pertains to . In this example , one result is a site VADM , and the other is an averaged VADM from both sites . For now , leave magicmethodcodes blank for the Averaged VADM result : in a minute well see how MagIC GUI catches this error . Save and close the result grid . <p> Next , you @ @ @ @ @ @ @ @ @ @ final button on the main frame , prepare upload txt file . Depending on the size of your contribution , this can take a minute ; with our small example , it should be fairly quick . After hitting upload , you will see an error message , and the main frame will direct you to the problem . Validations will check for missing required fields , invalid data , and missing ancestors ( for instance , a specimen with no sample ) . <p> Open the result grid to fix the error . Click Show help for more information about validations . In this case , it is a simple fix : add method code LP-PI to the average VADM result . <p> Now that you 've fixed your error , try prepare upload txt file again . This time there should be no problems , and you are ready to upload your data ! <p> The Py part of PmagPy stands for Python , the language in which all the code is written . It is not essential , but it is helpful , to understand a bit about @ @ @ @ @ @ @ @ @ @ because no one should be using programs as black boxes without understanding what they are doing . As all the programs are open source , you have the opportunity to look into them . If you understand a bit about how computers work yourself , you will be able to follow along what the programs are doing and even modify them to work better for you . In this chapter , you will find a brief introduction to the computer skills necessary for using the programs properly . We have tried to make this tutorial operating system independent . All the examples should work equally well on Mac OS , Windows and Unix-type operating systems . For a more complete explanation of the marvelous world of UNIX , refer to the website at LONG ... For handy tricks in DOS , try this link : http : **35;1117;TOOLONG . For an introduction to programming in Python , see the Python Programming Chapter . For now , we are interested in having the skills to find a command line and navigate the file system in order to get started with PmagPy @ @ @ @ @ @ @ @ @ @ ( *NIX ) , you may never have encountered a command line . Using any of the command line programs requires accessing the command line . If you are using the MacOS operating system , look for the Terminal application in the Utilities folder within the Applications folder . When the Terminal application is launched , you will get a terminal window . The Unix ( and MacOS ) Bash shell has a $ sign as a prompt . Other shells have other command line prompts , such as the antiquated C-shell used by Lisa Tauxe ( do n't  ask ) which has a % prompt which is used in the examples here . <p> Under the Windows operating system , you can find your command line by searching for the " cmd " application . You 'll use this if you are using the full pip installation of PmagPy . If you are doing the standard PmagPy install , you will create a shortcut to the PmagPy command window on your desktop . Double clicking on that will give you a window in which you can type PmagPy commands . @ @ @ @ @ @ @ @ @ @ command line from there on . <p> Note that the location of this program varies on different computers , so you may have to hunt around a little to find yours . Also , the actual " prompt " will vary for different machines . <p> When you first open a terminal window , you are in your " home " directory . Fundamental to all operating systems is the concept of directories and files . On windows-based operating systems ( MacOS or Windows ) , directories are depicted as " folders " and moving about is accomplished by clicking on the different icons . In the world of terminal windows , the directories have names and are arranged in a hierarchical sequence with the top directory being the " root " directory , known as " / " ( or C : backslash in Windows ) and the file system looks something like this : <p> Within the root directory , there are subdirectories ( e.g. Applications and Users in bold face ) . In any directory , there can also be " files " ( e.g. dircartexample.dat in @ @ @ @ @ @ @ @ @ @ system relies on what is called a " pathname " . Every object has an " absolute " pathname which is valid from anywhere on the computer . The absolute pathname in *NIX always begins from the root directory / and in DOS ( the operating system working in the Windows command line window ) , it is C : backslash . <p> The absolute pathname to the home directory lisa in the figure is /Users/lisa . Similarly , the absolute pathname to the directory containing PmagPy scripts would be /Users/lisa/PmagPy . There is also a " relative " pathname , which is in reference to the current directory ( the one you are sitting in ) . If user " lisa " is sitting in her home directory , the relative pathname for the file dircartexample.dat in the directory datafiles would be **36;1154;TOOLONG . When using relative pathnames , it is useful to remember that . / refers to the current directory and .. / refers to the directory " above " . Also , lisas home directory would be lisa , or if you are logged in as @ @ @ @ @ @ @ @ @ @ that you have found your command line and are comfortable in your home directory , you can view the contents of your directory with the Unix command ls or the DOS command dir . You can make a new directory with the command <p> % mkdir NEWDIRECTORYNAME <p> This command works in both Unix and DOS environments ) and you can move into your new directory with the command <p> % cd NEWDIRECTORYNAME <p> To move back up into the home directory , just type cd .. remembering that .. refers to the directory above . Also , cd by itself will transport you home from where ever you are ( there 's no place like home .... ) . You can also change to any arbitrary directory by specifying the full path of the destination directory . <p> Programs that operate at the command line level print output to the screen and read input from the keyboard . This is known as " standard input and output " or " standard I/O " . One of the nicest things about working at the command line level is the ability to @ @ @ @ @ @ @ @ @ @ typing input to a program with the keyboard , it can be read from a file using the symbol &lt; . Output can either be printed to the screen ( standard output ) , redirected into a file using the symbol &gt; , appended to the end of a file with &gt;&gt; or used as input to another program with the pipe operator ( ) . <p> There are many ways of editing text and the subject is beyond the scope of this documentation . Text editing is a blessing and a curse . You either love it or hate it and in the beginning , and if you are used to programs like Word , you will certainly hate it . ( And if you are used to a decent text editor , you will hate Word ! ) . But you ca n't use Word because the output is in a weird format that no scripting languages read easily . So you have to use text editor that will produce a plain ( ascii ) file , like Notepad , TextWrangler , Sublime Text or Atom . TextWrangler is @ @ @ @ @ @ @ @ @ @ in the Windows operating system and the Atom text editor is a free cross-platform option with lots of nice packages available that extend its functionality . Enthought Pythons Canopy programming environment comes with its own text editor . <p> The PmagPy software package is a comprehensive set of programs for paleomagnetists and rock magnetists . For installation , follow the instructions in the Installing PmagPy Chapter in this cookbook . <p> When you type something on your command line , your operating system looks for programs of the same name in special places . These are special " paths " so the directory with your Python scripts has to " be in your path " . To inform the operating system of the new directory , you need to " set your path " . It should have been set when you installed PmagPy . <p> The Demag GUI ( demaggui.py ) program enables the display and analysis of paleomagnetic demagnetization data . The software will display specimen level data within the chosen directory as a Zijderveld plot , equal area plot and intensity plot . Interpretations can be made @ @ @ @ @ @ @ @ @ @ Mean directions can be calculated and displayed for these interpretations . These interpretations can be exported as MagIC pmag files from the program . <p> The best way to launch the Demag GUI application is through Pmag GUI . If you have installed PmagPy using pip , ( or if PmagPy has been added to your PATH ) , you can type pmaggui.py at the command line to launch it . Anaconda users will instead type pmagguianaconda . <p> Alternatively , you can navigate to the PmagPy home directory and type python. /pmaggui.py . Within Pmag GUI , data can be converted from the format of a particular lab into MagIC format so that it can be displayed and analyzed within Demag GUI . The program can be started by clicking on the Demag GUI button in Pmag GUI . <p> If you want to launch Demag GUI directly , ( assuming PmagPy was properly installed using pip or has been added to your PATH ) , you can simply type demaggui.py at the command line . Anaconda users will type demagguianaconda instead . Note : on OSX it is @ @ @ @ @ @ @ @ @ @ the drop-down boxes behave better when Demag GUI is launched this way . <p> Demag GUI can also be launched through the command line by navigating to the directory containing demaggui.py and running it with : <p> A new fit can be added by clicking the add fit button or if no fit has yet been created for the current specimen by double clicking on two measurements in the list of measurements to the left or by double clicking on the data points on the zijderveld plot . It is also possible to add interpretations in mass using the interpretation editor tool described bellow . Additionally , you can select the fit you would like to edit or view by using the drop-down menu under the add fit button . Once you have selected a fit , the shape of the end points of the selected fit will turn to diamond shapes on all plots to distinguish them from the other data points . <p> Once the desired fit is selected , its bounds can be edited using the drop-down boxes under the bounds header . <p> Another way to @ @ @ @ @ @ @ @ @ @ in the list on the left . The included steps in the currently selected interpretation are shown in highlighted in blue on the measurement list and the measurements marked " bad " are shown in highlighted in red . In the case of duplicate measurements , the first good measurement with the same treatment is used as a bound . All points between the selected bounds that are flagged good ( i.e. not flagged bad and marked in red ) , including duplicate measurements , will be included in the interpreted fit . <p> Finally , you can select the bounds of an interpretation directly off the Zijderveld plot by hovering your mouse over a measurement ( should change to a hand shape ) and double clicking . <p> When first created , the fit will be given a generic name such as Fit 1 . The name of the fit can be changed from the default by typing into the drop-down box containing fit name and then pressing enter . The default fit type is a least-squares line . You can choose different fits , such as a line @ @ @ @ @ @ @ @ @ @ the drop-down menu under the label interpretation type . Plane fits can be plotted as either poles , full planes , partial planes , best fit vectors , or best fit vectors and full plane ( Note : plane poles will be displayed as squares and best fit vectors will display as sideways triangles on high level mean plot ) . This display option can be changed in the second drop-down menu under interpretation type . <p> The properties of the currently selected fit to the data can be seen in the upper center of the GUI in a box labeled Interpretation Directions and Statistics . <p> If you would like to delete a single interpretation , select the one you wish to delete from the interpretation drop-down menu and click delete . If you wish to clear all interpretations you may go into the interpretation editor located under the tools menu , select the fits you wish to delete and click the " delete selected " button . <p> You can switch current specimen by clicking the next or previous button under the specimen box in the side bar @ @ @ @ @ @ @ @ @ @ can also select specimen from the drop-down menu or type the name of the specimen directly into the specimen box and hit enter to go directly to that specimen . Finally , you can double click on any of the points on the higher level means plot to switch directly to that specimen and interpretation . <p> The choice between coordinate systems ( i.e. specimen , geographic or tilt-corrected ) is available on the left above the list of steps . The data list and the plots will update to reflect the chosen coordinate system . <p> You can alter the X axis of the Zijderveld plot using the Zijderveld plot interactions box to set X=North , East , or NRM dec . <p> Due to flux jumps or other such errors , individual measurements should sometimes be excluded from interpretation . Such measurements can be flagged as " bad " by right clicking them within the measurement list and the measurement will then be highlighted in red . Additionally , you can double right click on the point you want to make bad in the Zijderveld plot to toggle @ @ @ @ @ @ @ @ @ @ be change from " g " to " b " when a measurement is marked as bad the step will not be included in fits that are made to the data . Any measurement marked as bad will be colored red in the step list and will be shown as an empty rather than filled circle on the Zijderveld , equal area and M/M0 plots . To change a bad measurement back to being good , one can right click on it again . Upon doing so , the red highlighting will go away , the data will be shown colored in within the plots and any fit that spans that data point will be recalculated to include it . <p> Acceptance criteria can be set by using the menu option Analysis ? Acceptance Criteria ? Change Acceptance Criteria . These criteria will be written to a pmagcriteria.txt table . These criteria can then be used to exclude interpretations that fail checks against this criteria during export . <p> The four plots that take up the majority of the center of the GUI are where data and their interpretations are @ @ @ @ @ @ @ @ @ @ and this is signified by a cross shaped cursor when you mouse over them . To zoom simply click and drag the rectangle to the desired area . You can switch to pan mode by right clicking on any one of the graphs and then clicking and dragging will pan around the plot . Finally , to return to the original plot zoom level and position simply click the middle mouse button to return home . Note : In the absence of a middle mouse button pressing both right and left mouse buttons at the same time works on most laptops in the case of Macbooks clicking with two fingers should work , and if using Apples magic mouse we recommend you download the MagicPrefs program which will allow you to configure your mouse however you prefer . <p> On the Zijderveld plot you have the additional option to select the current interpretations bounds by double clicking on a measurement point . You can also double right click on a measurement point in the zijderveld plot to mark it bad . <p> On the equal area plots , both the @ @ @ @ @ @ @ @ @ @ on any interpretation to switch to that specimen and interpretation immediately . <p> Once you have picked out your interpretations , you can save the session data in two different ways : ( 1 ) as a . redo file which will allow you to have the fits preserved to be view again with Demag GUI or ( 2 ) as MagIC pmag* tables to be uploaded to the MagIC database or otherwise processed . In addition , you may save image files of the plots . <p> The . redo File : You can use the menu option File ? Save current interpretations to a redo file to create this file type , you can just click the save button next to add fit , or you can use the hotkey ctrl-s . The advantage of the . redo file type being that it is designed to save your place when analysing a large dataset . Loading a redo file will reload all interpretations previously created any special colors assigned to them and take you to the specimen you saved the redo file on allowing you to pick up @ @ @ @ @ @ @ @ @ @ does NOT load previous interpretations on start up you must go to the menu option File ? Import previous interpretations from a redo file ( hotkey : ctrl-r ) to restore your previous session . <p> The Pmag Tables : By going to the menu File ? Save MagIC pmag tables you can export your interpretations made in Demag GUI to the MagIC pmag tables which can then be used by other MagIC programs or uploaded to the MagIC database . You can export any or all of the three coordinate systems upon selecting this option and you may choose to save pmagsamples , pmagsites , and pmagresults tables in addition to the pmagspecimens table that is output . If you choose to output additional information you will be prompted by a pop up window for additional information . Note : This save format loads on start up of the GUI immediately restoring your interpretations . Selection of this option will overwrite your demaggui.redo file in the working directory . <p> Images of Plots : Select the menu option File ? Save plot ? Save all plots to save all @ @ @ @ @ @ @ @ @ @ individually . If you zoom or pan any of the plots the shifted image will be saved not the originally plotted image although the plot will redraw and reset to the original image in the GUI . <p> You can flag the current specimen interpretation ( marked by large diamonds on all plots ) good or bad by using the menu option Analysis ? Flag Interpretations . The list of interpretations in the interpretation editor tool of Demag GUI can also be used to toggle interpretations good or bad in the same way that measurements can be marked good or bad in the measurement list , by right clicking on the entry you want toggled . This will change the shape of the interpretation to a small diamond on all plots , remove it from use in any higher level means , and mark the entry specimen ? flag in the pmag tables b instead of g to signify this . <p> You can check sample orientation by using the menu option Analysis ? Sample Orientation ? Check Sample Orientations ( hotkey : ctrl-o ) . This function will set @ @ @ @ @ @ @ @ @ @ current site level and display the wrong arrow ( up triangle ) , wrong compass ( down triangle ) , and rotated sample for declanation incraments of 5 degrees ( dotted circle ) . This allows you to check if the sample orientation is correct and thus can be used in analysis . If you determine the current sample orientation to be bad you can mark it as such using the menu option Analysis ? Sample Orientation ? Mark Sample Bad ( hotkey : ctrl- . ) . This will change the sample ? orientation ? flag in the er ? samples file to b not g and will prevent you from marking the specimen interpretations good in that sample so you do not use the improperly oriented data in your final results . If you later realize this was a mistake you can mark the sample orientation good again using Analysis ? Sample Orientation ? Mark Sample Good ( hotkey : ctrl- , ) . Finally , to turn off the check sample orientations data simply select the Check Sample Orientations option again and it will be removed . @ @ @ @ @ @ @ @ @ @ of the current specimen . <p> The set of drop-down boxes to the right of the interpretation data are there to determine what level you want to analyse in the high level means plot and are grouped into the Display Level and Mean Options boxes . <p> The Display Level boxes consist of the upper drop-down menu which allows you to select the level at which interpretations are displayed options being all interpretations in the current : sample , site , location , or study . The lower drop-down menu let 's you select what the current sample , site , location , or study is . <p> The top drop-down menu in the Mean Options box let 's you chose what kind of mean you would like to take of the specimen components currently displayed . The lower drop-down menu let 's you pick which specimen components to display allowing you to display All components , No components , or any single component . <p> The mean statistics for the chosen high level mean are displayed in the lower right of the GUI and can be cycled through using the arrow buttons @ @ @ @ @ @ @ @ @ @ high level means . <p> It is possible to toggle on and off displaying any one of the means in the high level plot which can be useful in the case of a cluttered graph of all components . This can be done by going to the menu option Analysis ? Toggle Mean Display and selecting the name of the component you would like to toggle . <p> All interpretations marked bad will appear as small diamonds regardless of type on the high level mean plot . The below gives examples for a number of plane display options of bad interpretations ( the symbols off to the side ) , best fit vectors to the means ( sideways triangles ) , plane poles ( squares ) , and the planes themselves . <p> In order to more easily view and edit specimen interpretation data there is a specimen interpretation editor which can be launched using the menu option Tools ? Interpretation Editor ( hotkey : ctrl-e ) . Note : If you would like more help than provided here the interpretation editor has in context help same as the main @ @ @ @ @ @ @ @ @ @ of Interpretations : This panel contains a list which details the fits made to the data and their parameters from which you can select which interpretation to view by double clicking it . In the list , the currently selected interpretation is highlighted blue as shown in the image below . You can mark interpretations as bad which removes them from any Fisher means or other high level means by right clicking on their entry in the list . All interpretations marked bad are colored red in the list and marked as a small diamond on the plot . The specimen entry associated with this fit will be given a bad ( b ) flag within the pmagspecimens table . You can search through interpretations by using the search bar above the list . Finally , interpretations can be highlighted by clicking on the list and holding the shift or ctrl/command key to select multiple interpretations . <p> Buttons and Boxes : Highlighting entries allows you to delete or alter the characteristics of multiple interpretations at once without having to select each one in turn . This mass alteration is @ @ @ @ @ @ @ @ @ @ and then clicking the " apply changes to highlighted fits " button . You can delete highlighted fits using the " delete highlighted fits " button . The " add fit to highlighted specimens " button in the interpretation editor adds a fit to all highlighted specimens in the list on the left . You can use the " add new fit to all specimens " as a convenient option to add a new interpretation with all the attributes described in the above Name/Color/Bounds boxes to every specimen with measurement data . This is a useful method for quickly analysing a new dataset by checking for components between common unblocking steps like giving every specimen a general magnetite interpretation inferring about where that should be ( e.g. bounds=300C to 580C , name=MAG , color=violet ) . <p> Additional High Level Means Options : The interpretation editor also allows the displaying of site and sample means as the primary points on the high level mean plot by changing the bottom left display options drop-down box . Though it does not yet allow taking fisher means of sample means or site means @ @ @ @ @ @ @ @ @ @ " None " if this option is changed from specimens . <p> Another tool offered by Demag GUI is the VGP ( Virtual Geomagnetic Pole ) Viewer which allows you to view your VGPs before exporting to MagIC tables . This tool can be opened by using the menu option Tools ? View VGPs ( hotkey : ctrl-shift-v ) . The VGP viewer requires latitude and longitude data for sites and locations in order to calculate all VGPs from specimen interpretations , if this data is not already contained in the MagIC tables imported when the GUI started then it will be asked for during calculation so have it ready . The VGP viewer allows you to select viewing samples , sites , or location VGPs from the drop-down menu at the top . Plot interactions are the same here as in the main GUI and can be zoomed , panned , and points selected in the same manner . The list on the left shows all the data for the currently displayed VGPs . <p> The white box in the far top right of the GUI is there to @ @ @ @ @ @ @ @ @ @ interpretation such as missing data or duplicate data and can be useful in debugging and analysis . <p> Finally , if you need more help working with Demag GUI it offers in context assistance with the menu option Help ? Usage and Tips ( hotkey : ctrl-h ) which will change your cursor and then let you click on whichever aspect of the GUI you want help with . Once you have clicked a yellow pop-up box with information should appear in most cases , not all features have information but most should . <p> Once open , Thellier GUI loads files already prepared in a This Project directory and the interpretations from Thellier GUI are part of the workflow of Pmag GUI . This section is a brief introduction on how to use Thellier GUI as a stand alone application . Much more information is available within this manual : Thellier GUI full manual . <p> A complete list of the definitions for paleointensity statistics used by Thellier GUI is available as a supplement to the article by Paterson et al. , 2014 and available for download here : @ @ @ @ @ @ @ @ @ @ directory " dialog window will appear as soon as the GUI is started . Your ThisProject directory should include a file named magicmeasurements.txt ( created for example by PmagGUI . If a file named rmaganisotropy.txt is in the project directory , then the program reads in the anisotropy data . Reading and processing the measurements files may take several seconds , depending on the number of the specimens . <p> When your ThisProject project directory is selected , the program reads all the measurement data , checks them , processes them and sorts them . If non-linear-TRM ( NLT ) data exist in magicmeasurement.txt then the program tries to process the data using Equations ( 1 ) -(3) in Shaar et al. , 2010 . The program reads magicmeasurement.txt , and processes the measurements for presentation as Arai and Zijderveld plots . We recommend that you check all the warnings and errors in ThellierGUI.log before starting to interpret the data . For details about warnings and error messages during these steps , consult the tutorial document in the thellierGUI folder in datafiles . Also , consult the Preferences to change @ @ @ @ @ @ @ @ @ @ columns of the measurement data : Step is " N " for NRM , " Z " for zero field step , " I " for infield step , " P " for pTRM check , and " T " for tail check . The temperature of each step is given in C. Also shown declination , inclination and moment ( in units of Am2 ) <p> Arai plot : Arai plot normalized by NRM0. blue circles are zero field steps , red circles are infield steps , triangles are pTRM checks , blue squares are tail checks . Temperatures are displayed near data points . Temperature bounds and best fit line are marked in green . SCAT box is marked with dashed lines ( only if SCAT is True ) . <p> Zijderveld plot : A Zijderveld plot of the NRM step . The x axis is rotated to the direction of the NRM , blue is the x-y projection , and red is x-z projection . <p> Equal area plot : An equal area projections of the NRM steps . solid circles are positive inclination . open circles @ @ @ @ @ @ @ @ @ @ in blue , pTRMs are in red . <p> sample data : If at least two specimens have a saved interpretation , then their values are displayed on this plot . The mean n++ standard deviation of the mean are marked as horizontal lines . <p> The bottom of the main panel include paleointensity statistics . The first line has the threshold values ( empty if N/A ) . The second line is the specimens statistics . For details see Appendix A1 in Shaar and Tauxe ( 2013 ) . <p> PmagPy scripts work by calling them on a command line . The python scripts must be placed in a directory that is in your " path " . To see if this has been properly done , type dircart.py -h on the command line and you should get a help message . If you get a " command not found " message , you need to fix your path ; check the " installing python " page on the software website . Another possible cause for failure is that somehow , the python scripts are no longer executable . @ @ @ @ @ @ @ @ @ @ the scripts , and type the command : chmod a+x *. py <p> For people who hate command line programs and prefer graphical user interfaces with menus , etc. , some of the key programs for interpreting paleomagnetic and rock magnetic data are packaged together in a program called Pmag GUI . This can be invoked by typing pmaggui.py on the command line . The Pmag GUI program generates the desired commands for you behind the scenes , so you do not have to learn UNIX or how to use the command line ( except to call Pmag GUI itself ) . Nonetheless , some understanding of what is actually happening is helpful , because Pmag GUI is more limited than full range of PmagPy programs . So , here is a brief introduction to how the PmagPy programs work . <p> All PmagPy programs print a help message out if you type : programname.py-h on the command line . Many have an " interactive " option triggered by typing programname.py -i . Many also allow reading from standard input and output . The help message will explain how each @ @ @ @ @ @ @ @ @ @ the command line options : <p> Switches are from one to three characters long , preceded by a - . <p> The switch -h always prints the help message and -i allows interactive entry of options . <p> Options for command line switches immediately follow the switch . For example : -f INPUT -F OUTPUT will set the input file to INPUT and the output to OUTPUT . <p> A link to these source code/help menu files is provided for each program listed below with the format programnamedocs . <p> Examples of how to use command line PmagPy programs are given below . In all examples , the $ prompt stands for whatever command line prompt you have . The example data files referred to in the following are located in the datafiles directory bundled with the PmagPy software distribution . <p> Anisotropy of anhysteretic or other remanence can be converted to a tensor and used to correct natural remanence data for the effects of anisotropy remanence acquisition . For example , directions may be deflected from the geomagnetic field direction or intensities may be biased by strong anisotropies in @ @ @ @ @ @ @ @ @ @ anhysteretic or thermal remanence in many specific orientations , the anisotropy of remanence acquisition can be characterized and used for correction . We do this for anisotropy of anhysteretic remanence ( AARM ) by imparting an ARM in 9 , 12 or 15 positions . Each ARM must be preceded by an AF demagnetization step . The 15 positions are shown in the k15magic.py example . <p> For the 9 position scheme , aarmmagic.py assumes that the AARMs are imparted in positions 1,2,3 , 6,7,8 , 11,12,13 . Someone ( a.k.a . Josh Feinberg ) has kindly made the measurements and saved them an SIO formatted measurement file named armmagicexample.dat in the datafile directory called aarmmagic . Note the special format of these files - the treatment column ( column #2 ) has the position number ( 1,2,3,6 , etc. ) followed by either a " 00 " for the obligatory zero field baseline step or a " 10 " for the in-field step . These could also be 0 and 1 . <p> We need to first import these into the magicmeasurements format and then calculate the anisotropy tensors @ @ @ @ @ @ @ @ @ @ paleointensity or directional data for anisotropy of remanence . <p> So , first use the program siomagic.py to import the AARM data into the MagIC format . The DC field was 50 T , the peak AC field was 180 mT , the location was Bushveld and the lab protocol was AF and Anisotropy . The naming convention used Option # 3 ( see help menu ) . <p> Then use the program aarmmagic.py to calculate the best-fit tensor and write out the MagIC tables : rmaganisotropy and rmagresults . These files can be used to correct remanence data in a pmagspecimens format table ( e.g , intensity data ) for the effects of remanent anisotropy ( e.g. , using the program thelliermagicredo.py . <p> Here is a transcript of a session that works . Note that the siomagic.py command is all on one line ( which is what the terminal backslash means ) . <p> You can also use this program by reading in a filename using the -f option or from standard input ( with &lt; ) . Try this out with the test file in the angle @ @ @ @ @ @ @ @ @ @ the input file using " cat " ( or " type " on a DOS prompt ) . Then use angle.py to calculate the angles . You can also save your output in a file angle.out with the -F option : <p> Anisotropy data can be plotted versus depth . The program anidepthplot.py uses MagIC formatted data tables of the rmaganisotropy.txt and ersamples.txt types . rmaganisotropy.txt stores the tensor elements and measurement meta-data while ersamples.txt stores the depths , location and other information . Bulk susceptibility measurements can also be plotted if they are available in a magicmeasurements.txt formatted file . <p> In this example , we will use the data from Tauxe et al . ( 2012 ) measured on samples obtained during Expedition 318 of the International Ocean Drilling Program . To get the entire dataset , go to the MagIC data base at : http : //earthref.org/MAGIC/ and find the data using the search interface . As a short-cut , you can use the " permalink " : <p> Download the text file by clicking on the icon under the red arrow in : <p> Unpack the @ @ @ @ @ @ @ @ @ @ data into the different holes . Change directories into Location2 ( which contains the data for Hole U1359A ) . Or , you can use the data in the anidepthplot directory of the example data files . <p> Samples were collected from the eastern margin a dike oriented with a bedding pole declination of 110 and dip of 2 . The data have been imported into a rmaganisotropy formatted file named dikeanisotropy.txt . <p> Make a plot of the data using anisomagic.py . Use the site parametric bootstrap option and plot out the bootstrapped eigenvectors . Draw on the trace of the dike . <p> The specimen eigenvectors are plotted in the left-hand diagram with the usual convention that squares are the V1 directions , triangles are the V2 directions and circles are the V3 directions . All directions are plotted on the lower hemisphere . The bootstrapped eigenvectors are shownin the middle diagram . Cumulative distributions of the bootstrapped eigenvalues are shown to the right with the 95% confidence bounds plotted as vertical lines . It appears that the magma was moving in the northern and slightly up direction @ @ @ @ @ @ @ @ @ @ anisomagic.py that come in handy . In particular , one often wishes to test if a particular fabric is isotropic ( the three eigenvalues can not be distinguished ) , or if a particular eigenvector is parallel to some direction . For example , undisturbed sedimentary fabrics are oblate ( the maximum and intermediate directions can not be distinguished from one another , but are distinct from the minimum ) and the eigenvector associated with the minimum eigenvalue is vertical . These criteria can be tested using the distributions of bootstrapped eigenvalues and eigenvectors . <p> The following session illustrates how this is done , using the data in the test file sedanisotropy.txt in the anisomagic directory . <p> The top three plots are as in the dike example before , showing a clear triaxial fabric ( all three eigenvalues and associated eigenvectors are distinct from one another . In the lower three plots we have the distributions of the three components of the chosen axis , V3 , their 95% confidence bounds ( dash lines ) and the components of the designated direction ( solid line ) . This @ @ @ @ @ @ @ @ @ @ as a red pentagon . The minimum eigenvector is not vertical in this case . <p> The program apwp.py calculates paleolatitude , declination , inclination from a pole latitude and longitude based on the paper Besse and Courtillot ( 2002 ; see Essentials Chapter 16 for complete discussion ) . Use it to calculate the expected direction for 100 million year old rocks at a locality in La Jolla Cove ( Latitude : 33N , Longitude 117W ) . Assume that we are on the North American Plate ! ( Note that there IS no option for the Pacific plate in the program apwp.py , and that La Jolla was on the North American plate until a few million years ago ( 6 ? ) . <p> Note that as with many PmagPy programs , the input information can be read from a file and the output can be put in a file . For example , we put the same information into a file , apwpexample.dat and use this syntax : <p> Anisotropy of thermal remanence ( ATRM ) is similar to anisotropy of anhysteretic remanence ( AARM ) @ @ @ @ @ @ @ @ @ @ . Therefore , the program atrmmagic.py is quite similar to aarmmagic.py . However , the SIO lab procedures for the two experiments are somewhat different . In the ATRM experiment , there is a single , zero field step at the chosen temperature which is used as a baseline . We use only six positions ( as opposed to nine for AARM ) because of the additional risk of alteration at each temperature step . The positions are also different : <p> The file atrmmagicexample.dat in the atrmmagic directory is an SIO formatred data file containing ATRM measurement data done in a temperature of 520C . Note the special format of these files - the treatment column ( column #2 ) has the temperature in centigrade followed by either a " 00 " for the obligatory zero field baseline step or a " 10 " for the first postion , and so on . These could also be 0 and 1 , etc .. <p> Use the program siomagic.py to import the ATRM data into the MagIC format . The DC field was 40 T. The naming convention used option @ @ @ @ @ @ @ @ @ @ the program atrmmagic.py to calculate the best-fit tensor and write out the MagIC tables : rmaganisotropy and rmagresults formatted files . These files can be used to correct remanence data in a pmagspecimens format table ( e.g , intensity data ) for the effects of remanent anisotropy ( e.g. , using the program thelliermagicredo.py . <p> Many paleomagnetists save orientation information in files in this format : Sample Azimuth Plunge Strike Dip ( AZDIP format ) , where the Azimuth and Plunge are the declination and inclination of the drill direction and the strike and dip are the attitude of the sampled unit ( with dip to the right of strike ) . The MagIC database convention is to use the direction of the X coordinate of the specimen measurement system . To convert an AzDip formatted file ( example.az ) for samples taken from a location name " Northern Iceland " into the MagIC format and save the information in the MagIC ersamples.txt file format , use the program azdipmagic.py : <p> Note that there are many options for relating sample names to site names and we used the @ @ @ @ @ @ @ @ @ @ of the site name to designate each sample ( e.g. , is132d is sample d from site is132 ) . We have also specified certain field sampling and orientation method codes ( -mcd ) , here field sampling-field drilled ( FS-FD ) and sample orientation-Pomeroy ( SO-POM ) . The location was " Northern Iceland " . See the help menu for more options . <p> Another way to do this is to use the orientationmagic.py program which allows much more information to be imported . <p> Note : azdipmagic.py does not currently support MagIC 3 , but will soon . <p> Use the program bvdm to convert an estimated paleofield value of 33 T obtained from a lava flow at 22 N latitude to the equivalent Virtual Dipole Moment ( VDM ) in Am2 . Put the input information into a file called vdminput.dat and read from it using standard input : <p> NB : this program no longer maintained - use plotmapPTS.py for greater functionality . <p> Python has a complete map plotting package and PmagPy has a utility for making simple base maps for projects . Site @ @ @ @ @ @ @ @ @ @ formatted text file can be plotted using basemapmagic.py . There are many options , so check the help message for more details . Note that if you want to use high resolution datafiles or the etopo20 meshgrid ( -etp option ) , you must install the high resolution continental outlines . You can use installetopo.py for that . As an example , use the program basemapmagic.py to make a simple base map with site locations in a MagIC ersites.txt formatted file named basemapexample.txt . <p> $ basemapmagic.py -f basemapexample.txt <p> which makes this plot : <p> Use the buttons at the bottom of the plot to resize or save the plot in the desired format . <p> It is often useful to plot measurements from one experiement against another . For example , rock magnetic studies of sediments often plot the IRM against the ARM or magnetic susceptibility . All of these types of measurements can be imported into a single magicmeasurements formatted file , using magic method codes and other clues ( lab fields , etc. ) to differentiate one from another . Data were obtained from a Paleogene @ @ @ @ @ @ @ @ @ @ , ARM , magnetic susceptibility and remanence data were uploaded to the MagIC database . The magicmeasurements formatted file for this study is saved in coremeasurements.txt . <p> Use the program biplotmagic.py to make a biplot of magnetic susceptibility against ARM . Note that the program makes use of the MagIC method codes which are LT-IRM for IRM , LT-AF-I for ARM ( AF demagnetization , in a field ) , and LP-X for magnetic susceptibility . <p> First , to find out which data are available , run the program like this : <p> The program bootams.py calculates bootstrap statistics for anisotropy tensor data in the form of : <p> x11 x22 x33 x12 x23 x13 <p> It does this by selecting para-data sets and calculating the Hext average eigenparameters . It has an optional parametric bootstrap whereby the s for the data set as a whole is used to draw new para data sets . The bootstrapped eigenparameters are assumed to be Kent distributed and the program calculates Kent error ellipses for each set of eigenvectors . It also estimates the standard deviations of the bootstrapped eigenvalues . @ @ @ @ @ @ @ @ @ @ the data in file bootamsexamples.data : <p> Note that every time bootams gets called , the output will be slightly different because this depends on calls to random number generators . If the answers are different by a lot , then the number of bootstrap calculations is too low . The number of bootstraps can be changed with the -nb option . <p> Paleointensity experiments are quite complex and it is easy to make a mistake in the laboratory . The SIO lab uses a simple chart that helps the experimenter keep track of in-field and zero field steps and makes sure that the field gets verified before each run . You can make a chart for an infield-zerofield , zerofield-infield ( IZZI ) experiment using the program chartmaker.py . Make such a chart using 50C steps up to 500C followed by 10C steps up to 600C . <p> The chart allows you to fill in the file name in which the data were stored and the field value intended for the infield steps . The designations Z , I , T , and P are for zero-field , in-field @ @ @ @ @ @ @ @ @ @ are fields for the date of the runs , the fields measured in different zones in the oven prior to the start of the experiment , and the start and stop times . The numbers , e.g. , 100.1 are the treatment temperatures ( 100 ) followed by the code for each experiment type . These get entered in the treatment fields in the SIO formatted magnetometer files ( see siomagic.py ) . <p> It is sometimes useful to measure susceptibility as a function of temperature , applied field and frequency . Here we use a data set that came from the Tiva Canyon Tuff sequence ( see Carter-Stiglitz , 2006 ) . Use the program chimagic.py to plot the data in the magicmeasurements formatted file : chimagicexample.dat . <p> $ chimagic.py -f chimagicexample.dat IRM-Kappa-2352 1 out of 2 IRM-OldBlue-1892 2 out of 2 Skipping susceptibitily - AC field plot as a function of temperature enter save to save files , return to quit <p> produced this plot : <p> You can see the dependence on temperature , frequency and applied field . These data support the suggestion that there @ @ @ @ @ @ @ @ @ @ MagIC tables have many columns only some of which are used in a particular instance . So combining files of the same type must be done carefully to ensure that the right data come under the right headings . The program combinemagic.py can be used to combine any number of MagIC files from a given type . For an example of how to use this program , see agmmagic.py . <p> Most paleomagnetists use some form of Fisher Statistics to decide if two directions are statistically distinct or not ( see Essentials Chapter 11 for a discussion of those techniques . But often directional data are not fisher distributed and the parametric approach will give misleading answers . In these cases , one can use a boostrap approach , described in detail in Essentials Chapter 12 . Here we use the program commonmean.py for a bootstrap test for common mean to check whether two declination , inclination data sets have a common mean at the 95% level of confidence . The data sets are : commonmeanexfile1.dat and commonmeanexfile2.dat . But first , let 's look at the data in equal area @ @ @ @ @ @ @ @ @ @ commonmeanexfile1.dat -f2 commonmeanexfile2.dat Doing first set of directions , please be patient .. Doing second set of directions , please be patient .. Save plots , &lt;Return&gt; to quit <p> The three plots are : <p> These suggest that the two data sets share a common mean . <p> Now compare the data in commonmeanexfile1.dat with the expected direction at the 5N latitude that these data were collected ( Dec=0 , Inc=9.9 ) . <p> Use the program controt.py to make an orthographic projection with latitude = -20 and longitude = 0 at the center of the African and South American continents reconstructed to 180 Ma using the Torsvik et al . ( 2008 ) poles of finite rotation . Do this by first holding Africa fixed . Move the output plot to fixedafrica.svg . Then make the plot for Africa adjusted to the paleomagnetic reference frame . Make the continental outlines in black lines and set the resolution to low . <p> If one of the MagIC related programs in PmagPy created a very lean looking ersamples.txt file ( for example using azdipmagic.py ) and you need to add @ @ @ @ @ @ @ @ @ @ , etc. ) , you can convert the ersamples.txt file into an orient.txt file format , edit it in , for example Excel , and then re-import it back into the ersamples.txt file format . Try this on the ersamples formatted file in the convertsamples directory , **25;1192;TOOLONG . <p> Use the program coredepthplot.py to plot various measurement data versus sample depth . The data must be in the MagIC data formats . The program will plot whole core data , discrete sample at a bulk demagnetization step , data from vector demagnetization experiments , and so on . There are many options , so check the help menu before you begin . <p> We can try this out on some data from DSDP Hole 522 , measured by Tauxe and Hartl ( 1997 ) . These can be downloaded and unpacked ( see downloadmagic.py for details ) , or you can try it out on the data files in the directory coredepthplot . You must specify a lab-protocol ( LP ) for plotting . In this example , we will plot the alternating field ( AF ) data after @ @ @ @ @ @ @ @ @ @ on a log scale and , as this is a record of the Oligocene , we will plot the Oligocene time scale , using the calibration of Gradstein et al . ( 2004 ) , commonly referred to as " GTS04 " for the the Oligocene . We are only interested in the data between 50 and 150 meters ( the -d option sets this ) and we will suppress the declinations ( -D ) . <p> NB : The best place to customize your selection criteria is within Demag GUI or Thellier GUI . But if you want to do it on the command line , then here 's how . <p> The MagIC database allows documentation of which criteria were used in selecting data on a specimen , sample or site level and to easily apply those criteria ( and re-apply them as they change ) in preparing the different tables . These choices are stored in the pmagcriteria table in each MagIC project directory ( see Pmag GUI documentation ) . <p> Certain PmagPy programs use the datafile pmagcriteria.txt to select data ( for example thelliermagic.py and specimensresultsmagic.py @ @ @ @ @ @ @ @ @ @ sets , you can use the program customizecriteria.py . This program is also called by the Pmag GUI under the Utilities menu . Try it out on pmagcriteria.txt . This is a " full vector " set of criteria - meaning that it has both directions and intensity flags set . Change the specimenalpha95 cutoff to 180. from whatever it is now set to . Save the output to a new file named newcriteria.txt . <p> Use the program dayplotmagic.py to make Day ( Day et al. , 1977 ) , or Squareness-Coercivity and Squareness-Coercivity of Remanence plots ( e.g. , Tauxe et al. , 2002 ) from the rmaghyseresis formatted data in dayplotmagicexample.dat . <p> Paleomagnetic data are frequently plotted in equal area projection . PmagPy has several plotting programs which do this ( e.g. , eqarea.py , but occasionally it is handy to be able to convert the directions to X , Y coordinates directly , without plotting them at all . The program dieq.py does this . Here is an example transcript of a session using the datafile dieqexample.dat : <p> Use the programs digeo.py to convert @ @ @ @ @ @ @ @ @ @ to geographic adjusted coordinates . The orientation of laboratory arrow on the specimen was : azimuth = 347 ; plunge = 27. digeo.py works in the usual three ways ( interactive data entry , command line file specification or from standard input . So for a quickie answer for a single specimen , you can use the interactive mode : <p> which spits out our answer of Declination = 5.3 and inclination = 71.6 . <p> For more data , it is handy to use the file entry options . There are a bunch of declination , inclination , azimuth and plunge data in the file digeoexample.dat in the digeo directory . First look at the data in specimen coordinates in equal area projection , using the program eqarea.py . Note that this program only pays attention to the first two columns so it will ignore the orientation information . <p> $ eqarea.py -f digeoexample.dat <p> which should look like this : <p> The data are highly scattered and we hope that the geographic coordinate system looks better ! To find out try : <p> Generate a Fisher distributed set @ @ @ @ @ @ @ @ @ @ D = 0 , I = 42 using the program fishrot.py . Calculate the mean direction of the data set using gofish.py . Now use the program dirot.py to rotate the set of directions to the mean direction . Look at the data before and after rotation using eqarea.py . <p> Note that every instance of fisher.py will draw a different sample from a Fisher distribution and your exact plot and average values will be different in detail every time you try this ( and that 's part of the fun of statistical simulation . ) <p> Use the program ditilt.py to rotate a direction of Declination = 5.3 and Inclination = 71.6 to " stratigraphic " coordinates . The strike was 135 and the dip was 21 . The convention in this program is to use the dip direction , which is to the " right " of this strike . <p> Use dmagmagic.py to plot out the decay of all alternating field demagnetization experiments in the magicmeasurements formatted file in dmagmagicexample.dat . These are from Lawrence et al . ( 2009 ) . Repeat for all thermal measurements , @ @ @ @ @ @ @ @ @ @ not paleointensity experiments . Try this at the location level and then at the site level . <p> This program unpacks . txt files downloaded from the MagIC database into individual directories for each location into which the individual files for each table ( e.g. , erlocations.txt , magicmeasurements.txt , pmagresults.txt and so on ) get placed . As an example , go to the MagIC data base at http : **27;1219;TOOLONG . Enter " Tauxe and 2004 " into the Reference Text Search field will show you several references . Look for the one for Tauxe , L. , Luskin , C. , Selkin , P. , Gans , P. and Calvert , A. ( 2004 ) . Download the text file under the " Data " column and save it to your desktop . Make a folder into which you should put the downloaded txt file called MagICdownload and move the file into it . Now use the program downloadmagic.py to unpack the . txt file ( zmab0083201tmp03.txt ) . <p> Data are frequently published as equal area projections and not listed in data tables . These data @ @ @ @ @ @ @ @ @ @ the outer rim is unity ) and converted to approximate directions with the program eqdi.py . To use this program , install a graph digitizer ( GraphClick from http : **37;1248;TOOLONG works on Macs ) . <p> Digitize the data from the equal area projection saved in the file eqarea.png in the eqdi directory . You should only work on one hemisphere at a time ( upper or lower ) and save each hemisphere in its own file . Then you can convert the X , Y data to approximate dec and inc data - the quality of the data depends on your care in digitizing and the quality of the figure that you are digitizing . <p> Try out eqdi.py on your datafile , or use eqdiexample.dat which are the digitized data from the lower hemisphere and check your work with eqarea.py . You should retrieve the lower hemisphere points from the eqarea.py example . <p> $ eqdi.py -f eqdiexample.dat &gt;tmp $ eqarea.py -f tmp <p> NB : To indicate that your data are UPPER hemisphere ( negative inclinations ) , use the -up switch . <p> You can @ @ @ @ @ @ @ @ @ @ data using eqarea.py with the original png file . <p> Use the program fishrot.py to generate a Fisher distiributed set of data drawn from a distribution with mean declination of 42 and a mean inclination of 60 . LSave this to a file called it fishrot.out . Use eqarea.py to plot an equal area projection of the data . <p> Use the program tk03.py to generate a set of simulated data for a latitude of 42N including reversals . Then use the program eqareaell.py to plot an equal area projection of the directions in diexample.txt and plot confidence ellipses . Here is an example for Bingham ellipses . <p> Follow the instructions for downloading and unpacking a data file from the MagIC database or use the file in the downloadmagic directory already downloaded from the MagIC website . Plot the directional data for the study from the pmagresults.txt file along with the bootstrapped confidence ellipse . <p> The information printed to the window is the pmagresultname in the data table , the method codes ( here the geochronology method , the type of demagnetization code and the types of demagnetization @ @ @ @ @ @ @ @ @ @ The information following " mode 1 " are the bootstrapped ellipse parameters . <p> Use the program findEI.py to find the optimum flattening factor f which , when used to " unflatten " the data yields inclination and elongation ( ratio of major and minor eigenvalues of orientation matrix , see the section on eigenvalues in the textbook ) most consistent with the TK03.GAD paleosecular variation model of Tauxe and Kent ( 2004 ) . <p> In this example , the original expected inclination at paleolatitude of 42 ( 61 ) is recovered within the 95% confidence bounds . <p> Note that the correction implemented by findEI.py is , by default , meant to be a " study " level correction in which the distribution of data is determined primarily by secular variation ( the TK03.GAD model ) . Alternatively , if you wish to correct " flattened " data to a Fisher distribution ( a " site level " correction ; see section 4.1 of Tauxe and Kent , 2004 ) you can specify this with the -sc flag like so : <p> $ findEI.py -f findEIexample.dat -sc <p> @ @ @ @ @ @ @ @ @ @ this correction-by-site to be reliable . <p> Apparently these directions were acquired prior to folding because the 95% confidence bounds on the degree of untilting required for maximizing concentration of the data ( maximum in principle eigenvalue ) t1 of orientation matrix ( see the section on eigenvalues in the textbook ) includes 100% . <p> This program performs the same test as foldtest.py . The only difference is that it reads MagIC formatted input files and allows the application of selection criteria as specified in the pmagcriteria.txt formatted file . <p> Draw a set of 10 directions drawn from a Fisher distribution with a true mean declination of 15 , a true mean inclination of 41 , and a ? of 50 and save it to a file , then use gofish.py to calculate the Fisher parameters : <p> which according to the help message from gofish.py -h is : mean dec , mean inc , N , R , k , a95 , csd . Your results will vary because every instance of fishrot.py draws a different sample from the Fisher distribution . <p> Draw a set of @ @ @ @ @ @ @ @ @ @ latitude of 42N ( see 14 ) , including reversals . Calculate the eigenparameters of the orientation matrix ( the principal components ) using goprinc.py <p> grabmagickey.py is a utility that will print out any column ( -key option ) from any MagIC Database formatted file . For example , we could print out all the site latitudes from the ersites.txt file down loaded in the downloadmagic.py example : <p> Now read from the file igrfexample.dat which requests field information for near San Diego for the past 3000 years . The program will evaluate the field vector for that location for dates from 1900 to the present using the IGRF/DGRF coefficients in the IGRF-11 model available from : http : **39;1287;TOOLONG . For the period from 1000 BCE to 1800 , the program uses the CALS3k.4b of Korte and Constable ( 2011 ) available at : http : **25;1328;TOOLONG . Prior to that and back to 8000 BCE , the program uses the coefficients of Korte et al. , 2011 for the CALS10k.1b , available at : http : **25;1355;TOOLONG <p> $ igrf.py -f igrfexample.dat -F igrf.out <p> You can @ @ @ @ @ @ @ @ @ @ by specifying the age range and location on the command line . Try this for the location of San Diego for the age range : -3000 1925 in increments of 50 years : <p> Use the program incfish.py to calculate average inclination for inclination only data simulated by fishrot.py for an average inclination of 60 . If you save the declination , inclination pairs , you can compare the incfish.py answer with the Fisher mean . The datafile incfishexampledi.dat has the declination , inclination pairs and incfishexampleinc.dat has just the inclinations . <p> Someone ( Saiko Sugisaki ) measured a number of samples from IODP Expedition 318 Hole U1359A for IRM acquisition curves . She used the ASC impulse magnetizers coils # 2 and 3 and saved the data in the SIO format . Convert these to the MagIC measurements format , combine them in a single magicmeasurements.txt file with combinemagic.py and plot the data with irmaqmagic.py . <p> The first line of each set of four has the specimen name , azimuth , plunge , and bedding strike and dip the next three lines are sets of five measurements @ @ @ @ @ @ @ @ @ @ : <p> The 15 measurements for each specimen , along with orientation information and the specimen name were saved in the file k15example.dat . Convert these to the MagIC format using the program k15magic.py : <p> Use k15s.py to calculate the best-fit tensor elements and residual error for the data in the file k15example.dat ( same file as for k15magic.py . These are : the specimen name , azimuth and plunge and the strike and dip , followed by the 15 measurements made using the Jelinek 1977 scheme shown in the k15magic.py example . Calculate the . s data in specimen , geographic and tilt adjusted coordinates : <p> Output files are in the format of the file KLY4Smagicexample.dat ( found in the **28;1382;TOOLONG folder ) . One option is for orientation information to be output as an azdip formatted file ( see azdipmagic.py . ) Try it out on some data from IODP Expedition 318 ( Hole U1359B ) published by Tauxe et al. , ( 2012 ) . Note that the sample naming convention is # 5 ( -ncn 5 ) and the sample name is the same @ @ @ @ @ @ @ @ @ @ follows : <p> %KLY4Smagic.py -f KLY4Smagicexample.dat -spc 0 -ncn 5 <p> This command will create the files needed by the MagIC database and the data can be plotted using anisomagic.py . If you were to import the sample files from the LIMS data base for these samples , you could plot them versus depth , or as equal area projections using anidepthplot.py and anisomagic.py respectively . <p> Someone ( Saiko Sugisaki ) subjected a number of specimens from IODP Expedition 318 Hole U1359A specimens to a 3-D IRM experiment and saved the data in the SIO format . Use lowrie.py to make plots of blocking temperature for the three coercivity fractions . <p> This program works exactly like lowrie.py , but works on magicmeasurements.txt formatted files . Use siomagic.py to import the lowrieexample.dat data file into the MagIC format . Then use lowriemagic.py to plot the data : <p> This program takes any MagIC formatted file and selects lines that match the search criteria and saves the output as a new MagIC formatted file of the same type . Selection criteria match whole or partial strings , avoid whole or @ @ @ @ @ @ @ @ @ @ syntax is to specify the input file with the -f option , the key you wish to search on with -key , followed by the string you wish to use as a criterion , followed with a T for match exactly , F avoid entirely , has for matching partially and not for avoiding any partial match , and min , max and eval for evaluating the numerical value . The -F option sets the output file . <p> For example , you could pick out all the records that match a certain site name string . You could select out records with specified method codes . You could get all records with inclinations that are positive . <p> Use magicselect.py to pick out all the best-fit lines based on AF demagnetization data from the pmagspecimens.txt file unpacked from the MagIC database in the downloadmagic.py example . <p> This program imports the binary format generated by the 2G proprietary software . It will place the orientation information in the ersamples.txt file and ersites.txt file by default . Naming and orientation conventions can be specified as in the sections on Naming @ @ @ @ @ @ @ @ @ @ method codes that pertain to ALL samples ( see section on field information . ) <p> This program imports Micromag hysteresis files into magicmeasurements formatted files . Because this program imports data into the MagIC database , specimens need also to have sample/site/location information which can be provided on the command line . If this information is not available , for example if this is a synthetic specimen , specify -syn SYNNAME for synthetic on the command line instead of the -spn SPECNAME switch . <p> Someone named Lima Tango has measured a synthetic specimen named myspec for hysteresis and saved the data in a file named agmmagicexample.agm . The backfield IRM curve for the same specimen was saved in agmmagicexample.irm . Use the program AGMmagic.py to import the data into a magicmeasurements formatted output file . These were measured using cgs units , so be sure to set the units switch properly . Combine the two output files together using combinemagic.py . This can be plotted using hysteresismagic.py or quickhyst.py. hysteresismagic.py will calculate various hysteresis parameters and put them in the relevant magic tables for you . s <p> @ @ @ @ @ @ @ @ @ @ formats , including the CIT format developed for the Caltech lab and now utilized in magnetometer control software that ships with 2G magnetometers that utilized a vertical sample changer system . The documentation for the CIT sample format is here : LONG ... Demagnetization data for each specimen are in their own file in a directory with all the data for a site or study . These files are strictly formatted with fields determined by the character number in the line . A datafile generated in the UC Berkeley paleomagnetism lab , for a specimen published in Fairchild et al. , ( 2016 ) ( available in the MagIC database : ( https : **27;1412;TOOLONG ) has this format : <p> There must be a file with the suffix . sam in the same directory as the specimen data files which gives details about the specimens and a list of the specimen files in the directory . The . sam file has the form : <p> The first line is a comment ( in this case the site name ) , the second is the latitude and longitude followed by @ @ @ @ @ @ @ @ @ @ correction was applied to the specimen orientations so the value of the declination correction is set to be 0 . <p> For detailed description of the file format , check the PaleoMag website . <p> Use the program CITmagic.py to import the data files from the example data files in the CITmagic directory of the MeasurementImport directory of the datafiles directory . The location name was " Slate Islands " , the naming convention was #2 , the specimen name is specified with 1 character ( -spc 1 ) , we do n't  wish to average replicate measurements ( -A ) and they were collected by drilling and with a magnetic compass ( -mcd " FS-FD:SO-MAG " ) . <p> If you have a data file format that is not supported , you can relabel column headers to fit the generic format in genericmagic.py : <p> specimen <p> treatment <p> treatmenttype <p> moment <p> decs <p> incs <p> sr01e2 <p> 0 <p> A <p> 2.93E-02 <p> 200.6 <p> 27.9 <p> sr01e2 <p> 10 <p> A <p> 2.83E-02 <p> 201 <p> 27.7 <p> sr01e2 <p> 20 <p> A <p> 2.61E-02 <p> @ @ @ @ @ @ @ @ @ @ 2.37E-02 <p> 199.9 <p> 27.7 <p> sr01e2 <p> 60 <p> A <p> 1.85E-02 <p> 202.5 <p> 27.9 <p> . <p> . <p> For more options , here is the help message for genericmagic.py : <p> A generic file is a tab-delimited file . Each columns should have a header . The file must include the follwing headers ( the order of the columns is not important ) : <p> specimen string specifying specimen name <p> treatment : a number with one or two decimal point ( X.Y ) coding for thermal demagnetization : 0.0 or 0 is NRM . X is temperature in celcsius Y is always 0 coding for AF demagnetization : 0.0 or 0 is NRM . X is AF peak field in mT Y is always 0 coding for Thellier-type experiment : 0.0 or 0 is NRM X is temperature in celcsius Y=0 : zerofield Y=1 : infield Y=2 : pTRM check Y=3 : pTRM tail check Y=4 : Additivity check # Ron , Add also 5 for Thellier protocol <p> treatmenttype : N : NRM A : AF T : Thermal <p> moment : magnetic moment @ @ @ @ @ @ @ @ @ @ one of the following headers are required : <p> decs : declination in specimen coordinate system ( 0 to 360 ) incs : inclination in specimen coordinate system ( -90 to 90 ) <p> decg : declination in geographic coordinate system ( 0 to 360 ) incg : inclination in geographic coordinate system ( -90 to 90 ) <p> dect : declination in tilt-corrected coordinate system ( 0 to 360 ) inct : inclination in tilt-corrected coordinate system ( -90 to 90 ) <p> This program is called within Pmag GUI but could be used as a stand alone program if you wish . <p> Just for fun , download the whole core data from IODP expedition 318 , Site U1359 , Hole A. ( Section Half set to A ) from the IODP LIMS database using WebTabular . These data were used in Tauxe et al . ( 2012 ) after some editing ( removal core ends and disturbed intervals ) . Convert them into the MagIC format using ODPcsvmagic.py : <p> This format is the one used to import . PMD formatted magnetometer files ( used for example @ @ @ @ @ @ @ @ @ @ the MagIC format . ( See LONG ... for the PaleoMac home page . The version of these files that PMDmagic.py expects ( UCSC version ) contains demagnetization data for a single specimen and have a format like this : <p> The first line is a comment line . The second line has the specimen name , the core azimuth ( a= ) and plunge ( b= ) which are assumed to be the lab arrow azimuth and plunge ( Orientation scheme #4 ) D. The third line is a header explaining the columns in the file . <p> Use pmdmagic.py to convert the file ss0101a.pmd in the directory PMD in the PMDmagic folder of the Measurementimport directory in the example datafiles directory . These were taken at a location named Summit Springs and have a naming convention of the type XXXXYYY , where YYY is sample designation with Z characters from site XXX , ornaming convention # 4-2 . A single character distinguishes the specimen from the sample ( -spc 1 ) . All samples were oriented with a magnetic compass . <p> Because each file must be imported @ @ @ @ @ @ @ @ @ @ output file for each input file ( otherwise you will overwrite the default each time ) and set the switch for sample file to append for subsequent imports : <p> The program siomagic.py allows conversion of the SIO format magnetometer files to the MagIC common measurements format . It allows various experiment types so read the help message . The SIO format is a space delimited file : <p> The treatment field is the temperature ( in centigrade ) , the AF field ( in mT ) , the impulse field strength , etc . For special experiments like IRM acquisition , the coil number of the popular ASC impulse magnetizer can be specified if the treatment steps are in volts . The position for anisotropy experiments or whether the treatment is " in-field " or in zero field also require special formatting . The units of the intensity field are in cgs and the directions are relative to the lab arrow on the specimen . Here are some examples of commonly used specimens and conversions from field arrow to lab arrow . <p> As an example , we use @ @ @ @ @ @ @ @ @ @ on a set of samples from the location " Socorro " , including AF , thermal , and thellier experimental data . These were saved in sioafexample.dat , siothermalexample.dat , **25;1441;TOOLONG respectively . The lab field for the thellier experiment was 25 T and was applied along the specimens Z axis ( phi=0 , theta=90 ) . Convert the example files into magicmeasurement formatted files with names like afmeasurements.txt , etc . Then combine them together with combinemagic.py : <p> The Agico Kappabridge instrument comes with the SUFAR program which makes the measurements and saves the data in a txt file like that in **25;1468;TOOLONG in the sufar4ascmagic directory . These data were measured on a KLY4S instrument with a spinning mode . Import them into the MagIC format : <p> The first two lines are headers . The first column of the second line is the applied field in T. The rest of this line is azimuth and plunge of the fiducial line and dip direction and dip of the bedding plane . The data columns are : specimen , treatment , intensity , declination and inclination . the @ @ @ @ @ @ @ @ @ @ is of the form XXX.YY where XXX is the temperature and YY is 00 , 11 , 12 , 13 , 14 OR 0 , 1 , 2 , 3 , 4 where 0 is the NRM step , 11/1 is the pTRM acquisition step , 12/2 is the pTRM check step and 13/3 is the pTRM tail check . 14/4 is the additivity check . <p> Use the program TDTmagic.py to import the ThellierTool tdt formatted file TDTmagicexample.dat into the MagIC format . The field was 52.12 T applied along the 0,0 direction . <p> This program takes specimen weights or volumes from an erspecimen.txt formatted file and normalizes the magnetic moment data to make magnetizations . Weights must be in kilograms and volumes in m3 . <p> Use the program measurementsnormalize.py to generate weight normalized magnetizations for the data in the file magicmeasurements.txt in the it measurementsnormalize directory . Use the specimen weights in the file specimenweights.txt . <p> The programs zeqmagic.py and thelliermagic.py as well as the two GUIs Demag GUI and Thellier GUI make pmagspecimen formatted files which can be used for further data reduction either @ @ @ @ @ @ @ @ @ @ Sometimes it is useful to redo the calculation , using anisotropy corrections or a change in coordinate systems , etc . The re-doing of these specimen level calculations is handled by , for example zeqmagicredo.py or thelliermagicredo.py . These programs use magicmeasurements formatted files and perform calculations as dictated by a " redo " file which has the specimen name , bounds for calculation and , in the case of the demagnetization data interpretation , the type of calculation desired ( best-fit lines with directional estimation magic method code:DE-BFL , best-fit planes with those with magic method code DE-BFP , etc . ) . <p> Make " redo " files from the existing pmagspecimen formatted file in the data files downloaded from the MagIC website as in downloadmagic.py and examine them as follows : <p> After making NRM measurements , it is frequently useful to look at the directions in equal area projection to get a " quick look " at the results before proceeding to step wise demagnetization . The data in the magicmeasurements files are usually in specimen coordinates - not geographic , so we need a way @ @ @ @ @ @ @ @ @ @ and save them in a pmagspecimens formatted file for plotting with eqareamagic.py . The program nrmspecimensmagic.py will do this for you . <p> Get into the directory you made for the downloadmagic.py example . Use nrmspecimensmagic.py to convert the NRM measurements in magicmeasurements.txt to geographic coordinates saved in a file named nrmspecimens.txt . The orientation data are in the file ersamples.txt . Then plot the specimen directions for the entire study using eqareamagic.py : <p> Try to import the file orientationexample.txt into the ersamples.txt and ersites.txt files using orientationmagic.py . Click here for details about the orient.txt file format . It has field information for a few sites . The samples were oriented with a Pomeroy orientation device ( the default ) and it is desirable to calculate the magnetic declination from the IGRF at the time of sampling ( also the default ) . Sample names follow the rule that the sample is designated by a letter at the end of the site name ( convention #1 - which is the default ) . So we do this by : <p> This program reads in the magicmeasurements.txt file and @ @ @ @ @ @ @ @ @ @ can then be put in columns labeled specimenvolume and specimenweight respectively . Volumes must be in m3 and weights in kg . ( Yes you can do the math ... ) . <p> Try this out on the magicmeasurements.txt file created in the irmaqmagic.py example . Pretend you have a bunch of specimen weights you want to use to normalize the NRM with the program measurementsnormalize.py . <p> $ parsemeasurements.py site record in ersites table not found for : 318-U1359A-002H-1-W-109 site record in ersites table not found for : 318-U1359A-002H-2-W-135 site record in ersites table not found for : 318-U1359A-002H-3-W-45 site record in ersites table not found for : 318-U1359A-002H-4-W-65 ..... <p> The program appears a bit flustered because you have no ersites.txt file in this directory . If you DID , you would overwrite whatever site name was in that file onto the specimen table . This allows you to carry the changes in that table through to the specimen table ( see orientationmagic.py . ) <p> This program calculates best-fit lines , planes or Fisher averages through selected treatment steps . The file format is a simple space @ @ @ @ @ @ @ @ @ @ , declination and inclination . Calculate the best-fit line through the first ten treatment steps in data file zeqexample.txt : <p> This program plots cumulative distribution functions of a single column of input data . Use as an example , a normally distributed set of 1000 data points generated by gaussian.py . Use the defaults of zero mean with a standard deviation of 1 . <p> NOTE : This program only works if you have installed basemap ( full version of Canopy Python as opposed to the free version . ) It may not work with free versions . plotmappts.py will generate a simple map of the data points in a file ( lon lat ) on the desired projection . If you want to use high resolution or the etopo20 meshgrid ( -etp option ) , You must install the etopo20 data files and can use installetopo.py for that . There are many options , so check the documentation ( -h option ) for details . <p> Draw a set of 200 uniformly distributed points on the globe with the program uniform.py . Plot these on an orthographic projection @ @ @ @ @ @ @ @ @ @ a latitude of 30N . If you have installed the high resolution data sets , use the -etp option to plot the topographic mesh . Plot the points as large ( size = 10 ) white dots . Also note that for some reason the . svg output does not place nicely with illustrator . <p> This program extracts a tab delimited txt file from a pmagresults formatted file . This allows you to publish data tables that have identical data to the data uploaded into the MagIC database . Try this out in the directory created for the downloadmagic.py example : <p> This program reads in a file with location , age and destination plate and rotates the data into the destination plate coordinates using the rotations and methods in Essentials Appendix A.3.5 . Alternatively , you can supply your own rotation parameters with the -ff option . <p> First , save the location of Cincinnati ( 39.1 latitude , -84.7 longitude ) in a file called ptrot.input using either the UNIX cat function or your favorite text editor . Save the data as a lon/lat pair separated by @ @ @ @ @ @ @ @ @ @ Cincinnati to ( South ) African ( saf ) coordinates at 80 Ma . Plot both points using plotmapPTS.py . You should save the lon , lat , plate , age , destinationplate information in a file called ptrotexample.dat in the following : <p> Now , use the program to rotate a selection of North American poles from 180-200 Ma ( in the file nam180-200.txt in the ptrot directory ) to Pangea A coordinates ( finite rotation pole and angle in nampanA.frp . Note that plotmapPTS.py reads in longitude/latitude pairs , while ptrot.py reads in latitude longitude pairs . <p> Makes a quantile-quantile plot of the input data file against a normal distribution . The plot has the mean , standard deviation and the D statistic as well as the Dc statistic expected from a normal distribution . Use qqplot.py to test whether the data generated with gaussian.py is in fact normally distributed . ( It will be 95% of the time ! ) . <p> hysteresismagic.py makes plots of hysteresis loops and calculates the main hysteresis parameters . For a quick look with no interpretation , you can use @ @ @ @ @ @ @ @ @ @ in the hysteresismagic directory . <p> Same as revtest.py but for pmagsites MagIC formatted files . Try it out on the data file revtestsites.txt . Then try using customizecriteria.py to change or create a pmagcriteria.txt file that fits your needs and redo the reversals test using only the selected sites . <p> This program tests whether two sets of observations ( one with normal and one with reverse polarity ) could have been drawn from distributions that are 180 apart using the McFadden and McElhinny ( 1990 ) implementation of the Watson ( 1983 ) V statistic test for a common mean . The implementation of the V statistic test in this program is the same as in watsonsv.py . Use revtestmm1990.py to perform a reversal test on data from the Ao et al. , 2013 study of Early Pleistocene fluvio-lacustrine sediments of the Nihewan Basin of North China . <p> Let 's plot the combined data set Aoetal2013combined.txt using eqarea.py : <p> $ eqarea.py -f Aoetal2013combined.txt <p> Conduct a reversal test between the normal directions Aoetal2013norm.txt and reversed directions Aoetal2013rev.txt : <p> Watsons V : 2.7 Critical value of V @ @ @ @ @ @ @ @ @ @ , the null hypothesis that the two populations are drawn from distributions that share a common mean direction ( antipodal to one another ) can not be rejected . <p> M&amp;M1990 classification : <p> Angle between data set means : 2.9 Critical angle for M&amp;M1990 : 4.3 The McFadden and McElhinny ( 1990 ) classification for this test is : A <p> As with watsonsv.py , this program produces a plot that displays the cumulative distribution ( CDF ) of the Watson V values resulting from the Monte Carlo simulation . In the plot below , the red line is the CDF , the green solid line is the value of the V statistic calculated for the two populations and the dashed blue line is the critical value for the V statistic . In this example , the data are consistent with the two directional groups sharing a common mean as evidenced by V ( green solid line ) being a lower value than the critical value of V ( dashed blue line ) . <p> Import . s format file output from the stilt.py example into an rmaganisotropy formatted @ @ @ @ @ @ @ @ @ @ with anisomagic.py . To see how this works , use the program smagic.py as follows : <p> This is the same as scalc.py but works on pmagresults formatted files . Try it out on the pmagresults.txt file in the directory created for the downloadmagic.py example . Use a VGP co-latitude cutoff of 30 . <p> There are frequently outlier directions when looking at data at the site level . Some paleomagnetists throw out the entire site , while some arbitrarily discard individual samples , assuming that the orientations were bad . Lawrence et al . ( 2009 ) suggested a different approach in which common causes of misorientation are specifically tested . For example , if the arrow indicating drill direction ( see orientation conventions ) was drawn the wrong way on the sample in the field , the direction would be off in a predictable way . Similarly ( and more commonly ) extraneous marks on the sample were used instead of the correct brass mark , the directions will fall along a small circle which passes through the correct direction . The program siteeditmagic.py plots data by site @ @ @ @ @ @ @ @ @ @ select a particular specimen direction and plots the various pathological behaviors expected from common misorientations . If the stray directions can reasonably be assigned to a particular cause , then that sample orientation can be marked as bad with a note as to the reason for exclusion and the directions from that sample can be excluded from the site mean , and the reason can be documented in the MagIC database . <p> The latter plausibly fits the " wrong mark " hypothesis , while the former does not fit any of the common pathologies . The sample orientation in the ersamples.txt file has a note that it is bad and will be ignored by the PmagPy programs when calculating site means . <p> Starting from the magnetometer output files , paleomagnetists interpret the data in terms of vectors , using software like zeqmagic.py or thelliermagic.py . The interpretations from these programs are stored in pmagspecimen formatted files . Once a pmagspecimens format file has been created , the data are usually averaged by sample and/or by site and converted into VADMs and/or VGPs and put in a pmagresults formatted @ @ @ @ @ @ @ @ @ @ available . Data must be selected or rejected according to some criteria at each level ( for example , the specimen MAD direction must be less than some value or the site ? must be greater than some value ) . This onerous task can be accomplished using the program specimensresultsmagic.py . This program has many optional features , so the reader is encouraged just to look at the documentation . <p> Test it out by using the data files created in the siteeditmagic.py example . Block the interpretation of some of the samples owing to bad orientation . Change the selection criteria using customizecriteria.py . Then generate a new pmagspecimen.txt file using mkredo.py , zeqmagicredo.py and combinemagic.py . Then do the averaging by site with specimensresultsmagic.py . The age bounds of the data are 0-5 Ma ( -age 0 5 Ma ) . Use the existing ( or modified ) criteria ( -exc option ) . There are some paleointensity data , so use the current site latitude for the VADM calculation ( -lat option ) . Calculate the data in the geographic coordinate system ( -crd g ) @ @ @ @ @ @ @ @ @ @ pmagresultsextract.py <p> Follow the instructions for downloadmagic.py but search for Tauxe and Hartl and 1997 . Download the smartbook ( text file ) and unpack it into a new directory using the downloadmagic.py command and the file zmab0094214tmp02.txt as the input file ( in the Tauxe and Hartl directory ) . First run stripmagic.py to see what is available for plotting , then plot the inclination data versus depth ( pos ) . Then plot the VGP latitudes versus age : <p> Use the program sundec.py to calculate azimuth of the direction of drill . You are located at 35 N and 33 E. The local time is three hours ahead of Universal Time . The shadow angle for the drilling direction was 68 measured at 16:09 on May 23 , 1994 . <p> The program sundec.py works either by interactive data entry or by reading from a file . <p> Unpack the data with downloadmagic.py . The original interpretations are stored in a file called pmagspecimens.txt , but thelliermagic.py looks for a specimen file called thellierspecimens.txt . To create such a file , use the program mkredo.py followed by @ @ @ @ @ @ @ @ @ @ data using thelliermagic.py and change some of the interpretations , for example adding one for the specimen s2s0-01 which was rejected by Shaar et al . ( 2011 ) . This has both non-linear TRM acquisition and ATRM anisotropy data , so the corrections will have to be applied using thelliermagicredo.py after exiting the thelliermagic.py program . <p> This program is no longer maintained . This function is embedded in thelliergui.py when you import a specimen file . <p> This program allows the recalculation of paleointensity experimental data derived from Thellier type experiments . It allows correction for remanence anisotropy from AARM or ATRM ellipsoids ( stored in rmaganisotropy format files ( see Essentials Chapter 13 and the aarmmagic.py and atrmmagic.py ) and non-linear TRM corrections , if TRM aquisition data are available ( see also trmaqmagic.py ) . <p> Apply the corrections to the data you re-interpreted in the thelliermagic.py example , and create a new pmagspecimens.txt formatted file , use the program thelliermagicredo.py . First , create a " redo " file using , then re-do the calculations using thelliermagicredo.py , including anisotropy and non-linear TRM corrections . @ @ @ @ @ @ @ @ @ @ <p> Sometimes it is useful to generate a distribution of synthetic geomagnetic field vectors that you might expect to find from paleosecular variation of the geomagnetic field . The program tk03.py generates distributions of field vectors from the PSV model of Tauxe and Kent ( 2004 ) . Use this program to generate a set of vectors for a latitude of 30 and plot them with eqarea.py . <p> This program is no longer maintained . This function is embedded in Pmag GUI . <p> In the SIO laboratory , our default specimen naming scheme has a logical relationship to the sample and site names . Sometimes there is no simple relationship and the MagIC import procedure ca n't parse the sample name into a site name , or the original concept of a site gets changed , for example if several lava flows turn out to be the same one . In any case , there must be a way to change the way data get averaged after the measurement data get converted to a magicmeasurments formatted file . To do this , the sample , site relationship can be @ @ @ @ @ @ @ @ @ @ described in orientationmagic.py docuementation . After the correct relationships are in the ersamples.txt file , these must be propagated throughout the rest of the MagIC tables , starting with the magicmeasurements.txt file . The program updatemeasurements.py will do this at the measurements level . specimensresultsmagic.py will then propagate the changes through to the pmagresults.txt file . <p> This program takes all the MagIC formatted files and puts them into a file which can be imported into the MagIC console software for uploading into the MagIC database . As an example , we can " repackage " the file used file downloaded in the downloadmagic.py example . You could re-interpret that data or fix records with errors , then re-upload the corrected file . <p> In the uploadmagic directory in the example datafiles directory , someone has entered all the site latitudes as longitudes and vice versa ( surprisingly common ! ) . You can fix this in the ersites.txt file , by swapping the headers samplelat/samplelon in the ersamples.txt file and sitelat/sitelon in the ersites.txt file then run specimensresultsmagic.py to propogate the changes through to the results table . Be @ @ @ @ @ @ @ @ @ @ the latitude switch ( -lat ) , the geographic coordinate switch ( -crd g ) and the age switch ( -age 0 5 Ma ) . Then re-package the files for uploading . <p> AFTER fixing the ersamples.txt and ersites.txt files , do the following : <p> Make a plot of the VGPs calculated for the dataset downloaded in the downloadmagic.py example . Install the high resolution and the etopo20 data files . Use installetopo.py for that . and plot the data on an orthographic projection with the viewpoint at 60N and the Greenwich Meridian ( longitude = 0 ) . Make the points black dots with a size of 10pts . Save the file in png format <p> To use this program , you need to have basemap , which is not part of the free Python software distribution . Also , if you do n't  have the high resolution installed by installetopo.py , just leave the -etp switch off to get a plain plot . <p> The first number is Watsons F statistic for these two files ( see Essentials Chapter 11 ) and the second is the @ @ @ @ @ @ @ @ @ @ from the same fisher distribution ( share a common mean ) . In this case the data fail this test ( F is greater than the required number ) . Your results may vary ! <p> Use the program zeq.py to 1 ) plot a Zijderveld diagram of the data in zeqexample.txt. b ) Calculate a best-fit line from 15 to 90 mT . c ) Rotate the data such that the best-fit line is projected onto the horizontal axis ( instead of the default , North ) . d ) Calculate a best-fit plane from 5 to 80 mT . Save these plots . <p> $ zeq.py -f zeqexample.dat -u mT <p> By selecting b , you can pick the bounds and choose l for best-fit line or p for best-fit plane . You can rotate the X-Y axes by selecting h and setting the X axis to 312 . Finally , you can save your plots with the a option . You should have saved something like these plots : <p> The equal area projection has the X direction ( usually North in geographic coordinates ) to the @ @ @ @ @ @ @ @ @ @ the Zijderveld diagram . Solid symbols are lower hemisphere . The solid ( open ) symbols in the Zijderveld diagram are X , Y ( X , Z ) pairs . The demagnetization diagram plots the fractional remanence remaining after each step . The green line is the fraction of the total remanence removed between each step . <p> The plots will look similar to the zeq.py example , but the default here is for the X-axis to be rotated to the specimens NRM direction ( note how the X direction is rotated off from the top of the equal area projection ) . <p> The functionality of this program has been incorporated into Demag GUI so you should use that . <p> In the zeqmagic directory , use the program zeqmagicredo.py to create a pmagspecimens formatted file with data in geographic coordinates from the sample coordinate file zeqspecimens.txt . Assuming that sample orientations are in a file called ersamples.txt , use mkredo.py first to create file called zeqredo . Then use zeqmagicredo.py to create two pmagspecimen formatted files : one in specimen coordinates zeqspecimenss.txt and one in geographic coordinates @ @ @ @ @ @ @ @ @ @ <p> The best way to raise issues with any of the software or its documentation is to post to the issues page of the Github project : https : **33;1523;TOOLONG . Lisa Tauxe ( ltauxe@usd.edu ) can also be contacted by email with bug reports , suggestions and requests . <p> A number of the programs in PmagPy were developed to take advantage of the MagIC database and aid getting data in and out of it . So , we need some basic understanding of what MagIC is and how it is structured . MagIC is an Oracle database that is part of the EarthRef.org collection of databases and digital reference material . Anyone interested in the MagIC database should first become a registered EarthRef.org user . To do this , go to http : //earthref.org and click on the Register link in the Topmenu . Registration is not required for access to data or browsing around , but is required for uploading of data into the MagIC database , something which we sincerely hope you will have a chance to do . After you register , go to http @ @ @ @ @ @ @ @ @ @ MagIC search website . After downloading , the data can be unpacked and examined using various tools in the PmagPy package , for example using Pmag GUI . <p> Paleomagnetic and rock magnetic data are collected and analyzed in a wide variety of ways with different objectives . Data sets can be extremely large or can be the barest boned data summaries published in legacy data tables . The goal of MagIC has been to have the flexibility to allow a whole range of data including legacy data from publications or other databases to new studies which include all the measurements , field photos , methodology , and so on . The general procedure for the future will be to archive the data at the same time that they are published . So , to smooth the path , it is advisable to put your data into the MagIC format as early in the process as possible . All data that enters the database must be converted to the standard MagIC format either as a set of MagIC tables , or as one combined text file . These can then @ @ @ @ @ @ @ @ @ @ " ( or " tab delimited " ) means that the table is tab delimited . In theory other delimiters are possible , but PmagPy only uses tab delimited formats . The tablename is one of the table names . The tables are of four general types : EarthRef tables ( er ) shared in common with other EarthRef databases , MagIC tables ( magic ) common to both rock magnetic and paleomagnetic studies , Paleomagnetic tables ( pmag ) , data reduction useful in paleomagnetic studies , Rock magnetic tables ( rmag ) , data reduction useful for rock magnetic studies . Most studies use only some of these tables . Here are some useful tables for a typical paleomagnetic study ( starred are required in all cases ) : <p> table <p> Brief description <p> erlocations* <p> geographic information about the location(s) of the study <p> ersites <p> locations , lithologic information , etc. for the sampling sites <p> ersamples <p> orientation , sampling methods , etc. for samples <p> erspecimens <p> specimen weights , volumes <p> erages <p> age information . <p> erimages <p> images associated with @ @ @ @ @ @ @ @ @ @ photomicrographs , SEM images , etc . <p> ermailinglist <p> contact information for people involved in the study <p> magicmeasurements <p> measurement data used in the study <p> magicinstruments <p> instruments used in the study <p> pmagspecimens <p> interpretations of best-fit lines , planes , paleointensity , etc . <p> pmagsamples <p> sample averages of specimen data <p> pmagsites <p> site averages of sample data <p> pmagresults <p> averages , VGP/VADM calculations , stability tests , etc . <p> pmagcriteria <p> criteria used in study for data selection <p> rmagsusceptibility <p> experiment for susceptibility parameters <p> rmaganisotropy <p> summary of anisotropy parameters <p> rmaghysteresis <p> summary of hysteresis parameters <p> rmagremanence <p> summary of remanence parameters <p> rmagresults <p> summary results and highly derived data <p> products ( critical temperatures , etc . ) <p> rmagcriteria <p> criteria used in study for data selection <p> The second line of every file contains the column headers ( meta-data ) describing the included data . For example , an ersites table might look like this : <p> tab ersites <p> ersitename <p> erlocationname <p> sitelithology <p> sitetype <p> sitelat <p> sitelon <p> @ @ @ @ @ @ @ @ @ @ <p> -25.80 <p> ... <p> Although data can be entered directly into Excel spreadsheets by hand , it is easier to generate the necessary tables as a by-product of ordinary data processing without having to know details of the meta-data and method codes . The section on PmagPy describes how to use the PmagPy software for data analysis and generate the MagIC data tables automatically for the most common paleomagnetic studies involving directions and/or paleointensities . See also Pmag GUI . <p> The MagIC database tags records with " method codes " which are short codes that describe various methods associated with a particular data record . The complete list is available here : http : **32;1587;TOOLONG . Most of the time , you do not need to know what these are ( there are over a hundred ! ) , but it is helpful to know something about them . These are divided into several general categories like geochronology methods and field sampling methods . Method codes start with a few letters which designate the category ( e.g. , GM or FS for geochronogy and field sampling respectively ) @ @ @ @ @ @ @ @ @ @ a third part to describe methods with lesser or greater detail . The current ( version 2.4 ) method codes that describe various lab treatment methods to give you a flavor for how they work are listed in this table : <p> LT-AF-D 1453 @qwx861453 <p> Alternating field : Double demagnetization <p> with AF along X , Y , Z measurement <p> followed by AF along -X , -Y , -Z measurement <p> LT-AF-G 1453 @qwx861453 <p> Alternating field : Triple demagnetization <p> with AF along Y , Z , X measurement <p> followed by AF along Y and AF along Z measurement <p> LT-AF-I 1453 @qwx861453 <p> Alternating field : In laboratory field <p> LT-AF-Z 1453 @qwx861453 <p> Alternating field : In zero field <p> LT-CHEM 1453 @qwx861453 <p> Cleaning of porous rocks by chemical leaching with HCl <p> LT-FC 1453 @qwx861453 <p> Specimen cooled with laboratory field on <p> LT-HT-I 1453 @qwx861453 <p> High temperature treatment : In laboratory field <p> LT-HT-Z 1453 @qwx861453 <p> High temperature treatment : In zero field <p> LT-IRM 1453 @qwx861453 <p> IRM imparted to specimen prior to measurement <p> LT-LT-I 1453 @qwx861453 @ @ @ @ @ @ @ @ @ @ 1453 @qwx861453 <p> Low temperature treatment : In zero field <p> LT-M-I 1453 @qwx861453 <p> Using microwave radiation : In laboratory field <p> LT-M-Z 1453 @qwx861453 <p> Using microwave radiation : In zero field <p> LT-NO 1453 @qwx861453 <p> No treatments applied before measurement <p> LT-NRM-APAR 1453 @qwx861453 <p> Specimen heating and cooling : Laboratory <p> field anti-parallel to the NRM vector <p> LT-NRM-PAR 1453 @qwx861453 <p> Specimen heating and cooling : Laboratory <p> field parallel to the NRM vector <p> LT-NRM-PERP 1453 @qwx861453 <p> Specimen heating and cooling : <p> Laboratory field perpendicular to the NRM vector <p> LT-PTRM-I 1453 @qwx861453 <p> pTRM tail check : After zero field step , <p> perform an in field cooling <p> LT-PTRM-MD 1453 @qwx861453 <p> pTRM tail check : After in laboratory field step , <p> perform a zero field cooling at same temperature <p> LT-PTRM-Z 1453 @qwx861453 <p> pTRM tail check : After in laboratory field step , <p> perform a zero field cooling at a lower temperature <p> LT-T-I 1453 @qwx861453 <p> Specimen cooling : In laboratory field <p> LT-T-Z 1453 @qwx861453 <p> Specimen cooling : In zero field <p> @ @ @ @ @ @ @ @ @ @ <p> LP-X 1453 @qwx861453 <p> Susceptibility <p> LT-ZF-C 1453 @qwx861453 <p> Zero field cooled , low temperature IRM imparted <p> LT-ZF-CI 1453 @qwx861453 <p> Zero field cooled , induced M measured on warming <p> For uploading to the database , all the individual tables should be assembled into a single file . Each individual data table is separated from the next by a series of **40;1621;TOOLONG symbols , so a typical upload file might look like this : <p> To get out of Python interactive mode and back to your beloved command line , type the control key ( here-after ) and D at the same time . From your school experience with algebra , you will recognize a , b , and c in the above session as variables and + as an operation . If you have previous programming experience , you may have been surprised that we did n't  declare variables up front ( C programmers always have to ) . And , the variables above are behaving as integers , not floating point variables ( no decimals ) but they are not letters between i and @ @ @ @ @ @ @ @ @ @ set three numbers at once : <p> &gt;&gt;&gt; a=2 ; b=2 ; c=a+b ; c and &gt;&gt;&gt; d , e , f=4,5,6 # note syntax ! d=4 ; e=5 ; f=6 <p> Here are some rules governing variables and operations in Python : <p> Variable names are composed of alphanumeric characters , including - and . <p> Variable names are case sensitive : a is not the same as A. <p> Variable names do NOT have to be specified in advance ( unlike C ) <p> In Fortran , integers are i - n and floating points are all else - not the case in Python . You can make them whatever you want . <p> These two are fun : += and -= . They add to and subtract from respectively . <p> Parentheses determine order of operation ( as in any reasonable programming language ) . <p> For math functions , we can use various modules that either come standard with python ( the math module ) or are additions that come with the Enthought Python Edition we are using ( the NumPy module ) . A module @ @ @ @ @ @ @ @ @ @ is online help for any python function or method : just type help(FUNC) . <p> First of all - how do you pronounce NumPy ? According to Important People at Enthought ( e.g , Robert Kern ) and the SciPy email list , it should be pronounced " Num " as in " Number " and " Pie " as in , well , pie , or Python . But it is also pretty fun to say Numpee ! <p> So , that out of the way , what can NumPy do for us ? Turns out , a whole heck of a lot ! For now , we will just scratch the surface . It can , for example , give us the value of p as numpy.pi . Note how the module name comes first , then the name of the function we wish to use . In this case , the function just returns the value of p . <p> To use NumPy functions , we must first " import " the module with the command import . The first time you do this after installing Python @ @ @ @ @ @ @ @ @ @ it should be very quick . <p> There are four styles of the import command which all do pretty much the same thing , but differ in how you have to call the function after importing : <p> &gt;&gt;&gt; import numpy &gt;&gt;&gt; numpy.pi 3.1415926535897931 <p> This makes all the functions in NumPy available to you , but you have to call them with the numpy.FUNC syntax . <p> Importing as np does the same import as import numpy , but allows you to call NumPy anything you want for brevity and convenience . It has become a standard convention in scientific Python , including throughout the NumPy and SciPy source code and documentation , to import numpy as np . To import all the functions from NumPy and not have to type numpy at all : <p> &gt;&gt;&gt; from numpy import * &gt;&gt;&gt; pi 3.1415926535897931 <p> This imports all the umpty-ump functions , which is a heavy load on your memory and can lead to namespace collisions ( see good housekeeping tip below ) , but you can also just import a few , like pi or sqrt : <p> @ @ @ @ @ @ @ @ @ @ , returned a floating point variable ( 2.0 ) ? <p> Good housekeeping Tip #1 : We tend to import modules using the two options above . That way we know what module the functions we are using are come from - especially because we do n't  know off-hand ALL the functions available in any given module and there might be conflicts with my own function names or two different modules could have the same function ( like math and numpy ) . These conflicts are known as namespace collisions and can be avoiding by following the best practice of using syntax like : " import numpy as np . " <p> Here is a ( partial ) list of some useful NumPy functions : <p> absolute(x) <p> absolute value <p> arccos(x) <p> arccosine <p> arcsin(x) <p> arcsine <p> arctan(x) <p> arctangent <p> arctan2 ( y , x ) <p> arctangent of y/x in correct quadrant ( ***very useful ! ) <p> cos(x) <p> cosine <p> cosh(x) <p> hyperbolic cosine <p> exp(x) <p> exponential <p> log(x) <p> natural logarithm <p> log10(x) <p> base 10 log <p> sin(x) <p> sine <p> @ @ @ @ @ @ @ @ @ @ tan(x) <p> tangent <p> tanh(x) <p> hyperbolic tangent <p> Note that in the trigonometric functions , the argument is in RADIANS ! You can convert from degrees to radians by multiplying by : numpy.pi/180.0 . Also notice how these functions have parentheses , as opposed to numpy.pi which has none . The difference is that these take arguments , while numpy.pi just returns the value of p . <p> The time has come to talk about variable types . Weve been very relaxed up to now , because we do n't  have to declare them up front and we can often even change them from one type to another on the fly . But - variable types matter , so here goes . Python has integer , floating point ( both long and short ) , string and complex variable types . It is pretty clever about figuring out what is required . Here are some examples : <p> Lesson learned : you cant add a number and a string . And string addition is different ! But you really have to be careful with this . If you multiply @ @ @ @ @ @ @ @ @ @ you will convert the float to an integer when you really wanted all those numbers after the decimal ! So , if you want a float , use a float . <p> long() converts to a double precision floating point and complex() converts the two parts to a complex number . <p> There is another kind of variable called " boolean " . These are : true , false , and , or , and not . For the record , the integer 1 is true and 0 is false . These can be used to control the flow of the program as we shall learn later . <p> In previous programming experience , you may have encountered arrays , which are a nice way to group a sequence of numbers that belong together . In Python we also have arrays , but we also have more flexible data structures , like lists , tuples , and dictionaries , that group arbitrary variables together , like strings and integers and floats - whatever you want really . Well go through some attributes of the various data structures , starting with @ @ @ @ @ @ @ @ @ @ ( note it takes out up to , but not including , the last item number - do n't  ask me why ) . Or , we can slice it this way : <p> &gt;&gt;&gt; newlist = mylist3 : <p> which takes from the fourth item ( starting from 0 ! ) to the end . <p> To copy a list BEWARE ! You can make a copy - but it is n't an independent copy ( like in , e.g. , Fortran ) , but it is just another name for the SAME OBJECT , so : <p> &gt;&gt;&gt; mycopy = mylist &gt;&gt;&gt; mylist2 = " new " &gt;&gt;&gt; mycopy2 " new " <p> See how mycopy got changed when we changed mylist ? To spawn a new list that is a copy , but an independent entity : <p> &gt;&gt;&gt; mycopy = mylist : <p> Now try : <p> &gt;&gt;&gt; mylist2 = 1003 &gt;&gt;&gt; mycopy2 " new " <p> So now mycopy stayed the way it was , even as mylist changed . <p> Python is " object oriented " , a popular concept in coding circles . Well @ @ @ @ @ @ @ @ @ @ right now you can walk around feeling smug that you are learning an object oriented programming language . O.K. , what is an object ? Well , mylist is an object . Cool . What do objects have that might be handy ? Objects have " methods " which allow you to do things to them . Methods have the form : object.method() <p> Numbers are numbers . While there are more kinds of numbers ( complex , etc. ) , strings can be more interesting . Unlike in some languages , they can be denoted with single , double or triple quotes : e.g. , spam , " Sams spam " , or <p> Hi there we can type as many lines as we want <p> Strings can be added together ( newstring = " spam " + " alot " ) . They can be sliced ( newerstring = newstring0:3 ) . but they CAN NOT be changed in place - you cant do this : newstring0= " b " . To find more of the things you can and can not do to strings , see : @ @ @ @ @ @ @ @ @ @ Unlike lists , however , arrays have to be all of the same data type ( dtype ) , usually numbers ( integers or floats ) , although there is something called a character array . Also , the size and shape of an array must be known a priori and not determined on the fly like lists . For example , we can define a list with L= , then append to it as desired , but not so with arrays - they are much pickier and well see how to set them up later . <p> Why use arrays when you can use lists ? They are far more efficient than lists particularly for things like matrix math . But just to make things a little confusing , there are several different data objects that are loosely called arrays , e.g. , arrays , character arrays and matrices . These are all subclasses of ndarray . Below is a brief introduction to arrays and matrices using numpy : <p> Note the difference between np.linspace ( start , stop , N ) and np.arange ( start , stop , @ @ @ @ @ @ @ @ @ @ 14 evenly spaced elements between the start and stop values while np.arange creates an array with elements at colorbluestep intervals between the starting and stopping values . In some of the online examples , you will find the short-cuts for arange() and linspace as r(-5,5,20j) and r ( -5,5,1. ) respectively . <p> Python arrays have methods like dtype , ndim , shape , size , reshape() , ravel() , transpose() etc . Did you notice how some of these require parentheses and some do n't  ? The answer is that some of these are functions and some are classes , both of which we will get to later . <p> Let 's see what the methods can do . First , arrays made in the above example are of different data types . To find out what data type an array is , just use the method dtype as in : <p> &gt;&gt;&gt; D.dtype dtype ( " float64 " ) <p> Arrays , unlike lists , have dimensions and shape . Dimensions tell us how many axes there are with axes defined as in this illustration : <p> As shown @ @ @ @ @ @ @ @ @ @ 0 and 1 ) . To get Python to tell us this , we use the ndim method : <p> &gt;&gt;&gt; A = np.array ( 1,2,3 , 4,2,0 , 1,1,2 ) # just to remind you &gt;&gt;&gt; A.ndim 2 <p> Notice how zeros , ones and ndarray used a shape tuple in order to define the arrays in the examples above . The shape of an array is how many elements are along each axis . So , naturally we see that the C array is a 2x3 array . Python returns a tuple with the shape information using the shape method : <p> &gt;&gt;&gt; C.shape ( 2 , 3 ) <p> Let 's say we do n't  want a 2x3 array for the sequence in the array C , but we want a 3x2 array . Python can reshape an array with a different shape tuple like this : <p> &gt;&gt;&gt; C.reshape ( ( 3,2 ) ) array ( 1 , 2 , 3 , 4 , 5 , 6 ) <p> And sometimes we just want all the elements lined up along one axis . We could do that @ @ @ @ @ @ @ @ @ @ array ( the total number of elements ) . You can see that this is 6 here . We could even get python to tell us what the size is ( C.size ) and use that in the reshape size tuple . Alternatively , we can use the ravel() method which does n't  require us to know the size in advance : <p> &gt;&gt;&gt; C.ravel() array ( 1 , 2 , 3 , 4 , 5 , 6 ) <p> There are other ways to reshape , slice and dice arrays . The syntax for slicing of arrays is similar to that for lists : <p> &gt;&gt;&gt; B = A0:2 # carve the top two lines off of matrix A from above array ( 1 , 2 , 3 , 4 , 2 , 0 ) <p> Lots of applications in Earth Science require the transpose of an array : <p> Also , we can concatenate two arrays together with the - you guessed it - concatenate() method . For a lot more tricks with arrays , go to the NumPy Reference website here : http : **37;1663;TOOLONG . <p> @ @ @ @ @ @ @ @ @ @ , from a list or tuple to an array : A=numpy.array(L) , or from a list , a tuple or an array to a NumPy array : a=numpy.asarray(L) <p> Let 's go a bit deeper into slicing of arrays . First a review of lists . You will recall that in python , indexing starts with 0 , so for the list L=0,2,4,6,8 , L1 is 2 . The index of the last item is -1 , so L-1=8 . To find out what the index for the number 4 is , for example , we have the index() method : L.index(4) , which will return the number 2 . We actually already used this method when we implemented command line arguments , but it was n't really explained . We know that to reassign a given index a new value we use the syntax L1=2.5 . And to use a part of a list ( a slice ) we use , e.g. , B=L2:4 , which defines B as a list with Ls elements 2 and 3 ( 4 and 6 ) . And you also know that B=L2 : takes @ @ @ @ @ @ @ @ @ @ these examples , you can infer that the basic syntax for slicing is start:stop:step ; if the step is omitted it is assumed to be 1 . <p> Arrays ( and matrices ) work in a similar fashion to lists , but these are multidimensional objects , so things get hairy fast . The basic syntax is the same : start:stop:step , or i:j:k. but with Python arrays , we step through all the js for each i at step k . This is best shown with examples : <p> Now that you have fallen in love with Python and Numpy , we have a new treat for you : pandas . Pandas is a powerful module available with the Python distribution that you should have installed in the Installing PmagPy section . It supports elegant data structures and tools for wrangling the data which allow fast and user-friendly data analysis . There are two basic data structures : the Series ( a one-dimensional array like object that is a data array with an associated array of indices which can be numbers or text ) and the DataFrame ( a @ @ @ @ @ @ @ @ @ @ pandas by importing it , then create some data Series using several approaches . The common convention , utilized throughout pandas documention , is for pandas to be imported as pd : <p> Another very useful pandas object is the DataFrame . DataFrames have both row and column indices and are like a dictionary of Series so are much more flexible than a NumPy array object . Here is an example : <p> Are you tired of typing yet ? Python scripts are programs that can be run and re-run from the command line . You can type in the same stuff you 've been doing interactively into a script file ( ending in . py ) . You can edit scripts with your favorite text editor ( NOT Word ! ) . And then you can run them like this : <p> %python &lt; myscript.py <p> On a Mac or Unix system , you can put in a header line identifying the script as python ( # ! /usr/bin/env python ) , make it executable ( chmod a+x ) and run it like this : <p> $ myscript.py <p> On @ @ @ @ @ @ @ @ @ @ script without the header or the chmod command . But , because PCs do n't  come standard with a cat command , you have to create the file with Notepad , instead of with cat for all the examples using cat . <p> Here is an example that creates a script using the Unix cat command , makes it executable and then runs it : <p> In a Python script on Unix machines ( including MacOS ) , the first line MUST be : <p> # ! /usr/bin/env python <p> so that the file is interpreted as Python . Unlike Fortran or C , you CAN NOT start with a comment line ( try switching lines 1 and 2 and see what happens ) . <p> The second line is a comment line . Anything to the right of # is assumed to be a comment . Notice that print goes by default to your screen . Because the message is a string , you can use single or double quotes for the test message . You can get an apostrophe in your output by using double quotes and quote @ @ @ @ @ @ @ @ @ @ ! /usr/bin/env python # simple Python test program 2 ( printmess2.py ) print " The pump do n't  work cuz the vandals took the handles " print " She said " I know what its like to be dead " " <p> produces : <p> $ . /printmess2.py The pump do n't  work cuz the vandals took the handles She said I know what its like to be dead % <p> In the second print statement , " is necessary to prevent an error ( try it ) . This is an example of a Python escape code . These are used to escape some special meaning , as in an end-quote for a string in this example . We use the backslash to say that we really really want a quote mark here . Other escape codes are listed here : <p> Any reasonable programming language must provide a way to group blocks of code together , to be executed under certain conditions . In Fortran , for example , there are if statements and do loops which are bounded by the statements if , endif and do , @ @ @ @ @ @ @ @ @ @ of indentation to make the code more readable , but do not require it . In Python , indentation is the way that code blocks are defined - there are no terminating statements . Also , the initiating statement terminates in a colon . The trick is that all code indented the same number of spaces ( or tabs ) to the right belong together . The code block terminates when the next line is less indented . A typical Python program looks like this : <p> Any statement can be continued on the next line with the continuation character and the indentation of the following line is arbitrary . <p> If a code block consists of a single statement , then that may be placed on the same line as the colon . <p> The command break breaks you out of the code block . Use with caution ! <p> There is a cheat that comes in handy when you are writing a complicated program and want to put in the code blocks but do n't  want them to DO anything yet : the command pass does nothing and @ @ @ @ @ @ @ @ @ @ . <p> Good housekeeping Tip #2 : Always use only spaces or only tabs in your code indentation . Spaces ( 4 of them together ) are the preferred indentation method in Python . Whatever you do BE CONSISTENT because tabs are not the same as spaces in Python even if you cant tell the difference just by looking at it . <p> In the following , you will be shown how Python uses code blocks to create " do " and " while " loops , and " if " statements . <p> This script creates the list mylist with the line mylist=42 , " spam " , " ocelot " . The length of mylist is an integer value returned by len(mylist) . The script uses this integer as the stop value in the range() function , which returns a list of integers from 0 to the stop value MINUS ONE at intervals of one . The minus one convention can be hard to get used to , but it is typical of Python syntax ( and also of C ) so just deal with it . Anyway @ @ @ @ @ @ @ @ @ @ just like numpy.arange ( start , stop , step ) , but returns integers instead of floats . As in numpy.arange() , there is a short hand form when the minimum is zero and the interval is one , so we could ( and will ) just use the command range(stop) . <p> Python makes i step through the list of numbers from 0 to 2 , printing the ith element of mylist . Note how the print command is indented - this is the program block that is executed for each i . Note also that the line could have been on the previous line after the colon , because there is only one line in the program block . But never-mind , this way works too . When i finishes its business , the program block terminates . At that point , the program prints out the All done string . There is no " enddo " statement or equivalent in Python . <p> But , Python is far more fun than the old-school for i in syntax in the above code snippet . In Python we can @ @ @ @ @ @ @ @ @ @ script which does just that ( why not ? ) : <p> Note that of course we could have used any variable name instead of item , but it makes sense to use variable names that mean what they do . It is easier to understand what item stands for than just the Fortran style of i . <p> Here is an example with a little more heft to it . It creates a table of trigonometry functions , spitting them out with a formatted print statement : <p> Let 's pick this program apart a bit . First , notice the use of the variable deg2rad to convert from degrees to radians . Also notice how deg2rad is defined : deg2rad = np.pi/180. using the NumPy function for p and the decimal point after 180 . While in this case , it makes absolutely no difference ( try it ! ) , it is a good practice to use floating point numbers unless you want your variable to stay an integer . In fact : <p> Good housekeeping Tip #3 : Always use a decimal if you want your variable @ @ @ @ @ @ @ @ @ @ example , 180. or 180.0 give the same floating point number . <p> The expression ctheta = np.cos(theta*deg2rad) uses the numpy cosine function . Ideally , theta should be a floating point number while in fact it is an integer in this expression , but fortunately Python figures that out and converts it to a floating point number . Note that we could have also converted theta to a float first with the command float(theta) . <p> print " %5.1f %8.4f %8.4f %8.4f " % ( theta , ctheta , stheta , ttheta ) <p> To make the output look nice , we do not use <p> print theta , ctheta , stheta , ttheta <p> which would space the numbers irregularly among the columns and put out really long numbers . Instead , we explicitly specify the output format . The output format is given in the quotes . The format for each number follows the % , 5.1f is for 5 spaces of floating point output , with 1 space to the right of the decimal point . The single blank space between %5.1f and %8.4f is included @ @ @ @ @ @ @ @ @ @ reproduced exactly in the output , thus to put commas between the output numbers , write : <p> The " for loop " is just one way of controlling flow in Python . There are also if and while code blocks . These execute code blocks the same way as for loops ( colon terminated top statements , indented text , etc . ) . For both of these , the code block is executed if the top statement is TRUE . For the " if " block , the code is executed once , but in a " while " block the code keeps executing as long as the statement remains TRUE . <p> The key to flow control therefore is in the top statement of each code block ; if it is TRUE , then execute , otherwise skip it . To decide if something is TRUE or not ( in the boolean sense ) , we need to evaluate a statement using comparisons . here 's a handy table with comparisons ( relational operators ) in different languages : <p> Comparisons <p> F 77 <p> F90 <p> C @ @ @ @ @ @ @ @ @ @ <p> == <p> == <p> == <p> == <p> equals <p> . ne . <p> /= <p> ! = <p> = <p> ! = <p> does not equal <p> . lt . <p> &lt; <p> n++&lt; <p> &lt; <p> &lt; <p> less than <p> . le . <p> &lt;= <p> &lt;= <p> &lt;= <p> &lt;= <p> less than or equal to <p> . gt . <p> &gt; <p> &gt; <p> &gt; <p> &gt; <p> greater than <p> . ge . <p> &gt; = <p> &gt; = <p> &gt; = <p> &gt; = <p> greater than or equal to <p> . and . <p> . and . <p> &amp; <p> &amp; <p> and <p> . or . <p> . or . <p> <p> <p> or <p> These operators can be combined to make complex tests . Here is a juicy complicated statement : <p> if ( ( a &gt; b and c &lt;= 0 ) or d == 0 ) : code block <p> There are rules for the order of operations for these things like , multiplication gets done before addition . But these are easy to forget . You @ @ @ @ @ @ @ @ @ @ unsure or , better , just put in enough parenthesis to make it completely clear to anyone reading your code . <p> Good housekeeping Tip #4 : Use parentheses liberally - make the order of operation completely unambiguous even if you could get away with fewer . <p> One nice aspect of Python compared to C is that if you make a mistake and type , for example , <p> if ( a = 0 ) : <p> you will get an error message during compilation . In C this is a valid statement with a completely different meaning than is intended ! <p> if ( 2+2 ) ==4 : # note the use of == and parentheses in comparison statement print " I can put two and two together ! " <p> However , as in any other reasonable programming language , there are whistles and bells to the if code blocks . In Python these are : elif and else . A code block gets executed if the top if statement is FALSE and the elif statement is TRUE . If both the top if and the elif @ @ @ @ @ @ @ @ @ @ then Python will execute the block following the else . Consider these examples : <p> # ! /usr/bin/env python mylist= " jane " , " doug " , " denise " if " susie " in mylist : pass # do n't  do anything if " susie " not in mylist : print " call susie and apologize ! " mylist.append ( " susie " ) elif " george " in mylist : # if first statement is false , try this one print " susie and george both in list " else : # if both statements are false , do this : print " susie in list but george is n't " <p> As already mentioned , the while block continues executing as long as the while top statement is TRUE . In other words , the if block is only executed once , while the while block keeps looping until the statement turns FALSE . Here are a few examples : <p> All of these program blocks can also be done in an interactive session also using indentation . The interactive shell responds with ..... instead of &gt;&gt;&gt; @ @ @ @ @ @ @ @ @ @ statement . To signal that you are done with the program block , simply hit return : <p> Python would be no better than a rather awkward graphing calculator ( and we have n't even gotten to the graphing part yet ) if we could n't read data in and spit data out . You learned a rudimentary way of spitting stuff out already using the print statement , but there is a lot more to file I/O in Python . We would like to be able to read in a variety of file formats and output the data any way we want . In the following we will explore some of the more useful I/O options in Python . <p> If you are using Python interactively or want interactivity in a script , use the command : rawinput() . It acts as a prompt and reads in whatever is supplied prior to a return as a string . <p> X= # make a list to put the data in ans=float ( rawinput ( " Input numeric value for X : " ) ) X.append(ans) # append the value to X print @ @ @ @ @ @ @ @ @ @ In this example , the variable ans will be read in as a string variable , converted to a float and appended to the list , X. rawinput() is a simple but rather annoying way to enter things into a program . Another ( less annoying ) way is put the data in a file ( e.g. , myfile.txt ) with cat , paste , Excel ( saved as a text file ) , or whatever and read it into Python . The procedure is straight-forward : we must first open the file , then read it in and parse lines into the desired variables . <p> To open a file we use the command open() , one of Pythons built-in functions . For a complete list of these , see : <p> The open() function returns an object , complete with methods , like readlines() which , yes , reads all the lines . Suppose you have a file containing the coordinates of some seismic stations which you want to plot on a map or something . ( In fact , it is in the datafiles/LearningPython directory . The @ @ @ @ @ @ @ @ @ @ : <p> Here is a script ( ReadStations.py that will open a file station.list , read in the data and print it out line by line . <p> # ! /usr/bin/env python f=open(station.list) StationNFO=f.readlines() for line in StationNFO : print line <p> If you run this script , you will get this behavior : <p> $ ReadStations.py 9.02920 38.76560 2442 AAE <p> 42.63900 74.49400 1645 AAK <p> 37.93040 58.11890 678 ABKT <p> 51.88370 -176.68440 116 ADK etc . <p> The function open() has some bells and whistles to it and has the form open ( name , mode , buffering ) where the stuff in square brackets is optional . The name argument is the file name to open and mode is the way in which it should be opened , most commonly for reading r , writing w or appending a . We use the form rU for unformatted reading because we often want to read in files that were saved in Dos , Mac OR Unix line endings and rU figures all that out for you . Just in case you are curious , Unix lines end in @ @ @ @ @ @ @ @ @ @ windows ) lines end in rn. we never use the buffering argument and do n't  know what it does . <p> If you are curious about the line endings , try typing out the representation of the line repr(line) in the above script and you get all the stuff that is normally invisible like the apostrophes and the line terminations : <p> Let 's say you want to read in the data table into lists called Lats , Lons , and StaIDs ( the first three columns ) . You need to split each line into its columns and append the correct column into the appropriate list . Some languages automatically split on the spaces but Python reads in the entire line as a string and ignores the spaces or other possible delimiters ( commas , semi-colons , tabs , etc . ) . To split the line , we use the string function split(sep) where sep is an optional separator . If no separator is specified ( e.g. , line.split() ) , it will split on spaces . Anything could be a separator , but the most common ones are @ @ @ @ @ @ @ @ @ @ how a tab appears if you were to , say , print out the representation of the line , which shows all the invisibles . <p> Here is a slightly modified version of ReadStations.py , ParseStations.py which parses out the lines and puts numbers ( floats or integers ) in the right lists : <p> # ! /usr/bin/env python Lats , Lons , StaIDs , StaName= , , , # creates lists to put things in StationNFO=open ( " station.list " ) . readlines() # combines the open and readlines methods ! for line in StationNFO : nfo=line.strip ( " n " ) . split() # strips off the line ending and splits on spaces Lats.append ( float ( nfo0 ) ) # puts float of 1st column into Lats Lons.append ( float ( nfo1 ) ) # puts float of 2nd column into Lons StaIDs.append ( int ( nfo2 ) ) # puts integer of 3rd column into StaIDs StaName.append(nfo3)# puts the I 'd string into StaName print Lats-1 , Lons-1 , StaIDs-1 # prints out last thing appended <p> Python can also read from standard input . To do @ @ @ @ @ @ @ @ @ @ sys which among other things has a stdin method . So , instead of specifying a file name in the open command , we could substitute the following line : <p> In the special case where the data in a file are entirely numeric , you can read in the file with a special numpy function loadtxt() . This reads the data into a list whereby each element of the list is a list of numbers from each line . <p> We wrote a script ( ConvertStations.py ) to convert each of the stations in my list to their UTM equivalents ( assuming these were in a WGS-84 ellipsoid ) . It would be nice if after having done this to the data , we could then write it out somehow , preferably to a file . Of course we could use the print command like this : <p> But we yearn for more . So , more elegantly , we can open an output file for appending a or ( over ) writing w write a formatted string using the write method on the output file object with format @ @ @ @ @ @ @ @ @ @ the object outfile is opened for writing . Note that this will clobber anything in a pre-existing file by that name and 2 ) the output file gets written to in the statement with a write method on the output file object : <p> outfile.write ( %s %s %s %sn% ( StaName , Easting , Northing , Zone ) ) <p> The write statement uses the syntax : format string% ( list of variables tuple ) . Format strings have these rules : <p> For each variable in ( what you ... ) you need a format : %s for string , %i for integer , %f for float , %e for exponent <p> you can also specify further , e.g. : %7.1f for 7 characters with 1 after the decimal %10.3e for 10 characters with 3 after the decimal <p> So far you have learned how to use functions from program modules like NumPy . You can imagine that there are many bits of code that you might write that you will want to use again and again , say converting between degrees and radians and back , or @ @ @ @ @ @ @ @ @ @ , or converting between UTM and latitude/longitude coordinates ( as in UTM.py , my new favorite package ) . The basic structure of a program with a Python function is : <p> The first line must have def as the first three letters , must have a function name with parentheses and a terminal colon . If you want to pass some variables to the function , they go where inarg sits , separated by commas . There are no output variables here . <p> Although you can certainly write functional code without a document string , make a habit of always including one . Trust me - you 'll be glad you did . This docstring can later be used to remind you of what you thought you were doing years later . It can be used to print out a help message by the calling program and it also let 's others know what you intended . Notice the use of the triple quotes before and after the documentation string - that means that you can write as many lines as you want . <p> You do n't  need this @ @ @ @ @ @ @ @ @ @ body ( see , for example printkwargs() above ) . Python separates the entrance and the exit . See how it can be done in the gimmepi() example above . <p> It is considered good Python style to treat your main program block as a function too . ( This helps with using the document string as a help function and building program documentation in general . ) In any case , we recommend that you just start doing it that way too . In this case , we have to call the main program with the final ( not indented ) line main() : <p> Notice how in the above examples , all the functions preceded the main function . This is because Python is an interpreter and not compiled - so it wo n't know about anything declared below as it goes through the script line by line . On the other hand , we 've been running lots of functions and they were not in the program we used to call them . The trick here is that you can put a bunch of functions in a separate file @ @ @ @ @ @ @ @ @ @ module , just like we did with NumPy . Your functions can then be called from within your program in the same way as for NumPy . <p> So let 's say we put all the above functions in a file called myfuncs.py : <p> Inside a function , variable names have their own meaning which in many cases will be different from inside the calling function . So , variables names declared inside a function stay in the function . This is true unless you declare them to be " global " . Here is an example in which the main program " knows " about the functions variable V : <p> def myfunc() : global V V=123 def main() : myfunc() print V main() <p> In addition to being able to write your own functions , of course Python has LOTS of modules and a gazzillion functions . The Enthought distribution that you are using comes with scientific python modules that have functions for plotting , numerical recipes , image manipulation , animation , and so much more . <p> Before we go any further , we need to @ @ @ @ @ @ @ @ @ @ basis of " object oriented programming " OOP ( that again ! ) . Class objects lie behind plotting , for example , and a rudimentary understanding of what they are and how they work will come in handy when we start doing anything , but the simplest plotting exercises . <p> A class object is created by a call to a " class definition " which which can be thought of as a blueprint for the class object . Here is an simple example of a class definition : <p> In spite of superficial similarities , classes are not the same as functions . Although the Shape module is imported just the same as any other , to use it , we first have to create a class " instance " ( C=Shapes.Circle(r) ) . C is an object with " attributes " ( variables ) and " methods " . All methods ( parts that start with " def " ) , have an argument list . The first argument has to be a reference to the class instance itself , or " self " , followed by @ @ @ @ @ @ @ @ @ @ So the init method initializes the instance attributes of an object . In the above case , it defined the attribute r , which gets passed in when the class is first called . Asking for any attribute ( note the lack of parentheses ) , retrieves the current value of that attribute . Attributes can be changed ( as in C.r=2.0 ) . <p> The other methods ( area and circumference ) are defined like any function except note the use of self as the first argument . This is required in all class method definitions . In our case , no other parameters are passed in because the only one used is r , so the argument list consists of only self . Calling these methods returns the current values of these methods . <p> You can make a subclass ( child ) of the parent class which has all the attributes and methods of the parent , but may have a few attributes and methods of its own . You do this by setting up another class definition within a class . <p> So , the bottom @ @ @ @ @ @ @ @ @ @ category of things as variables , lists , dictionaries , etc . That is , they are data structures - they hold data , and the methods to process that data . If you are curious about classes , there 's lots more to know about classes that you can find in useful tutorials online : <p> So far you have learned the basics of Python , and NumPy . But Python was sold as a way of visualizing data and we have n't yet seen a single plot ( except a stupid one in the beginning ) . There are many plotting options within the Python umbrella . The most mature of these is matplotlib , a popular graphics module of Python . Actually matplotlib is a collection of a bunch of other modules , toolkits , methods and classes . For a fairly complete and readable tour of matplotlib , check out these links : <p> The first step should be obvious by now , it imports matplotlib . Figures are rendered on " backends " so they appear on screen . There are a lot of different back-ends with @ @ @ @ @ @ @ @ @ @ systems . We use the very old school backend called " TkAgg " backend because it " works " . So step 2 sets the backend : matplotlib.use ( " TkAgg " ) . The module matplotlib itself contains a lot of other modules . One of these , pylab is the " business end " that has a lot of plotting methods and classes . It must be loaded alongside matplotlib , so step 3 is : import pylab . After that the fun starts . <p> In the above example , we call the plot method with a list as an argument . As we mentioned , matplotlib uses the concept of " classes " to make plots and this has just happened behind the scenes . We could have named the plot instance with a the figure() method ( e.g. , fig=pylab.figure() ) and then referred to it later with the command fig.plot(1,2,3) , but we do n't  have to in this simple case - the class instance is implied and is the " current plot " . You can tell this , if you do the @ @ @ @ @ @ @ @ @ @ **27;1702;TOOLONG object at 0x4bd6eb0&gt; is Pythons way of telling you that you just created an object and something about it . In any case , when you give plot() a single sequence of values ( as above ) , it assumes they are y values and supplies the x values for you . <p> Attributes of the pylab class , such as the Y axis label can be changes with the ylabel method . As you can imagine , there are LOTS of methods , including , surprise , an xlabel method . <p> When we are done customizing the plot instance , we can view it with the show method . When that gets executed , we will get a plot something like this : <p> Once that happens , we wont be able to change the plot any more and in fact , we wont get our terminal back until the little plot window is closed . You can save your plot with the little disk icon in a variety of formats . Adobe Illustrator likes . svg , or . eps while Microsoft products like . png @ @ @ @ @ @ @ @ @ @ always have to close figures with the little red button , or save them with the disk icon , you can tweak the program like this : <p> The method **27;1731;TOOLONG . The . FMT can be one of several , e.g. , . eps , . svg , . ps , . pdf , . png , . gif , . jpg , etc . ) . Some of these ( the vector graphics ones like pdf , ps , eps and svg ) can be opened in Adobe Illustrator for modification . <p> As mentioned earlier , if you give plot() a single sequence of values , it assumes they are y values and supplies the x values for you . Garbage in , garbage out . But plot() takes an arbitrary number of arguments of the form : ( X1 , Y1 , linestyle1 , X2 , Y2 , linestyle2 , etc. ) , where linestyle is a string that specifies the line style as illustrated in this script called matplotlib2.py <p> From the code , you can probably figure out that a line style of r @ @ @ @ @ @ @ @ @ @ triangles . There are many other attributes that can be controlled : linewidth , dash style , etc. and we invite you to check out the matplotlib documentation . <p> By now , you should understand enough about classes , keyword argument passing and other pythonalia to be able to figure things out on your own . But do n't  panic , I 'm going to lead you through a few more examples , which we hope will speed you on your plotting way . <p> As already mentioned , pylab has the concept of " current figure " which subsequent commands refer to . In the preceding examples , we only had one figure , so we did n't  have to name it , but for fancier figures with several plots , we can create named figure objects by invoking a figure instance : <p> fig = pylab.figure ( num=1 , figsize= ( 5,7 ) ) . <p> Notice the syntax whereby figsize is a method with width and height ( in inches ) specified by a tuple and num is the figure number . Notice that these are keyword @ @ @ @ @ @ @ @ @ @ the list of **kwargs in the online documentation located here : <p> Once we have a figure instance ( sometimes called a " container " ) , we can do all kinds of things , including adding subplots . To do this , we can use the syntax : <p> fig.addsubplot(211) <p> Here the argument 211 means 2 rows , one column and this is the first plot . To make plots side by side , you would use : fig.addsubplot(121) for 1 row , two columns , etc . <p> After each addsubplot command , that subplot becomes the current figure for plotting on . If you want more freedom , say , you want to make a subplot at an arbitrary place , use the addaxes ( left , bottom , width , height ) 0 method , e.g. , addaxes(0.1,0.1,0.7,0.3) . The values are 0-1 in relative figure coordinates . <p> To illustrate these new concepts , consider the example code , matplotlib3.py : <p> We already met xlabel and ylabel . But text can be added in a other ways , e.g. , using the title @ @ @ @ @ @ @ @ @ @ one of our early examples to show how some of these things work : <p> The title appears at the top of the plot . Text labels get places at the x and y coordinates on the plot and the legend will appear in the upper/lower right/left corner as specified in the string . The pylab.text ( x , y , string , kwargs ) method also has optional key word arguments , specifying font , size , color and the like . The legend labelist is a list of labels for each plot element . So , every line or point style that you want in your legend , append a label to the label list after the relevant plot command . Also note that the legend and xlabel methods use a special format for strings ( rLateX String ) which allows embedded LaTeX equation syntax to make scientific equations look right - so now you have to learn LaTeX ! . Finally , the arrow gets drawn with the annotate method , which has a lot of other attributes as well . Check the matplotlib documentation for details @ @ @ @ @ @ @ @ @ @ matplotlib , e.g. , histograms , pie charts , contour plots , whisker plots , etc . I 'm just going to show you a few examples . The best thing to do is to look through the online documentation for a plot that looks like what you need , then modify it . This is ALWAYS a good approach - start with something that works and fiddle with it until it suits your own particular needs . <p> Data analysis in Python benefits from an increasingly robust ecosystem of packages for scientific computing and plotting . A tool that has gained increasingly widespread use for conducting and presenting data analysis is the Jupyter notebook . The Jupyter notebook builds on the IPython project which began as a way to bring increased interactivity to data analysis in Python ( Pn++rez &amp; Granger , 2007 ) and has evolved to include an interactive notebook environment that seeks to enable the full trajectory of scientific computing from initial analysis and visualization onwards to collaboration and through to publication . The project has expanded to enable a reproducible interactive computing environment for many other @ @ @ @ @ @ @ @ @ @ the language-agnostic parts of its architecture have recently been rebranded as Project Jupyter ( http : //jupyter.org ) . Jupyter notebooks allow for executable code , results , text and graphical output to coherently coexist in a single document . With these combined components , they are excellent tools both for conducting data analysis and presenting results . <p> Underlying the PmagPy programs that are accessible through the command line and the GUI interfaces ( e.g. , Pmag GUI ) are two main function libraries : pmag.py and pmagplotlib.py . The functions within these modules can be imported and called upon within a Jupyter notebook running a Python 2.7 kernel . In addition to these functions , Nick Swanson-Hysell created a module called ipmag.py that is in active development and contains functions that replicate and extend functionality that is found within the PmagPy command line programs for use in the notebook environment . <p> The main example notebook ( **27;1760;TOOLONG ) combines data from two different studies for the sake of developing a mean paleomagnetic pole from the upper portion of a sequence of volcanics in the North American Midcontinent @ @ @ @ @ @ @ @ @ @ The two data files used within the notebook can be downloaded from the MagIC database . The digital object identifier ( doi ) search option allows for the data files to be readily located as LONG ... and LONG ... Downloading these data files from the database and putting them into folders within a local Project Directory allows them to be accessed within the Jupyter notebook . Within the notebook , these data are unpacked into their respective MagIC formatted tab delimited data files . The data are then loaded into dataframes and filtered using several different criteria ( stratigraphic height and polarity ) . Several functions from the ipmag module are used for making equal area projections and calculating statistics . In addition to combining the data sets to calculate the mean pole , the code in the notebook conducts a bootstrap fold test on the data using the approach of Tauxe &amp; Watson ( 1994 ) as well as a common mean test . The data recombinations and calculations done in this notebook are examples of portions of the data analysis workflow which are often difficult to document @ @ @ @ @ @ @ @ @ @ the potential for the use of notebooks for data manipulation and analysis of paleomagnetic data . Additional functionality available within PmagPy is demonstrated within the additional PmagPy examples notebook ( **30;1789;TOOLONG ) as small vignettes of example code . Functions related to paleomagnetic and rock magnetic data analysis are shown as examples . The notebook also illustrates some of the interactivity that can be built into the notebook utilizing IPython widgets . <p> To execute and edit notebook code yourself , you can clone or download this repository : LONG ... If you installed the Enthought Canopy or Anaconda Python distribution you will have the IPython and Jupyter packages installed . If you have another Python distribution you will want to make sure that you have IPython and Jupyter ( installation instructions can currently be found here : http : **26;1821;TOOLONG and here : LONG ... To view and edit the notebooks , type ipython notebook or jupyter notebook on the command line . This command will open up a local IPython server in your default web browser within which you can use the directory to navigate to the datafiles @ @ @ @ @ @ @ @ @ @ You will see something like this : <p> Notebooks are constructed as a series of cells which can be text or code . To view the source of a text cell , just click on it . To render it , click on the run button ( sideways triangle on the toolbar ) . Similarly , to run the code in a code cell , click on the cell and then the run button ( or use the shift+enter short cut ) . To execute the entire notebook , click on the Cell button and choose Run All . <p> The easiest way to access PmagPy functionality in the notebook is to follow the regular install instructions using pip . This automatically adds all of the pmagpy module into your Python path . You can go straight to looking at the data ! <p> A third way to acces PmagPy inside a notebook is to set the path to the location of your downloaded PmagPy folder within the notebook itself . The default location on MacOS is in a folder called PmagPy in your home directory ( e.g. , **26;1849;TOOLONG @ @ @ @ @ @ @ @ @ @ e.g. , **40;1877;TOOLONG . To do this you must import os and then have a line that reads sys.path.insert ( 0 , /Users/ ...... ) where the path reflects your own path . If PmagPy is in your path you should be able to run the first code block which imports the PmagPy modules and makes them availible available to use within the notebook . The notebooks also import several other Python modules useful for scientific computing ( numpy ) , data manipulation ( pandas ) and plotting ( matplotlib ) . <p> Now you are ready to look at some data . In the code block under the heading Reading data from MagIC format results files , data are read in from a file downloaded and unpacked from the MagIC database . The notebook shows how to read in the data into a pandas DataFrame , and plot the directions on an equal area projection : <p> There are several other tricks shown off in the notebook , which should be enough to get you started using ipmag in a Python notebook environment . Conducting data analysis using PmagPy @ @ @ @ @ @ @ @ @ @ tests to be available and for the decisions made in the specific implementation of such tests to be transparently presented . <p> Although there is much much more to do in Python , this documentation is aimed at getting and using PmagPy , so that 's it for this chapter . Congratulations if you made it to the end ! <p> If you do n't  see one of the above messages , you have not fully installed the Python distribution . You 'll need to try again . In order to fully install Canopy Python ( it will be your Applications folder ) and select " yes " to use Canopy Python as your default Python . <p> I 've installed the proper Python distribution , but PmagPy does n't  work <p> For Mac users : <p> When you try to run eqarea.py -h to test your installation , you get this error message : <p> -bash : eqarea.py : command not found <p> This probably means that you have not correctly installed PmagPy . On your command line , try : <p> pip list <p> You should see both pmagpy- ( version @ @ @ @ @ @ @ @ @ @ on that list . If you do n't  see them , go ahead and reinstall : <p> " eqarea.py " is not recognized as an internal or external command , operable program or batch file . <p> First , remember that if you have a standard pip install , you need to use eqarea instead of eqarea.py . This is caused by a strange Windows quirk , but what you need to know is this : anytime the Cookbook gives a command , you 'll need to drop the " . py " and all will be well . <p> If eqarea -h does n't  work , this probably means that you have not correctly installed PmagPy . On your command line , try : <p> pip list <p> You should see both pmagpy- ( version ? number ) and pmagpy-cli- ( version ? number ) on that list . If you do n't  see them , go ahead and reinstall : <p> If you only have the core packages installed with the Python 2.7 Anaconda distribution ( plus the PmagPy package you just installed ) , you may get the @ @ @ @ @ @ @ @ @ @ : <p> ImportError : No module named wx <p> To correct this error , simply execute at the command line : <p> conda install wxpython <p> My path is correctly set , but I still get this error message : <p> -bash : eqarea.py : command not found <p> For Mac users with a developer ? install , it is possible that you need to make the python scripts executable . On the command line in the directory with the scripts , type : chmod a+x *. py <p> If you are using the pip installation of PmagPy and you have any problems , you should start by uninstalling and reinstalling pmagpy and pmagpy-cli . To do this , use these commands : <p> Some of the modules used in PmagPy have dependencies that are not called directly . If you are missing one of those dependencies , the programs may fail in odd ways . In general , it is worthwhile to open Canopy , go into the package manager , and update all packages . More specifically , if you see an error message like this : <p> @ @ @ @ @ @ @ @ @ @ not be found <p> you may be missing the MKL package . Open Canopy , go to package manager , and select " Available packages " . Scroll down to MKL , and install it . This should fix the problem ! Note that windows users who are having trouble opening one of the GUIs should try this solution whether or not they actually see this error message . <p> Alternative : open the Canopy application , and navigate to the Canopy Package Manager . Go to " Installed packages " and find matplotlib . Click matplotlibs " more info " button . The click " show " to see all versions of the package . Select matplotlib 1.4.3-7 and install . This will fix the segmentation error , however , it may cause the Thellier GUI menubar to behave oddly . For this reason , we recommend the first solution ( above ) . <p> Report a problem not listed above : e-mail ltauxe@ucsd.edu . Include the following information : 1 ) the version of PmagPy that you are using , 2 ) your operating system , 3 ) @ @ @ @ @ @ @ @ @ @ datafile that is giving trouble , if relevant . 
@@97506193 @1706193/ <h> Industry Partners Program <h> IPP : Industrys " Gateway " to SDSC <p> The IPP provides member companies with a framework for interacting with SDSC researchers and staff , exchanging information , receiving education &amp; training , and developing collaborations . Joining IPP is an ideal way for companies to get started collaborating with SDSC researchers and to stay abreast of new developments and opportunities on an ongoing basis . The expertise of SDSC researchers spans many domains including computer science , cybersecurity , data management , data mining &amp; analytics , engineering , geosciences , health IT , high performance computing , life sciences &amp; genomics , networking , physics , and many others . <p> The IPP provides multiple avenues for consultation , networking , training , and developing deeper collaborations . The IPP is an annual fee-based program that provides member companies with a variety of mechanisms for interacting and collaborating with SDSC researchers . The IPP serves an important function in maintaining SDSCs ties to industry and the high-technology economy . Membership fees fund a variety of preplanned and ad hoc activities designed to @ @ @ @ @ @ @ @ @ @ deeper collaborations . <h> SDSC : A Long History of Applied R&amp;D <p> From its founding in 1985 by a private company , General Atomics , through to its present affiliation with UCSD , SDSC has a long history of collaborating with and delivering value to industry . SDSC has a strong culture of conducting applied R&amp;D , leveraging science and technology to deliver cutting-edge solutions to real-world problems . From its roots in High Performance Computing to its present emphases in " Big Data " and Predictive Analytics , SDSC has much to offer industry partners in terms of knowledge and experience that is relevant to their world and impactful to their business . <h> IPP Membership Features and Benefits <p> Member companies can work within the IPP to engage with SDSC in a way that best suits their needs . Some examples are highlighted below . <p> Annual Research Review SDSC conducts an annual research review exclusively for IPP member companies . The research review provides a forum for SDSC to highlight current and forthcoming research projects and results . Topics are drawn from across the Center and @ @ @ @ @ @ @ @ @ @ events also offer networking opportunities with SDSC researchers and affiliates . The review is a full day and is normally held in the month of May . <p> Quarterly Focused Workshops and Training SDSC conducts three quarterly workshops and training activities exclusively for member compa-nies . The workshops cover topics of interest to industry such as " Big Data , " predictive analytics , cloud computing , bioinformatics , computational science for engineering , and others . Like the An-nual Research Review , these events offer networking opportunities with SDSC researchers as well as affiliates . Sessions are conducted both on-site and via webcast . Webcasts can be retrieved on-demand by member companies . These events are generally held in February , August and October . <p> Private Meetings with Researchers and Staff SDSC invites all IPP members to meet with re-search experts . We will work to identify the area of interest and researcher(s) to support members business needs . These meetings can be tailored as one-on-one , a small group , or as a brainstorming session with the entire team . <p> Private Workshops and Conferences SDSC will @ @ @ @ @ @ @ @ @ @ the member compa-nys executives or senior staff and SDSC research-ers . This format is designed to promote deeper exploration of topics such as emerging technology impacts on business strategy , or in-depth planning of potential research collaborations . SDSC also has conference facilities which can be reserved by member companies . <p> Visits by SDSC Researchers to Member On-Site Company Sessions SDSC can arrange visits to member companies by individual researchers or groups . On-site visits can range from an hour or two to a full-day sessions , and may include presentations as well as discus-sions and consultations . International or extensive travel requirements may be contingent on available program funding . <p> Collaboration with SDSC Researchers and Campus-wide Services For member companies that wish to engage more deeply with SDSC through research , consulting , or use of SDSCs computing and storage infrastruc-ture , the IPP liaison serves as a point of coordina-tion for discussions , development of proposals , and exploration of funding options and service agreements . In addition , the IPP can facilitate en-gagement with other UC San Diego administrative groups as needed , including Business @ @ @ @ @ @ @ @ @ @ Technology Transfer Office . <p> Continuing and Executive Education Opportunities SDSCs continuing and executive education pro-grams such as the " Data Mining Boot Camp " series , executive seminars on Cloud Computing and Big Data , and technical workshops or seminars can be made available to members . The IPP can also work with member companies to develop on-site training and education tailored to a members needs . Discounts for fee-based education and training are available to IPP member companies . <p> Industry Visitors Members may place a visiting researcher at SDSC for active involvement under a specific area of research . SDSC values visiting researchers and expects the visitor to be deeply engaged in the research . While not specifically designed to provide basic education and training , visiting researchers will be provided office space and access to university facilities . Typically , this arrangement extends up to 12 months . <h> IPP Membership Fees <p> IPP membership is obtained for a flat annual fee provided as a tax-deductible , unrestricted gift . <p> For inquiries or further information , please contact the Industrial Partners Progam at ipp@sdsc.edu <h> @ @ @ @ @ @ @ @ @ @ Biotech ResearchThursday , Sep 29 , 2016Technological advances have dramatically reduced the cost and increased the throughput of DNA sequencing , resulting in the generation of vast amounts of sequence data that must be stored and analyzed to support research . As a national leader in high-performance computing and storage , SDSC has been sought out for its infrastructure and expertise in storing and analyzing genomic and ancillary data for life sciences research . 
@@97506196 @1706196/ <p> AutoDock3.0 A suite of automated docking tools designed to predict how small molecules , such as substrate or drug candidates , bind to a receptor of known 3D structure <p> CHARMMCHARMM ( Chemistry at HARvard Molecular Mechanics ) is a molecular dynamics and energy minimization program <p> Disulfide by Design An application for the rational design of disulfide bonds in proteins and for exporting mutant PDB files containing the modeled disulfides for visualization in other molecular modeling software , for Windows <p> LOOPPLinear Optimization of Protein Potentials . Cornell Theory Center program for potential optimization and alignments of sequences and structures <p> Macromoltek Molecular simulations simplified . Antibody modeling , side-chain packing , renumbering , and other web-based computational tools for antibody development available in easy-to-use workspaces . Macromoltek also provides a consulting service for customers in need of more specialized analysis . <p> MCCE Multi-Conformation Continuum Electrostatics software : Calculates theoretical pKas of residues in proteins and provides the modulating factors of pKas based on the structure in PDB format <p> OpenContact OpenContact is an open source , PC software tool for quickly mapping the energetically dominant @ @ @ @ @ @ @ @ @ @ . A simple GUI is provided to the user to perform the mapping and no knowledge of the underlying programs are required . Output files to the user are in the form of color graphics , spreadsheets , and text files . 
@@97506198 @1706198/ <p> AbstractThe epithelial-mesenchymal transition ( EMT ) is a highly conserved program necessary for orchestrating distant cell migration during embryonic development . Multiple studies in cancer have demonstrated a critical role for EMT during the initial stages of tumorigenesis and later during tumor invasion . Transcription factors ( TFs ) such as SNAIL , TWIST , and ZEB are master EMT regulators that are aberrantly overexpressed in many malignancies . Recent evidence correlates EMT-related transcriptomic alterations with metabolic reprograming in cancer . Metabolic alterations may allow cancer to adapt to environmental stressors , supporting the irregular macromolecular demand of rapid proliferation . One potential metabolic pathway of increasing importance is the hexosamine biosynthesis pathway ( HBP ) . The HBP utilizes glycolytic intermediates to generate the metabolite UDP-GlcNAc . This and other charged nucleotide sugars serve as the basis for biosynthesis of glycoproteins and other glycoconjugates . Recent reports in the field of glycobiology have cultivated great curiosity within the cancer research community . However , specific mechanistic relationships between the HBP and fundamental pathways of cancer , such as EMT , have yet to be elucidated . Altered @ @ @ @ @ @ @ @ @ @ mediate many cellular changes associated with EMT including cell-cell adhesion , responsiveness to growth factors , immune system evasion , and signal transduction programs . Here , we outline some of the basics of the HBP and putative roles the HBP may have in driving EMT-related cancer processes . With novel appreciation of the HBPs connection to EMT , we hope to illuminate the potential for new therapeutic targets of cancer . 
@@97506199 @1706199/ <h> MyPDB Login <h> About the PDB Archive and the RCSB PDB <p> The Protein Data Bank ( PDB ) archive is the single worldwide repository of information about the 3D structures of large biological molecules , including proteins and nucleic acids . These are the molecules of life that are found in all organisms including bacteria , yeast , plants , flies , other animals , and humans . Understanding the shape of a molecule deduce a structure 's role in human health and disease , and in drug development . The structures in the archive range from tiny proteins and bits of DNA to complex molecular machines like the ribosome . <p> The PDB archive is available at no cost to users . The PDB archive is updated weekly . <p> The PDB was established in 1971 at Brookhaven National Laboratory under the leadership of Walter Hamilton and originally contained 7 structures . After Hamilton 's untimely death , Tom Koetzle began to lead the PDB in 1973 , and then Joel Sussman in 1994 . Led by Helen M. Berman , the Research Collaboratory for Structural @ @ @ @ @ @ @ @ @ @ the PDB in 1998 . In 2003 , the wwPDB was formed to maintain a single PDB archive of macromolecular structural data that is freely and publicly available to the global community . It consists of organizations that act as deposition , data processing and distribution centers for PDB data . Stephen K. Burley became Director in 2014 . <p> In addition , the RCSB PDB supports a website where visitors can perform simple and complex queries on the data , analyze , and visualize the results . Details about the history , function , progress , and future goals of the RCSB PDB can be found in our Annual Reports and Newsletters . <p> RCSB PDB development follows our Mission , Vision , and Values ( PDF ) . Staff are located at Rutgers , The State University of New Jersey and the University of California , San Diego . Job listings for open positions are posted online . <h> RCSB PDB Users and Impact <p> The RCSB PDB has an international community of users , including biologists ( in fields such as structural biology , biochemistry , genetics @ @ @ @ @ @ @ @ @ @ as bioinformatics , software developers for data analysis and visualization ) ; students and educators ( all levels ) ; media writers , illustrators , textbook authors ; and the general public . <p> The website ( rcsb.org ) is accessed by &gt;1 million unique visitors per year . <p> RCSB PDB services have broad impact across research and education . The primary RCSB PDB citation ( Berman et al. , Nucleic Acids Research 2000 ) is one of the top-cited scientific publications of all time . A bibliometric analysis performed by Clarivate Analytics ( PDF ) in 2017 shows PDB motivated high-quality research throughout the world . Papers citing had a citation-based impact exceeding the world-average in 16 scientific fields including Biology &amp; Biochemistry , Computer Science , Plant &amp; Animal Sciences , Physics , Environment/Ecology , Mathematics and Geosciences . <p> An economic analysis performed by the Rutgers Office of Research Analytics in 2016 noted that a reasonable estimate to replicate the PDB data archive is $12 billion ( PDF ) . The use value of RCSB PDB data and services is estimated at $5.5 billion annually . @ @ @ @ @ @ @ @ @ @ Committee is made up of an international team of experts in X-ray crystallography , cryoEM , NMR , bioinformatics and education . RCSB PDB appreciates the valuable feedback they provide on an ongoing basis . <p> The RCSB PDB is a member of the wwPDB , a collaborative effort with PDBe ( UK ) , PDBj ( Japan ) , and BMRB ( USA ) to ensure the PDB archive is global and uniform . <p> As the wwPDB archive keeper , the RCSB PDB updates the PDB archive at ftp : //ftp.wwpdb.org weekly . The structures included in each release are highlighted on the RCSB PDB home page and clearly defined on the FTP site . These sites are maintained 24 hours a day , seven days a week . A failover system automatically redirects internet traffic to a mirror site , if needed . 
@@97506201 @1706201/ 1450 @qwx861450 <h> NSF Awards $15 Million to Create Science Gateways Community Institute <h> Multi-Partner Project Led by SDSC to Meet Researchers Needs across All Science Domains <p> Published July 29 , 2016 <p> The five key areas for the Science Gateways Community Institute to increase the number , ease of use , and effective application of gateways to serve the greater research and engineering community . Source : SDSC <p> The National Science Foundation ( NSF ) has awarded a five-year $15 million grant to a collaborative team led by the San Diego Supercomputer Center ( SDSC ) at UC San Diego to establish a Science Gateways Community Institute to accelerate the development and application of highly functional , sustainable science gateways that address the needs of researchers across the full spectrum of NSF directorates . <p> The Institutes goal is to increase the number , ease of use , and effective application of gateways for the greater research and engineering community , resulting in broader gateway use and more widespread engagement in science by professionals , citizen scientists , students , and more . The project will @ @ @ @ @ @ @ @ @ @ was part of a larger NSF announcement in which the agency committed $35 million to create two Scientific Software Innovation Institutes ( S2I2 ) that will serve as long-term hubs for scientific software development , maintenance and education . <p> " The Institutes will ultimately impact thousands of researchers , making it possible to perform investigations that would otherwise be impossible and expanding the community of scientists able to perform research on the nations cyberinfrastructure , " said Rajiv Ramnath , program director in the division of Advanced Cyberinfrastructure at NSF . <p> A science gateway is a community-developed set of tools , applications , and data services and collections that are integrated through a web-based portal or suite of applications . Such gateways provide scientists access to many of the tools used in cutting-edge research telescopes , seismic shake tables , supercomputers , sky surveys , undersea sensors , and more and connect often diverse resources in easily accessible ways that save researchers and institutions time and money . <p> " Gateways foster collaborations and the exchange of ideas among researchers and can democratize access , providing broad access @ @ @ @ @ @ @ @ @ @ leading research institutions . " said Nancy Wilkins-Diehr , SDSC associate director and principal investigator for the project . <p> " Because of Nancys dedication , SDSC has for many years been a leader in research using science gateways , " said SDSC Director Michael Norman . " Such gateways , including the ever-popular CIPRES ( CyberInfrastructure for Phylogenetic RESearch ) gateway used to explore evolutionary relationships between species , have been accessed by thousands of users via our supercomputing resources including our new Comet system . " <p> " In XSEDE , we have observed tremendous growth in terms of the number of gateway users , the number of processing hours used on HPC resources and the number of published research papers using gateways in the last couple of years , " said Wilkins-Diehr . " We see the services offered by SGCI dovetailing nicely with those offered by XSEDE . In the XSEDE Extended Collaborative Support ( ECSS ) program , our primary focus is supporting developers of existing gateways with their back-end connections to XSEDE resources . SGCI frees us up to offer services developing front ends @ @ @ @ @ @ @ @ @ @ not . " <p> Multiple Partnerships , Multiple Components <p> In early 2015 , the NSF identified science gateways as one of two focus areas for the implementation phase of its Software Institute program . Through a $500,000 award , a team led by Wilkins-Diehr developed a strategic plan for a much larger Science Gateways Institute as part of that Software Institute programs conceptualization phase . <p> The new SGCI award brings together expertise from a wide range of partner universities and institutions including Elizabeth City State University in North Carolina ; Indiana University ; University of Notre Dame ; Purdue University ; the Texas Advanced Computing Center ( TACC ) at the University of Texas , Austin ; and the University of Michigan at Ann Arbor . <p> The Institutes five-component design is the result of several years of studies , including many focus groups and a 5,000-person survey of the research community , including NSF principal investigators , campus CIOs and CTOs , and others . Those component areas include : <p> An Incubator , to provide shared expertise in business and sustainability planning , cybersecurity , user @ @ @ @ @ @ @ @ @ @ will be led by Michael Zentner ( Information Technology at Purdue University ) . <p> Extended Developer Support , to provide expert developers for up to one year to projects that request assistance and demonstrate the potential to achieve the most significant impacts on their research communities . Led by Marlon E. Pierce ( Indiana University ) . <p> The Scientific Software Collaborative , to oversee a component-based , open-source , extensible framework for gateway design , integration , and services , including gateway hosting and capabilities for external developers to integrate their own software into Institute offerings . Led by Maytal Dahan ( TACC ) . <p> Community Engagement and Exchange , which will provide a forum for communication and sharing experiences among gateway developers within the NSF , across federal agencies , and internationally . Led by Katherine A. Lawrence ( University of Michigan ) with support from Sandra Gesing ( University of Notre Dame ) . <p> Workforce Development , to increase the pipeline of gateway developers with training programs , including special emphasis on recruiting underrepresented minorities , and by helping universities form gateway support groups @ @ @ @ @ @ @ @ @ @ University ) . <p> The work is funded via NSF award number is ACI-1547611 and more information about SGCI is available here . <p> " Were excited about the opportunity to build community nationally and internationally , " added Wilkins-Diehr . " Sharing expertise about basic infrastructure allows developers to concentrate on the novel , the challenging , and the cutting-edge development needed by their specific user community . <h> SDSC and Science Gateways <p> SDSC has a proven track record in leading the creation of science gateways , which are widely used throughout numerous domains , from astronomy and biophysics to molecular dynamics and the neurosciences . Access is also available through the National Science Foundations XSEDE ( eXtreme Science and Engineering Discovery Environment ) program . <p> One of the most popular science gateways is the CIPRES ( CyberInfrastructure for Phylogenetic RESearch ) gateway , a web-based portal developed eight years ago by SDSC researchers that allows scientists to explore evolutionary relationships between species . Recently , CIPRES was used extensively to help create a new tree of life under a UC Berkeley-led study that added more than @ @ @ @ @ @ @ @ @ @ , CIPRES reached a milestone by supporting its 2,500th scientific publication . <p> By using gateways , scientists can conduct their research in significantly shorter times without having to understand how to operate supercomputers , according to Mark Miller , principal investigator of the CIPRES gateway and an SDSC biologist . Typically , about 200 CIPRES jobs are running simultaneously on SDSCs Comet supercomputer , and another 100 on Gordon . <p> " The scheduling policy on Comet allows us to make big gains in efficiency because we can use anywhere between one and 24 cores on each node , " said Miller . " When you are running 200 small jobs 24/7 , those savings really add up in a hurry . " <h> Supporting the National BRAIN Initiative through the Neuroscience Gateway <p> Charting brain functions in unprecedented detail could lead to new prevention strategies and therapies for disorders such as Alzheimer 's disease , schizophrenia , autism , epilepsy , traumatic brain injury , and more . The BRAIN Initiative ( Brain Research through Advancing Innovative Neurotechnologies ) , launched by President Barack Obama in 2013 , @ @ @ @ @ @ @ @ @ @ map and decipher brain activity , including advanced computational resources and expertise . <p> In 2015 , the NSF and the United Kingdoms Biotechnology and Biological Sciences Research Council ( BBSRC ) awarded funding for a new Neuroscience Gateways project led by SDSC . That project , which will contribute to the national BRAIN initiative , is a collaboration between UC San Diego , with SDSCs Amit Majumdar as the principal investigator ( PI ) and Subhashini Sivagnanam as co-PI ; Yale University , with Ted Carnevale as PI ; and University College London , with Angus Silver as PI . <p> About SDSC <p> As an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community , including industry and academia . Cyberinfrastructure refers to an accessible , integrated network of computer-based resources and expertise , focused on , and @ @ @ @ @ @ @ @ @ @ cluster , and are both part of the National Science Foundations XSEDE ( eXtreme Science and Engineering Discovery Environment ) program , the most advanced collection of integrated digital resources and services in the world . 
@@97506204 @1706204/ 1450 @qwx861450 <h> SDSC to Improve the Performance of Life Science Applications <p> Spurred by the increasing reliance of life sciences researchers in the academic and private sectors on computational methods and data-enabled science , the San Diego Supercomputer Center ( SDSC ) at the University of California San Diego has inaugurated a new life sciences computing initiative focused on improving the performance of bioinformatics applications and associated analysis pipelines on current and future advanced computing systems . <p> Dramatic improvements in scientific instruments and techniques , such as Next Generation Sequencing ( NGS ) and Cryo-electron Microscopy ( Cryo-EM ) , are enabling the rapid accumulation of vast amounts of data including DNA/RNA molecular sequences and high-resolution imaging of biological structures from all manner of animal and plant organisms . The research and biotech community faces daunting challenges to manage and analyze this trove of data . <p> " Our experience with both on-campus researchers and biotech companies is that refined bioinformatics techniques and new technologies can be leveraged to improve the breadth and throughput of analyses , " noted Wayne Pfeiffer , an SDSC Distinguished Scientist and @ @ @ @ @ @ @ @ @ @ methods and analytics to derive scientific insights and commercial innovations from these growing pools of data , while advances in high-performance computing , storage , and networking are required to keep pace and help enable new discoveries . SDSCs initiative will focus on developing and applying rigorous approaches to assessing and characterizing computational methods and pipelines . It will then specify the architectures , platforms , and technologies for optimizing along dimensions such as performance and throughput . <p> " UC San Diego is a pillar in one of the most vibrant life science research ecosystems in the world , and life science computing has always been a major focus of SDSC , " said SDSC Director Michael Norman . " Over the past decade we have seen unprecedented growth in the use of advanced computing by researchers in this area , and we view this partnership as a unique opportunity to bring the combined experience of SDSC , Dell EMC , and Intel to bear on one of the most important areas of data-intensive research today . " <p> The initial work , supported by Dell EMC and Intel @ @ @ @ @ @ @ @ @ @ analysis pipelines and developing targeted recommendations for technical architectures to service those pipelines . Architectural recommendations will encompass integrated computing and storage platforms as well as networking fabrics . <p> " Dell EMC is dedicated to enabling and advancing the state-of-the-art for HPC solutions from the workgroup to the Top500 , " said Jim Ganthier , senior vice president , Validated Solutions and HPC Organization , Dell EMC . " We are pleased to collaborate with both Intel and SDSC to help drive dramatic performance improvements and results in life sciences research . " <p> For more information about SDSC and opportunities for other organizations to get involved , please contact Ron Hawkins , Director of Industrial Relations . <p> About SDSC <p> As an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community , including industry and academia . Cyberinfrastructure refers to an accessible , integrated network of computer-based resources and expertise , focused on , and health IT . SDSCs Comet joins the Centers data-intensive Gordon cluster , and are both part of the National Science Foundations XSEDE ( Extreme Science and Engineering Discovery Environment ) program . 
@@97506205 @1706205/ <h> Poseview Image of NAG in 4W4O <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of GAL in 4W4O <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MAN in 4W4O <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of BMA in 4W4O <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of FUC in 4W4O <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of GOL in 4W4O <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of ZN in 4W4O <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of ACT in 4W4O <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506207 @1706207/ <h> Providing More Space to Users <h> Gene View <h> Gene Structure and Mapping to PDB Entries for Human Genes <p> A new Gene View illustrates the correspondences between the human genome and 3D structure . All human genes have been mapped to representative PDB protein chains ( chosen from sequence clusters at 40% sequence identity ) to show which regions of a gene are available in PDB coordinates . <p> The graphical view is built on top of the BioDalliance genome browser . Users can use the mouse to move through the region around a human gene as shown in the Gene View . <p> Gene View displays information in several horizontal rows called tracks , which can be reordered by dragging the title box . The view can be zoomed in or out . <p> The Genome track displays the position of the region of the genome and the nucleotide ( when zoomed out as a letter code , when zoomed in as a color-coded region ( Guanine : black , Adenine : green , Cytosine : blue , Thymine : red ) . <p> The PDB @ @ @ @ @ @ @ @ @ @ protein chains . Clicking on the region displays more information about the mapped protein . <h> Protein Feature View <h> Enhanced with New Information <p> The Protein Feature View has been redesigned to improve readability and to add descriptions of protein function , subunit structure , domains from UniProt , and an Exon Structure track from the gene mapping described above . <p> Protein Feature View can be launched from the " Molecular Description " section of a Structure Summary page as well as by searching for a UniProt I 'd or Gene Symbol . <h> Ramachandran Plots <h> Enhancing Protein Structure Validation <p> Structure Summary pages for X-ray crystal structures now provide access to a PDF version of the full PDB Validation Report for the entry in addition to the display of the report 's " slider " graphic . These reports , which provide an assessment of the quality of a structure and highlight specific concerns , were created using the recommendations of the wwPDB X-ray Validation Task Force . <p> Also , links to new Ramachandran plots created by MolProbity ( version 4.02b ) have been added to @ @ @ @ @ @ @ @ @ @ a PDF file . Ramachandran plots provide an independent method to evaluate the conformational quality of protein structures . <p> Reference data for six classes of Ramachandran plots . Conformations outside the outer contour line are considered outliers ( Source : J. Richardson , et al . ) <p> Access to these views was suggested by the wwPDB X-ray Validation Task Force . The Ramachandran plots provide a visual representation of outliers tabulated in the Validation Report PDF . <p> Links to the full validation report and Ramachandran Plots from the Structure Summary page for entry 3WQ8 <p> Previously , Ramachandran plots were located on the Geometry tab , which has been retired . Additional validation data and statistics are available from the Structure Summary Links tab , including reports by WHATCHECK and PROCHECK. 
@@97506208 @1706208/ <p> The 3D Similarity tab provides pre-calculated systematic structure comparisons for all PDB proteins . The new version of this feature provides domain-based protein structure alignments instead of chain-based alignments . <p> The procedure to calculate the domain-split representative is an extension of our sequence clustering approach . In order to remove redundancy , we start with a 40% sequence identity clustering procedure . A representative chain is taken from each sequence cluster . In cases where the representative chain comprises multiple domains , each of those domains is included in database searches . If available , the domain assignment provided by SCOP 1.75 is used . Otherwise algorithmic domain assignments are computed using the ProteinDomainParser software . <h> Highlighting of Search Terms in Results <p> Keyword and Macromolecule name searches , which can be performed by clicking the " search " button on the top-bar search without specifically selecting an auto-complete suggestion , now highlight the matching sentence within the text of the PDB entry . <p> Also highlighted by each search is the name of the data field ( title , citation , molecule name , remark @ @ @ @ @ @ @ @ @ @ found . For example , searching for the terms " mutant p53 " on the top-bar search reveals that some of the results match structure titles , while others match REMARK 900 records ( related entries ) or words within the keywords section . <h> LINK Records <p> When searching , it is possible to specify the type of connectivity ( covalent bond , disulfide bridge , etc. ) , the three-letter codes for one or both of the linked residues , and/or the atom names of the linked atoms . <p> For example , a user can find any structure which contains coordination of an alanine ( ALA ) carbonyl oxygen ( O ) to calcium ( CA ) . <h> Obsolete Ligands <p> There are several cases where ligand entries have been withdrawn from the wwPDB Chemical Component Dictionary and marked with the status " obsolete . " Some are incorrectly defined chemical structures , but most are redundant ligands that have been superseded by an identical ligand with a different three-letter code . <p> For normal searches of the PDB , obsolete ligands are excluded . However @ @ @ @ @ @ @ @ @ @ we now provide an Obsolete Ligand search function whereby the user can find information on both obsolete ligands and their superseding couterparts , simply by entering the three-letter code in the top-bar search . <p> For example , the obsolete ligand 0AC has been superseded by identical ligand 1CR . <p> The Uniprot and EC numbers that are displayed in the structure summary page are checked for consistency with the UniProt and the IUBMB web sites on a weekly basis . <p> Information from related UniProt entries is used , and simple improvements , such as the forwarding of obsolete EC numbers , are performed automatically . <p> This results in better coverage , quality , and consistency in the cross-referencing that is applied to structure pages , to relevant hierachical classification browsers , to relevant searches and associated functionality such as drill-down charts and auto-completion suggestions , and to relevant reports and associated web services . <p> For each ligand included in the report , a sub-table can be selected to show lists of all related PDB entries that contain the ligand , the entries that contain the ligand @ @ @ @ @ @ @ @ @ @ ligand as part of a larger , polymeric ligand . <p> These reports now list the PDB IDs associated with a ligand , rather than just including the number of related entries . To display this sub-table , select the triangle ( ) shown next to the Ligand I 'd . See a report example with ligand I 'd 017 's sub-table displayed . <p> The sub-table limits the display to 15 PDB IDs in each column . For the ligands associated with more than 15 PDB IDs , the ... more link will launch a query for that set of structures . <p> The screen shot image above shows a Ligand Summary Report with 12 ligand IDs . The sub-table has been displayed for ligand I 'd 017 . The big red arrow shows the query result page after clicking the ... more link . The result page is a query of " Chemical I 'd 017 and Polymeric Type Any . " <p> As part of the tabular report system , the Ligand Summary Report can be exported in three formats : , Excel 97-2003 , Excel 2007 or newer versions , @ @ @ @ @ @ @ @ @ @ the exported file , the three PDB I 'd lists with complete PDB IDs are always included in the exported file as additional columns . Due to Excel 's cell limitation , we suggest that the user export the file in CSV format if some ligands in the report are associated with more than 6500 PDB IDs . <p> All tabular report features are also available , including sorting , filtering , export to other report formats , and column customization . For more details , please visit our previous release note on Tabular Report Improvements . 
@@97506210 @1706210/ <h> Tabular Report <p> The Tabular Report system provides a broad view of PDB data and a friendly interface for users to browsing , filtering , and searching . Users can view data in predefined standard reports or create customized reports . <h> Standard Reports vs . Custom Reports <p> The tabular report system includes a list of pre-generated summary reports and links to custom reports . These reports are accessible from the Reports pulldown menu on the query results page . The pre-generated summary reports usually focus on certain aspect of the structures , such as Sequence Report , Ligands Report , Custom Report , etc . <p> Some users may want to have a report containing data items from multiple standard reports or specially want to include Domain details , they can select " Tabular Report " from the drop down menu to link to Custom Tabular Report , and then select the interested items and generate a custom report . <h> Report Features <h> Pagination and Bidirectional Sorting <p> By default , the first 20 records sorted by PDB I 'd are rendered . The user can @ @ @ @ @ @ @ @ @ @ Table sorts can be done on the entire report by clicking the column headers . Two triangles are displayed in the current sorted column . The screen shot below is a Structure Summary report sorted by Resolution column . <p> Column resizing : All column widths are resizable by dragging the line between two columns . <p> Report resizing : The report table is resizeable by dragging the triangle at the right bottom corner . <p> Sorting entity-based data : Sequence Report , Biological Details Report , and reports contain entity based data will always be sorted by the combination of the structure I 'd and chain IDs . Any entry with multiple chains will always be displayed with chain IDs sorted ascending . <p> Inline Filter : This feature provides an easy way for user to search data in the tabular report . User can input the criteria data in any text box , and the records matching the data will be retrieved automatically . <h> One-to-Many Relationships in Report Tables <p> Reports that have to display a lot of information for a single structure are formatted in a compact @ @ @ @ @ @ @ @ @ @ associated with multiple GO IDs . The Excel and CSV export will keep the one-to-many presentation . <h> Exporting to other formats <p> Tables can be exported in three formats : <p> Excel 97-2003 format : Multiple work sheets will be generated for very large data sets to accommodate the row limitation ( 65,536 ) in older versions of Excel . <p> Excel 2007 or newer versions : This version supports up to 1,048,576 rows per work sheet . <p> The Excel spreadsheets have been reformatted with customized column width , text wrapping , alignment , and hyperlinks on selected columns . Formatting issues with PDB IDs that resemble floating point numbers such as 1E10 have been resolved . <p> Only the filtered result set will be exported to Excel or CSV , if user conducted a search using Inline Filter or Filter Results . And the presentation of one-to-many relationships is maintained in the exported Excel report . 
@@97506212 @1706212/ <h> Born : Paris , April 1 , 1776 <h> Died : Paris , June 26 , 1831 <h> Revolutionary Mathematician <p> By all accounts , Sophie Germain was a somewhat withdrawn child . She was the second of three daughters of a Parisian silk merchant , Ambroise-Frantois Germain . One sister married a government official and the other a physician . Sophie never married , lived at home all her life , and pursued her mathematical studies with what her recent biographers term " limitless passion and devotion . " * <p> Her first biographer , an Italian mathematician named Libri , is the source of two stories told about Germain that seem to frame her personality . As a 13-year-old , while talk of the Revolution swirled in her household , she withdrew to her father 's library . There she read about Archimedes , so engrossed in his mathematical musings that he ignored a Roman invader of Syracuse , who thereupon killed him . She may have seen in Archimedes ' mathematics " an environment where she too could live untouched by the confusion of social @ @ @ @ @ @ @ @ @ @ , and Libri relates that her parents were so opposed to her behavior that she took to studying at night . They responded by leaving her fire unlit and taking her candles . Sophie studied anyway , swaddled in blankets , by the light of smuggled candles . <p> On the establishment in 1795 of the Ecole Polytechnique , which women could not attend , Germain befriended students and obtained their lecture notes . She submitted a memoir to the mathematician J. L. Lagrange under a male student 's name . Lagrange saw talent in the work , sought out the author , and was bowled over to discover it had been written by a woman . She continued to study , corresponding with leading mathematicians of the day . <p> Her mathematical work shifted from number theory to more applied mathematics . The occasion was the demonstration by a visitor to Paris , one E. F. F. Chladni , of curious patterns produced on small glass plates covered with sand and played , as though the plates were violins , by using a bow . The sand moved about @ @ @ @ @ @ @ @ @ @ patterns resulting from the " playing " of different notes caused great excitement among the Parisian polymaths . It was the first " scientific visualization " of two-dimensional harmonic motion . Napoleon authorized an extraordinary prize for the best mathematical explanation of the phenomenon , and a contest announcement was issued . <p> Sophie Germain 's entry was the only one . While it contained mathematical flaws and was rejected , her approach was correct . All the other possible entrants in the contest were prisoners of the ruling paradigm , consideration of the underlying molecular structure theorized for materials . The mathematical methodologies appropriate to the molecular view could not cope with the problem . But Germain was not so encumbered . <p> Various mathematicians helped her to pursue a new application , and she won the prize on her third attempt , in 1816 . The very public prizewinning gained her some attention . But her gender kept her " always on the outside , like a foreigner , at a distance from the professional scientific culture . " ? <p> Perhaps only a lone genius like Germain @ @ @ @ @ @ @ @ @ @ work of pure intellection like a beacon to later generations of women who dared to do mathematics for the joy of it . <p> ** Ibid . ( But in fact she or the history book drew the wrong conclusion . Archimedes did not die for his absent-mindedness but was a target of the Roman soldiers precisely because he had been the " brains " behind the Syracusan defenses , directing the building of catapults and even developing a mirror system to focus light on the Roman ships and set their sails afire. ) 
@@97506216 @1706216/ <p> Homeworks will usually be " assigned " on Friday and " due " Friday of the following week . <p> Course Outline : <p> I. Instruction Set Architecture <p> II . Computer System Performance and Performance Metrics <p> III . Computer Arithmetic and Number Systems <p> IV . CPU Architecture <p> V. The Memory/Cache Hierarchy <p> VI . Pipelining <p> VII . Superscalars <p> VIII . Parallel Machines <p> Grading Information : <p> The grade for 141 will be based on homeworks , one midterm , and a final , as follows : <p> homeworks : 0% ( but 100% important ! ) <p> class participation and quizzes : 10% <p> midterm : 40% <p> final : 50% <p> subjective influences like class participation will have an impact in the margins -- it does pay to let the professor know who you are ! <p> The final will be inclusive of all course material . <p> The grade for 141L will be based solely on 4-5 lab reports . I expect that most students will complete the lab assignments as specified ; thus the quality of the lab @ @ @ @ @ @ @ @ @ @ grades . <p> Homework will not be turned in . Late lab assignments are not encouraged . You will have two grace days during the quarter . I.e. , you can turn one lab assignment in two days late , or two assignments in one day late . I recommend not spending those days frivolously early in the quarter . After you have spent your grace days , late assignments will be accepted , but with no guarantees that they will be graded , and with significant penalties if they are . We will make every effort to return assignments to you in a timely manner -- limiting your ability to turn things in late is , unfortunately , critical to that goal . Anytime after the end of class counts as a day late . The second day begins 24 hours later . The weekend counts as a single day . Thus , something turned in Monday ( before 9 a.m. ! ) that was due Thursday is two days late . <p> You have the right of appeal for grading on all tests ; however , an appeal @ @ @ @ @ @ @ @ @ @ , and may result in an unfavorable judgment on another problem . You have one week from the time the midterms are returned to make appeals , including addition errors on your score . Check it over carefully when you get it . All appeals must be made in writing and given to the instructor . <p> There is no grading of homeworks . But you need to know how to solve homework problems to pass the tests . A few homework questions will appear on the test VERBATIM . Some other test questions will be very similar to homework . There will be no posted solutions to homework . But we will be happy to assist you in discovering the right answer in office hours , reviews , and sections . Solving the homework is your responsibility and if you do it well and consistently it is sure to result in a good grade . Finding the right answer for yourself ensures you can do the same on the test . <p> STRICTLY speaking there is no curve . It is fine with me if everyone gets As . @ @ @ @ @ @ @ @ @ @ Every quarter I give all the grades A through F. GENERALLY speaking the average is a C , one standard deviation above is a B , two above is an A to A+ , one below is a D , two below is an F. However I reserve the right to draw these lines according to the distribution . If everyone gets 90% then everyone gets As . <p> Integrity : <p> Cheating WILL be taken seriously . It is not fair to honest students to take cheating lightly , nor is it fair to the cheater to let him/her go on thinking that is a reasonable alternative in life . Do n't test me on this one . <p> Tests are closed notes , closed book , closed neighbor , closed mouth , open mind , STATE YOUR ASSUMPTIONS about unclear or vague questions . Some questions are deliberately vague or ambigious in order to test your ability to make reasonable assumptions ( just like in real life ) . Reasonable assumptions I did not anticipate will receive full marks . However it is up to me to decide @ @ @ @ @ @ @ @ @ @ and will be punished : <p> -Receiving , providing , or soliciting assistance from another student during a test. -Having touching or looking at a book or any writen material except the test and the blackboard during a test . <p> Penalties -- anyone copying information or having information copied during a test will receive an F for the class and will not be allowed to drop . They will be reported to their college dean . If you can prove non-cooperative copying took place , your grade may be restored , but you must prove it to the dean -- I do n't want to be involved . 
@@97506217 @1706217/ <p> AbstractChemokine CXCL8 and its receptor CXCR1 are key mediators in combating infection and have also been implicated in the pathophysiology of various diseases including chronic obstructive pulmonary disease ( COPD ) and cancer . CXCL8 exists as monomers and dimers but monomer alone binds CXCR1 with high affinity . CXCL8 function involves binding two distinct CXCR1 sites the N-terminal domain ( Site-I ) and the **27;1919;TOOLONG domain ( Site-II ) . Therefore , higher monomer affinity could be due to stronger binding at Site-I or Site-II or both . We have now characterized the binding of a human CXCR1 N-terminal domain peptide ( hCXCR1Ndp ) to WT CXCL8 under conditions where it exists as both monomers and dimers . We show that the WT monomer binds the CXCR1 N-domain with much higher affinity and that binding is coupled to dimer dissociation . We also characterized the binding of two CXCL8 monomer variants and a trapped dimer to two different hCXCR1Ndp constructs , and observe that the monomer binds with G+10- to 100-fold higher affinity than the dimer . Our studies also show that the binding constants of monomer @ @ @ @ @ @ @ @ @ @ dissociation constant , can vary significantly as a function of pH and buffer , and so the ability to observe WT monomer peaks is critically dependent on NMR experimental conditions . We conclude that the monomer is the high affinity CXCR1 agonist , that Site-I interactions play a dominant role in determining monomer vs. dimer affinity , and that the dimer plays an indirect role in regulating monomer function . 
@@97506220 @1706220/ <p> RATIONALE : S-nitrosylation ( SNO ) , an oxidative post-translational modification of cysteine residues , responds to changes in the cardiac redox-environment . Classic biotin-switch assay and its derivatives are the most common methods used for detecting SNO . In this approach , the labile SNO group is selectively replaced with a single stable tag . To date , a variety of thiol-reactive tags have been introduced . However , these methods have not produced a consistent data set , which suggests an incomplete capture by a single tag and potentially the presence of different cysteine subpopulations . <p> OBJECTIVE : To investigate potential labeling bias in the existing methods with a single tag to detect SNO , explore if there are distinct cysteine subpopulations , and then , develop a strategy to maximize the coverage of SNO proteome . <p> METHODS AND RESULTS : We obtained SNO-modified cysteine data sets for wild-type and S-nitrosoglutathione reductase knockout mouse hearts ( S-nitrosoglutathione reductase is a negative regulator of S-nitrosoglutathione production ) and nitric oxide-induced human embryonic kidney cell using 2 labeling reagents : the cysteine-reactive pyridyldithiol and iodoacetyl based @ @ @ @ @ @ @ @ @ @ SNO-modified residues were detected by both tags , whereas the remaining SNO sites were only labeled by 1 reagent . Characterization of the 2 distinct subpopulations of SNO residues indicated that pyridyldithiol reagent preferentially labels cysteine residues that are more basic and hydrophobic . On the basis of this observation , we proposed a parallel dual-labeling strategy followed by an optimized proteomics workflow . This enabled the profiling of 493 SNO sites in S-nitrosoglutathione reductase knockout hearts . 
@@97506221 @1706221/ <p> Using a unique computational approach to rapidly sample , in millisecond time intervals , proteins in their natural state of gyrating , bobbing , and weaving , a research team from UC San Diego and Monash University in Australia has identified promising drug leads that may selectively combat heart disease , from arrhythmias to cardiac failure . <p> Reported in the September 5 , 2016 Proceedings of the National Academy of Sciences ( PNAS ) Early Edition , the researchers used the computing power of Gordon and Comet , based at the San Diego Supercomputer Center ( SDSC ) at UC San Diego ; and Stampede , at the Texas Advanced Computing Center at the University of Texas at Austin , to perform an unprecedented survey of protein structures using accelerated molecular dynamics or aMD a method that performs a more complete sampling of the myriad shapes and conformations that a target protein molecule may go through . <p> The computing resources were provided by the National Science Foundation-funded Extreme Science and Engineering Discovery Environment ( XSEDE ) program , one of the most advanced collections of integrated @ @ @ @ @ @ @ @ @ @ The supercomputing power of Gordon , Comet , and Stampede allows us to run hundreds-of-nanosecond aMD simulations , which are able to capture millisecond timescale events in complex biomolecules , " said the studys first author Yinglong Miao , a research specialist with the Howard Hughes Medical Institute at UC San Diego and research scientist with the UC San Diego Department of Pharmacology . <p> Though effective in most cases , todays heart medications many of which act on M2 muscarinic acetylcholine receptors or M2 mAChRs that decrease heart rate and reduce heart contractions may carry side effects , sometimes serious . That 's because the genetic sequence of M2 mAChRs primary orthosteric binding site is " highly conserved , " and found in at least four other receptor types that are widely spread in the body , yielding unwanted results . <p> For this reason , drug designers are seeking a different approach , homing in on molecular targets or so-called " allosteric binding sites " that reside away from the receptors primary binding site and are built around a more diverse genetic sequence and structure than their counterpart orthosteric @ @ @ @ @ @ @ @ @ @ kind of cellular dimmer-switch that , once turned on , fine tunes the activation and pharmacological profile of the target receptor . <p> " Allosteric sites typically exhibit great sequence diversity and therefore present exciting new targets for designing selective therapeutics , " said the studys co-investigator J. Andrew McCammon , the Joseph E. Mayer Chair of Theoretical Chemistry , a Howard Hughes Medical Institute investigator , and Distinguished Professor of Pharmacology , all at UC San Diego . McCammon was named the winner of the 2016-17 Joseph O. Hirschfelder Prize in Theoretical Chemistry , awarded by the Theoretical Chemistry Institute at the University of Wisconsin-Madison , last week . <p> In particular , drug designers have begun to aggressively search for allosteric modulators to fine-tune medications that bind to G protein-coupled receptors ( GPCRs ) , the largest and most diverse group of membrane receptors in animals , plants , fungi and protozoa . These cell surface receptors act like an inbox for messages in the form of light energy , hormones and neurotransmitters , and perform an incredible array of functions in the human body . <p> In @ @ @ @ @ @ @ @ @ @ act by binding to GPCRs , treating diseases including cancer , asthma , schizophrenia , Alzheimers and Parkinsons disease , and heart disease . <p> More Targeted Therapies <p> Though many of the GPCR drugs have made their way to the medicine cabinet , most -- including M2 mAChR targeted drugs -- exhibit side effects owing to their lack of specificity . All these drugs target the orthosteric binding sites of receptors , thus creating the push to find more targeted therapies based on allosteric sites . <p> " The problem here is that molecules that bind to these allosteric sites have proven extremely difficult to identify using conventional high-throughput screening techniques , " said McCammon , also a chemistry and biochemistry professor in UC San Diegos Division of Physical Sciences . <p> Enter accelerated molecular dynamics and supercomputing . As described in this latest study , called Accelerated structure-based design of chemically diverse allosteric modulators of a muscarinic G protein-coupled receptor , some 38 lead compounds were selected from a database of compounds from the National Cancer Institute , using computationally enhanced simulations to account for binding strength and @ @ @ @ @ @ @ @ @ @ hallmarks of an allosteric behavior in subsequent in vitro experiments , with about a dozen showing strong affinity to the M2 mAChR binding site . Of these , the researchers highlighted two showing both strong affinity and high selectivity in studies of cellular behavior . These cutting-edge experiments were performed by collaborators from the Monash Institute of Pharmaceutical Sciences . <p> " To our knowledge , this study demonstrates for the first time an unprecedented successful structure-based approach to identify chemically diverse and selective GPCR allosteric modulators with outstanding potential for further structure-activity relationship studies , " the researchers wrote . <p> The next steps will involve an investigation of the chemical properties of these novel molecules by the molecular chemists from Monash , led by Celine Valant and her colleague Arthur Christopoulos . <p> " This is just the beginning . We believe that it will be possible to apply our combined cutting-edge in silico and in vitro techniques to a wide array of receptor targets that are involved in some of the most devastating diseases , " said Valant , the studys co-lead investigator from Monash . <p> @ @ @ @ @ @ @ @ @ @ the UC San Diego Department of Pharmacology ; and Ee Von Moo and Patrick M. Sexton from Monash University . <p> Funding for this research was provided by grants from the National Science Foundation ( MCB1020765 , the National Institutes of Health ( GM31749 ) , Howard Hughes Medical Institute , the National Biomedical Computation Resource ( NBCR ) , and the National Health and Medical Research Council ( NHMRC ) of Australia ( APP1055134 ; APP1082318 ) . In addition to XSEDE , supercomputing time was also provided by the Hopper and Edison supercomputers through the National Energy Research Scientific Computing Center ( NERSC ) . <p> About SDSC <p> As an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community , including industry and academia . Cyberinfrastructure refers to an accessible , integrated network of computer-based resources and expertise , focused on @ @ @ @ @ @ Comet joins the Centers data-intensive Gordon cluster , and are both part of the National Science Foundations XSEDE ( Extreme Science and Engineering Discovery Environment ) program . 
@@97506224 @1706224/ <h> Drug and Drug Target Mapping <h> Drugs and Drug Targets from DrugBank <p> Two tables provide access to information about drug and drug target information from DrugBank that are mapped to PDB entries with each weekly update . These tables , accessed from the Tools menu , can be searched , filtered , and sorted . <p> Primary Drug Targets : Lists primary drug targets in the PDB , regardless if the drug molecule is part of the PDB entry ( e.g. , apo forms of drug targets , drug target with different bound ligands ) . Biotherapeutics , such as complexes with monoclonal antibodies , are included . <h> Search for Drugs <p> These tables can be limited to specific drugs by searching for a list of drugs by trade or generic names : <h> Drugs Bound to Primary Target Table <p> The table above shows the data available for the drug Acarbose , which maps to four primary drug targets ( see Target Name column ) based on a 30% sequence identity cutoff . Two PDB entries ( 2QMJ , 3TOP ) match the first primary @ @ @ @ @ @ @ @ @ @ and contain the two stereoisomers ( ACR , QPS ) of acarbose . For the other three drug targets , no good drug target match could be found ; the sequence identity ranges from 44% to 60% . In these cases , the proteins in these PDB entries are homologous to the primary targets . <h> Primary Drug Targets Only Table <p> The table above shows the data available for two primary drug targets of Abatacept . The first target matches the PDB entries ( 1NCN , 1I85 ) , and the second target matches the PDB entry ( 1DR9 ) with 100% sequence identity . Note , there is no entry in the PDB that contains the drug Abatacept , so the table ' Drugs Bound to Primary Targets ' is empty . <h> Description of Table Columns <p> Column Name <p> Description <p> Generic name <p> Generic name of a drug . For approved drugs , this name is usually the WHO 's International Nonproprietary Name . <p> Best three matches from the PDB that contain the drug or its stereoisomer and have &gt;= 30% sequence identity to @ @ @ @ @ @ @ @ @ @ <p> Sequence identities for the best three matches between the primary target sequence and the sequence in the PDB entries . <p> All PDB IDs <p> Select the word " Search " in the last column to perform a BLAST search for the primary target sequence against all sequences in the PDB with a default e-value cutoff of 10 . This search produces a query results list ordered by the e-value of the BLAST search . 
@@97506230 @1706230/ <h> MyPDB Login <h> mmCIF Format <p> The mmCIF file format , which has been developed under the auspices of the International Union of Crystallography ( IUCr ) , is to extend the Crystallographic Information File ( CIF ) data representation used for describing small molecule structures and associated diffraction experiments . <p> This extension is referred to as the macromolecular Crystallographic Information File ( mmCIF ) . <p> CIF was developed to describe small molecule organic structures and the crystallographic experiment by the International Union of Crystallography ( IUCr ) Working Party on Crystallographic Information at the behest of the IUCr Commission on Crystallographic Data and the IUCr Commission on Journals . <p> The result of this effort was a core dictionary of data items sufficient for archiving the small molecule crystallographic experiment and its results . <p> The format of the small molecule CIF dictionary and the data files based upon that dictionary conform to a restricted version of the Self Defining Text Archive and Retrieval ( STAR ) representation developed by Hall . <p> STAR permits a data organization that may be understood by analogy with a spoken language . 
@@97506232 @1706232/ <h> CSE 151 Lecture Notes INTRODUCTION TO ARTIFICIAL INTELLIGENCE <h> Solving Problems with Search <h> Administrivia : <p> The first programming assignment is being handed out today . It is due at the beginning of class in two weeks on Thursday , April 25 . <p> Problem-solving agents : find sequence of actions that achieve goals . <p> Problem-Solving Agents <p> Problem-Solving Steps : <p> Goal transformation : where a goal is set of acceptable states . <p> Problem formation : choose the operators and state space . <p> search <p> execute solution <p> Formulating Problems <p> Types of problems : <p> Single state problems : state is always known with certainty . <p> Multi state problems : know which states might be in . <p> Contingency problems : constructed plans with conditional parts based on sensors . <p> Exploration problems : agent must learn the effect of actions . <p> Formal definition of a problem : <p> Initial state ( or set of states ) <p> set of operators <p> goal test on states <p> path cost <p> Measuring performance : <p> Does it find a solution ? @ @ @ @ @ @ @ @ @ @ the total cost ? <p> ( total cost = path cost + search cost ) <p> Choosing states and actions : <p> Abstraction : remove unnecessary information from representation ; makes it cheaper to find a solution . <p> Example Problems : <p> Toy problems : <p> 8-puzzle <p> 8-queen/n-queen <p> cryptarithmetic <p> vacuum world <p> missionaries and cannibals <p> Real World <p> Traveling Salesperson ( NP hard ) <p> VLSI layout <p> robot navigation <p> assembly sequencing <p> Searching for Solutions <p> Operators expand a state : generate new states from present ones . <p> search strategy : tells which state to expand next . <p> fringe or frontier : discovered states to be expanded <p> Some search strategies : <p> Evaluation criteria : <p> completeness : will it find a solution if one exists ? <p> time efficiency : <p> space efficiency : <p> optimality of solution : <p> Uninformed or blind searches : <p> Breadth first : ( FIFO queuing strategy ) <p> complete : yes <p> time and space : O(branchfactordepth) ( empirically space a larger problem than time ) <p> optimality : yes , if @ @ @ @ @ @ @ @ @ @ expand lo cost fringe node ) <p> optimal : yes , if no negative costs <p> a generalization of Breadth first . <p> Depth-first ( uses a stack ) <p> completeness : no ! may go down and not come back <p> time : O(branchfactordepth) <p> space : O(depth) <p> optimality : no ! returns first found , not necessarily ideal <p> depth-limited ( DFS down to some cutoff ) <p> completeness : yes , provided solution exists before cutoff <p> time : **25;1948;TOOLONG <p> space : O(depthlimit) <p> optimality : no ! <p> Iterative Deepening ( successively increasing depth-limited ) <p> diameter of state space : max anticipated path length for most problems . <p> completeness : yes ( like Breadth-first ) <p> time : O(branchfactordiameter) <p> space : O(diameter) ( like Depth 1st ) <p> optimality : yes ( like Breadth-first ) <h> Problem solving agents <p> Consider the following design for a simple problem solving agent : <p> function **30;1975;TOOLONG returnsaction <p> static:s , an action sequence , initially empty static:state , a description of the current world state static:g , a goal , initially null static:problem @ @ @ @ @ @ @ @ @ @ ifs is empty then <p> g state ) problem state ) s problem ) <p> action s , state ) s s ) returnaction <p> The agent will first formulate its goal , then it will formulate a problem whose solution is a path ( sequence of actions ) to the goal , and then it will solve the problem using search . <p> Goal formulation Often the first step in problem-dolving is to simplify the performance measure that the agent is trying to maximize . <p> Formally , a " goal " is a set of desirable world-states . <p> " Goal formulation " means ignoring all other aspects of the current state and the performance measure , and choosing a goal . <p> Example : if you are in Arad ( Romania ) and your visa will expire tomorrow , your goal is to reach Bucharest airport . <p> Be sure to notice and understand the difference between a " goal " and a description OF a goal . Technically " reach Bucharest airport " is a description of a goal . You can apply this description to @ @ @ @ @ @ @ @ @ @ set of goal states . <p> Problem formulation After goal formulation , the agent must do problem formulation . This means choosing a relevant set of states , operators for moving from one state to another , the goal test function and the path cost function . <p> The relevant set of states should include the current state , which is the initial state , and ( at least one ! ) goal state . <p> The operators correspond to " imaginary " actions that the agent might take . <p> The goal test function is a function which determines if a single state is a goal state . <p> The path cost is the sum of the cost of individual actions along a path from one state to another . <h> Types of problems <p> Single state problems <p> Consider the vacuum cleaner world . <p> Imagine that our intelligent agent is a robot vacuum cleaner . <p> Let 's suppose that the world has just two rooms . The robot can be in either room and there can be dirt in zero , one , or two rooms @ @ @ @ @ @ @ @ @ @ the dirt cleaned up . Formally , the goal is state 7 , state 8 . <p> Note that the notation indicates a set . <p> Problem formulation : we already know what the set of all possible states is . The operators are " move left " , " move right " , and " vacuum " . <p> Let 's draw the state space : <p> Multiple state problems <p> Suppose that the robot has no sensor that can tell it which room it is in and it does n't know where it is initially . <p> Then it must consider sets of possible states . <p> Notice that regardless of what the initial state is , the sequence of actions right , vacuum , left , vacuum ends up in a goal state . <p> Contingency problems <p> Suppose that the " vacuum " action sometimes actually deposits dirt on the carpet--but only if the carpet is already clean ! <p> Now right , vacuum , left , vacuum is NOT a correct plan , because one room might be clean originally , but them become dirty @ @ @ @ @ @ @ @ @ @ vacuum , vacuum does n't work either , and so on . <p> There does n't exist any FIXED plan that always works . <p> An agent for this environment MUST have a sensor and it must combine decision-making , sensing , and execution . This is called interleaving . <p> Exploration problems <p> So far we have assumed that the robot is ignorant of which rooms are dirty today , but that the robot knows how many rooms there are and what the effect of each available action is . <p> Suppose the robot is completely ignorant . Then it must take actions for the purpose of acquiring knowledge about their effects , NOT just for their contribution towards achieving a goal . <p> This is called " exploration " and the agent must do learning about the environment . <p> Measuring performance With any intelligent agent , we want it to find a ( good ) solution and not spend forever doing it . <p> The interesting quantities are , therefore , <p> the search cost--how long the agent takes to come up with the solution to the @ @ @ @ @ @ @ @ @ @ in the solution are . <p> The total cost of the solution is the sum of the above two quantities . <p> Choosing states and actions The secret of success for any intelligent agent is to choose state descriptions and actions well . The keyword is abstraction . <p> Abstraction means leaving out details about the world which are irrelevant such as the scenery or how hungry the driver is in the case of driving from Arad to Bucharest to get a new passport . <p> Saving less information about the world makes it much simpler to calculate what effect actions will have so the agent can solve the problem more quickly . <p> Cleverly chosen actions can also make the search go much faster . <p> For example , consider the 8-puzzle . <p> It is much better to let the actions be " move the blank up , down , left or right " than specifying actions that move the individual tiles or the tiles in a particular location because the branching factor of the search will be much less . <p> Consider another example , the 8-queens @ @ @ @ @ @ @ @ @ @ on a chessboard such that no queen threatens another . <p> A queen threatens another queen in the same row , column or diagonal . <p> In this example , the two queens on the corners are the only queens threatening each other . <p> The states and operators for this problem could be : <p> States : any arrangement of 0 to 8 queens on the board . <p> Operators : add a queen to any square . <p> This is a bad choice because there are 648 possible sequences to investigate . <p> A better formulation would be to choose a smarter operator : that <p> States : any arrangement of 0 to 8 queens on the board . <p> Operators : place a queen in the left-most empty column such that it is not attacked by any other queen . 
@@97506233 @1706233/ <h> Salmonids in Northern California Rivers , to be or not to be ? <p> Although not as fuzzy as the now extinct California Grizzly bear ( Ursus arctos californicus ) , salmonids ( fish of the salmonidae family like salmon and trout ) are arguably an equally emblematic animal of the state of California . Unfortunately , the past few centuries have been tough for the native salmonids populations of Northern California and they may soon share the same fate as the California Grizzly . Steelhead trout ( Oncorhynchus mykiss ) , Chinook salmon ( Oncorhynchus tshawytscha ) , and Coho Salmon ( Oncorhynchus kisutch ) populations have been long dependent on human intervention due to overfishing , river damming , and other human made changes to their habitat ( Moyle et al. , 2008 ) . Despite continued efforts to support populations of Chinook and Coho salmon through captive breeding and stricter fishing regulations , they face an uncertain future due to a changing climate . <p> Despite recent El Nino-Southern Oscilliation providing long-needed precipitation , California remains in the midst of one of the worst droughts @ @ @ @ @ @ @ @ @ @ sight in the parched areas of Central and Southern California . With the focus fixed on Central Valley farmers , it is easy to lose sight on vulnerable organisms that we see infrequently , particularly the fish of Northern California . The delta smelt ( Hypomesus transpacificus ) has been the center of attention over the past few decades , pitting farmers against environmentalists in the use of water for ecological purposes . However , it seems the battle to save the delta smelt may be lost as recent historical , record-breaking drought conditions have put them on track for extinction ( see National Geographic Article ) . The aquatic ecosystems home to Chinook salmon and steelhead trout in northern California have also been impacted , but their future in drying rivers and ponds is far more complex than simply water availability . <p> Chinook and Coho salmon and steelhead trout are sensitive to water conditions but they can exert some controls on their ecosystem . They prefer cold waters and will actively seek refuge from warm waters ( Nielsen et al 1994 ) . In addition to the temperature @ @ @ @ @ @ @ @ @ @ also impact the suitability of water bodies for salmonids . Dr. Mary Power at the University of California , Berkeley and her colleagues have been studying the aquatic ecosystems of the Eel River Critical Zone Observatory . Her research along with studies of others has shown that both winter and summer flows control river food chains , controlling the availability of food for steelhead trout ( Power et al. , 2008 ) . In years with bed scouring winter floods and normal summer baseflows , trout in the Eel River were well fed . The scour eliminated large armored insects ( grazing Caddisflies ) and released edible algae , which in turn fed fast-growing , soft-bodied insects ( like Mayflies ) that , unlike the armored Caddisflies , are good prey for fish . After drought winters without flood scour , many large armored grazing Caddisflies survive , mow down algae , and reduce edible insects for the trout . <p> With climate change predicted to change precipitation patterns in California , summer droughts may be the knockout punch for salmonids . " The worst case , however , is @ @ @ @ @ @ @ @ @ @ low flow summers prevent it from feeding fish . Neither fish nor edible algae can tolerate the hot stagnant conditions that are set up when severe drought and water extraction for marijuana cultivation are combined . " Dr. Power says . If edible algae bloom in early summer , but die as flows go below critical in late summer , cyanobacteria that tolerate the hot stagnant pools absorb nutrients the dying algae release , and proliferate ( Power et al 2015 ) . These blooms of potentially toxic cyanobacteria have been linked to a number of dog deaths in the Eel and Russian Rivers . To Dr. Power , this is a double whammy : " Its not salmon or no salmon , its salmon or toxic cyanobacteria " in the rivers of Northern California . <p> In spite of these problems , Chinook , Coho and Steelhead populations appear to be holding on ( Moyle , 2008 ) . The debate over water allotment still rages on with farmers and environmentalists struggling to find a happy agreement . Continued study of the salmonids in Northern California is needed to @ @ @ @ @ @ @ @ @ @ the way of the California Grizzly bear . <p> Have any questions swirling in your noodle about the rock , soil , water , fauna , or flora of the critical zone ? Send them our way at **25;2007;TOOLONG . <h> COMMENT ON " Adventures in the Critical Zone " <p> All comments are moderated . If you want to comment without logging in , select either the " Start/Join the discussion " box or a " Reply " link , then " Name " , and finally , " I 'd rather post as a guest " checkbox . <h> ABOUT THIS BLOG <p> General Disclaimer : Any opinions , findings , conclusions or recommendations presented in the above blog post are only those of the blog author and do not necessarily reflect the views of the U.S. CZO National Program or the National Science Foundation . For official information about NSF , visit www.nsf.gov. 
@@97506234 @1706234/ <h> A comparison of three methods of decellularization of pig corneas to reduce immunogenicity <p> AbstractAIM : To investigate whether decellularization using different techniques can reduce immunogenicity of the cornea , and to explore the decellularized cornea as a scaffold for cultured corneal endothelial cells ( CECs ) . Transplantation of decellularized porcine corneas increases graft transparency and survival for longer periods compared with fresh grafts . <p> METHODS : Six-month-old wild-type pig corneas were cut into 100-200 -m thickness , and then decellularized by three different methods : 1 ) 0.1% sodium dodecyl sulfate ( SDS ) ; 2 ) hypoxic nitrogen ( N2 ) ; and 3 ) hypertonic NaCl . Thickness and transparency were assessed visually . Fresh and decellularized corneas were stained with hematoxylin/eosin ( H&amp;E ) , and for the presence of galactose-+1,3-galactose ( Gal ) and N-glycolylneuraminic acid ( NeuGc , a nonGal antigen ) . Also , a human IgM/IgG binding assay was performed . Cultured porcine CECs were seeded on the surface of the decellularized cornea and examined after H&amp;E staining . <p> RESULTS : All three methods of decellularization reduced @ @ @ @ @ @ @ @ @ @ while the collagen structure remained preserved . No remaining nuclei stained positive for Gal or NeuGc , and expression of these oligosaccharides on collagen was also greatly decreased compared to expression on fresh corneas . Human IgM/IgG binding to decellularized corneal tissue was considerably reduced compared to fresh corneal tissue . The cultured CECs formed a confluent monolayer on the surface of decellularized tissue . <p> CONCLUSION : Though incomplete , the significant reduction in the cellular component of the decellularized cornea should be associated with a significantly reduced in vivo immune response compared to fresh corneas. 
@@97506235 @1706235/ <h> Find Topography Data <p> The map below shows the extent of datasets currently available via OpenTopography . These topographic data are available in a range of formats depending upon who acquired the data and what product types were delivered . The classes of data types available include : lidar point cloud data , standard digital elevation models ( DEMs ) and Google Earth imagery files . For lidar point cloud data , web-based tools are also available to process these data into custom DEMs . <p> Navigation of the Google Map below is performed using the zoom bar and navigation arrows in the upper left hand corner . To make a selection , click the " SELECT A REGION " button and draw a box on the map that corresponds to the portion of the lidar dataset that you are interested in . Results will appear below the map . Available data are shown as dots or polygons on the map color coded by data center . <p> Datasets listed below are hosted by OpenTopography and are available in point cloud format for download and processing ( e.g. @ @ @ @ @ @ @ @ @ @ data products such as raster and Google Earth Image overlays are also available . Click the button to the right of the dataset name to access the available data products . <p> Datasets listed below were contributed to OpenTopography by members of our community . These datasets are not hosted by OpenTopography , but are shown here as a service to our users . For each dataset basic metadata are shown . Click the button to the right of the dataset to access these data . OpenTopography is not responsible for the accessibility of these data . 
@@97506243 @1706243/ <h> Interactive Parallel Batch Jobs <h> Commands <p> In order to run interactive parallel batch jobs on TSCC , use the qsub -Icommand , which provides a login to the launch node as well as the PBSNODEFILE file with all nodes assigned to the interactive job . <p> Other qsub options can be used , such as those described by the man qsub command . <p> As with any job , the interactive job will wait in the queue until the specified number of nodes become available . Requesting fewer nodes and shorter wall clock times may reduce the wait time because the job can more easily backfill among larger jobs . <p> The showbf command gives information on available time slots . This provides an accurate prediction of when the submitted job will be allowed to run . <p> To run an interactive job with a wall clock limit of 30 minutes using two PDAF nodes and 32 processors per node : <p> qsub -I -q pdafm -l walltime=00:30:00 -l nodes=2:ppn=32 <h> Environment Modules <h> Managing Your Shell Environment <p> TSCC uses the Environment Modules package to control @ @ @ @ @ @ @ @ @ @ its common usage . You can learn more at the Modules home page . <h> Overview <p> The Environment Modules package provides for dynamic modification of a shell environment . Module commands set , change , or delete environment variables , typically in support of a particular application . They also let the user choose between different versions of the same software or different combinations of related codes . <p> For example , if the pgi module and openmpiib module are loaded and the user compiles with mpif90 , the generated code is compiled with the Portland Group Fortran 90 compiler and MPI libraries utilizing openmpi are linked . By unloading the openmpiib module , loading the mvapich2ib module , and compiling with mpif90 , the Portland compiler is used but linked with mvapich2 support . <h> Default Modules <p> Several modules that determine the default TSCC environment are loaded at login time . These include the intel and openmpiib modules to set the default compiler environment . Default modules are noted below in the complete list . <p> Note that the order in which modules are loaded is significant @ @ @ @ @ @ @ @ @ @ and subsequently the intel module is loaded , the intel compiler will be used . Also , some modules depend on others so may be loaded or unloaded as a consequence of another module command . For example , if intel and mvapich2ib modules are both loaded , running the command unload intel will automatically unload mvapich2ib . Subsequently issuing the load intel command will not automatically reload mvapich2ib . <p> Complete documentation is available in the module(1) and modulefile(4) manpages . <p> Note that Data Oasis storage is not backed up and files stored on this system may be lost or destroyed without recourse to restore them . Long-term file storage should be maintained in your $HOME directory or project storage . <h> Compiling MPI Codes <p> MPI source code should be recompiled for TSCC using the following default compiler commands : <p> Documentation is available in HTML and PDF formats in $MKLROOT/ .. /Documentation . <h> MKL Link Libraries <p> To link the MKL libraries , please refer to the Intel MKL Link Line Advisor Web page . This tool accepts inputs for several variables based on your @ @ @ @ @ @ @ @ @ @ <p> When using the output generated by this site , substitute the TSCC path of the Intel MKL for the value $MKLPATH in the generated script . That value is $MKLROOT/lib/em64t . <p> All third-party applications can be found in /opt , or view the complete description of TSCC software packages in the TSCC Quick Start Guide . <h> Compiling Parallel Codes <p> The default compiler/mpi stack combination is the Intel compiler ( ifort , icc ) and openmpiib . Other compilers and mpi variants may be accessed by loading the appropriate modules . The commands mpicc , mpiCC , mpif77 and mpif90 will access a particular compiler/mpi combination based on the module choices . Read about Modules to learn how TSCC manages compiler configurations . <h> Debugging with DDT <p> The following describes a basic setup of a DDT debugging session . The procedure diverges between steps 9 &amp; 10 , where the option to start a job from the queue using the debugger or to attach to an already running process is presented . <h> Step-by-Step Tutorial <h> Follow this procedure to learn debugging on TSCC using DDT @ @ @ @ @ @ @ @ @ @ -X option to ssh command ) <p> Run this command to set up your environment : <p> module load ddt <p> This is equivalent to putting this line in your . bashprofile file : <p> export **38;2034;TOOLONG <p> and then running this command to reload the current shell environment : <p> . /. bashprofile <p> Make sure your code is compiled with optimization turned off by compiling with -O0 ( that is capital letter " O " followed by number zero ) , and symbol table information enabled by compiling with the -g option <p> Run this command to start the DDT client : <p> /opt/ddt/bin/ddt <p> To start a debugging session , from the " Session " menu select " New Session " and then " Run " from the submenu . <p> In the " Run " window , enter the full path to the executable in the " Application " field and any command line arguments in the " Arguments " field . <p> In the " Run " window , click the " Change " button to bring up the Options window . On the " @ @ @ @ @ @ @ @ @ @ Implementation " , or select " generic " if you encounter a problem while debugging . Select " none " to debug a serial or non-mpi code . <p> If you are running an interactive debugging job or plan to attach to a running job , specify a hosts file in the " Attach hosts file " field and add host names to that file , one line per host . <p> To submit a job for debugging through the queue : <p> Still in the " Options " window , click on the " Job Submission " tab . <p> Check " Submit job through queue or configure own ' mpirun ' command " . <p> To use the predefined template for a PBS ( TORQUE ) job , click browse ( the folder icon ) and select the **26;2074;TOOLONG file . <p> In the " Submit command " field , enter qsub . Leave the " Regexp for job i 'd " field blank . In the " Cancel command " field , enter qdel . In the " Display command " field , enter qstat . <p> Check @ @ @ @ @ @ @ @ @ @ " 8 " in the " PROCPERNODETAG " field ( standard for the batch queue ) . <p> Click on the " Edit Queue Submission Parameters ... " button to bring up the " Queue Submission Parameters " window . <p> Enter the wall clock time limit ( in HH:MM:SS format ) and the queue name . If you are using mpi , enter the full path to mpirun . <p> Click " OK " to return to the Options window , then click OK in the Options window to return to the Run window . <p> In the Run window , select the number of nodes you want to allocate and the select the " Submit " button . <p> Wait for the jobs to start ... <p> Enjoy ! <p> To attach to a running job : <p> from the " Session " menu , select " New session " and then " Attach " from the submenu . <p> In the field " Filter for process names containing " enter the name of the executable ( just the name is sufficient , do not enter the full @ @ @ @ @ @ @ @ @ @ in the host file ( see step 9 ) , DDT will scan the specified hosts for processes with the given name and attempt to attach to them . If you have submitted a job to the queue , obtain the host list from ( for example ) checkjob &lt;job number&gt; . <h> Note : <h> Bundling Serial Jobs <h> How to submit multiple serial jobs with a single script <p> Occasionally , a group of serial jobs need to be run on TSCC . Rather than submit each job individually , they can be grouped together and submitted using a single batch script procedure such as the one described below . <h> Overview <p> Although it 's preferable to run parallel codes whenever possible , sometimes that is not cost-effective , or the tasks are simply not parallelizable . In that case , using a procedure like this can save time and effort by organizing multiple serial jobs into a single input file and submitting them all in one step . <p> The code for this process is given below in a very simple example that uses basic shell @ @ @ @ @ @ @ @ @ @ can easily be substituted for those commands and submitted using a modified version of these scripts and run from your own home directory . <p> Note that the /home filesystem on TSCC uses autofs . Under autofs , filesystems are not always visible to the ls command . If you cd to the/home/beta directory , for example , it will get mounted and become accessible . You can also reference it explicitly , e.g. ls /home/beta , to verify its availability . Autofs is used to minimize the number of mounts visible to active nodes . All users have their own filesystem for their home directory . <p> The components used in this example include : <p> submit.qsub ( batch script to run to submit the job ) <p> myscript.pl ( perl script invoked by batch job ) <p> jobs-list ( input to perl script with names of serial jobs ) <p> getid ( executable to obtain the processor number , called by perl script ) <p> Line 5 in the above script writes output like this to the file &lt;line-5&gt; . This output does not appear in the shared @ @ @ @ @ @ @ @ @ @ , 0 users , load average : 0.92 , 1.00 , 0.99 <h> Summary and Potential Other Uses <p> A modification of this procedure is available from TSCC User Support ( member-only list ) that matches the number of scripts to the number of processors , when more scripts are being run than processors are available . <p> It should also be possible to modify this script to run parallel jobs . Feel free to try it or ask support for help through the TSCC Discussion List . 
@@97506244 @1706244/ <p> This browser is either not Javascript enabled or has it turned off . This site will not function correctly without Javascript . <h> Redundancy in the Protein Data Bank <p> Statistics <p> The following table shows the number of non-redundant sequences as determined by blastclust at several levels of sequence identity . <p> Method <p> Description <p> # of Clusters <p> blast <p> 100% identity <p> 73060 <p> blast <p> 95% identity <p> 51057 <p> blast <p> 90% identity <p> 48484 <p> blast <p> 70% identity <p> 42612 <p> blast <p> 50% identity <p> 36504 <p> blast <p> 40% identity <p> 32269 <p> blast <p> 30% identity <p> 27440 <p> Notes on Blast Clustering <p> Blast clustering is performed with the following parameters ( example 95% ) : <p> -p T -b T -S 95 <p> The ' -b T ' parameter in BLASTClust means that the sequence identity threshold is enforced over both members of a sequence pair . <p> The ' -p T ' parameter means that both input sequences are protein sequences . <p> The ' -S 95 ' here : the percent identity threshold @ @ @ @ @ @ @ @ @ @ Note : BLASTClust uses the default parameter -L , which specifies the length coverage threshold for including in a cluster . It set to 0.9 by default . This means that two sequences need to have &gt;= 90% coverage in the alignment for clustering them together . <p> As the single worldwide repository for macromolecular structures , the Protein Data Bank holds a body of data that contains considerable redundancy in regard to both sequence and structure . We have incorporated into the query interface the ability to select a subset of structures from which similar sequences have been largely removed . In most cases , the selected subset will contain far fewer structures than the complete result set . However , the following caveats should be kept in mind : <p> Sequence similarity is defined on a chain basis , but results are returned on a structure basis . <p> Many structures in the PDB contain multiple protein chains , or even hybrids of DNA and protein chains . <p> Sequence similarity is only assessed for protein chains . <p> The relationship between sequence similarity and structure similarity is @ @ @ @ @ @ @ @ @ @ options available on the Structure Summary page under " External Links " ( in the left hand navigation menu ) in the " Structure Classification " section . <p> The primary purpose of this feature is to filter a list of likely highly similar structures to provide one or more representatives . Results may differ from other so-called non-redundant sets ( e.g. PDBSELECT Hobohm U. , and Sander C. , Protein Science , 3 : 522-524 , 1994 ) . <h> Algorithm for Removing Similar Sequences <p> The query implementation for removing similar sequences is based on pre-calculated clusters of protein chains . All protein chains of at least 20 amino acids are clustered by blastclust at 100% , 95% , 90% , 70% , 50% , 40% , and 30% sequence identity ( defined as number of identical residues out of total in the sequence alignment ) . <p> In each cluster , the chains are sorted ( i.e. ranked ) according to the following criteria ( in this order ) : 
@@97506245 @1706245/ 1450 @qwx861450 <h> SDSC/UCSD Team Uncovers Signaling Links to Glioblastoma Factor <p> Published July 21 , 2016 <p> Glioblastoma is a highly aggressive form of brain cancer with a survival rate generally less than 16 months . With no effective treatments available , a team including researchers from the San Diego Supercomputer Center ( SDSC ) at UC San Diego and John Wayne Cancer Institute at Providence Saint Johns Health Center sought to uncover several underlying molecular networks that may yield attractive therapeutic targets for this deadly disease . <p> The results , published in the July 16 online issue of Oncotarget , focus on a transcription factor called OLIG2 ( Oligodendrocyte lineage transcription factor ) , known to play a critical role in brain tumor development . Transcription factors control which genes are turned " on " or " off " at any given time . <p> " This study offers a new way of thinking about , and establishing priorities for , treatment options for glioblastoma , " said first author Igor Tsigelny , a research scientist with SDSC as well as at the UC San Diego @ @ @ @ @ @ @ @ @ @ <p> Specifically , the team created a series of molecular networks or " modules " , grouping together genes related to OLIG2 . Among other things , the researchers found that molecules in each module tended to organize themselves according to specific functions in various processes related to cancer : some turn cell functions on and off ; others direct cell proliferation and differentiation ; others signal the formation of blood vessels to tumors ; and so on . <p> " The results suggest that drug development efforts should be looked at from the perspective of signaling networks including critical hubs and identifying combination therapies to overcome resistance mechanisms , " said Santosh Kesari , the studys senior author and the Chair of Translational Neuro-Oncology and Neurotherapeutics at the John Wayne Cancer Institute and the Pacific Neuroscience Institute , both located in Santa Monica . <p> Like many cancer-forming genes , OLIG2 initially plays a role in the growth and function of " normal " cells and tissues , in this case working during specific developmental stages to create neural stem cells that form the central nervous system . This @ @ @ @ @ @ @ @ @ @ has been completed . But with brain cancer , there 's significant evidence that OLIG2 is switched back " on " , promoting the activity of cell growth and survival genes when they should not be , leading to fast-growing tumors . <p> Given this evidence , the researchers decided to identify OLIG2-related signaling pathways and genes responsible for its activity , using a systems biology approach to build a " hierarchical " gene network based on their relationships to other molecules . Genes with the largest number of interconnections or relationships were placed at the top of the hierarchy , with others having fewer connections falling underneath . <p> To create their network , the researchers -- who included Valentina Kouznetsova , associate project scientist at SDSC and UC San Diego ; and Nathan Lian , a student in the Research Experience for High School Students ( REHS ) summer program at SDSC -- turned to a computer program called VisANT , a web-based tool for mining data , used for biological pathway analysis and for querying the visualization of gene regulation and gene networks for glioblastoma . In addition @ @ @ @ @ @ @ @ @ @ to define high-level connections between groups of proteins , complexes , pathways , sub-networks , or simply " gene modules . " <p> " Essentially , VisANT has an intrinsic database of functional links between proteins . It helps to show a comprehensive picture of numerous interactions between all available genes in the gene network , grouping them based on connectivity with surrounding genes , " said Tsigelny . <p> " Because genes in these modules are working together and the modules are hierarchically integrated in biological networks , we call them coherent-gene modules ( CGM ) , " added Kesari . <p> After further refinement of genetic signaling networks with other software , the results showed that families of molecules interacting with OLIG2 in brain cancer form eight CGMs . <p> Aside from observing how molecules in each module tended to organize themselves according to specific functions in various processes related to cancer , the modules revealed " feedback loops " in signaling pathways involving OLIG2 that create conditions for the constant activation of proteins responsible for tumor growth and its rapid spread . <p> " Receptors included in @ @ @ @ @ @ @ @ @ @ said Kesari . <p> In addition , the research identified the gene in the hierarchy with the most connections as epidermal growth factor receptor ( EGFR ) ; followed by HSP90 , which stabilizes the number of proteins required for tumor growth , and CALM1 , regulators of cell signaling and neurotransmitter function ; and then KDM1A and NCOR1 , both epigenetic factors . <p> " There are treatments of EGFR overactivation , " Tsigelny said . " The KDM1A and NCOR1 are involved in epigenetic regulation and their activation has to be also addressed . So , the bottom line is that a drug combination has to take into account both . " <p> This latest research relates to a previous study published last year in which Valentina Kouznetsova and coauthors developed a computational strategy to search databases for 3D molecular structures for small molecules that might engage a hotspot between OLIG2 and partner proteins . With this approach , Tsigelny and Kesari identified a few molecules that would likely fit the OLIG2 interaction , and then tested a few for their ability to kill glioblastoma tumors . <p> @ @ @ @ @ @ @ @ @ @ loops " have to be interrupted with specific combinations of drugs , rather than a single therapeutic agent directed against one specific activity or pathway , " said Kesari . <p> Grants from American Brain Tumor Association Drug Discovery Grant ( in tribute to Francis X. Colden III ) , and Voices Against Brain Cancer to S. Kesari provided support for this research , in addition to the Chemical Computing Group and Chris Williams . <p> About SDSC <p> As an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community , including industry and academia . Cyberinfrastructure refers to an accessible , integrated network of computer-based resources and expertise , focused on , and health IT . SDSCs Comet joins the Centers data-intensive Gordon cluster , and are both part of the National Science Foundations XSEDE ( eXtreme Science and @ @ @ @ @ @ @ @ @ @ of integrated digital resources and services in the world . 
@@97506247 @1706247/ <h> NSG Portal Education : Multiple Sclerosis and NEURON <h> Understanding Multiple Sclerosis Through Simulation <h> The Study <p> Part 1 : A code was generated to model a single uniformly myelinated axon . Based on known facts about the propagation of action potentials , the following graph was predicted in which a decaying current travels to a node and is regenerated , as shown by the spike ( Figure 1 ) . <p> The code was loaded into the NEURON Simulator and a graph similar to that in Figure 1 was generated . <p> Part 2 : The original code was modified to demonstrate demyelination , resulting in conduction failure . <p> The following graph ( Figure 3 ) represents the failure of action potential propagation which was predicted from known biological facts about signal conduction failure . <p> The NEURON simulation produced a graph ( Figure 4 ) similar to the graph in Figure 3 . As the action potential reached the node , it was recharged and continued to move along the length of the axon . When partial demyelination occurred ( see next section to @ @ @ @ @ @ @ @ @ @ potential failed to regenerate itself causing a neurological disorder such as Multiple Sclerosis . <h> Test It on NEURON <p> In the second part of the study , it was stated that the original code was modified . Now we will discuss how to modify the code to demonstrate demyelination . Signal propagation through axons can be explained using simple RC Circuits . An RC circuit consists of a resistor ( circled in blue in fig.5 ) and a capacitor ( circled in green in fig.5 ) . When the switch circuit closes , current starts to flow . Charge begins to build up in the capacitor , and when the capacitor is fully charged current ceases to flow . Axons can be modeled in the same way . In a vertebrate axon , myelin decreases the membrane capacitance , allowing signals ( analogous to current in the circuit ) to flow down the axon . As the myelin degenerates , the membrane capacitance starts to rise and no longer allows signals to pass through . This theory can be tested on NEURON . Using the neuron model ( hoc @ @ @ @ @ @ @ @ @ @ changing the capacitance of the demyelinated section can demonstrate the expected results . <p> The red arrow indicates the direction of charge flow . As the charge flows , it begins to build up inside the capacitor ( i.e. charging a capacitor ) . However , once the capacitor is fully charged , current ceases to flow . <p> It can be observed that when an axon is perfectly myelinated ( i.e. almost zero membrane capacitance so in the hoc code cm= 0.005 in the myelinated and delmyelinated portions ) , the action potential moves along the entire length of the axon , transmitting the signal.The following figure demonstrates five time points along the length of the axon . The units on the x-axis are in microns ( the length of the axon ) and the units on the y-axis are in mV . The images show the action potential moving along the length of the axon . <p> As the demyelination begins to occur at t= 3 , the membrane capacitance begins to increase and the signal can not travel through the whole axon . ( In the hoc @ @ @ @ @ @ @ @ @ @ portion has been changed to cm=0.01 uF to obtain the following graphs , Figure 7 ) . The last graph shows that the signal does not make it though to the end of the axon , demonstrating conduction failure at 30000 microns . The following figure demonstrates four time points along the length of the axon . <p> As the demyelination continues , the membrane capacitance begins to increase and the signal can not travel through the whole axon . ( In the hoc code the capacitance of the demyelinated portion " demyel " portion has been changed to cm=0.015 uF to obtain the following graphs , Figure 7 ) . The graph at t = 4 shows that the signal does not make it though to the end of the axon , demonstrating conduction failure at 30000 microns . The following figure demonstrates four time points along the length of the axon . <p> As demeylination continues to increase in the second half of the axon , capacitace further increases . The graph at t = 2 indicates that amount of myelin degenerated in too high for an action @ @ @ @ @ @ @ @ @ @ half-way though the length of the axon . ( In the hoc code the capacitance of the demyelinated portion " demyel " has been changed to cm=0.02 uF to obtain the following graphs , Figure 8 ) . The following figure demonstrates three time points along the length of the axon. 
@@97506255 @1706255/ <h> Poseview Image of Y01 in 4XP9 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CLR in 4XP9 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MAL in 4XP9 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NAG in 4XP9 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MPO in 4XP9 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of P4G in 4XP9 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of 1WE in 4XP9 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CL in 4XP9 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NA in 4XP9 <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506256 @1706256/ <h> Modern Data Science Academy <p> Data analytics is not new . Businesses of all sizes and across all industries have been using data to make decisions as long as there has been data to use . However , with the exponential rise in the volume of data and the increased accessibility of sophisticated data analysis tools , businesses are now better able to leverage data to drive strategies , processes , and products and to support accurate and impactful decision-making . To stay competitive in this increasingly data-driven climate , businesses and individuals across industries must keep pace with ever-evolving data analysis tools and techniques . <p> The Modern Data Science Academy provides workshops on current data science topics taught by leading SDSC researchers and practitioners . Through hands-on training , using state-of-the-art systems , individuals and corporations will acquire the tools and techniques needed to start making better data-driven decisions today . Projects and case studies will showcase how to apply techniques and skills to real-world data . <h> By the Numbers <p> According to Indeed , Data Mining salaries for job postings nationwide are 68% higher @ @ @ @ @ @ @ @ @ @ By leveraging data analytics , Kroger has added over $12 billion in incremental revenue , UPS has saved over 39 million gallons of fuel and avoided driving 364 million unnecessary miles , and Sprint has increased their network capacity by 90% . Learn more <h> In the News <p> Modern Data Science and Evolution of BIModern big data discovery tools enable all employees to access the data , streamlining the data prep process , and allowing data scientists to spend more time on advanced analytics . Read more <p> How Big Data Is Transforming The Healthcare SectorBig Data will leave no sector untouched as it continues to change the way we think about everything from sales to human resources , and medicine and healthcare are no different . Read more <p> Using Embedded Analytics to Transform Your BusinessSelf-service analytics can bring significant benefits to businesses , as it enables users of all roles and skill sets to access and analyze data . However , it 's not enough to just provide easy access to data analytics . Read more <p> The Modern Data Science Academy workshops are offered to groups @ @ @ @ @ @ @ @ @ @ San Diego Supercomputer Center or at your own facility . Instruction is provided by both faculty and industry experts . <h> Machine Learning Essentials I Workshop <p> Explore the tools and techniques used to explore , analyze , and leverage data to construct data mining solutions to scientific and business problems . <h> NoSQL Databases I Workshop <p> Learn the principles and practices of NoSQL database systems and , through lectures and hands-on exercises , will present several different types of NoSQL systems used today . <h> Big Data Processing with Apache Spark <p> Apache Spark is growing as the standard platform for Big Data processing on Cloud infrastructure , thanks to its speed , flexibility and ease of use . The goal of this course is to teach the basis of distributed computing and introduce the Spark Python interface . <h> Graph Analytics <p> Graph Analytics is a rapidly developing area where a combination of graph-theoretic , statistical , and database techniques are applied to model , store , retrieve , and perform analyses on graph-structured data . The Graph Analytics course offers a broad overview as well as insight into specific analytical techniques . 
@@97506257 @1706257/ <h> UC San Diego 's Strategic Plan <h> Goal 1 <p> Admitted students begin their transition to college by learning about UC San Diego at Triton Day . <p> Delivering an educational and overall experience that develops students who are capable of solving problems , leading , and innovating in a diverse and interconnected world <h> Strategy 1 <p> Provide coordinated and comprehensive academic , professional , and career advising across all colleges , departments , and units . <p> UC San Diego provides extensive academic , professional , and career advising through colleges , departments , programs , and units such as the Career Services Center . We are committed to an advising system that recognizes the individuality of each student and provides the critical support , guidance , and opportunities a student needs to thrive . <p> Ironically , our success at creating a broad array of advising services has given rise to unintended challenges . Some students , confronted by a long menu of options , do not know where to start or whom to ask for specific kinds of advice . Others may not learn @ @ @ @ @ @ @ @ @ @ been missed . To meet the needs of our students , we are making academic and career advising more integrated and accessible . We are investing $3 million to support undergraduate education resources and services , such as tutoring , advising , teaching assistants , and upgrades of undergraduate laboratories . In addition , we have made a two-year commitment to significantly increase funding for the Office of Academic Support and Instructional Services ( OASIS ) by 50 percent , which provides free tutoring , as well as peer counseling and mentoring ; the number of students able to attend the OASIS Summer Bridge transition program increased to 160 students , up from 130 . We are also enhancing student health and well-being services by improving student-to-counselor ratios and reducing student wait times . <p> Access : Provide expanded outreach services and professional development to improve local pathways to higher education . <p> College advising begins long before matriculation . To ensure the widest possible access to UC San Diego , K " 12 students must be academically prepared for college and understand and appreciate the value of a college @ @ @ @ @ @ @ @ @ @ educational partners , we will provide both outreach services and teacher professional development to assess and improve local pathways to higher education . <p> Transitions : Help students navigate their early experience on campus . <p> For most students the transition from high school , community college , or life abroad to UC San Diego is complex . Many of them are overwhelmed by the possibilities they confront , the rules they must navigate , and the cultural and psychological adjustments they face . We will establish a first-year transition program to help them adjust to classroom life , select a major , find internships , learn how to conduct research , manage time , maintain academic integrity , and build community . <p> To plan , track , and record the wide range of academic and co-curricular experiences that constitute a complete education , we are creating a record system that integrates an enhanced electronic transcript , a co-curricular record , and an electronic portfolio . The record will also provide academic and co-curricular road maps with timelines for suggested activities and learning opportunities . <p> Careers : Invest @ @ @ @ @ @ @ @ @ @ between the Career Services Center , and Alumni and Community Engagement to help students network . <p> To launch a successful career , students need help finding jobs , identifying internship opportunities , and making connections . Alumni are a rich source of academic and professional mentors , internships , and employment opportunities . By leveraging the power of more than 160,000 UC San Diego alumni , the Career Services/Alumni and Community Engagement partnership is helping students to define and achieve their career goals , and to inspire alumni participation in the educational , career , and professional development of our students . <h> Strategy 2 <p> Rethink curriculum and pedagogy to improve retention and graduation rates and increase student and faculty engagement . <p> To sustain our leadership in higher education , we must transform the ways we approach both teaching and learning . Drawing from the current best thinking about educational strategies and performance metrics , we will strengthen our support for the intellectual , academic , cognitive , and social development of our undergraduate and graduate students , increase our investment in mentoring and leadership opportunities for @ @ @ @ @ @ @ @ @ @ to the latest research on teaching and learning . <p> Curriculum : Review graduation rate data and curriculum requirements . <p> All departments , colleges , and majors are reviewing their graduation rates and curriculum with an eye towards taking advantage of new knowledge about teaching and learning , responding to changing disciplinary needs , and improving time-to-degree . <p> Teaching and learning centers : Create teaching and learning centers based on new knowledge and learning practices that equip faculty and coordinate programs and services across the campus . <p> To promote a culture of educational engagement , improve visibility and access , and enhance the opportunities for undergraduates to interact with faculty , these centers will coordinate programs and services that are currently distributed across the university . We will hire an executive director of the Teaching and Learning Commons , which is the umbrella for the new centers . We will also establish an Assessment Center to keep track of all the interventions . <p> Educational technology : Ensure that faculty and teaching assistants have the resources , information , and training needed to incorporate the appropriate educational @ @ @ @ @ @ @ @ @ @ in the twenty-first century . <p> We are establishing real and virtual next-generation classrooms and laboratories that provide students with interactive and immersive experiences . Over the next three years , we will invest $4 million a year to expand instructional technology , upgrade classrooms and computer labs , and provide faculty support to improve teaching and student learning . In addition , we are allocating more than $14 million to renovate teaching laboratories , including $11 million to renovate undergraduate biology and chemistry laboratories . Faculty will have an opportunity to view successful technology in action , learn about critical technical details , and consult with outside experts and with one another regarding the best pedagogical methods . <p> Writing : Assess and expand the role of writing to advance students ' academic and career goals . <p> To strengthen student learning and engagement , we are exploring ways to integrate writing into the curriculum . For undergraduates , focused writing that reflects engagement in educational activities such as internships , study abroad , and research projects can be life changing . For graduate students , developing strong academic @ @ @ @ @ @ @ @ @ @ period after advancement to candidacy . <h> Strategy 3 <p> Strengthen the connection between academic and high-impact , cocurricular experiences . <p> The campus faces the ongoing challenge of linking formal undergraduate classroom instruction with relevant real-world experiences that enhance the learning process . Experiential learning opportunities outside the classroom for UC San Diego students include placement in cutting-edge laboratories and research facilities , opportunities to live and learn in other countries , and participation in public service programs at home and abroad . Participants in such cocurricular activities have higher retention rates and shortened time-to-degree . After graduation , these students are more attractive to employers and command higher incomes . To increase experiential learning opportunities for students and postdoctoral scholars , we have created the Frontiers of Innovation Scholars Program . We are investing $7.2 million over two years in two hundred seed grants to support multidisciplinary research fellowships for undergraduate students , doctoral students , and some postdoctoral scholars working with faculty mentors in the lab . The goal is to promote diverse research experiences and partnerships across fields , and to expose students to research projects @ @ @ @ @ @ @ @ @ @ <p> Transferrable skills : Establish specific expectations for transferrable skills in all departments , colleges , and major programs . <p> We are guiding students as they plan their education by mapping these transferrable skills to courses and experiences in which they may be acquired . <p> Experiential learning portal : Develop a comprehensive university portal so that students and experience providers can be matched effectively . <p> We will enhance the existing interactive Undergraduate Research Portal to become an online clearinghouse for research , study abroad , internships , and community service . 
@@97506261 @1706261/ <h> Find Topography Data <p> The map below shows the extent of datasets currently available via OpenTopography . These topographic data are available in a range of formats depending upon who acquired the data and what product types were delivered . The classes of data types available include : lidar point cloud data , standard digital elevation models ( DEMs ) and Google Earth imagery files . For lidar point cloud data , web-based tools are also available to process these data into custom DEMs . <p> Navigation of the Google Map below is performed using the zoom bar and navigation arrows in the upper left hand corner . To make a selection , click the " SELECT A REGION " button and draw a box on the map that corresponds to the portion of the lidar dataset that you are interested in . Results will appear below the map . Available data are shown as dots or polygons on the map color coded by data center . <p> Datasets listed below are hosted by OpenTopography and are available in point cloud format for download and processing ( e.g. @ @ @ @ @ @ @ @ @ @ data products such as raster and Google Earth Image overlays are also available . Click the button to the right of the dataset name to access the available data products . <p> Datasets listed below were contributed to OpenTopography by members of our community . These datasets are not hosted by OpenTopography , but are shown here as a service to our users . For each dataset basic metadata are shown . Click the button to the right of the dataset to access these data . OpenTopography is not responsible for the accessibility of these data . 
@@97506270 @1706270/ <h> Press Kit <h> SDSCs Evolving Mission <p> Founded by the National Science Foundation ( NSF ) almost three decades ago as one of the nations first academic supercomputer centers , SDSCs mission has since evolved and expanded , in part to include the creation and fostering of collaborations across the University of California system . As part of that effort , SDSC has worked to align itself with three UC principles when it comes to UC-wide research investments : <p> Act as one system of multiple campuses to enhance UCs influence and advantage <p> SDSCs portfolio of high-performance computing resources , along with its big data expertise and outreach programs , are all essential ingredients to stimulating collaboration across the UC system , whether it is finding ways to better predict the impact of earthquakes and wildfires or developing new drugs to combat debilitating diseases . <p> Confronting such societal challenges requires collaboration among researchers who have the scientific vision , technological skill , and innovative approaches to advance discovery . To that end , in 2014 SDSC launched an initiative called UC@SDSC an engagement strategy that highlights @ @ @ @ @ @ @ @ @ @ resources and technical expertise as a valuable asset to the entire UC system . While SDSC already has numerous engagement initiatives underway , here are some highlights , aligned with those three themes : <h> Collaboration <h> LHC@UC <p> Frank Wnrthwein , a noted expert in high-energy particle physics and advanced computation , recently joined SDSC as head of the Distributed High-Throughput Computing Group . Wnrthweins mission is to develop and deploy a high-capacity , shared data and compute platform anchored at SDSC that will serve the entire UC system . The initiative is called LHC@UC because in 2013 , Wnrthwein and his team used SDSCs Gordon supercomputer to provide auxiliary computing capacity to the Open Science Grid by processing massive data sets generated by the Compact Muon Solenoid ( CMS ) , one of two particle detectors at the Large Hadron Collider ( LHC ) . One of the key benefits of such a UC network is that individual PIs across all UC campuses will have direct access to SDSCs expertise and resources from their home institutions . <h> UC Collaborative Research Opportunity ( CRO ) Activity <p> This @ @ @ @ @ @ @ @ @ @ to apply for a CRO mini-grant to support collaborative work , in turn leading to an extramural grant proposal . In addition to the strategic top down CRO LHC@UC mentioned above , SDSC currently has two PI-initiated ( bottoms up ) CRO awards : SDSC Computational Scientist Yifeng Cui is collaborating on a seismic simulations proposal with UCR ; and Ross Walker , an assistant research professor at SDSC , is coordinating a UCSD/UCR/UCI proposal for a GPU cluster to the NIH . <h> Innovation <h> Made in UC Initiative <p> This project focuses on the cataloguing of Made in UC software and technologies that specifically relate to data science and computational science . We are partnering with UC researchers on benchmarking various technologies and extending software so that they can be run on different types of clusters and be able to scale to big data problems . This project has already spawned several collaboration brainstorming sessions between SDSC PIs and researchers at UC Irvine , UC Merced , UC Riverside , and UC Santa Cruz . <p> SDSC has made significant contributions to the Pacific Research Platform , which @ @ @ @ @ @ @ @ @ @ well as other universities across California and the western region . The immediate objective is to develop a " regional Science DMZ " across the regional partners that opens up the capabilities of high-performance networking to advance data transfers and scientific collaborations for researchers across all scientific domains . This initiative is receiving high visibility across all campuses at the CIO and VCR levels . Working with Larry Smarr , the IDI Chair and Director of CalIT2 , SDSC staff are active contributors , and participated in a recent high-profile demonstration of this capability at the CENIC 2015 conference earlier this month . <h> Education <h> SDSC Summer Institute <p> The theme for the SDSC Summer Institute week-long workshops is " HPC for the Long Tail of Science . " This is an ideal program for UC researchers who would like to become more familiar with advanced computation as it relates to data management , running jobs on SDSC resources , reproducibility , database systems , and other techniques for turning data into knowledge . Participants will receive hands-on training using SDSCs Gordon and Comet supercomputers . The program is @ @ @ @ @ @ @ @ @ @ - especially current and potential users of SDSC 's data-intensive resources . Familiarity with UNIX/Linux environments is essential . Some programming experience in C/C++ , Fortran , Java , R , Python , Perl , MATLAB or other languages is preferred . . <h> UCSD Research Experience for High School Students ( REHS ) <p> Since 2010 , SDSC has sponsored research internships for talented high school students from all over San Diego county . These internships provide opportunities for qualified students to work side-by-side with SDSC staff and researchers , including some of the world 's leading computational scientists , helping them solve some of the most challenging and important problems facing us today . Students spend eight weeks during the summer at SDSC applying existing skills and learning new ones doing real science in an exciting and inspiring academic environment . Students find great reward from the experience knowing they are making important contributions to society while at the same time learning many of the life skills they will need to excel in their future endeavors both in college and in their future careers . Their internship experience @ @ @ @ @ @ @ @ @ @ the broader SDSC community in the form of a professional scientific poster . To date , SDSC has sponsored internships for more than 300 high school students coming from a wide range of backgrounds and experiences . Learn more about the REHS Program and other K-12 education opportunities . <h> Byte Basics - the anatomy of a byte : <p> Gigabyte : A billion bytes ; equal to information contained in a stack of books almost three stories high . <p> Terabyte : A trillion bytes ; about equal to the information printed on paper made from 50,000 trees . <p> Petabyte : A quadrillion bytes . It would take 1,900 years to listen to a petabyte 's worth of songs if you had a large enough MP3 player . <p> Exabyte : One quintillion bytes ; every word ever spoken by humans could be stored on five exabytes . <p> Zettabtye : One sextillion bytes ; enough data to fill a stack of DVDs reaching halfway to Mars . <h> Rating a supercomputer 's performance : <p> Megaflops : A million floating point operations per second . The original @ @ @ @ @ @ @ @ @ @ : A billion floating point operations per second . Today 's personal computers are capable of gigaflops performance . <p> Teraflops : A trillion ( 1012 ) floating point operations per second . Most of today 's supercomputers are capable of teraflops performance . <p> Petaflops : A quadrillion ( 1015 ) floating point operations per second . The latest supercomputer barrier to be broken . The fastest systems can now achieve about 2.5 petaflops . <p> Exaflops : A quintillion ( 1018 ) floating point operations per second , and the new frontier for supercomputers , provided we can make exascale supercomputers 100 to 1,000 times as energy-efficient as today 's fastest machines . 
@@97506274 @1706274/ <p> Q : What is the difference between NSGportal and NSG-R ? NSGportal is a simple online portal that the user can login to submit their simulation code and retrieve results . The same functionality if accessed programmatically using RESTful services is called NSG-R . Please note that the underlying infrastructure is different for both access points.If you run/save output through the web portal , you will not be able to access the same data using NSG-R and vice versa . <p> Q : What is the cost to use NSG ? A : Free ! ( for all academic users ) . Industry and for-profit organizations please contact us for usage restrictions . <p> Q : How do I get an account for NSG ? A : Please go to the NSG registration to fill out the form . Once approved you will have an account to access both NSGportal and NSG-R services . We recommend users to choose the best access mechanism according to their comfort level and needs . <p> Q : What software applications are available through the NSG ? A : The list of @ @ @ @ @ @ @ @ @ @ code written in Python . Commonly used python modules are made available . Analysis tools such as R , Octave are also available through our backend HPC resources.We are always interested to know what other neuronal simulation or analysis tools users might be interested in . Please nsghelp@sdsc.edu <p> Q : I need you to install a software , what is the procedure ? A : Please contact us at nsghelp@sdsc.edu for any new software or features request . <p> Q : How should I upload my input file ? A : NSG supports only zipped file format . Please compress your directory and upload the input file . The zip file should have only one directory ( containing the model code ) in the top level . In unix or mac , once you have a directory with your model , use tar and gzip to create a compressed tar file : cd &lt;model directory&gt; cd .. zip -r input &lt;model directory&gt; <p> Q : What is the largest number of nodes and cores I can request ? A : It depends on the resource you choose to run @ @ @ @ @ @ @ @ @ @ a maximum of 72 nodes with 24 cores each . On Stampede , we allow a maximum of 256 nodes with 16 cores each . But as usage requirements grow , we will increase these limits <p> Q : What is the longest time I may request ? A : 48 hours ( but based on users ' requirements we will increase this time limit ) <p> Q : I am an user from the United States and have my own allocation on NSF 's XSEDE high performance resources . Can I still use neuronal simulation codes via NSG and charge to my own allocation ? A : Yes . Please email nsghelp@sdsc.edu to discuss this and use this feature . <p> Q : I need to do parameter sweep study on my model . How can I do it on NSG ? A : Bundled job submission is available for Brian . Please create a file called jobs-list and write your executable command line options , one per line Eg. jobs-list : python one.py 1 2 python one.py 4 5 python one.py 5 7 ( if you have question @ @ @ @ @ @ @ @ @ @ job submission only available for Brian ? A : Yes currently it is only available for Brian . We are looking into providing bundled job submission for other neuronal simulation tools also . If you have request for this , please email nsghelp@sdsc.edu <p> Q : I am teaching a computational neuroscience course ( or workshop ) . Can my students use NSG for the class ( or workshop ) ? A : Yes . Please email nsghelp@sdsc.edu well ahead of the start date of the class . 
@@97506275 @1706275/ 1450 @qwx861450 <h> Program Helps High School Students MAP their Academic Future <p> Published June 27 , 2016 <p> The San Diego Supercomputer Center ( SDSC ) at the University of California San Diego , in collaboration with the UC San Diego Division of Health Sciences , is preparing to launch the second year of a new mentoring program designed to provide a pathway for high school students to gain access to experts in their field of interest . <p> The first phase of the Mentor Assistance Program ( MAP ) , co-founded by Ange Mason , SDSCs education program manager , and Kellie Church , assistant professor in the Department of Reproductive Medicine within UC San Diegos School of Medicine , was recently celebrated at a symposium at SDSC . The event drew almost 300 guests , including school district administrators , principals , and teachers as well as parents and family members of students . The first phase of the MAP program ran from November 2015 through May of this year . <p> MAP is focused on creating mentoring relationships that will enhance students desire to learn @ @ @ @ @ @ @ @ @ @ fields , including science , technology , engineering , and mathematics , or what 's referred to as STEM . During its inaugural year , the MAP program included 77 students from 33 high schools who were accepted out of 225 applications , 34 mentors from 24 UC San Diego campus departments , and 65 students presented 48 projects at the symposium . <p> In late 2014 , SDSCs Mason and Health Sciences Church began discussing the feasibility of a long-term mentoring program for high school students that would give underserved students initial access to UC San Diego researchers , thereby giving them a way to pursue a career of their choice . One goal was to ensure that the program provided a good degree of flexibility for faculty , post docs , and staff as mentors , who typically have very busy schedules . Another was to make the program a rewarding experience for high school students across San Diego County . <p> " We carefully designed this program so the mentors could decide their own level of involvement with the students , and provide alternatives such as mentoring via @ @ @ @ @ @ @ @ @ @ visits , " said SDSCs Mason . " As a culmination of all of our work to date , the recent symposium was one of the proudest moments of my professional career . Some 90 percent of the students said they would return if given the opportunity , and half of them were offered summer internships , giving them the opportunity to continue their research experience throughout the summer . " <p> " The impact of the MAP program became utterly clear at the Symposium where not only students , but parents , teachers , principals , and even superintendents , repeatedly stated that this program has made a significant impact in students lives , " said Church . <p> " The MAP program was simply phenomenal , " said participant Zoe Smith , who attends one of San Diegos High Tech High schools . " Along with learning computer science content , I was able to communicate with different people in order to work toward a finished project . My mentor was awesome , my teammate was great , and our final product was a success . I could @ @ @ @ @ @ @ @ @ @ Daniel Shevchuk , who attends Del Norte High School , part of the Poway Unified School District , worked with UC San Diego Professor Christine Hunefeldt in the universitys History Department . He and fellow interns AJ Maloney and Tyler Satoda studied the European exploration of the Amazon Rainforest . " I 've never listened to deeper and more meaningful conversations than in this internship , " said Shevchuk . " Thank you MAP ! " <p> The second year of the MAP program was announced to UC San Diego faculty , postdocs and staff earlier this month . Mentors interested in participating have until July 1 to file their applications . The MAP application process opens to high school students in grades 11 and 12 on September 1 , with a deadline of September 30 , 2016 . <p> Almost all of the UC San Diego mentors are returning to participate in MAPs next program , which will run from November 2016 to May 2017 , said Mason . " We look forward to welcoming them back to help us give our students an amazing experience as they look @ @ @ @ @ @ @ @ @ @ choices . " <p> The symposium event included a keynote address by Karen Flammer , SDSCs Education Director and co-founder of Sally Ride Science . " As educators , we also know that the most successful people tend to be life-long learners who build multiple skill sets , such as combining expertise in science , the arts , history , and sports , " said Flammer . " Congratulations to all the students and mentors for their exceptional efforts this year , as we kick off MAPs second year in the coming months . " <p> Full details on becoming a MAP mentor and applications for download can be accessed here . Information specific to students will be posted on August 1 , 2016 . <p> About SDSC <p> As an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community , including industry and academia . Cyberinfrastructure refers to an accessible , integrated network of computer-based resources and expertise , focused on , and health IT . SDSCs Comet joins the Centers data-intensive Gordon cluster , and are both part of the National Science Foundations XSEDE ( eXtreme Science and Engineering Discovery Environment ) program , the most advanced collection of integrated digital resources and services in the world . 
@@97506277 @1706277/ <h> Poseview Image of CLR in 4XP5 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of 42F in 4XP5 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MAL in 4XP5 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of P4G in 4XP5 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CL in 4XP5 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NA in 4XP5 <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506281 @1706281/ 1450 @qwx861450 <h> KC Claffy among " 10 Women to Know in Networking / Communications " <p> KC Claffy , principal investigator and founding director of the Center for Applied Internet Data Analysis ( CAIDA ) at the San Diego Supercomputer Center ( SDSC ) , has been named to the second annual " 10 Women in **25;2102;TOOLONG That You Should Know " list . <p> Now in its second year , the list is compiled and coordinated by N2 Women ( Networking/Networking Women ) , a discipline-specific community for researchers in the communications and networking research fields . The organizations main goal is to foster connections among under-represented women in computer networking and related research fields . The full list of this years award recipients can be found here . <p> Nominations are solicited both from the N2 Women community as well as through several mailing lists related to networking and communications . More than 150 people from around the world submitted nominations , resulting in over 140 distinct names of accomplished women in the field , according to the organization . <p> A committee of five N2 @ @ @ @ @ @ @ @ @ @ Many people from around the world submitted one or more nominations for this list , and it was very difficult to choose only 10 amazing women , " said Oana Iova , a post-doc in the D3S research group with the Department of Information Engineering and Computer Science ( DISI ) at theUniversity of Trento , Italy , and the awards co-chair who led the nomination and selection processes this year . " We focused on women who have had a major impact in networking and/or communications . We also wanted a list that reflected presented our diversity , and specifically the diversity in the area of **25;2129;TOOLONG . " <p> " I am honored to join such a distinguished group on this year 's N2 Women 's list , " said Claffy , who founded CAIDA in 1997 as a collaboration among commercial , government , and academic research sectors to promote greater cooperation in the engineering and maintenance of a robust , scalable global internet infrastructure . " I encourage other women working in networking and communications to attend or help organize an N2 Women event at their @ @ @ @ @ @ @ @ @ @ workshop . " <p> Claffy also is an adjunct professor of Computer Science and Engineering ( CSE ) at UC San Diego , and SDSC is an Organized Research Unit of the university . Today , CAIDAs research interests include internet cartography , or detailed analyses of the changing nature of the internet 's topology , routing and traffic dynamics . CAIDA also investigates the implications of these changes on network science , architecture , infrastructure security and stability , and public policy . <p> Earlier this year CAIDA was awarded a $1.4 million grant from the U.S. Department of Homeland Security to demonstrate and illuminate structural and dynamic aspects of the internet infrastructure relevant to cybersecurity vulnerabilities , including macroscopic stability and resiliency analyses , grey markets for IPv4 addressing resources , and on-demand router-level topology inference . <p> Claffy received the 2015 IEEE ( Institute of Electrical and Electronics Engineers ) Internet Award for her " seminal contributions to the field of internet measurement , including security and network data analysis , and for distinguished leadership in and service to the internet community by providing open-access data and @ @ @ @ @ @ @ @ @ @ institute . <p> About N2 Women <p> The goal of Networking Networking Women ( N2 Women ) is to foster connections among the under-represented women in computer networking and related research fields . N2 Women is an ACM SIGMOBILE program that is supported by the IEEE Communications Society , Microsoft Research , HP Labs , Google , and the IEEE Computer Society ( CS ) Technical Committee on Computer Communications ( TCCC ) . <p> About SDSC <p> As an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community , including industry and academia . Cyberinfrastructure refers to an accessible , integrated network of computer-based resources and expertise , focused on , and health IT . SDSCs Comet joins the Centers data-intensive Gordon cluster , and are both part of the National Science Foundations XSEDE ( Extreme Science and Engineering Discovery Environment ) program . 
@@97506283 @1706283/ <h> Events <p> 7/9/17 - 7/13/17PEARC17 ( Practice and Experience in Advanced Research Computing ) Conference An evolution of the annual XSEDE conferences , the PEARC conference series is being ushered in with support from many organizations . In addition to XSEDE , supporters include the Advancing Research Computing on Campuses : Best Practices Workshop ( ARCC ) , the Science Gateways Community Institute ( SGCI ) , the Campus Research Computing Consortium ( CaRC ) , the ACI-REF consortium , the Blue Waters project , ESnet , the Open Science Grid , Compute Canada , the EGI Foundation , the Coalition for Academic Scientific Computation ( CASC ) , and Internet2 . Poster submissions are due May 1 . New Orleans , LA <p> 7/25/17 - 7/28/17Earth Science Information Partners ( ESIP ) ESIP meetings are a member-led mix of plenary talks , breakout sessions , poster presentations , technical workshops , and networking opportunities that encourage discussion about emerging topics in earth science data while providing exposure to new technologies . Attendees include earth science data and information technology practitioners ; researchers representing a variety of scientific @ @ @ @ @ @ @ @ @ @ earth , ecology , data and social sciences ; science educators ; and anyone working in science and technology-related fields interested in advancing earth science information best practices . ESIP is a partner in the West Big Data Innovation Hub ( WBDIH ) and the National Data Service ( NDS ) . Bloomington , IN <p> 7/31/17 - 8/4/17SDSC High Performance Computing Summer InstituteThe Summer Institute provides attendees with an overview of topics in High Performance Computing and Data Science to accelerate their learning process through highly interactive classes with hands-on tutorials on the Comet Supercomputer . The program is aimed at researchers in academia and industry , especially in domains not traditionally engaged in supercomputing . Accepting applications through May 5 , 2017 . SDSC at UC San Diego <p> 9/13/17 - 9/15/17MetroLab SummitJoin the West Big Data Innovation Hub ( WBDIH ) at the MetroLab Network Annual Summit -- a national network of 40 city-university partnerships focused on urban innovation . The Summit will bring together leaders from local government , universities , industry and non-profit and is an opportunity to share , discuss , and present on @ @ @ @ @ @ @ @ @ @ local government.Atlanta , GA 
@@97506284 @1706284/ <p> As an instance owner on the host running db2 , issue the following command <p> $ db2start <p> Stopping the instance <p> $ db2stop <p> Connect to the database as instance owner <p> $ db2 <p> as a user of the database : <p> $source instance/sqllib/db2cshrc ( csh users ) <p> $ . **26;2156;TOOLONG ( sh users ) <p> $ db2 connect to databasename <p> Create a table <p> $ db2-&gt; create table employee <p> ( I 'd SMALLINT NOT NULL , <p> NAME VARCHAR(9) , <p> DEPT SMALLINT CHECK ( DEPT BETWEEN 10 AND 100 ) , <p> JOB CHAR(5) CHECK ( JOB IN ( ' Sales ' , ' Mgr ' , ' Clerk ' ) ) , <p> HIREDATE DATE , <p> SALARY DECIMAL(7,2) , <p> COMM DECIMAL(7,2) , <p> PRIMARY KEY ( I 'd ) , <p> CONSTRAINT YEARSAL CHECK ( YEAR(HIREDATE) &gt; 1986 OR SALARY &gt; 40500 ) ) <p> A simple version : <p> db2-&gt; create table employee ( Empno smallint , Name varchar(30) ) <p> Create a schema <p> If a user has SYSADM or DBADM authority , then the user can create @ @ @ @ @ @ @ @ @ @ is created , IMPLICITSCHEMA authority is granted to PUBLIC ( that is , to all users ) . The following example creates a schema for an individual user with the authorization I 'd ' joe ' <p> CREATE SCHEMA joeschma AUTHORIZATION joe <p> Create an alias <p> The following SQL statement creates an alias WORKERS for the EMPLOYEE table : <p> CREATE ALIAS WORKERS FOR EMPLOYEE <p> You do not require special authority to create an alias , unless the alias is in a schema other than the one owned by your current authorization I 'd , in which case DBADM authority is required . <p> Create an Index : <p> The physical storage of rows in a base table is not ordered . When a row is inserted , it is placed in the most convenient storage location that can accommodate it . When searching for rows of a table that meet a particular selection condition and the table has no indexes , the entire table is scanned . An index optimizes data retrieval without performing a lengthy sequential search . The following SQL statement creates a <p> non-unique index called @ @ @ @ @ @ @ @ @ @ sorted in ascending order : <p> CREATE INDEX LNAME ON EMPLOYEE ( LASTNAME ASC ) <p> The following SQL statement creates a unique index on the phone number column : <p> CREATE UNIQUE INDEX PH ON EMPLOYEE ( PHONENO DESC ) <p> Drop a database : <p> Db2 drop database sample <p> Alter tablespace <p> Adding a Container to a DMS Table Space You can increase the size of a DMS table space ( that is , one created with the MANAGED BY DATABASE clause ) by adding one or more containers to the table <p> space . The following example illustrates how to add two new device containers ( each with 40 000 pages ) to a table space on a UNIX-based system : <p> ALTER TABLESPACE RESOURCE <p> ADD ( DEVICE ' /dev/rhd9 ' 10000 , <p> DEVICE ' /dev/rhd10 ' 10000 ) <p> The following SQL statement drops the table space ACCOUNTING : <p> DROP TABLESPACE ACCOUNTING <p> You can reuse the containers in an empty table space by dropping the table space but you must COMMIT the DROP TABLESPACE command , or have had AUTOCOMMIT on @ @ @ @ @ @ @ @ @ @ SQL statement creates a new temporary table space called TEMPSPACE2 : <p> CREATE TEMPORARY TABLESPACE TEMPSPACE2 MANAGED BY SYSTEM USING ( ' d ' ) <p> Once TEMPSPACE2 is created , you can then drop the original temporary table space TEMPSPACE1 with the command : DROP TABLESPACE TEMPSPACE1 <p> Add Columns to an Existing Table <p> When a new column is added to an existing table , only the table description in the system catalog is modified , so access time to the table is not affected immediately . Existing records are not physically altered <p> until they are modified using an UPDATE statement . When retrieving an existing row from the table , a null or default value is provided for the new column , depending on how the new column was defined . Columns that are added after a table is created can not be defined as NOT NULL : they must be defined as either NOT NULL WITH DEFAULT or as nullable . Columns can be added with an SQL statement . The following statement uses the ALTER TABLE statement to add three columns to the EMPLOYEE @ @ @ @ @ @ @ @ @ @ NOT NULL WITH DEFAULT <p> ADD HIREDATE DATE <p> ADD WORKDEPT CHAR(3) <p> GrantPermissions by Users <p> The following example grants SELECT privileges on the EMPLOYEE table to the user HERON : <p> GRANT SELECT ON EMPLOYEE TO USER HERON <p> The following example grants SELECT privileges on the EMPLOYEE table to the group HERON : <p> GRANT SELECT ON EMPLOYEE TO GROUP HERON <p> GRANT SELECT , UPDATE ON TABLE STAFF TO GROUP PERSONNL <p> If a privilege has been granted to both a user and a group with the same name , you must specify the GROUP or USER keyword when revoking the privilege . The following example revokes the SELECT privilege on the EMPLOYEE table from the user HERON : <p> At a minimum , you should consider restricting access to the SYSCAT.DBAUTH , SYSCAT.TABAUTH , SYSCAT.PACKAGEAUTH , SYSCAT.INDEXAUTH , SYSCAT.COLAUTH , and SYSCAT.SCHEMAAUTH catalog views . This would prevent information on user privileges , which could be used to target an authorization name for break-in , becoming available to everyone with access to the database . The following statement makes the view available to every authorization @ @ @ @ @ @ @ @ @ @ <p> And finally , remember to revoke SELECT privilege on the base table : <p> REVOKE SELECT ON TABLE SYSCAT.TABAUTH FROM PUBLIC <p> Delete Records from a table <p> db2-&gt; delete from employee where empno = ' 001 ' <p> db2-&gt; delete from employee <p> The first example will delete only the records with emplno field = 001 The second example deletes all the records <p> Import Command <p> Requires one of the following options : sysadm , dbadm , control privileges on each participating table or view , insert or select privilege , example : <p> db2-&gt;import from testfile of del insert into workemployee <p> where testfile contains the following information LONG ... <p> or your alternative is from the command line : <p> db2 " import from ' testfile ' of del insert into workemployee " <p> db2 &lt; test.sql where test.sql contains the following line : <p> db2 import from test file of del insert into workemployee <p> Load Command : <p> Requires the following auithority : sysadm , dbadm , or load authority on the database : <p> example : db2 " load from ' testfile @ @ @ @ @ @ @ @ @ @ have to specify the full path of testfile in single quotes <p> Authorization Level : <p> One of the following : <p> sysadm <p> dbadm <p> load authority on the database and <p> INSERT privilege on the table when the load utility is invoked in INSERT mode , TERMINATE mode <p> Describes the access plan selection for static SQL statements in packages that are stored in the DB2 common server systems catalog . Given the database name , package name , package creator abd section <p> number the tool interprets and describes the information in these catalogs . <p> DB2exfmt - Explain Table Format Tool <p> DB2icrt - Create an instance <p> DB2idrop - Dropan instance <p> DB2ilist - List instances <p> DB2imigr - Migrate instances <p> DB2iupdt - Update instances <p> Db2licm - Installs licenses file for product ; <p> db2licm -a db2entr.lic <p> DB2look - DB2 Statistics Extraction Tool <p> Generates the updates statements required to make the catalog statistics of a test database match those of a production . It is advantageous to have a test system contain asubset of your production system 's data . <p> @ @ @ @ @ @ @ @ @ @ outputs a tablespace n table index , and column information about each table in that database Authorization : Select privelege on system catalogs Required 
@@97506288 @1706288/ <h> Poseview Image of MAL in 3RUM <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of BGC in 3RUM <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MAN in 3RUM <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of RAM in 3RUM <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of BXY in 3RUM <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of RST in 3RUM <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of SO4 in 3RUM <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of IPA in 3RUM <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506295 @1706295/ <h> Sequence &amp; Structure Alignment <p> Comparisons can be made for any protein in the PDB archive and for customized or local files not in the PDB . Special features include support for both rigid-body and flexible alignments and detection of circular permutations . <h> Protein Symmetry <p> The Jmol Symmetry view highlights global , local , and helical symmetry among subunits . The view displays the symmetry axes , a polyhedron that reflects the symmetry , and a color scheme that emphasizes the symmetry . <h> Hemoglobin <h> Streptavidin <h> Inovirus <h> Structure Quality <p> The slider graphic compares important global quality indicators for a given structure with the PDB archive . Global percentile ranks ( black vertical boxes ) are calculated with respect to all X-ray structures available prior to 2011 . Resolution-specific percentile ranks ( white vertical boxes ) are calculated considering entries with similar resolution . <p> This graphic is from the wwPDBValidation Report , which provides a more detailed assessment of the quality of a structure and highlights specific concerns . These reports were created using the recommendations of wwPDB Validation Task Forces . @ @ @ @ @ @ @ @ @ @ . PDFs of Ramachandran plots ( created by MolProbity ) are provided to offer an independent method to evaluate the conformational quality of protein structures . <p> For examples , view the Structure Summary pages for 1CBS ( 1.8+ structure of a small protein and a ligand , an entry with better overall quality relative to all X-ray structures ) and 1FCC ( a 3.2+ structure with worse overall quality relative to all X-ray structures ) . 
@@97506297 @1706297/ <h> Frequently Asked Questions : <h> Q : What is TC-system ? <p> A : The TC system is analogous to the Enzyme Commission ( EC ) system for classification of enzymes , except that it incorporates both functional and phylogenetic information . Descriptions , TC numbers , and examples of over 600 families of transport proteins are provided . Transport systems are classified on the basis of five criteria , and each of these criteria corresponds to one of the five numbers or letters within the TC# for a particular type of transporter . Thus a TC # normally has five components as follows : V.W.X.Y.Z. V ( a number ) corresponds to the transporter class ( i.e. , channel , carrier ( porter ) , primary active transporter , group translocator or transmembrane electron flow carrier ) ; W ( a lettter ) corresponds to the transporter subclass which in the case of primary active transporters refers to the energy source used to drive transport ; X ( a number ) corresponds to the transporter family ( sometimes actually a superfamily ) ; Y ( a number @ @ @ @ @ @ @ @ @ @ found , and Z corresponds to a specific transporter with a particular substrate or range of substrates transported . <h> Q : How can I search TCDB with a specific TC Identification number ( TC I 'd or TC # ) ? <p> A : You can search TCDB with a TC I 'd at various classification levels . For instance , 1 ) you can search for a TC-Class with just one digit representing the that Class ( e.g. , 2 ) , 2 ) for a TC-Subclass , you can search with number followed by a letter representing the Subclass ( e.g. , 2 . A ) , 3 ) for a TC-Family ( Superfamily ) , you can search with 3 digits representing that Family ( e.g. , 2 . A.1 ) , 4 ) for a subfamily belonging to a family ( or family belonging to a superfamily ) , you can search with a 4 digits TC # ( e.g. , 2 . A.1.1 ) and 5 ) for a specific transporter , you can search with a 5 digit TC # ( e.g. , 2 . @ @ @ @ @ @ @ @ @ @ What 's the difference between a TC Family and a TC Superfamily ? <p> A : Because of the requirements of the IUBMB for a static system , the distinction between superfamily and family is blurred . " Superfamily " is defined as a large family consisting of sequence divergent members . Thus , with the establishment of homology between distantly related families , superfamilies can be created . These superfamilies can be found under " Superfamilies " . If the superfamily was recognized before the IUBMB requirement for a static system was implemented , superfamilies are listed under a single TC family number ( e.g. , 2 . A.1 ) , but if implemented later , the families within a superfamily assume different TC numbers ( e.g. , 2 . A.2 ) . All superfamilies can be viewed on the " SUPERFAMILIES " hyperlink . <h> Q : How are TC IDs assigned to transporter proteins ? <p> A : Any two transport systems in the same subfamily of a transporter family that transport the same substrate(s) are given the same TC# , regardless of whether they are orthologues @ @ @ @ @ @ @ @ @ @ or paralogues ( e.g. , arose within a single organism by gene duplication ) . Sequenced homologues of unknown function are not normally assigned a TC# unless they represent a unique ( sub ) family or are from an unrepresented organismal kingdom . If multiple dissimilar subunits comprise a transport system , all are listed under a single 5 digit TC I 'd . <h> Q : Where are TC Classes 6 and 7 ? <p> A : Classification categories 6 and 7 are reserved for future , yet to be discovered classes . If and when new classes are discovered , they will receive these TC class numbers . <h> Q : How can I leave suggestions and feedback about TCDB ? <h> Q : How often is TCDB updated ? <p> A : Usually every week , but it mainly depends on the availability of new data . However , the update of the www page as well as the I 'd mapping files ( in the download link ) are updated as soon as a new entry is inserted into the database . <h> Q : From where do @ @ @ @ @ @ @ @ @ @ come from published ( or occasionally unpublished ) data , evaluated by our curators . The process of screening the literature has been greatly enhanced with the introduction of Machine Learning programs that distinguish documents relevant to TCDB from irrelevant ones . The classifier ranks new , unlabeled documents and pass them onto an human expert . They are then carefully checked/analyzed before inserting into TCDB . Since TCDB is a representative database of transporter , not all the functionally characterized transport systems are included , particularly orthologues with the same function ( see above ) . <h> Q : How can I search TCDB with my sequence ? <p> A : You can search again TCDB using BLAST search ( http : **30;2184;TOOLONG ) either with your protein sequence in FASTA format or with UniProt protein accession number ( e.g. , P48048 ) . The BLAST search is always kept up-to-date with the sequences updated as soon as they are entered into database . <h> Q : What is FASTA format ? <p> A : FASTA format is a text-based format for representing either nucleotide sequences or peptide sequences @ @ @ @ @ @ @ @ @ @ using single-letter codes . A sequence in FASTA format begins with a single-line description , followed by lines of sequence data . The description line is distinguished from the sequence data by a greater-than ( " &gt; " ) symbol in the first column . Here is an example : <h> Q : What is UniProt Accession number ? <p> Accession number is assigned to each sequence upon inclusion into UniProtKB . Since , almost all the proteins entries ( except the once that are not currently available ) in TCDB are from UniProtKB , we also retain the accession numbers assigned to them . 
@@97506300 @1706300/ <h> Molecular Function Browser <p> Searches or browses for structure by Gene Ontology Molecular Function terms . Molecular Function terms are a set of controlled vocabulary terms defined by the Gene Ontology project ( http : //www.geneontology.org ) . <p> The Molecular Function Browser permits the user to navigate through the Molecular Function hierarchy to arrive at the subset of particular interest . The browser opens up the top level in the hierarchy . Clicking on the arrow/folder icons expands the respective nodes . Clicking on the name of the node will retrieve all PDB IDs assigned to that node . <h> Examples <p> Search by a term <p> Protein Kinase Activity <p> Type Protein Kinase Activity in the text box above the tree and click the " Find in Tree " button . To see the number of structures related to Protein Kinase Activity . Click on the name to retrieve a list of these structures . <p> Browse the molecular process tree <p> Kinase Regulator Activity <p> Mouse over enzyme regulator activity to see the number of structures that belong to this category . Expand this node @ @ @ @ @ @ @ @ @ @ kinase regulator activity to see how many structures have been assigned to this term . Click on the name to retrieve this set of structures . 
@@97506301 @1706301/ <h> Born : Erlangen , Germany , March 23 , 1882 <h> Died : Bryn Mawr , Pennsylvania , April 14 , 1935 <h> Creative Mathematical Genius <p> It might be that Emmy Noether was designed for mathematical greatness . Her father Max was a math professor at the University of Erlangen . Scholarship was in her family ; two of her three brothers became scientists as well . Emmy would surpass them all . Ultimately Max would become best known as Emmy Noether 's father . <p> Amalie Emmy Noether spent an average childhood learning the arts that were expected of upper middle class girls . Girls were not allowed to attend the college preparatory schools . Instead , she went to a general " finishing school , " and in 1900 was certified to teach English and French . But rather than teaching , she pursued a university education in mathematics <p> She audited classes at Erlangen as one of two women among thousands of men , then took the entrance exam . She entered the University of Gttingen in 1903 , again as an auditor , @ @ @ @ @ @ @ @ @ @ finally let women enroll . She received her mathematics Ph.D . in 1907 . <p> Noether worked at the Mathematical Institute of Erlangen , without pay or title , from 1908 to 1915 . It was during this time that she collaborated with the algebraist Ernst Otto Fischer and started work on the more general , theoretical algebra for which she would later be recognized . She also worked with the prominent mathematicians Hermann Minkowski , Felix Klein , and David Hilbert , whom she had met at Gttingen . In 1915 she joined the Mathematical Institute in Gttingen and started working with Klein and Hilbert on Einstein 's general relativity theory . In 1918 she proved two theorems that were basic for both general relativity and elementary particle physics . One is still known as " Noether 's Theorem . " <p> But she still could not join the faculty at Gttingen University because of her gender . Noether was only allowed to lecture under Hilbert 's name , as his assistant . Hilbert and Albert Einstein interceded for her , and in 1919 she obtained her permission to @ @ @ @ @ @ @ @ @ @ she became an " associate professor without tenure " and began to receive a small salary . Her status did not change while she remained at Gttingen , owing not only to prejudices against women , but also because she was a Jew , a Social Democrat , and a pacifist. * <p> During the 1920s Noether did foundational work on abstract algebra , working in group theory , ring theory , group representations , and number theory . Her mathematics would be very useful for physicists and crystallographers , but it was controversial then . There was debate whether mathematics should be conceptual and abstract ( intuitionist ) or more physically based and applied ( constructionist ) . Noether 's conceptual approach to algebra led to a body of principles unifying algebra , geometry , linear algebra , topology , and logic . <p> In 1928-29 she was a visiting professor at the University of Moscow . In 1930 , she taught at Frankfurt . The International Mathematical Congress in Zurich asked her to give a plenary lecture in 1932 , and in the same year she was awarded @ @ @ @ @ @ @ @ @ @ , in April 1933 she was denied permission to teach by the Nazi government . It was too dangerous for her to stay in Germany , and in September she accepted a guest professorship at Bryn Mawr College . She also lectured at the Institute for Advanced Study in Princeton . The guest position was extended , but in April 1935 she had surgery to remove a uterine tumor and died from a postoperative infection . 
@@97506306 @1706306/ <p> The women scientists profiled here span several centuries and several nationalities . Despite many barriers , women all over the world have participated in unraveling the secrets of nature since the dawn of civilization . As historian of science Naomi Oreskes said recently , " The question is not why there have n't been more women in science ; the question is rather why we have not heard more about them . " Most of the women whose stories are told here , in fact , were active in recent times , when the sciences had already become professionalized endeavors . <p> This publication stems from a project undertaken at the San Diego Supercomputer Center ( SDSC ) in early 1997 , when a new wing was added to the center 's building . It featured a classroom designed for workshops in the most advanced computational and visualization techniques . The classroom was furnished with 16 new Silicon Graphics workstations. * <p> The machines had Internet addresses , which were strings of numbers , but since humans misremember numbers , they all needed memorable names as well . @ @ @ @ @ @ @ @ @ @ girls and young women interested in careers in the sciences , ** we named each machine after a woman who had a career in or made a significant contribution to a scientific discipline . Brief biographies were written for each woman selected , and these were put on the walls of the classroom . They were also gathered in this pamphlet , which we hope to distribute to audiences beyond our computational laboratory . <p> Many of the women celebrated here were mathematicians , physicists , or astronomers , all fields strongly related to the computational sciences . But there are also two biologists , two biochemists , a geological pioneer , a doctor , and an industrial psychologist , which is also appropriate , as these fields are also developing significant computational components . <p> The common thread running through their stories is their record of accomplishment . Each was able to make a significant contribution and each achieved recognition in her field . To one degree or another , all of these women faced obstacles to their scientific work that arose simply because they were women . Many @ @ @ @ @ @ @ @ @ @ allowed to work only without the pay or privileges accorded to men doing the same work . Engaging in normal scientific collaborations was an impossibility for some and a great difficulty for others , barred as they were from the milieux in which male scientists met and conversed . <p> But these women in science were also women specifically situated in time and place . They also struggled in common with their male counterparts against fascism , racism , and discrimination based on class and ethnicity . Some achieved such pinnacles as the Nobel Prize , while others have been nearly lost to history . We find that , in simply naming some computers , we have been privileged to enter a rich historical territory , one little enough explored--and we invite you to share it with us. 
@@97506309 @1706309/ <p> AbstractUlcers and chronic wounds are a particularly common problem in diabetics and are associated with hyperglycemia . In this targeted review , we summarize evidence suggesting that defective wound healing in diabetics is causally linked , at least in part , to hyperglycemia-induced changes in the status of hyaluronan ( HA ) that resides in the pericellular coat ( glycocalyx ) of endothelial cells of small cutaneous blood vessels . Potential mechanisms through which exposure to high glucose levels causes a loss of the glycocalyx on the endothelium and accelerates the recruitment of leukocytes , creating a proinflammatory environment , are discussed in detail . Hyperglycemia also affects other cells in the immediate perivascular area , including pericytes and smooth muscle cells , through exposure to increased cytokine levels and through glucose elevations in the interstitial fluid . Possible roles of newly recognized , cross-linked forms of HA , and interactions of a major HA receptor ( CD44 ) with cytokine/growth factor receptors during hyperglycemia , are also discussed . 
@@97506311 @1706311/ <h> Data Oasis User Guide <h> Data Oasis User Guide <h> System Overview <h> Overview <p> SDSC Data Oasis is an on-line , high-performance , Lustre-based storage resource with a 4 PB capacity that is available to all users of SDSC Comet . It was designed to meet the needs of data-intensive research by providing easy-to-use , high capacity , short- to medium-term storage with useable bandwidth on the order of 100 GB/s and latencies that are far lower than near-line and tape-based storage systems . However , it is not an archival system and stored data is single-copy and not backed up . <p> Data Oasis is divided into several file systems , including local scratch spaces for Comet and a shared , persistent 2 PB Project space that is available to users with an allocation on either machine . All projects on Comet receive a default allocation of 500 GB . <h> System Configuration <p> On Gordon , the Data Oasis Project Storage is mounted via 64 I/O nodes which act also as routers . The Gordon compute nodes are connected using QDR Infiniband to the I/O @ @ @ @ @ @ @ @ @ @ Oasis 's Arista 7508 switches via 2+10GigE links . <p> System Access <h> Allocations <p> The default allocation is 500 GB of Project Storage to be shared among all users of a project . Projects that require more than 500 GB of Project Storage must request additional space by sending an email to help@xsede.org . This e-mail from the project PI should provide a 500 words-or-less justification that includes : <p> Amount of storage being requested <p> How the storage will be used <p> How this augments other non-SDSC storage resources available to the project <p> The project 's storage requests will be reviewed by SDSC staff , and a decision will be made within 5 business days . <h> Methods of Access <p> The Data Oasis Project Storage space is mounted on both Comet and can be accessed as a filesystem on all login and compute nodes . Each user 's personal space can be found in <p> **43;2216;TOOLONG <p> where allocationname is the project 's six-character allocation name ( found by running the showaccounts command ) and username is the user 's local login name . <p> Since @ @ @ @ @ @ @ @ @ @ file transfer utilities such as scp , sftp , and rsync can be used for transfers of modest size or scale . To enable the efficient transfer of larger amounts of data , Data Oasis is also mounted on the SDSC data mover servers : <h> Transferring Data to/from SDSC Data Oasis <h> Data Transfer Methods <p> While using the standard UNIX file transfer tools ( scp , sftp , rsync ) is acceptable for simple and small file transfers ( &lt; 1 GB ) to and from Data Oasis , they can not realize the maximum performance of the Data Oasis storage resource because of their limited internal buffers and inability to stripe transfers across multiple data mover servers . The preferred method for transferring big data ( both large file sizes and large numbers of files ) is using GridFTP ( a part of the Globus Toolkit ) . Keep in mind that attempting to transfer large numbers of small files will result in poor performance . Whenever possible , create archives of directories with large file counts before initiating the data transfers . <p> The XSEDE Data @ @ @ @ @ @ @ @ @ @ to use GridFTP and its associated GUI- and terminal-based tools in XSEDE . To facilitate GridFTP with SDSC Data Oasis , the following data movers have Data Oasis mounted under /oasis/projects/nsf : <p> In the case of a 341 MB file transfer test case , GridFTP achieved an average 171 MB/s while scp achieved only 34.1 MB/s . When transferring terabytes of data , GridFTP is clearly preferable . <h> Caveats to Users <p> This resource is based on a Lustre filesystem which has some limitations . A comprehensive list of Lustre best-practices is beyond the scope of this guide , but it is important to minimize unnecessary access of file metadata . For example , <p> The " lfs " command is available by default on Gordon and can be loaded using " module load lustre " on Comet . <h> Troubleshooting / Common Errors <p> Problem : Any attempts to access files on Data Oasis just hang OR access is extremely **29;2261;TOOLONG : This can occur on both login nodes and compute nodes and typically results from Data Oasis being overloaded . These conditions typically " un-hang " @ @ @ @ @ @ @ @ @ @ , contact help@xsede.org with the system ( or specific compute nodes ) on which this is occurring . <p> Problem : /oasis/projects/nsf exists but is emptySolution : This problem is infrequent and should be reported to the XSEDE helpdesk with the system ( or specific compute nodes ) on which this is occurring . <h> Reference <h> Policies <p> SDSC Data Oasis Projects Storage is provided on a per-project basis and is available for the duration of the associated compute allocation period . Data will be retained for three months beyond the end of the project , by which time the data must be migrated elsewhere . <p> Data Oasis Projects Storage is not subject to automatic purges , but be aware that the data stored there is single-copy and not backed up ! Users are responsible for ensuring that critical data are duplicated elsewhere . Data accidentally deleted from Data Oasis can not be recovered . 
@@97506312 @1706312/ <h> Introduction <p> A lot of XSEDE users request allocations on SDSC Trestles and Gordon because we are one of two XSEDE sites ( the other being PSC and Blacklight ) with a Gaussian license that permits XSEDE users to use the software . I 've found that many start-up allocations wanting to use Gaussian involve users who have never used a batch system , a remote resource , or even a command line . In the interest of providing a very quick crash course for such users , here are my notes on making the jump from Gaussian on a PC or workstation to Gaussian on an SDSC XSEDE resource . <p> This guide assumes the reader has never used a batch system , an XSEDE resource , or the Linux command line . Since Trestles gets most of the new Gaussian users on XSEDE , I will assume the reader is using that system . Instructions for using Gordon are quite similar . <h> Logging into SDSC Trestles <p> The XSEDE User Portal has a guide to getting started , and it covers all the options about @ @ @ @ @ @ @ @ @ @ options can be confusing at first , so for the sake of keeping it as simple as possible , I 'll lay out every step . <p> Log in with your XSEDE username and password . If you do not have an XSEDE User Portal account , you will have to create one and then get your project PI ( your supervisor ) to add that account to your group 's project <p> Scroll down to Trestles and click the link under the " Login Name " column . This will take you to the GSI-SSH Terminal Java applet which will take a while to load , then dump you at a black screen with white text . <p> This is the Linux terminal , and the last line is your prompt which lists your username , your current machine ( trestles-login1 or trestles-login2 ) , your current directory ( is an abbreviation for your home directory ) , and a dollar sign ( $ ) which means you are logged in as a regular ( not administrative ) user . <p> Typographic conventions hold that commands you are supposed @ @ @ @ @ @ @ @ @ @ the shell " ) be preceded by a $ to represent the shell prompt . So , if this guide says to issue the following command : <p> $ pwd <p> you do n't actually type the dollar sign . It 's just there to tell you to type the pwd command in the Linux shell . I also will forego the black background from my samples below . You know what your terminal looks like . <h> Getting Permission and Loading Gaussian <p> Because Gaussian requires a license to use , new login accounts must be given permission to run Gaussian before they can actually use it . Chances are you will need to request this permission by sending an email to help@xsede.org . Once your request is processed , it may take a few hours for the changes to take effect . If you want to check to see if you can run Gaussian , you can use the groups command : <p> $ groups <p> rut100 gaussian <p> If you do not see gaussian listed in the output of this command , you will not be able @ @ @ @ @ @ @ @ @ @ enabled for Gaussian , you can load its associated module with this command : <p> $ module load gaussian <p> This will give you access the Gaussian commands like formchk , unfchk , and of course g09 . However , do not skip ahead and just start running g09 ! If you do , you will make a lot of other users upset and you will get a sternly worded e-mail from me or one of my colleagues . <h> Gaussian Job Setup <p> At this point I assume you have a Gaussian job you want to run , and it consists of the following files on your personal computer : <p> input.com - your Gaussian input file <p> molecule.chk - a Gaussian checkpoint file containing the data for the molecule you want to simulate . If you are starting from scratch , the coordinates of your nuclei will be in your . com file and you will not have this checkpoint file . <h> Creating a job directory <p> The first thing you want to do is create a directory in which you want all this simulation 's data @ @ @ @ @ @ @ @ @ @ create a directory called job1 . To then go into that directory , <p> $ cd job1 <h> Transferring files to Trestles <p> Now you need to transfer your Gaussian input files from your computer to Trestles . The easiest way to do that is using the XSEDE File Manager , which is a Java applet that allows you to drag-and-drop files from your personal computer to any XSEDE resource . On the left will be your local files , and on the right is a list of XSEDE resources on which you have an account . Double click " SDSC Trestles Appro Rocks Cluster " to connect to it , and your job1 directory should appear . Double click it , and drag-and-drop your Gaussian job files onto Trestles . <p> Back in your terminal session , you should be able to type the ls command and see the files you just uploaded . <p> $ ls <p> input.com molecule.chk <h> Setting up the queue script <p> Up until now , the steps have been very generic and can be used by any user to get started on Trestles @ @ @ @ @ @ @ @ @ @ , or any other XSEDE supercomputer , you will have to interact with the batch system which is really what distinguishes using a shared supercomputer from using your personal computer . <p> At SDSC we use use the Torque Resource Manager which is comprised of a number of commands ( e.g. , qsub , qstat , qdel , and qmod ) , and running your simulation through the batch system requires a queue script to " glue " together the inner workings of Torque and Gaussian . <p> The name of this queue script is arbitrary , but I like to give them the extensions of . qsub . So , you will have to create a file called g09job.qsub using a command-line text editor . The nano editor is perhaps the easiest to use . Issue this command : <p> $ nano g09job.qsub <p> to create and edit a file called g09job.qsub . You will see a screen like this : <p> Some common nano commands are shown at the bottom : ctrl+x exits , ctrl+w to search , etc . You will need to paste the following lines @ @ @ @ @ @ @ @ @ @ <p> #PBS -q shared <p> #PBS -l nodes=1:ppn=16 <p> #PBS -l walltime=02:30:00 <p> . **25;2292;TOOLONG <p> module load gaussian <p> cd $PBSOWORKDIR <p> export **36;2319;TOOLONG <p> g09 &lt; input.com &gt; output.txt <p> Now exit nano ( ctrl+x ) and say yes to " Save modified buffer ( ANSWERING " No " WILL DESTROY CHANGES ) ? " to save your changes . This is the absolute bare minimum queue script you will need to run a Gaussian job , and the details of what each line means can be found in the Trestles User Guide and at the end of this tutorial . For now , there are only two important lines . The first one is <p> #PBS -l nodes=1:ppn=16 <p> This tells Torque that your job will require one node and sixteen CPU cores on that node . You will then have to modify your Gaussian input file , input.com , to actually use these sixteen cores . Open up that input.com file in nano and make sure the following red Link 0 commands are present above the Route section : <p> %chk=molecule.chk <p> %nproc=16 <p> %mem=31GB <p> @ @ @ @ @ @ @ @ @ @ <p> The %nproc option tells Gaussian to use 16 cores , which must be the same as what your queue script requests . The %mem option specifies how much memory Gaussian can use . On Trestles , there is a max of 2 GB available per core , but it is good practice to not specify this absolute max since the operating system and other system programs on the node will also need some memory . <p> Everything else in our Gaussian input file can remain unchanged . <h> Running Gaussian <p> Once you have your input file set up , you still can not run Gaussian yet . Unlike a workstation where you can just use the g09 command , Trestles ( and all modern supercomputers ) requires you to submit your job to a batch system that schedules and launches everyone 's job in a fair manner . <h> Getting the Job Script <p> You will have to submit jobs to Trestles using the qsub command and a job submission script which contains more Linux terminal commands to be executed on one of the compute nodes . I @ @ @ @ @ @ @ @ @ @ To do that , use <p> $ cp **42;2357;TOOLONG /home/username/job1/ <p> If you want to copy my input.com as well , you can do <p> $ cp **40;2401;TOOLONG /home/username/job1/ <p> You can also see what other files I have in my sample directory using ls **31;2443;TOOLONG . <h> Setting your Walltime <p> Now if you ls from within your job1 directory , you should see input.com , molecule.chk ( if you had a checkpoint file you transferred from your personal computer ) , and g09job.qsub . You can view the contents of the submit script ( g09job.qsub ) by typing cat g09job.qsub . If you want to edit it , you can use the " nano " editor ( e.g. , nano g09job.qsub ) . To exit nano , press ctrl+x . You can google for " nano editor tutorial " to learn more about using nano . <p> If you want to edit g09job.qsub , I recommend changing the line which reads <p> #PBS -l walltime=48:00:00 <p> That line says that your job needs 48 hours to complete ; if you know your job takes less time ( e.g. @ @ @ @ @ @ @ @ @ @ you can change that to , say , <p> #PBS -l walltime=00:15:00 <p> for 15 minutes . <h> Submitting your Job <p> To actually run your Gaussian simulation , use the qsub command : <p> $ qsub g09job.qsub <p> The job may sit in queue for a while , and you can check its status by typing qsub -u username . The second-to-last column ( labeled " S " ) is the job state . Q means it 's in queue , R means it is running , and C means the job has finished . <p> Once your job finishes , you should have a new file called output.txt which you can view using cat , edit using nano , and download to your computer using the XSEDE File Manager . 
@@97506314 @1706314/ <p> Abstract Our previous studies showed : ( i ) that growth-arrested G0/G1 rat mesangial cells stimulated to divide in hyperglycemic medium initiate intracellular hyaluronan synthesis that induces autophagy and the cyclin D3-induced formation of a monocyte-adhesive extracellular hyaluronan matrix after completing cell division ; and ( ii ) that heparin inhibits the intracellular hyaluronan and autophagy responses , but after completing division , induces hyaluronan synthesis at the plasma membrane with the formation of a larger monocyte-adhesive hyaluronan matrix . This study shows : ( i ) that the non-terminal trisaccharide of heparin is sufficient to initiate the same responses as intact heparin , ( ii ) that a fully sulfated tetrasaccharide isolated from bacterial heparin lyase 1 digests of heparin that contains a + " -2S-iduronate on the non-reducing end does not initiate the same responses as intact heparin , and ( iii ) that removal of the + " -2S-iduronate to expose the fully sulfated trisaccharide ( **29;2476;TOOLONG ) does initiate the same responses as intact heparin . These results provide evidence that mammalian heparanase digestion of heparin and heparan sulfate exposes a cryptic motif on @ @ @ @ @ @ @ @ @ @ dividing cells . 
@@97506318 @1706318/ <p> Proteins of the OMF family ( Li et al. , 2001 ; Wong et al. , 2001 ) function in conjunction with a primary cytoplasmic membrane transporter of the MFS ( TC #2 . A.1 ) ( Pao et al. , 1998 ) , the ABC superfamily ( TC #3 . A.1 ) ( Saurin et al. , 1999 ) , the RND superfamily ( TC #2 . A.6 ) ( Tseng et al. , 1999 ) and the PET family ( TC #9 . B.4 ) ( Harley and Saier , 2000 ) as well as a membrane fusion protein ( MFP ; TC #8 . A.1 ) ( Dinh et al. , 1994 ) . The complex thus formed allows transport ( export ) of various solutes ( heavy metal cations ; drugs , oligosaccharides , proteins , etc. ) across the two envelopes of the Gram-negative bacterial cell envelope in a single energy-coupled step . The OMF proteins probably form homotrimeric 12 stranded -barrel-type pores in the outer membrane through which the solutes pumped out of the cytoplasm or cytoplasmic membrane pass in response to @ @ @ @ @ @ @ @ @ @ . In one case , the complex of primary transporter , MFP and OMF forms transiently in response to substrate binding ( LTtoffT et al. , 1996 ) . In another case involving AcrA ( RND superfamily 8 . A.1.6.1 ) and TolC ( 1 . B.17.1.1 ) , the interaction appears to be substrate independent ( Husain et al. , 2004 ) . <p> The Serratia marcescens hemophore is secreted by a type I secretion system consisting of three proteins : a membrane ABC protein , an adaptor protein , and the TolC-like outer membrane factor ( Cescau et al. , 2007 ) . Assembly of these proteins is induced by substrate binding to the ABC protein . A hemophore mutant lacking the last 14 C-terminal amino acids is not secreted but rather interacts with the ABC protein and promotes a stable multiprotein complex . Strains expressing the transporter and the mutant protein are sensitive to detergents ( sodium dodecyl sulfate SDS ) . TolC is trapped in the transporter , jammed by the truncated substrate , and therefore is not present at sufficient concentrations to allow the efflux @ @ @ @ @ @ @ @ @ @ , the hemophore proved to interact with the ABC protein via two nonoverlapping sites . The C-terminal peptide , which functions as an intramolecular signal sequence in the complete substrate , may have intermolecular activity and trigger complex dissociation ( Cescau et al. , 2007 ) . <p> The crystal structure of E. coli TolC has been solved to 2.1 + resolution ( Koronakis et al. , 1997 , 2000 ) , and the VceC homologue of Vibrio cholerae has been solved to 1.8 + resolution ( Federici et al. , 2005 ) . Three TolC protomers form a continuous , solvent-accessible conduit , a channel tunnel over 140 + long that spans both the outer membrane ( as 12 -strands , 4 each per protomer ) and the periplasmic space ( as 12 a-helices , 6 continuous , 6 discontinuous , 4 each protomer ) . The a-helices are continuous with the -strands . The periplasmic end of the tunnel is sealed by sets of coiled helices that might untwist upon contact with the primary permease to open the channel ( Andersen et al. , 2001 ; Koronakis et @ @ @ @ @ @ @ @ @ @ states in a symmetrical opening transition of the TolC of E. coli exit duct have been identified ( Pei et al. , 2011 ) . <p> The OMFs exhibit a preudosymmetrical structure due to the presence of two internally duplicated segments . Thus , the outer membrane -barrel is assembled from the three protomers with each one contributing 4 -strands . Each strand is between 10 and 13 residues long . The strands both curve and twist , yielding a superhelical structure , but the channel is wide open and fully accessible to solvent . The possibility of channel closure due to conformational mobility has not been excluded ( Koronakis et al. , 2000 ) . The results clearly suggest that the OMF ( and not the MFP ) is largely responsible for the formation of both the trans-outer membrane and trans-periplasmic channels . The roles played by the MFP have yet to be determined . <p> OMF family members are found in most classes of proteobacteria in cyanobacteria , spirochetes , and in species of Deinococcus , Aquafex and Porphyromonas . The proteins are of 347-541 aas in length @ @ @ @ @ @ @ @ @ @ secretory pathway ( GSP ) TC#3.A.5 . <p> A two-receptor model for colicin E1 ( ColE1 ) translocation across the outer membrane of Escherichia coli has been proposed ( Masi et al. , 2007 ) . ColE1 initially binds to the vitamin B12 receptor BtuB and then translocates through the TolC channel-tunnel , presumably in a mostly unfolded state . In the early events in the import of ColE1 , cleavage of colicin requires the presence of the receptor BtuB and the protease OmpT , but not that of TolC . Strains expressing OmpT cleaved ColE1 at K84 and K95 in the N-terminal translocation domain , leading to the removal of the TolQA box , which is essential for ColE1 's cytotoxicity . Thus , OmpT degrades colicin at the cell surface to protect sensitive E. coli cells . Secondary binding of ColE1 to TolC depends on primary binding to BtuB , and alterations to residues in the TolC channel can interfere with the translocation of ColE1 but not binding of ColE1 to TolC ( Masi et al. , 2007 ) . 
@@97506319 @1706319/ <h> Managed Services <p> Most organizations consider technology to be a means to an end and lack the necessary resources and expertise to effectively operate enterprise infrastructure . That is where we come in . We have a team of experts with the knowledge and experience to provide a holistic solution . Our compliant managed services are designed for customers who need a secure environment to protect data and prefer a more hands-off approach when it comes to the intricacies and requirements of compliance . <h> Custom Solutions <p> We understand that every organization is unique and requires a tailored solution . Our team works cooperatively with our customers to create custom solutions that meet and exceed their needs . The Sherlock Cloud platform has already helped support a number of critical applications including preventing Medical Fraud , Cancer Research , Enterprise Risk Management , Population Health Research and more . <h> Compliance in AWS <p> We have expanded our compliance expertise to now offer our customers a solution utilizing the AWS Cloud . Our customers will have the option to choose managed compliant services operating on premise at @ @ @ @ @ @ @ @ @ @ of the two . Sherlock belongs to a select group of academic cloud service providers that partner with AWS to offer compliant managed services . <h> Consulting <p> Navigating the sea of compliance can be challenging and confusing , and failure to properly secure your organizations intellectual assets could leave you vulnerable to attacks and unwanted penalties for failure to comply . Our experts are available to help guide you through the unchartered waters of compliance , enterprise IT and cybersecurity , and minimize the challenges . <h> Data Lab <p> Sherlock Data Lab helps transform digital data into meaningful information using a hybrid approach to data management that includes both a top-down and bottom-up design . The framework provides data integration capabilities , allowing data to be captured , rationalized , homogenized , and managed using best practices and standards . This includes dynamic mappings , transformations , and master data management techniques utilizing established governance methodologies , with specific focus on data quality and metadata management . <h> Sherlock at Work <p> Sherlock Cloud was established to provide managed compliant services to meet the secure computing and data @ @ @ @ @ @ @ @ @ @ customers . Our compliance involves managing the entire software and hardware platform and the necessary management processes ( end-to-end compliant services ) . Not only is our comprehensive compliance portfolio a proven resource , but we strongly believe our intangibles , namely our superb team , experience , and knowledge provide our customers with an edge . We work together with our customers to jointly solve problems and create an environment that not only meets , but surpasses , their needs . <p> Nilofeur SamuelDirector , UCOP Risk Services <p> The Sherlock Cloud team was able to seamlessly take our data and application infrastructure from an open environment to their secure HIPAA-compliant environment in approximately 6 months . This aggressive timeline would not have been feasible absent the expertise and necessary skillset possessed by Sherlock Cloud . The end product has greatly helped secure our data and applications , and we are extremely pleased with the outcome . <p> James LaceyProfessor &amp; Director of Cancer Etiology , Beckman Research Institute , City of Hope <p> Sherlock Cloud is entirely HIPAA- and FISMA-compliant , meaning the Sherlock Cloud team has specific @ @ @ @ @ @ @ @ @ @ Data Mart and research require . Our partnership with SDSCs Sherlock Cloud will modernize the long-standing CTS project through Sherlock Clouds innovative data management solutions , which will make the California Teachers Study an example for other studies like this . <p> Andrew HackbarthPresident , Ursa Health <p> Thanks for your attention to these periodic support requests . Problems will always occur , but you all continue to distinguish yourselves by the immediate full-court press deployed to address them . We appreciate it . <p> Bob SinkovitsPrincipal Investigator , Human Vaccines Project <p> The Sherlock Cloud was exactly what we needed for the Human Vaccines Project . Rather than building the infrastructure from scratch to manage our sensitive data , Sherlock made it easy for us to deploy our services and databases in a secure environment so that we could focus on the science instead of worrying about compliance . <h> Sherlock Service Offerings <p> Sherlock Cloud contains a FISMA-certified environment that has successfully passed yearly audits since its inception in 2008 . This environment was developed and is maintained in accordance with NIST 800-53 requirements , which govern system @ @ @ @ @ @ @ @ @ @ Cloud is the largest FIMSA-certified cloud within the UC system . <p> A more recent addition to the Sherlock Cloud portfolio is our CUI-compliant environment ( NIST 800-171 ) . The federal government requires nonfederal entities to protect non-classified information that they receive from a federal entity ; CUI must be protected when processed , stored , transmitted and used in nonfederal information systems . As the CUI requirements provided in NIST 800-171 are based on the NIST 800-53 Moderate baseline ( and FIPS 200 ) , the Sherlock Team was able to seamlessly create an environment that provides the necessary safeguards . <p> Sherlock Cloud is collaborating with AWS to duplicate its managed services in the AWS Cloud to leverage scale and automation public Clouds offer . Sherlock Cloud understands that public Cloud offerings such as AWS and Azure do not offer compliant services ; customers are expected to buy compliant compute and storage services and build the necessary management services on top to meet compliance . Sherlock Clouds approach is to address this gap by building the necessary services on top of the public Cloud resources , thereby @ @ @ @ @ @ @ @ @ @ us <p> Sherlock , an offering of SDSC 's Health Cyberinfrastructure Division , is a Center of Excellence focused on managed information technology , compliance , and data services for academia and government that spans many IT disciplines , including compliant cloud hosting , cloud security , and data management . <h> News &amp; Events <p> Sherlock is based at the San Diego Supercomputer ( SDSC ) on the University of California , San Diego campus . For the latest Sherlock press releases , news , and events , please visit the SDSC News &amp; Events page . 
@@97506325 @1706325/ <h> Platelet hyaluronidase-2 : an enzyme that translocates to the surface upon activation to function in extracellular matrix degradation <p> AbstractFollowing injury , platelets rapidly interact with the exposed extracellular matrix ( ECM ) of the vessel wall and the surrounding tissues . Hyaluronan ( HA ) is a major glycosaminoglycan component of the ECM and plays a significant role in regulating inflammation . We have recently reported that human platelets degrade HA from the surfaces of activated endothelial cells into fragments capable of inducing immune responses by monocytes . We also showed that human platelets contain the enzyme hyaluronidase-2 ( HYAL2 ) , one of two major hyaluronidases that digest HA in somatic tissues . The deposition of HA increases in inflamed tissues in several inflammatory diseases , including inflammatory bowel disease ( IBD ) . We therefore wanted to define the mechanism by which platelets degrade HA in the inflamed tissues . In this study , we show that human platelets degrade the proinflammatory matrix HA through the activity of HYAL2 and that platelet activation causes the immediate translocation of HYAL2 from a distinct population of +-granules @ @ @ @ @ @ @ @ @ @ Finally , we show that patients with IBD have lower platelet HYAL2 levels and activity than healthy controls . 
@@97506327 @1706327/ <h> WEDNESDAY , August 2nd PARALLEL SESSIONS <p> Time <p> Track 1Auditorium <p> Track 2Synthesis Center E-B143 <p> 8:00 " 8:30 <p> Coffee <p> - <p> 8:30 " 12:00 <p> GPU Computing and ProgrammingAndreas Goetz , Research Scientist and Principal Investigator , SDSC This session provides an introduction to massively parallel computing with graphics processing units ( GPUs ) . The use of GPUs is becoming increasingly popular across all scientific domains since GPUs can significantly accelerate time to solution for many computational tasks . Participants will be introduced to essential background of the GPU chip architecture and will learn how to program GPUs via the use of libraries , OpenACC compiler directives , and CUDA programming . The session will incorporate hands-on exercises for participants to acquire the skills to use and develop GPU aware applications . <p> Spark for Scientific ComputingAndrea Zonca , Senior Computational Scientist , SDSC Apache Spark is a cluster computing framework extensively used in Industry to process large amount of data ( up to 1PB ) distributed across thousands of nodes . It has been designed as a successor of Hadoop focusing on @ @ @ @ @ @ @ @ @ @ Scala and Java . This session will provide an overview of the capabilities of Spark and how they can be leveraged to solve problems in Scientific Computing . Next it will feature a hands-on introduction to Spark , from batch and interactive usage on Comet to running a sample map/reduce example in Python . The final part will be devoted to two key libraries in the Spark ecosystem : Spark SQL , a general purpose query engine that can interface to SQL databases or JSON files and Spark MLlib , a scalable Machine Learning library . <p> Performance OptimizationBob Sinkovits , Director for Scientific Computing Applications , SDSC This session is targeted at attendees who both do their own code development and need their calculations to finish as quickly as possible . We 'll cover the effective use of cache , loop-level optimizations , force reductions , optimizing compilers and their limitations , short-circuiting , time-space tradeoffs and more . Exercises will be done mostly in C , but emphasis will be on general techniques that can be applied in any language . <p> Scientific visualization with VisIt and data @ @ @ @ @ @ @ @ @ @ largely understood and used as an excellent communication tool by researchers . This narrow view often keeps scientists from fully using and developing their visualization skillset . This tutorial will provide a " from the ground up " understanding of visualization and its utility in error diagnostic and exploration of data for scientific insight . When used effectively visualization can provide a complementary and effective toolset for data analysis , which is one of the most challenging problems in computationaldomains . In this tutorial we plan to bridge these gaps by providing end users with fundamental visualization concepts , execution tools , customization and usage examples . Finally , a short introduction to SeedMe.org will be provided where users will learn how to share their visualization results ubiquitously . <h> THURSDAY , August 3rd PARALLEL SESSIONS <p> Time <p> Track 1Auditorium <p> Track 2Synthesis Center E-B143 <p> 8:00 " 8:30 <p> Coffee <p> - <p> 8:30 " 12:00 <p> Parallel Computing using MPI &amp; Open MPPietro Cicotti , Senior Computational Scientist , SDSC This session is targeted at attendees who are looking for a hands-on introduction to parallel computing using @ @ @ @ @ @ @ @ @ @ with an introduction and basic information for getting started with MPI . An overview of the common MPI routines that are useful for beginner MPI programmers , including MPI environment set up , point-to-point communications , and collective communications routines will be provided . Simple examples illustrating distributed memory computing , with the use of common MPI routines , will be covered . The OpenMP section will provide an overview of constructs and directives for specifyingparallel regions , work sharing , synchronization and data scope . Simple examples will be used to illustrate the use of OpenMP shared-memory programming model , and important run time environment variables Hands on exercises for both MPI and OpenMP will be done in C and FORTRAN . <p> Machine Learning OverviewMai Nguyen , Lead for Data Analytics , SDSCPaul Rodriguez , Research Analyst , SDSC Machine learning is an interdisciplinary field focused on the study and construction of computer systems that can learn from data without being explicitly programmed . Machine learning techniques can be used to uncover patterns in your data and gain insights into your problem . This session provides an overview @ @ @ @ @ @ @ @ @ @ explore , analyze , and leverage data to construct data-driven solutions applicable to any domain . Topics covered include the machine learning process , data exploration , data preparation , classification , and cluster analysis . Concepts and algorithms will be introduced , followed by exercises to allow hands-on experience using R and RStudio . <p> Python for HPCAndrea Zonca , Senior Computational Scientist , SDSC In this session we will introduce four key technologies in the Python ecosystem that provide significant benefits for scientific applications run in supercomputing environments . Previous Python experience is recommended but not required . ( 1 ) The Jupyter Notebook allows users to execute code on a single compute node through a local browser for interactive data exploration and visualization . The Jupyter Notebook supports live Python code , explanatory text , LaTeX equations and plots in the same document . ( 2 ) IPython Parallel provides a simple , flexible and scalable way of running thousands of Python serial jobs by spawning IPython kernels ( namely engines ) on any HPC batch scheduler. ( 3 ) Numba makes it possible to run pure @ @ @ @ @ @ @ @ @ @ data types of the input and output arguments . Pure Python prototype code can be gradually optimized by pushing the most computationally intensive functions to the GPU without the need to implement code in CUDA or OpenCL. ( 4 ) Dask is a flexible parallel computing library that allows to build a distributed computation using simple operators and then let the library automatically handle distributing data , executing the computation hierarchically and gather back the results . <p> Scalable Machine LearningMai Nguyen , Lead for Data Analytics , SDSCPaul Rodriguez , Research Analyst , SDSC Machine learning is an integral part of knowledge discovery in a wide variety of applications . From scientific domains to social media analytics , the data that needs to be analyzed has become massive and complex . This session provides an introduction to approaches that can be used to perform machine learning at scale . Tools and procedures for executing machine learning techniques on HPC will be presented . Spark will also be covered . In particular , we will use Spark 's machine learning library , MLlib , to demonstrate how distributed computing can @ @ @ @ @ @ @ @ @ @ : Knowledge of fundamental machine learning algorithms and techniques is required . ( See description for Machine Learning Overview. ) 
@@97506328 @1706328/ <h> WorDS of Data Science beginning with D <p> Click one of the letters above to go to the page of all terms beginning with that letter . <p> Data Science <p> Data Science is about extracting knowledge from data . At the- WorDS Center , we define data science as a multidisciplinary craft that combines people , process , computational and Big Data platforms , application-specific purpose and programmability . Publications and provenance of the data products leading to these publications are also important for data science . <p> People : The data scientists are often seen as people who possess skills on a variety of topics including : science or business domain knowledge ; analysis using statistics , machine learning and mathematical knowledge ; data management , programming and computing . In practice , this is generally a group of researchers comprised of people with complementary skills. - <p> Process : The process of data science includes techniques for statistics , machine learning , programming , computing and data management . Data science workflows- combine such steps in executable graphs . We believe that process-oriented thinking is @ @ @ @ @ @ @ @ @ @ and techniques to applications . Challenges for the data science process include 1 ) how to easily integrate all needed tasks to build such a process ; 2 ) how to find the best computing resources and efficiently schedule process executions to the resources based on process definition , parameter settings , and user preferences . <p> Purpose : Purpose comes when people use generalizable processes with a particular goal in - mind . The purpose can be related to a scientific analysis with a hypothesis or a business metric that needs to be analyzed based often on Big Data . Note that similar reusable processes can be applicable to many applications with different purposes when employed within different workflows . <p> Platforms : Based on the needs of an application-driven purpose and the amount of data and computing required to perform this application , different computing and data platforms can be used as a part of the data science process . This scalability should be made part of any data science solution architecture . <p> Programmability : Capturing a scalable data science process requires aid from programming languages , @ @ @ @ @ @ @ @ @ @ . Tools that provide access to such programming techniques are key to making the data science process programmable on a variety of platforms . <p> Execution of such a data science process requires access to many datasets , Big and small , bringing new opportunities and challenges to Data Science . There are many Data Science steps or tasks , such as Data Collection , Data Cleaning , Data Processing/Analysis , Result Visualization , resulting in a Data Science Workflow . Data Science Processes may need user interaction and other manual operations , or be fully automated . 
@@97506330 @1706330/ <h> Poseview Image of STU in 5KQ5 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of ADP in 5KQ5 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of AMP in 5KQ5 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of 6VT in 5KQ5 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of SO4 in 5KQ5 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CL in 5KQ5 <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506331 @1706331/ <h> Poseview Image of Y01 in 4XP6 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CLR in 4XP6 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MAL in 4XP6 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NAG in 4XP6 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of B40 in 4XP6 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CL in 4XP6 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NA in 4XP6 <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506332 @1706332/ <h> Web Interface <p> The support vector machine ( SVM ) algorithm learns to distinguish between two given classes of data . This page allows you to train an SVM on a labeled training set and then use the trained SVM to make predictions about the classifications of an unlabeled test set . <p> Please note there are some limitations to how you can use this site . Very large or long-running jobs can not be run . The number of concurrent or waiting jobs is limited . If you have large data sets , or want to run the SVM many times , try the command line tools . The command line tools also give you access to additional features of the software such as feature selection and built-in cross-validation . <h> Inputs <p> Instead of uploading your own files , you can run the SVM using a demonstration data set by checking this box and clicking the ' submit ' button : <p> Important ! 90% of problems users have with the site are due to problems with input file formats . Please look closely at the @ @ @ @ @ @ @ @ @ @ . <p> Training data File containing a set of fixed-length , real-valued vectors that will serve as the training set : <p> This file should be a tab-delimited text file . The first row should contain feature names , and the first column should contain example names . The rest of the file should consist of a matrix of numbers , with each row of the matrix corresponding to an example . The limit on the total number of values in the file ( rows * columns ) is 1000000 . Here is an example matrix containing 10 vectors with 4 features in each vector . <p> Class labels File containing the training set classification labels : <p> This file should be a tab-delimited text file containing two columns . The first column should contain the same example identifiers as the training set file , in the same order . The second column should contain class labels ( 1 for the positive class , and -1 for the negative class ) . Here is an example . <p> Test data File containing the test set vectors : <p> This file @ @ @ @ @ @ @ @ @ @ should contain the same number of features as the training set , but may contain different examples . The data must not have any missing values . The limit on the total number of values in the file ( rows * columns ) is 1000000 . Here is an example . <p> After you click the submit button , the server will produce two output files : <p> The training set classifications . This tab-delimited output file will contain , for each training set member , the predicted class given by the trained SVM , as well as the corresponding discriminant value , which is proportional to the distance between the given example and the separating hyperplane . It also contains the weights for each example , which is the information used to classify test examples . Training examples with non-zero weights are support vectors . <p> The test set classifications , similar to the training set classifications file ( but with no weights ) . <p> You may click now to use the default SVM training options . <h> Training options <p> You may select alternate options from the @ @ @ @ @ @ @ @ @ @ options control how the input data is treated . Adjusting the data so the mean is zero and variance is one can help performance if your variables are heterogenous in scale . <p> Subtract from each element in the input data the mean of the elements in that row , giving the row a mean of zero . <p> Divide each element in the input data by the standard deviation of the elements in that row , giving the row a variance of one . <h> Kernel settings <p> The default kernel uses the simple dot product , making the SVM a linear classifier ( the feature space and the input space are equivalent ) . You may change the kernel to be a polynomial or radial basis function by setting the following options . Kernel matrix normalization ( performed by default ) adjusts all points to lie on the surface of the unit hypersphere in the feature space . <p> Raise the kernel to a given power ( forming a polynomial kernel ) . <p> Convert the kernel to a radial basis function.If K is the base kernel , @ @ @ @ @ @ @ @ @ @ -D ( x , y ) 2 ) / ( 2 w2 ) , where w is the width of the kernel ( see below ) and D ( x , y ) is the distance between x and y , defined as D ( x , y ) = sqrtK ( x , x ) 2 - 2 K ( x , y ) + K ( y , y ) 2 . <p> The width of the radial basis kernel is set using a heuristic : it is the median of the distance from each positive training point to the nearest negative training point . This option specifies a multiplicative factor to be applied to that width . <h> Soft margin options <p> The soft margin allows errors during training , which can both allow the SVM to find solutions with noisy data and help prevent overfitting and improve generalization performance . You may use either the one-norm or two-norm soft margin by setting the following options . The two-norm soft margin is the default . Enabling the one-norm soft margin by setting the constraint options disables the @ @ @ @ @ @ @ @ @ @ to the diagonal of the kernel matrix ( n+/N ) * m * k , where n+ is the number of positive training examples if the current example is positive ( and similarly for negative training examples ) , N is the total number of training examples , m is the median value of the diagonal of the kernel matrix , and k is the value specified here . <p> Enable one-norm soft margin : Set an explicit upper bound on the magnitude of the weights for positive training examples . <p> Enable one-norm soft margin : Set an explicit upper bound on the magnitude of the weights for negative training examples . <h> Miscellaneous options <p> Set the convergence threshold to the given value . Training halts when the objective function changes by less than this amount . A larger value will tend to speed execution , at the possible cost of some accuracy . <p> Check this to leave the HTML output unsorted . The default is to sort the HTML-ized output by discriminant ( prediction ) with positives listed first . <p> The svm software was developed @ @ @ @ @ @ @ @ @ @ and Computer Science at the University of Washington and Paul Pavlidis ( University of British Columbia ) . The web server was built and is maintained by Paul Pavlidis ( paul@chibi.ubc.ca ) , with contributions from Ilan Wapinski , Andrew Liu and Phan Lu . The project was funded by National Science Foundation grants DBI-0078523 and ISI-0093302. 
@@97506333 @1706333/ <h> Foreword by the Author <p> I originally wrote this paper in 1981 for a course in writing research papers at Rose-Hulman Institute of Technology . It was written on a DEC PDP-11/70 computer using the RUNOFF text formatting program , and having it on line from the beginning made it easy to save an electronic copy for future use . The instructor , Dr. Peter Parshall ( of " Peter Parshall picked apart my perfect paper " fame ) , awarded the grade of A- to my work . <p> In 1995 , with the World Wide Web available as a means of publication , I retrieved the original document from my archives and converted it to the HTML format seen here . Other than format conversions and the deletion of the bibliography ( which the Notes section renders superfluous ) , the paper is exactly as I wrote it then . ( Well , I also fixed a couple of spelling errors and added a missing word . These modifications are identified in the HTML source . ) I am both gratified and disappointed that the conclusions @ @ @ @ @ @ @ @ @ @ Incompleteness Theorem is Not an Obstacle to Artificial Intelligence <p> Artificial Intelligence . The idea of men building a machine which is capable of thinking , originating ideas , and responding to external stimuli in the same manner as a man might is fascinating to some people -- frightening to others . Whether or not artificial intelligence ( or AI ) is possible has been the subject of debate for quite some time now . As early as 1842 , a memoir by Lady Ada Lovelace read : " The Analytical Engine has no pretentions whatever to originate anything . It can do whatever we know how to order it to perform . " 1 This attitude -- that machines can not " think " for themselves -- has been widespread since then , and many arguments to this effect have been presented . Two of the more common of these arguments run along the lines of : " thinking is a function of man 's immortal soul ; " 2 or that computers can not really experience emotions and hence can not be like people. 3 Both of these @ @ @ @ @ @ @ @ @ @ , perhaps , the most convincing of any of the arguments against AI is based upon Kurt Gdel 's Incompleteness Theorem which says that a " sufficiently powerful " formal system can not consistently produce certain theorems which are isomorphic to true statements of number theory. 4 The implication of this Theorem , according to J. R. Lucas , is " that minds can not be explained as machines , " 5 that machines are inherently inferior . The purpose of this paper is to show that , in fact , Gdel 's Theorem has little if any impact upon the prospect of producing artificial intelligence comparable to man 's . In this paper the terms " machine " , " artificial intelligence machine " ( or " AI machine " ) , " computer " , " computer program " , and " electronic digital computer " will all be taken to mean the same thing . This is because the only means by which anyone today realistically expects to produce artificial intelligence is by programming a digital electronic computer . Some familiarity with modern computers is expected of @ @ @ @ @ @ @ @ @ @ Kurt Gdel published a paper which included a theorem which was to become known as his Incompleteness Theorem . This theorem stated that : <p> In more common mathematical terms , this means that " all consistent axiomatic formulations of number theory include undecidable propositions . " 7 One more time : any consistent formal system which is capable of producing simple arithmetic is incomplete in that there are true statements of number theory which can be expressed in the notation of the system , but which are not theorems of the system . Gdel 's Theorem uses several terms , the implications of which must be examined in determining its applicability to AI . These terms are : formal system , consistency , completeness , and theorem . <p> A formal system is defined by a finite set of meaningless symbols and a few very rigid rules of inference . A group of symbols arranged in a particular way is called a string , and there are a fixed number of strings which are called axioms ( or postulates ) . The rules of inference provide a means of @ @ @ @ @ @ @ @ @ @ manipulate symbols in a string to produce a new string . A theorem is any string which can be can be derived from the axiom(s) by applying zero or more of the rules of inference in succession to the axiom(s) . <p> A formal system is a concept just like a number is . In the same manner that numerals are used to represent numbers , a theorem of a formal system is represented by a group of symbols called a string . <p> A very simple example of a formal system ( called SFS , for Simple Formal System ) uses only the symbol " X " . Note that the quotes are not part of the symbol , but only delimit it . There is one axiom for SFS ; it is the string " X " . There are two rules of inference in SFS : <p> Given a SFS string , any single instance of the substring " X " may be replaced by the substring " XX " . <p> Given a SFS string , any single instance of the substring " XX " may @ @ @ @ @ @ @ @ @ @ Thus , given the axiom " X " only the first rule may be applied : replacing the X in the string with XX gives the result " XX " . Either of the two rules may be applied to this new theorem ; rule 1 produces " XXX " and rule 2 leaves " X " , which is already known to be a theorem . SFS is trivial enough that the following observations about it are quite obvious : <p> Starting with the axiom , repeated application of rule 1 will result in new , unique theorems being formed while rule 2 can only produce old theorems . <p> There are an infinite number of theorems in SFS . <p> By repeated application of one of the two rules of inference , any theorem can be derived from any other theorem . <p> So far , there has been no mention of any meaning for the theorems of SFS . This is because SFS theorems have no meaning ; they are merely strings of X 's , such as " XXXX " or " XXXXXXXXXXXXXXXX " . Nevertheless @ @ @ @ @ @ @ @ @ @ SFS theorems . For example , if we count the number of X 's in a theorem ( the only distinguishing feature between them ) we could say , for instance , that the theorem " XXXXX " represents ( or means ) the number five . If we wish to use this interpretation of SFS then it would be very reasonable to call the first rule of inference the Increment Rule and rule 2 the Decrement Rule . <p> SFS is so simple that it does not meet the " sufficiently powerful " clause of Gdel 's Theorem . SFS only produces numbers -- it does not make any assertions , such as " 3 + 4 = 7 . " However , it is possible to construct a formal system which has theorems which , for example , can be interpreted as " there are infinitely many prime numbers . " 8 <p> Gdel proved his Incompleteness Theorem in a rather bizarre but effective manner . He said that , given a formal system which can produce statements of number theory , there is a string ( written @ @ @ @ @ @ @ @ @ @ is not a theorem of the formal system , but it can be seen that the interpretation of that string is a true statement . This string ( called G9 ) is interpreted as " this string is not a theorem of formal system X , " where X represents the formal system in question . If X is a consistent formal system then it can not produce G as a theorem , for if it were to do so then X would contradict itself by saying that G is a theorem ( i.e. , true ) while G , which has been asserted to be true , says that G is not a theorem ( i.e. , false ) . This is what is meant by consistency in a formal system : " every theorem of the formal system , upon interpretation , comes out true ( in some imaginable world ) . " 10 <p> In his article Minds , Machines and Gdel , J. R. Lucas explains quite thoroughly why he believes that Gdel 's Theorem is evidence " that there is some elusive and ineffable quality @ @ @ @ @ @ @ @ @ @ He sums up his own arguments as follows : <p> However complicated a machine we construct , it will , if it is a machine , correspond to a formal system , which in turn will be liable to the Gdel procedure for finding a formula unprovable-in-that- system . This formula the machine will be unable to produce as true , although a mind can see that it is true . And so the machine will not be an adequate model of the mind. 12 <p> What he means is that Gdel 's Theorem applies to any machine that can be built ; humans , from the outside of the formal system of the computer , " can concoct a certain statement of number theory which is true , but the computer is blind to the statement 's truth . " 13 Hence , a man knows something ( statement G ) which the computer can not know and the conclusion is that a computer can not be as intelligent as a man , even in principle . <p> Lucas has erred in his ideas about the limits ( or @ @ @ @ @ @ @ @ @ @ of this toward AI . He infers from Gdel 's Theorem that the statement " G is not a theorem of formal system X " is true and so there is something that a mind can prove which can not be proved by formal system X. He then goes on to conclude that -- for this aforementioned reason -- minds and machines can not be the same. 14 No objection to this will be made , but this conclusion of Lucas ' comes as no surprise as not even two minds should be expected to be the same . Lucas oversteps his bounds when he goes on to defy Gdel 's Theorem by saying that minds are complete15 -- that there is nothing minds can not ( as opposed to do not ) know . He argues that " it is inherent in our idea of a conscious mind that it can reflect upon itself and criticize its own performances , and no extra part is required to do this . " 16 In contrast , " a machine which can be made in a manner of speaking to ' @ @ @ @ @ @ @ @ @ @ take this ' into account ' without thereby becoming a different machine , namely the old machine with a new part added . " 17 Lucas does not say so outright , but the implication is strong from this that Gdel 's Theorem does not apply to minds , that there is " some elusive and ineffable quality to human intelligence , which makes it unattainable by . . . machines . " 18 <p> In a moment we shall see that Gdel 's Theorem is , indeed , applicable to a mind , but before proceeding further the meanings of the terms " true " and " false " must be considered . Consistency demands that the concepts of " true " and " false " be mutually exclusive -- something can not be both true and false at the same time . Now , a mind may , by itself , come to some particular conclusion by following any line of reasoning it chooses : formal or informal , consistent or inconsistent , logical or illogical . However , in order for a mind to convince a different @ @ @ @ @ @ @ @ @ @ 1 ) convince the target mind to follow this same line of reasoning ; 2 ) convince the target mind that this line of reasoning makes sense and is " correct ; " and 3 ) that this line of reasoning actually does lead to the desired conclusion . Fortunately there is a method of reasoning -- called logic -- which is almost universally accepted by humans as " the way to prove something . " Logic is a highly formalized , necessarily consistent method of reasoning , so it is no surprise that it can be likened to a formal system . This formal system , whose rules effect the small " obvious " steps between ideas will be referred to as the " Human Logic System " or HLS. 19 It so happens that anything which can be derived within HLS is called " true " by human minds . Thus , the negation of a HLS theorem is called " false . " It is generally held , although Gdel 's Second Theorem20 says it can not be proved , that HLS is consistent . To assume @ @ @ @ @ @ @ @ @ @ idea that man has ever asserted . <p> When Lucas says , " ' G is not a theorem of formal system X ' is true , " it is apparent that his statement is no more than a theorem of HLS . In the same light , Gdel 's Incompleteness Theorem is a theorem of HLS , so it is called true . Let us apply Gdel 's theorem to HLS by considering the statement : " G is an undecidable proposition of HLS . " This can be restated as : " G is neither a theorem nor the negation of a theorem of HLS . " The meaning of G is " this statement is not a theorem of HLS , " but the undecidability is driven home when G is expressed as : <p> This version of G is , quite simply , undecidable in HLS ( try it ! ) . Thus it is seen that Gdel 's Incompleteness Theorem applies directly to logical human thought processes as well as to abstract formal systems so that , in this sense , a mind can not @ @ @ @ @ @ @ @ @ @ our minds act like consistent formal systems -- and we have seen evidence that this is true -- then minds can not claim superiority ( by virtue of Gdel 's Theorem ) over machines since the machine can use virtually the same argument to claim its superiority over the mind . What all this boils down to is that Gdel 's Incompleteness Theorem apples equally to machines and minds . There is a statement G which , when suitably represented , can not be produced as a theorem of any mechanical formal system , nor by a mind . Hence there is no valid reason to use Gdel 's Theorem to discredit the possibility of creating artificial intelligence . <p> One way to circumvent Gdel 's Theorem is by creating an inconsistent system . Lucas claims that an inconsistent mind would " remain content with . . . its inconsistencies and . . . happily affirm both halves of a contradiction . " 22 Nevertheless , while a mind can be shown to exhibit behavior like that of a consistent formal system , it can also be shown to act @ @ @ @ @ @ @ @ @ @ " Lucas can not consistently determine that this statement is true . " 23 Any person other than Lucas can , without being inconsistent , determine that the statement is true , but Lucas can not for if he says that it is true then he does , indeed , contradict himself . What might seem amazing is that Lucas , after thinking for a moment or two , will announce that this version of G is true . Now , if Lucas could be represented by a consistent formal system , then Gdel 's Theorem says that he would be unable to determine the truth of this G. Therefore we must conclude that Lucas can not be represented by a consistent formal system . This statement can be generalized by noting that anyone could be substituted for Lucas in the above example and so we must conclude that human minds , in general , can not be represented by consistent formal systems . <p> Just a moment ago we noted that minds act like consistent formal systems and now we are saying that minds are inconsistent ( now that @ @ @ @ @ @ @ @ @ @ people act rationally most of the time ( for the purposes of this paper , the terms rational and consistent are synonymous ) so it appears , in spite of Lucas , 24 that a mind is capable of maintaining a good deal of control over its inconsistencies . <p> The mind can be thought of as a consistent formal system which is capable of entertaining inconsistent ideas . This is absolutely necessary if the mind is to be capable of producing indirect proofs -- which use a hypothesis ( assumed to be false ) to produce a statement which is inconsistent with known facts , thus proving that the opposite of the hypothesis is true -- which rely on the mind 's ability to to create and then detect inconsistency . <p> We have seen that minds can behave both as consistent formal systems , and as inconsistent systems . Therefore , if we are to build a machine which can be compared fairly with a human mind it is only logical to expect the machine to behave both consistently and inconsistently , as well . Inconsistency is sufficient @ @ @ @ @ @ @ @ @ @ Theorem inapplicable and , contrary to Lucas , 25 an inconsistent system need not always behave inconsistently as evidenced by the vast number of rational people in this world . There are many hurdles to be overcome before Man creates anything worthy of the name , " Artificial Intelligence . " Pattern recognition is probably the most difficult problem to solve : when are things the same and when are they different ? 26 Yet , progress in AI is hampered by a lack of understanding of how the human mind functions rather than by some theoretical brick wall . Artificial intelligence is not a reality . . . yet . 
@@97506338 @1706338/ <p> **27;2507;TOOLONG ( BAR ) and Fes-CIP4 homology BAR ( F-BAR ) proteins generate tubular membrane invaginations reminiscent of the megakaryocyte ( MK ) demarcation membrane system ( DMS ) , which provides membranes necessary for future platelets . The F-BAR protein PACSIN2 is one of the most abundant BAR/F-BAR proteins in platelets and the only one reported to interact with the cytoskeletal and scaffold protein filamin A ( FlnA ) , an essential regulator of platelet formation and function . The FlnA-PACSIN2 interaction was therefore investigated in MKs and platelets . PACSIN2 associated with FlnA in human platelets . The interaction required FlnA immunoglobulin-like repeat 20 and the tip of PACSIN2 F-BAR domain and enhanced PACSIN2 F-BAR domain membrane tubulation in vitro . Most human and wild-type mouse platelets had 1 to 2 distinct PACSIN2 foci associated with cell membrane GPIb+ , whereas Flna-null platelets had 0 to 4 or more foci . Endogenous PACSIN2 and transfected enhanced green fluorescent protein-PACSIN2 were concentrated in midstage wild-type mouse MKs in a well-defined invagination of the plasma membrane reminiscent of the initiating DMS and dispersed in the absence of FlnA @ @ @ @ @ @ @ @ @ @ platelet territories were not readily visualized in Flna-null MKs . We conclude that the FlnA-PACSIN2 interaction regulates membrane tubulation in MKs and platelets and likely contributes to DMS formation . 
@@97506341 @1706341/ <h> UC San Diego 's Strategic Plan <p> Defining the Future of the Public Research University <h> Goal 2 <p> The campus community celebrates the opening of Raza Resource Centro <p> Cultivating a diverse and inclusive university community that encourages respectful open dialogue , and challenges itself to take bold actions that will ensure learning is accessible and affordable for all <h> Strategy 4 <p> The university aims to abide by the principles of equity , diversity , and inclusion in all its endeavors . These principles are embodied in our UC San Diego Principles of Community . Equity , diversity , and inclusion are an integral part of all five goals in the Strategic Plan , and we will continually review our practices supporting these ideals . <p> We embrace diversity when we acknowledge that people from a wide range of backgrounds and identities make unique and valuable contributions to our university community . We embrace equity when we insist on fair and nondiscriminatory treatment for all and seek to overcome the historical exclusion of some from higher education . We embrace inclusion when we create a climate @ @ @ @ @ @ @ @ @ @ for all individuals . <p> We are expanding our investment in equity , diversity , and inclusion programs and centers , including a $305,000 allocation for four diversity pilot grants and training programs . In addition , we 've created an open and ongoing dialogue about these topics through advisory committees , councils , open forums , and symposia . We 've also added four new resource centers on campus : Black Resource Center , Raza Resource Centro , Inter-Tribal Resource Center , and Student Veterans Resource Center . <p> We have learned that creation and innovation prosper best when they emanate from students , faculty , and staff members of diverse backgrounds and viewpoints . Many voices and cultures offer different ways of seeing the world , solving problems , and working together . <p> Perceptions of climate " the current attitudes , behaviors , and standards of faculty , staff , administrators , and students concerning the level of respect for individual needs , abilities , and potential " vary greatly according to race , ethnicity , and gender . We must understand the basis for these disparate @ @ @ @ @ @ @ @ @ @ will provide an opportunity to review comprehensive data and develop specific and measurable action plans that address these concerns . <p> We will continue to develop effective partnerships with the K " 12 community to prepare more underrepresented minority students . We will use our existing campus community centers and department programs to improve recruitment , orientation , retention , and graduation of underrepresented minorities at UC San Diego . One of these efforts is a new $1 million program intended to help faculty over the next two years to recruit underrepresented undergraduate students from Historically Black Colleges and Universities and Hispanic-Serving Institutions to participate in collaborative research projects on the UC San Diego campus during the summer , which will hopefully attract students here for graduate study . <p> We also need to improve diversity within the staff , particularly at the upper levels of the administration . With the recent leadership reorganization , we have been able to achieve some diversification of our leadership team and this will continue to be an ongoing goal . We must also expand opportunities for staff to learn about different cultures so @ @ @ @ @ @ @ @ @ @ ( increases in students with disabilities , student parents , veterans , international students , non-English speakers , former foster youth ) . We will reexamine our processes and training and development portfolios to identify and eliminate hurdles to progress . <p> Finally , we will strive to make inclusion of diverse voices and viewpoints an integral part of our university culture . An important first step was to review responses to regularly administered organizational surveys and subsequently develop plans that improve climate , and support other mechanisms that monitor progress . <h> Strategy 5 <p> Expand existing programs and implement new approaches that result in accessible and affordable learning for all . <p> While we have taken steps forward in recent years , they are not enough . Current demographic data show that diversity within our student body must expand if we are to reach a critical mass of those from traditionally underrepresented groups . We must do more to attract , enroll , and retain a diverse student body . <p> K " 12 presence : Enhance our already strong presence and role in the California Student Opportunity @ @ @ @ @ @ @ @ @ @ Imperial Counties to raise the achievement levels of low-income and first-generation students . <p> Through CalSOAP we will increase awareness of UC San Diego among low-income and first-generation K " 12 students and continue to fund our partnerships with regional K " 12 schools to raise achievement levels . We are also investing $1 million over three years in UC San Diego 's Center for Research and Educational Equity , Access , and Teaching Excellence ( CREATE ) to initiate the STEM Success Initiative , a visionary effort linking UC San Diego faculty , staff , and students and the San Diego education community in a shared effort to support K " 20 STEM ( science , technology , engineering , and mathematics ) education in the region . <p> Scholarship growth : Expand the Chancellor 's Associates Scholars Program by one hundred students per year for low-income students from local high schools and community colleges . <p> The Chancellor 's Associates Scholarship was originally offered to low-income students from three local partner high schools and has now been extended to three regional community colleges , federally recognized tribal members @ @ @ @ @ @ @ @ @ @ participated in both UC San Diego 's Academic Connections and Reality Changers , an organization founded by an alumnus to help underserved students prepare for college . The number of scholars in the program continues to increase . In order to fund the program at full ramp-up " we expect four hundred Chancellor 's Associates Scholars on campus in one given academic year " we have committed $4 million per year of private funds to support these bright students . This program , and other undergraduate scholarships , will be a major focus in our upcoming fundraising campaign . <p> Access : Support high-achieving students who reflect the demographics of California . <p> We are improving access for local community college students through UniversityLink , a transfer guarantee program that helps low-income students , and providing personalized support services designed to maximize student success and improve graduation rates . We also continue to develop broader and deeper relationships between the campus and our local community to meet stated needs such as economic development and urban planning . 
@@97506344 @1706344/ <h> File Formats <h> PDBML/XML File Format <p> The Protein Data Bank Markup Language ( PDBML ) provides a representation of PDB data in XML format . The description of this format is provided in XML schema of the PDB Exchange Data Dictionary . This schema is produced by direct translation of the mmCIF format PDB Exchange Data Dictionary . Other data dictionaries used by the PDB have been electronically translated into XML/XSD schemas. <h> mmCIF File Format and PDB Exchange Dictionary <p> The Protein Data Bank ( PDB ) uses macromolecular Crystallographic Information File ( mmCIF ) data dictionaries to describe the information content of PDB entries . The PDB Exchange data dictionary consolidates content from a variety of crystallographic dictionaries including : the IUCr Core , mmCIF , Image and symmetry dictionaries . The PDB Exchange Dictionary also includes extensions describing NMR , Cryo-EM , and protein production data . PDB data processing , data exchange , annotation , and database management operations all make heavy use of the data format and the content of the PDB Exchange Dictionary . Software tools are used to convert mmCIF @ @ @ @ @ @ @ @ @ @ . <h> PDB File Format <p> The Protein Data Bank ( PDB ) format provides a standard representation for macromolecular structure data derived from X-ray diffraction and NMR studies . This representation was created in the 1970 's and a large amount of software using it has been written . 
@@97506348 @1706348/ <h> Poseview Image of 0G6 in 5E8E <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of SIA in 5E8E <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NAG in 5E8E <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CIT in 5E8E <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of GAL in 5E8E <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MAN in 5E8E <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of BMA in 5E8E <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of PEG in 5E8E <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of PO4 in 5E8E <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NA in 5E8E <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506352 @1706352/ <h> Getting Started With the VisIt Visualization Package <p> VisIt is a free interactive parallel visualization and graphical analysis tool for viewing scientific data on Unix and PC platforms . One of the best ways to get started using VisIt on Gordon is in Client-Server mode . Below are step-by-step instructions for : <h> Setting Gordon 's Host Profile in VisIt <p> Check for available versions of VisIt on Gordon <p> VisIt requires matching versions on the client ( your computer ) and the server ( Gordon ) . For example : VisIt 2.4. x matches with VisIt2.4.0 and VisIt2.4.1 and VisIt2.4.2 , etc . Login to Gordon and use the list command to identify installed versions of VisIt . <p> % ls /opt/visit <p> Make sure that your computer has one of the matching versions of VisIt installed on Gordon , if not then download and install a compatible version from the VisIt website . <p> Open VisIt from the Options menu and choose Host Profiles <p> In the Host profiles dialog box : <p> Click New on the lower left of the window Enter the following in @ @ @ @ @ @ @ @ @ @ on Gordon <p> The VisIt application relies on complex integration with job scheduler and several dependent libraries making these instructions not easily transferable to other XSEDE resources . This section provides a simple way to run VisIt in batch mode on Gordon . <p> Render Script : Examine the render.py script . This python script will perform the following tasks : <p> Start parallel compute engines on specified nodes and cores <p> Open a multirect3d data file <p> Create a volume-rendering plot for variable d using raycasting <p> Save the rendered image to batchvisit directory <p> Delete the plot , close the file , close the compute engine and then terminate <p> Job Script : Examine the submit.sh script . This job submission shell script requests nodes and cores for rendering and then runs the render.py script . Note that the following parameters may need to be modified : <p> The script currently requests 2 nodes with 16 cores for 5 minutes . This may need to be increased for more computationally intensive visualization tasks . <p> Change your allocation account in the line #PBS -A gue998 . If you @ @ @ @ @ @ @ @ @ @ , run the showaccounts command . <p> Test Run : Submit the job script using qsub on Gordon <p> % qsub submit.sh <p> Once the job completes , you should see myBatchRender0000.png and myBatchrender* log files in your batchvisit directory . <p> Render Script : Examine the renderbench.py script . This python script will perform the following tasks : <p> Start parallel compute engines on specified nodes and cores <p> Open a multirect3d data file <p> Create a volume-rendering plot for variable d using raycasting <p> Save the rendered image to benchvisit directory <p> Measure the total rendering time for a given node and core configuration <p> Delete the plot , close the file , close the compute engine and then terminate <p> Launch Script : Examine the launchbench.sh script . This shell script will automatically create and submit several batch jobs requesting different numbers of nodes and cores for benchmark rendering . Note that the following parameters may need to be modified : <p> Change your allocation account in line 13 : myAccount=PROJECT . If you are unsure of which accounts you are authorized to use , run the @ @ @ @ @ @ @ @ @ @ 7 may need to be increased for very large visualization problems . <p> Test Run : <p> Change permissions and execute the launchbench.sh script . <p> % chmod a+x launchbench.sh% . /launchbench.sh <p> A script named myBenchsetup will be created in benchvisit directory <p> Finally , the myBenchsetup script is automatically submitted for different configurations in a way such that only one job runs at a time to avoid data file read contention . <p> Once the job completes you should see several images named myBenchRender*.png and myBenchnodes*cores* log files in the benchvisit directory . <p> Examine the render time consumed by each configuration <p> % tail -n 3 myBenchnodes*.o* <p> While the test case chosen here is trivial , this method could be very useful with large data sets . <h> If you are unable to connect to Gordon through VisIt , check the following : <p> Make sure that the VisIt application version on your computer is compatible with a VisIt version available on Gordon . Check versions of VisIt installed on Gordon by using the list command : <p> % ls /opt/visit <p> For example : VisIt2.3.X @ @ @ @ @ @ @ @ @ @ Check that the Path to Visit Installation is set correctly in your Gordon Host profile . Refer to how to create a Host Profile for Gordon documentation . <p> Ensure that your computer or network firewall is not blocking the VisIt server to connect back to the VisIt client on your computer . <p> In some cases erasing the local and remote visit settings can help resolve issues.Note : You will lose all your Host Profiles and will need to recreate them . Delete the " . visit " folder on your computer 's home or " My Documents " folder . Delete the " . visit " folder on Gordon in your home folder . 
@@97506354 @1706354/ <h> Poseview Image of Y01 in 4XP1 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CLR in 4XP1 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MAL in 4XP1 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NAG in 4XP1 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of P4G in 4XP1 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of LDP in 4XP1 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of EDO in 4XP1 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CL in 4XP1 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NA in 4XP1 <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506356 @1706356/ <h> Poseview Image of GCP in 4DST <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of 9LI in 4DST <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of GOL in 4DST <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of DMS in 4DST <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of EDO in 4DST <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of ACT in 4DST <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MG in 4DST <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506358 @1706358/ <h> Format for our talk <p> Important parts of source code are in red ( If I do n't talk about it , ask ! ) <p> Application is a genetic algorithm <p> Easy to understand and program <p> Offers rich opportunities for enhancement <h> What is a Genetic Algorithm <p> A " suboptimization " system <p> Find good , but maybe not optimal , solutions to difficult problems <p> Often used on NP-Hard or combinatorial optimization problems <p> Requirements <p> Solution(s) to the problem represented as a string <p> A fitness function <p> Takes as input the solution string <p> Output the desirability of the solution <p> A method of combining solution strings to generate new solutions <p> Find solutions to problems by Darwinian evolution Potential solutions ar though of as living entities in a population The strings are the genetic codes for the individuals Fittest individuals are allowed to survive to reproduce <h> Simple algorithm for a GA <p> Generate a initial population , a collection of strings <p> do for some time <p> evaluate each individual ( string ) of the population using the fitness function @ @ @ @ @ @ @ @ @ @ <p> allow the fittest individuals to " sexually " reproduce replacing the old population <p> allow for mutation <p> end do <h> Our example problem <p> Instance <p> Given a map of the N states or countries and a fixed number of colors <p> Find a coloring of the map , if it exists , such that no two states that share a boarder have the same color <p> Notes <p> In general , for a fixed number of colors and an arbitrary map the only known way to find if there is a valid coloring is a brute force search with the number of combinations = ( NUMBEROFCOLORS ) ** ( NSTATES ) <p> The strings of our population are integer vectors represent the coloring <p> Our fitness function returns the number of boarder violations <p> The GA searches for a mapping with few , hopefully 0 violations <p> This problem is related to several important NPHARD problems in computer science <p> Processor scheduling <p> Communication and grid allocation for parallel computing <p> Routing <h> Start of real Fortran 90 discussion <h> A preview : Comparing a FORTRAN 77 @ @ @ @ @ @ @ @ @ @ one of the random number generators from : Numerical Recipes , The Art of Scientific Computing . Press , Teukolsky , Vetterling and Flannery . Cambridge University Press 1986 . <p> program darwinuse numzimplicit none ! now part of the standard , put it after the use statements write ( * , * ) " pi has " , precision(pi) , " digits precision " , pi call setsize() write ( * , * ) " genesize= " , genesizeend program <p> program darwin use numz use ranmod ! interface required if we have ! optional or intent arguments real(b8) x , y x=ran1(myseed=12345) ! we can specify the name of the argument y=ran1() write ( * , * ) x , y x=ran1(12345) ! with only one optional argument we do n't need to y=ran1() write ( * , * ) x , yend program <p> Intent is a hint to the compiler to enable optimization <p> intent(in) <p> We will not change this value in our subroutine <p> intent(out) <p> We will define this value in our routine <p> intent(inout) <p> The normal situation <h> Derived data types @ @ @ @ @ @ @ @ @ @ to group different types of data together ( integers , reals , character , complex ) <p> Can not be done in F77 although people have " faked " it <p> Example <p> In our GA we define a collection of genes as a 2d array <p> We call the fitness function for every member of the collection <p> We want to sort the collection of genes based on result of fitness function <p> Define a data type that holds the fitness value and an index into the 2d array <p> Create an array of this data type , 1 for each member of the collection <p> Call fitness function with the result being placed into the new data type along with a pointer into the array <p> Again modules are a good place for data type definitions <p> module galapagos use numz type thefit ! the name of the typesequence ! sequence forces the data elements ! to be next to each other in memory ! where might this be useful ? real(b8) val ! our result from the fitness function integer index ! the index into our collection @ @ @ @ @ @ @ @ @ @ <p> Use the % to reference various components of the derived data type <p> program darwin use numz use galapagos ! the module that contains the type definition use face ! contains various interfaces implicit none ! define an allocatable array of the data type ! than contains an index and a real value type ( thefit ) , allocatable , target : : results ( : ) ! create a single instance of the data type type ( thefit ) best integer , allocatable : : genes ( : , : ) ! our genes for the genetic algorithm integer j integer numgenes , genesize numgenes=10 genesize=10 allocate ( results ( numgenes ) ) ! allocate the data type ! to hold fitness and index allocate ( genes ( numgenes , genesize ) ) ! allocate our collection of genes call initgenes(genes) ! starting data write ( * , ' ( " input " ) ' ) ! we can put format in write statement do LONG ... ! just a dummy routine for now LONG ... enddoend program <h> User defined operators <p> Motivation <p> With derived data @ @ @ @ @ @ @ @ @ @ <p> ( Assignment is predefined ) <p> Example : <p> &lt; , &gt; , == not defined for our data types <p> We want to find the minimum of our fitness values so we need &lt; operator <h> Recursive functions introduction <p> Anything that can be done with a do loop can be done using a recursive function <p> Motivation <p> Sometimes it is easier to think recursively <p> Divide an conquer algorithms are recursive by nature <p> Fast FFTs <p> Searching <p> Sorting <p> Algorithm of searching for minimum of an array function findmin(array) is size of array 1 ? min in the array is first element else find minimum in left half of array using findmin function find minimum in right half of array using findmin function global minimum is min of left and right half end function <h> Fortran 90 recursive functions <p> Recursive functions should have an interface <p> The result and recursive keywords are required as part of the function definition <p> We can define a generic interface for these two functions and call them using the same name <p> ! note we have two @ @ @ @ @ @ @ @ @ @ indicate function overloading ! both functions are called " findmin " in the main programinterface findmin <p> ! ! the first is called with an array of reals as input recursive function realmin(ain) result ( themin ) use numz real(b8) theminreal(b8) , dimension ( : ) : : ain end function <p> ! the second is called with a array of data structures as input recursive function typemin(ain) result ( themin ) use numz use galapagos real(b8) themintype ( thefit ) , dimension ( : ) : : ain end function end interface <p> ! use a recursive subroutine operating on the real arraywrite ( * , * ) " the lowest fitness : " , findmin(z) ! use a recursive subroutine operating on the data structurewrite ( * , * ) " the lowest fitness : " , findmin(results) <h> Fortran Minval and Minloc routines <p> Fortran has routines for finding minimum and maximum values in arrays and the locations <p> minval <p> maxval <p> minloc ( returns an array ) <p> maxloc ( returns an array ) <p> ! we show two other methods of getting the minimum @ @ @ @ @ @ @ @ @ @ real array write ( * , * ) " the lowest fitness : " , minval(z) , minloc(z) <h> Pointer assignment <p> This is how we use the pointer function defined above <p> worst is a pointer to our data type <p> note the use of =&gt; <p> ! use a recursive subroutine operating on the data ! structure and returning a pointer to the **31;2536;TOOLONG ! note pointer assignment ! what will this line write ? write ( * , * ) " the lowest fitness : " , worst <h> More pointer usage , association and nullify <p> Motivation <p> Need to find if pointers point to anything <p> Need to find if two pointers point to the same thing <p> Need to deallocate and nullify when they are no longer used <p> Usage <p> We can use associated() to tell if a pointer has been set <p> We can use associated() to compare pointers <p> We use nullify to zero a pointer <p> ! This code will print " true " when we find a match , ! that is the pointers point to the same object @ @ @ @ @ @ @ @ @ @ ( f10.8 , i4 , l3 ) " ) results(j)%val , &amp; results(j)%index , &amp;associated ( tmp , worst ) enddonullify(tmp) <p> Notes : <p> If a pointer is nullified the object to which it points is not deallocated . <p> In general , pointers as well as allocatable arrays become undefined on leaving a subroutine <p> This can cause a memory leak <h> Pointer usage to reference an array without copying <p> Motivation <p> Our sort routine calls a recursive sorting routine <p> It is messy and inefficient to pass the array to the recursive routine <p> Solution <p> We define a " global " pointer in a module <p> We point the pointer to our input array <p> module Mergemodtypes use galapagos type(thefit) , allocatable : : work ( : ) ! a " global " work arraytype(thefit) , pointer : : apntr ( : ) ! this will be the pointer to our input arrayend module Mergemodtypes <p> subroutine Sort ( ain , n ) use Mergemodtypes implicit none integer n type(thefit) , target : : ain(n) allocate ( work ( n ) ) nullify(apntr) apntr=&gt;ain ! @ @ @ @ @ @ @ @ @ @ we reference it just like an array call RecMergeSort ( 1 , n ) ! very similar to the findmin functions deallocate(work) returnend subroutine Sort <p> In our main program sort is called like this : <p> ! our sort routine is also recursive but ! also shows a new usage for pointers call sort ( results , numgenes ) do j=1 , numgenes write ( * , " ( f10.8 , i4 ) " ) results(j)%val , &amp; results(j)%index enddo <h> Data assignment with structures <p> ! we can copy a whole structure ! with a single assignment best=results(1) write ( * , * ) " best result " , best <h> Using the user defined operator <p> Click here to see how this routine is defined <p> ! using the user defined operator to see if best is worst ! recall that the operator . converged . checks to see if %index matches **25;2569;TOOLONG write ( * , * ) " worst result " , worst write ( * , * ) " converged= " , ( best . converged . worst ) <h> Passing arrays with a @ @ @ @ @ @ @ @ @ @ lower bound ! in this routine we give the array a specific lower ! bound and show how to use a pointer to reference ! different parts of an array using different indices call boink1 ( z , lbound ( z , 1 ) ) ! why not just lbound(z) instead of lbound ( z , 1 ) ? ! lbound(z) returns a rank 1 array <p> subroutine boink1 ( a , n ) use numz implicit none integer , intent(in) : : n real(b8) , dimension ( n : ) : : a ! this is how we set lower bounds in a subroutine write ( * , * ) lbound(a) , ubound(a) end subroutine <p> Warning : because we are using an assumed shape array we need an interface <p> nullify(xyz) ! nullify sets a pointer to null write ( * , ' ( l5 ) ' ) associated(xyz) ! is a pointer null , should be call boink(xyz) write ( * , ' ( l5 ) ' , advance= " no " ) associated(xyz) if ( associated ( xyz ) ) write ( * , ' ( @ @ @ @ @ @ @ @ @ @ numz implicit none real(b8) , dimension ( : ) , pointer : : a if ( associated ( a ) ) deallocate(a) allocate ( a ( 10 ) ) end subroutine <p> F T 10 <h> Our fitness function <p> Given a fixed number of colors , M , and a description of a map of a collection of N states . Find a coloring of the map such that no two states that share a boarder have the same coloring . <h> Non advancing and character I/O <p> Motivation <p> We read the states using the two character identification <p> One line per state and do not know how many boarder states per line <p> Note : Our list of states is presorted <p> character(len=2) a ! we have a character variable of length 2 read ( 12 , * ) nstates ! read the number of statesallocate ( map ( nstates ) ) ! and allocate our map do i=1 , nstates read ( 12 , " ( a2 ) " , advance= " no " ) map(i)%name ! read the name ! write ( * , * @ @ @ @ @ @ @ @ @ @ ( i ) %list ) ! " zero out " our list do read ( 12 , " ( 1x , a2 ) " , advance= " no " ) a ! read list of states ! without going to the ! next line if ( lge ( a , " xx " ) . and . lle ( a , " xx " ) ) then ! if state == xxbackspace(12) ! go to the next line read ( 12 , " ( 1x , a2 ) " , end=1 ) a ! go to the next line exit endif 1 continue if ( llt ( a , map(i)%name ) ) then ! we only add a state to ! our list if its name ! is before ours thus we ! only count boarders 1 time ! what we want put into our linked list is an index ! into our map where we find the bordering state ! thus we do the search here ! any ideas on a better way of doing this search ? found=-1 do j=1 , i-1 if ( lge ( a , @ @ @ @ @ @ @ @ @ @ ) ) then ! write ( * , * ) a found=j exit endif enddo if ( found == -1 ) then write ( * , * ) " error " stop endif ! found the index of the boarding state insert it into our list ! note we do the insert into the linked list for a particular state call insert ( found , map(i)%list ) endif enddo enddo <p> Example 2 : Creating a format statement at run time ( array of integers and a real ) <p> ! testvect is an array that we do not know its length until run time nstate=9 ! the size of the array write ( fstr , ' ( " ( " , i4 , " i1,1x , f10.5 ) " ) ' ) nstates write ( * , * ) " format= " , fstr write ( * , fstr ) testvect , result <p> format= ( 9i1,1x , f10.5 ) <p> Any other ideas for writing an array when you do not know its length ? <p> Example 3 : Reading from a string <p> integer ht , minut @ @ @ @ @ @ @ @ @ @ " ) hr , minut , sec <h> Inquire function <p> Motivation <p> Need to get information about I/O <p> Inquire statement has two forms <p> Information about files ( 23 different requests can be done ) <p> Information about space required for binary output of a value <p> Example : find the size of your real relative to the " standard " real <p> Useful for inter language programming <p> Useful for determining data types in MPI ( MPIREAL or MPIDOUBLEPRECISION ) 
@@97506359 @1706359/ <h> UC San Diego 's Strategic Plan <h> Goal 5 <p> Creating an agile , sustainable , and supportive infrastructure by ensuring a dedication to service , people , and financial stewardship <h> Strategy 12 <p> Enhance financial sustainability through new revenue and efficient use of existing revenue . <p> In 2012 , the launch year of the strategic planning process , UC San Diego reached a critical turning point . For the first time in university history , our students contributed more than the State of California to the cost of their education , with state funds amounting to less than 7 percent of the university 's total revenue . Despite a financial base supported by diversified revenue streams and $3.4 billion in total revenues in 2012 , threats to long-term financial sustainability loom . Further clouding the picture are recent federal government cuts to awarded funding through sequestration , increased costs due to new governmental regulations and reporting requirements , and the likelihood of additional costs related to new mandatory government programs . <p> In 2012 , the medical center accounted for 31 percent of revenues , @ @ @ @ @ @ @ @ @ @ 11 percent , other educational/medical programs 11 percent , state funding for education less than 7 percent , and auxiliary services and other revenue 7 percent . The challenge going forward is to use our existing revenue more efficiently while pursuing new funding streams to support our students , research , and patient care . <p> In order to enhance the university 's financial sustainability , accountability and transparency , we hired two new vice chancellors " a Vice Chancellor and Chief Financial Officer , and a Vice Chancellor for Advancement . <p> New revenue and efficiencies : Identify , review , and implement best practices as well as new revenue ideas , paradigms , and opportunities for revenue generation . <p> As our institution matures and grows as an education , research , and public service enterprise , funding needs for the critical infrastructure , maintenance , repair , and other key aspects of our support and service framework are rising . Funding requirements in this category have grown beyond the current funding capacity , leading to deferred maintenance and other actions that could have a negative , long-term @ @ @ @ @ @ @ @ @ @ If the university is to thrive under such profound resource constraints , we must identify and implement long-term strategies that offer financial flexibility , generate new and improved funding streams , further diversify our funding , and create organizational agility . Our newly established Standing Committee on Service- and People-Oriented Culture will assess employee training and development ; processes and policies ; current technologies ; financial systems and reporting ; and organizational and performance controls . The committee will also look at ways to centralize services in order to increase efficiencies . <p> UC San Diego has a long history of proactive and out-of-the-box thinking when it comes to addressing ways to achieve administrative efficiencies while meeting the requirements of our stakeholders " from our faculty and students , to governmental funding agencies . In an effort to generate more net revenue , we will evaluate our existing revenue sources in terms of efficiency and opportunity for growth . This is not an effort to cut costs at the expense of service , but a holistic assessment of how to deliver the necessary level of service while identifying opportunities to @ @ @ @ @ @ @ @ @ @ include exploring additional investment options , debt strategies , new payables and collecting receivables strategies , and improved forecasting and modeling tools . Additionally , we will seek increased support from public agencies by further leveraging our government relations activity across the university through external advocacy to enhance our direct and indirect revenues . We will also look for ways to expand our sales and service revenue and optimize the management of funds by improving flow , value , and impact . <p> Philanthropy : Define a multiyear campus fund-raising goal and strategy , linked to the Strategic Plan , to significantly enhance the endowment for scholarships , fellowships , and patient care . <p> It is vitally important that we raise money for critical imperatives such as access and inclusion , and at the same time create funding pools that are not encumbered or restricted . That is why we 're increasing the budget of university Advancement to $40 million over the next two years to ramp up for the largest capital campaign in UC San Diego 's history . An important metric for our long-term success is to @ @ @ @ @ @ @ @ @ @ for continued operations . <p> Building a campuswide culture of philanthropy across the university that is embraced by students , parents , alumni , faculty , and staff is vital to our long-term success . Faculty and staff are the backbone of UC San Diego , and their vote of confidence is a profound public endorsement from those who know the institution best . Private giving by these individuals demonstrates a personal and professional commitment to the university 's mission . Educating students about philanthropy and the importance of giving back sets the stage for lifelong relationships with their alma mater and private financial support . As part of this culture of philanthropy , we encourage students to become the next generation of donors once they join the ranks of UC San Diego alumni . <p> Budget model : Ensure the campus has employed best practices in financial tracking and resource allocations . <p> The university as a whole is dealing with significant budgetary uncertainty that impacts nearly all of our diversified revenue streams . We will undertake a comprehensive review and assessment of our current budget and resource allocation @ @ @ @ @ @ @ @ @ @ sustainable funding model that promotes transparency , efficiency , and agility . <h> Strategy 13 <p> Identify new models for excellent service that prioritize delivery to our stakeholders while addressing regulatory , compliance , and reporting requirements . <p> People : Emphasize employee development of critical skills in collaboration , team building , and innovative approaches . <p> Staff support our students , are integral to our research programs , and provide patient care in our hospitals and clinics . Our annual Staff@Work survey consistently indicates that the staff has high levels of satisfaction in their work . However , it is also clear that we must continue to improve the work environment , provide the appropriate tools , and offer opportunities for the staff to polish their existing skills and acquire new ones . <p> During the last several years , budgets have declined while workloads have increased , as a result of a higher volume of transactions , increased research activity , and a profusion of new regulations . Faced with the dilemma of doing more with less , the university must continue to find ways to enhance @ @ @ @ @ @ @ @ @ @ dedicated staff and the university 's mission . At the top of the list are culture change , professional development , and compensation issues . <p> Culture change : We need to provide skill-building workshops and realign metrics and reward systems such as incentives for smart risk taking , as opposed to gate keeping . Leaders who will inspire staff members , challenge them to think critically , and empower them to act boldly are essential . <p> Professional development : A significant portion of our current training and development portfolio focuses on transactional tasks . However , in addition to technical know-how , our new environment requires critical thinking , emotional intelligence , and other key skills that have previously not been emphasized . As we ask the staff to rethink what they do and how and why they do it , and urge them to become more service-oriented , their ability to design and manage a defined project will become increasingly important . Encouraging staff to engage in innovation that challenges the status quo requires that we adjust our tolerance for risk . <p> Compensation : We need @ @ @ @ @ @ @ @ @ @ marketplace so that we continue to attract and retain the best and brightest staff . <p> Processes and policies : Assess structures , funding sources , and policies to develop new tactics to comply with all mandates and regulations . <p> Our processes and the need for reordering them reflect the complexity of our organizational structures , funding sources , and government rules and regulations . In light of this complexity , it is time to redefine our goals , reimagine our processes , and deliver an agile , supportive , and self-sustaining blueprint for the future . <p> Since a significant amount of administrative costs occur at the university department level , improving effectiveness must begin with an in-depth assessment of end-to-end processes from the departments to the central administration . This assessment must lead to a new twenty-first-century business model that has a primary focus on optimizing our processes to deliver business outcomes and provide customer-centric support that advance our research , teaching , and service priorities . Where we have or may have shared services , we will assess service quality and make sure that shared service @ @ @ @ @ @ @ @ @ @ technology , financial management and reporting systems , and sustainable energy are critical tools for operational efficiency . Historically , UC San Diego effectively used technology as an enabling tool to automate manual processes , reduce transaction time , improve visibility , and offset personnel reductions . To effectively address current and future needs , however , we must exploit technology more strategically " and not just to automate processes . To catalyze transformational change , we must assess and optimize our existing structures and processes prior to implementing new technology . <p> Modernizing our university information systems is a top priority . We have allocated $6 million to enhance our IT administrative efficiencies and maintain a supportive campus infrastructure . Employing common standards and protocols would allow us to fully leverage the potential of emerging technology . We need the ability to coordinate , analyze , and report on data between student information systems , financial information systems , advancement information systems , and research administrative information systems . Changes in data delivery to staff , faculty , and students must be accommodated . Additionally , deploying big data @ @ @ @ @ @ @ @ @ @ and administrative decision making . <p> The university 's financial systems are robust and reliable , but often do not provide the type of integration and flexible reporting demanded by our current and future business environment . Modernizing our legacy information systems , including the financial information systems component , would enable us to significantly improve financial reporting speed and capability by integrating and normalizing data . Improving our legacy information systems will also allow us to implement flexible tools for financial reporting . <p> We will consider two alternatives for upgrading our legacy information systems holistically : replacing the existing information systems and using Software as a Service ( SaaS ) , or porting existing information systems to new platforms as many other top research universities have done . <p> UC San Diego iscommitted to sustainable energy and will continue to focus on alternate energy and green technology solutions that strategically increase efficiency and directly benefit the campus . We are targeting zero waste by 2020 and climate neutrality by 2025 by leveraging collaborations among our university staff , faculty , students , and researchers . 
@@97506360 @1706360/ <h> Tools to MSn Sequence and Document the Structures of Glycan Epitopes <p> AbstractSequential disassembly ( MSn ) has been applied to fully characterise and document native samples containing glycan epitopes with their synthetic analogues . Both sample types were prepared by methylation , solvent phase extracted , directly infused and spatially resolved . Product ions of all samples were compiled and contrasted using management tools prepared for the fragment ion library . Each of the epitopes was further disassembled to confirm the multiple structural isomers probable within component substructures of linkage and branching . All native samples tested proved to be matched with their synthetic analogues and reasonably identical on either linear or cylindrical ion traps . Not surprisingly , spectra of mixed epitopes fragment independently , being uninfluenced by similarities . The approach has been coupled with computational tools for data handling and presentation . 
@@97506361 @1706361/ <h> An introduction to crystal physics <h> Description of the physical properties of crystals <h> Ervin Hartmann <h> Teaching aims <p> The teaching aim of this booklet is to give an overall view about crystal physics without the separate discussion of the individual physical properties of crystals . It may be called Essential Crystal Physics ' . Crystal physics is based on physics , crystallography and mathematics . Therefore this booklet is suitable for advanced undergraduates or initial postgraduates who are already acquainted with the elements of solid state physics , of crystallography and of vector calculations . According to the author 's experience , 4-6 hours are sufficient to form a true notion of the essentials of crystal physics for non-specialists in that field . <p> The International Union of Crystallography is a non-profit scientific union serving the world-wide interests of crystallographers and other scientists employing crystallographic methods . 
@@97506365 @1706365/ <h> OpenTopography Bulk Download <p> This resource provides bulk downloads of raster data from datasets hosted by OpenTopography . It is designed for advanced users seeking to download large amounts of data quickly and who have the bandwidth , expertise , and software necessary to manage the gigabytes of data available . We expect that users accessing bulk data have the ability to automate downloads from the command line via tools such as wget , curl , or a download manager such as DownThemAll <p> Each dataset directory contains all of the raster data for a given data collection and are typically organized into various layers type for download efficiency . When possible , a README file in the directory describes the dataset organization and other important information . Metadata for all datasets can be accessed via the OpenTopography Metadata page and users should consult the collection survey report for additional information on data organization , file naming conventions , coordinate systems , etc . <p> By accessing data via OpenTopography you agree to acknowledge OpenTopography and the dataset source as specified in the dataset metadata and on OpenTopography @ @ @ @ @ @ @ @ @ @ other materials produced using these data . <h> Point Reyes , CA : Landscape Response to Tectonics <p> NCALM Seed . PI : Kristin Morell , Penn State University . The survey area is 76.31 square kilometers , located 26 kilometers northwest of San Francisco , California in the Point Reyes National Seashore . This survey was flown as a part of Seed Money Survey Campaign that took place in Aug-Sept 2009 . This section was surveyed over two days : Sept 8 , 2009 and Sept 9 , 2009 . Lidar data was collected to investigate the landscape response to tectonics by studying the coupling between channels and hillslopes during transient adjustment to an increase in uplift rate . <p> ( * ) The SRTM service requires each side of the bounding box be greater than approximately 250 meters . The SRTM service is now disabled for this job . To enable it again , please try a larger bounding box . <p> These options allow users to describe and keep track of their jobs . Information entered below is recorded along with other job parameters in your personal @ @ @ @ @ @ @ @ @ @ registered OpenTopography users ) . 
@@97506370 @1706370/ <h> Personal Structure Annotations With MyPDB <p> MyPDB now allows you to save personal annotations and notes on the structure summary tab of any entry . You can also " tag " an entry to add it to a " favorites list " . A summary page ( click here ) let 's you easily access all of these tagged structures and annotations . All information stored using this new feature will be saved in your MyPDB account . <h> Transporter Classification Database Browser <p> The Transporter Classification Database ( www.tcdb.org ) organizes membrane transport proteins using the Transporter Classification ( TC ) system . It is similar to the Enzyme Commission ( EC ) system , but incorporates both functional and phylogenetic information . Descriptions , TC numbers , and examples of over 600 families of transport proteins are provided . Transport systems are classified on the basis of five criteria , and each of these criteria corresponds to one of the five numbers or letters within the TC number for a particular type of transporter . <p> The RCSB PDB database can now be browsed using this TC @ @ @ @ @ @ @ @ @ @ Database interface to browse TC superfamilies , view the number of associated PDB structures , and explore for the individual structures . <p> Advanced Search can be used to combine searches for transporter classifications with queries for other specific types of data . Select the Transporter Classification Browser option from the Biology section of the Advanced Search pull-down menu . <h> Structural Biology Knowledgebase Widget <p> The Structural Biology Knowledgebase ( SBKB ) is a free , comprehensive resource that integrates the results of structural biology and genomics efforts with other publicly available biological information to facilitate further research . A new widget on every RCSB PDB Structure Summary page links the related information at the SBKB . It asynchronously loads data about available models , protein targets , related biological annotations , related clones and PepcDB related protocols with links to SBKB reports page . <p> The SBKB is part of the Protein Structure Initiative:Biology Network and funded by NIGMS . <h> Ligand Download Page <p> The Ligand Download page is available from the left-hand menu of the web page , from the " Display/Download " drop-down menu of @ @ @ @ @ @ @ @ @ @ results page , and from the Ligand Summary page . It can be used to download Structure Data files ( SDF ) for one or more ligands . <p> The Structure Data File ( SD-Format V2000 ) is one of the most popular chemical structure file formats ( Dalby et al. , J. Chem . Inf . Comput . Sci. 1992 , 32 : 244-255 ) . A single SDF file can comprise multiple small molecules and associated data . <h> Ligand Images on Structure Summary Page <p> To improve the usability of the ligand data the Ligand Chemical Component , External Ligand Annotations and Modified Residues boxes now include an image of each ligand that is present . You can hover over the image to see a larger preview , or if you click on it , it opens the large version of the image in a shadowbox and provides you with a link to download it . <h> Integration of Binding Affinity Data From BindingMOAD Database <p> Binding affinity data from BindingMOAD ( http : //www.bindingmoad.org/ ) have been integrated with the RCSB PDB website , after the @ @ @ @ @ @ @ @ @ @ orange Ligand External Annotations widget in the Structure Summary page lists available binding affinity data from both BindingDB and BindingMOAD . The data are linked to BindingDB and BindingMOAD details pages , respectively . <p> The BindingMOAD data can also be queried through Advanced Search to find PDB entries of protein-ligand complexes with associated binding affinity data . IC50 , EC50 , Ki , and Kd must be specified in nM , thermodynamic data ? G , ? H , -T ? S in kJ/mol , and Ka in M-1 . <h> Binding Affinity <h> Structural Genomics Centers <p> A predefined report for Structure Genomics Centers is accessible from the Generate Reports drop-down list . Select Structure Genomics Centers under Summary Report on the result page . A new category that includes the Structure Genomics Center details has been added to custom reports as well . <h> Drill-down by Enzyme Classification Number <p> The pie charts available from the top of each results page now include summary statistics and drill-down links based on the Enzyme Classification system . The drill-downs are ideal for the EC system due to its hierarchical @ @ @ @ @ @ @ @ @ @ type of search ( text , sequence etc ) according to enzyme classes of interest in just a few mouse clicks . The summary charts provide a quick look at types of structures in the query results , and can be used to filter outliers or focus in on interesting results . <h> Drill-down by SCOP Domain <p> The SCOP drill-down interface is another summary pie chart option . It can give a quick look at the domains that are included in a result set , and can be used to restrict them in entries that contain SCOP domains of interest . <h> Two-Dimensional Macromolecule-Ligand Interaction Diagrams <p> The display of 2D macromolecule-ligand interaction diagrams is a new feature on the Structure Summary page . The diagrams , generated by PoseView , depict the structure diagram of the ligand and interacting residues . A thumbnail image is displayed in the Ligand Chemical Component widget under the " Interactions " header . By clicking on the thumbnail image , a full-size image will be displayed that may also be saved . 1452 @qwx861452 <h> High-resolution Image Generation <p> Simple Viewer , @ @ @ @ @ @ @ @ @ @ for saving high-resolution images in JPEG , PNG , and TIFF formats . The width and height of an image can be specified in pixels or in physical dimensions of inches and millimeters . This feature is useful for journal publications , where images of a particular size and resolution specified in Dots Per Inch ( DPI ) are required , and for high-resolution prints . <p> The Save Image dialog box is available from the File menu for the viewers listed above . The image to the left shows an example of the Save Image dialog box with custom settings for an image of size 11 x 8.5 inches with a print resolution of 300 DPI to be saved in the JPEG format . The image file type can be selected from the " Files of type " menu . <h> Improved alignment display <p> We have enhanced the display of sequence alignment results to color-code the alignment according to the degree of similarity . We are now also using a color palette which is more legible for color blind individuals . <h> Support for Chemical Shifts Files <p> @ @ @ @ @ @ @ @ @ @ deposition of chemical shift data will be mandatory when submitting NMR entries to the PDB . <p> Following the public release of structures deposited under this new policy , the associated chemical shifts files will be available for download from the Structure Summary page . In addition , the PDB Current Holdings Breakdown will list the number of structures in the PDB that have a chemical shifts data file . <h> Orange Color for External Resources <p> We have changed the way we display data that has been loaded from external resources by highlighting them with a new color . All external annotations are now contained in orange colored boxes instead of our regular blue ones to hightlight the difference between our primary and externally gathered data . 
@@97506371 @1706371/ <p> AbstractIt has now become increasingly clear that a complete atomic description of how biomacromolecules recognize each other requires knowledge not only of the structures of the complexes but also of how kinetics and thermodynamics drive the binding process . In particular , such knowledge is lacking for **25;2596;TOOLONG ( GAG ) complexes . Isothermal titration calorimetry ( ITC ) is the only technique that can provide various thermodynamic parameters-enthalpy , entropy , free energy ( binding constant ) , and stoichiometry-from a single experiment . Here we describe different factors that must be taken into consideration in carrying out ITC titrations to obtain meaningful thermodynamic data of protein-GAG interactions . 
@@97506372 @1706372/ <h> Join the Triton Shared Computing Cluster ! <h> Program in a nutshell <p> Researchers use equipment purchase funds to buy compute servers ( i.e. nodes ) that will be operated as part of the cluster . An additional infrastructure fee covers the purchase of shared components ( racks , network switches , cables , etc . ) . <p> Participating researchers may then have dedicated use of their purchased nodes , or they may run larger computing jobs by sharing idle nodes owned by other researchers.1 The main benefit is access to a much larger cluster than would typically be available to a single lab . <p> Researchers also pay an annual operations fee for each of their purchased nodes . This fee covers labor to maintain the system , utilities , software licenses , etc . Currently , the UCSD Administration substantially subsidizes this fee for UCSD researchers . Other campus administrations may wish to consider subsidizing the fee for their researchers . <p> Researchers may run jobs in a glean queue2 which does not count against their annual allocation of computing time . <p> Participation in @ @ @ @ @ @ @ @ @ @ the duration of the equipment warranty . Researchers may leave their nodes in the system for a fourth year , though equipment failing during this period may not be repaired . <p> Researchers may remove and take possession of their nodes at any time ; once equipment is removed from the cluster it may not be returned.3 <h> Program Features and Benefits <p> Access to a much larger cluster than most labs could typically afford <p> Professionally administered no need to have postdocs or grad students maintain your computing system <p> Access to a community of researchers and users that can share tips and information <h> Current Status <p> As of May 2015 , there are 14 groups and 230 users participating in the program , for a total of 170 nodes ( 3,000 processors ) and 80+ teraflops of computing power . <p> Participating researchers/labs are working in the fields of engineering , chemistry , genomics , oceanography , physics , and many others . <h> Participation/Purchase Opportunity <p> As of January 2015 , TSCC has selected a new vendor , AdvancedHPC , to provide updated technology for the @ @ @ @ @ @ @ @ @ @ we have received very attractive pricing on new purchases . As a result , we are seeking researchers who wish to join the program or increase their level of participation . <p> This is an ideal opportunity for faculty with startup packages or equipment purchase funds to gain access to a significant computing resource at attractive pricing . <h> Warranty <p> Three years , next business day on-site service <h> Notes <p> Researchers receive an annual allocation of computing time equivalent to running their purchased nodes continuously . Running larger jobs on shared nodes would draw down the allocation proportionately faster . <p> Glean queue jobs are subject to immediate preemption so should be short jobs or capable of being restarted . <p> Researchers purchasing a number of nodes that is not a multiple of four may need to purchase a separate carrier chassis for their nodes in the event they wish to remove the nodes and make them operational elsewhere . <p> Currently , the UCSD IDI program subsidizes a substantial portion of the annual operating fee for UCSD researchers . Researchers participating from other UC campuses may wish @ @ @ @ @ @ @ @ @ @ a subsidy as well . 
@@97506374 @1706374/ <h> OpenTopography Bulk Download <p> This resource provides bulk downloads of raster data from datasets hosted by OpenTopography . It is designed for advanced users seeking to download large amounts of data quickly and who have the bandwidth , expertise , and software necessary to manage the gigabytes of data available . We expect that users accessing bulk data have the ability to automate downloads from the command line via tools such as wget , curl , or a download manager such as DownThemAll <p> Each dataset directory contains all of the raster data for a given data collection and are typically organized into various layers type for download efficiency . When possible , a README file in the directory describes the dataset organization and other important information . Metadata for all datasets can be accessed via the OpenTopography Metadata page and users should consult the collection survey report for additional information on data organization , file naming conventions , coordinate systems , etc . <p> By accessing data via OpenTopography you agree to acknowledge OpenTopography and the dataset source as specified in the dataset metadata and on OpenTopography @ @ @ @ @ @ @ @ @ @ other materials produced using these data . <p> NCALM Seed . PI : Ryan Wood , San Jose State University . The survey area consists of a 49 square kilometer polygon located 145 miles northeast of Boise , Idaho , spanning the border of the Payette and Bitterroot National Forests . Lidar data were collected to investigate transient hillslope response to knickpoint migration up a watershed . <p> ( * ) The SRTM service requires each side of the bounding box be greater than approximately 250 meters . The SRTM service is now disabled for this job . To enable it again , please try a larger bounding box . <p> These options allow users to describe and keep track of their jobs . Information entered below is recorded along with other job parameters in your personal lidar Job archive accessed via myOpenTopo ( available only to registered OpenTopography users ) . 
@@97506377 @1706377/ <h> Guide to Interviewing <p> Asking the right questions in interviews can help supervisors get the answers that will enable them to select the right candidate for the right job . <h> Goals for Interview Questions <p> Asking the right interview questions should give you , the supervisor , the following information : <p> Confirmation of the candidate 's education , training , and experience listed in the resume <p> Information about performance and accomplishments <p> Indication of the candidate 's compatibility with the culture ( e.g. work pace , work style ) of your department <p> Reasons behind the candidate 's desire to change jobs <h> Questions NOT to Ask <p> Avoid unintentionally discriminatory questions that violate equal opportunity laws . Questions to avoid include -- but are not limited to -- the following : <p> What year did you graduate from high school ? <p> Where were you born ? <p> Where did you learn a foreign language ? <p> What are your child-care arrangements ? <p> What are your religious practices ? <p> How many days did you miss because of illness last year ? <p> @ @ @ @ @ @ @ @ @ @ been arrested ? <h> Questions TO Ask <p> Behavioral interviewing techniques probe beyond superficial answers , requiring candidates to assess themselves and recall examples of behavior . Most behvioral questions are formed as either self-appraisal queries or situational queries , as shown in the examples below : <p> If you had the choice of working in a job with peaks and valleys in the workload or a job with a steady volume of work , which would you choose and why ? <p> Tell me about a time when you had to make a critical decision in your supervisor 's absence . How did you handle it ? <p> Open-ended questions : These questions require an explanation from the candidate . Open-ended questions begin with words such as " what " , " why " , " how " , " describe " , and " explain " . For example : <p> What is the greatest asset you will bring to this job ? <p> Describe the most important thing you do at your current job . <p> Tell me about the last time you had a short deadline @ @ @ @ @ @ @ @ @ @ had to adapt to your job 's changing needs ? <p> Neutral questions do not reveal a bias toward an acceptable or correct answer . For example : <p> If you had to choose between one extreme or the other , would you want a supervisor who leaves you alone to get your work done and only wants to hear from you if there 's a problem , or would you prefer someone who meets with you regularly to help you focus on your goals for the day or week ? <p> Use questions that can be answered with a " yes " or " no " to confirm information you already have . In general , use these types of questions sparingly because they do n't add new information . For example : <p> Were you with XYZ company 10 years before you relocated to San Diego ? <p> Follow-up questions probe the candidate 's attitudes or delve further into the issue . For example , you may start with a broad question : <p> " What are your responsibilities as the administrative assistant ? " A candidate may @ @ @ @ @ @ @ @ @ @ phones , type , keep the calendar , arrange travel , and file documents . Although this information confirms the resume , it does not give information about the relationship with the supervisor , consequences of actions , or pride in work output . To get this kind of information , ask follow-up questions , such as : <p> What aspects of your job are most crucial ? <p> How many hours a week do you find it necessary to work in order to get your job done ? 
@@97506378 @1706378/ <h> Three Investigators to Participate in Glycobiology Workshop <p> July , 2015 Three investigators from the Cleveland Clinic PEG ( Drs . Vincent Hascall , Ron Midura , and Mark Lauer ) traveled to the Cardio-PEG at Johns Hopkins University to participate in a 2-week long Glycobiology Workshop hosted by Drs . Jerry Hart and Natasha Zachara . On Tuesday July 28th , Dr. Lauer set up a half-day hands-on workshop to demonstrate a technology for analyzing glycosaminoglycan structure known as Fluorophore-Assisted Carbohydrate Electrophoresis ( FACE ) . Dr. Midura provided a morning lecture covering the underlying principles of FACE analysis of glycosaminoglycans . Dr. Hascall gave an evening keynote lecture on the roles of hyaluronan in diabetes pathology and stem cell differentiation . 
@@97506383 @1706383/ <p> Abstract We present data that hyaluronan ( HA ) polysaccharides , about 14-86 monosaccharides in length , are capable of accepting only a single heavy chain ( HC ) from inter-+-inhibitor via transfer by tumor necrosis factor-stimulated gene 6 ( TSG-6 ) and that this transfer is irreversible . We propose that either the sulfate groups ( or the sulfation pattern ) at the reducing end of the chondroitin sulfate ( CS ) chain of bikunin , or the core protein itself , enables the bikunin proteoglycan ( PG ) to accept more than a single HC and permits TSG-6 to transfer these HCs from its relatively small CS chain to HA . To test these hypotheses , we investigated HC transfer to the intact CS chain of the bikunin PG , and to the free chain of bikunin . We observed that both the free CS chain and the intact bikunin PG were only able to accept a single HC from inter-+-inhibitor via transfer by TSG-6 and that HCs could be swapped from the bikunin PG and its free CS chain to HA . Furthermore , a @ @ @ @ @ @ @ @ @ @ a single heavy chain . We discuss explanations for these observations , including the intracellular assembly of inter-+-inhibitor . In summary , these data demonstrate that the sulfation of the CS chain of bikunin and/or its core protein promote HC transfer by TSG-6 to its relatively short CS chain , although they are insufficient to enable the CS chain of bikunin to accept more than one HC in the absence of other cofactors. 
@@97506384 @1706384/ <p> PostgreSQL uses one semaphore per allowed connection ( maxconnections ) and allowed autovacuum worker process ( autovacuummaxworkers ) , in sets of 16 . Each such set will also contain a 17th semaphore which contains a " magic number " , to detect collision with semaphore sets used by other applications . The maximum number of semaphores in the system is set by SEMMNS , which consequently must be at least as high as maxconnections plus autovacuummaxworkers , plus one extra for each 16 allowed connections plus workers ( see the formula in Table 17-1 ) . The parameter SEMMNI determines the limit on the number of semaphore sets that can exist on the system at one time . Hence this parameter must be at least ceil ( ( maxconnections + autovacuummaxworkers + 4 ) / 16 ) . Lowering the number of allowed connections is a temporary workaround for failures , which are usually confusingly worded " No space left on device " , from the function semget. maxconnections = 2000 autovacuum = off ( ( maxconnections + autovacuummaxworkers + 4 ) / 16 ) 2000 + @ @ @ @ @ @ @ @ @ @ of semaphores and semaphore sets which can be created enter : cat /proc/sys/kernel/sem This returns 4 numbers indicating : SEMMSL - The maximum number of semaphores in a sempahore set SEMMNS - The maximum number of sempahores in the system SEMOPM - The maximum number of operations in a single semop call SEMMNI - The maximum number of sempahore sets List # ipcs -s ------ Semaphore Arrays -------- key semid owner perms nsems 0x00000000 0 root 600 1 0x00000000 32769 root 600 1 0x002fa327 65538 root 666 2 # ipcs -s -i 65538 Semaphore Array semid=65538 uid=0 gid=0 cuid=0 cgid=0 mode=0666 , accessperms=0666 nsems = 2 otime = Mon Sep 23 11:07:23 2013 ctime = Mon Sep 23 10:36:26 2013 semnum value ncount zcount pid 0 1 0 0 3049 1 100 0 0 3049 check for pid 3049 remove Semaphore # /usr/bin/ipcrm -s 65538 <p> The site maintainers ( andreas , sbliven , pcicotti ) need to use sudo to perform actions as the maintainer user . The maintainer user owns all files pertaining to the site . sudo -u maint505712 This arrangement allows us to add and remove @ @ @ @ @ @ @ @ @ @ breaking ownership dependencies . The ability to run commands as maint505712 is only valid on ion-21-13 for andreas and sbliven , and on both ion-21-13 and ion-21-16 for pcicotti . Apache runs as a frontend to tomcat . As such , apache contains a mapping of apache URLs to tomcat urls . ( See **31;2623;TOOLONG , ) The user pcicotti may edit this file on ion-21-13 and ion-21-16 using vi , as well as restart httpd . Tomcat webapps are served from /scratch/tomcat// Auto-deployment and auto-unpacking of war files is disabled . Not to worry . They 're just zip files . Something like this should work : sudo -umaint505712 mkdir /scratch/tomcat/hello cd /scratch/tomcat/hello sudo -umaint505712 unzip **28;2656;TOOLONG <p> ln -s /osg /projects/ion-osg The NFS directory is also accessible now on the login nodes , but that mount is over the 1GbE interface <p> security <p> I had to adjust the iptables rules for ion-21-11 and fix the rules in /etc/hosts. * : ssakai@gordon-fe1 nodes$ rocks add firewall host=ion-21-11 action=ACCEPT chain=INPUT network=ibnet1 protocol=tcp service=2049 **27;2686;TOOLONG flags= " ' -s 10.7.0.0/255.255.0.0 ' " host list : ' ion-21-11 ' checking for @ @ @ @ @ @ @ @ @ @ network=ibnet1 protocol=tcp service=111 **31;2715;TOOLONG flags= " ' -s 10.7.0.0/255.255.0.0 ' " host list : ' ion-21-11 ' checking for host ion-21-11 ssakai@gordon-fe1 nodes$ rocks add firewall host=ion-21-11 action=ACCEPT chain=INPUT network=ibnet1 protocol=udp service=2049 **27;2748;TOOLONG flags= " ' -s 10.7.0.0/255.255.0.0 ' " host list : ' ion-21-11 ' checking for host ion-21-11 ssakai@gordon-fe1 nodes$ rocks sync host firewall ion-21-11Error running : ssh -T -x ion-21-11 " /sbin/service iptables restart &gt; /dev/null 2&gt;&amp;1 " ion-21-11 returned error code 256 ssakai@gordon-fe1 nodes$ Also made the following adjustments to exports on ion-21-11 : - export /osg/export instead of /osg . We probably do n't want to export squid 's stuff . allow : ddp169 to do stuff in their exported directory , do n't allow them to mess with squid . - chown : ddp169 /osg/export - chmod g+ws /osg/export - chown root:root /osg - chmod 755 /osg - export WITH root squash . Do not export anything with norootsquash unless we talk about it first . Finally , please make the following adjustments : - ALWAYS mount with options nodev , nosuid We were noticed by the people running central services at CERN and @ @ @ @ @ @ @ @ @ @ opened ( one per squid process ) so that they can monitor the squid performance ( they will also tell us if we are torturing them in a wrong way ) . Here are the instructions for this : LONG ... I 've added five more squid processes , for a total of six . I 've also enabled the SNMP monitor port for CERN 's IPs , though I have no way of testing whether their monitoring works correctly or not <p> gidftp <p> Scott : gird-map Looks like the first glance was correct . We do n't have that CA in our trusted users list . The XSEDE CA tarball that we use is supposed to be mostly the same as the igtf distribution , so I guess this is one of those things that falls outside of " mostly the same " . I 've added the CERN Root CA and CERN Trusted Certification Authority certs to our trust anchors . <p> software <p> CMS software squid listening on 10.7.104.112:3128 <p> OpenTopology ( ion-21-13 ) Chaitan <p> DB2 ( removed ! ! ! ) <p> fixpack 8 @ @ @ @ @ @ @ @ @ @ installations , log on as root . For non-root installations , log on with the user I 'd that owns the non-root installation . 2 . Change to the directory that contains the fix pack image . 3 . Launch the installation by issuing the installFixPack command . For example , . /installFixPack -b DB2DIR where DB2DIR is the location of the DB2 database products that you want to update . In clustered environments where some instances are not mounted , add the -f hastandbyignore option . For example , . /installFixPack -b DB2DIR -f hastandbyignore 
@@97506385 @1706385/ <h> OpenTopography Bulk Download <p> This resource provides bulk downloads of raster data from datasets hosted by OpenTopography . It is designed for advanced users seeking to download large amounts of data quickly and who have the bandwidth , expertise , and software necessary to manage the gigabytes of data available . We expect that users accessing bulk data have the ability to automate downloads from the command line via tools such as wget , curl , or a download manager such as DownThemAll <p> Each dataset directory contains all of the raster data for a given data collection and are typically organized into various layers type for download efficiency . When possible , a README file in the directory describes the dataset organization and other important information . Metadata for all datasets can be accessed via the OpenTopography Metadata page and users should consult the collection survey report for additional information on data organization , file naming conventions , coordinate systems , etc . <p> By accessing data via OpenTopography you agree to acknowledge OpenTopography and the dataset source as specified in the dataset metadata and on OpenTopography @ @ @ @ @ @ @ @ @ @ other materials produced using these data . <h> Piercy , CA : Response of Bedrock River to Base Level Lowering , SF Eel River <p> NCALM Seed Project . PI : Melissa Foster , Humboldt State University . The survey area consists of a 44 square kilometer polygon located next to Piercy , CA . The data were collected to study the response of a bedrock-controlled river and its tributaries to base level lowering . The study investigates the knickpoints and channel response of the South for Eel River . <p> ( * ) The SRTM service requires each side of the bounding box be greater than approximately 250 meters . The SRTM service is now disabled for this job . To enable it again , please try a larger bounding box . <p> These options allow users to describe and keep track of their jobs . Information entered below is recorded along with other job parameters in your personal lidar Job archive accessed via myOpenTopo ( available only to registered OpenTopography users ) . 
@@97506386 @1706386/ <p> The Kepler Workflow Environment collaboratively developed by the WorDS team has been the most cited ( according to Web of Science ) and one of the most well-adopted and influential scientific workflow products with hundreds of thousands of downloads . <p> Innovative techniques developed by the WorDS team on utilization of distributed data parallel ( DDP ) programming patterns as a programming model for big data scientific workflows are a first in applying such techniques to scientific workflow development . <p> Long-term leadership of the WorDS team on application of diverse distributed computing techniques to scientific workflows within the Kepler environment has resulted in a number of international collaborations with modules contributed by the partners as well as the WorDS team . <p> Our provenance work for modeling and utilization of provenance of e-science applications was influential in the discussions for producing standards for provenance . <p> The research and development efforts at the WorDS Center is built around application-driven computing to advance computational practices involving process management and scientific workflows . To achieve this goal , our current research encompasses diverse areas of scientific computing and data @ @ @ @ @ @ @ @ @ @ distributed computing ranging across clusters to clouds , bioinformatics , observatory systems , conceptual data querying , and software modeling . These sub-categories of our research are explained in more detailed at this page. - <h> Distributed Data-Parallel ( DDP ) Task Execution <p> DDP patterns such as MapReduce have become increasingly popular for building - Big Data applications . We integrated Map , Reduce and other Big Data programing patterns , such as Match , CoGroup and Cross , into the Kepler Scientific Workflow System to execute Big Data workflows on top of Hadoop and Stratosphere systems . The advantages of the integration include : ( i ) its heterogeneous nature in which Big Data programming patterns are placed as part of other workflow tasks ; ( ii ) its visual programming approach that does not require scripting of Big Data patterns ; ( iii ) its adaptability for execution of distributed data-parallel applications on different execution engines ; ( iv ) our analysis on how to get the best application performances by configuring different factors , including selected data parallel pattern and data parallel number . <h> Data @ @ @ @ @ @ @ @ @ @ of using scientific workflow systems is the ability to make provenance collection a part of the workflow . Such provenance includes not only the standard data lineage information but also information about the context in which the workflow was used , execution that processed the data , and the evolution of the workflow design . We propose a framework for data and process provenance in the Kepler Scientific Workflow System . Based on the requirements and issues related to data and workflow provenance in a multi-disciplinary workflow system , we introduce how generic provenance capture can be facilitated in Kepler 's actor-oriented workflow environment . We describe the usage of the stored provenance information for efficient rerun of scientific workflows . We also study provenance of Big Data workflows and its effects on workflow execution performance by recording the provenance of MapReduce workflows . <h> Workflow Fault Tolerance <p> We build fault tolerance framework for Kepler-based scientific workflows to support implementation of a range of comprehensive failure coverage and recovery options . The framework is divided into three major components : ( i ) a general contingency Kepler actor that @ @ @ @ @ @ @ @ @ @ ( ii ) an external monitoring module that tracks the underlying workflow components , and monitors the overall health of the workflow execution , and ( iii ) a checkpointing mechanism that provides smart resume capabilities for cases in which an unrecoverable error occurs . This framework takes advantage of the provenance data collected by the Kepler-based workflows to detect failures and help in fault-tolerance decision making . <h> Workflow as a Service ( WFaaS ) <p> We build WFaaS service model to support workflow publishing , query and execution in the Cloud . WFaaS enables composition of multiple software packages and services based on the logic encapsulated within a workflow service . WFaaS facilitates a service and management environment for flexible application integration via workflows . Utilizing other cloud services , such as IaaS and DaaS , WFaaS provides interfaces to get proper data , software packages , and VMs for workflow execution . Our WFaaS architecture allows for management of multiple concurrently running workflows and virtual machines in the Cloud . Based on this architecture , we research and benchmark heuristic workflow scheduling algorithms for optimized workflow execution @ @ @ @ @ @ @ @ @ @ experiment with multiple algorithms to find proper configurations for reduced cost and increase price/performance ratio without affecting much performance . <h> Sensor Data Management and Streaming Data Analysis <p> Sensor networks are increasingly being deployed to create field-based environmental observatories . As the size and complexity of these networks increase , many challenges arise including monitoring and controlling sensor devices , archiving large volumes of continuously generated data , and the management of heterogeneous hardware devices . The Kepler Sensor Platform ( SensorView ) , an open-source , vender- neutral extension to a scientific workflow system for full-lifecycle management of sensor networks . This extension addresses many of the challenges that arise from sensor site management by providing a suite of tools for monitoring and controlling deployed sensors , as well as for sensor data analysis , modeling , visualization , documentation , archival , and retrieval . An integrated scheduler interface has been developed allowing users to schedule workflows for periodic execution on remote servers . We evaluate the scalability of periodically executed sensor archiving workflows that automatically download , document , and archive data from a sensor site @ @ @ @ @ @ @ @ @ @ scientific workflow execution in distributed environments including HPC Cluster , Grid and Cloud , we develop a high-level distributed execution framework for dataflow/workflow applications , and the mechanisms to make the framework easy-to-use , comprehensive , adaptable , extensible , and efficient . To apply and refine the high-level framework based on different execution environments or requirement characteristics , we propose several concrete distributed architectures/approaches . The first one is the distributed data-parallel architecture explained above . The second one , named Master-Slave , can accelerate parameter sweep workflows by utilizing ad-hoc network computing resources . For a typical parameter sweep workflow , this architecture can realize concurrent independent sub-workflow executions with minimal user configuration , allowing large gains in productivity with little of the typical overhead associated with learning distributed computing systems . The third one is to facilitate e-Science discovery using scientific workflows in Grid environments . It presents a scientific workflow based approach to facilitate computational chemistry processes in the Grid execution environment by providing features such as resource consolidation , task parallelism , provenance tracking , fault tolerance and workflow reuse . The fourth one is @ @ @ @ @ @ @ @ @ @ Cluster and Cloud environments . It discusses how to interact with traditional cluster resource managers like Oracle Grid Engine ( formerly SGE ) and recent ones like Hadoop using a dataflow-based scheduling approach to balance physical/virtual compute resource load for data-parallel workflow executions . 
@@97506390 @1706390/ <p> Collection Overview : NCALM Seed . PI : Kristin Morell , Penn State University . The survey area is 76.31 square kilometers , located 26 kilometers northwest of San Francisco , California in the Point Reyes National Seashore . This survey was flown as a part of Seed Money Survey Campaign that took place in Aug-Sept 2009 . This section was surveyed over two days : Sept 8 , 2009 and Sept 9 , 2009 . Lidar data was collected to investigate the landscape response to tectonics by studying the coupling between channels and hillslopes during transient adjustment to an increase in uplift rate . 
@@97506395 @1706395/ <p> The first issue that needs to be addressed regarding Monte Carlo K calculations is that of source convergence . Before accumulating any K tally data , enough batches must be performed and discarded to allow the source neutron distribution to attain the fundamental mode Whi71 . Good spatial sampling is important for attaining and maintaining the fundamental eigenmode . Maintaining the fundamental eigenmode may be difficult , especially for systems with high dominance ratio , due to the batch-to-batch correlations in the spatial distributions of fission neutrons . This is due to the fact that for systems with high dominance ratio , there is less neutron communication between different regions of the system and spatial correlations between batches may prevail . <p> The second issue deals with the assumption that the batch eigenvalues are independent . For systems with a high dominance ratio , batch-to-batch correlations among fission neutron distribution exist , and this assumption is invalid . This results in an underestimation of the standard deviation Moo76 , Gel90a . Various studies Mac73 , Gas75 have been done to account for this phenomenon . <p> The third @ @ @ @ @ @ @ @ @ @ versus the optimum number of neutrons per batch Lew93 , as illustrated in figures 2.2 and 2.3 . Figure 2.2 pertains to the case where a large number of neutrons per batch has been followed for a few batches , while figure 2.3 shows the case where a small number <p> Figure : Few Batches with Large Number of Histories per Batch . <p> of neutrons per batch was followed for a large number of generations . Note that in figure 2.2 , the fundamental eigenmode is not reached , even though the variance in the eigenvalue estimate is small . On the other hand , figure 2.3 shows that the fundamental <p> Figure : Large Number of Batches with Few Histories per Batch . <p> eigenmode is reached , albeit with a large variance in the eigenvalue estimator . <p> Lastly , we address the issue of bias in Monte Carlo eigenvalue calculation Gel91 . The bias is a result of the fission source normalization done after each batch . Numerical experiments Bow83 suggest that biases in fluxes and eigenvalues are negligibly small for most practical cases . The @ @ @ @ @ @ @ @ @ @ negligible and smaller than a single standard deviation , also has strong theoretical support Gel90b . A detailed analysis of bias Bri86 , Gel94 gives the following equation ; <p> ( 29 ) <p> where , K0 is the biased Monte Carlo eigenvalue computed by averaging over generations or batches ; is the true eigenvalue ; is the true variance in K0 ; is the apparent variance in K0 , computed by assuming that estimates of K from different generations are independent ; and NG is the active number of generations . Thus the bias , , is given by , <p> ( 30 ) <p> and the relative bias ( compared to the standard deviation ) is bounded by <p> ( 31 ) <p> For typical eigenvalue calculations one prefers , and hence if NG is less than 800 the bias will be less that a standard deviation Gel90a . It should be noted that the bias itself is independent of NG since is proportional to . Therefore , an adequate number of histories per batch should be tracked ; and a number of batches should be simulated which @ @ @ @ @ @ @ @ @ @ that the bias in the estimate of K is a significant fraction of the standard deviation . 
@@97506397 @1706397/ 1450 @qwx861450 <h> Teams will Model , Measure , and Monitor Internet Infrastructure <p> Published December 16 , 2015 <p> Researchers and students from various institutions around the world are invited to participate in a " hackathon " hosted by the Center for Applied Internet Data Analysis ( CAIDA ) at the San Diego Supercomputer Center ( SDSC ) , and jointly organized with other universities and institutions . The goal is to promote the development of tools used to model , measure , and monitor the routing infrastructure of the global Internet for both operational and research objectives . <p> The focus of the inaugural event , to be held February 6-7 , 2016 at SDSC , an Organized Research Unit of the University of California , San Diego , is live Border Gateway Protocol ( BGP ) measurements and monitoring . BGP is the de-facto standard inter-domain routing protocol for the Internet ; in effect its central nervous system . Its primary function is to exchange reachability information among Autonomous Systems ( networks administered by different organizations ) within the Internet . <p> An application to participate @ @ @ @ @ @ @ @ @ @ Applications will be accepted up until one week before the event or until capacity is reached . There is no application or participation fee but , due to limited availability , application does not guarantee participation . The deadline to apply for travel reimbursement is December 22 , 2015 . <p> A hackathon is a social event that brings computer programmers together , typically for more than one day , to intensively collaborate to create or improve a software code base . CAIDA 's BGP Hackathon is scheduled just before NANOG 66 in San Diego on February 8-10 , and precedes CAIDA 's annual AIMS ( Active Internet Measurements ) workshop , to be held at SDSC February 10-12 . The organizers hope to attract attendees who are already planning to be in San Diego for the NANOG meeting , and vice versa . NANOG is a professional association for those involved in Internet engineering and operations . <p> Teams and Challenges <p> The BGP Hackathon will be structured so each team of two to four participants will focus on a specific software development challenge . The hackathon 's program @ @ @ @ @ @ @ @ @ @ participants may propose totally new challenges or modify existing ones . The set of potential challenges is listed on the Hackathon Wiki , and will continuously evolve in the days preceding the event . At the end of the hackathon on February 7 , participants will make short presentations to a jury , followed by a reception to announce the winning teams . Full details are at the CAIDA BGP Hackathon webpage . <p> The organizing institutions and sponsors will provide participating teams with access to data sources and a toolbox that includes live streaming of BGP data , the new BGPMon interface , BGP processing tools and APIs such as the opensource BGPStream software framework , the PEERING testbed , RIPE RIS , visualization tools , and data-plane active measurement platforms such as CAIDA Ark and RIPE Atlas . <p> " Participating teams will work on challenges that extend , integrate , and demonstrate the utility of existing platforms and data for understanding or solving a range of practical problems , " said Alberto Dainotti , a research scientist with CAIDA and leading organizer of the event . " @ @ @ @ @ @ @ @ @ @ BGP prefix hijacking , evaluating anycast performance , or effectively visualizing phenomena . " <p> CAIDAs BGP Hackathon is sponsored by industry , professional organizations , and government agencies , including the National Science Foundation , the U.S. Department of Homeland Security , Google ( NetOPs and Open Source Research Groups ) , Comcast , the Internet Society ( ISOC ) , and ACM SIGCOMM . Additional co-organizers include Colorado State University , the University of Southern California , Universidade Federal de Minas Gerais ( Brazil ) , RouteViews , RIPE NCC , and FORTH . <p> About CAIDA <p> The Center for Applied Internet Data Analysis ( CAIDA ) , based at the San Diego Supercomputer Center , is a collaborative undertaking among organizations in the commercial , government , and research sectors aimed at promoting greater cooperation in the engineering and maintenance of a robust , scalable global Internet infrastructure . <p> About SDSC <p> As an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community @ @ @ @ @ @ @ @ @ @ accessible , integrated network of computer-based resources and expertise , focused on , and health IT . SDSCs Comet joins the Centers data-intensive Gordon cluster , and are both part of the National Science Foundations XSEDE ( eXtreme Science and Engineering Discovery Environment ) program , the most advanced collection of integrated digital resources and services in the world . 
@@97506399 @1706399/ <h> OpenTopography Bulk Download <p> This resource provides bulk downloads of lidar point cloud data from datasets hosted by OpenTopography . It is designed for advanced users seeking to download large amounts of data quickly and who have the bandwidth , expertise , and software necessary to manage the gigabytes of data available . We expect that users accessing bulk data have the ability to automate downloads from the command line via tools such as wget , curl , or a download manager such as DownThemAll <p> Each dataset directory contains all of the point cloud data for a given data collection and are typically organized into tiles ( e.g. , 1 km2 tiles ) and provided in the lossless LASZip format for download efficiency . When possible , a README file in the directory describes the dataset organization and other important information . Metadata for all datasets can be accessed via the OpenTopography Metadata page and users should consult the collection survey report for additional information on data organization , file naming conventions , coordinate systems , etc . <p> By accessing data via OpenTopography you agree to @ @ @ @ @ @ @ @ @ @ dataset metadata and on OpenTopography 's data acknowledgement page in publications , presentations , and other materials produced using these data . <p> NCALM Seed . PI : Ryan Wood , San Jose State University . The survey area consists of a 49 square kilometer polygon located 145 miles northeast of Boise , Idaho , spanning the border of the Payette and Bitterroot National Forests . Lidar data were collected to investigate transient hillslope response to knickpoint migration up a watershed . <p> These options allow users to describe and keep track of their jobs . Information entered below is recorded along with other job parameters in your personal lidar Job archive accessed via myOpenTopo ( available only to registered OpenTopography users ) . 
@@97506401 @1706401/ <h> OpenTopography Bulk Download <p> This resource provides bulk downloads of lidar point cloud data from datasets hosted by OpenTopography . It is designed for advanced users seeking to download large amounts of data quickly and who have the bandwidth , expertise , and software necessary to manage the gigabytes of data available . We expect that users accessing bulk data have the ability to automate downloads from the command line via tools such as wget , curl , or a download manager such as DownThemAll <p> Each dataset directory contains all of the point cloud data for a given data collection and are typically organized into tiles ( e.g. , 1 km2 tiles ) and provided in the lossless LASZip format for download efficiency . When possible , a README file in the directory describes the dataset organization and other important information . Metadata for all datasets can be accessed via the OpenTopography Metadata page and users should consult the collection survey report for additional information on data organization , file naming conventions , coordinate systems , etc . <p> By accessing data via OpenTopography you agree to @ @ @ @ @ @ @ @ @ @ dataset metadata and on OpenTopography 's data acknowledgement page in publications , presentations , and other materials produced using these data . <h> Piercy , CA : Response of Bedrock River to Base Level Lowering , SF Eel River <p> NCALM Seed Project . PI : Melissa Foster , Humboldt State University . The survey area consists of a 44 square kilometer polygon located next to Piercy , CA . The data were collected to study the response of a bedrock-controlled river and its tributaries to base level lowering . The study investigates the knickpoints and channel response of the South for Eel River . <p> These options allow users to describe and keep track of their jobs . Information entered below is recorded along with other job parameters in your personal lidar Job archive accessed via myOpenTopo ( available only to registered OpenTopography users ) . 
@@97506404 @1706404/ <p> Adrian , I think Phil was working on making this part of node interface setup ' pluggable ' which would make this kind of change easier to do but with current Rocks I think your best option is to patch the Rocks ' rocks report host interface ' command in this file ... LONG ... So that any/all Rocks command(s) that ' reset ' the network interface will do the right thing . You could store your additional options in an attribute for the node(s) and not need to modify the Rocks database which would be required ( along with MANY more modifications ) to get the ' rocks set host interface &lt;option&gt; ' behavior . If this kind of activity is beyond your level of experience then the extend-compute.xml file ( possibly with the OPTION in a node attribute ) is much easier but WILL require additional work if/when you ever run ' rocks sync host interface ' for a node . Regards , Trevor &gt; On Mar 6 , 2017 , at 10:39 AM , Adrian Sevcenco **31;2777;TOOLONG wrote : &gt;&gt; On 03/06/2017 07:55 PM , @ @ @ @ @ @ @ @ @ @ like this in extend-compute.xml : &gt;&gt;&gt;&gt; &lt;post&gt; &gt;&gt; &lt;file LONG ... mode= " append " &gt;&gt; LINKDELAY=10 &gt;&gt; &lt;/file&gt; &gt;&gt; &lt;/post&gt; &gt;&gt;&gt;&gt; That just appends the line you need to the existing file . Leaves what anaconda and rocks do alone . &gt; Hi ! thanks for idea ! i will try it ( i already use this for fstab and &gt; other files ) but would this survive a change in network settings of the &gt; nodes ? &gt;&gt; Thank you ! &gt; Adrian **28;2810;TOOLONG Bart Brashers &gt;&gt;&gt;&gt;&gt; -----Original Message----- &gt;&gt;&gt; From : **39;2840;TOOLONG mailto:npaci-rocks- **39;2881;TOOLONG On Behalf Of Adrian Sevcenco &gt;&gt;&gt; Sent : Monday , March 06 , 2017 9:29 AM &gt;&gt;&gt; To : **43;2922;TOOLONG Subject : Rocks-Discuss Re : set interface option for all nodes ( present &gt;&gt;&gt; and to be installed ) &gt;&gt;&gt;&gt;&gt;&gt; On 03/06/2017 03:16 AM , Ing . Gonzalo Arroyo wrote : &gt;&gt;&gt;&gt; Hi Adrian ! &gt;&gt;&gt; Hi ! **28;2967;TOOLONG you should add the command to do your synced customization to every &gt;&gt;&gt; node in &gt;&gt;&gt;&gt; the XML you need , for example : &gt;&gt;&gt;&gt; LONG ... **32;2997;TOOLONG In this part you can define @ @ @ @ @ @ @ @ @ @ yes , i use extend , replace-*.xml extensively , but i was hoping for a &gt;&gt;&gt; rocks set host interface option : ) &gt;&gt;&gt;&gt;&gt;&gt; my worry is/was that if i do something with a script in ifcfg-eth files &gt;&gt;&gt; any rocks sync network will overwrite the changes ... &gt;&gt;&gt;&gt;&gt;&gt; could some Rocks expert comment if could it be possible to add interface &gt;&gt;&gt; option using **28;3031;TOOLONG ? ( or should i use replace ? ) &gt;&gt;&gt;&gt;&gt;&gt; Thank you ! &gt;&gt;&gt; Adrian LONG ... &lt;post&gt; &gt;&gt;&gt;&gt; &lt; ! -- Insert your post installation script here . This &gt;&gt;&gt;&gt; code will be executed on the destination node after the &gt;&gt;&gt;&gt; packages have been installed . Typically configuration files &gt;&gt;&gt;&gt; are built and services setup in this section . --&gt; **32;3061;TOOLONG &lt; ! -- WARNING : Watch out for special XML chars like ampersand , &gt;&gt;&gt;&gt; greater/less than and quotes . A stray ampersand will cause the &gt;&gt;&gt;&gt; kickstart file building process to fail , thus , you wo n't be able &gt;&gt;&gt;&gt; to reinstall any nodes . It is recommended that after you create an &gt;&gt;&gt;&gt; XML node file , that you @ @ @ @ @ @ @ @ @ @ shell= " python " &gt; **32;3163;TOOLONG &lt; ! -- This is python code that will be executed on the &gt;&gt;&gt;&gt; frontend node during kickstart file generation . You may contact &gt;&gt;&gt;&gt; the database , make network queries , etc . These sections are &gt;&gt;&gt;&gt; generally used to help build more complex configuration &gt;&gt;&gt;&gt; files . The ' shell ' attribute is optional and may point to any &gt;&gt;&gt;&gt; language interpreter such as " bash " , " perl " , " ruby " , etc. &gt;&gt;&gt;&gt; By default shell= " bash " . --&gt; **32;3197;TOOLONG &lt;/eval&gt; &gt;&gt;&gt;&gt; * /usr/bin/yourcommand* **32;3231;TOOLONG ( that command could be a perl or another script to edit your files of &gt;&gt;&gt;&gt; interfaces ) **32;3265;TOOLONG Regards ! LONG ... *Este mensaje es confidencial . Puede contener informaci=n amparada por &gt;&gt;&gt; el &gt;&gt;&gt;&gt; secreto comercial . Si usted ha recibido este e-mail por error , deber &gt;&gt;&gt;&gt; eliminarlo de su sistema . No deber copiar el mensaje ni divulgar su &gt;&gt;&gt;&gt; contenido a ninguna persona . Muchas gracias. * &gt;&gt;&gt;&gt; This message is confidential . It may also contain information that is &gt;&gt;&gt;&gt; privileged or not authorized @ @ @ @ @ @ @ @ @ @ by &gt;&gt;&gt;&gt; mistake , delete it from your system . You should not copy the messsage &gt;&gt;&gt; nor &gt;&gt;&gt;&gt; disclose its contents to anyone . Thanks . **32;3299;TOOLONG El dom. , 5 de mar. de 2017 a la(s) 21:15 , Adrian Sevcenco &lt; **43;3333;TOOLONG escribi= : **32;3378;TOOLONG Hi ! Is there a way to set in interface option for all nodes &gt;&gt;&gt;&gt; automatically ( and also as default for future nodes ) ? **32;3412;TOOLONG It seems that i need in my ifcfg-eth script the presence of &gt;&gt;&gt;&gt; LINKDELAY=10 **32;3446;TOOLONG Thank you ! &gt;&gt;&gt;&gt; Adrian **32;3480;TOOLONG -------------- next part -------------- &gt;&gt;&gt;&gt; A non-text attachment was scrubbed ... &gt;&gt;&gt;&gt; Name : smime.p7s &gt;&gt;&gt;&gt; Type : **27;3514;TOOLONG &gt;&gt;&gt;&gt; Size : 2292 bytes &gt;&gt;&gt;&gt; Desc : S/MIME Cryptographic Signature &gt;&gt;&gt;&gt; Url : LONG ... LONG ... LONG ... -- &gt;&gt;&gt; LONG ... &gt;&gt;&gt; Adrian Sevcenco , Ph.D . &gt;&gt;&gt; Institute of Space Science - ISS , Romania &gt;&gt;&gt; adrian.sevcenco at cern.ch , spacescience.ro &gt;&gt;&gt; LONG ... &gt;&gt;&gt;&gt;&gt;&gt; -------------- next part -------------- &gt;&gt;&gt; A non-text attachment was scrubbed ... &gt;&gt;&gt; Name : smime.p7s &gt;&gt;&gt; Type : **27;3543;TOOLONG &gt;&gt;&gt; Size : 2292 bytes &gt;&gt;&gt; Desc : @ @ @ @ @ @ @ @ @ @ **28;3572;TOOLONG -- &gt; LONG ... &gt; Adrian Sevcenco , Ph.D . &gt; Institute of Space Science - ISS , Romania &gt; adrian.sevcenco at cern.ch , spacescience.ro &gt; LONG ... &gt;&gt; -------------- next part -------------- &gt; A non-text attachment was scrubbed ... &gt; Name : smime.p7s &gt; Type : **27;3602;TOOLONG &gt; Size : 2292 bytes &gt; Desc : S/MIME Cryptographic Signature &gt; Url : LONG ... 
@@97506406 @1706406/ <p> I 've no experience with mellanox , but with Intel/Qlogic kernel modules are compiled at install time for the runing kernel . I 've manualy set up an install script runing once at the first boot after node install to be sure that the right kernel module are built whatever the running kernel is . Patrick V-Twin Farriers a Tcrit : &gt; I found it necessary to reinstall the mellanox drivers manually on my &gt; cluster whenever a kernel update came down , I could never get the mellanox &gt; drivers to reinstall automatically on a new kernel . Ultimately , I disabled &gt; kernel updates in my nightly yum update cron task , and only perform a kernel &gt; update once a month or so when I can supervise the process myself &gt; ( automating the install was fine , until a compute node restarted , and then &gt; it would n't have the necessary infiniband drivers , which would create all &gt; sorts of havoc. ) &gt;&gt; All of my compute nodes are centos 6.8 at this point with no i 'll effects on &gt; cluster operations @ @ @ @ @ @ @ @ @ @ On Behalf Of Peter &gt; Kjellstrm &gt; Sent : Monday , January 2 , 2017 9:01 AM &gt; To : Carroll , Thomas **28;3676;TOOLONG &gt; Cc : Discussion of Rocks Clusters **39;3706;TOOLONG &gt; Subject : Rocks-Discuss Re : upgrading nodes to new kernel in Rocks 6.1 -- &gt; IB issues &gt;&gt; On We 'd , 28 Dec 2016 15:19:30 +0000 &gt; " Carroll , Thomas " **28;3747;TOOLONG wrote : &gt; ... &gt;&gt; The nodes are correctly updated from 2.6.32-431.17.1. el6.x8664 to &gt;&gt; 2.6.32-642.11.1. el6.x8664. &gt; ... &gt;&gt; mlx4 : Fatal : ABI version 4 of **34;3777;TOOLONG is &gt;&gt; not supported ( min supported 2 , max supported 3 ) libibverbs : Warning : &gt;&gt; no userspace device-specific driver found for &gt;&gt; **34;3813;TOOLONG &gt; That would hint at an updated kernel but old libraries ( libibverbs , libmlx4 , &gt; .. ) . &gt;&gt; /Peter &gt;&gt;&gt; 
@@97506410 @1706410/ <h> To My Best Friends <h> ... friends are the flowers in the garden of life . <h> I love you guys ! ! ! <p> Ivonne Thank you for always being there for me . It really helped just to have someone to talk to . I hope that this coming year , we 'll both be able to get Ballet Ensemble on our schedules ( being that I have other top priorities ) . Well ... call me . I love you ... <p> Leah I 've known you since I was six , if I remember right . I wish I did n't have to move to California , but I guess you 'll just have to move here ! I hope that I 'll get to see you again soon . Thank you for sticking with me through all my " little-kid " years ... ; ) <p> Christa We 've been best friends since we were about six or seven . All I can say is , thank you for all those sleepovers , late nights , climbing the tree at school , and @ @ @ @ @ @ @ @ @ @ hope I get to see you again . Love ya ... <p> Kory There 's so much I want to say , but I talk to you every day , so what matters ? I hope you can come visit soon ! I really miss you . I hope things work out for you .. everything ... I love you ... <p> Sean Although I met you just recently , it 's been pretty cool to talk to you online . We have so much in common it kind of scares me . I think it 's cool that you play the bagpipes . I could never do that . You are one of the few guys who writes long e-mails back to me ! ; o ) <p> Jerry I also just met you , and I barely know you , but you are really sweet ! I hope we can meet again sometime , and that your computer wroks so I can put a pic of you here ... Love ya ... <p> Zack ( My big brother ) I really have no idea where to begin . @ @ @ @ @ @ @ @ @ @ really been counting ) you 've always been there for me . You give me the best advice , comfort me , laugh with me ... I love talking to you . You cheer me up when I am down . I want to thank you a million times over for being there for me . <p> Joe Thank you for helping me with my Music Theory homework . I hope I make it through the class ! Good luck in band this year , I am so sad that I ca n't be in it with you ... <p> Ben I have n't known you for very long , maybe since January or so . ( I have no idea ! ) Anyways , it 's been cool , funny , and just plain FUN hanging with you this year . I do n't know how you handle our lunch groups craziness , but I 'm glad you do . And guess what ... I 'm a FRESHMAN ! ! ! Call me when you 're bored ( like usual ) . Luv ya ... <p> Kevin S. So @ @ @ @ @ @ @ @ @ @ start thanking you , hating you , forgiving you ... I really hope that you get into SDSU . Thank you for being there for me , for however long that was . I will always remember you ... <p> Jamie Really , my youngest guy friend ... Well , thank you for sticking with me through my Bat Mitzvah . It really helped to have someone to talk to . I was so lost , confused , and my mom said I lost weight . Whatever ! Even though you DITCHED ME at the malll ! Ugh ! Punkhead ! Although it was n't your fault , I 'm still feeling dumb walking around the mall with $100 by myself looking like a loner . Oh well . I miss you this year . Love you to death ... <p> Jon P. You know what I hate about you ? You 're so perfect , I mean ... when you bragged to me the other day on the phone , you were SO RIGHT ! Anyways , thank you for talking to me when I 'm down , and @ @ @ @ @ @ @ @ @ @ it . Really . Well , Racoon ... your life sounds like it 's going pretty well . Good luck , and umm ... about architecture ... hope we can work together some day . I went to an architecture firm recently , I guess I should tell you about it . Well , call me ... And it sounds so weird to say it , but I love you ... like a brother ... like a bestfriend ... and like a bestfriend 's ex-boyfriend ... : o ) 
@@97506412 @1706412/ <p> The Protein Feature View provides a graphical summary of a full-length protein sequence from UniProt and how it corresponds to PDB entries . It also loads annotations from external databases ( such as Pfam ) and homology models information from the Protein Model Portal . Annotations visualizing predicted regions of protein disorder ( computed with JRONN ) and hydrophobic regions ( as computed using a sliding window approach ) are displayed . <p> By default , representative PDB entries are used to give an overview for which regions of the UniProtKB sequence PDB entries are available . This view can be expanded to show all available PDB entries <p> This feature is available from the Molecular Description widget on Structure Summary pages . Selecting the Protein Feature View link launches more display options . <h> Improved Handling of Viral Polyproteins and Other Multi-component Proteins <p> To enable more accurate search results , polyproteins and multi-component proteins are now cross-referenced with PDB entities ( MOLIDs ) at the component level based on alignment and residue ranges from UniProt . <p> For example , type " Gag-Pol " in the @ @ @ @ @ @ @ @ @ @ polyprotein structures or only those for a particular component ( protease , reverse transcriptase , integrase , etc . ) <p> Another example are caspases , which are expressed as zymogens , that are proteolytically cleaved into two subunits . Type " Caspase " in the top search bar . The search suggestions list the subunits , for example : Caspase-1 subunit p20 , Capspase-1 subunit p10 . <h> PDB Entity Names as Synonyms for Macromolecule Names <p> Macromolecule name searches in the top menu bar make suggestions based on UniProt names . To provide more accurate suggestions , frequently used names of PDB entities have been assigned as synonyms for the corresponding UniProt names . <p> For example , by typing HIV-1 protease on the top bar search , users are provided with a suggestion to retrieve all " HIV-1 protease ( Gag-Pol polyprotein , Protease ) " structures for HIV-1 organisms . <p> Using synonyms makes the suggestions list more powerful than plain text searching . It makes PDB entries that use alternative molecule names such as retropepsin easier to find . <p> To help users browse search @ @ @ @ @ @ @ @ @ @ to the original results browser , now called the " detailed view , " we have added a consensed view , a new gallery view , and a timeline view . The views are synchronized ; selecting or deselecting a structure in one view will have the same effect in the others . <h> Timeline <p> The timeline view takes the current search results and generates a visual timeline ordered by release date , similar to the Author Profile feature . <p> The PDB IDs shown in the timeline link to their respective Structure Summary pages . To avoid performance issues , search results that return more then 3000 results have their timeline view disabled . A future release will re-enable this feature for all search queries . <h> Tabular Reports ( PubMedCentral I 'd ) <p> PubMed Central IDs are now included in the " Primary Citation " and customized tabular reports . Each PubMed Central I 'd is hyperlinked to the corresponding article at PubMed Central . <p> The Tabular Report system provides a broad view of PDB data and a friendly interface for users to browse , filter , @ @ @ @ @ @ @ @ @ @ various predefined standard reports or by creating customized reports with selected data items . For more details please refer to the Tabular Report help page . <h> Links to AtlasCBS <p> An external link to AtlasCBS has been added to the Structure Summary page 's Link tab under the Ligand Features section . View external links tab for 1A4G <p> Given a target-ligand database with chemical ligand structures and associated biological affinities/activities , the AtlasCBS server generates two-dimensional , dynamical , representations of its content based on ligand efficiency indices . These variables allow an effective decoupling of the chemical ( angular ) and biological ( radial ) components . BindingDB , PDBBind and ChEMBL databases are currently implemented . Proprietary datasets can also be uploaded and compared . 
@@97506419 @1706419/ <h> Poseview Image of CDL in 4D6U <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of PEE in 4D6U <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of HEC in 4D6U <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of HEM in 4D6U <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of G8U in 4D6U <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of FES in 4D6U <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of PO4 in 4D6U <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of GOL in 4D6U <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506423 @1706423/ <h> Query Results Browser Changes <p> A few usability improvements have been rolled into the query results browser page . The " Display/Download " , " Generate Reports " and " Query Options " dropdown lists are now more accessible . The " Display/Download " menu options operate only on the selected PDB IDs ( all by default ) . As before the " Generate Reports " pulldown menu includes a list of pre-generated summary reports and links to custom reports and image galleries . Improvements to the query pagination was also added now displaying page and result information . <h> Improved Tabular Report System <p> A new Tabular Report System that supports very large data sets was implemented . The new system also provides richer functionality in a more intuitive way . Access to the tabular reports has been simplified on the query results pages ( see above ) . <h> Pagination through the navigation bar <p> The above image is an example of the new navigation bar for reports . Instead of including all structures in a report on a single page , reports are now available @ @ @ @ @ @ @ @ @ @ first 15 records sorted by PDB I 'd will be rendered . The user can page through the report using the navigation bar . There is an option to list 30 , 45 , or 60 records per page . <h> Sorting , hyperlinking , and column resizing <p> Table sorts can be done on the entire report by clicking the column headers . Within the reports themselves , PDB IDs link to that entry 's Structure Summary page , PubMed IDs link to the abstract , and Ligand IDs link to a Ligand Summary page . All column widths are resizable by dragging the line between two columns . <h> Exporting to other formats <p> The report exporting feature has been greatly enhanced . Tables can be exported in three formats : <p> Excel 97-2003 format : Multiple work sheets will be generated for very large data sets to accommodate the row limitation ( 65,536 ) in older versions of Excel . <p> Excel 2007 or newer versions : This version supports up to 1,048,576 rows per work sheet . <p> The Excel spreadsheets have been reformatted with customized column @ @ @ @ @ @ @ @ @ @ selected columns . Formatting issues with PDB IDs that resemble floating point numbers such as 1E10 have been resolved . <h> New Advanced Search Interface <p> This release introduces a new Advanced Search interface that allows users to perform complex queries in an intuitive manner . Multiple simple queries can be combined into one complex search . Search criteria are more clearly defined and the search buttons have been rearranged to make it easier to add and remove search parameters . <p> We are also introducing a new , more intuitive help system that will eventually replace our current robohelp system . By default , the new help system will open in a shadow box interface . Clicking on the pop-up link will cause help to open in a separate window . The user can choose whether to keep the default shadow box or always have help open in a new window . <h> Display of Large Structures <p> We have made several enhancements to our Jmol page in order to display large structures . Previously , some structures with very large coordinate files failed to display because they would @ @ @ @ @ @ @ @ @ @ applet . We are now able to display structures that contain a large number of chains ( e.g. 1GAV , see image to the left ) , structures that contain very large molecules ( e.g. 1JJ2 ) , structures that contain many models of relatively small macromolecules ( e.g. 2HYN ) , and structures that contain multiple models of large molecules ( e.g. 1HTQ ) . <p> These structures are loaded into Jmol using a version of the PDB coordinate file that includes only backbone atoms for all polymers ( Carbon-alpha atoms for proteins and phosphate atoms for nucleic acid chains ) and all ligands including waters . All HETATM records are retained and displayed as well . In addition , for the display of the asymmetric unit , only the first MODEL from the PDB coordinate file is loaded and displayed ( Jmol command : load filename 1 ) . The display of biological assemblies , however , requires the inclusion of all MODELs . <p> Users should be aware that even these filtered coordinate files are large files and hence may take a while to download and display @ @ @ @ @ @ @ @ @ @ computer ) . <h> Split Entries <p> A number of structures in the PDB are so large that the historical limitations of the PDB file format ( 5 columns for atom numbering , 1 column for chain I 'd ) has required them to be split across multiple PDB coordinate files . These structures include the extremely large ribosome complexes ( e.g. 1GIX , 1GIY ) , and they also include other structures that contains a very large number of atoms or chains , such as the vault protein featured in the images to the left ( 2ZUO , 2ZV4 , 2ZV5 ) . <p> These structures are now prominently identified by the new " Split Entry " box on the entry 's Structure Summary page . This box lists the PDB IDs of all entries that make up the composite structure , and links to the Structure Download Tool to easily access the related files in any format . <p> On the right hand side of the Structure Summary page , the visualization options have been adapted to accommodate these split entries . Using the forward and backward buttons , @ @ @ @ @ @ @ @ @ @ biological assemblies and the asymmetric unit ( just as in the case of single , non-split entries ) . The Jmol link underneath the image toggles accordingly between the composite biological assembly and the composite asymmetric unit . The display of such large , composite entries in Jmol was enabled by loading PDB files limited to CA and P atoms ( see Display of Large Structures above ) , and by using the Jmol load files command . <h> Download of Biological Assemblies <p> The origin of biological assembly information has been added to the Download Files menu ( A = author assigned biological assembly , S = software generated biological assembly ) . A mouse over the biological assembly file name in the pull-down menu displays additional information , i.e. the software used to generate the biological assembly . This information has also been added to the bottom of the Biological Assembly widget on the structure summary page . <h> Support for CE algorithm in Comparison Tool <p> In our previous release we introduced the RCSB PDB Comparison Tool , which calculates pairwise sequence and structure comparisons . You @ @ @ @ @ @ @ @ @ @ . The comparison tool is also integrated into the Sequence Similarity report ( Show example ) . <p> With this release , the CE algorithm algorithm has been incorporated into the Comparison Tool as a stand-alone application via Java Web Start . <p> The 3D display shows the alignment of the two selected proteins . If there are any ligands associated with the chains selected for comparison , they are displayed in the superimposition as well . The GUI uses the following rules to support interpretation of the alignment : In bold are the aligned positions . Colored in orange are the aligned positions of the 1st structure . Cyan is used for the 2nd structure . Ligands are always colored in orange/cyan to identify which of the chains they belong to . <h> Alignment of Any PDB Files <p> The application that runs jCE and jFatCat via Java Web start now also supports alignments of custom user provided PDB files . Since the application runs locally , calculations can be performed without submitting coordinate files to public servers . Additional functionality now includes saving and loading of the alignment @ @ @ @ @ @ @ @ @ @ Citations in Literature Report <p> The Literature report , available from any entry 's Structure Summary page , currently displays open-access articles from PubMedCentral that cite that particular PDB I 'd . In this release we have added support for viewing the textual context in which a PDB structure has been cited allowing users to get a quick overview of the articles and identify any that are of particular interest . <h> New External Widgets <p> With this release 3 new widgets are being introduced that can be installed and used on any external website . <h> RCSB PDB Molecule of the Month Widget <p> The Molecule of the Month widget embeds a Molecule of the Month ( MOM ) image on your website and links to that feature . The widget can be customized to specify width , colors , amount of text show , and which feature to display . If you do not specify a particular MOM issue , the widget will automatically use the currently featured structure . <h> RCSB PDB Tag Library <p> The RCSB PDB Tag Library is a rich markup widget that allows easy @ @ @ @ @ @ @ @ @ @ The widget provides enhanced functionality that automatically links back to RCSB PDB pages . Check out the example on the left by rolling over any of the underlined words . Currently 3 different tags are provided : pdbidtag provides an image and link to the Structure Summary page , pdbmenutag for a particular PDB I 'd with useful links to view the entry in Jmol , to the Structure Summary Page , and to the PDB file ; and pdbkeywordtag links to a keyword query results page . <h> RCSB PDB Image Library <p> The RCSB PDB Image Library embeds an image of any PDB entry on your website. 
@@97506433 @1706433/ <p> I am not the primary point of contact for the clusters . I 'm the last resort . If fact I was only called because everyone else was out . I usually work on Linux workstations and servers . Anyway the problem has been resolved . I was root when I tried to restart pbsserver . The main user said it was ok to reboot , but that did n't do a thing . It turns out someone had a script that was submitting jobs . One of the compute nodes was in a wierd state . It appeared to be running so it was getting all of these jobs , but just queing them up . I did n't discover this , someone else did . We cleared out all of the jobs and then we were able to restart pbsserver . To answer your question about both SGR and Maui/Torque being installed . I did n't build this cluster , though I will get my chance shortly to build a small one . Thanks . On Friday , January 17 , 2014 9:44 AM , Bart @ @ @ @ @ @ @ @ @ @ service pbsserver restart # service maui restart # ssh compute-0-0 ' service pbs restart ' Then check if things are running : # ps aux grep pbsserver 500 14724 0.0 0.0 61204 708 pts/11 S+ 09:40 0:00 grep pbsserver root 29830 0.0 0.0 74396 30016 ? S Jan09 8:43 **26;3849;TOOLONG # ps aux grep maui 500 14748 0.0 0.0 61204 712 pts/11 S+ 09:40 0:00 grep maui maui 30852 0.0 0.1 60852 41268 ? Ss Jan16 1:07 /opt/maui/sbin/maui # ssh compute-0-0 ' ps aux grep pbsmom ' root 15211 0.0 0.0 18720 6440 ? SLs Jan15 0:48 /opt/torque/sbin/pbsmom 500 29812 0.0 0.0 70540 3592 ? Ss 09:40 0:00 tcsh -c ps aux grep pbsmom 500 29838 0.0 0.0 61152 728 ? S 09:40 0:00 grep pbsmom Did you install both the SGE and Torque/Maui rolls when you installed your cluster ? Bart Brashers &gt; -----Original Message----- &gt; From : **30;3877;TOOLONG at sdsc.edu **30;3909;TOOLONG &gt;bounces at sdsc.edu On Behalf Of brown wrap &gt; Sent : Friday , January 17 , 2014 8:30 AM &gt; To : ' npaci-rocks-discussion at sdsc.edu ' &gt; Subject : Rocks-Discuss Ca n't start PBS server &gt;&gt;&gt;&gt;&gt; @ @ @ @ @ @ @ @ @ @ . I am trying to support a &gt; user . His initial problem was pbsmom was not running . I started that , but then &gt; when he did a qstat : &gt;&gt; qstat &gt; Unable to communicate with localhost(127.0.0.1) &gt; Can not connect to specified server host ' localhost ' . &gt; qstat : can not connect to server localhost ( errno=111 ) Connection refused &gt;&gt; I then found the pbsserver was not running . When I tried to restart it : &gt;&gt; . /pbsserver status &gt; pbsserver dead but subsys locked &gt;&gt;&gt; Things are stuck at this point . Where do I look , or better yet , how do I start &gt; pbsserver ? &gt; -------------- next part -------------- &gt; An HTML attachment was scrubbed ... &gt; URL : LONG ... LONG ... This message contains information that may be confidential , privileged or otherwise protected by law from disclosure . It is intended for the exclusive use of the Addressee(s) . Unless you are the addressee or authorized agent of the addressee , you may not review , copy , distribute or disclose to anyone the @ @ @ @ @ @ @ @ @ @ received this message in error , please contact the sender by electronic reply to email at environcorp.com and immediately delete all copies of the message . -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... 
@@97506436 @1706436/ <p> Stoopid email program that wrapped the lines ! Sorry about that . Use " qdel -p &lt;JobID&gt; " where JobID is the job number you want to remove . Bart &gt; -----Original Message----- &gt; From : **30;3941;TOOLONG at **38;3973;TOOLONG &gt;bounces at sdsc.edu On Behalf Of Bart Brashers &gt; Sent : Thursday , June 16 , 2011 8:20 AM &gt; To : Discussion of Rocks Clusters &gt; Subject : Re : Rocks-Discuss how to delete a job which execute in the &gt; nodesthathave been removed &gt;&gt; # man qdel grep -A 8 purge &gt;&gt; -p Forcibly purge the job from the server . This &gt; should &gt; only be used if a running job will not exit &gt; because its &gt; allocated nodes are unreachable . The admin &gt; should make &gt; every attempt at resolving the problem on the &gt; nodes . If &gt; a job 's mother superior recovers after purging &gt; the job , &gt; any epilogue scripts may still run . This option &gt; is only &gt; available to a batch operator or the batch &gt; administra- &gt; tor . &gt;&gt; Bart Brashers &gt;&gt; &gt; @ @ @ @ @ @ @ @ @ @ &gt; &gt; bounces at sdsc.edu On Behalf Of hnuzhoulin &gt; &gt; Sent : Thursday , June 16 , 2011 12:17 AM &gt; &gt; To : torqueusers ; npaci-rocks-discussion &gt; &gt; Subject : Rocks-Discuss how to delete a job which execute in the &gt; nodes &gt; &gt; thathave been removed &gt; &gt; &gt; &gt; &gt; &gt; Few days ago , I remove a node from my cluster . But I forget to &gt; delete the &gt; &gt; job which is executing in this node . &gt; &gt; &gt; &gt; Now I can not delete the job.The error is qdel : Server could not &gt; connect to &gt; &gt; MOM . &gt; &gt; So , how to deal with it . &gt; &gt; Thanks so much . &gt; &gt; &gt; &gt; -------------- &gt; &gt; hnuzhoulin &gt; &gt; 2011-06-16 &gt; &gt; t &gt;&gt;&gt; This message contains information that may be confidential , privileged or &gt; otherwise protected by law from disclosure . It is intended for the exclusive &gt; use of the Addressee(s) . Unless you are the addressee or authorized agent of &gt; the addressee , you may not review , copy @ @ @ @ @ @ @ @ @ @ any information contained within . If you have received this &gt; message in error , please contact the sender by electronic reply to &gt;email at environcorp.com and immediately delete all copies of the message . This message contains information that may be confidential , privileged or otherwise protected by law from disclosure . It is intended for the exclusive use of the Addressee(s) . Unless you are the addressee or authorized agent of the addressee , you may not review , copy , distribute or disclose to anyone the message or any information contained within . If you have received this message in error , please contact the sender by electronic reply to email at environcorp.com and immediately delete all copies of the message . 
@@97506441 @1706441/ <p> It turns out that compute-0-20 was n't mounting the home directories for some reason . Unfortunately , qstat -j &lt;jobid&gt; does n't tell me which node the job failed to start on , otherwise I 'd have diagnosed this immediately . As it were , the user forwarded the grid engine email to me that showed node 20 was the culprit . -----Original Message----- From : **30;4077;TOOLONG at sdsc.edu **37;4109;TOOLONG at sdsc.edu On Behalf Of Mike Hanby Sent : Monday , August 13 , 2007 09:40 To : npaci-rocks-discussion at sdsc.edu Subject : Rocks-Discuss Jobs in Eqw " ca n't chdir to " Howdy , I have a user that needs to submit a large number of serial jobs to the grid engine ( Rocks 4.2.1 with GE 6.0u8 ) . The user has thousands of jobs . I wrote a script that submits them in batches of 32 , with a sleep of 30 minutes between batches ( the jobs should run for approx 30 minutes ) . The script sleeps for 3 seconds between each qsub , so as not to submit 32 within a short @ @ @ @ @ @ @ @ @ @ an NFS mounted home directory : /jobs/job1 ... /jobs/job1000 I have the grid engine configured for maxjobs of 32 . The problem is , a job or two out of each 32 ends up as Eqw immediately after submission with the error : error reason 1 : 08/13/2007 09:09:09 1258:14116 : error : ca n't chdir to **25;4148;TOOLONG : No such file or I found a thread that discussed this issue : LONG ... 2879. html The NFS server is a separate server ( nas-0-2 ) from the head node and is configured in auto.home for the users directory . The switch is GigE . The NAS server is the same hardware as the rest of the cluster , dual AMD Opteron with 4GB 's of RAM . I have another use who 's jobs have gone into Eqw for the same reason , and his home directory is on nas-0-0 , i.e. a different NFS server . Has anyone encountered this and found a resolution ? Thanks , Mike -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... **29;4175;TOOLONG 
@@97506443 @1706443/ <h> Poseview Image of CAI in 1K8A <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CD in 1K8A <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of K in 1K8A <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CL in 1K8A <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MG in 1K8A <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NA in 1K8A <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506444 @1706444/ <p> On Thu , Dec 13 , 2012 at 9:47 PM , Robert Verstandig &lt; R.Verstandig at curtin.edu.au&gt; wrote : &gt; Hi Guys &gt;&gt; I have got a little further . I rescue-booted the node and was able to set &gt; the root password . I then restarted the node and could log in as root . &gt;&gt; Ssh is not being started on the node because the /etc/ssh/sshhostdsakey &gt; and /etc/ssh/sshhostrsakey files are empty , i.e. , just a carrige return &gt; in the file , no private keys generated . &gt;&gt; Which part of the installation does this on the nodes ? its in the file ssh-client.xml &gt; Is this happening because of the duplicate ssh-rsa key in the &gt; /tmp/authorizedkeys not related . &gt; section of the node 's profile or is there something else going on here . &gt; How can I reset the generated ssh environment for the nodes to default ? &gt;please send the complete output of " rocks list host profile " also " rocks list roll " &gt;&gt; Cheers &gt;&gt; Rob &gt;&gt; -----Original Message----- &gt; From : Robert Verstandig &gt; Sent @ @ @ @ @ @ @ @ @ @ : ' Discussion of Rocks Clusters ' &gt; Subject : RE : Rocks-Discuss ssh : connect to host compute-0-1 port 22 : &gt; Connection refused &gt;&gt; Hi Bart &gt;&gt; I 'm just following up some of your and Phil 's suggestions . As you thought &gt; the qsub option did not work . If the node profile is ok my next step will &gt; be to boot the node into rescue mode and see whether I can work out what 's &gt; going on from the logs . &gt;&gt; Cheers &gt;&gt; Rob &gt;&gt;&gt; -----Original Message----- &gt; From : **30;4206;TOOLONG at sdsc.edu mailto : **34;4238;TOOLONG at sdsc.edu On Behalf Of Bart Brashers &gt; Sent : Friday , 14 December 2012 8:45 AM &gt; To : Discussion of Rocks Clusters &gt; Subject : Re : Rocks-Discuss ssh : connect to host compute-0-1 port 22 : &gt; Connection refused &gt;&gt; &gt; Qrsh appears to be a SGE component which we do n't have installed . The &gt; &gt; password change also does not work as it uses ssh to update the node . &gt; &gt; I tried a reinstall after the reset but @ @ @ @ @ @ @ @ @ @ &gt; &gt; I have removed all customisations from the ' nodes ' directory and &gt; &gt; rebuild the distro then reinstalled the node - same error persists . &gt; &gt; &gt; &gt; I think my only options now are to a ) install the SGE roll and hope it &gt; &gt; does n't screw up the paths etc across the cluster ( we use MPI and &gt; &gt; Torque here ) ; &gt;&gt; Do n't do that , it will screw things up . &gt;&gt; For Torque , use &gt;&gt; # qsub -I -l nodes=compute-0-0 &gt;&gt; to start a shell on compute-0-0 . But I doubt it will work , because jobs &gt; are started via ssh ( as root , I believe ) . &gt;&gt; &gt; b ) try to boot the node into rescue mode and hope the logs can provide &gt; &gt; a clue or ; &gt;&gt; Good idea . Look in /root/*.log. &gt;&gt; &gt; c ) reinstall the cluster - last resort . &gt; &gt; &gt; &gt; Is there anything else I can check with the ssh config on the frontend &gt; &gt; that may be @ @ @ @ @ @ @ @ @ @ look at /root/.ssh and the permissions . &gt;&gt; Did you test that you can make a valid kickstart file , before you &gt; re-installed the node ? &gt;&gt; # rocks list host profile compute-0-0 &gt; /tmp/c0.ks &gt;&gt; Clearly something has changed somewhere since you last successfully &gt; re-installed a compute node . You 'll have to track it down . Or some &gt; service is n't running . Did you try rebooting the headnode ( painful , but &gt; less painful that re-installing ) ? &gt;&gt; Bart Brashers &gt;&gt; &gt; &gt; &gt; -----Original Message----- &gt; &gt; From : **30;4274;TOOLONG at sdsc.edu&gt; &gt; **30;4306;TOOLONG bounces at sdsc.edu On Behalf Of Luca &gt; &gt; Clementi &gt; &gt; Sent : Friday , 14 December 2012 3:29 AM &gt; &gt; To : Discussion of Rocks Clusters &gt; &gt; Subject : Re : Rocks-Discuss ssh : connect to host compute-0-1 port 22 : &gt; &gt; Connection refused &gt; &gt; &gt; &gt; On Thu , Dec 13 , 2012 at 6:48 AM , Robert Verstandig &gt; &gt; &lt;R.Verstandig at curtin.edu.au&gt; &gt; &gt; wrote : &gt; &gt; &gt; Hi Guys &gt; &gt; &gt; &gt; &gt; &gt; I @ @ @ @ @ @ @ @ @ @ . The problem &gt; &gt; &gt; node &gt; &gt; crashed and auto-installed on restart but since then I 've been unable &gt; &gt; to ssh to the node . I also have KVM access but can not log into the &gt; &gt; node directly as any user . Every username gives me an incorrect &gt; username/password error . &gt; &gt; &gt; &gt; &gt; &gt; I have tried reinstalling the node and even completely removing and &gt; &gt; &gt; re-adding &gt; &gt; it . The installl appears to go through fine and the node restarts &gt; &gt; correctly after the install and gets to a login prompt without any &gt; &gt; errors . I can ping the node and it appears fine and healthy in &gt; &gt; Ganglia . The firewall config looks OK . The node also appears to be &gt; &gt; fine and online in the Torque queues-just no ssh . &gt; &gt; &gt; &gt; &gt; &gt; &gt; If Torque is OK you can try to get an interactive shell through qrsh &gt; &gt; ( I do n't have torque but I think that 's the command ) . @ @ @ @ @ @ @ @ @ @ a password to the node , following this doc : &gt; &gt; LONG ... &gt; ation- &gt; &gt; passwd.html#AEN1693 &gt; &gt; &gt; &gt; Then you can do a local login using the KVM and check what ssh is not up . &gt; &gt; # /etc/init.d/sshd restart &gt; &gt; And then look in /var/log/messages for more clues . &gt; &gt; &gt; &gt; &gt; I am thinking that the ssh config files that are copied to the node &gt; &gt; &gt; are &gt; &gt; corrupted or have changed but I 'm not sure what I need to check . &gt; &gt; &gt; &gt; You can check if any custom made " node " file has been defined doing an : &gt; &gt; # ls LONG ... &gt; &gt; by default you should have only a &gt; &gt; skeleton.xml &gt; &gt; &gt; &gt; If you have defined other files , the problem is almost certainly there . &gt; &gt; &gt; &gt; &gt; &gt; &gt; Following are some of the configurations : &gt; &gt; &gt; &gt; This looks ok. &gt; &gt; &gt; &gt; Luca &gt; &gt; &gt;&gt;&gt; &gt; This message contains information that may @ @ @ @ @ @ @ @ @ @ from disclosure . It is intended for the &gt; exclusive use of the Addressee(s) . Unless you are the addressee or &gt; authorized agent of the addressee , you may not review , copy , distribute or &gt; disclose to anyone the message or any information contained within . If you &gt; have received this message in error , please contact the sender by &gt; electronic reply to email at environcorp.com and immediately delete all &gt; copies of the message . &gt;&gt;&gt; -- Philip Papadopoulos , PhD University of California , San Diego 858-822-3628 ( Ofc ) 619-331-2990 ( Fax ) -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... 
@@97506447 @1706447/ <h> Poseview Image of CLM in 1NJI <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CD in 1NJI <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of K in 1NJI <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CL in 1NJI <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MG in 1NJI <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NA in 1NJI <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506451 @1706451/ <h> Poseview Image of SIA in 4FQC <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NAG in 4FQC <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of GAL in 4FQC <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MAN in 4FQC <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of BMA in 4FQC <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of FUC in 4FQC <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of TAM in 4FQC <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506454 @1706454/ <h> Born : Lowell , Massachusetts , August 1 , 1905 <h> Died : Richmond Hill , Ontario , January 28 , 1993 <h> A Gift of Stars <p> An astronomer who brought the gift of the stars to everyone , Helen Sawyer Hogg led a life of remarkable achievement . She is well known for her research on variable stars in globular clusters , but she is perhaps best remembered for her astronomy column , which ran in the Toronto Star from 1951 to 1981 . Helen wanted everyone to find the same joy in the stars that she did . She encouraged women to enter science , and her students remember her for her enthusiasm and warmth . <p> She entered Mount Holyoke College with the intention of studying chemistry , but in 1925 she changed her mind and began her study of astronomy . Her decision was fixed permanently a year later when Annie Jump Cannon visited Mount Holyoke . Indeed , on graduating in 1926 , Sawyer went to the Harvard Observatory to work with Cannon and Harlow Shapley on star clusters . She obtained @ @ @ @ @ @ @ @ @ @ did not give graduate degrees in science to women at that time . ) <p> In 1930 , she married Frank Hogg , a fellow astronomy student at Harvard . Together they moved to Victoria , British Columbia , to work at the Dominion Astrophysical Observatory where Frank had a job . Helen was not hired with her husband , and she had to work as his volunteer assistant . <p> Hogg started her work on variable stars in globular clusters while in Victoria . She developed a technique for measuring the distance of galaxies beyond the Milky Way . She took pictures of variable stars and catalogued the cyclical changes in their brightness , which she then used to calculate their distance . Her detailed observations were published in catalogs that are still used today . <p> In 1935 , the family moved to Ontario so that Frank could take a job at the University of Toronto . Helen became an assistant at the David Dunlop Observatory , where she would work until her death . She also taught at the University of Toronto and became a professor in @ @ @ @ @ @ @ @ @ @ acting chair of the department of astronomy at Mount Holyoke College , and in 1955-1956 she was a program director in astronomy for the National Science Foundation . She was considered a world expert on the night sky . <p> In 1950 she won the Annie Jump Cannon prize of the American Astronomical Society . In 1967 she was awarded the Centennial Medal of Canada and in 1976 she was made a Companion of the Order of Canada--one of the highest honors in the nation . She became the first woman president of the physical sciences section of the Royal Society of Canada in 1960 . She was also the first female president of the Royal Canadian Institute ( 1964-1965 ) and founding president of the Canadian Astronomical Society ( 1971-1972 ) . <p> She remained active in astronomy until late in life . She published more than 200 papers during her long and distinguished career . In addition , she wrote The Stars Belong to Everyone , a popular guide to astronomy , and hosted an astronomy television series in the 1970s . Hogg received honorary degrees from six @ @ @ @ @ @ @ @ @ @ of Science and Technology 's observatory in Ottawa and the University of Toronto 's southern observatory in Chile were named for her . When Helen Sawyer Hogg died of a heart attack in 1993 , she had been a leading authority in astronomy for more than 60 years . 
@@97506466 @1706466/ <h> Enzyme Classification Browser <p> Searches or browses for structures by Enzyme Commission ( EC ) Number . The Enzyme Classification Browser is a hierarchical representation of enzymes by the enzyme classification according to the nomenclature of the International Union of Biochemistry and Molecular Biology ( IUBMB ) . <p> Structures of a particular enzyme can be searched in the browser tree by using a term or a partial or full EC number . <p> The enzyme browser permits the user to navigate through the hierarchy of enzyme classification to arrive at the subset of particular interest . The browser opens up the top level in the hierarchy . Clicking on the arrow/folder icons expands the respective nodes . Clicking on the name of the node will retrieve all PDB IDs assigned that EC number . <p> Note : Since the publication of the Enzyme Handbook IUBMB has obsoleted several EC entries either by deleting them completely or allocating them to ( some times more than one ) new EC entries . Some of these EC numbers are still referred by PDB entries , and they are labeled accordingly @ @ @ @ @ @ @ @ @ @ . <h> Examples <p> Search by a term <p> Histidine kinase <p> Type Histidine kinase in the text box above the tree and click the button " Find in Tree " . The tree expands and nodes matching the keyword expand . To see the number of matching PDB structures mouse over an entry . To view the list of structures , click on a node . <p> Search by EC number <p> 3.2.1 <p> Type the partial EC number 3.2.1 in the search box and click the button " Find in Tree " . The tree expands and all nodes 3.2.1. are highlighted in gray . <p> Browse the enzyme classification tree <p> Transferases <p> Mouse over Transferases to see the number of structures with the term transferase . Expand the node 2.7 : Transferring phosphorous-containing groups by clicking on the triangle . Mouse over 2.7.11 : Protein-serine/threonine kinases to see the number of structures related to this node . Click on the 2.7.11 : Protein-serine/threonine kinases to display the list of protein-serine/threonine kinases. 
@@97506469 @1706469/ <p> This directory contains the data files required as input to the liftOver utility . This tool -- which requires a Linux platform -- allows the mass conversion of coordinates from one assembly to another . The executable file for the utility can be downloaded from http : **36;4338;TOOLONG The file names reflect the assembly conversion data contained within in the format **38;4376;TOOLONG . For example , a file named mm9ToHg18.over.chain.gz file contains the liftOver data needed to convert mm9 ( Mouse Build 37 ) coordinates to hg18 ( Human Build 36 ) . If no file is available for the assembly in which you 're interested , please send a request to the genome mailing list ( genome@soe.ucsc.edu ) and we will attempt to provide you with one . To download a large file or multiple files from this directory , we recommend that you use ftp rather than downloading the files via our website . To do so , ftp to hgdownload.cse.ucsc.edu ( user : anonymous ) , then cd to goldenPath/mm9/liftOver . To download multiple files , use the " mget " command : mget &lt;filename1&gt; @ @ @ @ @ @ @ @ @ @ all the files in the directory ) LONG ... Please refer to the credits page LONG ... for guidelines and restrictions regarding data use for these assemblies . LONG ... Alternate methods to ftp access . Using an rsync command to download the entire directory : rsync -avzP LONG ... . For a single file , e.g. mm8ToHg18.over.chain.gz rsync -avzP LONG ... . Or with wget , all files : wget --timestamping LONG ... With wget , a single file : wget --timestamping LONG ... -O mm9ToHg18.over.chain.gz To uncompress the *. chain.gz files : gunzip &lt;file&gt;.chain.gz The liftOver utility can read the files in their . gz format , it is not necessary to uncompress them to use with the liftOver command . This file last updated : 2007-07-26 - 26 July 2007 
@@97506474 @1706474/ <p> Yes I have performed " rocks sync config " stiil it is showing it entry in list . compute-0-21 : private eth0 00:30:48:c6:6b:de 10.168.255.234 255.255.0.0 ----------- e1000 compute-0-21 compute-0-21 : ------- eth1 00:30:48:c6:6b:df -------------- ----------- ----------- e1000 --------------- compute-0-20 : private ----- 00:0f:b5:89:b5:6b -------------- 255.255.0.0 ----------- ------ compute-0-20 compute-0-28 : private eth0 00:30:48:c6:6b:e2 10.168.255.227 255.255.0.0 ----------- e1000 compute-0-28 compute-0-28 : ------- eth1 00:30:48:c6:6b:e3 -------------- ----------- ----------- e1000 --------------- how to remove it ----- Original Message ----- From : " Michael Duncan " &lt;MDuncan at x-iss.com&gt; To : " Discussion of Rocks Clusters " **26;4416;TOOLONG at sdsc.edu&gt; Sent : Monday , April 27 , 2009 1:02 PM Subject : Re : Rocks-Discuss How to remove compute node Did you perform a " rocks sync config " after the insert-ethers command ? ------------------ Michael Duncan Systems Analyst mduncan at x-iss.com eXcellence in IS Solutions , Inc . ( X-ISS ) Office : 713.862.9200 x215 http : //www.x-iss.com Making IT Work for You HPC &amp; Enterprise IT Solutions -----Original Message----- From : **30;4444;TOOLONG at sdsc.edu **37;4476;TOOLONG at sdsc.edu On Behalf Of hardik patel Sent : Monday , April 27 , 2009 @ @ @ @ @ @ @ @ @ @ Re : Rocks-Discuss How to remove compute node we have run " insert-ethers --remove compute-0-20 " command but this will not remove entry from list . still " rocks list host " command show compute-0-20 entry .... ----- Original Message ----- From : " Michael Duncan " &lt;MDuncan at x-iss.com&gt; To : " Discussion of Rocks Clusters " **26;4515;TOOLONG at sdsc.edu&gt; Sent : Monday , April 27 , 2009 11:16 AM Subject : Re : Rocks-Discuss How to remove compute node insert-ethers --remove compute-0-20 ------------------ Mike -----Original Message----- From : **30;4543;TOOLONG at sdsc.edu **37;4575;TOOLONG at sdsc.edu On Behalf Of hardik patel Sent : Monday , April 27 , 2009 11:02 AM To : npaci-rocks-discussion at sdsc.edu Subject : Rocks-Discuss How to remove compute node Hi , I have Rock cluster with 36 compute node + 1 head node . Somehow while adding compute node in my cluster we got entry of mac address of switch in list of host . compute-0-20 : private ----- 00:0f:b5:89:b5:6b -------------- 255.255.0.0 ----------- ------ compute-0-20 this is not compute node but head node is consider as compute-20 node . in real it is 1 gig @ @ @ @ @ @ @ @ @ @ can we remove this entry from list of headnode. while running some test it creating problem . when we try to remove this compute-20 node it is showing below error : root at cluster # rocks remove host compute-0-20 Traceback ( most recent call last ) : File " /opt/rocks/bin/rocks " , line 223 , in ? command.runWrapper ( name , argsi : ) File LONG ... line 1459 , in runWrapper self.run ( self. params , self. args ) File LONG ... t.py " , line 94 , in run self.runPlugins(host) File LONG ... line 1210 , in runPlugins plugin.run(args) File LONG ... npxeboot.py " , line 75 , in run **39;4614;TOOLONG ' , args ) File LONG ... line 1152 , in command o.runWrapper ( name , args ) File LONG ... line 1459 , in runWrapper self.run ( self. params , self. args ) File LONG ... ot/init.py " , line 139 , in run hexstr = ' %02x ' % ( int(i) ) ValueError : invalid literal for int() : can any one help us how to remove this node from head node . -- LONG @ @ @ @ @ @ @ @ @ @ with it are intended solely for the use of the addressee and may contain legally privileged and confidential information . If the reader of this message is not the intended recipient , or an employee or agent responsible for delivering this message to the intended recipient , you are hereby notified that any dissemination , distribution , copying , or other use of this message or its attachments is strictly prohibited . If you have received this message in error , please notify the sender immediately by replying to this message and please delete it from your computer . Any views expressed in this message are those of the individual sender unless otherwise stated.Company has taken enough precautions to prevent the spread of viruses . However the company accepts no liability for any damage caused by any virus transmitted by this email . LONG ... -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... **29;4655;TOOLONG NOTICE : This message may contain privileged or otherwise confidential information . If you are not the intended recipient , please immediately advise the sender by reply email and delete @ @ @ @ @ @ @ @ @ @ disclosing the contents . Email Scanned for Virus &amp; Dangerous Content by : www.CleanMailGateway.com -- LONG ... Disclaimer : This e-mail message and all attachments transmitted with it are intended solely for the use of the addressee and may contain legally privileged and confidential information . If the reader of this message is not the intended recipient , or an employee or agent responsible for delivering this message to the intended recipient , you are hereby notified that any dissemination , distribution , copying , or other use of this message or its attachments is strictly prohibited . If you have received this message in error , please notify the sender immediately by replying to this message and please delete it from your computer . Any views expressed in this message are those of the individual sender unless otherwise stated.Company has taken enough precautions to prevent the spread of viruses . However the company accepts no liability for any damage caused by any virus transmitted by this email . LONG ... NOTICE : This message may contain privileged or otherwise confidential information . If you are not the intended recipient @ @ @ @ @ @ @ @ @ @ delete the message and any attachments without using , copying or disclosing the contents . Email Scanned for Virus &amp; Dangerous Content by : www.CleanMailGateway.com -- LONG ... Disclaimer : This e-mail message and all attachments transmitted with it are intended solely for the use of the addressee and may contain legally privileged and confidential information . If the reader of this message is not the intended recipient , or an employee or agent responsible for delivering this message to the intended recipient , you are hereby notified that any dissemination , distribution , copying , or other use of this message or its attachments is strictly prohibited . If you have received this message in error , please notify the sender immediately by replying to this message and please delete it from your computer . Any views expressed in this message are those of the individual sender unless otherwise stated.Company has taken enough precautions to prevent the spread of viruses . However the company accepts no liability for any damage caused by any virus transmitted by this email . LONG ... -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... 
@@97506477 @1706477/ <p> BcCZOII member , Nate Rock , and undergrad assistant extraordinaire , Clayton Jensen , were able to brave the September 2013 catastrophic flooding in Boulder County to collect water samples in the Boulder Creek CZO . On 12 September 2013 , they were able to sample at Betasso , Boulder Creek at Orodell , and a few other spots on Boulder Creek , capturing samples near peak conditions . On 13 September 2013 , although they were turned away at the canyon mouth , they were able to sample Boulder Creek at 6 spots from Eben G. Fine Park down to the 75th Street gage site . Other CZO members plan to continue sampling at those 6 sites over the next few days as flood waters recede . <p> BcCZOII members were able to observe debris flows and rock slides in the Boulder , CO area after the recording-setting Boulder flooding . <p> BcCZOII members have begun to undertake a water sampling program to collect as many samples as possible to document this event . <p> BcCZOII member , Harihar Rajaram , was able to photograph Boulder Creek @ @ @ @ @ @ @ @ @ @ . In addition , he was able to witness firsthand the flooding on 75th Street east of the Walden Ponds . <p> 75th stream gaging station , pictures of the flow upstream and downstream of the bridge and the flooding on 75th east of the Walden Ponds . ( photo credit : Harihar Rajaram ) <p> Prior to Wednesday , the single wettest day on record was July 31 , 1919 , when 4.80 inches of rain were recorded , according to Bob Henson , a science writer at the National Center for Atmospheric Research . ( Daily Camera POSTED : 09/12/2013 04:54:28 AM MDT ) 
@@97506480 @1706480/ <h> Poseview Image of CLY in 1YJN <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CD in 1YJN <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of K in 1YJN <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CL in 1YJN <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MG in 1YJN <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NA in 1YJN <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506493 @1706493/ <h> Poseview Image of PPU in 1Q82 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CD in 1Q82 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of K in 1Q82 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CL in 1Q82 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MG in 1Q82 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NA in 1Q82 <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506500 @1706500/ 1450 @qwx861450 <h> SDSCs CAIDA Group to Explore the Internets " Uncharted Territory " <h> NSF-Funded Award to Evaluate New Internet Architectures <p> Published March 9 , 2015 <p> The Center for Applied Internet Data Analysis ( CAIDA ) at the San Diego Supercomputer Center has been awarded a $1.2 million grant from the National Science Foundation ( NSF ) to measure and quantify the changing nature of the Internets topology and what it means for the Internets future in terms of design , operations , scientific study , and public policy . <p> The goals of this three-year project are aligned with the NSF 's Networking Technology and Systems ( NeTS ) program and include : advancing our fundamental understanding of how content distribution dynamics affect ISP network management capabilities ; developing metrics to quantify the impact of emerging interconnection patterns on the resiliency , efficiency , and market power of modern networks ; and revisiting longstanding but now questionable topology modeling assumptions and offering new models that are better empirically grounded . <p> " From both the scientific and policy perspectives , much of the Internets evolving @ @ @ @ @ @ @ @ @ @ , director of CAIDA , a collaboration started in 1997 among commercial , government , and academic research sectors to promote greater cooperation in the engineering and maintenance of a robust , scalable global Internet infrastructure . <p> " As the Internet expands to satisfy the demands and expectations of an ever-increasing percentage of the world 's population , profound changes are occurring at myriad levels : from interconnection structure and traffic dynamics to creating new economic and political issues that need to be addressed , " said Claffy , principal investigator for the new project . " These changes also pose broader challenges for technology investment and future network design so a key goal of this project is to establish a baseline against which to evaluate future Internet architecture designs and implementations . " <p> The project , called NeTS : Large : Collaborative Research : Mapping Interconnection in the Internet : Colocation , Connectivity and Congestion , is being done in collaboration with David Clark , a senior research scientist at the MIT Computer Science and Artificial Intelligence Laboratory ( MIT/CSAIL ) , and his research group . @ @ @ @ @ @ @ @ @ @ of the worlds leading centers of information technology . <p> " Measuring the extent and location of congestion will give us improved insights into the causes of poor performance of applications , and as well insights into the business relationships among the providers that make CAIDA/CSAIL project is being structured into two foundational tasks and a set of research questions that build on those tasks . The first task is to construct a new type of semantically rich Internet map , which will guide the second task : a measurement study of traffic congestion dynamics induced by evolving interconnection and traffic management practices of content delivery networks ( CDNs ) and ISPs . <p> " This new map and measurement techniques will frame our inquiry into issues relevant to network operators , researchers , policymakers , and users , " said Amogh Dhamdhere , a CAIDA research scientist and co-PI of the project . " These inquiries will inform infrastructure resiliency assessments , improve network modeling integrity , as well as stimulate informed Internet policy debates . " <p> A @ @ @ @ @ @ @ @ @ @ 's website . The project is funded under NSF award # 1414177 and scheduled to run through September 2017 . <p> About CAIDA The Center for Applied Internet Data Analysis , formed in 1997 , investigates practical and theoretical aspects of the Internet in order to : provide macroscopic insights into Internet infrastructure , behavior , usage , and evolution , foster a collaborative environment in which data can be acquired , analyzed , and ( as appropriate ) shared , inform and improve the integrity of Internet science , technology , and communications public policies . CAIDA is one of several centers of excellence based at the San Diego Supercomputer Center . <p> About SDSC As an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and cyberinfrastructure , providing resources , services , and expertise to the national research community , including industry and academia . Cyberinfrastructure refers to an accessible , integrated network of computer-based resources and expertise , focused on @ and health IT . In 2015 SDSC will debut Comet , a new petascale supercomputer that will join its data-intensive Gordon cluster . SDSC is a partner in XSEDE ( eXtreme Science and Engineering Discovery Environment ) , the most advanced collection of integrated digital resources and services in the world . 
@@97506503 @1706503/ <p> This directory contains a dump of the UCSC genome annotation database for the Feb. 2009 assembly of the human genome ( hg19 , GRCh37 Genome Reference Consortium Human Reference 37 ( GCA000001405.1 ) ) . The annotations were generated by UCSC and collaborators worldwide . The Feb. 2009 human reference sequence ( GRCh37 ) was produced by the Genome Reference Consortium : LONG ... Note on chrM : Since the release of the UCSC hg19 assembly , the Homo sapiens mitochondrion sequence ( represented as " chrM " in the Genome Browser ) has been replaced in GenBank with the record NC012920 . We have not replaced the original sequence , NC001807 , in the hg19 Genome Browser . We plan to use the Revised Cambridge Reference Sequence ( rCRS , LONG ... in the next human assembly release . Files included in this directory ( updated nightly ) : - *. sql files : the MySQL commands used to create the tables - *. txt.gz files : the database tables in a tab-delimited format compressed with gzip . To see descriptions of the tables underlying Genome Browser @ @ @ @ @ @ @ @ @ @ : LONG ... and click the " describe table schema " button . There is also a " view table schema " link on the configuration page for each track . LONG ... If you plan to download a large file or multiple files from this directory , we recommend you use ftp rather than downloading the files via our website . To do so , ftp to hgdownload.cse.ucsc.edu , then go to the directory **25;4686;TOOLONG . To download multiple files , use the " mget " command : mget ... - or - mget -a ( to download all the files in the directory ) Alternate methods to ftp access . Using an rsync command to download the entire directory : rsync -avzP LONG ... . For a single file , e.g. gc5BaseBw.txt.gz rsync -avzP LONG ... . Or with wget , all files : wget --timestamping LONG ... With wget , a single file : wget --timestamping LONG ... -O gc5BaseBw.txt.gz Please note that some files contents , such as this example gc5BaseBw.txt.gz , will point to the data being hosted in another /gbdb/ location , which refers @ @ @ @ @ @ @ @ @ @ : gunzip <p> . txt.gz The tables can be loaded directly from the . txt.gz compressed file . It is not necessary to uncompress them to load into a database , as shown in the example below . To load one of the tables directly into your local mirror database , for example the table chromInfo : ## create table from the sql definition $ hgsql hg19 &lt; chromInfo.sql ## load data from the txt.gz file $ zcat chromInfo.txt.gz hgsql hg19 --local-infile=1 -e ' LOAD DATA LOCAL INFILE " /dev/stdin " INTO TABLE chromInfo ; ' All the files and tables in this directory are freely usable for any purpose . 
@@97506509 @1706509/ <h> Poseview Image of ZLD in 3CPW <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CD in 3CPW <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of SR in 3CPW <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of ACE in 3CPW <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of K in 3CPW <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of CL in 3CPW <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MG in 3CPW <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NA in 3CPW <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506510 @1706510/ <h> MyPDB Login <h> Advanced Search : Overview <p> Advanced search provides the capability of combining multiple searches of specific types of data in a logical AND or OR . The result is a list of structures that comply with ALL or ANY search criteria , respectively . More complex logic will be provided in a later release of the RCSB PDB . The easiest way to understand how to use advanced search is to work through an example : <h> Step 1 Choose a query type <p> This will produce a long list of possible query types organized by category . If you are not sure of what one to pick select one and a brief description will appear . Once a query type is selected you can also select the question mark to get more help on that query . If it was not what you were seeking , select a different query type . <p> In the above example a query type of UniProt Accession Number(s) is selected and a brief prompt is provided . <h> Step 2 Get a Result Count <p> Click the button @ @ @ @ @ @ @ @ @ @ of PDB structures that match the query . <p> In this example 183 structures are found to contain this UniProt identifier P69905 . Note that this query is matching a sequence , but it is structures that are returned . That is , the structure may contain not only this single sequence but other different sequences as well . All structures will be listed that contain the selected sequence , irrespective of what else they contain . <p> The result count provides a sense of how focused the query has become and whether it needs to be refined further . At this point the user may select Submit Query to list the 183 structures that match the search criteria or further refine the query . <h> 3 . Refining the Query <p> By selecting the + a search criteria can be added . <p> Here we have added a search ( query type deposition date ) for all structures deposited between January 1 , 2008 and November 1 , 2009 . As previously the result count indicates the number of structure that match the single criteria . In this example @ @ @ @ @ @ @ @ @ @ . Final Query Result <p> At this point additional search criteria could be added if desired . Alternatively the Submit Query button will return all structures deposited in the selected time period AND which contain the designated protein sequence from UniProt. <h> 5 Sequence Filtering <p> While not relevant here since structures are selected that match a specific sequence , in other circumstances you may reduce the number of sequence redundant structures returned by using the Remove Similar Sequences option . <p> In the above example the 10008 structures deposited between January 1 , 2008 and November 1 , 2009 are filtered such that if any one of the polypeptide chains in a given structure matches any polypeptide chain in another structure at 90% or greater sequence identity , only one of those structures will be displayed . For this query this has the effect of reducing the number of structures resulting from the query from 10008 to 5523 . A variety of sequence identities can be selected as cutoffs. <h> 6 . Changing the Logic <p> The default is a AND condition between individual components of the query which @ @ @ @ @ @ @ @ @ @ <p> The alternative is Match Any which is a logical OR operator . <h> 7 . Selecting type of Results <p> Several types of searches that are related with Chemical Components like " Chemical Name " or " SMILES " , provide the option to retrieve Ligand ( short term for chemical components ) results instead of the default " Structure " results . By selecting " Ligand " from the " Results " drop box for a Chemical Name search , for the word " adenosine " for example , one may retrieve the Ligands that match the search criteria - instead of the PDB Structures that contain such Ligands . It is possible to use this option even in composite searches as long as one of the sub-queries relates to chemical components . 
@@97506517 @1706517/ <h> Born : Budapest , Hungary , February 17 , 1905 <h> Died : Budapest , Hungary , February 16 , 1977 <h> Founder of Recursive Function Theory <p> R=zsa PTter ( originally Politzer ) grew up in a country torn by war and civil strife in which simply living from day to day was never easy . She made major contributions to mathematical theory for which she received some recognition in her lifetime , but her name , which should be written together with the names of the founders of computational theory ( Gdel , Turing , Church , Kleene ) , is all but forgotten today . In this , she no doubt shares the fate of other Eastern European scientists of the same period . <p> " No other field can offer , to such an extent as mathematics , the joy of discovery , which is perhaps the greatest human joy , " * said R=zsa PTter in her lectures to general audiences , which were often titled " Mathematics is Beautiful . " In the mouth of another , this might be a naive @ @ @ @ @ @ @ @ @ @ <p> PTter enrolled at Etvs Lornd University in 1922 with the intention of studying chemistry but soon discovered that her real interest was mathematics . She studied with world-famous mathematicians , including Lip=t FejTr and J=sef Knrschk , and it was here that she met a longtime collaborator , Lszl= Kalmr , who first called her attention to the subject of recursive functions . <p> After she graduated in 1927 , PTter lived by taking tutoring jobs and high-school teaching . She also began graduate studies . Kalmr told her about Gdel 's work on the subject of incompleteness , ** whereupon she devised her own , different proofs , focusing on the recursive functions used by Gdel . She gave a paper on the recursive functions at the International Congress of Mathematicians in Zurich in 1932 , where she first proposed that such functions be studied as a separate subfield of mathematics . More papers followed , and she received her Ph.D . summa cum laude in 1935 . In 1937 , she became a contributing editor of the Journal of Symbolic Logic . <p> Forbidden to teach by @ @ @ @ @ @ @ @ @ @ to the ghetto in Budapest , PTter continued working during the war years . In 1943 , she wrote and printed a book , Playing with Infinity , a discussion of ideas in number theory and logic for the lay reader . Many copies were destroyed by bombing and the book was not distributed until the war ended . She lost her brother and many friends and fellow mathematicians to Fascism , and a foreword to later editions of Playing with Infinity ? memorializes them . <p> In 1945 , the war over , she obtained her first regular position at the Budapest Teachers ' College . In 1951 she published a monograph , Recursive Functions , which went through many editions and which earned her the state 's Kossuth Award . When the teachers ' college was closed in 1955 , she became a professor at Etvs Lornd University , until her retirement in 1975 . In 1976 , she published Recursive Functions in Computer Theory . <p> She was called Aunt R=zsa by generations of students and worked to increase opportunities in mathematics for girls and young women @ @ @ @ @ @ @ @ @ @ 1977 . In her eulogy , her student Ferenc Genzwein recalled that she taught " that facts are only good for bursting open the wrappings of the mind and spirit " in the " endless search for truth . " <p> * " Mathematics is Beautiful , " an address delivered to high school teachers and students in 1963 and published in the journal Mathematik in der Schule 2 ( 1964 ) , pp. 81-90 . An English translation by Leon Harkleroad ( Cornell University ) was published in The Mathematical Intelligencer 12 ( 1990 ) , pp. 58-64 . We are indebted to Leon Harkleroad for permission to quote from published and unpublished materials . 
@@97506524 @1706524/ <p> Edit /etc/default/useradd to set the HOME variable to /users . That will make " rocks sync users " change the new lines /etc/auto.home correctly , as well as create them in /users instead of /state/partition1/home or /export/home . After a new install , have a regular user ( you ? ) ssh to each node , so they get added to /. ssh/knownhosts . Copy those entries to /etc/ssh/sshknownhosts . That will let you skip the " tentakel pwd " part for each new user . Here 's what I do as root : # useradd dude # passwd dude # smbpasswd -a dude # su - dude # to create the ssh keys , empty passphrase # exit # back to being root # rocks sync users That 's it . Bart &gt; -----Original Message----- &gt; From : **30;4746;TOOLONG at sdsc.edu mailto:npaci-rocks- &gt;discussion-bounces at sdsc.edu On Behalf Of Larry Baker &gt; Sent : Tuesday , January 24 , 2012 11:37 AM &gt; To : Discussion of Rocks Clusters &gt; Subject : Rocks-Discuss Create new user account &gt;&gt; Please include a section in the next version of the @ @ @ @ @ @ @ @ @ @ accounts , and how to &gt; verify that a new user account has been set up properly . There is no &gt; mention of how to do that in the current ( 5.4. x ) Base Roll User 's &gt; Guide . I followed my old notes from Rocks 4.3 ( /usr/sbin/useradd , &gt; rocks sync users ) , which seem to work . I have to manually edit /etc/ &gt; auto.home to add the new autofs mapping to /home/userid . I would have &gt; thought rocks sync users would have done that . I put user home &gt; directories in /users , not /state/partition1 , so maybe that explains &gt; why some of the built-in procedures do not work . I verify ( grep &gt; userid ) the new userid entries in /etc/passwd , /etc/shadow ( plus /etc/ &gt; group and /etc/gshadow if a new group is needed ) and the autofs &gt; mapping in /etc/auto.home . I make sure the home directory gets &gt; created in /users/userid , I run passwd to set an initial password , &gt; then su -l userid to create the SSH keys ( @ @ @ @ @ @ @ @ @ @ I run rocks sync users , then su -l userid , tentakel pwd , &gt; which adds all the compute nodes to the list of known hosts . I think &gt; that covers what should result for a new user account . Am I missing &gt; anything ? &gt;&gt; Larry Baker &gt; US Geological Survey &gt; 650-329-5608 &gt;baker at usgs.gov This message contains information that may be confidential , privileged or otherwise protected by law from disclosure . It is intended for the exclusive use of the Addressee(s) . Unless you are the addressee or authorized agent of the addressee , you may not review , copy , distribute or disclose to anyone the message or any information contained within . If you have received this message in error , please contact the sender by electronic reply to email at environcorp.com and immediately delete all copies of the message . 
@@97506528 @1706528/ <h> Born : New York , New York , December 9 , 1906 <h> Died : Arlington , Virginia , January 1 , 1992 <h> Pioneer Computer Scientist <p> The new discipline of computing and the sciences that depend upon it have led the way in making space for women 's participation on an equal basis . That was in some ways true for Grace Murray Hopper , and it is all the more true for women today because of Hopper 's work . <p> Grace Brewster Murray graduated from Vassar with a B.A. in mathematics in 1928 and worked under algebraist Oystein Ore at Yale for her M.A. ( 1930 ) and Ph.D . ( 1934 ) . She married Vincent Foster Hopper , an educator , in 1930 and began teaching mathematics at Vassar in 1931 . She had achieved the rank of associate professor in 1941 when she won a faculty fellowship for study at New York University 's Courant Institute for Mathematics . <p> Hopper had come from a family with military traditions , thus it was not surprising to anyone when she resigned her @ @ @ @ @ @ @ @ @ @ for Voluntary Emergency Service ) in December 1943 . She was commissioned a lieutenant in July 1944 and reported to the Bureau of Ordnance Computation Project at Harvard University , where she was the third person to join the research team of professor ( and Naval Reserve lieutenant ) Howard H. Aiken . She recalled that he greeted her with the words , " Where the hell have you been ? " and pointed to his electromechanical Mark I computing machine , saying " Here , compute the coefficients of the arc tangent series by next Thursday . " <p> Hopper plunged in and learned to program the machine , putting together a 500-page Manual of Operations for the Automatic Sequence-Controlled Calculator in which she outlined the fundamental operating principles of computing machines . By the end of World War II in 1945 , Hopper was working on the Mark II version of the machine . Although her marriage was dissolved at this point , and though she had no children , she did not resume her maiden name . Hopper was appointed to the Harvard faculty as a research @ @ @ @ @ @ @ @ @ @ Eckert-Mauchly Corporation . <p> Hopper never again held only one job at a time . She remained associated with Eckert-Mauchly and its successors ( Remington-Rand , Sperry-Rand , and Univac ) until her official " retirement " in 1971 . Her work took her back and forth among institutions in the military , private industry , business , and academe . In December 1983 she was promoted to commodore in a ceremony at the White House . When the post of commodore was merged with that of rear admiral , two years later , she became Admiral Hopper . She was one of the first software engineers and , indeed , one of the most incisive strategic " futurists " in the world of computing . <p> Perhaps her best-known contribution to computing was the invention of the compiler , the intermediate program that translates English language instructions into the language of the target computer . She did this , she said , because she was lazy and hoped that " the programmer may return to being a mathematician . " Her work embodied or foreshadowed enormous numbers of developments @ @ @ @ @ @ @ @ @ @ , formula translation , relative addressing , the linking loader , code optimization , and even symbolic manipulation of the kind embodied in Mathematica and Maple . <p> Throughout her life , it was her service to her country of which she was most proud . Appropriately , Admiral Hopper was buried with full Naval honors at Arlington National Cemetery on January 7 , 1992. 
@@97506541 @1706541/ <p> I downloaded the binary files from the SGE website ( the architecture of my client is different from the architecture of my cluster ) and exported the environment variables ( it seems to be ok ) . But when I try a command ( qstat , for example ) , I get this message : error : commlib error : access denied ( client IP resolved to host name " **26;4778;TOOLONG " . This is not identical to clients host name " localhost.localdomain " ) unable to contact qmaster using port 536 on host " parafisio.ufjf.br " Do you know what is this ? ( Sorry for bugging you ... ) Thanks , Vinicius Vieira On 11/7/06 , Bernhard Gschaider &lt;Bernhard.Gschaider at ice-sf.at&gt; wrote : &gt;&gt; &gt;&gt;&gt;&gt;&gt; On Tue , 7 Nov 2006 15:53:06 -0200 &gt; &gt;&gt;&gt;&gt;&gt; " VV " == Vinfcius Vieira &lt;vfvieira at gmail.com&gt; wrote : &gt;&gt; VV&gt; Hello , I 'm trying to configure a machine outside my cluster as &gt; VV&gt; a submit host . I have already mounted the /opt/gridengine &gt; VV&gt; directory from the frontend in my client machine and I have @ @ @ @ @ @ @ @ @ @ host with " qconf &gt; VV&gt; -as " command . But now I do n't know how to use this submit &gt; VV&gt; host . Thanks for anyone who can help me to solve this &gt; VV&gt; problem . Vinicius Vieira &gt;&gt; Hi ! &gt;&gt; Appart from what Wilfrid said in the other mailing ( firewall ) I did it &gt; this way ( your milage may vary , I did it on CentOS , which is very &gt; similar to Rocks ) : &gt;&gt; - I pulled the sge-V60u8-1.rpm from the frontend and installed it on &gt; the submit host ( this created the **27;4806;TOOLONG . No need &gt; to mount it from the frontend ) &gt;&gt; - added a file **28;4835;TOOLONG on the system ( the &gt; variables Wilfrid was talking about ) : &gt;&gt; export SGEROOT=/opt/gridengine &gt;&gt; if -e $SGEROOT ; then &gt; **26;4865;TOOLONG ; export SGEARCH &gt;&gt; export SGEQMASTERPORT=536 &gt; export **26;4893;TOOLONG : $PATH &gt; export MANPATH=$SGEROOT/man : manpath &gt; export LONG ... &gt; fi &gt;&gt; - added a file **41;4921;TOOLONG with one &gt; line ( the " outer " -name of the cluster ) : @ @ @ @ @ @ @ @ @ @ adminuser root &gt; defaultdomain none &gt; ignorefqdn true &gt; spoolingmethod classic &gt; spoolinglib libspoolc &gt; spoolingparams **30;5033;TOOLONG &gt; binarypath /opt/gridengine/bin &gt; qmasterspooldir **37;5065;TOOLONG &gt; securitymode none &gt;&gt; - add the machine as a submit host obviously &gt;&gt; ( I think that 's all ) &gt;&gt; PS : without any changes you can even use the machine as a execution &gt; host ( which is quite nice for small " test " -jobs . You just have to maker sure &gt; that regular jobs end up on that machine , with fixed attributes ) &gt;&gt; 
@@97506550 @1706550/ <p> Patches and tools developed by Tsutomu Shimomura to manipulate the Alcatel firmware . This includes software to unpack and re-pack the firmware into a useable form , an example list of which bytes to patch , and a tool for applying patches to close the reported vulnerabilities . We do not recommend that anyone actually apply these patches , rather one should ask their service provider for a fix . <p> A tool to determine the " EXPERT " mode password for an Alcatel Speed Touch . NOTE : This applies to the specific firmware version cited in the original advisory . It is not expected to work with any newer firmware . Alcatel provided newer firmware to its customers in late 2001. 
@@97506556 @1706556/ <h> Tutorial <h> Workflow <p> The NaPDoS bioinformatic pipeline is shown in the following diagram . The web interface to this pipeline is divided into five consecutive steps . Click on the link for each step to get detailed instructions . <p> Clicking on the " seek " button brings up a new window showing estimated processing time and search parameters to be used . <p> Analysis does not actually begin until the " submit job " button on this new window is clicked . <h> Advanced Settings <p> Default parameter settings are recommended for routine use . However , in some cases , users may wish to boost sensitivity by choosing less stringent HMM or BLAST criteria , or shorter minimum sequence lengths . Users should be aware that these adjustments may increase false positive predictions . Conversely , selectivity can be improved by using lower e-values and longer minimum match lengths , at the cost of decresed sensitivity . <h> HMM search <p> For genomic sequences only , preliminary domain candidate information based on Hidden Markov Model ( HMM ) search is displayed on a separate page @ @ @ @ @ @ @ @ @ @ total number and positions of PKS/NRPS operons present in a genomic or metagenomic sequence set . However , these intial results should be interpreted with some caution , for the following reasons : <p> For incomplete draft genomes or metagenomes , some candidates detected at this stage may represent partial or overlapping gene fragments or duplications . <p> Some candidates identified by HMM search only may encode protein fragments with PKS/NRPS-related functionality ( e.g. fatty acid synthases ) , but do not actually produce compounds traditionally classified as natural products . <p> More stringent search methods are applied in later stages of the NaPDoS analysis pipeline to help resolve these issues . <h> BLAST search <p> A BLAST search is performed against curated reference database examples to identify matches to known PKS/NRPS pathways . Some suggested guidelines for interpreting blast scores are presented below . To proceed with further analysis , one or more candidate sequences must be selected using check boxes . Three different output options are available : <p> Output selected sequences provides trimmed candidate sequences in a fasta file format . These sequences can then be used @ @ @ @ @ @ @ @ @ @ recommended ) , in case similar domains might not yet have been added to NaPDoS . <p> Output alignment will display MUSCLE 3 results for selected candidates and their blast matches in MSF format . The MSF file can be downloaded for additional offline analysis , for example to make manual adjustments to the alignment , or create custom trees using alternative programs . <p> Construct tree progresses to the next stage of the NaPDoS analysis pipeline , inserting candidate sequences plus their blast matches into a manually curated alignment of previously characterized database sequences . <p> In some cases , the number of candidate matches on this page may be fewer than the number reported on the earlier genomic summary page , reflecting differences between HMM and BLAST stringencies used for the analysis . <h> Tree Construction <p> Selected candidate sequences plus their blast matches are trimmed and inserted into a manually curated reference alignment , keeping the original reference alignment intact . This alignment is used to build a tree , which is often more useful than blast results alone in predicting whether pathway products for candidate domains @ @ @ @ @ @ @ @ @ @ examples 4 . <h> Tree output options <p> Phylogenetic domain trees are built using FASTTREE to estimate maximum likelihood 5 . <p> Tree building does not actually begin until the " submit job " button is clicked . <p> After the tree is built , users choose either Newick ( plain text ) format or an SVG graphics image as an output format . <p> User sequences are highlighted in the SVG graphics image format with red dots , as shown in the example below . <p> FastTree output does not include bootstrap values . However , the program does provide confidence values , which are included in the Newick format output . These values can be visualized by opening the Newick file with most stand-alone GUI interface tree viewing programs , for example the open source software FigTree . <h> Interpreting Results <p> BLAST hits for KS or C domains with more than 85%-90% identity at the amino acid level indicate that the query domains may be associated with the production of the same or a similar compound as those produced by the reference pathway . If you detect @ @ @ @ @ @ @ @ @ @ in the NaPDoS database , a BLAST search against the NCBI nr database is recommended . Although we will update the database regularly , the NaPDoS database does not contain all characterized biosynthetic pathways . If this search does not find any known domain with more than 85% identity , the biosynthetic gene cluster has most likely not yet been characterized . In these cases it is possible that the encoded compound is new . <p> Constructing a phylogenetic tree can classify the domains , which may not necessarily be shown by the best BLAST hits . This classification can be informative in terms of predicting the type of compound produced . The domain classes have been defined based on the clades observed in the reference trees 6 . 
@@97506569 @1706569/ <h> Poseview Image of NAG in 3KAS <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MAN in 3KAS <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of BMA in 3KAS <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of FUC in 3KAS <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of PO4 in 3KAS <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of K in 3KAS <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506570 @1706570/ <h> SDSC to Participate in New Cancer Cell Mapping Initiative <h> Breaking the Bottleneck in Interpreting Cancer Genomes <p> Researchers from the University of California , San Diego School of Medicine and University of California , San Francisco with support from a diverse team of collaborators including the San Diego Supercomputer Center ( SDSC ) have launched an ambitious new project to determine how all of the components of a cancer cell interact . <p> " Were going to draw the complete wiring diagram of a cancer cell , " said Nevan Krogan , director of the UC San Francisco division of QB3 , a quantitative biosciences research institute , in announcing the Cancer Cell Map Initiative , or CCMI . <p> Krogan is an investigator at Gladstone Institutes and co-director of CCMI with Trey Ideker , chief of medical genetics in the UC San Diego Department of Medicine and founder of the UC San Diego Center for Computational Biology &amp; Bioinformatics . <p> The CCMI will provide key infrastructure for the recently announced alliance between UC San Diego Health Sciences and San Diego-based Human Longevity Inc. , which @ @ @ @ @ @ @ @ @ @ Diego cancer patients . It also will leverage resources and information from the National Cancer Institute ( NCI ) , including large databases of cancer genomes and pathways that are being developed in collaboration with SDSC , an Organized Research Unit of UC San Diego , and UC Santa Cruz . <p> " We have the genomic information already , " said Ideker . " The bottleneck is how to interpret the cancer genomes . A comprehensive map of cancer cells would help and accelerate the development of personalized therapy , the central aim of precision medicine. <p> The CCMI combines expertise at UC San Diego in extracting knowledge from big biomedical data sets with advances developed at UC San Francisco for experimentally interrogating the structure and function of cells . It is a multi-million dollar collaboration between the UC San Diego Moores Cancer Center and the UCSF Helen Diller Family Comprehensive Cancer Center ; funded by QB3 at UCSF , UC San Diego Health Sciences and support from Fred Luddy , founder of ServiceNow , a provider of enterprise service management software . <p> The full press release on @ @ @ @ @ @ @ @ @ @ on the UC San Diego website. 
@@97506571 @1706571/ <h> Poseview Image of SIA in 1OW0 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NDG in 1OW0 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of NAG in 1OW0 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of GAL in 1OW0 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of MAN in 1OW0 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of BMA in 1OW0 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of FUC in 1OW0 <h> Help 1451 @qwx861451 1452 @qwx861452 <h> Poseview Image of FUL in 1OW0 <h> Help 1451 @qwx861451 1452 @qwx861452 
@@97506588 @1706588/ <p> Henry , Changing the ip address of the head node is possible , but very tricky to get right . Using DHCP will work just fine , except that you will want to set your DHCP server to give a permanent lease to the head head node , it is not good for the head node ip address to change . The ip address of the head node is embedded in too many configuration files for the cluster . In fact , even the nodes only use DHCP for their initial configuration and then set a static IP that matches the one they received from the DHCP server on the head node . The best and easiest way to change the head node ip is to reinstall the headnode using the restore roll . The other way is to search the database for every occurrence of the old ip and replace it with the new one . Then do ROCKS SYNC CONFIG . Then search the contents of /etc for any occurrence of the original ip and replace them all with the new . Next you want to @ @ @ @ @ @ @ @ @ @ replace it with the new one . If you got them all there the head node should be good . Now you need to reinstall all your backend nodes , or go through the same process on them to make sure they actually understand who the head node is . I recently went through this on ROCKS 4.1 due to a university name change which resulted in a change of our ip range . It was nasty . Scott -----Original Message----- From : **30;5104;TOOLONG at sdsc.edu **37;5136;TOOLONG at sdsc.edu On Behalf Of Henry Xu Sent : Friday , December 05 , 2008 7:50 AM To : npaci-rocks-discussion Subject : Rocks-Discuss How to change FrontEnd 's eth1 IP setting ? Hello , I got 2 questions : 1 . Does rocks support eth1 of frontend to work on DHCP mode ? I changed system file **41;5175;TOOLONG to enable the interface get IP from DHCP server in public network . After restart network service , /etc/sysconfig/network still contained the old gateway ip. " rocks sync config " did not work then . So , how to set eth1 with DHCP by rocks @ @ @ @ @ @ @ @ @ @ to Static by edit **41;5218;TOOLONG . Then issued the following commands to change IP of eth1 : # rocks set host interface ip cluster eth1 192.168.0.3 # rocks set host interface gateway cluster eth1 192.168.0.1 # rocks set network netmask public 255.255.255.0 # rocks sync config Now IP is set successfully . But the following error message continue to display on console : error : comlib error : ca n't connect to service ( no route to host ) error : unable to contact qmaster using port 536 on host " cluster.hpc.org " Command " qstat " has correct response . All compute nodes pop up the same error message when they are reboot or shutdown . Command " qstat " on compute node also got the same error message . After checking old posts , I found somebody use the following commands to change eth1 's IP : # echo ' update appglobals set value= " &lt;new IP address&gt; " where value= " &lt;OLD IP address&gt; " ' mysql -u apache cluster # echo ' update networks set IP= " &lt;new IP address&gt; " where IP= " &lt;OLD IP @ @ @ @ @ @ @ @ @ @ They do n't tell how to set netmask and gateway . So could you let me know the correct method to change eth1 's IP of frontend node ? root at cluster # rocks list host interface HOST SUBNET IFACE MAC IP NETMASK GATEWAY MODULE NAME cluster : private eth0 00:22:19:8a:f7:0e 10.0.0.1 255.255.0.0 ----------- bnx2 cluster cluster : public eth1 00:22:19:8a:f7:10 192.168.0.3 255.255.255.0 192.168.0.1 bnx2 cluster.hpc.org compute-1-1 : private eth0 00:22:19:91:38:4b 10.0.255.254 255.255.0.0 ----------- bnx2 compute-1-1 compute-1-1 : ------- eth1 00:22:19:91:38:4d ------------ ------------- ----------- bnx2 --------------- compute-1-2 : private eth0 00:22:19:91:36:62 10.0.255.253 255.255.0.0 ----------- bnx2 compute-1-2 compute-1-2 : ------- eth1 00:22:19:91:36:64 ------------ ------------- ----------- bnx2 --------------- compute-1-3 : private eth0 00:22:19:91:3c:5c 10.0.255.252 255.255.0.0 ----------- bnx2 compute-1-3 compute-1-3 : ------- eth1 00:22:19:91:3c:5e ------------ ------------- ----------- bnx2 --------------- compute-1-4 : private eth0 00:22:19:91:3a:01 10.0.255.251 255.255.0.0 ----------- bnx2 compute-1-4 compute-1-4 : ------- eth1 00:22:19:91:3a:03 ------------ ------------- ----------- bnx2 --------------- compute-1-5 : private eth0 00:22:19:91:38:33 10.0.255.250 255.255.0.0 ----------- bnx2 compute-1-5 compute-1-5 : ------- eth1 00:22:19:91:38:35 ------------ ------------- ----------- bnx2 --------------- Regards , Henry -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... **29;5261;TOOLONG 
@@97506591 @1706591/ <h> Eliminating Plaintext Passwords on Your Network A work-in-progress , based on experiences at the San Diego Supercomputer Center . <p> Introduction <p> At SDSC , eliminating plaintext passwords is one of a handful of key strategies we have implemented which have been quite effective at preventing intrusions to our hosts . <p> Motivation <p> We manage to support thousands of users , spread out across the planet , without mandating homogeneity of client software or simply not providing service . Our users can remotely access our systems as effectively as they can locally . We could not do so and keep our systems secure without eliminating plaintext passwords . <p> This stategy and effort to eliminate plaintext passwords came out of several security yearly reviews performed at SDSC from 1994 through 1997 . In these reviews , we analyzed all the security events from those years and determined the underlying causes of the problems . It was obvious from these analyses that the easiest and most common way for our user accounts ( and hosts ) to be compromised was network password sniffers running at the home sites @ @ @ @ @ @ @ @ @ @ 1997 ) , sniffers running at other sites accounted for almost all of our significant security events . <p> ... and many others , including most Windows remote access programs , many other file sharing tools , etc . <p> Why is this a problem ? Because anyone monitoring ( eavedropping ) on the network tranmission can intercept and use those passwords to gain access to your systems . And most of the intrusions we 've seen include the use of a sniffer ( eavesdropping software ) : a intruder will install a sniffer just to see if they can pick up some good passwords while they are in the neighborhood . <p> Compromise of a user password is on of the most difficult intrusions to detect . When a valid username and password is presented , how does the system know whether or not it is being presented by the actual authorized user ? How do you , as a system administrator , know whether or not a particular session belongs to the actual user or an intruder with a stolen password ? You might be able to make @ @ @ @ @ @ @ @ @ @ or some pattern of behavior like time of access , but that 's prone to error and rather cumbersome to implement . <p> The best strategy is to prevent interception of passwords in the first place . This can be done in several ways : <p> Keep all machines standalone , without any network access . <p> While this certainly rememdies the password interception problem , it also can limit the usefulness of your computers . <p> Only allow services on networks and systems that you control , so that sniffers ca n't be installed . <p> This too can work , but limits what your users can do remotely , while travelling , etc . <p> Use software and protocols which either do not transmit passwords , or encrypt passwords so that the intercepted information is not useable by an intruder . <p> ( We like number 3 . ) <p> The Technology <p> The key to eliminating plaintext passwords is realizing that there is no one solution that fixes everything . Instead , we rely on a combination of solutions for the different services we support . In @ @ @ @ @ @ @ @ @ @ users to choose their own client software . 
@@97506604 @1706604/ <p> The disks that constitute the boot drives for AIX , must be disks that are supported to boot on RS/6000 platforms . A machine and disk configuration may be queried to determine if that combination of machine and disk supports booting from disk : bootinfo -B hdiskX If the command returns a " 1 " then it will be bootable by AIX . Any other value indicates that this disk is not a candidate for rootvg mirroring . Procedure : The following steps assume the user has rootvg contained on hdisk0 and is attempting to mirror the rootvg to a new disk hdisk1. 1 . Extend rootvg to hdisk1 : extendvg rootvg hdisk1 If the user encounters the error message : 0516-050 Not enough descriptor space left in this volume group . Either try adding a smaller PV or use another volume group . Then the user may not add hdisk1 to rootvg for mirroring . The user may attempt to mirror rootvg 's logical volumes to another disk that already exists in rootvg and meets the criteria and restrictions listed in the " restrictions " section above @ @ @ @ @ @ @ @ @ @ disk to the rootvg . If neither option is possible , then mirroring rootvg can not be performed on this system . 2 . Disable QUORUM , by running the following : chvg -Qn rootvg 3 . Mirror the logical volumes that make up the AIX operating system : mklvcopy hd1 2 hdisk1 # /home file system mklvcopy hd2 2 hdisk1 # /usr file system mklvcopy hd3 2 hdisk1 # /tmp file system mklvcopy hd4 2 hdisk1 # / ( root ) file system mklvcopy hd5 2 hdisk1 # blv , boot logical volume mklvcopy hd6 2 hdisk1 # paging space ( if the user has other paging devices , rootvg and non-rootvg , they must also mirror those logical volumes in addition to hd6 ) mklvcopy hd8 2 hdisk1 # file system log mklvcopy hd9var 2 hdisk1 # /var file system If hd5 consists of more than one logical partition , then after mirroring hd5 the user must verify that the mirrored copy of hd5 resides on contiguous physical partitions . This can be verified with the command : lslv -m hd5 If the mirrored hd5 partitions are not @ @ @ @ @ @ @ @ @ @ of hd5 ( on hdisk1 ) and rerun the mklvcopy for hd5 using the " -m " option . The user should consult documentation on the usage of the " -m " option for mklvcopy. 4 . Synchronize the newly created mirrors : syncvg -v rootvg 5 . Bosboot to initialize all boot records and devices : bosboot -a 6 . Initialize the boot list : bootlist -m normal hdisk0 hdisk1 Warning : Even though this command identifies the list of possible boot disks it does not guarantee that the system will boot from the alternate disk in all cases involving failures of the first disk . In such situations , it may be necessary for the user to boot from the installation/maintenance media , select maintenance , reissue the bootlist command leaving out the failing disk , and then reboot . On some models , firmware provides a utility for selecting the boot device at boot time . This may also be used to force the system to boot from the alternate disk . 7 . Shutdown and reboot the system : shutdown -Fr This is so that the @ @ @ @ @ @ @ @ @ @ a broken disk <p> 1 . To replace a bad disk drive in a mirrored volume group , enter unmirrorvg workvg hdisk7 reducevg workvg hdisk7 rmdev -l hdisk7 -d replace the disk drive , let the drive be renamed hdisk7 extendvg workvg hdisk7 mirrorvg workvg Note : By default in this example , mirrorvg will try to create 2 copies for logical volumes in workvg . It will try to create the new mirrors onto the replaced disk drive . However , if the original system had been triply mirrored , there may be no new mirrors created onto hdisk7 , as other copies may already exist for the logical volumes . 2 . To sync the newly created mirrors in the background , enter : mirrorvg -S -c 3 workvg 3 . To create an exact mapped volume group , enter : mirrorvg -m datavg hdisk2 hdisk3 
@@97506607 @1706607/ <h> Differences between versions 3.0 and 3.2 <p> This is not a comprehensive list , but it does give most of the major changes . <p> 3.0 sessions can not be imported to 3.2 . You need to contact the Workbench administrators bwbhelp@sdsc.edu to do this . If you have been automatically moved over from 3.0 at the old Biology Workbench site , this conversion will have been done automatically -- please check your account to see if the conversion was successful . We no longer have access to the Biology Workbench 3.0 server , so if you had data there that was n't automatically converted in early February , 2000 , it is unrecoverable . <p> The SRS Database search has been replaced by Ndjinn Multiple Database Search . Ndjinn is a text-based database engine , and brief instructions on its use are provided on the setup page and in the help file . <p> The MSASHADE alignment coloring program has been replaced by BOXSHADE . BOXSHADE is a more complete implementation of the Boxshade program , and many additional options are available in this module . <p> @ @ @ @ @ @ @ @ @ @ to allow one to do the same analyses that PROTDIST and PROTPARS do on protein alignments <p> MOTIFGREPDB and MOTIFGREP have been replaced by PATTERNMATCH and PATTERNMATCHDB <p> The View tool now allows the user to view the sequences or alignments in various formats . In addition , it allows the user to download a sequence ( or all the sequences ) in the format being viewed ( the icons on the title bar are for downloading -- see the help file for View for more information ) . <p> The Add tool now can handle multiple-sequence files in the Protein and Nucleic modes , and has a few additional options , like color-coding of non-standard amino acid or nucleic coes . Read the help file on Add and Edit for more information on the changes in these tools . <p> Sessions are now handled somewhat differently . There is no longer a " Default Mode " in which the data gets lost after exiting the browser . Instead , one is placed in a " Default Session " when they enter the Biology Workbench . All data in the @ @ @ @ @ @ @ @ @ @ it . The user is allowed to rename the default session , which then creates a new , empty default sessions , and the data that was formerly in the default session will be a session with the new name that was specified . The default session can also be copied , but the users will probably find it easiest just to create a new session and work from there . <p> Multiple database selection is now available within all the BLAST tools , the FASTA and SSEARCH tools , and PATTERNMATCHDB . <p> PSIBLAST has been added in a limited fashion . The ability to use user-defined postition-specific matrices is not yet available , but otherwise this should be a fairly useful implementation of PSIBLAST . <p> The non-redundant protein database has changed . We now make our own non-redundant databases , comprised of all the protein sequences in all the databases available in the Biology Workbench . In the past , we used the Genbank non-redundant protein database . <p> Minor updates and interface improvements have been made in a large number of programs . 
@@97506613 @1706613/ <p> From reijo.rasinkangas at oulu.fi Fri Sep 1 04:12:19 2006 From : reijo.rasinkangas at oulu.fi ( Reijo Rasinkangas ) Date : Fri , 01 Sep 2006 14:12:19 +0300 Subject : Rocks-Discuss Installation problem In-Reply-To : LONG ... References : **32;5292;TOOLONG LONG ... Message-ID : **32;5326;TOOLONG Hi Philip , Philip Papadopoulos wrote : &gt; Here 's what I would do , since you already have the your node mac &gt; addresses and what you want them to be named , I would write a small &gt; script that does the following . &gt; &gt; for each ( node , macaddrees , ip ) &gt; insert-ethers --mac= " mac " --ip= " ip " --name= " node " &gt; --netmask= " netmask " --appliance= " Compute " --norestart --batch Thanks , this was exactly the kind of advice I was looking for ! I managed to add all 45 nodes into the database with a php script . The --name options did not work , but I just left it out : I got the compute-0-x naming in the right order by giving the commands in the same order as the @ @ @ @ @ @ @ @ @ @ fool around , though , since even with the above parameters the /etc/dhcpd.conf etc. files were rewritten after each insert-ethers command ! However , since the created files also worked right away ( and the mandriva nodes function now as before ) , this was only a minor inconvenience ( however , it is better to warn users beforehand if doing this kind of thing ) . A couple of questions more . When I look at the mySQL database , table nodes has defaulted to CPUs=1 for all our nodes . In reality , there are two CPUs in each . I guess it is ok the change this by hand in the database and run " insert-ethers --update " ? I do n't know which configuration files this parameter relates to , but most likely it should be correct . Trickier problem is that the first attempt to install Rocks in a node failed still . Similar problem was described in LONG ... so this looks like a Rocks bug . However , I must confess that I made also one mistake here . After adding all our @ @ @ @ @ @ @ @ @ @ our Ethernet Switch . Of course I used --appliance= " network " , but this failed . As I though it more important to have the switch in the dhcpd.conf than to have the database exactly correct , I added the switch as a node . This seems to work , but of course there is a possibility that some problems will arise because of this . Any ideas how to clean this mess ? Could it be enough to change the Membership in nodes table from compute ( 2 ) to Ethernet Switch ( 5 ) ? Regards , Reijo http : //cc.oulu.fi/rar/ From llefton at math.gatech.edu Fri Sep 1 05:19:24 2006 From : llefton at math.gatech.edu ( Lew Lefton ) Date : Fri , 01 Sep 2006 08:19:24 -0400 Subject : Rocks-Discuss bug with substituting OS rolls with RHEL or RHEL rebuild CDs In-Reply-To : References : Message-ID : **40;5360;TOOLONG Greg Bruno wrote : &gt; we have confirmed that in rocks 4.2 , you ca n't substitute the OS rolls &gt; with RHEL or RHEL rebuild CDs ( e.g. , CentOS or ScientificLinux ) . &gt; &gt; we @ @ @ @ @ @ @ @ @ @ maintenance release &gt; of 4.2 to address it . &gt; &gt; i apologise for this bug as this important test case was not executed &gt; during the final testing of rocks 4.2. &gt; &gt; we plan to get the maintenance release out as quickly as possible , but &gt; we still need to run through a couple more test cases before we &gt; release it . Can you provide an unofficial , but hopefully realistic estimate of when the 4.2.1 release will be available ? I 'm currently trying to decide whether to rebuild a cluster with 4.1 or wait for the 4.2 maintenance release . Thanks , Lew From wjerikson at hampshire.edu Fri Sep 1 06:35:12 2006 From : wjerikson at hampshire.edu ( Wm . Josiah Erikson ) Date : Fri , 01 Sep 2006 09:35:12 -0400 Subject : Rocks-Discuss second interface and DHCP Message-ID : **38;5402;TOOLONG Hello all , I have a ROCKS 4.1 cluster of 24 nodes that have two ethernet interfaces . I decided to try out putting the second interface of the compute nodes on the render-farm network by plugging the second interface into the @ @ @ @ @ @ @ @ @ @ I enabled the second interface in BIOS , reinstalled the compute node , and it brought up eth0 and eth1 , and they were the right names for the right interfaces , and eth1 even got a DHCP address just like I wanted it to , which was all totally awesome , except that eth1 comes up after eth0 , which means that /etc/resolv.conf gets overwritten with the non-cluster version ... is there any way to get around this , like change the order of interface initialization , or should I just assign a static IP to eth1 like it describes in the docs ? Thanks , -Josiah From jeremymann at gmail.com Fri Sep 1 06:37:25 2006 From : jeremymann at gmail.com ( Jeremy Mann ) Date : Fri , 1 Sep 2006 08:37:25 -0500 Subject : Rocks-Discuss Figuring out dbreport In-Reply-To : References : LONG ... Message-ID : LONG ... Greg , I 've done that but all the ks.cfg file contains is : # # Kickstart Generator version 4.1 # # # Node Traversal Order # # # # Debugging Information # # %packages --ignoredeps --ignoremissing %pre %installclass @ @ @ @ @ @ @ @ @ @ , Jeremy Mann wrote : &gt; &gt; On a hunch , I moved the 2 XML scripts out of &gt; &gt; **38;5442;TOOLONG and rebuilt the distro with &gt; &gt; rocks-dist . Same problem . Is there some utitlity that can tell me &gt; &gt; where and why kickstart is n't working ? &gt; &gt; we start with : &gt; &gt; # dbreport kickstart compute-0-0 &gt; /tmp/ks.cfg &gt; &gt; then if you see error messages from the above command line , then use &gt; them to help track down the issue . &gt; &gt; - gb &gt; -- Jeremy From jimmy.wilcox at ntlworld.com Fri Sep 1 00:26:12 2006 From : jimmy.wilcox at ntlworld.com ( jimmy.wilcox at ntlworld.com ) Date : Fri , 1 Sep 2006 8:26:12 +0100 Subject : Rocks-Discuss ROCJS 4.2 install with USB floppy Message-ID : LONG ... Hi I am trying to install Rocks 4.2 on a Intel SR2500PALLX sytem . I need to provide a E1000 network driver disk during install . This system only supports a USB floppy drive . Rocks sees the floppy as /dev/sdb . The frontened installs fine . While installing a compute-node @ @ @ @ @ @ @ @ @ @ install fails as it tries to mount /tmp/sdb . This does not exist on the compute node . I tried re-installing the head node , removing the USB floppy as soon as the E1000 driver was found but this had no effect . I also tried customising compute-node partitions forcing the partitons onto /dev/sda but this did not work either . Any suggestions ? Jim **41;5482;TOOLONG Email sent from www.ntlworld.com Virus-checked using McAfee(R) Software Visit **25;5525;TOOLONG for more information From echrzano at cscf.cs.uwaterloo.ca Fri Sep 1 04:21:47 2006 From : echrzano at cscf.cs.uwaterloo.ca ( Edward Chrzanowski CSCF ) Date : Fri , 1 Sep 2006 07:21:47 -0400 ( EDT ) Subject : Rocks-Discuss Matlab and mixed architecture Message-ID : LONG ... &gt; Hello , &gt; &gt; I am building a ROCKS cluster , and one of the main applications that &gt; will be used is Matlab . The machines in the cluster are 64-bit Opteron &gt; towers , and I am about to purchase a machine for the head node . I was &gt; looking at purchasing a 64-bit Xeon machine , but I am curious if anyone &gt; knows @ @ @ @ @ @ @ @ @ @ mixed-architecture ROCKS install . If anyone has any advice about &gt; purchasing a head node , or any general advice about installing Matlab on &gt; ROCKS , I would greatly appreciate it . I was planning to build a Matlab &gt; RPM via http : **36;5552;TOOLONG , but if anyone has any &gt; other suggestions , I 'd be glad to hear them . &gt; Matlab does not differentiate between architectures in this case . If you had mixed 32-bit and 64-bit you might have a problem . Are you running parallel matlab ? &gt; Thanks ! &gt; &gt; Erika From greg.bruno at gmail.com Fri Sep 1 08:01:12 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : Fri , 1 Sep 2006 08:01:12 -0700 Subject : Rocks-Discuss bug with substituting OS rolls with RHEL or RHEL rebuild CDs In-Reply-To : **40;5590;TOOLONG References : **40;5632;TOOLONG Message-ID : On 9/1/06 , Lew Lefton wrote : &gt; Greg Bruno wrote : &gt; &gt; we have confirmed that in rocks 4.2 , you ca n't substitute the OS rolls &gt; &gt; with RHEL or RHEL rebuild CDs ( e.g. , CentOS @ @ @ @ @ @ @ @ @ @ isolated the bug and we are working on a maintenance release &gt; &gt; of 4.2 to address it . &gt; &gt; &gt; &gt; i apologise for this bug as this important test case was not executed &gt; &gt; during the final testing of rocks 4.2. &gt; &gt; &gt; &gt; we plan to get the maintenance release out as quickly as possible , but &gt; &gt; we still need to run through a couple more test cases before we &gt; &gt; release it . &gt; &gt; Can you provide an unofficial , but hopefully realistic estimate of when &gt; the 4.2.1 release will be available ? I 'm currently trying to decide &gt; whether to rebuild a cluster with 4.1 or wait for the 4.2 maintenance &gt; release . we are targeting 3-4 weeks from today . - gb From philip.papadopoulos at gmail.com Fri Sep 1 08:02:03 2006 From : philip.papadopoulos at gmail.com ( Philip Papadopoulos ) Date : Fri , 1 Sep 2006 08:02:03 -0700 Subject : Rocks-Discuss bug with substituting OS rolls with RHEL or RHEL rebuild CDs In-Reply-To : **40;5674;TOOLONG References : **40;5716;TOOLONG Message-ID : LONG ... Approximately @ @ @ @ @ @ @ @ @ @ Lew Lefton wrote : &gt; &gt; Greg Bruno wrote : &gt; &gt; we have confirmed that in rocks 4.2 , you ca n't substitute the OS rolls &gt; &gt; with RHEL or RHEL rebuild CDs ( e.g. , CentOS or ScientificLinux ) . &gt; &gt; &gt; &gt; we have isolated the bug and we are working on a maintenance release &gt; &gt; of 4.2 to address it . &gt; &gt; &gt; &gt; i apologise for this bug as this important test case was not executed &gt; &gt; during the final testing of rocks 4.2. &gt; &gt; &gt; &gt; we plan to get the maintenance release out as quickly as possible , but &gt; &gt; we still need to run through a couple more test cases before we &gt; &gt; release it . &gt; &gt; Can you provide an unofficial , but hopefully realistic estimate of when &gt; the 4.2.1 release will be available ? I 'm currently trying to decide &gt; whether to rebuild a cluster with 4.1 or wait for the 4.2 maintenance &gt; release . &gt; &gt; Thanks , &gt; Lew &gt; &gt; -- Philip Papadopoulos , PhD @ @ @ @ @ @ @ @ @ @ -------------- An HTML attachment was scrubbed ... URL : LONG ... From pmitchel at email.unc.edu Fri Sep 1 08:50:44 2006 From : pmitchel at email.unc.edu ( Paul Mitchell ) Date : Fri , 1 Sep 2006 11:50:44 -0400 ( EDT ) Subject : Rocks-Discuss cross kickstart problems In-Reply-To : References : LONG ... Message-ID : Hello , I 'm trying to boot a i386 client form my x8664 , 4.2 , frontend . I popped the kernel disk into the client , and let it boot up . Insert-ethers on the frontend caught the node . On my frontend , I had performed the following steps : Downloaded the full disk image for i386 , LONG ... mounted it at /mnt/cdrom and ran : rocks-dist copyroll cd /home/install/ rocks-dist dist But the client never actually kickstarts. here 's what happens : 1 ) We go through the Hallasan menu , whereupon I select client 2 ) I get a sending request frame then an Authenticate Installation Server , with the corect parameters , so I know it 's talking to the frontend . Here I would assume that it should @ @ @ @ @ @ @ @ @ @ a set of frames which ask me to Choose a Language , WHat typoe of keyboard , and an Installation Method ( what type of media ? ) . Here I can choose from Local cdrom , hard disk , NFS image , FTP or HTTP . Why is n't it finding the rolls and xml files on the frontend ? FTR , I re-ran the rocks-dist as follows : rocks-dist --arch=i386 dist whereupon it complained that I needed to run the frontends architecture first , so I ran : rocks-dist --arch=x8664 dist followed by : root at rocks # rocks-dist --arch=i386 dist Cleaning distribution Resolving versions ( base files ) including " kernel " ( 4.2 , i386 ) roll ... including " area51 " ( 4.2 , i386 ) roll ... including " java " ( 4.2 , i386 ) roll ... including " bio " ( 4.2 , i386 ) roll ... including " os " ( 4.2 , i386 ) roll ... including " hpc " ( 4.2 , i386 ) roll ... including " base " ( 4.2 , i386 ) roll ... including " @ @ @ @ @ @ @ @ @ @ " condor " ( 4.2 , i386 ) roll ... including " web-server " ( 4.2 , i386 ) roll ... including " ganglia " ( 4.2 , i386 ) roll ... including " sge " ( 4.2 , i386 ) roll ... including " viz " ( 4.2 , i386 ) roll ... Including critical RPMS Resolving versions ( RPMs ) including " kernel " ( 4.2 , i386 ) roll ... including " area51 " ( 4.2 , i386 ) roll ... including " java " ( 4.2 , i386 ) roll ... including " bio " ( 4.2 , i386 ) roll ... including " os " ( 4.2 , i386 ) roll ... including " hpc " ( 4.2 , i386 ) roll ... including " base " ( 4.2 , i386 ) roll ... including " grid " ( 4.2 , i386 ) roll ... including " condor " ( 4.2 , i386 ) roll ... including " web-server " ( 4.2 , i386 ) roll ... including " ganglia " ( 4.2 , i386 ) roll ... including " sge " ( 4.2 , @ @ @ @ @ @ @ @ @ @ , i386 ) roll ... Resolving versions ( SRPMs ) including " kernel " ( 4.2 , i386 ) roll ... including " area51 " ( 4.2 , i386 ) roll ... including " java " ( 4.2 , i386 ) roll ... including " bio " ( 4.2 , i386 ) roll ... including " os " ( 4.2 , i386 ) roll ... including " hpc " ( 4.2 , i386 ) roll ... including " base " ( 4.2 , i386 ) roll ... including " grid " ( 4.2 , i386 ) roll ... including " condor " ( 4.2 , i386 ) roll ... including " web-server " ( 4.2 , i386 ) roll ... including " ganglia " ( 4.2 , i386 ) roll ... including " sge " ( 4.2 , i386 ) roll ... including " viz " ( 4.2 , i386 ) roll ... Creating files ( symbolic links - fast ) Applying netstg2.img Applying updates.img Applying comps.xml Installing XML Kickstart profiles installing " condor " profiles ... installing " hpc " profiles ... installing " ganglia " profiles ... installing @ @ @ @ @ @ @ @ @ @ ... installing " viz " profiles ... installing " bio " profiles ... installing " java " profiles ... installing " sge " profiles ... installing " area51 " profiles ... installing " kernel " profiles ... installing " grid " profiles ... installing " os " profiles ... installing " site " profiles ... Generating hdlist ( rpm database ) LONG ... error while loading shared libraries : librpm-4.3.so : can not open shared object file : No such file or directory making " torrent " files for RPMS Cleaning distribution Resolving versions ( base files ) including " kernel " ( 4.2 , i386 ) roll ... including " base " ( 4.2 , i386 ) roll ... Including critical RPMS Resolving versions ( RPMs ) including " kernel " ( 4.2 , i386 ) roll ... including " base " ( 4.2 , i386 ) roll ... Resolving versions ( SRPMs ) including " kernel " ( 4.2 , i386 ) roll ... including " base " ( 4.2 , i386 ) roll ... Creating files ( symbolic links - fast ) Applying netstg2.img Applying updates.img Applying @ @ @ @ @ @ @ @ @ @ you are building the OS roll , this is not a problem Installing XML Kickstart profiles installing " kernel " profiles ... installing " base " profiles ... Generating hdlist ( rpm database ) Linking boot stages from lan Building Roll Links So what step am I missing , or alternately , what should I select as my Installation media ? Thanks , as usual , for any help , Paul Mitchell LONG ... Paul Mitchell email : pmitchel at email.unc.edu phone : ( 919 ) 962-9778 office : I have an office , room 14 , Phillips Hall LONG ... From debbiet at arlut.utexas.edu Fri Sep 1 09:25:49 2006 From : debbiet at arlut.utexas.edu ( Debbie Tropiano ) Date : Fri , 1 Sep 2006 11:25:49 -0500 Subject : Rocks-Discuss Adding IB host info to cluster Message-ID : LONG ... Hello - I need to add the host info for Infiniband to my Rocks cluster and am trying to determine the best method . It seems that the compute nodes do n't honor the /etc/hosts.local file like the head node does ( tried this and it broke my webserver @ @ @ @ @ @ @ @ @ @ Is adding them to DNS on the head node the best way ? If so , is there a way to add them to the database or can I only add them via the flat files ? Are those flat files repopulated via the database automatically ? Alternately , they could be appended directly to the /etc/hosts file via the extend-compute.xml file . Any thoughts on this ? What have others done ? FWIW We 're using the IBGold drivers . Thanks , Debbie -- Debbie Tropiano debbiet at arlut.utexas.edu Environmental Sciences Laboratory +1 512 835 3367 w Applied Research Laboratories of UT Austin +1 512 835 3544 fax P.O. Box 8029 , Austin , TX 78713-8029 home email : debbie at icus.com From greg.bruno at gmail.com Fri Sep 1 10:00:14 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : Fri , 1 Sep 2006 10:00:14 -0700 Subject : Rocks-Discuss cross kickstart problems In-Reply-To : References : LONG ... Message-ID : On 9/1/06 , Paul Mitchell wrote : &gt; Hello , &gt; I 'm trying to boot a i386 client form my x8664 , 4.2 , frontend. @ @ @ @ @ @ @ @ @ @ and let it boot up . &gt; Insert-ethers on the frontend caught the node . &gt; &gt; On my frontend , I had performed the following steps : &gt; &gt; Downloaded the full disk image for i386 , &gt; LONG ... &gt; &gt; mounted it at /mnt/cdrom and ran : rocks-dist copyroll &gt; &gt; cd /home/install/ &gt; rocks-dist dist &gt; &gt; But the client never actually kickstarts. here 's what happens : &gt; &gt; 1 ) We go through the Hallasan menu , whereupon I select client &gt; 2 ) I get a sending request frame then an Authenticate Installation Server , &gt; with the corect parameters , so I know it 's talking to the frontend. &gt; &gt; Here I would assume that it should continue kickstarting , but instead , I &gt; get thrown into a set of frames which ask me to Choose a Language , WHat &gt; typoe of keyboard , and an Installation Method ( what type of media ? ) . &gt; &gt; Here I can choose from Local cdrom , hard disk , NFS image , FTP or HTTP. &gt; Why is n't @ @ @ @ @ @ @ @ @ @ ? &gt; &gt; FTR , I re-ran the rocks-dist as follows : &gt; &gt; rocks-dist --arch=i386 dist &gt; &gt; whereupon it complained that I needed to run the frontends architecture &gt; first , so I ran : &gt; &gt; rocks-dist --arch=x8664 dist &gt; &gt; followed by : &gt; &gt; root at rocks # rocks-dist --arch=i386 dist &gt; Cleaning distribution &gt; Resolving versions ( base files ) &gt; including " kernel " ( 4.2 , i386 ) roll ... &gt; including " area51 " ( 4.2 , i386 ) roll ... &gt; including " java " ( 4.2 , i386 ) roll ... &gt; including " bio " ( 4.2 , i386 ) roll ... &gt; including " os " ( 4.2 , i386 ) roll ... &gt; including " hpc " ( 4.2 , i386 ) roll ... &gt; including " base " ( 4.2 , i386 ) roll ... &gt; including " grid " ( 4.2 , i386 ) roll ... &gt; including " condor " ( 4.2 , i386 ) roll ... &gt; including " web-server " ( 4.2 , i386 ) roll ... &gt; including " ganglia @ @ @ @ @ @ @ @ @ @ " sge " ( 4.2 , i386 ) roll ... &gt; including " viz " ( 4.2 , i386 ) roll ... &gt; Including critical RPMS &gt; Resolving versions ( RPMs ) &gt; including " kernel " ( 4.2 , i386 ) roll ... &gt; including " area51 " ( 4.2 , i386 ) roll ... &gt; including " java " ( 4.2 , i386 ) roll ... &gt; including " bio " ( 4.2 , i386 ) roll ... &gt; including " os " ( 4.2 , i386 ) roll ... &gt; including " hpc " ( 4.2 , i386 ) roll ... &gt; including " base " ( 4.2 , i386 ) roll ... &gt; including " grid " ( 4.2 , i386 ) roll ... &gt; including " condor " ( 4.2 , i386 ) roll ... &gt; including " web-server " ( 4.2 , i386 ) roll ... &gt; including " ganglia " ( 4.2 , i386 ) roll ... &gt; including " sge " ( 4.2 , i386 ) roll ... &gt; including " viz " ( 4.2 , i386 ) roll ... &gt; Resolving @ @ @ @ @ @ @ @ @ @ 4.2 , i386 ) roll ... &gt; including " area51 " ( 4.2 , i386 ) roll ... &gt; including " java " ( 4.2 , i386 ) roll ... &gt; including " bio " ( 4.2 , i386 ) roll ... &gt; including " os " ( 4.2 , i386 ) roll ... &gt; including " hpc " ( 4.2 , i386 ) roll ... &gt; including " base " ( 4.2 , i386 ) roll ... &gt; including " grid " ( 4.2 , i386 ) roll ... &gt; including " condor " ( 4.2 , i386 ) roll ... &gt; including " web-server " ( 4.2 , i386 ) roll ... &gt; including " ganglia " ( 4.2 , i386 ) roll ... &gt; including " sge " ( 4.2 , i386 ) roll ... &gt; including " viz " ( 4.2 , i386 ) roll ... &gt; Creating files ( symbolic links - fast ) &gt; Applying netstg2.img &gt; Applying updates.img &gt; Applying comps.xml &gt; Installing XML Kickstart profiles &gt; installing " condor " profiles ... &gt; installing " hpc " profiles ... &gt; installing " @ @ @ @ @ @ @ @ @ @ ... &gt; installing " base " profiles ... &gt; installing " viz " profiles ... &gt; installing " bio " profiles ... &gt; installing " java " profiles ... &gt; installing " sge " profiles ... &gt; installing " area51 " profiles ... &gt; installing " kernel " profiles ... &gt; installing " grid " profiles ... &gt; installing " os " profiles ... &gt; installing " site " profiles ... &gt; Generating hdlist ( rpm database ) &gt; LONG ... error the line above tells me you ran rocks-dist in the directory ' /root ' and not ' /home/install ' . try : # cd /home/install # rm -rf rocks-dist # rocks-dist dist # rocks-dist --arch=i386 dist - gb From scott at cse.ucdavis.edu Fri Sep 1 11:30:29 2006 From : scott at cse.ucdavis.edu ( Scott Beardsley ) Date : Fri , 01 Sep 2006 11:30:29 -0700 Subject : Rocks-Discuss httpd on compute nodes In-Reply-To : References : LONG ... Message-ID : **40;5758;TOOLONG Is there a reason why httpd is running on compute nodes in both 4.1 and 4.2 ? It appears to be the same config that 's installed @ @ @ @ @ @ @ @ @ @ compute nodes to free up resources . -- Scott From pmitchel at email.unc.edu Fri Sep 1 11:46:00 2006 From : pmitchel at email.unc.edu ( Paul Mitchell ) Date : Fri , 1 Sep 2006 14:46:00 -0400 ( EDT ) Subject : Rocks-Discuss cross kickstart problems In-Reply-To : References : LONG ... Message-ID : On Fri , 1 Sep 2006 , Greg Bruno wrote : &gt; # cd /home/install &gt; # rm -rf rocks-dist &gt; # rocks-dist dist &gt; # rocks-dist --arch=i386 dist Thanks , Greg . That got me a little further down the road ( though I had to first remove the /root/rocks-dist and the /home/install/rocks-dist before it worked ) . I 'm not all the way home though . Unfortunately , I 'm back in the same place I was last week : As it starts to read in the rolls , I get the classic : Unable to read header list . This may be due to a missing file or bad media . Press to try again . Of course , this never resolves . The confusion I 'm having here is that the same message @ @ @ @ @ @ @ @ @ @ kickstarted successfully last week ) as well . Furthermore , both sets of rolls were downloaded directly as iso images , so I know the media is not the problem . What 's changed on the frontend machine ? I think I may have already asked this before , but I 'm still somewhat unclear - is there a method for determining exactly which header it 's complaining about ? IN other words , where is the log file for the kickstart ? - I ca n't seem to find it . Paul Mitchell LONG ... Paul Mitchell email : pmitchel at email.unc.edu phone : ( 919 ) 962-9778 office : I have an office , room 14 , Phillips Hall LONG ... From debbiet at arlut.utexas.edu Fri Sep 1 12:04:04 2006 From : debbiet at arlut.utexas.edu ( Debbie Tropiano ) Date : Fri , 1 Sep 2006 14:04:04 -0500 Subject : Rocks-Discuss Adding IB host info to cluster In-Reply-To : LONG ... References : LONG ... Message-ID : LONG ... Nevermind . I thought that I 'd looked everywhere but I now see my answser in the Users Guide @ @ @ @ @ @ @ @ @ @ at 11:25:49AM -0500 , Debbie Tropiano wrote : &gt; Hello - &gt; &gt; I need to add the host info for Infiniband to my Rocks cluster &gt; and am trying to determine the best method . It seems that the &gt; compute nodes do n't honor the /etc/hosts.local file like the &gt; head node does ( tried this and it broke my webserver ... which &gt; is yet another question : - ) . &gt; &gt; Is adding them to DNS on the head node the best way ? If so , is &gt; there a way to add them to the database or can I only add them &gt; via the flat files ? Are those flat files repopulated via the &gt; database automatically ? Alternately , they could be appended &gt; directly to the /etc/hosts file via the extend-compute.xml file . &gt; Any thoughts on this ? What have others done ? &gt; &gt; FWIW We 're using the IBGold drivers . &gt; &gt; Thanks , &gt; Debbie &gt; -- &gt; Debbie Tropiano debbiet at arlut.utexas.edu &gt; Environmental Sciences Laboratory +1 512 835 3367 w &gt; Applied Research @ @ @ @ @ @ @ @ @ @ P.O. Box 8029 , Austin , TX 78713-8029 home email : debbie at icus.com -- Debbie Tropiano debbiet at arlut.utexas.edu Environmental Sciences Laboratory +1 512 835 3367 w Applied Research Laboratories of UT Austin +1 512 835 3544 fax P.O. Box 8029 , Austin , TX 78713-8029 home email : debbie at icus.com From wangd at uci.edu Fri Sep 1 12:21:10 2006 From : wangd at uci.edu ( wangd at uci.edu ) Date : Fri , 1 Sep 2006 12:21:10 -0700 ( PDT ) Subject : Rocks-Discuss Bad ARP tables on compute nodes In-Reply-To : References : LONG ... Message-ID : LONG ... &gt; On 8/31/06 , wangd at uci.edu wrote : &gt;&gt; Hi , &gt;&gt; &gt;&gt; We 've got a 27 node Opteron cluster here ( 1 frontend , 26 compute ) that &gt;&gt; seems to be having problems with ARP resolution . We 're running Rocks &gt;&gt; 4.1 &gt;&gt; with a CentOS kernel ( 2.6.9-22ELsmp ) . &gt;&gt; &gt;&gt; The first symptom was that ssh from one compute node to another did not &gt;&gt; always work . For instance , while going from compute-0-0 to &gt;&gt; compute-0-1,2 is @ @ @ @ @ @ @ @ @ @ &gt;&gt; &gt;&gt; We tried pinging compute-0-3 from both compute-0-0 and compute-0-2. &gt;&gt; &gt;&gt; wangd at compute-0-0 $ ping compute-0-3 &gt;&gt; PING compute-0-3.local ( 192.168.0.251 ) 56(84) bytes of data . &gt;&gt; &gt;From 192.168.0.252 : icmpseq=0 Redirect Host ( New nexthop : &gt;&gt; 192.168.0.251 ) &gt;&gt; &gt;&gt; wangd at compute-0-2 $ ping compute-0-3 &gt;&gt; PING compute-0-3.local ( 192.168.0.251 ) 56(84) bytes of data . &gt;&gt; 64 bytes from 192.168.0.251 : icmpseq=0 ttl=64 time=0.101 ms &gt;&gt; &gt;&gt; It seems that compute-0-0 is mistakenly resolving compute-0-3 to &gt;&gt; compute-0-2 's MAC . &gt;&gt; &gt;&gt; To confirm this , we check their ARP tables ( note compute-0-0 's entries &gt;&gt; have the same MAC address both compute-0-2 and compute-0-3 ) : &gt;&gt; &gt;&gt; wangd at compute-0-0 $ /sbin/arp -a &gt;&gt; ? ( 192.168.0.10 ) at 00:D0:68:0E:AA:66 ether on eth0 &gt;&gt; ? ( 192.168.0.251 ) at 00:D0:68:0D:F5:7D ether on eth0 &gt;&gt; ? ( 192.168.0.252 ) at 00:D0:68:0D:F5:7D ether on eth0 &gt;&gt; &gt;&gt; wangd at compute-0-2 $ /sbin/arp -a &gt;&gt; ? ( 192.168.0.10 ) at 00:D0:68:0E:AA:66 ether on eth0 &gt;&gt; ? ( 192.168.0.254 ) at 00:D0:68:0D:F5:C1 ether on eth0 &gt;&gt; &gt;&gt; For reference the relevant arp @ @ @ @ @ @ @ @ @ @ at 00:D0:68:0D:F5:C1 ether on eth0 &gt;&gt; compute-0-2.local ( 192.168.0.252 ) at 00:D0:68:0D:F5:7D ether on eth0 &gt;&gt; compute-0-3.local ( 192.168.0.251 ) at 00:D0:68:0D:F5:E3 ether on eth0 &gt;&gt; &gt;&gt; The frontend never has a problem making connections to nodes ( as long as &gt;&gt; they are up and listening ; ) . This is puzzling . The frontend stores &gt;&gt; the &gt;&gt; IP-MAC mapping only for dhcpd purposes , and uses normal ARP exchanges &gt;&gt; when &gt;&gt; making connections , so why does it never get bogus responses , while the &gt;&gt; compute nodes sometimes do . When there is a bogus response , the bogus &gt;&gt; MAC &gt;&gt; is always ( AFAIK ) the MAC of the numerically preceding node . &gt;&gt; &gt;&gt; Now , I understand that this could be a switch problem or a kernel &gt;&gt; problem , &gt;&gt; but I wanted to confirm that Rocks does n't have the frontend do anything &gt;&gt; special to have its ARP tables more accurate , or that compute nodes have &gt;&gt; somewhat ' less accurate ' ARP methods . Has anyone else seen this &gt;&gt; problem ? &gt; &gt; on the @ @ @ @ @ @ @ @ @ @ # dbreport ethers &gt; &gt; on compute-0-2 , what is the output of : &gt; &gt; # ifconfig -a &gt; &gt; on compute-0-3 , what is the output of : &gt; &gt; # ifconfig -a &gt; &gt; - gb Here you go . ---------------- wangd at ipcc $ dbreport ethers # # Do NOT Edit ( generated by dbreport ) # 00:D0:68:0E:AA:67 ipcc.local 00:d0:68:0d:f5:c1 compute-0-0.local 00:d0:68:0d:f7:27 compute-0-1.local 00:d0:68:0d:f5:7d compute-0-2.local 00:d0:68:0d:f5:e3 compute-0-3.local 00:d0:68:0d:f7:4b compute-0-4.local 00:d0:68:0d:f5:e1 compute-0-5.local 00:d0:68:09:9b:a0 compute-0-6.local 00:d0:68:0d:f7:25 compute-0-7.local 00:d0:68:0d:f7:1f compute-0-8.local 00:d0:68:0d:f7:21 compute-0-9.local 00:d0:68:0d:f5:b9 compute-0-10.local 00:d0:68:0d:f5:bd compute-0-11.local 00:d0:68:0d:f5:bb compute-0-12.local 00:d0:68:0d:f7:4d compute-0-13.local 00:d0:68:0d:f5:d9 compute-0-14.local 00:d0:68:0d:f6:ed compute-0-15.local 00:d0:68:0d:f5:bf compute-0-16.local 00:d0:68:0d:f7:23 compute-0-17.local 00:d0:68:0d:f5:83 compute-0-18.local 00:d0:68:0d:f5:85 compute-0-19.local 00:d0:68:0d:f6:e9 compute-0-20.local 00:d0:68:0d:f6:eb compute-0-21.local 00:d0:68:0d:f6:e7 compute-0-22.local 00:d0:68:0d:f7:4f compute-0-23.local 00:d0:68:0d:f5:7f compute-0-24.local 00:d0:68:0d:f6:e5 compute-0-25.local **45;5800;TOOLONG wangd at compute-0-2 $ ifconfig -a -bash : ifconfig : command not found wangd at compute-0-2 $ /sbin/ifconfig -a eth0 Link encap:Ethernet HWaddr 00:D0:68:0D:F5:7D inet addr:192.168.0.252 Bcast:192.168.0.255 Mask:255.255.255.0 inet6 addr : fe80 : : 2d0:68ff:fe0d:f57d/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:590954 errors:0 dropped:0 overruns:0 frame:0 TX packets:41999 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:54982050 ( 52.4 MiB ) TX bytes:4539341 ( 4.3 MiB ) Interrupt:10 eth1 Link encap:Ethernet HWaddr 00:D0:68:0D:F5:7E @ @ @ @ @ @ @ @ @ @ TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 ( 0.0 b ) TX bytes:0 ( 0.0 b ) Interrupt:9 lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr : : : 1/128 Scope:Host UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:606 errors:0 dropped:0 overruns:0 frame:0 TX packets:606 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:42476 ( 41.4 KiB ) TX bytes:42476 ( 41.4 KiB ) sit0 Link encap:IPv6-in-IPv4 NOARP MTU:1480 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 ( 0.0 b ) TX bytes:0 ( 0.0 b ) wangd at compute-0-2 $ **35;5847;TOOLONG wangd at compute-0-3 $ /sbin/ifconfig -a eth0 Link encap:Ethernet HWaddr 00:D0:68:0D:F5:E3 inet addr:192.168.0.251 Bcast:192.168.0.255 Mask:255.255.255.0 inet6 addr : fe80 : : 2d0:68ff:fe0d:f5e3/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:588446 errors:0 dropped:0 overruns:0 frame:0 TX packets:38676 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:54362402 ( 51.8 MiB ) TX bytes:4024931 ( 3.8 MiB ) Interrupt:10 eth1 Link encap:Ethernet HWaddr 00:D0:68:0D:F5:E4 BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 ( 0.0 b ) TX bytes:0 @ @ @ @ @ @ @ @ @ @ addr:127.0.0.1 Mask:255.0.0.0 inet6 addr : : : 1/128 Scope:Host UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:2258 errors:0 dropped:0 overruns:0 frame:0 TX packets:2258 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:158060 ( 154.3 KiB ) TX bytes:158060 ( 154.3 KiB ) sit0 Link encap:IPv6-in-IPv4 NOARP MTU:1480 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 ( 0.0 b ) TX bytes:0 ( 0.0 b ) wangd at compute-0-3 $ --------------- -Daniel From wangd at uci.edu Fri Sep 1 12:37:30 2006 From : wangd at uci.edu ( wangd at uci.edu ) Date : Fri , 1 Sep 2006 12:37:30 -0700 ( PDT ) Subject : Rocks-Discuss Bad ARP tables on compute nodes In-Reply-To : LONG ... References : LONG ... LONG ... Message-ID : LONG ... Okay , I just wanted to confirm that there was n't any special behavior . I 'm still new to Rocks , and even though I 've messed with a lot of it already , it still feels like a black box that sits on top of an otherwise normal system . I had n't thought about checking @ @ @ @ @ @ @ @ @ @ IPMI was trying to allocate itself an adjacent IP then that could be problematic . Is there a way to have insert-ethers/dhcpd assign IP addresses with a stride of 2 instead of 1 ? Of course , there 's probably a way to make IPMI or the switch behave better . Thanks for your suggestions ! -Daniel &gt; What is your connection topology -- the symptoms are often characteristic &gt; of a loop somewhere in your network and/or multiple network elements with &gt; the &gt; same IP address ( check the IP address of your switches ) . &gt; &gt; There is n't anything " special " that rocks does with arp entries . I would &gt; highly doubt that &gt; this is a endpoint problem , but rather a network problem ( loops and/or &gt; multiple nodes with &gt; the same IP address ) . If your nodes have IPMI or service processors , you &gt; need to check &gt; that these do n't duplicate addresses . &gt; &gt; -P &gt; &gt; &gt; &gt; On 8/31/06 , Greg Bruno wrote : &gt;&gt; &gt;&gt; On 8/31/06 , wangd at uci.edu @ @ @ @ @ @ @ @ @ @ We 've got a 27 node Opteron cluster here ( 1 frontend , 26 compute ) &gt;&gt; that &gt;&gt; &gt; seems to be having problems with ARP resolution . We 're running Rocks &gt;&gt; 4.1 &gt;&gt; &gt; with a CentOS kernel ( 2.6.9-22ELsmp ) . &gt;&gt; &gt; &gt;&gt; &gt; The first symptom was that ssh from one compute node to another did &gt;&gt; not &gt;&gt; &gt; always work . For instance , while going from compute-0-0 to &gt;&gt; &gt; compute-0-1,2 is fine , going from compute-0-0 to compute-0-3 is not . &gt;&gt; &gt; &gt;&gt; &gt; We tried pinging compute-0-3 from both compute-0-0 and compute-0-2. &gt;&gt; &gt; &gt;&gt; &gt; wangd at compute-0-0 $ ping compute-0-3 &gt;&gt; &gt; PING compute-0-3.local ( 192.168.0.251 ) 56(84) bytes of data . &gt;&gt; &gt; &gt;From 192.168.0.252 : icmpseq=0 Redirect Host ( New nexthop : &gt;&gt; 192.168.0.251 &gt;&gt; ) &gt;&gt; &gt; &gt;&gt; &gt; wangd at compute-0-2 $ ping compute-0-3 &gt;&gt; &gt; PING compute-0-3.local ( 192.168.0.251 ) 56(84) bytes of data . &gt;&gt; &gt; 64 bytes from 192.168.0.251 : icmpseq=0 ttl=64 time=0.101 ms &gt;&gt; &gt; &gt;&gt; &gt; It seems that compute-0-0 is mistakenly resolving compute-0-3 to &gt;&gt; &gt; @ @ @ @ @ @ @ @ @ @ this , we check their ARP tables ( note compute-0-0 's entries &gt;&gt; &gt; have the same MAC address both compute-0-2 and compute-0-3 ) : &gt;&gt; &gt; &gt;&gt; &gt; wangd at compute-0-0 $ /sbin/arp -a &gt;&gt; &gt; ? ( 192.168.0.10 ) at 00:D0:68:0E:AA:66 ether on eth0 &gt;&gt; &gt; ? ( 192.168.0.251 ) at 00:D0:68:0D:F5:7D ether on eth0 &gt;&gt; &gt; ? ( 192.168.0.252 ) at 00:D0:68:0D:F5:7D ether on eth0 &gt;&gt; &gt; &gt;&gt; &gt; wangd at compute-0-2 $ /sbin/arp -a &gt;&gt; &gt; ? ( 192.168.0.10 ) at 00:D0:68:0E:AA:66 ether on eth0 &gt;&gt; &gt; ? ( 192.168.0.254 ) at 00:D0:68:0D:F5:C1 ether on eth0 &gt;&gt; &gt; &gt;&gt; &gt; For reference the relevant arp entries from the frontend : &gt;&gt; &gt; compute-0-0.local ( 192.168.0.254 ) at 00:D0:68:0D:F5:C1 ether on eth0 &gt;&gt; &gt; compute-0-2.local ( 192.168.0.252 ) at 00:D0:68:0D:F5:7D ether on eth0 &gt;&gt; &gt; compute-0-3.local ( 192.168.0.251 ) at 00:D0:68:0D:F5:E3 ether on eth0 &gt;&gt; &gt; &gt;&gt; &gt; The frontend never has a problem making connections to nodes ( as long &gt;&gt; as &gt;&gt; &gt; they are up and listening ; ) . This is puzzling . The frontend stores &gt;&gt; the &gt;&gt; &gt; IP-MAC mapping only @ @ @ @ @ @ @ @ @ @ when &gt;&gt; &gt; making connections , so why does it never get bogus responses , while &gt;&gt; the &gt;&gt; &gt; compute nodes sometimes do . When there is a bogus response , the bogus &gt;&gt; MAC &gt;&gt; &gt; is always ( AFAIK ) the MAC of the numerically preceding node . &gt;&gt; &gt; &gt;&gt; &gt; Now , I understand that this could be a switch problem or a kernel &gt;&gt; problem , &gt;&gt; &gt; but I wanted to confirm that Rocks does n't have the frontend do &gt;&gt; anything &gt;&gt; &gt; special to have its ARP tables more accurate , or that compute nodes &gt;&gt; have &gt;&gt; &gt; somewhat ' less accurate ' ARP methods . Has anyone else seen this &gt;&gt; problem ? &gt;&gt; &gt;&gt; on the frontend , what is the output of : &gt;&gt; &gt;&gt; # dbreport ethers &gt;&gt; &gt;&gt; on compute-0-2 , what is the output of : &gt;&gt; &gt;&gt; # ifconfig -a &gt;&gt; &gt;&gt; on compute-0-3 , what is the output of : &gt;&gt; &gt;&gt; # ifconfig -a &gt;&gt; &gt;&gt; - gb &gt;&gt; &gt;&gt; &gt; &gt; &gt; -- &gt; Philip Papadopoulos , PhD &gt; University @ @ @ @ @ @ @ @ @ @ part -------------- &gt; An HTML attachment was scrubbed ... &gt; URL : &gt; LONG ... &gt; From philip.papadopoulos at gmail.com Fri Sep 1 13:01:19 2006 From : philip.papadopoulos at gmail.com ( Philip Papadopoulos ) Date : Fri , 1 Sep 2006 13:01:19 -0700 Subject : Rocks-Discuss Bad ARP tables on compute nodes In-Reply-To : LONG ... References : LONG ... LONG ... LONG ... Message-ID : LONG ... insert-ethers --inc=2 will do this . But , You need to find the source of the arp/ip address conflict issue . Can you examine the arp tables in your switch ? Rocks automates alot of things , but in the end one of the things it does is write configuration files in the formats that existing tools understand . ( We do n't have a special version of named , for example , merely a programatically generated configuration file for named -- it 's regular named with a " proper " configuration file ) . The " Do not Edit " warnings at the beginning of many of these files should be heeded because , many files are regenerated after every node @ @ @ @ @ @ @ @ @ @ you can do a #dbreport and see the reports from the rocks database that are available . Some of the sub-reports can generate multiple files , so from the database there are upwards of 30 config files that can be programmatically generated . Just trying to make it a little less " black box " ' y . -P On 9/1/06 , wangd at uci.edu wrote : &gt; &gt; Okay , I just wanted to confirm that there was n't any special behavior. &gt; I 'm still new to Rocks , and even though I 've messed with a lot of it &gt; already , it still feels like a black box that sits on top of an otherwise &gt; normal system . &gt; &gt; I had n't thought about checking IPMI , but that 's a great idea . If IPMI &gt; was trying to allocate itself an adjacent IP then that could be &gt; problematic . Is there a way to have insert-ethers/dhcpd assign IP &gt; addresses with a stride of 2 instead of 1 ? &gt; &gt; Of course , there 's probably a way to make @ @ @ @ @ @ @ @ @ @ Thanks for your suggestions ! &gt; -Daniel &gt; &gt; &gt; What is your connection topology -- the symptoms are often &gt; characteristic &gt; &gt; of a loop somewhere in your network and/or multiple network elements &gt; with &gt; &gt; the &gt; &gt; same IP address ( check the IP address of your switches ) . &gt; &gt; &gt; &gt; There is n't anything " special " that rocks does with arp entries . I would &gt; &gt; highly doubt that &gt; &gt; this is a endpoint problem , but rather a network problem ( loops and/or &gt; &gt; multiple nodes with &gt; &gt; the same IP address ) . If your nodes have IPMI or service processors , you &gt; &gt; need to check &gt; &gt; that these do n't duplicate addresses . &gt; &gt; &gt; &gt; -P &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; On 8/31/06 , Greg Bruno wrote : &gt; &gt;&gt; &gt; &gt;&gt; On 8/31/06 , wangd at uci.edu wrote : &gt; &gt;&gt; &gt; Hi , &gt; &gt;&gt; &gt; &gt; &gt;&gt; &gt; We 've got a 27 node Opteron cluster here ( 1 frontend , 26 @ @ @ @ @ @ @ @ @ @ be having problems with ARP resolution . We 're running Rocks &gt; &gt;&gt; 4.1 &gt; &gt;&gt; &gt; with a CentOS kernel ( 2.6.9-22ELsmp ) . &gt; &gt;&gt; &gt; &gt; &gt;&gt; &gt; The first symptom was that ssh from one compute node to another did &gt; &gt;&gt; not &gt; &gt;&gt; &gt; always work . For instance , while going from compute-0-0 to &gt; &gt;&gt; &gt; compute-0-1,2 is fine , going from compute-0-0 to compute-0-3 is &gt; not . &gt; &gt;&gt; &gt; &gt; &gt;&gt; &gt; We tried pinging compute-0-3 from both compute-0-0 and compute-0-2. &gt; &gt;&gt; &gt; &gt; &gt;&gt; &gt; wangd at compute-0-0 $ ping compute-0-3 &gt; &gt;&gt; &gt; PING compute-0-3.local ( 192.168.0.251 ) 56(84) bytes of data . &gt; &gt;&gt; &gt; &gt;From 192.168.0.252 : icmpseq=0 Redirect Host ( New nexthop : &gt; &gt;&gt; 192.168.0.251 &gt; &gt;&gt; ) &gt; &gt;&gt; &gt; &gt; &gt;&gt; &gt; wangd at compute-0-2 $ ping compute-0-3 &gt; &gt;&gt; &gt; PING compute-0-3.local ( 192.168.0.251 ) 56(84) bytes of data . &gt; &gt;&gt; &gt; 64 bytes from 192.168.0.251 : icmpseq=0 ttl=64 time=0.101 ms &gt; &gt;&gt; &gt; &gt; &gt;&gt; &gt; It seems that compute-0-0 is mistakenly resolving compute-0-3 to &gt; &gt;&gt; @ @ @ @ @ @ @ @ @ @ &gt; To confirm this , we check their ARP tables ( note compute-0-0 's &gt; entries &gt; &gt;&gt; &gt; have the same MAC address both compute-0-2 and compute-0-3 ) : &gt; &gt;&gt; &gt; &gt; &gt;&gt; &gt; wangd at compute-0-0 $ /sbin/arp -a &gt; &gt;&gt; &gt; ? ( 192.168.0.10 ) at 00:D0:68:0E:AA:66 ether on eth0 &gt; &gt;&gt; &gt; ? ( 192.168.0.251 ) at 00:D0:68:0D:F5:7D ether on eth0 &gt; &gt;&gt; &gt; ? ( 192.168.0.252 ) at 00:D0:68:0D:F5:7D ether on eth0 &gt; &gt;&gt; &gt; &gt; &gt;&gt; &gt; wangd at compute-0-2 $ /sbin/arp -a &gt; &gt;&gt; &gt; ? ( 192.168.0.10 ) at 00:D0:68:0E:AA:66 ether on eth0 &gt; &gt;&gt; &gt; ? ( 192.168.0.254 ) at 00:D0:68:0D:F5:C1 ether on eth0 &gt; &gt;&gt; &gt; &gt; &gt;&gt; &gt; For reference the relevant arp entries from the frontend : &gt; &gt;&gt; &gt; compute-0-0.local ( 192.168.0.254 ) at 00:D0:68:0D:F5:C1 ether on &gt; eth0 &gt; &gt;&gt; &gt; compute-0-2.local ( 192.168.0.252 ) at 00:D0:68:0D:F5:7D ether on &gt; eth0 &gt; &gt;&gt; &gt; compute-0-3.local ( 192.168.0.251 ) at 00:D0:68:0D:F5:E3 ether on &gt; eth0 &gt; &gt;&gt; &gt; &gt; &gt;&gt; &gt; The frontend never has a problem making connections to nodes ( as long &gt; &gt;&gt; @ @ @ @ @ @ @ @ @ @ ) . This is puzzling . The frontend stores &gt; &gt;&gt; the &gt; &gt;&gt; &gt; IP-MAC mapping only for dhcpd purposes , and uses normal ARP exchanges &gt; &gt;&gt; when &gt; &gt;&gt; &gt; making connections , so why does it never get bogus responses , while &gt; &gt;&gt; the &gt; &gt;&gt; &gt; compute nodes sometimes do . When there is a bogus response , the &gt; bogus &gt; &gt;&gt; MAC &gt; &gt;&gt; &gt; is always ( AFAIK ) the MAC of the numerically preceding node . &gt; &gt;&gt; &gt; &gt; &gt;&gt; &gt; Now , I understand that this could be a switch problem or a kernel &gt; &gt;&gt; problem , &gt; &gt;&gt; &gt; but I wanted to confirm that Rocks does n't have the frontend do &gt; &gt;&gt; anything &gt; &gt;&gt; &gt; special to have its ARP tables more accurate , or that compute nodes &gt; &gt;&gt; have &gt; &gt;&gt; &gt; somewhat ' less accurate ' ARP methods . Has anyone else seen this &gt; &gt;&gt; problem ? &gt; &gt;&gt; &gt; &gt;&gt; on the frontend , what is the output of : &gt; &gt;&gt; &gt; &gt;&gt; # dbreport ethers &gt; @ @ @ @ @ @ @ @ @ @ of : &gt; &gt;&gt; &gt; &gt;&gt; # ifconfig -a &gt; &gt;&gt; &gt; &gt;&gt; on compute-0-3 , what is the output of : &gt; &gt;&gt; &gt; &gt;&gt; # ifconfig -a &gt; &gt;&gt; &gt; &gt;&gt; - gb &gt; &gt;&gt; &gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt; &gt; -- &gt; &gt; Philip Papadopoulos , PhD &gt; &gt; University of California , San Diego &gt; &gt; 858-822-3628 &gt; &gt; -------------- next part -------------- &gt; &gt; An HTML attachment was scrubbed ... &gt; &gt; URL : &gt; &gt; &gt; LONG ... &gt; &gt; &gt; &gt; &gt; -- Philip Papadopoulos , PhD University of California , San Diego 858-822-3628 -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From philip.papadopoulos at gmail.com Fri Sep 1 13:06:20 2006 From : philip.papadopoulos at gmail.com ( Philip Papadopoulos ) Date : Fri , 1 Sep 2006 13:06:20 -0700 Subject : Rocks-Discuss httpd on compute nodes In-Reply-To : **40;5884;TOOLONG References : LONG ... **40;5926;TOOLONG Message-ID : LONG ... You can disable if you want , without much loss of functionality . We export the /proc filesystem via http as a matter of convenience for inspecting @ @ @ @ @ @ @ @ @ @ &gt; &gt; Is there a reason why httpd is running on compute nodes in both 4.1 and &gt; 4.2 ? It appears to be the same config that 's installed on the head . I &gt; was considering disabling it for compute nodes to free up resources . &gt; &gt; -- &gt; Scott &gt; &gt; -- Philip Papadopoulos , PhD University of California , San Diego 858-822-3628 -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From philip.papadopoulos at gmail.com Fri Sep 1 14:07:03 2006 From : philip.papadopoulos at gmail.com ( Philip Papadopoulos ) Date : Fri , 1 Sep 2006 14:07:03 -0700 Subject : Rocks-Discuss ROCJS 4.2 install with USB floppy In-Reply-To : LONG ... References : LONG ... Message-ID : LONG ... See if the following post ( using the cached version because our archives are still incorrect ) fixes your problem LONG ... -P On 9/1/06 , jimmy.wilcox at ntlworld.com wrote : &gt; &gt; Hi &gt; &gt; I am trying to install Rocks 4.2 on a Intel SR2500PALLX sytem . I need to &gt; provide a E1000 network driver disk during install @ @ @ @ @ @ @ @ @ @ . Rocks sees the floppy as /dev/sdb . The &gt; frontened installs fine . While installing a compute-node the new E1000 &gt; driver is found . However , the install fails as it tries to mount /tmp/sdb. &gt; This does not exist on the compute node . &gt; &gt; I tried re-installing the head node , removing the USB floppy as soon as &gt; the E1000 driver was found but this had no effect . I also tried customising &gt; compute-node partitions forcing the partitons onto /dev/sda but this did not &gt; work either . &gt; &gt; Any suggestions ? &gt; &gt; Jim &gt; &gt; **41;5968;TOOLONG &gt; Email sent from www.ntlworld.com &gt; Virus-checked using McAfee(R) Software &gt; Visit **25;6011;TOOLONG for more information &gt; &gt; -- Philip Papadopoulos , PhD University of California , San Diego 858-822-3628 -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From philip.papadopoulos at gmail.com Fri Sep 1 14:24:32 2006 From : philip.papadopoulos at gmail.com ( Philip Papadopoulos ) Date : Fri , 1 Sep 2006 14:24:32 -0700 Subject : Rocks-Discuss second interface and DHCP In-Reply-To : **38;6038;TOOLONG References : **38;6078;TOOLONG @ @ @ @ @ @ @ @ @ @ the order of bring up is irrelevant , when your DHCP lease expires , it will renew itself and during the renewal will likely refresh it 's DNS , default router settings and other goodies from the DHCP server . Meaning it might work if you could reverse the order of intialization ( you would have to rewrite the Redhat-supplied code for network initialization and reverse sort by interfaces to accomplish this -- we most definitely do not go down and mess around with the standard startup order of services ) , but then may mysteriously stop working when the DHCP lease expired . -P On 9/1/06 , Wm . Josiah Erikson wrote : &gt; &gt; Hello all , &gt; I have a ROCKS 4.1 cluster of 24 nodes that have two ethernet &gt; interfaces . I decided to try out putting the second interface of the &gt; compute nodes on the render-farm network by plugging the second &gt; interface into the correct VLAN on my switch and seeing what happened . I &gt; enabled the second interface in BIOS , reinstalled the compute node , and &gt; it brought @ @ @ @ @ @ @ @ @ @ names for the right &gt; interfaces , and eth1 even got a DHCP address just like I wanted it to , &gt; which was all totally awesome , except that eth1 comes up after eth0 , &gt; which means that /etc/resolv.conf gets overwritten with the non-cluster &gt; version ... is there any way to get around this , like change the order of &gt; interface initialization , or should I just assign a static IP to eth1 &gt; like it describes in the docs ? &gt; Thanks , &gt; -Josiah &gt; &gt; -- Philip Papadopoulos , PhD University of California , San Diego 858-822-3628 -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From philip.papadopoulos at gmail.com Fri Sep 1 14:33:25 2006 From : philip.papadopoulos at gmail.com ( Philip Papadopoulos ) Date : Fri , 1 Sep 2006 14:33:25 -0700 Subject : Rocks-Discuss Installation problem In-Reply-To : **32;6118;TOOLONG References : **32;6152;TOOLONG LONG ... **32;6186;TOOLONG Message-ID : LONG ... The cpus is reported by nodes that kickstart and affects your queueing system configuration , if you have a queueing system . Sorry for the --name ( @ @ @ @ @ @ @ @ @ @ a " --norestart " to tell it to not restart services or rewrite configs . Then after finishing you can do an #insert-ethers --update To fix your naming , ip addresses , appliance types , etc . Try the following : # insert-ethers --dump &gt; /tmp/mycluster.sh #add-extra-nic --dump &gt;&gt; /tmp/mycluster.sh Then edit the lines in /tmp/mycluster.sh to reflect exactly what you want . You can fix cpus , nodenames , etc . Finally , remove all nodes ( except the frontend ) from your database . For every node : # insert-ethers --remove Now you are ready to repopulate with your fixed data : # /tmp/mycluster.sh ; # you will get some warnings which should be benign . # insert-ethers --update -P On 9/1/06 , Reijo Rasinkangas wrote : &gt; &gt; Hi Philip , &gt; &gt; Philip Papadopoulos wrote : &gt; &gt; Here 's what I would do , since you already have the your node mac &gt; &gt; addresses and what you want them to be named , I would write a small &gt; &gt; script that does the following . &gt; &gt; &gt; &gt; for each ( node @ @ @ @ @ @ @ @ @ @ mac " --ip= " ip " --name= " node " &gt; &gt; --netmask= " netmask " --appliance= " Compute " --norestart --batch &gt; &gt; Thanks , this was exactly the kind of advice I was looking for ! I &gt; managed to add all 45 nodes into the database with a php script . The &gt; --name options did not work , but I just left it out : I got the &gt; compute-0-x naming in the right order by giving the commands in the &gt; same order as the old nodes were named . There was no time to fool &gt; around , though , since even with the above parameters the &gt; /etc/dhcpd.conf etc. files were rewritten after each insert-ethers &gt; command ! However , since the created files also worked right away &gt; ( and the mandriva nodes function now as before ) , this was only a &gt; minor inconvenience ( however , it is better to warn users beforehand &gt; if doing this kind of thing ) . &gt; &gt; A couple of questions more . When I look at the mySQL database , table @ @ @ @ @ @ @ @ @ @ . In reality , there &gt; are two CPUs in each . I guess it is ok the change this by hand in &gt; the database and run " insert-ethers --update " ? I do n't know which &gt; configuration files this parameter relates to , but most likely it &gt; should be correct . &gt; &gt; Trickier problem is that the first attempt to install Rocks in a &gt; node failed still . Similar problem was described in &gt; &gt; &gt; LONG ... &gt; &gt; so this looks like a Rocks bug . &gt; &gt; However , I must confess that I made also one mistake here . After &gt; adding all our nodes to the database , I tried to include also our &gt; Ethernet Switch . Of course I used --appliance= " network " , but this &gt; failed . As I though it more important to have the switch in the &gt; dhcpd.conf than to have the database exactly correct , I added the &gt; switch as a node . This seems to work , but of course there is a &gt; possibility that some problems @ @ @ @ @ @ @ @ @ @ to clean this mess ? Could it be enough to change the Membership &gt; in nodes table from compute ( 2 ) to Ethernet Switch ( 5 ) ? &gt; &gt; Regards , &gt; Reijo &gt; http : //cc.oulu.fi/rar/ &gt; -- Philip Papadopoulos , PhD University of California , San Diego 858-822-3628 -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From mgreaney at fnal.gov Fri Sep 1 15:07:28 2006 From : mgreaney at fnal.gov ( MargaretGreaney ) Date : Fri , 01 Sep 2006 17:07:28 -0500 ( CDT ) Subject : Rocks-Discuss rocks 4.1 appliance partitioning Message-ID : Hi , I 've been reviewing what people on the list do to get a different partition scheme than what compute appliance provides . We have a partition method , but I want to try to use a method closer to the rocks method . I would like to have an appliance that has different partitioning , not an extension . I do n't want to replace the compute appliance with a different default partition , but want the new appliance , farmwn-app , to have @ @ @ @ @ @ @ @ @ @ scheme can be done with **26;6220;TOOLONG with my partition information . But how do I enter it into the farmwn-app.xml ? and how does it know to not use the auto-partition.xml ? here is what is in my farmwn-app.xml , located in LONG ... LONG ... ( this is just a skeleton ) farm-partition ( this has part info ) --------- the farm-partition.xml is in the nodes directory . When I try to install the node , it gets stuck on auto-partition.xml which I can see by entering alt f3 on the console . I removed . /rocks-release before rebooting this node , which did have a regular compute install earlier . Do I need to change any other xml files ? I know to do rocks-dist dist when adding new xmls or changing the graph . thanks , Margaret -- Margaret Greaney Telephone : 630-840-4623 Fermilab E-mail : mgreaney at fnal.gov CD/CSS/FCS From mjk at sdsc.edu Fri Sep 1 15:25:29 2006 From : mjk at sdsc.edu ( mason j. katz ) Date : Fri , 1 Sep 2006 15:25:29 -0700 Subject : Rocks-Discuss rocks 4.1 appliance partitioning In-Reply-To : @ @ @ @ @ @ @ @ @ @ explicitly is your graph is n't correct . The replace-* and extend-* node files are a simple hack to allow you to add/modify Rocks without creating a new graph file . I think what you really want here is : auto-partitoncompute This will automatically use the **26;6248;TOOLONG file if it exists , otherwise will use auto-partition.xml . Also the ordering of **29;6276;TOOLONG should probably be explicit . The following only says that they happen last , between the two of them the order is not specified . **28;6307;TOOLONG -mjk On Sep 1 , 2006 , at 03:07 P , MargaretGreaney wrote : &gt; Hi , &gt; &gt; I 've been reviewing what people on the list do to get a different &gt; partition scheme &gt; than what compute appliance provides . We have a partition method , &gt; but I &gt; want to try to use a method closer to the rocks method . &gt; &gt; I would like to have an appliance that has different partitioning , &gt; not an extension . &gt; I do n't want to replace the compute appliance with a different &gt; default partition , @ @ @ @ @ @ @ @ @ @ have a different partition &gt; scheme . &gt; &gt; I know the different partition scheme can be done with replace-auto- &gt; partition.xml with &gt; my partition information . But how do I enter it into the farmwn- &gt; app.xml ? and how &gt; does it know to not use the auto-partition.xml ? &gt; &gt; here is what is in my farmwn-app.xml , located in /home/install/site- &gt; **27;6337;TOOLONG : &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; replace-auto-partiton &gt; compute &gt; &gt; &gt; &gt; replace-auto-partition ( this is just a skeleton ) &gt; farm-partition ( this has part info ) &gt; &gt; &gt; &gt; &gt; &gt; --------- &gt; the farm-partition.xml is in the nodes directory . When I try to &gt; install the node , it gets &gt; stuck on auto-partition.xml which I can see by entering alt f3 on &gt; the console . &gt; I removed . /rocks-release before rebooting this node , which did &gt; have a regular compute &gt; install earlier . &gt; &gt; Do I need to change any other xml files ? &gt; &gt; I know to do rocks-dist dist when @ @ @ @ @ @ @ @ @ @ &gt; thanks , &gt; &gt; Margaret &gt; &gt; -- &gt; Margaret Greaney Telephone : 630-840-4623 &gt; Fermilab E-mail : mgreaney at fnal.gov &gt; CD/CSS/FCS -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From wangd at uci.edu Fri Sep 1 15:34:51 2006 From : wangd at uci.edu ( Daniel Wang ) Date : Fri , 01 Sep 2006 15:34:51 -0700 Subject : Rocks-Discuss MANPATH in 4.2 In-Reply-To : References : LONG ... Message-ID : **32;6366;TOOLONG Greg Bruno wrote : &gt; On 8/16/06 , Mike Hallock wrote : &gt;&gt; Hello , &gt;&gt; &gt;&gt; Freshly installed Rocks 4.2 on a cluster , but found that the MANPATH &gt;&gt; environment variable is being improperly set and man is not finding &gt;&gt; anything . The path is being set to : &gt;&gt; &gt;&gt; $ echo $MANPATH &gt;&gt; /opt/gridengine/man : /usr/java/jdk1.5.007/man &gt;&gt; &gt;&gt; I think this is due to having a manpath set for java in &gt;&gt; /etc/profile.d/java.sh , which gets sourced before sge-binaries.sh , which &gt;&gt; will set a default man path only if MANPATH is currently empty . &gt; &gt; thanks for the bug report -- we 'll @ @ @ @ @ @ @ @ @ @ fix are you putting in ? I 'd like to roll it into my own installation , and want to do it the " right way " . As far as I can tell , sge-binaries.sh and java.sh get sourced in that order from /etc/init.d/globus . /etc/init.d/globus seems to be derived from **36;6400;TOOLONG ( am I right ? ) , which does n't seem to have had any revisions in CVS since Mike 's message . Just curious about the right way before I hack in something worse , ; ) -Daniel W. From ghuntress at ciw.edu Fri Sep 1 16:01:27 2006 From : ghuntress at ciw.edu ( Garret Huntress ) Date : Fri , 1 Sep 2006 19:01:27 -0400 Subject : Rocks-Discuss MANPATH in 4.2 In-Reply-To : **32;6438;TOOLONG References : LONG ... **32;6472;TOOLONG Message-ID : Daniel , As far as hacking something worse , I 've already done just that ; - ) Step 1 . ) Insert all MANPATH definitions defined in /etc/man.conf into a new profile.d script ( of yoru choice , I called it amansetup. ( c ) ? sh so that it gets sourced first @ @ @ @ @ @ @ @ @ @ looks like the following ( modify for equivalent csh file ) : export **29;6506;TOOLONG export PATH=$JAVAHOME/bin : $PATH if $#MANPATH -gt 0 ; then MANPATH=$JAVAHOME/man : $MANPATH else MANPATH=$JAVAHOME/man fi Step 3 . ) I think there was another fix involving an invalid MANTYPE variable , but I ca n't remember ( it was a problem in csh only , not sh/ bash ) . Voila . -Garret On Sep 1 , 2006 , at 6:34 PM , Daniel Wang wrote : &gt; Greg Bruno wrote : &gt;&gt; On 8/16/06 , Mike Hallock wrote : &gt;&gt;&gt; Hello , &gt;&gt;&gt; &gt;&gt;&gt; Freshly installed Rocks 4.2 on a cluster , but found that the MANPATH &gt;&gt;&gt; environment variable is being improperly set and man is not finding &gt;&gt;&gt; anything . The path is being set to : &gt;&gt;&gt; &gt;&gt;&gt; $ echo $MANPATH &gt;&gt;&gt; /opt/gridengine/man : /usr/java/jdk1.5.007/man &gt;&gt;&gt; &gt;&gt;&gt; I think this is due to having a manpath set for java in &gt;&gt;&gt; /etc/profile.d/java.sh , which gets sourced before sge- &gt;&gt;&gt; binaries.sh , which &gt;&gt;&gt; will set a default man path only if MANPATH is currently empty . &gt;&gt; &gt;&gt; thanks for the @ @ @ @ @ @ @ @ @ @ &gt;&gt; &gt;&gt; - gb &gt; &gt; What fix are you putting in ? I 'd like to roll it into my own &gt; installation , and want to do it the " right way " . As far as I can &gt; tell , sge-binaries.sh and java.sh get sourced in that order from / &gt; etc/init.d/globus . /etc/init.d/globus seems to be derived from &gt; **36;6537;TOOLONG ( am I right ? ) , which does n't &gt; seem to have had any revisions in CVS since Mike 's message . &gt; &gt; Just curious about the right way before I hack in something worse , ; ) &gt; -Daniel W. &gt; &gt; &gt; -- Garret W. Huntress System Administrator / System Developer Geophysical Laboratory Carnegie Institution of Washington 5251 Broad Branch Road , NW Washington , DC 20015 Email : ghuntress at ciw.edu Phone : ( 202 ) was scrubbed ... Name : Garret Huntress.vcf Type : text/directory Size : 18045 bytes Desc : @ @ @ @ @ @ @ @ @ @ An HTML attachment was scrubbed ... URL : LONG ... From greg.bruno at gmail.com Fri Sep 1 16:02:23 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : Fri , 1 Sep 2006 16:02:23 -0700 Subject : Rocks-Discuss MANPATH in 4.2 In-Reply-To : **32;6575;TOOLONG References : LONG ... **32;6609;TOOLONG Message-ID : On 9/1/06 , Daniel Wang wrote : &gt; &gt; What fix are you putting in ? I 'd like to roll it into my own &gt; installation , and want to do it the " right way " . As far as I can tell , &gt; sge-binaries.sh and java.sh get sourced in that order from &gt; /etc/init.d/globus . /etc/init.d/globus seems to be derived from &gt; **36;6643;TOOLONG ( am I right ? ) , which does n't seem &gt; to have had any revisions in CVS since Mike 's message . &gt; &gt; Just curious about the right way before I hack in something worse , ; ) append a line to /etc/man.config. - gb From federicosacerdoti at gmail.com Fri Sep 1 16:09:42 2006 From : federicosacerdoti at gmail.com ( Federico D. Sacerdoti ) Date : Fri @ @ @ @ @ @ @ @ @ @ roll version static in WAN distribution In-Reply-To : References : LONG ... Message-ID : LONG ... Thanks , Of course your 're right . Perhaps there could be a way to specify the kernel roll used in the database as well : If database present : consult it for enabled kernel roll else : parse roll.xml That way we can use the database after installation instead of editing xml files . -Federico On 8/31/06 , Greg Bruno wrote : &gt; On 8/30/06 , Federico D. Sacerdoti wrote : &gt; &gt; Hi , &gt; &gt; &gt; &gt; I noticed that my rocks-dist dist was not picking up the kernel roll &gt; &gt; when building the ' wan ' part of the distribution . Looking through the &gt; &gt; code revealed the reason was the setPristine function in &gt; &gt; pylib:build.py &gt; &gt; &gt; &gt; &gt; &gt; def setPristine(self) : &gt; &gt; self.withRolls = ( ' base ' , self.usageversion ) , &gt; &gt; ( ' kernel ' , self.usageversion ) &gt; &gt; self.withSiteProfiles = 0 &gt; &gt; return &gt; &gt; &gt; &gt; I have built my frontend with a custom kernel @ @ @ @ @ @ @ @ @ @ was made from : 2.6.15 . This causes problems &gt; &gt; since the above code wants it to be version 4.1 ( or what have you ) . &gt; &gt; &gt; &gt; I believe the logic should be " select * from rolls where name='kernel ' &gt; &gt; and enabled='yes ' " . It is reasonable to require that only one kernel &gt; &gt; roll be enabled , but not to tie us down to a specific version number . &gt; &gt; thanks for the suggestion and i agree , the hardcoded values need to &gt; go . but , we need something that will work without the database &gt; present , that is , it needs to work in the installation environment &gt; too . i believe we 'll end up parsing a file called ' /tmp/rolls.xml ' -- &gt; this file in the installation environment is a description of all the &gt; rolls that will be installed . &gt; &gt; - gb &gt; -- D. E. Shaw Research LLC New York NY From federicosacerdoti at gmail.com Fri Sep 1 16:11:46 2006 From : federicosacerdoti at gmail.com ( Federico @ @ @ @ @ @ @ @ @ @ 19:11:46 -0400 Subject : Rocks-Discuss frontend install question Message-ID : LONG ... I have a pesky problem that has affected my installs of two frontends under Rocks 4.1 . When I choose manual partitioning , I can not specify a new partition as type " SW Raid " or even " ext3 " . The only choices are ext2 , swap , vfat , PVM vol . Very annoying . I have not done an extensive search of the archives , so I apologize if this has already been answered . Thanks , Federico -- D. E. Shaw Research LLC New York NY From greg.bruno at gmail.com Fri Sep 1 16:15:54 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : Fri , 1 Sep 2006 16:15:54 -0700 Subject : Rocks-Discuss cross kickstart problems In-Reply-To : References : LONG ... Message-ID : On 9/1/06 , Paul Mitchell wrote : &gt; On Fri , 1 Sep 2006 , Greg Bruno wrote : &gt; &gt; &gt; # cd /home/install &gt; &gt; # rm -rf rocks-dist &gt; &gt; # rocks-dist dist &gt; &gt; # rocks-dist --arch=i386 dist &gt; &gt; Thanks , @ @ @ @ @ @ @ @ @ @ road ( though I had to &gt; first remove the /root/rocks-dist and the /home/install/rocks-dist before &gt; it worked ) . &gt; &gt; I 'm not all the way home though . Unfortunately , I 'm back in the same &gt; place I was last week : As it starts to read in the rolls , I get the &gt; classic : &gt; &gt; Unable to read header list . This may be due to a &gt; missing file or bad media . Press to try again . &gt; &gt; Of course , this never resolves . The confusion I 'm having here is that the &gt; same message has now shown up on my x8664 clients ( which kickstarted &gt; successfully last week ) as well . Furthermore , both sets of rolls were &gt; downloaded directly as iso images , so I know the media is not the problem . &gt; What 's changed on the frontend machine ? &gt; &gt; I think I may have already asked this before , but I 'm still somewhat &gt; unclear - is there a method for determining exactly which header it @ @ @ @ @ @ @ @ @ @ is the log file for the &gt; kickstart ? - I ca n't seem to find it . i guess i 'd start by looking at the httpd logs and # tail -f /var/log/httpd/* then PXE boot a node . as the node makes progress , you should see status messages from the above ' tail ' . hopefully , the error messages will point you in the direction of the root cause . - gb From greg.bruno at gmail.com Fri Sep 1 16:17:31 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : Fri , 1 Sep 2006 16:17:31 -0700 Subject : Rocks-Discuss frontend install question In-Reply-To : LONG ... References : LONG ... Message-ID : On 9/1/06 , Federico D. Sacerdoti wrote : &gt; I have a pesky problem that has affected my installs of two frontends &gt; under Rocks 4.1. &gt; &gt; When I choose manual partitioning , I can not specify a new partition as &gt; type " SW Raid " or even " ext3 " . The only choices are ext2 , swap , vfat , &gt; PVM vol . Very annoying . @ @ @ @ @ @ @ @ @ @ the archives , so I apologize if &gt; this has already been answered . generally , that means the ext3 module did n't load . you may want to double check your boot CD ISO and media . - gb From jbecker at northwestern.edu Fri Sep 1 16:26:39 2006 From : jbecker at northwestern.edu ( Jesse Becker ) Date : Fri , 1 Sep 2006 18:26:39 -0500 Subject : Rocks-Discuss Matlab and mixed architecture In-Reply-To : LONG ... References : LONG ... Message-ID : LONG ... On Fri , Sep 01 , 2006 at 07:21:47AM -0400 , Edward Chrzanowski CSCF wrote : &gt; &gt; I am building a ROCKS cluster , and one of the main applications that &gt; &gt; will be used is Matlab . The machines in the cluster are 64-bit Opteron &gt; &gt; towers , and I am about to purchase a machine for the head node . I was &gt; &gt; looking at purchasing a 64-bit Xeon machine , but I am curious if anyone &gt; &gt; knows of any compatibility problems with installing/running matlab on a &gt; &gt; mixed-architecture ROCKS install . If anyone has @ @ @ @ @ @ @ @ @ @ or any general advice about installing Matlab on &gt; &gt; ROCKS , I would greatly appreciate it . I was planning to build a Matlab &gt; &gt; RPM via http : **36;6681;TOOLONG , but if anyone has any &gt; &gt; other suggestions , I 'd be glad to hear them . &gt; &gt; &gt; Matlab does not differentiate between architectures in this case . &gt; If you had mixed 32-bit and 64-bit you might have a problem . In fact , Mathworks does n't support mixing toolboxes from 32bit systems on 64bit hosts . &gt;From : support at mathworks.com &gt;Unfortunately , there are certain toolboxes/products that are not &gt;available for the 64-bit Linux platform and The MathWorks does not &gt;support the installation of the 32-bit version of those products &gt;into a 64-bit installation . -- Jesse Becker GPG-fingerprint : BD00 7AA4 4483 AFCC 82D0 2720 0083 0931 9A2B 06A2 -------------- next part -------------- A non-text attachment was scrubbed ... Name : not available Type : **25;6719;TOOLONG Size : 1741 bytes Desc : not available Url : LONG ... From ghuntress at ciw.edu Fri Sep 1 16:30:29 2006 From : ghuntress @ @ @ @ @ @ @ @ @ @ 1 Sep 2006 19:30:29 -0400 Subject : Rocks-Discuss Trouble installing Rocks 4.2 : graphical install frozen at " Reading Package Information " In-Reply-To : References : LONG ... Message-ID : Hot ! Debug node was able to pxe boot and do the install without me touching a thing ! Final contents of **39;6746;TOOLONG was : default ks prompt 0 label ks kernel vmlinuz append ks initrd=initrd.img ramdisksize=150000 lang= devfs=nomount pxe kssendmac selinux=0 headless vnc Many thanks ! -Garret On Aug 31 , 2006 , at 10:36 PM , Greg Bruno wrote : &gt; On 8/31/06 , Garret Huntress wrote : &gt;&gt; &gt;&gt; Anaconda bombs with a traceback about importing gtk , then says it &gt;&gt; ca n't open &gt;&gt; display , and then reboots. &gt; &gt; try removing the ' nofb ' and ' vga=off ' flags . &gt; &gt; - gb -- Garret W. Huntress System Administrator / System Developer Geophysical Laboratory Carnegie Institution of Washington 5251 Broad Branch Road , NW Washington , DC 20015 Email : ghuntress at ciw.edu Phone : ( 202 ) was scrubbed ... Name : Garret Huntress.vcf Type : text/directory Size : 18045 bytes Desc : not available Url : LONG ... -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From baker at usgs.gov Fri Sep 1 17:07:12 2006 From : baker at usgs.gov ( Larry Baker ) Date : Fri , 1 Sep 2006 17:07:12 -0700 Subject : Rocks-Discuss MANPATH in 4.2 In-Reply-To : References : LONG ... **32;6787;TOOLONG Message-ID : LONG ... Greg , I have n't started playing with Rocks 4.2 yet , but I did run into the MANPATH problem because of my own scripts added to /etc/profile.d . My observation is that entries in /etc/man.config are not preserved if MANPATH is defined without preserving them explicitly using the manpath command ( in bash , you can also use $manpath ) . The correct method of preserving those entries is code such as : if $#MANPATH = 0 ; then export MANPATH= ? manpath : $PGI/linux86-64/6.1/man ? else export LONG ... fi for the . sh variant , and if ! ( $ ? MANPATH ) @ @ @ @ @ @ @ @ @ @ MANPATH ? $MANPATH : $PGI/linux86-64/6.1/man ? endif for the . csh variant . Previously , my code had looked similar to the code provided by Garrett Huntress : if $#MANPATH -gt 0 ; then MANPATH=$JAVAHOME/man : $MANPATH else MANPATH=$JAVAHOME/man fi which did not work . In that case , the only paths searched by man were the ones defined in MANPATH , not the entries in /etc/ man.config . ( That is why his first step to copy all the entries from . etc.man.config into another /etc/profile.d script is needed . ) Larry Baker US Geological Survey 650-329-5608 baker at usgs.gov On Sep 1 , 2006 , at 4:02 PM , Greg Bruno wrote : &gt; On 9/1/06 , Daniel Wang wrote : &gt;&gt; &gt;&gt; What fix are you putting in ? I 'd like to roll it into my own &gt;&gt; installation , and want to do it the " right way " . As far as I can &gt;&gt; tell , &gt;&gt; sge-binaries.sh and java.sh get sourced in that order from &gt;&gt; /etc/init.d/globus . /etc/init.d/globus seems to be derived from &gt;&gt; **36;6821;TOOLONG ( am I right ? ) @ @ @ @ @ @ @ @ @ @ any revisions in CVS since Mike 's message . &gt;&gt; &gt;&gt; Just curious about the right way before I hack in something worse , ; ) &gt; &gt; append a line to /etc/man.config. &gt; &gt; - gb From baker at usgs.gov Fri Sep 1 17:09:59 2006 From : baker at usgs.gov ( Larry Baker ) Date : Fri , 1 Sep 2006 17:09:59 -0700 Subject : Rocks-Discuss MANPATH in 4.2 In-Reply-To : References : LONG ... **32;6859;TOOLONG Message-ID : Garrett , The reason your step 1 is required is because your step 2 has the same problem I used to have in my /etc/profile.d scripts for the PGI Workstation compilers . The correct method of preserving the entries in /etc/man.config is code such as : if $#MANPATH = 0 ; then export MANPATH= ? manpath : $PGI/linux86-64/6.1/man ? else export LONG ... fi for the pgi.sh variant , and if ! ( $ ? MANPATH ) then setenv MANPATH ? manpath : $PGI/linux86-64/6.1/man ? else setenv MANPATH ? $MANPATH : $PGI/linux86-64/6.1/man ? endif for the pgi.csh variant . I.e. , you must include the output from manpath if MANPATH is @ @ @ @ @ @ @ @ @ @ usgs.gov On Sep 1 , 2006 , at 4:01 PM , Garret Huntress wrote : &gt; Daniel , &gt; &gt; As far as hacking something worse , I 've already done just that ; - ) &gt; &gt; Step 1 . ) Insert all MANPATH definitions defined in /etc/man.conf into &gt; a new profile.d script ( of yoru choice , I called it amansetup. ( c ) ? sh &gt; so that it gets sourced first ) &gt; &gt; Step 2 . ) Fix java.sh so that it looks like the following ( modify for &gt; equivalent csh file ) : &gt; export **29;6893;TOOLONG &gt; export PATH=$JAVAHOME/bin : $PATH &gt; if $#MANPATH -gt 0 ; then &gt; MANPATH=$JAVAHOME/man : $MANPATH &gt; else &gt; MANPATH=$JAVAHOME/man &gt; fi &gt; &gt; Step 3 . ) I think there was another fix involving an invalid MANTYPE &gt; variable , but I ca n't remember ( it was a problem in csh only , not sh/ &gt; bash ) . &gt; &gt; Voila . &gt; &gt; -Garret &gt; &gt; &gt; &gt; On Sep 1 , 2006 , at 6:34 PM , Daniel Wang wrote : @ @ @ @ @ @ @ @ @ @ Mike Hallock wrote : &gt;&gt;&gt;&gt; Hello , &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; Freshly installed Rocks 4.2 on a cluster , but found that the &gt;&gt;&gt;&gt; MANPATH &gt;&gt;&gt;&gt; environment variable is being improperly set and man is not finding &gt;&gt;&gt;&gt; anything . The path is being set to : &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; $ echo $MANPATH &gt;&gt;&gt;&gt; /opt/gridengine/man : /usr/java/jdk1.5.007/man &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; I think this is due to having a manpath set for java in &gt;&gt;&gt;&gt; /etc/profile.d/java.sh , which gets sourced before sge- &gt;&gt;&gt;&gt; binaries.sh , which &gt;&gt;&gt;&gt; will set a default man path only if MANPATH is currently empty . &gt;&gt;&gt; &gt;&gt;&gt; thanks for the bug report -- we 'll put in a fix . &gt;&gt;&gt; &gt;&gt;&gt; - gb &gt;&gt; &gt;&gt; What fix are you putting in ? I 'd like to roll it into my own &gt;&gt; installation , and want to do it the " right way " . As far as I can &gt;&gt; tell , sge-binaries.sh and java.sh get sourced in that order from / &gt;&gt; etc/init.d/globus . /etc/init.d/globus seems to be derived from &gt;&gt; **36;6924;TOOLONG ( am I right ? ) , which does n't &gt;&gt; seem to have had @ @ @ @ @ @ @ @ @ @ &gt;&gt; Just curious about the right way before I hack in something worse , ; ) &gt;&gt; -Daniel W. &gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; -- &gt; Garret W. Huntress &gt; System Administrator / System Developer &gt; &gt; Geophysical Laboratory &gt; Carnegie Institution of Washington &gt; 5251 Broad Branch Road , NW &gt; Washington , DC 20015 &gt; &gt; Email : ghuntress at ciw.edu &gt; Phone : ( 202 ) -478-8973 &gt; AIM : Garret Huntress &gt; &gt; &gt; &gt; -------------- next part -------------- &gt; An HTML attachment was scrubbed ... &gt; URL : LONG ... &gt; **45;6962;TOOLONG &gt; -------------- next part -------------- &gt; A non-text attachment was scrubbed ... &gt; Name : Garret Huntress.vcf &gt; Type : text/directory &gt; Size : 18045 bytes &gt; Desc : not available &gt; Url : LONG ... &gt; LONG ... &gt; -------------- next part -------------- &gt; An HTML attachment was scrubbed ... &gt; URL : LONG ... &gt; LONG ... -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From mhbucn at gmail.com Fri Sep 1 19:41:16 2006 From : mhbucn at gmail.com ( Manuel Herrera ) Date : @ @ @ @ @ @ @ @ @ @ Problems with ethernet and Rocks Message-ID : **32;7009;TOOLONG Hi . I have 1 frontend and 5 nodes , but im having troubles with the Ethernet Dlink DGE-530T(GigaLan) , When i try to install nodes , the frontend cant send files ...... i think the problem is the module of the ethernet , but i do n't  know how fix it , i hope that somebody here can give me a clue . Thanks . Manuel Herrera B. Universidad Catolica Del Norte Antofagasta , Chile From mhbucn at gmail.com Fri Sep 1 20:00:22 2006 From : mhbucn at gmail.com ( Manuel Herrera ) Date : Fri , 01 Sep 2006 23:00:22 -0400 Subject : Rocks-Discuss Problems with ethernet and Rocks 4.1 and/or 4.2 Message-ID : **34;7043;TOOLONG Hi . I have 1 frontend and 5 nodes , but im having troubles with the Ethernet Dlink DGE-530T(GigaLan) , When i try to install nodes , the frontend cant send files ...... i think the problem is the module of the ethernet , but i do n't  know how fix it , i hope that somebody here can give me a clue . Thanks @ @ @ @ @ @ @ @ @ @ Chile From wjerikson at hampshire.edu Sat Sep 2 07:31:19 2006 From : wjerikson at hampshire.edu ( Wm . Josiah Erikson ) Date : Sat , 02 Sep 2006 10:31:19 -0400 Subject : Rocks-Discuss second interface and DHCP In-Reply-To : LONG ... References : **38;7079;TOOLONG LONG ... Message-ID : **38;7119;TOOLONG Right . That makes sense . Thanks so much for taking the time to tell me what I should have already figured out ! : ) -Josiah Philip Papadopoulos wrote : &gt; Define the IP 's statically -- the order of bring up is irrelevant , when &gt; your DHCP lease expires , it will &gt; renew itself and during the renewal will likely refresh it 's DNS , default &gt; router settings and other goodies from the DHCP server . &gt; &gt; Meaning it might work if you could reverse the order of intialization ( you &gt; would have to rewrite the &gt; Redhat-supplied code for network initialization and reverse sort by &gt; interfaces to accomplish this -- we most definitely do not go down and &gt; mess around with the standard startup order of services ) , but then @ @ @ @ @ @ @ @ @ @ . &gt; &gt; -P &gt; &gt; &gt; &gt; On 9/1/06 , Wm . Josiah Erikson wrote : &gt; &gt;&gt; Hello all , &gt;&gt; I have a ROCKS 4.1 cluster of 24 nodes that have two ethernet &gt;&gt; interfaces . I decided to try out putting the second interface of the &gt;&gt; compute nodes on the render-farm network by plugging the second &gt;&gt; interface into the correct VLAN on my switch and seeing what happened . I &gt;&gt; enabled the second interface in BIOS , reinstalled the compute node , and &gt;&gt; it brought up eth0 and eth1 , and they were the right names for the right &gt;&gt; interfaces , and eth1 even got a DHCP address just like I wanted it to , &gt;&gt; which was all totally awesome , except that eth1 comes up after eth0 , &gt;&gt; which means that /etc/resolv.conf gets overwritten with the non-cluster &gt;&gt; version ... is there any way to get around this , like change the order of &gt;&gt; interface initialization , or should I just assign a static IP to eth1 &gt;&gt; like it describes in the docs ? &gt;&gt; Thanks @ @ @ @ @ @ @ @ @ @ nasr974 at hotmail.com Sat Sep 2 13:11:56 2006 From : nasr974 at hotmail.com ( Nasr Y.M.J.O. ) Date : Sat , 02 Sep 2006 20:11:56 +0000 Subject : Rocks-Discuss Lustre Roll Installation Message-ID : Dear All , I 'm trying to install lustre roll ( i386 ) for Rocks 4.1 by adding the roll on the fly but I get errors when running " kroll lustre &gt; /tmp/install-lustre-roll " as shown below : LONG ... # mount /dev/cdrom /mnt/cdrom mount : block device /dev/cdrom is write-protected , mounting read-only # cd /home/install # rocks-dist --install copyroll Copying roll from media ( directory " /mnt/cdrom " ) into mirror Copying " lustre " ( 4.1 , i386 ) roll ... 349930 blocks chmod a+rx **35;7159;TOOLONG # rocks-dist dist Cleaning distribution Resolving versions ( base files ) including " kernel " ( 4.1 , i386 ) roll ... including " area51 " ( 4.1 , i386 ) roll ... including " java " ( 4.1 , i386 ) roll ... including " lustre " ( 4.1 , i386 ) roll ... including " web-server " ( 4.1 , i386 ) roll ... including @ @ @ @ @ @ @ @ @ @ including " sge " ( 4.1 , i386 ) roll ... including " hpc " ( 4.1 , i386 ) roll ... including " ganglia " ( 4.1 , i386 ) roll ... including " os " ( 4.1 , i386 ) roll ... Resolving versions ( RPMs ) including " kernel " ( 4.1 , i386 ) roll ... including " area51 " ( 4.1 , i386 ) roll ... including " java " ( 4.1 , i386 ) roll ... including " lustre " ( 4.1 , i386 ) roll ... including " web-server " ( 4.1 , i386 ) roll ... including " base " ( 4.1 , i386 ) roll ... including " sge " ( 4.1 , i386 ) roll ... including " hpc " ( 4.1 , i386 ) roll ... including " ganglia " ( 4.1 , i386 ) roll ... including " os " ( 4.1 , i386 ) roll ... Resolving versions ( SRPMs ) including " kernel " ( 4.1 , i386 ) roll ... including " area51 " ( 4.1 , i386 ) roll ... including " java " @ @ @ @ @ @ @ @ @ @ " ( 4.1 , i386 ) roll ... including " web-server " ( 4.1 , i386 ) roll ... including " base " ( 4.1 , i386 ) roll ... including " sge " ( 4.1 , i386 ) roll ... including " hpc " ( 4.1 , i386 ) roll ... including " ganglia " ( 4.1 , i386 ) roll ... including " os " ( 4.1 , i386 ) roll ... Creating files ( symbolic links - fast ) Applying netstg2.img Installing XML Kickstart profiles installing " hpc " profiles ... installing " lustre " profiles ... installing " ganglia " profiles ... installing " web-server " profiles ... installing " base " profiles ... installing " java " profiles ... installing " sge " profiles ... installing " area51 " profiles ... installing " kernel " profiles ... installing " os " profiles ... installing " site " profiles ... Fixing Comps.xml Database Generating hdlist ( rpm database ) making " torrent " files for RPMS Cleaning distribution Resolving versions ( base files ) including " kernel " ( 4.1 , i386 ) roll ... including @ @ @ @ @ @ @ @ @ @ Resolving versions ( RPMs ) including " kernel " ( 4.1 , i386 ) roll ... including " base " ( 4.1 , i386 ) roll ... Resolving versions ( SRPMs ) including " kernel " ( 4.1 , i386 ) roll ... including " base " ( 4.1 , i386 ) roll ... Creating files ( symbolic links - fast ) Applying netstg2.img Installing XML Kickstart profiles installing " kernel " profiles ... installing " base " profiles ... Generating hdlist ( rpm database ) Linking boot stages from lan Building Roll Central Links # kroll lustre &gt; /tmp/install-lustre-roll Traceback ( most recent call last ) : File " /opt/rocks/sbin/kpp " , line 1883 , in ? app.run() File " /opt/rocks/sbin/kpp " , line 918 , in run handler.parseNode ( node , self.doEval ) File " /opt/rocks/sbin/kpp " , line 1084 , in parseNode parser.parse(fin) File LONG ... line 109 , in parse **38;7196;TOOLONG , source ) File LONG ... line 123 , in parse self.feed(buffer) File LONG ... line 220 , in feed self. **26;7236;TOOLONG File LONG ... line 38 , in fatalError raise exception xml.sax. **28;7264;TOOLONG : @ @ @ @ @ @ @ @ @ @ ( most recent call last ) : File " /opt/rocks/sbin/kroll " , line 361 , in ? app.run() File " /opt/rocks/sbin/kroll " , line 339 , in run doc = **29;7324;TOOLONG ( text , ' n ' ) ) File LONG ... line 60 , in fromString return self.fromStream ( stream , ownerDoc ) File LONG ... line 372 , in fromStream self.parser.parse(s) File LONG ... line 109 , in parse **38;7355;TOOLONG , source ) File LONG ... line 125 , in parse self.close() File LONG ... line 226 , in close self.feed ( " " , isFinal = 1 ) File LONG ... line 220 , in feed self. **26;7395;TOOLONG File LONG ... line 340 , in fatalError raise exception xml.sax. **28;7423;TOOLONG : : 1:0 : no element found LONG ... I 'm using the boot roll LONG ... + 4 OS cd 's . Any thoughts on how to fix this are really appreciated . nasr LONG ... Find just what you are after with the more precise , more powerful new MSN Search . http : //search.msn.com.my/ Try it now . From mashaojie at nuaa.edu.cn Mon Sep 4 @ @ @ @ @ @ @ @ @ @ ) Date : Tue , 5 Sep 2006 11:34:23 +0800 Subject : Rocks-Discuss about mpirun -nolocal errors Message-ID : **43;7453;TOOLONG Dear Sir : I want to use the second network to calculation . So I need to modify the hostname compute-0-0 into compute-0-0-myri0 in machinefile . In my opinion , I should add -nolocal option to excute mpirun , as following : /opt/mpich/gnu/mpirun -nolocal -np 8 -machinefile mymachinefile myprogram Howerver , an error occurs : connect to address 192.168.1.251 : Connection refused connect to address 192.168.1.251 : Connection refused trying normal rsh ( /usr/bin/rsh ) p08864 : p4error : Child process exited while making connection to remote process on compute-0-3-myri0 : 0 compute-0-3-myri0 : Connection refused As we known , ssh is used in rocks , instead of rsh . I try to ssh 192.168.1.251 or ssh compute-0-3-myri0 . It is OK . Please tell me how to solve the problem . Best Regards LONG ... Shaojie Ma Institute of Nano Science Nanjing University of Aeronautics and Astronautics mashaojie at nuaa.edu.cn Nanjing 210016 , China LONG ... From mashaojie at nuaa.edu.cn Mon Sep 4 20:18:36 2006 From : mashaojie at @ @ @ @ @ @ @ @ @ @ Sep 2006 11:18:36 +0800 Subject : Rocks-Discuss about mpirun -nolocal Message-ID : **43;7498;TOOLONG Dear Sir : I want to use the second network to calculation . So I need to modify the hostname compute-0-0 into compute-0-0-myri0 in machinefile . In my opinion , I should add -nolocal option to excute mpirun , as following : /opt/mpich/gnu/mpirun -nolocal -np 8 -machinefile mymachinefile myprogram Howerver , an error occurs : connect to address 192.168.1.251 : Connection refused connect to address 192.168.1.251 : Connection refused trying normal rsh ( /usr/bin/rsh ) p08864 : p4error : Child process exited while making connection to remote process on compute-0-3-myri0 : 0 compute-0-3-myri0 : Connection refused As we known , ssh is used in rocks , instead of rsh . I try to ssh 192.168.1.251 or ssh compute-0-3-myri0 . It is OK . Please tell me how to solve the problem . Best Regards LONG ... Shaojie Ma Institute of Nano Science Nanjing University of Aeronautics and Astronautics mashaojie at nuaa.edu.cn Nanjing 210016 , China LONG ... From greg.lindahl at qlogic.com Tue Sep 5 09:07:03 2006 From : greg.lindahl at qlogic.com ( Greg Lindahl ) Date : @ @ @ @ @ @ @ @ @ @ cpuspeed ? Message-ID : LONG ... In Rocks / CentOS 4.2 , is cpuspeed supposed to work ? It seems to be disabled in the kernel , whereas RHEL4U2 has it built in. -- greg From pmitchel at email.unc.edu Tue Sep 5 09:33:21 2006 From : pmitchel at email.unc.edu ( Paul Mitchell ) Date : Tue , 5 Sep 2006 12:33:21 -0400 ( EDT ) Subject : Rocks-Discuss Locked out of head node In-Reply-To : References : Message-ID : Folks , I 'm running a bioportal at UNC - or at least I was until this morning ! ) Over the last month or two , one of the head nodes has frozen up a couple of times- wo n't allow ssh/httpd connections . Luckily , the last time , I had already logged in at the console and using the Dell Remote Console SWitch - I was able to restart the machine . This morning , however , it froze up again , it would not let me log in as root on the console - and , no matter how many times I bring it up and down @ @ @ @ @ @ @ @ @ @ in single user looking at the mysql databases . FTR , under distributions , it 's ROCKS enterprise/3 , running 2.4.21-15 . ELsmp . I did find an error in the messages file regarding my last attempt at logging in : daphne login(pamunix)1984 : authentication failure ; logname=LOGIN uid=0 euid=0 tty=tty4 ruser= rhost= user=root ( This was , of course , after I tried changing the root password to see if that was the problem . I was thinking like a Solaris person and just ran the Unix passwd command . I never signed a certificate , so I imagine my password change was meaningless ? ) . I 'm also seeing auditd1215 : output error ; suspending execution daphne insmodd : Hint : insmod errors can be caused by incorrect ... and most ominously : myproxy-server : &lt;19874&gt; Failure : error in myproxysend Any hints on how to recover this machine will be appreciated . Paul Mitchell LONG ... Paul Mitchell email : pmitchel at email.unc.edu phone : ( 919 ) 962-9778 office : I have an office , room 14 , Phillips Hall LONG ... From philip.papadopoulos at @ @ @ @ @ @ @ @ @ @ gmail.com ( Philip Papadopoulos ) Date : Tue , 5 Sep 2006 09:42:51 -0700 Subject : Rocks-Discuss cpuspeed ? In-Reply-To : LONG ... References : LONG ... Message-ID : LONG ... Greg , The kernel is the latest errata kernel from CentOS/RedHat at the time of release ( we are working on a maintenance release of 4.2 because of some issues in 4.2 and a large number of changes in CentOS , Globus , Anaconda , and other goodies that arrived a couple of weeks after our release ) . If cpuspeed is n't working in the supplied kernel , then it does n't work for anybody who is using this errata kernel ( version .34-2 , if I recall correctly ) . We do not provide our own custom-built kernel. -P On 9/5/06 , Greg Lindahl wrote : &gt; &gt; In Rocks / CentOS 4.2 , is cpuspeed supposed to work ? It seems &gt; to be disabled in the kernel , whereas RHEL4U2 has it built in. &gt; &gt; -- greg &gt; &gt; -- Philip Papadopoulos , PhD University of California , San Diego 858-822-3628 -------------- next part @ @ @ @ @ @ @ @ @ @ ... From tim.carlson at pnl.gov Tue Sep 5 10:16:15 2006 From : tim.carlson at pnl.gov ( Tim Carlson ) Date : Tue , 5 Sep 2006 10:16:15 -0700 ( PDT ) Subject : Rocks-Discuss Locked out of head node In-Reply-To : References : Message-ID : On Tue , 5 Sep 2006 , Paul Mitchell wrote : I 'm guessing you have a full / partition and have been bitten by the auditd feature/bug . I would 1 ) clean up the audit files cd /var/log/audit.d rm -f * 2 ) turn audit off chkconfig audit off 3 ) reboot Tim Tim Carlson Voice : ( 509 ) 376 3423 Email : Tim.Carlson at pnl.gov Pacific Northwest National Laboratory HPCaNS : High Performance Computing and Networking Services &gt; Folks , &gt; I 'm running a bioportal at UNC - or at least I was until this morning ! ) &gt; &gt; Over the last month or two , one of the head nodes has frozen up a couple &gt; of times- wo n't allow ssh/httpd connections . Luckily , the last time , I had &gt; already logged in at the @ @ @ @ @ @ @ @ @ @ I was able to restart the machine . This morning , however , it froze up &gt; again , it would not let me log in as root on the console - and , no matter &gt; how many times I bring it up and down , it never allows anyone in. &gt; &gt; I 'm currently in single user looking at the mysql databases . FTR , under &gt; distributions , it 's ROCKS enterprise/3 , running 2.4.21-15 . ELsmp. &gt; &gt; I did find an error in the messages file regarding my last attempt at &gt; logging in : &gt; &gt; daphne login(pamunix)1984 : authentication failure ; logname=LOGIN uid=0 &gt; euid=0 tty=tty4 ruser= rhost= user=root &gt; &gt; ( This was , of course , after I tried changing the root password to see if &gt; that was the problem . I was thinking like a Solaris person and just ran &gt; the Unix passwd command . I never signed a certificate , so I imagine my &gt; password change was meaningless ? ) . &gt; &gt; I 'm also seeing auditd1215 : output error ; suspending execution &gt; @ @ @ @ @ @ @ @ @ @ by incorrect ... &gt; &gt; and most ominously : &gt; &gt; myproxy-server : &lt;19874&gt; Failure : error in myproxysend &gt; &gt; Any hints on how to recover this machine will be appreciated . &gt; &gt; Paul Mitchell &gt; LONG ... &gt; Paul Mitchell &gt; email : pmitchel at email.unc.edu &gt; phone : ( 919 ) 962-9778 &gt; office : I have an office , room 14 , Phillips Hall &gt; LONG ... &gt; &gt; From pmitchel at email.unc.edu Tue Sep 5 10:46:51 2006 From : pmitchel at email.unc.edu ( Paul Mitchell ) Date : Tue , 5 Sep 2006 13:46:51 -0400 ( EDT ) Subject : Rocks-Discuss Locked out of head node In-Reply-To : References : Message-ID : On Tue , 5 Sep 2006 , Tim Carlson wrote : &gt; On Tue , 5 Sep 2006 , Paul Mitchell wrote : &gt; &gt; I 'm guessing you have a full / partition and have been bitten by the auditd &gt; feature/bug . I would Very likely . I booted into single user mode , started httpd , networks and sshd and was able to log in ! / , however @ @ @ @ @ @ @ @ @ @ clean up the audit files &gt; &gt; cd /var/log/audit.d &gt; rm -f * I saved them to another computer , FWIW &gt; 2 ) turn audit off DONE &gt; chkconfig audit off DONE &gt; 3 ) reboot The machine came up - root was at 89% full , and I was able to log in at the console ! Thanks Tim , Paul Mitchell LONG ... Paul Mitchell email : pmitchel at email.unc.edu phone : ( 919 ) 962-9778 office : I have an office , room 14 , Phillips Hall LONG ... From pmitchel at email.unc.edu Tue Sep 5 11:19:48 2006 From : pmitchel at email.unc.edu ( Paul Mitchell ) Date : Tue , 5 Sep 2006 14:19:48 -0400 ( EDT ) Subject : Rocks-Discuss cross kickstart problems In-Reply-To : References : LONG ... Message-ID : On Fri , 1 Sep 2006 , Greg Bruno wrote : &gt; i guess i 'd start by looking at the httpd logs and &gt; &gt; # tail -f /var/log/httpd/* &gt; &gt; then PXE boot a node . as the node makes progress , you should see &gt; status messages from the above @ @ @ @ @ @ @ @ @ @ will point you in the direction of the root cause . Hello Greg , I restarted my httpd at debug level , and got the following : 192.168.33.253 - - 05/Sep/2006:14:08:28 -0400 " GET LONG ... HTTP/1.0 " 404 344 " - " " Wget/1.10.2 ( Red Hat modified ) " 192.168.33.253 - - 05/Sep/2006:14:08:28 -0400 " GET LONG ... HTTP/1.0 " 404 336 " - " " Wget/1.10.2 ( Red Hat modified ) " 192.168.33.253 - - 05/Sep/2006:14:08:28 -0400 " GET LONG ... HTTP/1.0 " 404 339 " - " " Wget/1.10.2 ( Red Hat modified ) " . . . 192.168.33.253 - - 05/Sep/2006:14:09:14 -0400 " GET LONG ... HTTP/1.0 " 404 332 " - " " Wget/1.10.2 ( Red Hat modified ) " 192.168.33.253 - - 05/Sep/2006:14:09:14 -0400 " GET LONG ... HTTP/1.0 " 404 324 " - " " Wget/1.10.2 ( Red Hat modified ) " Tue Sep 05 14:09:09 2006 error client 192.168.33.253 File does not exist : LONG ... Tue Sep 05 14:09:14 2006 error client 192.168.33.253 File does not exist : LONG ... SO it would seem it is unhappy with the torrent @ @ @ @ @ @ @ @ @ @ Hill , Paul LONG ... Paul Mitchell email : pmitchel at email.unc.edu phone : ( 919 ) 962-9778 office : I have an office , room 14 , Phillips Hall LONG ... From fernando.nino at free.fr Tue Sep 5 12:38:23 2006 From : fernando.nino at free.fr ( Fernando NINO ) Date : Tue , 5 Sep 2006 21:38:23 +0200 Subject : Rocks-Discuss Problem booting rocks 4.1 on new Opterons dual-core Message-ID : LONG ... hello , I have successfully installed and used for some months the 4.1 rocks distribution and just decided to add new compute nodes . The new hardware has an Adaptec 2420 RAID controller ; network installation goes through with success , but nodes never manage to boot . On debugging I noticed installation correctly loaded the " ' aacraid " driver needed to detect the two hard disks on the machine , and had no problems . On booting , the new hardware manages to use GRUB ( splash screen , usual kernel selection etc ... ) , but never gets to boot , just says loading initrd and then halts . My guess is that @ @ @ @ @ @ @ @ @ @ the sort .... If I install a standard ScientificLinux distribution on it ( another RHEL4 based one ) , the machine boots up fine . Any ideas ? Thanks , Fernando From mgreaney at fnal.gov Tue Sep 5 15:00:29 2006 From : mgreaney at fnal.gov ( MargaretGreaney ) Date : Tue , 05 Sep 2006 17:00:29 -0500 ( CDT ) Subject : Rocks-Discuss rocks 4.1 appliance partitioning In-Reply-To : LONG ... References : LONG ... Message-ID : On Fri , 1 Sep 2006 , mason j. katz wrote : &gt; Using " replace-auto-partition " explicitly is your graph is n't &gt; correct . The replace-* and extend-* node files are a simple hack to &gt; allow you to add/modify Rocks without creating a new graph file . I &gt; think what you really want here is : &gt; &gt; &gt; auto-partiton &gt; compute &gt; &gt; &gt; This will automatically use the **26;7543;TOOLONG file if it &gt; exists , otherwise will use auto-partition.xml. &gt; &gt; Also the ordering of **29;7571;TOOLONG should probably be &gt; explicit . The following only says that they happen last , between the &gt; two of them @ @ @ @ @ @ @ @ @ @ &gt; farm-partition &gt; If I use the set up above , I get a kickstart test output that still has the compute partitioning info included in it , dbpartinfo = ' hdb ' : ( ' hdb1 ' , ' 63 ' , ' 310857687 ' , ' 83 ' , ' ' , ' bootable ' , ' ' , ' ' ) , ( ' hdb2 ' , ' 310857750 ' , ' 29896965 ' , ' 83 ' , ' ' , ' ' , ' ' , ' ' ) , ( ' hdb3 ' , ' 340754715 ' , ' 29896965 ' , ' 83 ' , ' ' , ' ' , ' ' , ' ' ) , ( ' hdb4 ' , ' 370651680 ' , ' 119571795 ' , ' f ' , ' ' , ' ' , ' ' , ' ' ) , ( ' hdb5 ' , ' 370651743 ' , ' 29880837 ' , ' 83 ' , ' ' , ' ' , ' ' , ' ' ) , ( ' hdb6 ' , @ @ @ @ @ @ @ @ @ @ ' , ' ' , ' ' , ' ' , ' ' ) , ( ' hdb7 ' , ' 430429608 ' , ' 29896902 ' , ' 83 ' , ' ' , ' ' , ' ' , ' ' ) , ( ' hdb8 ' , ' 460326573 ' , ' 29896902 ' , ' 83 ' , ' ' , ' ' , ' ' , ' ' ) , ' hda ' : ( ' hda1 ' , ' 63 ' , ' 10490382 ' , ' 83 ' , ' ext3 ' , ' bootable ' , ' ' , ' / ' ) , ( ' hda2 ' , ' 10490445 ' , ' 140584815 ' , ' 83 ' , ' ext3 ' , ' ' , ' ' , ' /rest ' ) , ( ' hda3 ' , ' 151075260 ' , ' 4192965 ' , ' 82 ' , ' swap ' , ' ' , ' ' , ' swap ' ) , ( ' hda4 ' , ' 155268225 ' , ' 1028160 ' , ' @ @ @ @ @ @ @ @ @ @ ' , ' ' ) , ( ' hda5 ' , ' 155268288 ' , ' 1028097 ' , ' 83 ' , ' ext3 ' , ' ' , ' ' , ' /usr/vice/cache ' ) and then additionally my partition scheme : / --size 11000 --ondisk hda --grow/local/stage1 --size 4096 --ondisk hdb --grow/local/stage2 --size 4096 --ondisk hdb --growswap --size 4096 --ondisk hda There is a **26;7602;TOOLONG in the nodes directory , but it is being added instead of replaced and then on an install the node fails where it begins to do the partitioning work . Is this because the 2nd half is still not being ordered correctly ? Do I need to remove the auto-partition line in the last section ? **28;7630;TOOLONG I think I actually do need a new graph because I 'll need several appliances for different partitioning schemes and one **26;7660;TOOLONG wo n't be able to support this , correct ? There is an entry in the table named partitions . Do I need to remove this before reinstalling ? I 've reviewed the slides from the meeting presentations and find that for custom @ @ @ @ @ @ @ @ @ @ specific examples . But what I need is the ability to have about 7 different partitioning schemes . Do you have any slides or examples of how the appliance can be set up besides the " my-compute.xml " that is in the users guide ? That example worked fine by the way but it does n't help me with a replacement type of appliance . thank you for your help . margaret &gt; &gt; &gt; &gt; &gt; -mjk &gt; &gt; &gt; On Sep 1 , 2006 , at 03:07 P , MargaretGreaney wrote : &gt; &gt; &gt; Hi , &gt; &gt; &gt; &gt; I 've been reviewing what people on the list do to get a different &gt; &gt; partition scheme &gt; &gt; than what compute appliance provides . We have a partition method , &gt; &gt; but I &gt; &gt; want to try to use a method closer to the rocks method . &gt; &gt; &gt; &gt; I would like to have an appliance that has different partitioning , &gt; &gt; not an extension . &gt; &gt; I do n't want to replace the compute appliance with a different @ @ @ @ @ @ @ @ @ @ new appliance , farmwn-app , to have a different partition &gt; &gt; scheme . &gt; &gt; &gt; &gt; I know the different partition scheme can be done with replace-auto- &gt; &gt; partition.xml with &gt; &gt; my partition information . But how do I enter it into the farmwn- &gt; &gt; app.xml ? and how &gt; &gt; does it know to not use the auto-partition.xml ? &gt; &gt; &gt; &gt; here is what is in my farmwn-app.xml , located &gt; &gt; &gt; &gt; &gt; replace-auto-partiton &gt; &gt; compute &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; replace-auto-partition ( this is just a skeleton ) &gt; &gt; farm-partition ( this has part info ) &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; --------- &gt; &gt; the farm-partition.xml is in the nodes directory . When I try to &gt; &gt; install the node , it gets &gt; &gt; stuck on auto-partition.xml which I can see by entering alt f3 on &gt; &gt; @ @ @ @ @ @ @ @ @ @ rebooting this node , which did &gt; &gt; have a regular compute &gt; &gt; install earlier . &gt; &gt; &gt; &gt; Do I need to change any other xml files ? &gt; &gt; &gt; &gt; I know to do rocks-dist dist when adding new xmls or changing the &gt; &gt; graph . &gt; &gt; &gt; &gt; thanks , &gt; &gt; &gt; &gt; Margaret &gt; &gt; &gt; &gt; -- &gt; &gt; Margaret Greaney Telephone : 630-840-4623 &gt; &gt; Fermilab E-mail : mgreaney at fnal.gov &gt; &gt; CD/CSS/FCS &gt; &gt; From greg.bruno at gmail.com Tue Sep 5 15:55:58 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : Tue , 5 Sep 2006 15:55:58 -0700 Subject : Rocks-Discuss Problem booting rocks 4.1 on new Opterons dual-core In-Reply-To : LONG ... References : LONG ... Message-ID : On 9/5/06 , Fernando NINO wrote : &gt; hello , &gt; &gt; I have successfully installed and used for some months the 4.1 &gt; rocks distribution and just decided to add new compute nodes . The &gt; new hardware has an Adaptec 2420 RAID controller ; network &gt; installation goes through with success , @ @ @ @ @ @ @ @ @ @ I noticed installation correctly loaded the &gt; " ' aacraid " driver needed to detect the two hard disks on the machine , &gt; and had no problems . On booting , the new hardware manages to use &gt; GRUB ( splash screen , usual kernel selection etc ... ) , but never gets &gt; to boot , just says loading initrd and then halts . My guess is that &gt; the installed initrd.img lacks the aacraid driver or something of the &gt; sort .... If I install a standard ScientificLinux distribution on it &gt; ( another RHEL4 based one ) , the machine boots up fine . it may be a grub installation issue . you can try the following procedure on your compute node ( the procedure below discusses how to run it on a frontend , but just substitute the word ' compute ' for ' frontend ' ) . LONG ... let us know how it goes . - gb From federicosacerdoti at gmail.com Tue Sep 5 16:27:05 2006 From : federicosacerdoti at gmail.com ( Federico D. Sacerdoti ) Date : Tue , 5 Sep 2006 @ @ @ @ @ @ @ @ @ @ LONG ... A report on large nas devices . We just successfully built a 16-drive NAS appliance with Rocks 4.1 . This machine has 6TB of usable storage in 3U using 16 500GB drives . Two drives are kept for hot spares . With the raid-all code , all drives are SW Raid5 'd then LVM 'd to a logical volume . The OS and swap install on raid5 partitions of all drives - an earlier design we had called for seperate OS drives but this configuration is simpler and easier . A separate /boot partition is made for Grub with raid1 . With this many drives , automatic partitioning with raid becomes essential . The install is a bit hairy however , certain timeouts such as the watchdog are concerning . The " Searching for Kickstart keys " takes too long with 16 drives , as each of the 64 partitions are mounted and checked for keys . Not sure what the right answer is here . The enablefilesystems step also takes a long time , but is less worrying since the install watchdog has been stopped . We @ @ @ @ @ @ @ @ @ @ after the manufacturer assured us the brand new machine had been " burned in " . Of course there is no test like reality . The machine is up and appears healthy so far , passing a bonnie++ test over NFS . Linux 's mdadm seems fine with 16 drives on a single device , although we have yet to fully stress test the box . Ext3 filesystem is currently sized at 2.9TB using 50% available LVM space . -Federico -- D. E. Shaw Research LLC New York NY From jgs4466 at ksu.edu Tue Sep 5 17:43:00 2006 From : jgs4466 at ksu.edu ( Jeffrey Smith ) Date : Tue , 5 Sep 2006 19:43:00 -0500 Subject : Rocks-Discuss Node install problem Message-ID : LONG ... when i try to install the node for 4.2 rocks ... It installs runs the post script restarts and runs the install process over again . WHat do I need to do ? -- Kansas State University IDEAS lab Ward 3 321-591-2254 From jpummil at uark.edu Tue Sep 5 18:06:11 2006 From : jpummil at uark.edu ( Jeff Pummill ) Date : Tue , @ @ @ @ @ @ @ @ @ @ problem In-Reply-To : LONG ... References : LONG ... Message-ID : **31;7717;TOOLONG Check the obvious first ... Is the boot order in the BIOS set to harddrive first and netboot second ? -Jeff Pummill Jeffrey Smith wrote : &gt; when i try to install the node for 4.2 rocks ... It installs runs the &gt; post script restarts and runs the install process over again . WHat do I &gt; need to do ? &gt; &gt; -- &gt; Kansas State University &gt; IDEAS lab Ward 3 &gt; 321-591-2254 &gt; From debbiet at arlut.utexas.edu Tue Sep 5 18:14:21 2006 From : debbiet at arlut.utexas.edu ( Debbie Tropiano ) Date : Tue , 5 Sep 2006 20:14:21 -0500 Subject : Rocks-Discuss Dualing home directories In-Reply-To : &lt;20060823231347 . **26;7750;TOOLONG References : LONG ... &lt;20060823231347 . **26;7778;TOOLONG Message-ID : LONG ... Greg - This is what we wanted , so I 've followed your solution ( and also really hope that it 's an acceptable one : - ) . However I 've got a few questions : Do you have your frontend hostname in the ypservers file on your NIS master ? @ @ @ @ @ @ @ @ @ @ changes to allow the master to push the maps without errors ? I keep getting RPC portmapper errors when my NIS master pushes the maps . Or are those just ignorable ? You say that you use your extend-compute.xml file to set the /etc/auto.master file on the compute nodes to be different than what 's on the frontend . Does n't the 411 automatic updates over- write that file later ? Or do you have that turned off ? Thanks , Debbie On Thu , Aug 24 , 2006 at 01:13:47AM +0200 , Greg Byshenk wrote : &gt; There is another way to do this , and I describe it largely to see if there &gt; are any comments ( I suspect that it is not officially endorsed ) . &gt; &gt; For our cluster , we wanted to give users the same environment on the cluster &gt; as on the rest of the machines ( including workstations and servers that are &gt; not part of the cluster ) ; not just home directories , but nfs mounted software , &gt; etc . Of course , we already have a @ @ @ @ @ @ @ @ @ @ what we did was : set the NIS domain on the cluster to our common NIS &gt; domain , while making the frontend a slave NIS server to our master ( outside &gt; the cluster ) . Then , in extend-compute , modify the auto.master to load the &gt; same NIS maps as are used in the rest of our domain ( with /home/install &gt; added to our domain auto.home map ) . Similarly , password is loaded from &gt; our own domain . &gt; &gt; This may be a bit odd , but it allows for the same environment on all &gt; machines ( the workstations and non-cluster servers all run CentOS4.3 , just &gt; as does the cluster ) , which is useful for us , as most of what is run on the &gt; cluster is custom code , and this allows for the code to be tested by the &gt; users on their workstations , and then moved to the cluster for computation &gt; when it does what they want . -- Debbie Tropiano debbiet at arlut.utexas.edu Environmental Sciences Laboratory +1 512 835 3367 w Applied @ @ @ @ @ @ @ @ @ @ P.O. Box 8029 , Austin , TX 78713-8029 home email : debbie at icus.com From debbiet at arlut.utexas.edu Tue Sep 5 18:20:00 2006 From : debbiet at arlut.utexas.edu ( Debbie Tropiano ) Date : Tue , 5 Sep 2006 20:20:00 -0500 Subject : Rocks-Discuss Iptables hangs at shutdown when shooting nodes Message-ID : LONG ... Hello - I 've been having problems lately when shooting my compute nodes that iptables hangs at shutdown ( " Stopping iptables " ) after the kickstart . Is this a known problem ? And more importantly is there a work around ? Today it happened on 6 of 10 nodes and power-cycling is the only fix . Thanks in advance for any help , Debbie -- Debbie Tropiano debbiet at arlut.utexas.edu Environmental Sciences Laboratory +1 512 835 3367 w Applied Research Laboratories of UT Austin +1 512 835 3544 fax P.O. Box 8029 , Austin , TX 78713-8029 home email : debbie at icus.com From jgs4466 at ksu.edu Tue Sep 5 18:33:41 2006 From : jgs4466 at ksu.edu ( Jeffrey Smith ) Date : Tue , 5 Sep 2006 20:33:41 -0500 Subject : Rocks-Discuss @ @ @ @ @ @ @ @ @ @ **31;7839;TOOLONG Message-ID : LONG ... When I boot from the HD or net I get a grub error Quoting Jeff Pummill : &gt; Check the obvious first ... &gt; &gt; Is the boot order in the BIOS set to harddrive first and netboot &gt; second ? &gt; &gt; -Jeff Pummill &gt; &gt; &gt; Jeffrey Smith wrote : &gt; &gt; when i try to install the node for 4.2 rocks ... It installs runs &gt; the &gt; &gt; post script restarts and runs the install process over again . WHat &gt; do I &gt; &gt; need to do ? &gt; &gt; &gt; &gt; -- &gt; &gt; Kansas State University &gt; &gt; IDEAS lab Ward 3 &gt; &gt; 321-591-2254 &gt; &gt; &gt; &gt; -- Kansas State University IDEAS lab Ward 3 321-591-2254 From jpummil at uark.edu Tue Sep 5 19:14:27 2006 From : jpummil at uark.edu ( Jeff Pummill ) Date : Tue , 05 Sep 2006 21:14:27 -0500 Subject : Rocks-Discuss Node install problem In-Reply-To : LONG ... References : LONG ... **31;7872;TOOLONG LONG ... Message-ID : **33;7905;TOOLONG So ... it seems to complete the install the first time around , @ @ @ @ @ @ @ @ @ @ error with grub ? What exactly does the error say ? Are you doing a software raid on this device ? Have you had other versions of Rocks install and boot successfully ? -Jeff Pummill Jeffrey Smith wrote : &gt; When I boot from the HD or net I get a grub error &gt; &gt; Quoting Jeff Pummill : &gt; &gt; &gt;&gt; Check the obvious first ... &gt;&gt; &gt;&gt; Is the boot order in the BIOS set to harddrive first and netboot &gt;&gt; second ? &gt;&gt; &gt;&gt; -Jeff Pummill &gt;&gt; &gt;&gt; &gt;&gt; Jeffrey Smith wrote : &gt;&gt; &gt;&gt;&gt; when i try to install the node for 4.2 rocks ... It installs runs &gt;&gt;&gt; &gt;&gt; the &gt;&gt; &gt;&gt;&gt; post script restarts and runs the install process over again . WHat &gt;&gt;&gt; &gt;&gt; do I &gt;&gt; &gt;&gt;&gt; need to do ? &gt;&gt;&gt; &gt;&gt;&gt; -- &gt;&gt;&gt; Kansas State University &gt;&gt;&gt; IDEAS lab Ward 3 &gt;&gt;&gt; 321-591-2254 &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt; &gt; &gt; &gt; -- &gt; Kansas State University &gt; IDEAS lab Ward 3 &gt; 321-591-2254 &gt; -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From jgs4466 at ksu.edu @ @ @ @ @ @ @ @ @ @ ( Jeffrey Smith ) Date : Tue , 5 Sep 2006 21:39:25 -0500 Subject : Rocks-Discuss Node install problem In-Reply-To : **33;7940;TOOLONG References : LONG ... **31;7975;TOOLONG LONG ... **33;8008;TOOLONG Message-ID : LONG ... It boots and varifies the data pool , then Grub Read error occurs . N this is the first attemt to start running rocks .... I 'm kinda new at this . Quoting Jeff Pummill : &gt; So ... it seems to complete the install the first time around , but &gt; never &gt; appears to want to boot due to an error with grub ? &gt; &gt; What exactly does the error say ? &gt; &gt; Are you doing a software raid on this device ? &gt; &gt; Have you had other versions of Rocks install and boot successfully ? &gt; &gt; -Jeff Pummill &gt; &gt; Jeffrey Smith wrote : &gt; &gt; When I boot from the HD or net I get a grub error &gt; &gt; &gt; &gt; Quoting Jeff Pummill : &gt; &gt; &gt; &gt; &gt; &gt;&gt; Check the obvious first ... &gt; &gt;&gt; &gt; &gt;&gt; Is the boot order in the BIOS set @ @ @ @ @ @ @ @ @ @ &gt;&gt; &gt; &gt;&gt; -Jeff Pummill &gt; &gt;&gt; &gt; &gt;&gt; &gt; &gt;&gt; Jeffrey Smith wrote : &gt; &gt;&gt; &gt; &gt;&gt;&gt; when i try to install the node for 4.2 rocks ... It installs runs &gt; &gt;&gt;&gt; &gt; &gt;&gt; the &gt; &gt;&gt; &gt; &gt;&gt;&gt; post script restarts and runs the install process over again . &gt; WHat &gt; &gt;&gt;&gt; &gt; &gt;&gt; do I &gt; &gt;&gt; &gt; &gt;&gt;&gt; need to do ? &gt; &gt;&gt;&gt; &gt; &gt;&gt;&gt; -- &gt; &gt;&gt;&gt; Kansas State University &gt; &gt;&gt;&gt; IDEAS lab Ward 3 &gt; &gt;&gt;&gt; 321-591-2254 &gt; &gt;&gt;&gt; &gt; &gt;&gt;&gt; &gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt; &gt; -- &gt; &gt; Kansas State University &gt; &gt; IDEAS lab Ward 3 &gt; &gt; 321-591-2254 &gt; &gt; &gt; -- Kansas State University IDEAS lab Ward 3 321-591-2254 From jpummil at uark.edu Tue Sep 5 19:49:41 2006 From : jpummil at uark.edu ( Jeff Pummill ) Date : Tue , 05 Sep 2006 21:49:41 -0500 Subject : Rocks-Discuss Node install problem In-Reply-To : LONG ... References : LONG ... **31;8043;TOOLONG LONG ... **33;8076;TOOLONG LONG ... Message-ID : **33;8111;TOOLONG Hmmm ... Can you give us a bit more info regarding hardware , @ @ @ @ @ @ @ @ @ @ Did you take the default partition Rocks offered ? Adequate RAM ? -J . Pummill Jeffrey Smith wrote : &gt; It boots and varifies the data pool , then Grub Read error occurs . N &gt; this is the first attemt to start running rocks .... I 'm kinda new at &gt; this . &gt; &gt; Quoting Jeff Pummill : &gt; &gt; &gt;&gt; So ... it seems to complete the install the first time around , but &gt;&gt; never &gt;&gt; appears to want to boot due to an error with grub ? &gt;&gt; &gt;&gt; What exactly does the error say ? &gt;&gt; &gt;&gt; Are you doing a software raid on this device ? &gt;&gt; &gt;&gt; Have you had other versions of Rocks install and boot successfully ? &gt;&gt; &gt;&gt; -Jeff Pummill &gt;&gt; &gt;&gt; Jeffrey Smith wrote : &gt;&gt; &gt;&gt;&gt; When I boot from the HD or net I get a grub error &gt;&gt;&gt; &gt;&gt;&gt; Quoting Jeff Pummill : &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt;&gt; Check the obvious first ... &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; Is the boot order in the BIOS set to harddrive first and netboot &gt;&gt;&gt;&gt; second ? &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; -Jeff Pummill &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; @ @ @ @ @ @ @ @ @ @ try to install the node for 4.2 rocks ... It installs runs &gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; the &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt;&gt; post script restarts and runs the install process over again . &gt;&gt;&gt;&gt;&gt; &gt;&gt; WHat &gt;&gt; &gt;&gt;&gt;&gt; do I &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt;&gt; need to do ? &gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt;&gt; -- &gt;&gt;&gt;&gt;&gt; Kansas State University &gt;&gt;&gt;&gt;&gt; IDEAS lab Ward 3 &gt;&gt;&gt;&gt;&gt; 321-591-2254 &gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt; -- &gt;&gt;&gt; Kansas State University &gt;&gt;&gt; IDEAS lab Ward 3 &gt;&gt;&gt; 321-591-2254 &gt;&gt;&gt; &gt;&gt;&gt; &gt; &gt; &gt; -- &gt; Kansas State University &gt; IDEAS lab Ward 3 &gt; 321-591-2254 &gt; -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From greg.bruno at gmail.com Tue Sep 5 19:59:31 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : Tue , 5 Sep 2006 19:59:31 -0700 Subject : Rocks-Discuss rocks 4.1 appliance partitioning In-Reply-To : References : LONG ... Message-ID : On 9/5/06 , MargaretGreaney wrote : &gt; &gt; On Fri , 1 Sep 2006 , mason j. katz wrote : &gt; &gt; &gt; Using " replace-auto-partition " explicitly is your graph is n't &gt; &gt; correct . The replace-* and extend-* node @ @ @ @ @ @ @ @ @ @ to add/modify Rocks without creating a new graph file . I &gt; &gt; think what you really want here is : &gt; &gt; &gt; &gt; &gt; &gt; auto-partiton &gt; &gt; compute &gt; &gt; &gt; &gt; &gt; &gt; This will automatically use the **26;8146;TOOLONG file if it &gt; &gt; exists , otherwise will use auto-partition.xml. &gt; &gt; &gt; &gt; Also the ordering of **29;8174;TOOLONG should probably be &gt; &gt; explicit . The following only says that they happen last , between the &gt; &gt; two of them the order is not specified . &gt; &gt; &gt; &gt; &gt; &gt; auto-partition &gt; &gt; farm-partition &gt; &gt; &gt; &gt; If I use the set up above , I get a kickstart test output that still has &gt; the compute partitioning info included in it , &gt; dbpartinfo = ' hdb ' : ( ' hdb1 ' , ' 63 ' , ' 310857687 ' , ' 83 ' , ' ' , ' bootable ' , &gt; ' ' , ' ' ) , ( ' hdb2 ' , ' 310857750 ' , ' 29896965 ' , ' 83 ' , ' ' @ @ @ @ @ @ @ @ @ @ , ( ' hdb3 ' , &gt; ' 340754715 ' , ' 29896965 ' , ' 83 ' , ' ' , ' ' , ' ' , ' ' ) , ( ' hdb4 ' , ' 370651680 ' , &gt; ' 119571795 ' , ' f ' , ' ' , ' ' , ' ' , ' ' ) , ( ' hdb5 ' , ' 370651743 ' , ' 29880837 ' , ' 83 ' , &gt; ' ' , ' ' , ' ' , ' ' ) , ( ' hdb6 ' , ' 400532643 ' , ' 29896902 ' , ' 83 ' , ' ' , ' ' , ' ' , ' ' ) , &gt; ( ' hdb7 ' , ' 430429608 ' , ' 29896902 ' , ' 83 ' , ' ' , ' ' , ' ' , ' ' ) , ( ' hdb8 ' , &gt; ' 460326573 ' , ' 29896902 ' , ' 83 ' , ' ' , ' ' , ' ' , ' ' ) , ' hda ' : @ @ @ @ @ @ @ @ @ @ ' 10490382 ' , ' 83 ' , ' ext3 ' , ' bootable ' , ' ' , ' / ' ) , ( ' hda2 ' , ' 10490445 ' , &gt; ' 140584815 ' , ' 83 ' , ' ext3 ' , ' ' , ' ' , ' /rest ' ) , ( ' hda3 ' , ' 151075260 ' , &gt; ' 4192965 ' , ' 82 ' , ' swap ' , ' ' , ' ' , ' swap ' ) , ( ' hda4 ' , ' 155268225 ' , ' 1028160 ' , &gt; ' f ' , ' ' , ' ' , ' ' , ' ' ) , ( ' hda5 ' , ' 155268288 ' , ' 1028097 ' , ' 83 ' , ' ext3 ' , ' ' , &gt; ' ' , ' /usr/vice/cache ' ) when you want to change a node 's partitioning , you need to remove the previous partition info from the database . for details , see ' 5.5.3 . Customizing Compute Node Disk Partitions ' in : @ @ @ @ @ @ @ @ @ @ &gt; / --size &gt; 11000 --ondisk hda --grow &gt; LONG ... --size 4096 &gt; --ondisk hdb --grow &gt; LONG ... --size 4096 &gt; --ondisk hdb --grow &gt; swap --size &gt; 4096 --ondisk hda &gt; &gt; &gt; There is a **26;8205;TOOLONG in the nodes directory , but it is &gt; being added instead of replaced and then on an install the node fails &gt; where it begins to do the partitioning work . &gt; &gt; Is this because the 2nd half is still not being ordered correctly ? Do I &gt; need to remove the auto-partition line in the last section ? &gt; &gt; auto-partition &gt; farm-partition &gt; &gt; &gt; &gt; I think I actually do need a new graph because I 'll need several &gt; appliances for different partitioning schemes and one &gt; **26;8233;TOOLONG wo n't be able to support this , correct ? that is correct -- you 'll need individual partitioning files for each appliance type . or said another way , for each specific partitioning scheme , you 'll need to create a new appliance type and then specify a partitioning node for each . &gt; There @ @ @ @ @ @ @ @ @ @ I need to remove this &gt; before reinstalling ? yes . see the above link for details . &gt; I 've reviewed the slides from the meeting presentations and find &gt; that for custom partitioning , the use of extend-partition.xml is &gt; mentioned in two specific examples . But what I need is the ability &gt; to have about 7 different partitioning schemes . again , as stated above , you 'll need to create 7 appliances and 7 partitioning files . &gt; Do you have any slides or examples of how the appliance can be &gt; set up besides the " my-compute.xml " that is in the users guide ? That &gt; example worked fine by the way but it does n't help me with a replacement &gt; type of appliance . just replace ' my-compute ' and ' My Compute ' with your appliance names . - gb From greg.bruno at gmail.com Tue Sep 5 20:07:31 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : Tue , 5 Sep 2006 20:07:31 -0700 Subject : Rocks-Discuss Node install problem In-Reply-To : LONG ... References : LONG @ @ @ @ @ @ @ @ @ @ 9/5/06 , Jeffrey Smith wrote : &gt; It boots and varifies the data pool , then Grub Read error occurs . N &gt; this is the first attemt to start running rocks .... I 'm kinda new at &gt; this . try : LONG ... - gb From mashaojie at nuaa.edu.cn Tue Sep 5 23:07:52 2006 From : mashaojie at nuaa.edu.cn ( Shaojie Ma ) Date : We 'd , 6 Sep 2006 14:07:52 +0800 Subject : Rocks-Discuss mpirun -nolocal error when add ip for myrinet Message-ID : **43;8329;TOOLONG Dear Sir : I want to use the second network to calculation . So I need to modify the hostname compute-0-0 into compute-0-0-myri0 in machinefile . In my opinion , I should add -nolocal option to excute mpirun , as following : /opt/mpich/gnu/mpirun -nolocal -np 8 -machinefile mymachinefile myprogram Howerver , an error occurs : connect to address 192.168.1.251 : Connection refused connect to address 192.168.1.251 : Connection refused trying normal rsh ( /usr/bin/rsh ) p08864 : p4error : Child process exited while making connection to remote process on compute-0-3-myri0 : 0 compute-0-3-myri0 : Connection refused As we known , ssh is used @ @ @ @ @ @ @ @ @ @ ssh 192.168.1.251 or ssh compute-0-3-myri0 . It is OK . Please tell me how to solve the problem . Best Regards LONG ... Shaojie Ma Institute of Nano Science Nanjing University of Aeronautics and Astronautics mashaojie at nuaa.edu.cn Nanjing 210016 , China LONG ... From gbartsch at casl.umd.edu Wed Sep 6 06:59:25 2006 From : gbartsch at casl.umd.edu ( Gerhard Bartsch ) Date : We 'd , 6 Sep 2006 09:59:25 -0400 Subject : Rocks-Discuss Changing Home Directories on Frontend Message-ID : LONG ... Good Morning , I 've been playing around with ROCKS 4.2 to build a prototype cluster . For various reasons I 'd like to use a SAN for the users home directories on the frontend . How does one go about re-directing the user 's home directories under ROCKS 4.2 to a different server or SAN ? Gerhard Gerhard Bartsch University of Maryland -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From JACOBLIBERMAN at Dell.com Wed Sep 6 07:01:01 2006 From : JACOBLIBERMAN at Dell.com ( JACOBLIBERMAN at Dell.com ) Date : We 'd , 6 Sep 2006 09:01:01 -0500 Subject : @ @ @ @ @ @ @ @ @ @ : **43;8374;TOOLONG Message-ID : Shaojie , Does the command work when you omit the -nolocal option ? Also , what version of rocks are you running ? Section 5.6 of the rocks 4.1.1 userguide describes how to enable rsh on compute nodes . I would try turning off iptables temporarily as a troubleshooting step . ( iptables -F ) If that works you can adjust your rules accordingly . Thanks , jacob &gt; -----Original Message----- &gt; From : **30;8419;TOOLONG at sdsc.edu &gt; **37;8451;TOOLONG at sdsc.edu On Behalf Of &gt; Shaojie Ma &gt; Sent : Wednesday , September 06 , 2006 1:08 AM &gt; To : npaci-rocks-discussion at sdsc.edu &gt; Subject : Rocks-Discuss mpirun -nolocal error when add ip for myrinet &gt; &gt; Dear Sir : &gt; &gt; I want to use the second network to calculation . &gt; So I need to modify the hostname compute-0-0 into &gt; compute-0-0-myri0 in machinefile . In my opinion , I should &gt; add -nolocal option to excute mpirun , as following : &gt; &gt; /opt/mpich/gnu/mpirun -nolocal -np 8 -machinefile &gt; mymachinefile myprogram &gt; Howerver , an error occurs : &gt; &gt; connect to @ @ @ @ @ @ @ @ @ @ : Connection refused trying normal &gt; rsh ( /usr/bin/rsh ) &gt; p08864 : p4error : Child process exited while making &gt; connection to remote process on compute-0-3-myri0 : 0 &gt; compute-0-3-myri0 : Connection refused &gt; &gt; As we known , ssh is used in rocks , instead of rsh . I try &gt; to ssh 192.168.1.251 or ssh compute-0-3-myri0 . It is OK. &gt; Please tell me how to solve the problem . &gt; Best Regards &gt; LONG ... &gt; Shaojie Ma &gt; Institute of Nano Science &gt; Nanjing University of Aeronautics and Astronautics &gt; mashaojie at nuaa.edu.cn Nanjing 210016 , China &gt; LONG ... &gt; From hsunda3 at cct.lsu.edu Tue Sep 5 21:15:42 2006 From : hsunda3 at cct.lsu.edu ( Hari Sundararajan ) Date : Tue , 05 Sep 2006 23:15:42 -0500 Subject : Rocks-Discuss Using harddisk space on child node Message-ID : **36;8490;TOOLONG Hello ! I have 8 identical nodes , and each of them have 2 hard disks ( sda and sdb - 80 GB each ) . I have used the default partitioning scheme , and therefore on each machine only sda is being used . @ @ @ @ @ @ @ @ @ @ What would be the best way to share each machine 's sdb across all other machines ? Thanks ! , with regards , hari . S From jgs4466 at ksu.edu Wed Sep 6 07:59:50 2006 From : jgs4466 at ksu.edu ( Jeffrey Smith ) Date : We 'd , 6 Sep 2006 09:59:50 -0500 Subject : Rocks-Discuss Node install problem In-Reply-To : **33;8528;TOOLONG References : LONG ... **31;8563;TOOLONG LONG ... **33;8596;TOOLONG LONG ... **33;8631;TOOLONG Message-ID : LONG ... Abit NV8 motherboad GB of a 40G HD . I used the auto partition AMD processor Quoting Jeff Pummill : &gt; Hmmm ... &gt; &gt; Can you give us a bit more info regarding hardware , any special &gt; modifications during the initial install , etc ? Did you take the &gt; default &gt; partition Rocks offered ? Adequate RAM ? &gt; &gt; -J . Pummill &gt; &gt; &gt; Jeffrey Smith wrote : &gt; &gt; It boots and varifies the data pool , then Grub Read error occurs . &gt; N &gt; &gt; this is the first attemt to start running rocks .... I 'm kinda new at &gt; &gt; this . &gt; &gt; @ @ @ @ @ @ @ @ @ @ &gt; &gt;&gt; So ... it seems to complete the install the first time around , but &gt; &gt;&gt; never &gt; &gt;&gt; appears to want to boot due to an error with grub ? &gt; &gt;&gt; &gt; &gt;&gt; What exactly does the error say ? &gt; &gt;&gt; &gt; &gt;&gt; Are you doing a software raid on this device ? &gt; &gt;&gt; &gt; &gt;&gt; Have you had other versions of Rocks install and boot &gt; successfully ? &gt; &gt;&gt; &gt; &gt;&gt; -Jeff Pummill &gt; &gt;&gt; &gt; &gt;&gt; Jeffrey Smith wrote : &gt; &gt;&gt; &gt; &gt;&gt;&gt; When I boot from the HD or net I get a grub error &gt; &gt;&gt;&gt; &gt; &gt;&gt;&gt; Quoting Jeff Pummill : &gt; &gt;&gt;&gt; &gt; &gt;&gt;&gt; &gt; &gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt; Check the obvious first ... &gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt; Is the boot order in the BIOS set to harddrive first and netboot &gt; &gt;&gt;&gt;&gt; second ? &gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt; -Jeff Pummill &gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt; Jeffrey Smith wrote : &gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt;&gt; when i try to install the node for 4.2 rocks ... It installs &gt; runs &gt; &gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt;&gt; &gt; @ @ @ @ @ @ @ @ @ @ restarts and runs the install process over again . &gt; &gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt; WHat &gt; &gt;&gt; &gt; &gt;&gt;&gt;&gt; do I &gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt;&gt; need to do ? &gt; &gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt;&gt; -- &gt; &gt;&gt;&gt;&gt;&gt; Kansas State University &gt; &gt;&gt;&gt;&gt;&gt; IDEAS lab Ward 3 &gt; &gt;&gt;&gt;&gt;&gt; 321-591-2254 &gt; &gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt; -- &gt; &gt;&gt;&gt; Kansas State University &gt; &gt;&gt;&gt; IDEAS lab Ward 3 &gt; &gt;&gt;&gt; 321-591-2254 &gt; &gt;&gt;&gt; &gt; &gt;&gt;&gt; &gt; &gt; &gt; &gt; &gt; &gt; -- &gt; &gt; Kansas State University &gt; &gt; IDEAS lab Ward 3 &gt; &gt; 321-591-2254 &gt; &gt; &gt; -- Kansas State University IDEAS lab Ward 3 321-591-2254 From greg.bruno at gmail.com Wed Sep 6 08:14:35 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : We 'd , 6 Sep 2006 08:14:35 -0700 Subject : Rocks-Discuss Using harddisk space on child node In-Reply-To : **36;8666;TOOLONG References : **36;8704;TOOLONG Message-ID : On 9/5/06 , Hari Sundararajan wrote : &gt; Hello ! &gt; &gt; I have 8 identical nodes , and each of them have 2 hard disks ( sda and &gt; sdb - 80 GB each ) . @ @ @ @ @ @ @ @ @ @ therefore on each machine only sda is being used . &gt; &gt; How do I use the sdb of each machine ? What would be the best way to &gt; share each machine 's sdb across all other machines ? see section ' 5.5.4 . Forcing the Default Partitioning Scheme for All Disks on a Compute Node ' in : LONG ... - gb From hobbs at pergamos.net Wed Sep 6 08:47:44 2006 From : hobbs at pergamos.net ( Phil Hobbs ) Date : We 'd , 06 Sep 2006 11:47:44 -0400 Subject : Rocks-Discuss Broken record dept--any PVFS2 roll news ? Message-ID : **35;8742;TOOLONG ? Thanks , Phil Hobbs From gbartsch at casl.umd.edu Wed Sep 6 08:58:11 2006 From : gbartsch at casl.umd.edu ( Gerhard Bartsch ) Date : We 'd , 6 Sep 2006 11:58:11 -0400 Subject : Rocks-Discuss Changing Home Directories on Frontend In-Reply-To : LONG ... Message-ID : LONG ... Ok , So looking at my original post it was a little vague ... When I indicate that we 'd like to re-direct the home directories for users , I mean by having them reside on current file @ @ @ @ @ @ @ @ @ @ Windows shop . So that means I 'd like for their files to reside on a Windows Server and probably authenticate to AD ( active directory ) . This is not a requirement . I do have VMware ESX at my disposal , and can create new file servers with up to 1TB worth of storage ... so another Linux box is possible if there is no easy / seamless way to authenticate and attach to shares located on our Windows AD Domain . The question for everyone out there is : What have you done to relocate home directories ? ... any replies would be deeply appricated . Gerhard Bartsch UMD -----Original Message----- From : **30;8779;TOOLONG at sdsc.edu **37;8811;TOOLONG at sdsc.edu On Behalf Of Gerhard Bartsch Sent : Wednesday , September 06 , 2006 9:59 AM To : npaci-rocks-discussion at sdsc.edu Subject : Rocks-Discuss Changing Home Directories on Frontend Good Morning , I 've been playing around with ROCKS 4.2 to build a prototype cluster . For various reasons I 'd like to use a SAN for the users home directories on the frontend . How does one go @ @ @ @ @ @ @ @ @ @ to a different server or SAN ? Gerhard Gerhard Bartsch University of Maryland -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... **29;8850;TOOLONG From bbrashers at geomatrix.com Wed Sep 6 09:00:15 2006 From : bbrashers at geomatrix.com ( Bart Brashers ) Date : We 'd , 6 Sep 2006 09:00:15 -0700 Subject : Rocks-Discuss Changing Home Directories on Frontend Message-ID : Start by reading section 7.2.6 of the Rocks User Guide . Basically , edit /etc/auto.home on the frontend , then do a " make -C /var/411 " . Bart &gt; -----Original Message----- &gt; From : **30;8881;TOOLONG at sdsc.edu &gt; **37;8913;TOOLONG at sdsc.edu On Behalf Of &gt; Gerhard Bartsch &gt; Sent : Wednesday , September 06 , 2006 6:59 AM &gt; To : npaci-rocks-discussion at sdsc.edu &gt; Subject : Rocks-Discuss Changing Home Directories on Frontend &gt; &gt; Good Morning , &gt; &gt; &gt; &gt; I 've been playing around with ROCKS 4.2 to build a prototype cluster . &gt; For various reasons I 'd like to use a SAN for the users home &gt; directories &gt; on the frontend. &gt; &gt; &gt; &gt; How does one @ @ @ @ @ @ @ @ @ @ ROCKS 4.2 to a different server or SAN ? &gt; &gt; &gt; &gt; Gerhard &gt; &gt; &gt; &gt; Gerhard Bartsch &gt; University of Maryland &gt; &gt; &gt; &gt; &gt; &gt; -------------- next part -------------- &gt; An HTML attachment was scrubbed ... &gt; URL : &gt; LONG ... **39;8952;TOOLONG &gt; From ghuntress at ciw.edu Wed Sep 6 09:02:46 2006 From : ghuntress at ciw.edu ( Garret Huntress ) Date : We 'd , 6 Sep 2006 12:02:46 -0400 Subject : Rocks-Discuss Broken record dept--any PVFS2 roll news ? In-Reply-To : **35;8993;TOOLONG References : **35;9030;TOOLONG Message-ID : LONG ... Build an rpm , add it to extend-compute.xml . I can provide a spec file if needed , however you 'll need to modify the spec file as we compiled pvfs2 against the Topspin/Cisco MPI implementaiton. -Garret On Sep 6 , 2006 , at 11:47 AM , Phil Hobbs wrote : &gt; ? &gt; &gt; Thanks , &gt; &gt; Phil Hobbs From greg.bruno at gmail.com Wed Sep 6 09:12:15 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : We 'd , 6 Sep 2006 09:12:15 -0700 Subject : Rocks-Discuss Broken record dept--any @ @ @ @ @ @ @ @ @ @ Message-ID : On 9/6/06 , Phil Hobbs wrote : &gt; ? it is on my todo list . it got bumped down in priority once we decided it was necessary to put together a 4.2.1 maintenance release . after 4.2.1 is released for i386 and x8664 , i 'll be putting together the ia64 release , then pvfs2 is next . - gb From greg.lindahl at qlogic.com Wed Sep 6 09:19:49 2006 From : greg.lindahl at qlogic.com ( Greg Lindahl ) Date : We 'd , 6 Sep 2006 09:19:49 -0700 Subject : Rocks-Discuss cpuspeed ? In-Reply-To : LONG ... References : LONG ... Message-ID : &lt;20060906161949 . GD1859@greglaptop&gt; On Tue , Sep 05 , 2006 at 09:07:03AM -0700 , Greg Lindahl wrote : &gt; In Rocks / CentOS 4.2 , is cpuspeed supposed to work ? It seems &gt; to be disabled in the kernel , whereas RHEL4U2 has it built in . I think this is pilot error on my part , the system has PowerNow turned off in the BIOS . Doh ! -- g From bbrashers at geomatrix.com Wed Sep 6 09:23:36 2006 From : bbrashers at @ @ @ @ @ @ @ @ @ @ Sep 2006 09:23:36 -0700 Subject : Rocks-Discuss Changing Home Directories on Frontend Message-ID : You can mount Windows shares to both a frontend a compute nodes . The frontend will do NAT for you , so be aware that you 're using up bandwidth by trying to mount a server in your public LAN to the ( private ) compute nodes . You need to have SAMBA installed on the frontend ( I think -- if you 're a windows shop you probably want it installed anyway ) . You 'll need to use the " net ads join " command to have your frontend join the Windows domain . I can never remember the syntax , so I google it each time . You should probably add the Windows box to **29;9141;TOOLONG and re-start named . Then re-write your /etc/auto.home to use the cifs file system . An example line : usershare -fstype=cifs , rw : /win2k3/usershare Where win2k3 is the name you 're your windows machine , and usershare is some user 's share . You 'll have to work out the authentication yourself , but SAMBA can @ @ @ @ @ @ @ @ @ @ every user in the Windows domain to be able to log onto my frontend , so I manage the passwords by hand ( make users run passwd and smbpasswd each time they change their password ) rather than have the Win2K3 box authenticate users . Bart &gt; -----Original Message----- &gt; From : **30;9172;TOOLONG at sdsc.edu &gt; **37;9204;TOOLONG at sdsc.edu On Behalf Of &gt; Gerhard Bartsch &gt; Sent : Wednesday , September 06 , 2006 8:58 AM &gt; To : Gerhard Bartsch ; npaci-rocks-discussion at sdsc.edu &gt; Subject : Re : Rocks-Discuss Changing Home Directories on Frontend &gt; &gt; Ok , &gt; &gt; So looking at my original post it was a little vague ... &gt; &gt; When I indicate that we 'd like to re-direct the home directories for &gt; users , I mean by having them reside on current file servers . &gt; &gt; The hitch is that we are primarily a Windows shop . So that means I 'd &gt; like for their files to reside on a Windows Server and probably &gt; authenticate to AD ( active directory ) . &gt; &gt; This is not a requirement @ @ @ @ @ @ @ @ @ @ , and can &gt; create new file servers with up to 1TB worth of storage ... so another &gt; Linux box is possible if there is no easy / seamless way to &gt; authenticate &gt; and attach to shares located on our Windows AD Domain . &gt; &gt; The question for everyone out there is : What have you done &gt; to relocate &gt; home directories ? &gt; &gt; ... any replies would be deeply appricated. &gt; &gt; Gerhard Bartsch &gt; UMD &gt; &gt; -----Original Message----- &gt; From : **30;9243;TOOLONG at sdsc.edu &gt; **37;9275;TOOLONG at sdsc.edu On Behalf Of Gerhard &gt; Bartsch &gt; Sent : Wednesday , September 06 , 2006 9:59 AM &gt; To : npaci-rocks-discussion at sdsc.edu &gt; Subject : Rocks-Discuss Changing Home Directories on Frontend &gt; &gt; Good Morning , &gt; &gt; &gt; &gt; I 've been playing around with ROCKS 4.2 to build a prototype cluster . &gt; For various reasons I 'd like to use a SAN for the users home &gt; directories &gt; on the frontend. &gt; &gt; &gt; &gt; How does one go about re-directing the user 's home directories under &gt; ROCKS @ @ @ @ @ @ @ @ @ @ &gt; &gt; Gerhard &gt; &gt; &gt; &gt; Gerhard Bartsch &gt; University of Maryland &gt; &gt; &gt; &gt; &gt; &gt; -------------- next part -------------- &gt; An HTML attachment was scrubbed ... &gt; URL : &gt; LONG ... &gt; ments/2006 &gt; **29;9314;TOOLONG &gt; From mgreaney at fnal.gov Wed Sep 6 09:33:28 2006 From : mgreaney at fnal.gov ( MargaretGreaney ) Date : We 'd , 06 Sep 2006 11:33:28 -0500 ( CDT ) Subject : Rocks-Discuss rocks 4.1 appliance partitioning In-Reply-To : References : LONG ... Message-ID : hello , I need some more help with the partition appliance and am clarifying my problems . I am trying to install a worker or compute node named fnpt131 . Fnpt135 is my frontend . I need a partition appliance that will replace the compute partition information . The example of my-compute.xml for an appliance in the documentation is not for something that gets replaced in the compute appliance but an addition to it . root at fnpt135 nodes# pwd **37;9345;TOOLONG root at fnpt135 nodes# ls -l total 68 -rw-rw-r-- 1 root root 24193 Sep 1 13:26 farm-base.xml -rw-rw-r-- 1 root root 5652 Sep 5 @ @ @ @ @ @ @ @ @ @ farm-sendmail.xml -rw-rw-r-- 1 root root 4908 Aug 24 10:24 farms-stunnel-keytab.xml -rw-rw-r-- 1 root root 5254 Aug 29 16:24 farm-stunnel-keytab.xml -rw-rw-r-- 1 root root 1816 Aug 29 16:22 fermi-krb5.xml -rw-rw-r-- 1 root root 240 Sep 1 15:07 my-compute.xml drwxr-xr-x 2 root root 4096 Sep 6 10:18 save -rw-rw-r-- 1 root root 1721 Aug 24 10:13 skeleton.xml farm-partition.xml is not a skeleton , but contains my part info for both hda and hdb . I had a **26;9384;TOOLONG but when I ran the kickstart test I saw double partition info , one from **26;9412;TOOLONG and one from farm-partition.xml so I removed **26;9440;TOOLONG root at fnpt135 default# pwd LONG ... root at fnpt135 default# ls -ltr total 36 -rw-rw-r-- 1 root root 520 Aug 23 15:20 root.xml -rw-rw-r-- 1 root root 3409 Aug 23 15:20 kernel.xml -rw-rw-r-- 1 root root 374 Aug 23 15:20 base-rsh.xml -rw-rw-r-- 1 root root 6588 Sep 1 15:06 hpc.xml -rw-rw-r-- 1 root root 246 Sep 1 15:20 my-appliance.xml -rw-rw-r-- 1 root root 537 Sep 1 16:28 note -rw-rw-r-- 1 root root 322 Sep 5 16:36 farmwn-app.xml drwxr-xr-x 2 root root 4096 Sep 6 10:52 sav contents of farmwn-app.xml @ @ @ @ @ @ @ @ @ @ farmwn-app.xml : --snip-- computefarm-partition With either set up , after a rocks-dist dist , the kickstart output shows what I want , which is my partitioning scheme . root at fnpt135 default# mysql -u apache -e " select * from nodes " cluster LONG ... I 'd Site Name Membership CPUs Rack Rank Comment LONG ... 1 0 fnpt135 1 1 0 0 NULL 3 0 fnpt131 16 2 NULL NULL NULL 5 0 fnd0552 15 1 1 2 NULL LONG ... root at fnpt135 default# mysql -u apache -e " select * from appliances " cluster LONG ... I 'd Name ShortName Graph Node LONG ... 1 frontend f default server 2 compute c default compute 3 pvfs pv default pvfs-io 4 comp-pvfs cp default compute-pvfs 5 network n 6 power p 7 manager NULL 8 nas NULL default nas 9 myri NULL 11 server NULL default media-server 12 client NULL default client 16 farmwn-app NULL default farm-partition 15 my-compute NULL default my-compute LONG ... that was as a result of add-new-appliance --appliance-name " farmwn-app " --xml-config-file-name farm-partition mysql&gt; select * from partitions ; Empty set ( 0.00 sec ) ( @ @ @ @ @ @ @ @ @ @ . ) The kickstart test indicates this : --all --initlabel/ --size 11000 --ondisk hda --grow/local/stage1 --size 4096 --ondisk hdb --grow/local/stage2 --size 4096 --ondisk hdb --growswap --size 4096 --ondisk hda so now I see only my partition info , but I still see auto-partition just above that . when I have the farmwn-app.xml with no auto-partition entries , I do n't see the auto-partition mentioned in the kickstart output and do see my partition scheme . I thought that this would be ok , but when I install the node with either farmwn-app.xml , it stalls at the partition step , and asks " choose a drive to run fdisk on ? hda , hdb " The drives have been wiped out and have no partition info on them , checked this with fdisk . How do I get past this error please ? Margaret -- Margaret Greaney Telephone : 630-840-4623 Fermilab E-mail : mgreaney at fnal.gov CD/CSS/FCS From jpummil at uark.edu Wed Sep 6 09:57:19 2006 From : jpummil at uark.edu ( Jeff Pummill ) Date : We 'd , 06 Sep 2006 11:57:19 -0500 Subject : Rocks-Discuss Using harddisk space @ @ @ @ @ @ @ @ @ @ : **33;9544;TOOLONG Hari , While you still would have to automate recognition of the second drives , the PVFS2 parallel file system would be a good choice for utilizing the space and sharing across the nodes . Jeff F. Pummill Hari Sundararajan wrote : &gt; Hello ! &gt; &gt; I have 8 identical nodes , and each of them have 2 hard disks ( sda and &gt; sdb - 80 GB each ) . I have used the default partitioning scheme , and &gt; therefore on each machine only sda is being used . &gt; &gt; How do I use the sdb of each machine ? What would be the best way to &gt; share each machine 's sdb across all other machines ? &gt; &gt; Thanks ! , with regards , &gt; hari . S &gt; &gt; -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From jgs4466 at ksu.edu Wed Sep 6 10:49:22 2006 From : jgs4466 at ksu.edu ( Jeffrey Smith ) Date : We 'd , 6 Sep 2006 12:49:22 -0500 Subject : Rocks-Discuss Hardware Message-ID : LONG ... I know this @ @ @ @ @ @ @ @ @ @ everyone using . Motherboards - Brand/compatibility issues CPU - 64bit ? AMD/INTEL and kind I have put together a small cluster of about 2 to 3 nodes to work out issues and figuring the best way of approaching building a large cluster . I have run into some hardware issues with the ABIT not liking the GRUB and a few otheres .... we want to scale to 1000 nodes possible multiple core . So any suggestions would be appreciated . -- Kansas State University IDEAS lab Ward 3 321-591-2254 From dtripp at hawaii.edu Wed Sep 6 11:34:59 2006 From : dtripp at hawaii.edu ( Donald Tripp ) Date : We 'd , 06 Sep 2006 08:34:59 -1000 Subject : Rocks-Discuss Hardware In-Reply-To : LONG ... References : LONG ... Message-ID : If you are going to purchase 1000 nodes , have you considered purchasing hardware already configured ? There are many vendors who will work with educational institutions . You could implement 1000 nodes over several phases , 256 a phase or something like that . The problem with building the systems yourself is that availability of parts will change rapidly @ @ @ @ @ @ @ @ @ @ for replacement parts . A vendor would be better for maintenance purposes . Check this out : http : **33;9579;TOOLONG - Donald Tripp dtripp at hawaii.edu LONG ... High Performance Computing Systems Administrator University of Hawai'i at Hilo 200 W. Kawili Street Hilo , Hawaii 96720 On Sep 6 , 2006 , at 7:49 AM , Jeffrey Smith wrote : &gt; I know this is n't really tech . but what kinda hardware is everyone &gt; using . &gt; &gt; &gt; Motherboards - Brand/compatibility issues &gt; CPU - 64bit ? AMD/INTEL and kind &gt; &gt; I have put together a small cluster of about 2 to 3 nodes to work out &gt; issues and figuring the best way of approaching building a large &gt; cluster . &gt; &gt; I have run into some hardware issues with the ABIT not liking the GRUB &gt; and a few otheres .... &gt; &gt; we want to scale to 1000 nodes possible multiple core . So any &gt; suggestions would be appreciated . &gt; &gt; -- &gt; Kansas State University &gt; IDEAS lab Ward 3 &gt; 321-591-2254 -------------- next part -------------- An HTML attachment @ @ @ @ @ @ @ @ @ @ uark.edu Wed Sep 6 11:48:21 2006 From : jpummil at uark.edu ( Jeff Pummill ) Date : We 'd , 06 Sep 2006 13:48:21 -0500 Subject : Rocks-Discuss Hardware In-Reply-To : References : LONG ... Message-ID : **33;9614;TOOLONG Great advice from D. Tripp ! Once a system grows past about 32 nodes , it 's really nice to have integrators , support , etc . Select a few vendors and then ask them for references ( and then , check the references ... ) . There are some really good cluster vendors and some that are n't so great . A sound choice will ensure far less grey hairs and unhappy users in the long run . Jeff F. Pummill Donald Tripp wrote : &gt; If you are going to purchase 1000 nodes , have you considered &gt; purchasing hardware already configured ? There are many vendors who &gt; will work with educational institutions . You could implement 1000 &gt; nodes over several phases , 256 a phase or something like that . &gt; &gt; The problem with building the systems yourself is that availability &gt; of parts will change rapidly @ @ @ @ @ @ @ @ @ @ nodes for replacement parts . A vendor would be better for &gt; maintenance purposes . &gt; &gt; Check this out : &gt; &gt; http : **33;9649;TOOLONG &gt; &gt; &gt; - Donald Tripp &gt; dtripp at hawaii.edu &gt; LONG ... &gt; High Performance Computing Systems Administrator &gt; University of Hawai'i at Hilo &gt; 200 W. Kawili Street &gt; Hilo , Hawaii 96720 &gt; &gt; &gt; On Sep 6 , 2006 , at 7:49 AM , Jeffrey Smith wrote : &gt; &gt; &gt;&gt; I know this is n't really tech . but what kinda hardware is everyone &gt;&gt; using . &gt;&gt; &gt;&gt; &gt;&gt; Motherboards - Brand/compatibility issues &gt;&gt; CPU - 64bit ? AMD/INTEL and kind &gt;&gt; &gt;&gt; I have put together a small cluster of about 2 to 3 nodes to work out &gt;&gt; issues and figuring the best way of approaching building a large &gt;&gt; cluster . &gt;&gt; &gt;&gt; I have run into some hardware issues with the ABIT not liking the GRUB &gt;&gt; and a few otheres .... &gt;&gt; &gt;&gt; we want to scale to 1000 nodes possible multiple core . So any &gt;&gt; suggestions would be appreciated . &gt;&gt; @ @ @ @ @ @ @ @ @ @ 3 &gt;&gt; 321-591-2254 &gt;&gt; &gt; &gt; &gt; &gt; -------------- next part -------------- &gt; An HTML attachment was scrubbed ... &gt; URL : LONG ... &gt; -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From gbartsch at casl.umd.edu Wed Sep 6 11:47:33 2006 From : gbartsch at casl.umd.edu ( Gerhard Bartsch ) Date : We 'd , 6 Sep 2006 14:47:33 -0400 Subject : Rocks-Discuss Hardware In-Reply-To : LONG ... Message-ID : LONG ... I suspect that it 's a matter of what type of number crunching you are going to do and the amount of cash you have . I 'm using a bunch of Intel Xeon 5100 Dell 1950 servers for the final version of the cluster . ( Dual core 3Ghz with 4MB overall cache RAM ) Prior to the advent of the newest Intel Core2 based parts , I 'd have gone with an AMD Opteron based system ... especially if I were working with biostatistics or other highly floating point intensive math applications . I would still consider AMD Opteron based solutions if I intended to have more than two physical @ @ @ @ @ @ @ @ @ @ : Dell 2850 Server 3 x 300GB drives ( RAID 5 ) 4 x 1000baseT NICs ( bonded in two channels ) 16Gb of RAM 2 x 3.6GHz Xeon ( dual Core - Hypertreading off ) Nodes : Dell 1850 Servers 2 x 74GB drives ( Mirrored ) 2 x 1000baseT NICs ( bonded ) 8GB of RAM 2 x 3.6GHz Xeon ( single core ) Dell Precision 490n Workstations 2 x 160GB drives ( 1 unused ) 3 x 1000BaseT NICs ( integrated TG3 unused , PCI-X Intel units bonded ) 16Gb of RAM 2 x 3.2GHz Xeon ( dual core ) I 'm using what I have laying around now ... but I would probably not have gone with the workstations had they not been inherited from another project . Power considerations for the workstations turned out to be far worse than for the servers . -----Original Message----- From : **30;9684;TOOLONG at sdsc.edu **37;9716;TOOLONG at sdsc.edu On Behalf Of Jeffrey Smith Sent : Wednesday , September 06 , 2006 1:49 PM To : npaci-rocks-discussion at sdsc.edu Subject : Rocks-Discuss Hardware I know this is n't really tech . @ @ @ @ @ @ @ @ @ @ Brand/compatibility issues CPU - 64bit ? AMD/INTEL and kind I have put together a small cluster of about 2 to 3 nodes to work out issues and figuring the best way of approaching building a large cluster . I have run into some hardware issues with the ABIT not liking the GRUB and a few otheres .... we want to scale to 1000 nodes possible multiple core . So any suggestions would be appreciated . -- Kansas State University IDEAS lab Ward 3 321-591-2254 From mgreaney at fnal.gov Wed Sep 6 12:01:19 2006 From : mgreaney at fnal.gov ( MargaretGreaney ) Date : We 'd , 06 Sep 2006 14:01:19 -0500 ( CDT ) Subject : Rocks-Discuss rocks 4.1 appliance partitioning In-Reply-To : References : LONG ... Message-ID : &gt; &gt; Do you have any slides or examples of how the appliance can be &gt; &gt; set up besides the " my-compute.xml " that is in the users guide ? That &gt; &gt; example worked fine by the way but it does n't help me with a replacement &gt; &gt; type of appliance . &gt; &gt; just replace ' my-compute ' @ @ @ @ @ @ @ @ @ @ &gt; &gt; - gb &gt; Greg , I did this , replacing the my-compute with the partition appliance name . When I do the install , it stops at the partitioning step . I can see in F3 window , executeautopartition skip 0 on the F1 screen , I see choose a disk to run fdisk on There is something else that rocks is still seeing , preventing the partitioning . Margaret From mam3wn at virginia.edu Wed Sep 6 12:04:03 2006 From : mam3wn at virginia.edu ( Mark McCardell ) Date : We 'd , 6 Sep 2006 15:04:03 -0400 Subject : Rocks-Discuss Hardware In-Reply-To : LONG ... Message-ID : LONG ... Jeff , If you are considering 1000+ nodes I would strongly recommend an HPC integrator such as Dell , HP , Penguin Computing , or some other HPC provider . Just from a support/supply standpoint very few suppliers will be able to meet your volume requirements , let alone dealing with warranty/support issues would a time sink . Having a single point contact for support/warranty is worth it in my opinion . Mark McCardell Computer Systems Engineer Center for @ @ @ @ @ @ @ @ @ @ Charlottesville , VA 22902 **36;9755;TOOLONG -----Original Message----- From : **30;9793;TOOLONG at sdsc.edu **37;9825;TOOLONG at sdsc.edu On Behalf Of Jeffrey Smith Sent : Wednesday , September 06 , 2006 1:49 PM To : npaci-rocks-discussion at sdsc.edu Subject : Rocks-Discuss Hardware I know this is n't really tech . but what kinda hardware is everyone using . Motherboards - Brand/compatibility issues CPU - 64bit ? AMD/INTEL and kind I have put together a small cluster of about 2 to 3 nodes to work out issues and figuring the best way of approaching building a large cluster . I have run into some hardware issues with the ABIT not liking the GRUB and a few otheres .... we want to scale to 1000 nodes possible multiple core . So any suggestions would be appreciated . -- Kansas State University IDEAS lab Ward 3 321-591-2254 From dtripp at hawaii.edu Wed Sep 6 12:04:25 2006 From : dtripp at hawaii.edu ( Donald Tripp ) Date : We 'd , 06 Sep 2006 09:04:25 -1000 Subject : Rocks-Discuss Hardware In-Reply-To : LONG ... References : LONG ... Message-ID : LONG ... Also , keep in mind the @ @ @ @ @ @ @ @ @ @ in late 06 , effectively cutting the size of your cluster in half . LONG ... On Sep 6 , 2006 , at 8:47 AM , Gerhard Bartsch wrote : &gt; I suspect that it 's a matter of what type of number crunching you are &gt; going to do and the amount of cash you have . &gt; &gt; I 'm using a bunch of Intel Xeon 5100 Dell 1950 servers for the final &gt; version of the cluster . ( Dual core 3Ghz with 4MB overall cache RAM ) &gt; &gt; Prior to the advent of the newest Intel Core2 based parts , I 'd have &gt; gone &gt; with an AMD Opteron based system ... especially if I were working with &gt; biostatistics or other highly floating point intensive math &gt; applications . &gt; &gt; I would still consider AMD Opteron based solutions if I intended to &gt; have &gt; more than two physical processors in each node . &gt; &gt; Current configs : &gt; &gt; Front End : Dell 2850 Server &gt; 3 x 300GB drives ( RAID 5 ) &gt; 4 x 1000baseT NICs ( @ @ @ @ @ @ @ @ @ @ 2 x 3.6GHz Xeon ( dual Core - Hypertreading off ) &gt; &gt; Nodes : Dell 1850 Servers &gt; 2 x 74GB drives ( Mirrored ) &gt; 2 x 1000baseT NICs ( bonded ) &gt; 8GB of RAM &gt; 2 x 3.6GHz Xeon ( single core ) &gt; &gt; Dell Precision 490n Workstations &gt; 2 x 160GB drives ( 1 unused ) &gt; 3 x 1000BaseT NICs ( integrated TG3 unused , PCI-X Intel &gt; units bonded ) &gt; 16Gb of RAM &gt; 2 x 3.2GHz Xeon ( dual core ) &gt; &gt; I 'm using what I have laying around now ... but I would probably not &gt; have &gt; gone with the workstations had they not been inherited from another &gt; project . &gt; &gt; Power considerations for the workstations turned out to be far worse &gt; than for the servers . &gt; &gt; -----Original Message----- &gt; From : **30;9864;TOOLONG at sdsc.edu &gt; **37;9896;TOOLONG at sdsc.edu On Behalf Of Jeffrey &gt; Smith &gt; Sent : Wednesday , September 06 , 2006 1:49 PM &gt; To : npaci-rocks-discussion at sdsc.edu &gt; Subject : Rocks-Discuss Hardware &gt; &gt; I know @ @ @ @ @ @ @ @ @ @ is everyone &gt; using . &gt; &gt; &gt; Motherboards - Brand/compatibility issues &gt; CPU - 64bit ? AMD/INTEL and kind &gt; &gt; I have put together a small cluster of about 2 to 3 nodes to work out &gt; issues and figuring the best way of approaching building a large &gt; cluster . &gt; &gt; I have run into some hardware issues with the ABIT not liking the GRUB &gt; and a few otheres .... &gt; &gt; we want to scale to 1000 nodes possible multiple core . So any &gt; suggestions would be appreciated . &gt; &gt; -- &gt; Kansas State University &gt; IDEAS lab Ward 3 &gt; 321-591-2254 From landman at scalableinformatics.com Wed Sep 6 12:31:46 2006 From : landman at scalableinformatics.com ( Joe Landman ) Date : We 'd , 06 Sep 2006 15:31:46 -0400 Subject : Rocks-Discuss Hardware In-Reply-To : LONG ... References : LONG ... LONG ... Message-ID : LONG ... Donald Tripp wrote : &gt; Also , keep in mind the fact that Intel will be releasing a quad core &gt; chip in late 06 , effectively cutting the size of your cluster in half @ @ @ @ @ @ @ @ @ @ concerns over resource contention . You do n't want to be running a code that uses the full bandwidth of the memory system per core to be running on a multi-core system . Quad cores should be great for a number of codes that do n't beat on memory bandwidth . With quad cores you will be able to get 8 CPU cores per 1U unit . With ( on average ) 1/4 of the memory bandwidth . If this wo n't impact your code , great . ( Monte Carlo , et al ) . If you have a code which is bandwidth sensitive , it will be impacted in a multi-core run . That said , we did n't see much impact on real codes with real input decks when we ran cases last year : See LONG ... -- Joseph Landman , Ph.D Founder and CEO Scalable Informatics LLC , email : landman at scalableinformatics.com web : http : **29;9935;TOOLONG phone : +1 734 786 8423 fax : +1 734 786 8452 or +1 866 888 3112 cell : +1 734 612 4615 From federicosacerdoti at gmail.com @ @ @ @ @ @ @ @ @ @ ( Federico D. Sacerdoti ) Date : We 'd , 6 Sep 2006 15:42:30 -0400 Subject : Rocks-Discuss Configuring non-primary ethernet interfaces to DHCP Message-ID : LONG ... As part of the " kickstart from any ethernet interface " logic , all interfaces of a compute node are configured , which is wonderful . However I see that by default **37;9966;TOOLONG configures the " other " interfaces to dhcp ( other = ethN where N&gt;0 ) BOOTPROTO=dhcp ONBOOT=yes This causes problems when both interfaces are connected , but only one used . While this seems ludicrous , I have an ipmi service processor which I want to have its own port . It would be better to turn off the other interfaces . This should not affect the kickstart from anywhere function . If you want to channel bond , that is a seperate configuration . For all other interfaces , I suggest : BOOTPROTO=none ONBOOT=no Am I missing something ? I am about to make this change locally , but wanted to discuss it with the list . Thanks , Federico -- D. E. Shaw Research LLC New York NY @ @ @ @ @ @ @ @ @ @ : jpummil at uark.edu ( Jeff Pummill ) Date : We 'd , 06 Sep 2006 14:48:39 -0500 Subject : Rocks-Discuss Hardware In-Reply-To : LONG ... References : LONG ... LONG ... LONG ... Message-ID : **33;10005;TOOLONG How does this affect the way that one sets up scheduling for large parallel jobs ... say with LSF on a system utilizing nodes with dual Woodcrest cpu 's ? It seems as though communication and thruput become even more important considerations when setting up your queues when dual ( or quad ) cores are implemented . Some of our apps are cpu intensive , others are very " chatty " , and all run for days if not weeks on large numbers of cpu 's . Certainly better profiling of the apps will be in order , but has anyone in the Rocks community seen a need to substantially restructure the way that they structure their job submissions due to both enhancements AND restrictions of the new dual core products ? Thanks , Jeff F. Pummill Joe Landman wrote : &gt; Donald Tripp wrote : &gt;&gt; Also , keep in mind the fact @ @ @ @ @ @ @ @ @ @ in late 06 , effectively cutting the size of your cluster in half . &gt; &gt; Be careful about this , as there are real concerns over resource &gt; contention . You do n't want to be running a code that uses the full &gt; bandwidth of the memory system per core to be running on a multi-core &gt; system . Quad cores should be great for a number of codes that do n't &gt; beat on memory bandwidth . &gt; &gt; With quad cores you will be able to get 8 CPU cores per 1U unit . With &gt; ( on average ) 1/4 of the memory bandwidth . If this wo n't impact your &gt; code , great . ( Monte Carlo , et al ) . If you have a code which is &gt; bandwidth sensitive , it will be impacted in a multi-core run . &gt; &gt; That said , we did n't see much impact on real codes with real input &gt; decks when we ran cases last year : See &gt; LONG ... &gt; -------------- next part -------------- An HTML attachment was scrubbed ... @ @ @ @ @ @ @ @ @ @ 6 12:59:26 2006 From : philip.papadopoulos at gmail.com ( Philip Papadopoulos ) Date : We 'd , 6 Sep 2006 12:59:26 -0700 Subject : Rocks-Discuss Configuring non-primary ethernet interfaces to DHCP In-Reply-To : LONG ... References : LONG ... Message-ID : LONG ... Already done in 4.2 . You should upgrade ; - ) . -P On 9/6/06 , Federico D. Sacerdoti wrote : &gt; &gt; As part of the " kickstart from any ethernet interface " logic , all &gt; interfaces of a compute node are configured , which is wonderful . &gt; However I see that by default **37;10040;TOOLONG &gt; configures the " other " interfaces to dhcp ( other = ethN where N&gt;0 ) &gt; &gt; BOOTPROTO=dhcp &gt; ONBOOT=yes &gt; &gt; This causes problems when both interfaces are connected , but only one &gt; used . While this seems ludicrous , I have an ipmi service processor &gt; which I want to have its own port . &gt; &gt; It would be better to turn off the other interfaces . This should not &gt; affect the kickstart from anywhere function . If you want to channel &gt; bond @ @ @ @ @ @ @ @ @ @ all other interfaces , I suggest : &gt; &gt; BOOTPROTO=none &gt; ONBOOT=no &gt; &gt; Am I missing something ? I am about to make this change locally , but &gt; wanted to discuss it with the list . &gt; &gt; Thanks , &gt; Federico &gt; -- &gt; D. E. Shaw Research LLC &gt; New York NY &gt; &gt; -- Philip Papadopoulos , PhD University of California , San Diego 858-822-3628 -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From dtripp at hawaii.edu Wed Sep 6 13:00:39 2006 From : dtripp at hawaii.edu ( Donald Tripp ) Date : We 'd , 06 Sep 2006 10:00:39 -1000 Subject : Rocks-Discuss Hardware In-Reply-To : **33;10079;TOOLONG References : LONG ... LONG ... LONG ... **33;10114;TOOLONG Message-ID : LONG ... The more cpu cores you put in a single machine , the more important your interconnect becomes . Myricom produces a 10gbit version of Myrinet . I 've heard rumors of 100gbit ethernet cards . You will have to be very careful at to what apps you allow to use all the cores of a machine . If you @ @ @ @ @ @ @ @ @ @ in a machine . I know this may sound wasteful , but the communication wait time is wasteful as well , and the longer that is , the long the CPU sits anyway . I do n't have any specs on that , but I 've seen it happen when an interconnect gets saturated . If your users are running CPU intensive programs , then dual or " quad " quad cores would expedite your jobs . However , if you had , in theory , 8-16 cores in a machine , you would need 4x that in ram ... On Sep 6 , 2006 , at 9:48 AM , Jeff Pummill wrote : &gt; How does this affect the way that one sets up scheduling for large &gt; parallel jobs ... say with LSF on a system utilizing nodes with dual &gt; Woodcrest cpu 's ? It seems as though communication and thruput &gt; become even more important considerations when setting up your &gt; queues when dual ( or quad ) cores are implemented . &gt; &gt; Some of our apps are cpu intensive , others are very " @ @ @ @ @ @ @ @ @ @ not weeks on large numbers of cpu's. &gt; &gt; Certainly better profiling of the apps will be in order , but has &gt; anyone in the Rocks community seen a need to substantially &gt; restructure the way that they structure their job submissions due &gt; to both enhancements AND restrictions of the new dual core products ? &gt; &gt; Thanks , &gt; &gt; Jeff F. Pummill &gt; &gt; &gt; &gt; Joe Landman wrote : &gt;&gt; Donald Tripp wrote : &gt;&gt;&gt; Also , keep in mind the fact that Intel will be releasing a quad &gt;&gt;&gt; core chip in late 06 , effectively cutting the size of your &gt;&gt;&gt; cluster in half . &gt;&gt; &gt;&gt; Be careful about this , as there are real concerns over resource &gt;&gt; contention . You do n't want to be running a code that uses the &gt;&gt; full bandwidth of the memory system per core to be running on a &gt;&gt; multi-core system . Quad cores should be great for a number of &gt;&gt; codes that do n't beat on memory bandwidth . &gt;&gt; &gt;&gt; With quad cores you will be able to get 8 @ @ @ @ @ @ @ @ @ @ average ) 1/4 of the memory bandwidth . If this wo n't &gt;&gt; impact your code , great . ( Monte Carlo , et al ) . If you have a &gt;&gt; code which is bandwidth sensitive , it will be impacted in a multi- &gt;&gt; core run . &gt;&gt; &gt;&gt; That said , we did n't see much impact on real codes with real input &gt;&gt; decks when we ran cases last year : See http : // &gt;&gt; LONG ... &gt;&gt; -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From landman at scalableinformatics.com Wed Sep 6 13:18:13 2006 From : landman at scalableinformatics.com ( Joe Landman ) Date : We 'd , 06 Sep 2006 16:18:13 -0400 Subject : Rocks-Discuss Hardware In-Reply-To : **33;10149;TOOLONG References : LONG ... LONG ... LONG ... **33;10184;TOOLONG Message-ID : LONG ... Jeff Pummill wrote : &gt; How does this affect the way that one sets up scheduling for large &gt; parallel jobs ... say with LSF on a system utilizing nodes with dual &gt; Woodcrest cpu 's ? It seems as though communication and thruput become &gt; @ @ @ @ @ @ @ @ @ @ dual &gt; ( or quad ) cores are implemented . It helps to know about the memory access patterns and communication patterns of your jobs . Without this , you might need to do some test runs and see if they like going wide ( maximizing number of threads per processor ) or narrow ( minimizing the number of threads per processor ) . I just sent this in a message offline to Jeffrey , as I did n't  want to spam ( disclosure : we are a cluster **31;10219;TOOLONG company ) . You get the 1/N problem of a single fixed resource being shared by N requesters . It gets worse as N gets larger . Specifically , if you have a single low latency port ( 10GBe , Infiniband , whatever ) , and you have 1 thread using this , you are in great shape . Minimal contention for that port . Now add a second thread on the same machine contending for that pipe . In most cases , you do n't see an impact . Now add two more threads for 4 cores on 2 @ @ @ @ @ @ @ @ @ @ are running a code like LAMMPS , and you are sharing Myrinet/Infinipath the answer is a resounding yes . It is noticable at 2 threads per node , significant at 4 threads per node , and debilitating to overall performance at 8 threads per node . We did this as a study for a customer last year . Single points of information flow are points of serialization . You really do n't want serialization in your cluster , if at all possible . Or if you have to have them , then you need to engineer them into an acceptable compromise . Examples of such single points of information flow are your processor socket -&gt; memory interface , your low latency high bandwidth network connection , your head node private cluster interface , your scheduler , ... That is , as you start increasing the demands upon various of these resources , you are scaling them up in a way , they may not scale the way you have anticipated . We have worked with multiple customers to try to ameliorate their design and implementation issues that ran head first @ @ @ @ @ @ @ @ @ @ is n't fun to reverse engineer a cluster design that does not scale , yet has to . &gt; Some of our apps are cpu intensive , others are very " chatty " , and all &gt; run for days if not weeks on large numbers of cpu 's . It is easy to set up queues of single thread per node or multiple threads per node . Might be useful here . &gt; &gt; Certainly better profiling of the apps will be in order , but has anyone &gt; in the Rocks community seen a need to substantially restructure the way &gt; that they structure their job submissions due to both enhancements AND &gt; restrictions of the new dual core products ? We have had some of our cluster customers select single core units specifically to keep the measured contention down . Joe &gt; &gt; Thanks , &gt; &gt; Jeff F. Pummill &gt; &gt; &gt; &gt; &gt; Joe Landman wrote : &gt;&gt; Donald Tripp wrote : &gt;&gt;&gt; Also , keep in mind the fact that Intel will be releasing a quad core &gt;&gt;&gt; chip in late 06 , effectively @ @ @ @ @ @ @ @ @ @ Be careful about this , as there are real concerns over resource &gt;&gt; contention . You do n't want to be running a code that uses the full &gt;&gt; bandwidth of the memory system per core to be running on a multi-core &gt;&gt; system . Quad cores should be great for a number of codes that do n't &gt;&gt; beat on memory bandwidth . &gt;&gt; &gt;&gt; With quad cores you will be able to get 8 CPU cores per 1U unit . With &gt;&gt; ( on average ) 1/4 of the memory bandwidth . If this wo n't impact your &gt;&gt; code , great . ( Monte Carlo , et al ) . If you have a code which is &gt;&gt; bandwidth sensitive , it will be impacted in a multi-core run . &gt;&gt; &gt;&gt; That said , we did n't see much impact on real codes with real input &gt;&gt; decks when we ran cases last year : See &gt;&gt; LONG ... &gt;&gt; &gt; -------------- next part -------------- &gt; An HTML attachment was scrubbed ... &gt; URL : LONG ... -- Joseph Landman , Ph.D Founder and CEO Scalable @ @ @ @ @ @ @ @ @ @ http : **29;10252;TOOLONG phone : +1 734 786 8423 fax : +1 734 786 8452 or +1 866 888 3112 cell : +1 734 612 4615 From landman at scalableinformatics.com Wed Sep 6 13:34:26 2006 From : landman at scalableinformatics.com ( Joe Landman ) Date : We 'd , 06 Sep 2006 16:34:26 -0400 Subject : Rocks-Discuss Hardware In-Reply-To : LONG ... References : LONG ... LONG ... LONG ... **33;10283;TOOLONG LONG ... Message-ID : LONG ... Donald Tripp wrote : &gt; The more cpu cores you put in a single machine , the more important &gt; your interconnect becomes . Myricom produces a 10gbit version of Actually the more important reducing contention and latency becomes . &gt; Myrinet . I 've heard rumors of 100gbit ethernet cards . Last I heard this was not a standard or even in the process of becoming one . Some of the machines we have recently finished building or bidding on have DDR 4x IB in them . This will get you about aggregate 3.2 GB/s ( 1.6 GB/s up and down ) . As this is fairly close to what a well engineered @ @ @ @ @ @ @ @ @ @ flavor ) these days , this is a good thing for large messages . Still does n't do much for latency , and this is why things like Infinipath are so good , but InfiniPath is n't DDR now . ( I do n't work for Qlogic/PathScale , or own stock in them/Qlogic . We do put InfiniPath in some clusters we have sold , great stuff , hopefully DDR soon ) &gt; You will have to be very careful at to what apps you allow to use all &gt; the cores of a machine . If you experience bandwidth troubles , try &gt; using only half the cpus in a machine . I know this may sound &gt; wasteful , but the communication wait time is wasteful as well , and This is exactly the contention issue I was indicating in the other message . Contention increases latency and jitter . This is a bad thing for latency sensitive apps . This is also a good argument for some single core nodes , or multiple busses connected to local CPUs with their own **25;10318;TOOLONG adapters on them . &gt; @ @ @ @ @ @ @ @ @ @ anyway . I do n't have any &gt; specs on that , but I 've seen it happen when an interconnect gets &gt; saturated . I have measurements we did for a customer that beautifully illustrates why latency and contention are so terribly important in a real code . &gt; If your users are running CPU intensive programs , then dual or " quad " &gt; quad cores would expedite your jobs . However , if you had , in theory , &gt; 8-16 cores in a machine , you would need 4x that in ram ... And the resources would have 4x the contention . So if you have a program which uses 1.2 GB/s of memory bandwidth now , and it scales nicely on an SMP with each additional thread consuming 1.2 GB/s , 8 threads on a dual socket 4 core system will consume about 9.6 GB/s . This pretty much fills up the memory system assuming that the cores can each use the memory when they want to and there is no contention . This does n't normally happen . Each AMD socket can hit about @ @ @ @ @ @ @ @ @ @ need are 2 stream like operations per socket , and you have significant contention . 4 could be severely handicapped . On the 51xx series , you have a maximum of about 7 GB/s for 1 or 2 sockets . You can hit about 6 GB/s on one thread . If you have a 2 socket system , this means that 3 other cores need to be working out of cache , or idling in order not to contend with the original thread . Since the 51xx series does n't go beyond 2 sockets this is an annoyance , but hopefully not too upsetting . Its when you get to 4 cores per socket that this memory bandwidth issue starts getting debilitating . Of course memory bandwidth is n't the only issue . Any shared resource has a usually finite extent . When you consume it all with N threads , as you scale to N+M you should expect to see contention and other issues intrude on the performance . Quad cores will be good for some codes and systems . Your mileage may vary . We will try to @ @ @ @ @ @ @ @ @ @ play with . Joe &gt; &gt; &gt; On Sep 6 , 2006 , at 9:48 AM , Jeff Pummill wrote : &gt; &gt;&gt; How does this affect the way that one sets up scheduling for large &gt;&gt; parallel jobs ... say with LSF on a system utilizing nodes with dual &gt;&gt; Woodcrest cpu 's ? It seems as though communication and thruput &gt;&gt; become even more important considerations when setting up your &gt;&gt; queues when dual ( or quad ) cores are implemented . &gt;&gt; &gt;&gt; Some of our apps are cpu intensive , others are very " chatty " , and &gt;&gt; all run for days if not weeks on large numbers of cpu's. &gt;&gt; &gt;&gt; Certainly better profiling of the apps will be in order , but has &gt;&gt; anyone in the Rocks community seen a need to substantially &gt;&gt; restructure the way that they structure their job submissions due &gt;&gt; to both enhancements AND restrictions of the new dual core products ? &gt;&gt; &gt;&gt; Thanks , &gt;&gt; &gt;&gt; Jeff F. Pummill &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Joe Landman wrote : &gt;&gt;&gt; Donald Tripp wrote : &gt;&gt;&gt;&gt; Also , @ @ @ @ @ @ @ @ @ @ a quad &gt;&gt;&gt;&gt; core chip in late 06 , effectively cutting the size of your &gt;&gt;&gt;&gt; cluster in half . &gt;&gt;&gt; Be careful about this , as there are real concerns over resource &gt;&gt;&gt; contention . You do n't want to be running a code that uses the &gt;&gt;&gt; full bandwidth of the memory system per core to be running on a &gt;&gt;&gt; multi-core system . Quad cores should be great for a number of &gt;&gt;&gt; codes that do n't beat on memory bandwidth . &gt;&gt;&gt; &gt;&gt;&gt; With quad cores you will be able to get 8 CPU cores per 1U unit . &gt;&gt;&gt; With ( on average ) 1/4 of the memory bandwidth . If this wo n't &gt;&gt;&gt; impact your code , great . ( Monte Carlo , et al ) . If you have a &gt;&gt;&gt; code which is bandwidth sensitive , it will be impacted in a multi- &gt;&gt;&gt; core run . &gt;&gt;&gt; &gt;&gt;&gt; That said , we did n't see much impact on real codes with real input &gt;&gt;&gt; decks when we ran cases last year : See http : // &gt;&gt;&gt; LONG ... &gt;&gt;&gt; @ @ @ @ @ @ @ @ @ @ was scrubbed ... &gt; URL : LONG ... -- Joseph Landman , Ph.D Founder and CEO Scalable Informatics LLC , email : landman at scalableinformatics.com web : http : **29;10345;TOOLONG phone : +1 734 786 8423 fax : +1 734 786 8452 or +1 866 888 3112 cell : +1 734 612 4615 From mgreaney at fnal.gov Wed Sep 6 13:55:09 2006 From : mgreaney at fnal.gov ( MargaretGreaney ) Date : We 'd , 06 Sep 2006 15:55:09 -0500 ( CDT ) Subject : Rocks-Discuss default " replace " partition not working Message-ID : hello , I 've found something that may help with my problem . If I use the example in the documentation where you want to change the default partitioning with the **26;10376;TOOLONG , with contents : / --size 8000 --ondisk hda swap --size 1000 --ondisk hda /mydata --size 1 --grow --ondisk hda and then do a rocks-dist dist then put the node back to a compute node membership mysql&gt; select * from nodes ; LONG ... I 'd Site Name Membership CPUs Rack Rank Comment LONG ... 1 0 fnpt135 1 1 0 0 NULL 3 0 @ @ @ @ @ @ @ @ @ @ 1 1 2 NULL LONG ... 3 rows in set ( 0.00 sec ) mysql&gt; select * from partitions ; Empty set ( 0.00 sec ) then dbreport kickstart fnpt131 more -snip-- install reboot rootpw --iscrypted jwZm45OJKdp4s url --url LONG ... zerombr yes bootloader --location=mbr timezone America/Chicago part / --size 8000 --ondisk hda part swap --size 1000 --ondisk hda part /mydata --size 1 --grow --ondisk hda --snip shows partition changed to replacement example partitioning , then install , I still get an error " Choose a disk to run fdisk on " Here I am not doing anything custom and am just following documentation . Do you know why I would be getting this error ? On the node I am installing , when I go into the shell and do fdisk -l /dev/hda or hdb there is nothing there . I have previously cleared the disks with fdisk . I 've run the rocks-partition --del --node fnpt131 and nothing is in the partitions table for this node . thank you , Margaret -- Margaret Greaney Telephone : 630-840-4623 Fermilab E-mail : mgreaney at fnal.gov CD/CSS/FCS From Aron.B.Alexander at peterson.af.mil @ @ @ @ @ @ @ @ @ @ ( Alexander , Aron B SrA SMC/WXTS ) Date : We 'd , 6 Sep 2006 15:00:12 -0600 Subject : Rocks-Discuss Initial install of compute nodes via PXE In-Reply-To : LONG ... Message-ID : Please pardon the late reply , I was out of town . I ran both commands on the head-node and the output is included in the attached text files . Thank you for your assistance . -Aron From : Mason J. Katz mailto:mason.katz at gmail.com On Behalf Of mason j. katz Sent : Monday , August 28 , 2006 5:52 PM To : Alexander , Aron B SrA SMC/WXTS Cc : npaci-rocks-discussion at sdsc.edu Subject : Re : Rocks-Discuss Initial install of compute nodes via PXE Please send the output of " dbreport bug " , and " dbreport kickstart compute-0-0 " . -mjk On Aug 28 , 2006 , at 12:11 P , Alexander , Aron B SrA SMC/WXTS wrote : I have been looking through the documentation , but am unable to determine how to orchestrate a PXE boot/install of my compute nodes . I have the front-end installed and running and have already added @ @ @ @ @ @ @ @ @ @ and boot my first compute node . It is detected and added as compute-0-0 to the list with open parentheses ( ) . But , nothing seems to happen after this . To troubleshoot , I hooked up a monitor/keyboard to the compute node and can see that it booted into the Rocks installation program but is awaiting which language to use . If I answer that and the following keyboard layout question it then is unable to find the /home/install/rocks-dist nfs mount point on the server ( default ip of 10.1.1.1 ) . It is also unable to find the needed install files using ftp or http . I feel as though I am misunderstanding something fundamental , and will greatly appreciate it if someone can point me in the correct direction . Best regards , -Aron -------------- next part -------------- An embedded and charset-unspecified text was scrubbed ... Name : bug.txt Url : LONG ... -------------- next part -------------- An embedded and charset-unspecified text was scrubbed ... Name : kickstart.txt Url : LONG ... From dtripp at hawaii.edu Wed Sep 6 14:07:06 2006 From : dtripp at hawaii.edu @ @ @ @ @ @ @ @ @ @ 2006 11:07:06 -1000 Subject : Rocks-Discuss Hardware In-Reply-To : LONG ... References : LONG ... LONG ... LONG ... **33;10404;TOOLONG LONG ... LONG ... Message-ID : My apologies . I had one too many zeros regarding the ethernet cards . Should have been 10gbit ethernet . Foundry Networks , as well as others , are working on 10gbit ethernet cards . I 've read some articles about that in various trade magazines . - Don On Sep 6 , 2006 , at 10:34 AM , Joe Landman wrote : &gt; Donald Tripp wrote : &gt;&gt; The more cpu cores you put in a single machine , the more &gt;&gt; important your interconnect becomes . Myricom produces a 10gbit &gt;&gt; version of &gt; &gt; Actually the more important reducing contention and latency becomes . &gt; &gt;&gt; Myrinet . I 've heard rumors of 100gbit ethernet cards . &gt; &gt; Last I heard this was not a standard or even in the process of &gt; becoming one . &gt; &gt; Some of the machines we have recently finished building or bidding &gt; on have DDR 4x IB in them . This will @ @ @ @ @ @ @ @ @ @ up and down ) . As this is fairly close to what a &gt; well engineered program can feed to the CPU ( AMD or Intel flavor ) &gt; these days , this is a good thing for large messages . Still does n't &gt; do much for latency , and this is why things like Infinipath are so &gt; good , but InfiniPath is n't DDR now . &gt; &gt; ( I do n't work for Qlogic/PathScale , or own stock in them/Qlogic. &gt; We do put InfiniPath in some clusters we have sold , great stuff , &gt; hopefully DDR soon ) &gt; &gt;&gt; You will have to be very careful at to what apps you allow to use &gt;&gt; all the cores of a machine . If you experience bandwidth troubles , &gt;&gt; try using only half the cpus in a machine . I know this may sound &gt;&gt; wasteful , but the communication wait time is wasteful as well , and &gt; &gt; This is exactly the contention issue I was indicating in the other &gt; message . Contention increases latency and jitter . This is @ @ @ @ @ @ @ @ @ @ is also a good argument for &gt; some single core nodes , or multiple busses connected to local CPUs &gt; with their own **25;10439;TOOLONG adapters on them . &gt; &gt;&gt; the longer that is , the long the CPU sits anyway . I do n't have &gt;&gt; any specs on that , but I 've seen it happen when an interconnect &gt;&gt; gets saturated . &gt; &gt; I have measurements we did for a customer that beautifully &gt; illustrates why latency and contention are so terribly important in &gt; a real code . &gt; &gt;&gt; If your users are running CPU intensive programs , then dual or &gt;&gt; " quad " quad cores would expedite your jobs . However , if you had , &gt;&gt; in theory , 8-16 cores in a machine , you would need 4x that in ram ... &gt; &gt; And the resources would have 4x the contention . So if you have a &gt; program which uses 1.2 GB/s of memory bandwidth now , and it scales &gt; nicely on an SMP with each additional thread consuming 1.2 GB/s , 8 &gt; threads on a @ @ @ @ @ @ @ @ @ @ &gt; This pretty much fills up the memory system assuming that the cores &gt; can each use the memory when they want to and there is no &gt; contention . This does n't normally happen . &gt; &gt; Each AMD socket can hit about 6 GB/s sustained in stream like &gt; operations . All you need are 2 stream like operations per socket , &gt; and you have significant contention . 4 could be severely &gt; handicapped . On the 51xx series , you have a maximum of about 7 GB/ &gt; s for 1 or 2 sockets . You can hit about 6 GB/s on one thread . If &gt; you have a 2 socket system , this means that 3 other cores need to &gt; be working out of cache , or idling in order not to contend with the &gt; original thread . Since the 51xx series does n't go beyond 2 sockets &gt; this is an annoyance , but hopefully not too upsetting . &gt; &gt; Its when you get to 4 cores per socket that this memory bandwidth &gt; issue starts getting debilitating . &gt; @ @ @ @ @ @ @ @ @ @ . Any shared &gt; resource has a usually finite extent . When you consume it all with &gt; N threads , as you scale to N+M you should expect to see contention &gt; and other issues intrude on the performance . &gt; &gt; Quad cores will be good for some codes and systems . Your mileage &gt; may vary . We will try to do some studies as soon as we get some to &gt; play with . &gt; &gt; Joe &gt; &gt;&gt; On Sep 6 , 2006 , at 9:48 AM , Jeff Pummill wrote : &gt;&gt;&gt; How does this affect the way that one sets up scheduling for &gt;&gt;&gt; large parallel jobs ... say with LSF on a system utilizing nodes &gt;&gt;&gt; with dual Woodcrest cpu 's ? It seems as though communication and &gt;&gt;&gt; thruput become even more important considerations when setting &gt;&gt;&gt; up your queues when dual ( or quad ) cores are implemented . &gt;&gt;&gt; &gt;&gt;&gt; Some of our apps are cpu intensive , others are very " chatty " , &gt;&gt;&gt; and all run for days if not weeks on large numbers of cpu's. @ @ @ @ @ @ @ @ @ @ in order , but has &gt;&gt;&gt; anyone in the Rocks community seen a need to substantially &gt;&gt;&gt; restructure the way that they structure their job submissions &gt;&gt;&gt; due to both enhancements AND restrictions of the new dual core &gt;&gt;&gt; products ? &gt;&gt;&gt; &gt;&gt;&gt; Thanks , &gt;&gt;&gt; &gt;&gt;&gt; Jeff F. Pummill &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; Joe Landman wrote : &gt;&gt;&gt;&gt; Donald Tripp wrote : &gt;&gt;&gt;&gt;&gt; Also , keep in mind the fact that Intel will be releasing a &gt;&gt;&gt;&gt;&gt; quad core chip in late 06 , effectively cutting the size of &gt;&gt;&gt;&gt;&gt; your cluster in half . &gt;&gt;&gt;&gt; Be careful about this , as there are real concerns over resource &gt;&gt;&gt;&gt; contention . You do n't want to be running a code that uses the &gt;&gt;&gt;&gt; full bandwidth of the memory system per core to be running on a &gt;&gt;&gt;&gt; multi-core system . Quad cores should be great for a number of &gt;&gt;&gt;&gt; codes that do n't beat on memory bandwidth . &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; With quad cores you will be able to get 8 CPU cores per 1U &gt;&gt;&gt;&gt; unit . With ( on average ) 1/4 of the memory bandwidth @ @ @ @ @ @ @ @ @ @ great . ( Monte Carlo , et al ) . If you &gt;&gt;&gt;&gt; have a code which is bandwidth sensitive , it will be impacted &gt;&gt;&gt;&gt; in a multi- core run . &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; That said , we did n't see much impact on real codes with real &gt;&gt;&gt;&gt; input decks when we ran cases last year : See http : // &gt;&gt;&gt;&gt; LONG ... &gt;&gt;&gt;&gt; &gt;&gt; -------------- next part -------------- &gt;&gt; An HTML attachment was scrubbed ... &gt;&gt; URL : LONG ... &gt;&gt; **45;10466;TOOLONG &gt; &gt; &gt; -- &gt; &gt; Joseph Landman , Ph.D &gt; Founder and CEO &gt; Scalable Informatics LLC , &gt; email : landman at scalableinformatics.com &gt; web : http : **29;10513;TOOLONG &gt; phone : +1 734 786 8423 &gt; fax : +1 734 786 8452 or +1 866 888 3112 &gt; cell : +1 734 612 4615 &gt; From laytonjb at charter.net Wed Sep 6 14:25:35 2006 From : laytonjb at charter.net ( Jeffrey B. Layton ) Date : We 'd , 06 Sep 2006 17:25:35 -0400 Subject : Rocks-Discuss Hardware In-Reply-To : References : LONG ... LONG ... LONG ... **33;10544;TOOLONG LONG ... LONG ... @ @ @ @ @ @ @ @ @ @ are a number of companies working on 10Gig cards and there are several that are shipping . To go along with that there are companies that have 10Gig switches as well . From very small port counts to large port counts . But , the end result is that 10Gig is still expensive . The NICs are still a bit pricey and the switches get really pricey when you get larger . My guess is that at low port counts 10Gig would be about $1200-$1400 a port and for larger counts you are over $4500 or even $8000 a port pretty easily . Jeff &gt; My apologies . I had one too many zeros regarding the ethernet cards . &gt; Should have been 10gbit ethernet . Foundry Networks , as well as others , &gt; are working on 10gbit ethernet cards . I 've read some articles about &gt; that in various trade magazines . &gt; &gt; - Don &gt; &gt; On Sep 6 , 2006 , at 10:34 AM , Joe Landman wrote : &gt; &gt;&gt; Donald Tripp wrote : &gt;&gt;&gt; The more cpu cores you put in a @ @ @ @ @ @ @ @ @ @ . Myricom produces a 10gbit version of &gt;&gt; &gt;&gt; Actually the more important reducing contention and latency becomes . &gt;&gt; &gt;&gt;&gt; Myrinet . I 've heard rumors of 100gbit ethernet cards . &gt;&gt; &gt;&gt; Last I heard this was not a standard or even in the process of &gt;&gt; becoming one . &gt;&gt; &gt;&gt; Some of the machines we have recently finished building or bidding on &gt;&gt; have DDR 4x IB in them . This will get you about aggregate 3.2 GB/s &gt;&gt; ( 1.6 GB/s up and down ) . As this is fairly close to what a well &gt;&gt; engineered program can feed to the CPU ( AMD or Intel flavor ) these &gt;&gt; days , this is a good thing for large messages . Still does n't do much &gt;&gt; for latency , and this is why things like Infinipath are so good , but &gt;&gt; InfiniPath is n't DDR now . &gt;&gt; &gt;&gt; ( I do n't work for Qlogic/PathScale , or own stock in them/Qlogic . We &gt;&gt; do put InfiniPath in some clusters we have sold , great stuff , &gt;&gt; hopefully DDR soon @ @ @ @ @ @ @ @ @ @ at to what apps you allow to use &gt;&gt;&gt; all the cores of a machine . If you experience bandwidth troubles , &gt;&gt;&gt; try using only half the cpus in a machine . I know this may sound &gt;&gt;&gt; wasteful , but the communication wait time is wasteful as well , and &gt;&gt; &gt;&gt; This is exactly the contention issue I was indicating in the other &gt;&gt; message . Contention increases latency and jitter . This is a bad &gt;&gt; thing for latency sensitive apps . This is also a good argument for &gt;&gt; some single core nodes , or multiple busses connected to local CPUs &gt;&gt; with their own **25;10617;TOOLONG adapters on them . &gt;&gt; &gt;&gt;&gt; the longer that is , the long the CPU sits anyway . I do n't have any &gt;&gt;&gt; specs on that , but I 've seen it happen when an interconnect gets &gt;&gt;&gt; saturated . &gt;&gt; &gt;&gt; I have measurements we did for a customer that beautifully &gt;&gt; illustrates why latency and contention are so terribly important in a &gt;&gt; real code . &gt;&gt; &gt;&gt;&gt; If your users are running CPU intensive programs @ @ @ @ @ @ @ @ @ @ would expedite your jobs . However , if you had , in &gt;&gt;&gt; theory , 8-16 cores in a machine , you would need 4x that in ram ... &gt;&gt; &gt;&gt; And the resources would have 4x the contention . So if you have a &gt;&gt; program which uses 1.2 GB/s of memory bandwidth now , and it scales &gt;&gt; nicely on an SMP with each additional thread consuming 1.2 GB/s , 8 &gt;&gt; threads on a dual socket 4 core system will consume about 9.6 GB/s. &gt;&gt; This pretty much fills up the memory system assuming that the cores &gt;&gt; can each use the memory when they want to and there is no &gt;&gt; contention . This does n't normally happen . &gt;&gt; &gt;&gt; Each AMD socket can hit about 6 GB/s sustained in stream like &gt;&gt; operations . All you need are 2 stream like operations per socket , &gt;&gt; and you have significant contention . 4 could be severely &gt;&gt; handicapped . On the 51xx series , you have a maximum of about 7 GB/s &gt;&gt; for 1 or 2 sockets . You can hit about 6 @ @ @ @ @ @ @ @ @ @ 2 socket system , this means that 3 other cores need to be &gt;&gt; working out of cache , or idling in order not to contend with the &gt;&gt; original thread . Since the 51xx series does n't go beyond 2 sockets &gt;&gt; this is an annoyance , but hopefully not too upsetting . &gt;&gt; &gt;&gt; Its when you get to 4 cores per socket that this memory bandwidth &gt;&gt; issue starts getting debilitating . &gt;&gt; &gt;&gt; Of course memory bandwidth is n't the only issue . Any shared resource &gt;&gt; has a usually finite extent . When you consume it all with N threads , &gt;&gt; as you scale to N+M you should expect to see contention and other &gt;&gt; issues intrude on the performance . &gt;&gt; &gt;&gt; Quad cores will be good for some codes and systems . Your mileage may &gt;&gt; vary . We will try to do some studies as soon as we get some to play &gt;&gt; with . &gt;&gt; &gt;&gt; Joe &gt;&gt; &gt;&gt;&gt; On Sep 6 , 2006 , at 9:48 AM , Jeff Pummill wrote : &gt;&gt;&gt;&gt; How does this affect the way @ @ @ @ @ @ @ @ @ @ ... say with LSF on a system utilizing nodes with dual &gt;&gt;&gt;&gt; Woodcrest cpu 's ? It seems as though communication and thruput &gt;&gt;&gt;&gt; become even more important considerations when setting up your &gt;&gt;&gt;&gt; queues when dual ( or quad ) cores are implemented . &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; Some of our apps are cpu intensive , others are very " chatty " , and &gt;&gt;&gt;&gt; all run for days if not weeks on large numbers of cpu's. &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; Certainly better profiling of the apps will be in order , but has &gt;&gt;&gt;&gt; anyone in the Rocks community seen a need to substantially &gt;&gt;&gt;&gt; restructure the way that they structure their job submissions due &gt;&gt;&gt;&gt; to both enhancements AND restrictions of the new dual core products ? &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; Thanks , &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; Jeff F. Pummill &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; Joe Landman wrote : &gt;&gt;&gt;&gt;&gt; Donald Tripp wrote : &gt;&gt;&gt;&gt;&gt;&gt; Also , keep in mind the fact that Intel will be releasing a quad &gt;&gt;&gt;&gt;&gt;&gt; core chip in late 06 , effectively cutting the size of your &gt;&gt;&gt;&gt;&gt;&gt; cluster in half . &gt;&gt;&gt;&gt;&gt; Be careful about this , as there are @ @ @ @ @ @ @ @ @ @ want to be running a code that uses the &gt;&gt;&gt;&gt;&gt; full bandwidth of the memory system per core to be running on a &gt;&gt;&gt;&gt;&gt; multi-core system . Quad cores should be great for a number of &gt;&gt;&gt;&gt;&gt; codes that do n't beat on memory bandwidth . &gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt;&gt; With quad cores you will be able to get 8 CPU cores per 1U unit . &gt;&gt;&gt;&gt;&gt; With ( on average ) 1/4 of the memory bandwidth . If this wo n't &gt;&gt;&gt;&gt;&gt; impact your code , great . ( Monte Carlo , et al ) . If you have a &gt;&gt;&gt;&gt;&gt; code which is bandwidth sensitive , it will be impacted in a multi- &gt;&gt;&gt;&gt;&gt; core run . &gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt;&gt; That said , we did n't see much impact on real codes with real &gt;&gt;&gt;&gt;&gt; input decks when we ran cases last year : See http : // &gt;&gt;&gt;&gt;&gt; LONG ... &gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt; -------------- next part -------------- &gt;&gt;&gt; An HTML attachment was scrubbed ... &gt;&gt;&gt; URL : &gt;&gt;&gt; LONG ... &gt;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; -- &gt;&gt; Joseph Landman , Ph.D &gt;&gt; Founder and CEO &gt;&gt; Scalable Informatics LLC , &gt;&gt; email @ @ @ @ @ @ @ @ @ @ &gt;&gt; phone : +1 734 786 8423 &gt;&gt; fax : +1 734 786 8452 or +1 866 888 3112 &gt;&gt; cell : +1 734 612 4615 &gt;&gt; &gt; From greg.bruno at gmail.com Wed Sep 6 16:07:57 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : We 'd , 6 Sep 2006 16:07:57 -0700 Subject : Rocks-Discuss default " replace " partition not working In-Reply-To : References : Message-ID : On 9/6/06 , MargaretGreaney wrote : &gt; hello , &gt; &gt; I 've found something that may help with my problem . &gt; &gt; If I use the example in the documentation where you want to change the &gt; default partitioning with the **26;10675;TOOLONG , &gt; with contents : &gt; &gt; &gt; / --size 8000 --ondisk hda &gt; swap --size 1000 --ondisk hda &gt; /mydata --size 1 --grow --ondisk hda &gt; &gt; &gt; and then do a rocks-dist dist &gt; &gt; then put the node back to a compute node membership &gt; &gt; mysql&gt; select * from nodes ; &gt; LONG ... &gt; I 'd Site Name Membership CPUs Rack Rank Comment &gt; LONG ... &gt; 1 0 fnpt135 1 @ @ @ @ @ @ @ @ @ @ NULL NULL NULL &gt; 5 0 fnd0552 15 1 1 2 NULL &gt; LONG ... &gt; 3 rows in set ( 0.00 sec ) &gt; &gt; mysql&gt; select * from partitions ; &gt; Empty set ( 0.00 sec ) &gt; &gt; &gt; then dbreport kickstart fnpt131 more &gt; &gt; -snip-- &gt; install &gt; reboot &gt; rootpw --iscrypted jwZm45OJKdp4s &gt; url --url LONG ... &gt; zerombr yes &gt; bootloader --location=mbr &gt; timezone America/Chicago &gt; part / --size 8000 --ondisk hda &gt; part swap --size 1000 --ondisk hda &gt; part /mydata --size 1 --grow --ondisk hda &gt; &gt; &gt; --snip shows partition changed to replacement example partitioning , &gt; &gt; then install , I still get an error " Choose a disk to run fdisk on " &gt; &gt; Here I am not doing anything custom and am just following documentation . &gt; &gt; Do you know why I would be getting this error ? &gt; On the node I am installing , when I go into the shell and do fdisk -l /dev/hda or hdb &gt; there is nothing there . I have previously cleared the disks with fdisk. &gt; I @ @ @ @ @ @ @ @ @ @ in the &gt; partitions table for this node . what happens when you move **26;10703;TOOLONG out from under **27;10731;TOOLONG to /tmp , rebuild the distro , then try to reinstall a compute node . this will test if the problem is with **26;10760;TOOLONG - gb From philip.papadopoulos at gmail.com Wed Sep 6 17:06:58 2006 From : philip.papadopoulos at gmail.com ( Philip Papadopoulos ) Date : We 'd , 6 Sep 2006 17:06:58 -0700 Subject : Rocks-Discuss Hardware In-Reply-To : **36;10788;TOOLONG References : LONG ... LONG ... LONG ... **33;10826;TOOLONG LONG ... LONG ... **36;10861;TOOLONG Message-ID : LONG ... 10G is a $$ challenge at larger port counts . Myri10G Copper LONG ... are $800ea . The nice thing about Myrinet 10G is that is speaks myrinet protocol or 10GigE protocol depending on your switch . Force10 has a 24-port cut through switch ( S2410 ) LONG ... is $20K according to LONG ... So for a 24 node cluster , you are at 40K ( $1700/node list ) . Which is fairly expensive . When you go larger , then you start to be eaten up by 10GigE switch costs . @ @ @ @ @ @ @ @ @ @ as it used to be , but still pricey . -P On 9/6/06 , Jeffrey B. Layton wrote : &gt; &gt; While not quite the forum .... there are a number of companies &gt; working on 10Gig cards and there are several that are shipping . &gt; &gt; To go along with that there are companies that have 10Gig switches &gt; as well . From very small port counts to large port counts . &gt; &gt; But , the end result is that 10Gig is still expensive . The NICs are &gt; still a bit pricey and the switches get really pricey when you &gt; get larger . &gt; &gt; My guess is that at low port counts 10Gig would be about &gt; $1200-$1400 a port and for larger counts you are over &gt; $4500 or even $8000 a port pretty easily . &gt; &gt; Jeff &gt; &gt; &gt; My apologies . I had one too many zeros regarding the ethernet cards . &gt; &gt; Should have been 10gbit ethernet . Foundry Networks , as well as others , &gt; &gt; are working on 10gbit ethernet cards . I 've @ @ @ @ @ @ @ @ @ @ magazines . &gt; &gt; &gt; &gt; - Don &gt; &gt; &gt; &gt; On Sep 6 , 2006 , at 10:34 AM , Joe Landman wrote : &gt; &gt; &gt; &gt;&gt; Donald Tripp wrote : &gt; &gt;&gt;&gt; The more cpu cores you put in a single machine , the more important &gt; &gt;&gt;&gt; your interconnect becomes . Myricom produces a 10gbit version of &gt; &gt;&gt; &gt; &gt;&gt; Actually the more important reducing contention and latency becomes . &gt; &gt;&gt; &gt; &gt;&gt;&gt; Myrinet . I 've heard rumors of 100gbit ethernet cards . &gt; &gt;&gt; &gt; &gt;&gt; Last I heard this was not a standard or even in the process of &gt; &gt;&gt; becoming one . &gt; &gt;&gt; &gt; &gt;&gt; Some of the machines we have recently finished building or bidding on &gt; &gt;&gt; have DDR 4x IB in them . This will get you about aggregate 3.2 GB/s &gt; &gt;&gt; ( 1.6 GB/s up and down ) . As this is fairly close to what a well &gt; &gt;&gt; engineered program can feed to the CPU ( AMD or Intel flavor ) these &gt; &gt;&gt; days , this is a good @ @ @ @ @ @ @ @ @ @ &gt; &gt;&gt; for latency , and this is why things like Infinipath are so good , but &gt; &gt;&gt; InfiniPath is n't DDR now . &gt; &gt;&gt; &gt; &gt;&gt; ( I do n't work for Qlogic/PathScale , or own stock in them/Qlogic . We &gt; &gt;&gt; do put InfiniPath in some clusters we have sold , great stuff , &gt; &gt;&gt; hopefully DDR soon ) &gt; &gt;&gt; &gt; &gt;&gt;&gt; You will have to be very careful at to what apps you allow to use &gt; &gt;&gt;&gt; all the cores of a machine . If you experience bandwidth troubles , &gt; &gt;&gt;&gt; try using only half the cpus in a machine . I know this may sound &gt; &gt;&gt;&gt; wasteful , but the communication wait time is wasteful as well , and &gt; &gt;&gt; &gt; &gt;&gt; This is exactly the contention issue I was indicating in the other &gt; &gt;&gt; message . Contention increases latency and jitter . This is a bad &gt; &gt;&gt; thing for latency sensitive apps . This is also a good argument for &gt; &gt;&gt; some single core nodes , or multiple busses connected to local CPUs @ @ @ @ @ @ @ @ @ @ &gt; &gt;&gt; &gt; &gt;&gt;&gt; the longer that is , the long the CPU sits anyway . I do n't have any &gt; &gt;&gt;&gt; specs on that , but I 've seen it happen when an interconnect gets &gt; &gt;&gt;&gt; saturated . &gt; &gt;&gt; &gt; &gt;&gt; I have measurements we did for a customer that beautifully &gt; &gt;&gt; illustrates why latency and contention are so terribly important in a &gt; &gt;&gt; real code . &gt; &gt;&gt; &gt; &gt;&gt;&gt; If your users are running CPU intensive programs , then dual or &gt; &gt;&gt;&gt; " quad " quad cores would expedite your jobs . However , if you had , in &gt; &gt;&gt;&gt; theory , 8-16 cores in a machine , you would need 4x that in ram ... &gt; &gt;&gt; &gt; &gt;&gt; And the resources would have 4x the contention . So if you have a &gt; &gt;&gt; program which uses 1.2 GB/s of memory bandwidth now , and it scales &gt; &gt;&gt; nicely on an SMP with each additional thread consuming 1.2 GB/s , 8 &gt; &gt;&gt; threads on a dual socket 4 core system will consume about 9.6 GB/s. &gt; @ @ @ @ @ @ @ @ @ @ that the cores &gt; &gt;&gt; can each use the memory when they want to and there is no &gt; &gt;&gt; contention . This does n't normally happen . &gt; &gt;&gt; &gt; &gt;&gt; Each AMD socket can hit about 6 GB/s sustained in stream like &gt; &gt;&gt; operations . All you need are 2 stream like operations per socket , &gt; &gt;&gt; and you have significant contention . 4 could be severely &gt; &gt;&gt; handicapped . On the 51xx series , you have a maximum of about 7 GB/s &gt; &gt;&gt; for 1 or 2 sockets . You can hit about 6 GB/s on one thread . If you &gt; &gt;&gt; have a 2 socket system , this means that 3 other cores need to be &gt; &gt;&gt; working out of cache , or idling in order not to contend with the &gt; &gt;&gt; original thread . Since the 51xx series does n't go beyond 2 sockets &gt; &gt;&gt; this is an annoyance , but hopefully not too upsetting . &gt; &gt;&gt; &gt; &gt;&gt; Its when you get to 4 cores per socket that this memory bandwidth &gt; &gt;&gt; issue starts @ @ @ @ @ @ @ @ @ @ bandwidth is n't the only issue . Any shared resource &gt; &gt;&gt; has a usually finite extent . When you consume it all with N threads , &gt; &gt;&gt; as you scale to N+M you should expect to see contention and other &gt; &gt;&gt; issues intrude on the performance . &gt; &gt;&gt; &gt; &gt;&gt; Quad cores will be good for some codes and systems . Your mileage may &gt; &gt;&gt; vary . We will try to do some studies as soon as we get some to play &gt; &gt;&gt; with . &gt; &gt;&gt; &gt; &gt;&gt; Joe &gt; &gt;&gt; &gt; &gt;&gt;&gt; On Sep 6 , 2006 , at 9:48 AM , Jeff Pummill wrote : &gt; &gt;&gt;&gt;&gt; How does this affect the way that one sets up scheduling for large &gt; &gt;&gt;&gt;&gt; parallel jobs ... say with LSF on a system utilizing nodes with dual &gt; &gt;&gt;&gt;&gt; Woodcrest cpu 's ? It seems as though communication and thruput &gt; &gt;&gt;&gt;&gt; become even more important considerations when setting up your &gt; &gt;&gt;&gt;&gt; queues when dual ( or quad ) cores are implemented . &gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt; Some of our apps are @ @ @ @ @ @ @ @ @ @ and &gt; &gt;&gt;&gt;&gt; all run for days if not weeks on large numbers of cpu's. &gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt; Certainly better profiling of the apps will be in order , but has &gt; &gt;&gt;&gt;&gt; anyone in the Rocks community seen a need to substantially &gt; &gt;&gt;&gt;&gt; restructure the way that they structure their job submissions due &gt; &gt;&gt;&gt;&gt; to both enhancements AND restrictions of the new dual core products ? &gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt; Thanks , &gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt; Jeff F. Pummill &gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt; Joe Landman wrote : &gt; &gt;&gt;&gt;&gt;&gt; Donald Tripp wrote : &gt; &gt;&gt;&gt;&gt;&gt;&gt; Also , keep in mind the fact that Intel will be releasing a quad &gt; &gt;&gt;&gt;&gt;&gt;&gt; core chip in late 06 , effectively cutting the size of your &gt; &gt;&gt;&gt;&gt;&gt;&gt; cluster in half . &gt; &gt;&gt;&gt;&gt;&gt; Be careful about this , as there are real concerns over resource &gt; &gt;&gt;&gt;&gt;&gt; contention . You do n't want to be running a code that uses the &gt; &gt;&gt;&gt;&gt;&gt; full bandwidth of the memory system per core to be running on a &gt; &gt;&gt;&gt;&gt;&gt; multi-core system . Quad cores should @ @ @ @ @ @ @ @ @ @ do n't beat on memory bandwidth . &gt; &gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt;&gt; With quad cores you will be able to get 8 CPU cores per 1U unit . &gt; &gt;&gt;&gt;&gt;&gt; With ( on average ) 1/4 of the memory bandwidth . If this wo n't &gt; &gt;&gt;&gt;&gt;&gt; impact your code , great . ( Monte Carlo , et al ) . If you have a &gt; &gt;&gt;&gt;&gt;&gt; code which is bandwidth sensitive , it will be impacted in a multi- &gt; &gt;&gt;&gt;&gt;&gt; core run . &gt; &gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt;&gt;&gt; That said , we did n't see much impact on real codes with real &gt; &gt;&gt;&gt;&gt;&gt; input decks when we ran cases last year : See http : // &gt; &gt;&gt;&gt;&gt;&gt; LONG ... &gt; &gt;&gt;&gt;&gt;&gt; &gt; &gt;&gt;&gt; -------------- next part -------------- &gt; &gt;&gt;&gt; An HTML attachment was scrubbed ... &gt; &gt;&gt;&gt; URL : &gt; &gt;&gt;&gt; &gt; LONG ... &gt; &gt;&gt;&gt; &gt; &gt;&gt; &gt; &gt;&gt; &gt; &gt;&gt; -- &gt; &gt;&gt; Joseph Landman , Ph.D &gt; &gt;&gt; Founder and CEO &gt; &gt;&gt; Scalable Informatics LLC , &gt; &gt;&gt; email : landman at scalableinformatics.com &gt; &gt;&gt; web : http : **29;10926;TOOLONG &gt; &gt;&gt; phone : @ @ @ @ @ @ @ @ @ @ 786 8452 or +1 866 888 3112 &gt; &gt;&gt; cell : +1 734 612 4615 &gt; &gt;&gt; &gt; &gt; &gt; &gt; -- Philip Papadopoulos , PhD University of California , San Diego 858-822-3628 -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From lijunliu2005 at gmail.com Wed Sep 6 19:05:45 2006 From : lijunliu2005 at gmail.com ( Lijun Liu ) Date : We 'd , 6 Sep 2006 19:05:45 -0700 Subject : Rocks-Discuss About the minimum hardware requirements for Rocks Message-ID : Hi all , I am trying to install a cluster on 16 nodes with 256MB memory ( I do n't know the cpu info yet ) , which version do you recommend for me ? Is there a way to find out the Users Guide for the older version ? Thanks a lot for your response in advance . -- Best regards , Lijun Liu From lijunliu2005 at gmail.com Wed Sep 6 18:40:36 2006 From : lijunliu2005 at gmail.com ( Lijun Liu ) Date : We 'd , 6 Sep 2006 18:40:36 -0700 Subject : Rocks-Discuss Minimum hardware requirement for Rocks Message-ID : Hi all @ @ @ @ @ @ @ @ @ @ nodes with 256MB memory ( I do n't know the cpu info yet ) , which version do you recommend for me ? Is there a way to find out the Users Guide for the older version ? Thanks a lot for your response in advance . -- Best regards , Lijun Liu From fernando.nino at free.fr Thu Sep 7 04:49:23 2006 From : fernando.nino at free.fr ( Fernando NINO ) Date : Thu , 7 Sep 2006 13:49:23 +0200 Subject : Rocks-Discuss Problem booting rocks 4.1 on new Opterons dual-core In-Reply-To : References : LONG ... Message-ID : LONG ... Hello , I did try the grub installation and it did not work . After further fiddling with all the stuff , I noticed it was a much simpler error than I expected : grub installed ok , but it wo n't boot on the Centos 2.6.9-34 . ELsmp kernel . When booting with the uni-processor kernel everything was right ..... Of course , uni-processor is not an option for a dual-core bi-processor , but at least now I know I must upgrade the kernel . A have , @ @ @ @ @ @ @ @ @ @ you can replace the CentOS distribution for any other ( including redhat ) distribution . How do you do this , is it a matter of building the OS roll as any other roll ? Or is there a particular procedure to respect ? Thanks , Fernando Le 6 sept. 2006 ? 00:55 , Greg Bruno a ? crit : &gt; On 9/5/06 , Fernando NINO wrote : &gt;&gt; hello , &gt;&gt; &gt;&gt; I have successfully installed and used for some months the 4.1 &gt;&gt; rocks distribution and just decided to add new compute nodes . The &gt;&gt; new hardware has an Adaptec 2420 RAID controller ; network &gt;&gt; installation goes through with success , but nodes never manage to &gt;&gt; boot . On debugging I noticed installation correctly loaded the &gt;&gt; " ' aacraid " driver needed to detect the two hard disks on the machine , &gt;&gt; and had no problems . On booting , the new hardware manages to use &gt;&gt; GRUB ( splash screen , usual kernel selection etc ... ) , but never gets &gt;&gt; to boot , just says loading initrd and then halts @ @ @ @ @ @ @ @ @ @ the aacraid driver or something of the &gt;&gt; sort .... If I install a standard ScientificLinux distribution on it &gt;&gt; ( another RHEL4 based one ) , the machine boots up fine . &gt; &gt; it may be a grub installation issue . &gt; &gt; you can try the following procedure on your compute node ( the &gt; procedure below discusses how to run it on a frontend , but just &gt; substitute the word ' compute ' for ' frontend ' ) . &gt; &gt; LONG ... &gt; **25;10957;TOOLONG &gt; &gt; let us know how it goes . &gt; &gt; - gb &gt; From cozzolongo at planetek.it Thu Sep 7 05:53:38 2006 From : cozzolongo at planetek.it ( cozzolongo ) Date : Thu , 07 Sep 2006 14:53:38 +0200 Subject : Rocks-Discuss Scsi Reservation on vmWare under rocks Message-ID : LONG ... I 'm tryng to create a virtual cluster using rocks under vmWare ( Windows version ) ; is it possible to use scsi reservation to emulate a SAN system ? I hope in your reply ... From debbiet at arlut.utexas.edu Thu Sep 7 07:26:18 2006 From : @ @ @ @ @ @ @ @ @ @ , 7 Sep 2006 09:26:18 -0500 Subject : Rocks-Discuss Iptables hangs at shutdown when shooting nodes In-Reply-To : LONG ... References : LONG ... Message-ID : LONG ... Hello - OK . Yesterday 10 out of 10 compute nodes hung on this after a cluster-fork kickstart . So a followup question : Is iptables required to be running on the compute nodes ? We are using NAT , so do n't want to lose that functionality ( and will keep iptables up on the frontend ) , but if it 's not needed on the compute nodes I 'd like to remove it from my distribution so that it does n't keep causing us these problems . And before anyone comments about turning off the firewall capabilities , my cluster is on a separate network that is not connected to any other network , much less the internet . Thanks , Debbie " clearly not an expert about iptables " On Tue , Sep 05 , 2006 at 08:20:00PM -0500 , Debbie Tropiano wrote : &gt; Hello - &gt; &gt; I 've been having problems lately when shooting my compute @ @ @ @ @ @ @ @ @ @ iptables " ) after the kickstart . &gt; Is this a known problem ? And more importantly is there a work around ? &gt; &gt; Today it happened on 6 of 10 nodes and power-cycling is the only fix . &gt; &gt; Thanks in advance for any help , &gt; Debbie &gt; -- &gt; Debbie Tropiano debbiet at arlut.utexas.edu &gt; Environmental Sciences Laboratory +1 512 835 3367 w &gt; Applied Research Laboratories of UT Austin +1 512 835 3544 fax &gt; P.O. Box 8029 , Austin , TX 78713-8029 home email : debbie at icus.com -- Debbie Tropiano debbiet at arlut.utexas.edu Environmental Sciences Laboratory +1 512 835 3367 w Applied Research Laboratories of UT Austin +1 512 835 3544 fax P.O. Box 8029 , Austin , TX 78713-8029 home email : debbie at icus.com From mhallock at uiuc.edu Thu Sep 7 07:50:00 2006 From : mhallock at uiuc.edu ( Mike Hallock ) Date : Thu , 7 Sep 2006 09:50:00 -0500 Subject : Rocks-Discuss Iptables hangs at shutdown when shooting nodes In-Reply-To : LONG ... References : LONG ... LONG ... Message-ID : LONG ... iptables on the compute nodes @ @ @ @ @ @ @ @ @ @ if you only are using eth0 on them then the rules are pretty much a no-op . While I find it strange that the nodes hang at this point ofthe shutdown and may be a symptom of a larger problem , I would say that you would n't be any worse off security-wise without it . And NAT is handled completely by the frontend , so any changes to the compute nodes should n't affect that . -mike On Thu , Sep 07 , 2006 at 09:26:18AM -0500 , Debbie Tropiano wrote : &gt; Hello - &gt; &gt; OK . Yesterday 10 out of 10 compute nodes hung on this &gt; after a cluster-fork kickstart . So a followup question : &gt; &gt; Is iptables required to be running on the compute nodes ? &gt; We are using NAT , so do n't want to lose that functionality &gt; ( and will keep iptables up on the frontend ) , but if it 's &gt; not needed on the compute nodes I 'd like to remove it from &gt; my distribution so that it does n't keep causing us these @ @ @ @ @ @ @ @ @ @ turning off the firewall &gt; capabilities , my cluster is on a separate network that is &gt; not connected to any other network , much less the internet. &gt; &gt; Thanks , &gt; Debbie " clearly not an expert about iptables " &gt; &gt; On Tue , Sep 05 , 2006 at 08:20:00PM -0500 , Debbie Tropiano wrote : &gt; &gt; Hello - &gt; &gt; &gt; &gt; I 've been having problems lately when shooting my compute nodes that &gt; &gt; iptables hangs at shutdown ( " Stopping iptables " ) after the kickstart . &gt; &gt; Is this a known problem ? And more importantly is there a work around ? &gt; &gt; &gt; &gt; Today it happened on 6 of 10 nodes and power-cycling is the only fix . &gt; &gt; &gt; &gt; Thanks in advance for any help , &gt; &gt; Debbie &gt; &gt; -- &gt; &gt; Debbie Tropiano debbiet at arlut.utexas.edu &gt; &gt; Environmental Sciences Laboratory +1 512 835 3367 w &gt; &gt; Applied Research Laboratories of UT Austin +1 512 835 3544 fax &gt; &gt; P.O. Box 8029 , Austin , TX 78713-8029 home email @ @ @ @ @ @ @ @ @ @ debbiet at arlut.utexas.edu &gt; Environmental Sciences Laboratory +1 512 835 3367 w &gt; Applied Research Laboratories of UT Austin +1 512 835 3544 fax &gt; P.O. Box 8029 , Austin , TX 78713-8029 home email : debbie at icus.com From mgreaney at fnal.gov Thu Sep 7 08:57:48 2006 From : mgreaney at fnal.gov ( MargaretGreaney ) Date : Thu , 07 Sep 2006 10:57:48 -0500 ( CDT ) Subject : Rocks-Discuss default " replace " partition not working In-Reply-To : References : Message-ID : When I move the **26;10984;TOOLONG to a saved name , then run rocks-dist dist and install the plain compute node appliance , the node installs fine with no problems . So trying this again , and with these contents for **26;11012;TOOLONG root at fnpt135 nodes# cat **26;11040;TOOLONG A skeleton XML node file . This file is a template and is intended as an example of how to customize your Rocks cluster . Kickstart XML nodes such as this describe packages and " post installation " shell scripts for your cluster . XML files in the site-nodes/ directory should be named either " extend-name.xml " or " @ @ @ @ @ @ @ @ @ @ existing xml node . If your node is prefixed with replace , its instructions will be used instead of the official node 's . If it is named extend , its directives will be concatenated to the end of the official node . / --size 8000 --ondisk hda swap --size 1000 --ondisk hda /mydata --size 1 --grow --ondisk hda run rocks-dist dist run dbreport kickstart fnpt131 to see if the part info changed and it did ran nukeit.sh on fnpt131 ran **37;11068;TOOLONG on fnpt131 then install began and saw this Error Partitioning Could not allocate requested partitions : Unsatisfied partition request New Part Request --mountpoint : None unique I 'd : 6 type : physical volume ( LVM ) format : 1 badblocks:None device : None drive : ' hda ' primary : None size : 0 grow : 1 maxsize : None start : None end : None migrate : None origfstype : None Press " ok " to reboot your system ok then did this rebooted and then said grub , booting rocks reinstall starts installing , then get exactly the same error message as above . Now trying @ @ @ @ @ @ @ @ @ @ do this I see that the partition info is on the hda disk . Using fdisk to remove it via the f2 and shell . d 3 , d 2 , d 1 , w syncing disks exit &amp; reboot now get grub Error 22 so reboot and select net card Now it gets to the point where it will partition and it now says " Disk Setup " " Choose a disk to run fdisk on " hda hdb ok Edit Back The anaconda does not allow me to select the ok , so I have to reboot . Before this I go to F2 to the shell to see what the output of fdisk /dev/hda and /dev/hdb are , and there are no partitions listed . I do n't think that I missed anything from the manual and would appreciate any help to fix this . thanks , Margaret From greg.bruno at gmail.com Thu Sep 7 09:12:41 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : Thu , 7 Sep 2006 09:12:41 -0700 Subject : Rocks-Discuss default " replace " partition not working In-Reply-To : @ @ @ @ @ @ @ @ @ @ &gt; &gt; &gt; When I move the **26;11107;TOOLONG to a saved name , then &gt; run rocks-dist dist and install the plain compute node appliance , the &gt; node installs fine with no problems . &gt; &gt; So trying this again , and with these contents for &gt; **26;11135;TOOLONG &gt; root at fnpt135 nodes# cat **26;11163;TOOLONG &gt; &gt; &gt; &gt; &gt; &gt; &gt; A skeleton XML node file . This file is a template and is intended &gt; as an example of how to customize your Rocks cluster . Kickstart &gt; XML &gt; nodes such as this describe packages and " post installation " shell &gt; scripts for your cluster . &gt; &gt; XML files in the site-nodes/ directory should be named either &gt; " extend-name.xml " or " replace-name.xml " , where name is &gt; the name of an existing xml node . &gt; &gt; If your node is prefixed with replace , its instructions will be &gt; used &gt; instead of the official node 's . If it is named extend , its &gt; directives will &gt; be concatenated to the end of the official node . &gt; @ @ @ @ @ @ @ @ @ @ 8000 --ondisk hda &gt; swap --size 1000 --ondisk hda &gt; /mydata &gt; &gt; &gt; &gt; &gt; remove the lines : and then try to install your node again . on a related note , in 4.2 we addresseed this bug by removing the lines above from skeleton.xml. - gb From greg.bruno at gmail.com Thu Sep 7 09:21:01 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : Thu , 7 Sep 2006 09:21:01 -0700 Subject : Rocks-Discuss Problem booting rocks 4.1 on new Opterons dual-core In-Reply-To : LONG ... References : LONG ... LONG ... Message-ID : On 9/7/06 , Fernando NINO wrote : &gt; Hello , &gt; &gt; I did try the grub installation and it did not work . After &gt; further fiddling with all the stuff , I noticed it was a much simpler &gt; error than I expected : grub installed ok , but it wo n't boot on the &gt; Centos 2.6.9-34 . ELsmp kernel . When booting with the uni-processor &gt; @ @ @ @ @ @ @ @ @ @ not an &gt; option for a dual-core bi-processor , but at least now I know I must &gt; upgrade the kernel. &gt; &gt; A have , though a simple question ; in the documentation it says you &gt; can replace the CentOS distribution for any other ( including redhat ) &gt; distribution . How do you do this , is it a matter of building the OS &gt; roll as any other roll ? Or is there a particular procedure to respect ? before you do that , let 's try upgrading the kernel that will be installed on the compute nodes . try : # cd **33;11191;TOOLONG # wget LONG ... then rebuild your distro : # cd /home/install # rocks-dist dist then install one of your compute nodes . let us know how it goes . - gb From debbiet at arlut.utexas.edu Thu Sep 7 09:26:33 2006 From : debbiet at arlut.utexas.edu ( Debbie Tropiano ) Date : Thu , 7 Sep 2006 11:26:33 -0500 Subject : Rocks-Discuss Iptables hangs at shutdown when shooting nodes In-Reply-To : LONG ... References : LONG ... LONG ... LONG ... Message-ID @ @ @ @ @ @ @ @ @ @ help . I 'll pull iptables from my compute node distribution then . BTW I did see that others running CentOS have also had this problem but their fix was to upgrade to CentOS 4.2 ( which my /etc/redhat-release file says that I 'm running ) . See the URL LONG ... Thanks again , Debbie On Thu , Sep 07 , 2006 at 09:50:00AM -0500 , Mike Hallock wrote : &gt; iptables on the compute nodes do n't do much of anything ... In fact , if &gt; you only are using eth0 on them then the rules are pretty much a no-op. &gt; While I find it strange that the nodes hang at this point ofthe shutdown &gt; and may be a symptom of a larger problem , I would say that you would n't &gt; be any worse off security-wise without it . &gt; &gt; And NAT is handled completely by the frontend , so any changes to the &gt; compute nodes should n't affect that . &gt; &gt; -mike &gt; &gt; On Thu , Sep 07 , 2006 at 09:26:18AM -0500 , Debbie Tropiano wrote : @ @ @ @ @ @ @ @ @ @ Yesterday 10 out of 10 compute nodes hung on this &gt; &gt; after a cluster-fork kickstart . So a followup question : &gt; &gt; &gt; &gt; Is iptables required to be running on the compute nodes ? &gt; &gt; We are using NAT , so do n't want to lose that functionality &gt; &gt; ( and will keep iptables up on the frontend ) , but if it 's &gt; &gt; not needed on the compute nodes I 'd like to remove it from &gt; &gt; my distribution so that it does n't keep causing us these &gt; &gt; problems . &gt; &gt; &gt; &gt; And before anyone comments about turning off the firewall &gt; &gt; capabilities , my cluster is on a separate network that is &gt; &gt; not connected to any other network , much less the internet. &gt; &gt; &gt; &gt; Thanks , &gt; &gt; Debbie " clearly not an expert about iptables " &gt; &gt; &gt; &gt; On Tue , Sep 05 , 2006 at 08:20:00PM -0500 , Debbie Tropiano wrote : &gt; &gt; &gt; Hello - &gt; &gt; &gt; &gt; &gt; &gt; I 've been @ @ @ @ @ @ @ @ @ @ &gt; &gt; iptables hangs at shutdown ( " Stopping iptables " ) after the kickstart . &gt; &gt; &gt; Is this a known problem ? And more importantly is there a work around ? &gt; &gt; &gt; &gt; &gt; &gt; Today it happened on 6 of 10 nodes and power-cycling is the only fix . &gt; &gt; &gt; &gt; &gt; &gt; Thanks in advance for any help , &gt; &gt; &gt; Debbie &gt; &gt; &gt; -- &gt; &gt; &gt; Debbie Tropiano debbiet at arlut.utexas.edu &gt; &gt; &gt; Environmental Sciences Laboratory +1 512 835 3367 w &gt; &gt; &gt; Applied Research Laboratories of UT Austin +1 512 835 3544 fax &gt; &gt; &gt; P.O. Box 8029 , Austin , TX 78713-8029 home email : debbie at icus.com &gt; &gt; &gt; &gt; -- &gt; &gt; Debbie Tropiano debbiet at arlut.utexas.edu &gt; &gt; Environmental Sciences Laboratory +1 512 835 3367 w &gt; &gt; Applied Research Laboratories of UT Austin +1 512 835 3544 fax &gt; &gt; P.O. Box 8029 , Austin , TX 78713-8029 home email : debbie at icus.com -- Debbie Tropiano debbiet at arlut.utexas.edu Environmental Sciences Laboratory +1 512 835 3367 @ @ @ @ @ @ @ @ @ @ 3544 fax P.O. Box 8029 , Austin , TX 78713-8029 home email : debbie at icus.com From mgreaney at fnal.gov Thu Sep 7 09:54:46 2006 From : mgreaney at fnal.gov ( MargaretGreaney ) Date : Thu , 07 Sep 2006 11:54:46 -0500 ( CDT ) Subject : Rocks-Discuss default " replace " partition not working In-Reply-To : References : Message-ID : Greg , This did not work for me , so maybe there is something else wrong . I edited out the packages lines in the **26;11226;TOOLONG and saved the file . I still get the " choose a disk " message . Trying again , I put back on the plain compute install , then put back the updated **26;11254;TOOLONG and ran rocks-dist dist , removed the partition info in the db , rock-partition --delete --nodename fnpt131 , ran nuke.sh , cluster-kickstart-pxe and get the same long error I sent you before . I go into the shell , use fdisk to list hda and remove it and save it and reboot , then get again the " choose a disk " message . Margaret -- Margaret Greaney Telephone @ @ @ @ @ @ @ @ @ @ Thu , 7 Sep 2006 , Greg Bruno wrote : &gt; On 9/7/06 , MargaretGreaney wrote : &gt; &gt; &gt; &gt; &gt; &gt; When I move the **26;11282;TOOLONG to a saved name , then &gt; &gt; run rocks-dist dist and install the plain compute node appliance , the &gt; &gt; node installs fine with no problems . &gt; &gt; &gt; &gt; So trying this again , and with these contents for &gt; &gt; **26;11310;TOOLONG &gt; &gt; root at fnpt135 nodes# cat **26;11338;TOOLONG &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; A skeleton XML node file . This file is a template and is intended &gt; &gt; as an example of how to customize your Rocks cluster . Kickstart &gt; &gt; XML &gt; &gt; nodes such as this describe packages and " post installation " shell &gt; &gt; scripts for your cluster . &gt; &gt; &gt; &gt; XML files in the site-nodes/ directory should be named either &gt; &gt; " extend-name.xml " or " replace-name.xml " , where name is &gt; &gt; the name of an existing xml node . &gt; &gt; &gt; &gt; If @ @ @ @ @ @ @ @ @ @ be &gt; &gt; used &gt; &gt; instead of the official node 's . If it is named extend , its &gt; &gt; directives will &gt; &gt; be concatenated to the end of the official node . &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; / --size 8000 --ondisk hda &gt; &gt; swap --size 1000 --ondisk hda &gt; &gt; /mydata &gt; &gt; &gt; &gt; &gt; remove the lines : &gt; &gt; &gt; &gt; &gt; &gt; and then try to install your node again . &gt; &gt; on a related note , in 4.2 we addresseed this bug by removing the lines &gt; above from skeleton.xml. &gt; &gt; - gb &gt; From jbecker at northwestern.edu Thu Sep 7 10:09:08 2006 From : jbecker at northwestern.edu ( Jesse Becker ) Date : Thu , 7 Sep 2006 12:09:08 -0500 @ @ @ @ @ @ @ @ @ @ This is on a newly installed Rocks 4.2 configuration . Looks like the ' sgemaster ' service is configured to start twice , late in the boot process : root at host etc# find /etc/rc* -name S*sgemaster -ls cut -c 68- **28;11366;TOOLONG -&gt; **26;11396;TOOLONG **28;11424;TOOLONG -&gt; .. /init.d/sgemaster **28;11454;TOOLONG -&gt; **26;11484;TOOLONG **28;11512;TOOLONG -&gt; .. /init.d/sgemaster None of the files appears to belong to an RPM either ( based on ' rpm -qf ' ) . -- Jesse Becker GPG-fingerprint : BD00 7AA4 4483 AFCC 82D0 2720 0083 0931 9A2B 06A2 -------------- next part -------------- A non-text attachment was scrubbed ... Name : not available Type : **25;11542;TOOLONG Size : 963 bytes Desc : not available Url : LONG ... From mhbucn at gmail.com Thu Sep 7 10:39:32 2006 From : mhbucn at gmail.com ( Manuel Herrera ) Date : Thu , 07 Sep 2006 13:39:32 -0400 Subject : Rocks-Discuss sk98lin Message-ID : **34;11569;TOOLONG Hi all , I 'm trying to install a frontend , but im having troubles with my ethernet cards . It ? s the DGE-530T network adapter . When i try to install nodes ( with the same @ @ @ @ @ @ @ @ @ @ . Any Idea ? Thanks From greg.bruno at gmail.com Thu Sep 7 10:58:59 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : Thu , 7 Sep 2006 10:58:59 -0700 Subject : Rocks-Discuss S95sgemaster and S99sgemaster In-Reply-To : LONG ... References : LONG ... Message-ID : On 9/7/06 , Jesse Becker wrote : &gt; This is on a newly installed Rocks 4.2 configuration . Looks like the &gt; ' sgemaster ' service is configured to start twice , late in the boot &gt; process : &gt; &gt; &gt; root at host etc# find /etc/rc* -name S*sgemaster -ls cut -c 68- &gt; **28;11605;TOOLONG -&gt; **26;11635;TOOLONG &gt; **28;11663;TOOLONG -&gt; .. /init.d/sgemaster &gt; **28;11693;TOOLONG -&gt; **26;11723;TOOLONG &gt; **28;11751;TOOLONG -&gt; .. /init.d/sgemaster &gt; &gt; None of the files appears to belong to an RPM either ( based on ' rpm -qf ' ) . wow , that 's an embarassing one -- thanks for the bug report . we 'll get it fixed in 4.2.1. - gb From greg.bruno at gmail.com Thu Sep 7 11:07:18 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : Thu , 7 Sep 2006 @ @ @ @ @ @ @ @ @ @ : **34;11817;TOOLONG Message-ID : On 9/7/06 , Manuel Herrera wrote : &gt; Hi all , &gt; I 'm trying to install a frontend , but im having troubles with my ethernet &gt; cards . &gt; It ? s the DGE-530T network adapter. &gt; When i try to install nodes ( with the same Ethernet card ) , the node ca n't &gt; find the frontend. &gt; Any Idea ? try directly connecting the compute node to the frontend. that is , take an ethernet cable and plug it into eth0 on the frontend , then plug the other end into eth0 on the compute node . then try to install the compute node . if this works , then you know you have switch configuration issue . - gb From greg.bruno at gmail.com Thu Sep 7 11:22:21 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : Thu , 7 Sep 2006 11:22:21 -0700 Subject : Rocks-Discuss default " replace " partition not working In-Reply-To : References : Message-ID : On 9/7/06 , MargaretGreaney wrote : &gt; Greg , &gt; &gt; This did not work for me , so maybe @ @ @ @ @ @ @ @ @ @ the packages lines in the **26;11853;TOOLONG and saved the &gt; file . I still get the " choose a disk " message . &gt; &gt; Trying again , I put back on the plain compute install , then put back the &gt; updated &gt; **26;11881;TOOLONG and ran rocks-dist dist , removed the partition &gt; info in the db , rock-partition --delete --nodename fnpt131 , ran nuke.sh , &gt; cluster-kickstart-pxe &gt; and get the same long error I sent you before . I go into the shell , use &gt; fdisk to list hda and remove it and save it and reboot , then get again &gt; the " choose a disk " message . do you have access to a stock 4.1 cluster ? that is , a cluster that has n't been modified with any XML files ? if so , then try adding your **26;11909;TOOLONG file to it and then see what happens . also , send us your **26;11937;TOOLONG file and we 'll try it out on one of our stock 4.1 systems and we 'll report back our findings . - gb From mjk at sdsc.edu @ @ @ @ @ @ @ @ @ @ ( mason j. katz ) Date : Thu , 7 Sep 2006 11:22:56 -0700 Subject : Rocks-Discuss Minimum hardware requirement for Rocks In-Reply-To : References : Message-ID : LONG ... The minimum RAM has been at least 512MB for several releases ( likely a couple of years ) , sorry . -mjk On Sep 6 , 2006 , at 06:40 P , Lijun Liu wrote : &gt; Hi all , &gt; &gt; I am trying to install a cluster on 16 nodes with 256MB memory ( I do n't &gt; know the cpu info yet ) , which version do you recommend for me ? &gt; &gt; Is there a way to find out the Users Guide for the older version ? &gt; &gt; Thanks a lot for your response in advance . &gt; &gt; -- &gt; Best regards , &gt; Lijun Liu -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... From greg.bruno at gmail.com Thu Sep 7 11:37:02 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : Thu , 7 Sep 2006 11:37:02 -0700 Subject : Rocks-Discuss Initial install of @ @ @ @ @ @ @ @ @ @ Message-ID : On 9/6/06 , Alexander , Aron B SrA SMC/WXTS wrote : &gt; Please pardon the late reply , I was out of town . I ran both commands on &gt; the head-node and the output is included in the attached text files . &gt; &gt; Thank you for your assistance . what is the output of : # ps auwx grep httpd if httpd is n't running , send us the output of : # service httpd start - gb From CStackpole at barbnet.com Thu Sep 7 11:39:42 2006 From : CStackpole at barbnet.com ( Stackpole , Chris ) Date : Thu , 7 Sep 2006 13:39:42 -0500 Subject : Rocks-Discuss Minimum hardware requirement for Rocks In-Reply-To : LONG ... Message-ID : I ran into this problem on a test system I once built . I pulled the ram from one of the nodes bumping the frontend to 512 , installed Rocks on the frontend , then moved the ram back to the node . I did not have any issues with doing that . Just a suggestion you might want to try . Chris Stackpole -----Original Message----- @ @ @ @ @ @ @ @ @ @ Of mason j. katz Sent : Thursday , September 07 , 2006 1:23 PM To : Lijun Liu Cc : npaci-rocks-discussion at sdsc.edu Subject : Re : Rocks-Discuss Minimum hardware requirement for Rocks The minimum RAM has been at least 512MB for several releases ( likely a couple of years ) , sorry . -mjk On Sep 6 , 2006 , at 06:40 P , Lijun Liu wrote : &gt; Hi all , &gt; &gt; I am trying to install a cluster on 16 nodes with 256MB memory ( I do n't &gt; know the cpu info yet ) , which version do you recommend for me ? &gt; &gt; Is there a way to find out the Users Guide for the older version ? &gt; &gt; Thanks a lot for your response in advance . &gt; &gt; -- &gt; Best regards , &gt; Lijun Liu -------------- next part -------------- An HTML attachment was scrubbed ... URL : LONG ... **29;12036;TOOLONG From mgreaney at fnal.gov Thu Sep 7 12:33:49 2006 From : mgreaney at fnal.gov ( MargaretGreaney ) Date : Thu , 07 Sep 2006 14:33:49 -0500 ( CDT ) @ @ @ @ @ @ @ @ @ @ In-Reply-To : References : Message-ID : Greg , I do n't have a 4.1 cluster that has been modified yet for Fermi distributions . I 've been trying with this release to stay as close to Rocks as possible . My current cluster has just the compute app and the one app that I 'm trying to get to work . I 'll send the xml in a separate mail . thank you , Margaret -- Margaret Greaney Telephone : 630-840-4623 Fermilab E-mail : mgreaney at fnal.gov CD/CSS/FCS On Thu , 7 Sep 2006 , Greg Bruno wrote : &gt; On 9/7/06 , MargaretGreaney wrote : &gt; &gt; Greg , &gt; &gt; &gt; &gt; This did not work for me , so maybe there is something else wrong . I edited &gt; &gt; out the packages lines in the **26;12067;TOOLONG and saved the &gt; &gt; file . I still get the " choose a disk " message . &gt; &gt; &gt; &gt; Trying again , I put back on the plain compute install , then put back the &gt; &gt; updated &gt; &gt; **26;12095;TOOLONG and ran rocks-dist dist , removed @ @ @ @ @ @ @ @ @ @ --delete --nodename fnpt131 , ran nuke.sh , &gt; &gt; cluster-kickstart-pxe &gt; &gt; and get the same long error I sent you before . I go into the shell , use &gt; &gt; fdisk to list hda and remove it and save it and reboot , then get again &gt; &gt; the " choose a disk " message . &gt; &gt; do you have access to a stock 4.1 cluster ? that is , a cluster that &gt; has n't been modified with any XML files ? &gt; &gt; if so , then try adding your **26;12123;TOOLONG file to it and &gt; then see what happens . &gt; &gt; also , send us your **26;12151;TOOLONG file and we 'll try it &gt; out on one of our stock 4.1 systems and we 'll report back our &gt; findings . &gt; &gt; - gb &gt; From jbecker at northwestern.edu Thu Sep 7 12:55:57 2006 From : jbecker at northwestern.edu ( Jesse Becker ) Date : Thu , 7 Sep 2006 14:55:57 -0500 Subject : Rocks-Discuss automount of /home/install problem In-Reply-To : References : LONG ... Message-ID : LONG ... On Tue , @ @ @ @ @ @ @ @ @ @ wrote : &gt; On Tue , 22 Aug 2006 , Greg Bruno wrote : &gt; &gt; &gt;it looks like you did n't supply the kernel roll . &gt; &gt; &gt; &gt;when the Kernel/Boot Roll CD ejects , you 'll need to pop it back in &gt; &gt;order to select it . &gt; &gt; I wondered about that ! But I figured ROCKS would be clever enough to start &gt; with the kernel roll - WRONG ! ) I 'd just like to mention that I had exactly the same problem as Mr. Mitchell did , and the same solution , with the extra " copy vmlinuz and initrd.img " steps also appeared to work . -- Jesse Becker GPG-fingerprint : BD00 7AA4 4483 AFCC 82D0 2720 0083 0931 9A2B 06A2 -------------- next part -------------- A non-text attachment was scrubbed ... Name : not available Type : **25;12179;TOOLONG Size : 1012 bytes Desc : not available Url : LONG ... From mgreaney at fnal.gov Thu Sep 7 13:30:41 2006 From : mgreaney at fnal.gov ( MargaretGreaney ) Date : Thu , 07 Sep 2006 15:30:41 -0500 ( CDT ) Subject : @ @ @ @ @ @ @ @ @ @ References : Message-ID : Greg , I forgot that we do not have the " stock " rocks install like you suggested , and are using SL4 as the os . May I send you the anaconda.log from the node where the replacement xml is not working ? I wonder why it it shows that rocks is executing the autopartitionexecute step ? Also , why is it trying to do a logical volume set up ? Also , I 've been reminded by Steve here that in the past we 've always used the rocks anaconda in place of our anaconda , and that is worth a try . I will get the centos rpm and try it from the rocks cd . Margaret -- Margaret Greaney Telephone : 630-840-4623 Fermilab E-mail : mgreaney at fnal.gov CD/CSS/FCS On Thu , 7 Sep 2006 , Greg Bruno wrote : &gt; On 9/7/06 , MargaretGreaney wrote : &gt; &gt; Greg , &gt; &gt; &gt; &gt; This did not work for me , so maybe there is something else wrong . I edited From dtripp at hawaii.edu Thu Sep 7 14:14:33 2006 From @ @ @ @ @ @ @ @ @ @ Thu , 07 Sep 2006 11:14:33 -1000 Subject : Rocks-Discuss Minimum hardware requirement for Rocks In-Reply-To : References : Message-ID : **35;12206;TOOLONG I 've done the same thing on a small system of P3s . The installer for the 2.6 kernel checks for minimum ram , even though the kernel does n't actually require it at runtime . The compute nodes should install and run with only 256. - Don Stackpole , Chris wrote : &gt; I ran into this problem on a test system I once built . I pulled the ram &gt; from one of the nodes bumping the frontend to 512 , installed Rocks on &gt; the frontend , then moved the ram back to the node . I did not have any &gt; issues with doing that . &gt; &gt; Just a suggestion you might want to try . &gt; &gt; &gt; Chris Stackpole &gt; &gt; &gt; &gt; &gt; -----Original Message----- &gt; From : **30;12243;TOOLONG at sdsc.edu &gt; **37;12275;TOOLONG at sdsc.edu On Behalf Of mason j. &gt; katz &gt; Sent : Thursday , September 07 , 2006 1:23 PM &gt; To : Lijun Liu &gt; Cc @ @ @ @ @ @ @ @ @ @ Minimum hardware requirement for Rocks &gt; &gt; The minimum RAM has been at least 512MB for several releases ( likely &gt; a couple of years ) , sorry . &gt; &gt; -mjk &gt; &gt; &gt; On Sep 6 , 2006 , at 06:40 P , Lijun Liu wrote : &gt; &gt; &gt;&gt; Hi all , &gt;&gt; &gt;&gt; I am trying to install a cluster on 16 nodes with 256MB memory ( I do n't &gt;&gt; know the cpu info yet ) , which version do you recommend for me ? &gt;&gt; &gt;&gt; Is there a way to find out the Users Guide for the older version ? &gt;&gt; &gt;&gt; Thanks a lot for your response in advance . &gt;&gt; &gt;&gt; -- &gt;&gt; Best regards , &gt;&gt; Lijun Liu &gt;&gt; &gt; &gt; -------------- next part -------------- &gt; An HTML attachment was scrubbed ... &gt; URL : &gt; LONG ... &gt; **29;12314;TOOLONG &gt; &gt; &gt; From jbecker at northwestern.edu Thu Sep 7 14:21:11 2006 From : jbecker at northwestern.edu ( Jesse Becker ) Date : Thu , 7 Sep 2006 16:21:11 -0500 Subject : Rocks-Discuss Minimum hardware requirement for Rocks In-Reply-To : @ @ @ @ @ @ @ @ @ @ , Sep 07 , 2006 at 11:14:33AM -1000 , Donald Tripp wrote : &gt; I 've done the same thing on a small system of P3s . The installer for the &gt; 2.6 kernel checks for minimum ram , even though the kernel does n't &gt; actually require it at runtime . The compute nodes should install and run &gt; with only 256 . However , 64MB of RAM really *IS* too small for a compute node install ( having tried ) . -- Jesse Becker GPG-fingerprint : BD00 7AA4 4483 AFCC 82D0 2720 0083 0931 9A2B 06A2 -------------- next part -------------- A non-text attachment was scrubbed ... Name : not available Type : **25;12419;TOOLONG Size : 837 bytes Desc : not available Url : LONG ... From mgreaney at fnal.gov Thu Sep 7 14:26:24 2006 From : mgreaney at fnal.gov ( MargaretGreaney ) Date : Thu , 07 Sep 2006 16:26:24 -0500 ( CDT ) Subject : Rocks-Discuss need anaconda-runtime rpm Message-ID : hello , I 'm trying to use the anaconda that comes with rocks on my install but only find the anaconda rpm and anaconda-help rpm . @ @ @ @ @ @ @ @ @ @ the rocks-dist dist , I 'm getting an error Installing XML Kickstart profiles installing " hpc " profiles ... installing " base " profiles ... installing " kernel " profiles ... installing " site " profiles ... Fixing Comps.xml Database Generating hdlist ( rpm database ) warning - could not find anaconda-runtime warning - could not find genhdlist If cross-kickstarting , build for native arch first . Can I wget the anaconda-runtime from www.rocksclusters.org or should I try to get it from centos ? thanks , margaret here 's what I see on my iso copies : root at grindewald rocks# find /tmp/mnt -name " anaconda* " -print LONG ... LONG ... root at grindewald rocks# umount /tmp/mnt root at grindewald rocks# mount -o loop os-4.1-0.x8664.disk2.iso /tmp/mnt root at grindewald rocks# find /tmp/mnt -name " anaconda* " -print root at grindewald rocks# mount -o loop os-4.2-0.x8664.disk1.iso /tmp/mnt root at grindewald rocks# find /tmp/mnt -name " anacond* " -print LONG ... root at grindewald rocks# umount /tmp/mnt root at grindewald rocks# mount -o loop os-4.2-0.x8664.disk2.iso /tmp/mnt root at grindewald rocks# find /tmp/mnt -name " anacond* " -print LONG ... -- Margaret @ @ @ @ @ @ @ @ @ @ CD/CSS/FCS From greg.bruno at gmail.com Thu Sep 7 14:48:03 2006 From : greg.bruno at gmail.com ( Greg Bruno ) Date : Thu , 7 Sep 2006 14:48:03 -0700 Subject : Rocks-Discuss need anaconda-runtime rpm In-Reply-To : References : Message-ID : On 9/7/06 , MargaretGreaney wrote : &gt; hello , &gt; &gt; I 'm trying to use the anaconda that comes with rocks on my install &gt; but only find the anaconda rpm and anaconda-help rpm . I need also &gt; the anaconda-runtime rpm. &gt; When I do the rocks-dist dist , I 'm getting an error &gt; Installing XML Kickstart profiles &gt; installing " hpc " profiles ... &gt; installing " base " profiles ... &gt; installing " kernel " profiles ... &gt; installing " site " profiles ... &gt; Fixing Comps.xml Database &gt; Generating hdlist ( rpm database ) &gt; warning - could not find anaconda-runtime &gt; warning - could not find genhdlist &gt; If cross-kickstarting , build for native arch first . &gt; &gt; &gt; Can I wget the anaconda-runtime from &gt; www.rocksclusters.org or should I try to get it from centos ? the anaconda-runtime you want is @ @ @ @ @ @ @ @ @ @ output of : # find /home/install/ grep anaconda-runtime - gb From mgreaney at fnal.gov Thu Sep 7 15:17:56 2006 From : mgreaney at fnal.gov ( MargaretGreaney ) Date : Thu , 07 Sep 2006 17:17:56 -0500 ( CDT ) Subject : Rocks-Discuss need anaconda-runtime rpm In-Reply-To : References : 
@@97506614 @1706614/ <h> Creating and running parallel applications with MPI under OS X <p> Author : <p> Timothy Kaiser , Ph.D.tkaiser@sdsc.edu <p> Revised : <p> July 24 , 2003 <p> Much of the information on this page is dated . The information about password-less ssh and network address resolution is still important . For information about more recent developments in running MPI on OSX see : <p> This document discusses compiling and running MPI under Macintosh OS X 10.1 , in particular , the MPICH version of MPI using P4 . If you do n't know what P4 is , do n't worry . <h> These directions apply only to mpich-1.2.2.3 . There are some link issues associated with version 1.2.3 . They do n't look to serious but I have not had a chance to work it out . If you get these resolved before I do , please let me know and I 'll add the changes to this page . It looks like it has something to do with the compile scripts wanting to link in the profiling versions of the libraries . <p> For historical interest , @ @ @ @ @ @ @ @ @ @ presentation describing running parallel jobs on the Macintosh in 1995 . I believe I was the first person to do so . <p> There are a few issues associated with compiling and running MPICH under Macintosh OS X. None of them are major . These issues are related to : <p> Network setup <p> SSH logins <p> Configuration <p> Moving things to where they need to be <p> Linking <p> Running the jobs <p> We will use the P4 device when building MPICH . I have compiled with the " shared " memory option of P4 turned off . <p> Instructions are included for building the Fortran interface using Absoft 's compiler . If you do n't use Fortran just skip those parts . <p> Do not use " StuffIt Expander " or similar software to untar the file . You must use <p> tar -xf mpich.tar " <p> on the command line . <p> Do not use " StuffIt Expander " or similar software to untar the file . You must use <p> tar -xf mpich.tar " <p> on the command line . <p> A few of the commands @ @ @ @ @ @ @ @ @ @ tkaiser . " Please replace tkaiser with your user name as needed . <h> Network setup <p> Things work best if you have a static IP address . Dynamic DHCP works also if there is a nameserver mapping between your address and a name for your machine . That is , you must have access to a nameserver that knows about your machine . <p> Most of the network setup for OS X is done in Network panel of System Preference . Open this up and select the TCP/IP tab . When I am using DHCP over AirPort mine looks like : <p> The first setenv sets a " C " flag for building the library . This is needed so that the symbols in one of the object files , p4globals.o , link correctly when put in the library . As an alternative to this setenv you can add the " -c " option to the ranlib that is done after the libraries are built . <p> /usr/bin/ssh is used by the library to launch tasks on remote nodes . <p> **28;12446;TOOLONG is the path to the Fortran compiler @ @ @ @ @ @ @ @ @ @ that one of the tests in the configure process is successful . <p> Next do a <p> make mpilib <p> you will get messages like <p> ranlib : file : LONG ... has no symbols <h> Running an application <p> Now use scp to copy your application to all machines that you want to use to run your job . For example I will copy to a machine called peloton . <p> scp cpi peloton:cpi <p> where peloton is the name of a machine . If you are asked for a password when you do this go back and look at the section for setting up password-less logins . <p> To run a parallel application you need to specify where the various tasks will be run . This is done using a " Process Group " file . You then specify the process group file on the command line when you run you application like <p> cpi -p4pg cpi.p4pg <p> where cpi.p4pg is the file . <p> The format for the pg file is a little odd . Say I want to run my job using three tasks ; One @ @ @ @ @ @ @ @ @ @ a machine called ozark . Here is a process group file <p> local 0 peloton 1 /Users/tkaiser/cpi ozark 1 /Users/tkaiser/cpi <p> In theory , we can run multiple tasks on a single node and have the tasks swap messages using shared memory . ( I do n't have a multiple processor Mac so there is not a lot of use in doing this so I built MPICH with sharing memory turned off . ) <p> The first line specifies that I want to run a job locally with " 0 " additional tasks sharing memory . A number other than 0 will give an error . <p> The next lines specify that I want to run additional tasks on the given nodes , ( ozark and peloton ) running " 1 " copy of /Users/tkaiser/cpi . <p> I need to give the full path name to the application . For your process group file change /Users/tkaiser to your home directory name . 
@@97506623 @1706623/ <p> Abstract : Isolated rat bone marrow stromal cells cultured in osteogenic medium in which the normal 5.6 mm glucose is changed to hyperglycemic 25.6 mm glucose greatly increase lipid formation between 21-31 days of culture that is associated with decreased biomineralization , up-regulate expression of cyclin D3 and two adipogenic markers ( CCAAT/enhancer binding protein + and peroxisome proliferator-activated receptor + ) within 5 days of culture , increase neutral and polar lipid synthesis within 5 days of culture , and form a monocyte-adhesive hyaluronan matrix through an endoplasmic reticulum stress-induced autophagic mechanism . Evidence is also provided that , by 4 weeks after diabetes onset in the streptozotocin-induced diabetic rat model , there is a large loss of trabecular bone mineral density without apparent proportional changes in underlying collagen matrices , a large accumulation of a hyaluronan matrix within the trabecular bone marrow , and adipocytes and macrophages embedded in this hyaluronan matrix . These results support the hypothesis that hyperglycemia in bone marrow diverts dividing osteoblastic precursor cells ( bone marrow stromal cells ) to a metabolically stressed adipogenic pathway that induces synthesis of a hyaluronan @ @ @ @ @ @ @ @ @ @ process that demineralizes trabecular cancellous bone . 
@@97506624 @1706624/ <h> Recover from corrupted VG in AIX 4.1 ( corrupted filesystem ) <p> o A corrupted file system . o A corrupted journaled-file-system ( JFS ) log device . RECOVERY PROCEDURE To diagnose and fix the problem , you will need to run fsck ( file-system check ) on each file system which causes problems . If the file-system check fails , you may need to perform other steps . 1 . Run the following commands to check and repair file systems . ( The " -y " option gives fsck permission to repair file systems when necessary. ) fsck -y /dev/lvname If fsck indicates that o block 8 could not be read , the file system is prob- ably unrecoverable . See step 5 for information on unrecoverable file systems . o block 8 could be read , but one of the following errors is given : " fsck : Not an AIXV3 file system . " " fsck : Not an AIXV3 file system " " fsck : Not an AIX4 file system " " fsck : Not an AIXV4 file system " " fsck : Not @ @ @ @ @ @ @ @ @ @ invalid argument " o a file system has an unknown log record type , or fsck fails in the logredo process , then go to step 7. o IF THE FILE SYSTEM CHECKS WERE SUCCESSFUL , skip to step 9. 5 . The easiest way to fix an unrecoverable file system is to recreate it . This involves deleting it from the system and restoring it from a backup . 6 . Attempt to repair the file system with a corrupted superblock in AIX : In AIX V3 check the entry block 1000 of the filesystem in question : /usr/sbin/hdf /dev/lvname 1000 100 The output should have the " 43218765 " in the second column of the first line for AIX V3 . This is the magic number . You will also see the mount point and " vol= " entry for the filesystem . If the entry at address 1000 is something other then " 43218765 " fsck will give an error indicating that it is not an AIXV3 file system . Check the entry block 1F000 on the filesystem in question : /usr/sbin/hdf /dev/lvname 1F000 100 If you @ @ @ @ @ @ @ @ @ @ first line , then you can issue the following command to copy the BACKUP superblock to the CURRENT superblock : All AIX versions : dd count=1 bs=4k skip=31 seek=1 if=/dev/lvname of=/dev/lvname Replace " lvname " with the appropriate file system . For AIX 4. x only : fsck -p /dev/lvname Once the copying over is completed , check the integrity of the file system by issuing : fsck /dev/lvname In many cases , copying the backup superblock to the primary superblock will recover the file system . If this does not work , you will have to recreate the file system , and restore the data from a backup . 7 . A corruption of the JFS log logical volume has been detected . Use the logform command to reformat it . - ' /usr/sbin/rmlv hd8 ' - ' /usr/sbin/mklv -a e -t jfslog -y hd8 rootvg 1 ' - ' /usr/sbin/logform /dev/hd8 ' - ' sync ; sync ; sync Answer YES when asked if you want to destroy the log. 
@@97506626 @1706626/ 1450 @qwx861450 <h> SDSC Coordinates Effort to Establish the BigData Top100 List <p> The San Diego Supercomputer Center ( SDSC ) at the University of California , San Diego , today announced plans for a community-based effort to create the BigData Top100 List , the first global ranking of its kind for systems designed for big data applications . <p> The BigData Top100 List will rank systems according to their performance on an application-level workload specification , while also reporting on system efficiencies in terms of price/performance . As an application-level benchmark , the list will complement other rankings of high-performance computing ( HPC ) systems , such as the Top500 and Graph500 . <p> Preliminary information about the new list is at www.bigdatatop100.org , which also includes information for those interested in joining this consortium and supporting its development . <p> " The explosion in data and the value of repurposing and exploiting data assets have created what we now call the big data phenomenon , " said Chaitan Baru , SDSC Distinguished Scientist and director of the Center for Large-scale Data Systems research ( CLDS ) , @ @ @ @ @ @ @ @ @ @ the technical and management aspects of big data and other data-enabled applications now becoming pervasive among academia , government , and industry . <p> Baru also was recently appointed SDSCs Associate Director , Data Initiatives . The position reflects the Centers focus on addressing both the management and technical aspects of big data and other data-enabled applications now becoming pervasive among academia , government , and industry . <p> The BigData Top100 List initiative was announced at the OReilly Strata Conference in Santa Clara , California this week in a joint presentation by Baru and Milind Bhandarkar , Chief Scientist , Greenplum , a division of EMC and an industry sponsor of the CLDS . Baru and Bhandarkar are members of the initial BigData Top100 List steering group , which also includes Dhruba Borthakur ( Facebook ) , Eyal Gutkind ( Mellanox ) , Jian Li ( IBM ) , Raghunath Nambiar ( Cisco ) , Ken Osterberg ( Seagate ) , Scott Pearson ( Brocade ) , Meikel Poess ( Oracle ) , Tilmann Rabl ( University of Toronto ) , Richard Treadway ( NetApp ) , and Jerry @ @ @ @ @ @ @ @ @ @ the introduction of the new list and describing the benchmarking initiative was published in the March 2013 inaugural issue of the quarterly journal Big Data . <p> " The creation of a new journal focused solely on big data underscores the importance of this trend across all applications domains , in science as well as business , " said Bhandarkar . " The benchmarking effort is a pioneering activity in big data , and creating such a benchmark is a vital step toward fostering competition and innovation in the field . " <p> In addition , an online competition for refining the benchmark dataset and benchmark workload will be announced shortly on kaggle.com , a leading platform for predictive modeling competitions . <p> " We were excited when SDSC approached us with the idea of benchmarking data systems , " said Will Cukierski , who co-leads development of public competitions for Kaggle . " Competitions reward objective merit , and merit is what ought to matter as this industry matures . " <p> " Big data is now part of every sector and function of the global economy , and @ @ @ @ @ @ @ @ @ @ benchmarks to quantify system performance and price/performance on big data tasks and applications , " added Baru . " The existence of such benchmarks enables healthy competition among technology and solution providers , resulting eventually in product improvements and evolution of new technologies . " <p> About SDSCAs an Organized Research Unit of UC San Diego , SDSC is considered a leader in data-intensive computing and all aspects of big data , which includes data integration , performance modeling , data mining , software development , , and health IT . With its two newest supercomputer systems , Trestles and Gordon , SDSC is a partner in XSEDE ( Extreme Science and Engineering Discovery Environment ) , the most advanced collection of integrated digital resources and services in the world . 
@@97506629 @1706629/ <h> Abstract <p> Glycosaminoglycan ( GAG ) -bound and soluble chemokine gradients in the vasculature and extracellular matrix mediate neutrophil recruitment to the site of microbial infection and sterile injury in the host tissue . However , the molecular principles by which chemokine-GAG interactions orchestrate these gradients are poorly understood . This , in part , can be directly attributed to the complex interrelationship between the chemokine monomer-dimer equilibrium and binding geometry and affinities that are also intimately linked to GAG length . To address some of this missing knowledge , we have characterized the structural basis of heparin binding to the murine CXCL1 dimer . CXCL1 is a neutrophil-activating chemokine and exists as both monomers and dimers ( Kd = 36 ++m ) . To avoid interference from monomer-GAG interactions , we designed a trapped dimer ( dCXCL1 ) by introducing a disulfide bridge across the dimer interface . We characterized the binding of GAG heparin octasaccharide to dCXCL1 using solution NMR spectroscopy . Our studies show that octasaccharide binds orthogonally to the interhelical axis and spans the dimer interface and that heparin binding enhances the structural integrity @ @ @ @ @ @ @ @ @ @ . We generated a quadruple mutant ( H20A/K22A/K62A/K66A ) on the basis of the binding data and observed that this mutant failed to bind heparin octasaccharide , validating our structural model . We propose that the stability enhancement of dimers upon GAG binding regulates in vivo neutrophil trafficking by increasing the lifetime of " active " chemokines , and that this structural knowledge could be exploited for designing inhibitors that disrupt chemokine-GAG interactions and neutrophil homing to the target tissue . 
@@97506632 @1706632/ 1450 @qwx861450 <h> NSF Awards $20 Million to SDSC to Develop " Gordon " <p> The San Diego Supercomputer Center ( SDSC ) at UC San Diego has been awarded a five-year , $20 million grant from the National Science Foundation ( NSF ) to build and operate a powerful supercomputer dedicated to solving critical science and societal problems now overwhelmed by the avalanche of data generated by the digital devices of our era . <p> Among other features , this unique and innovative supercomputer will employ a vast amount of flash memory to help speed solutions now hamstrung by slower spinning disk technology . Also , new " supernodes " will exploit virtual shared-memory software to create large shared-memory systems that reduce solution times and yield results for applications that now tax even the most advanced supercomputers . <p> Called Gordon , SDSC 's latest supercomputer is slated for installation by Appro International , Inc. in mid-2011 , and will become a key part of a network of next-generation high-performance computers ( HPC ) being made available to the research community through an open-access national grid . Details @ @ @ @ @ @ @ @ @ @ , the leading international conference on high-performance computing , networking , storage and analysis , to be held in Portland , Oregon , November 14-20 . <p> Gordon is the follow-on to SDSC 's previously announced Dash system , the first supercomputer to use flash devices . Dash is a finalist in the Data Challenge at SC09 . <p> " We are clearly excited about the potential for Gordon , " said SDSC Interim Director Michael Norman , who is also the project 's principal investigator . " This HPC system will allow researchers to tackle a growing list of critical ' data-intensive ' problems . These include the analysis of individual genomes to tailor drugs to specific patients , the development of more accurate models to predict the impact of earthquakes on buildings and other structures , and simulations that offer greater insights into what 's happening to the planet 's climate . " <p> " Data-driven scientific exploration is now complementing theory , experimentation and simulation as tools scientists and engineers use in order to make the scientific breakthroughs sought by the National Science Foundation , " said @ @ @ @ @ @ @ @ @ @ for the National Science Foundation 's Office of Cyberinfrastructure . " SDSC 's Gordon will be the most recent tool that can be applied to data-driven scientific exploration . It was conceived and designed to enable scientists and engineers -indeed any area requiring demanding extensive data analysis - to conduct their research unburdened by the significant latencies that impede much of today 's progress . Gordon will do for data-driven science what tera-/peta-scale systems have done for the simulation and modeling communities , and provides a new tool to conduct transformative research . " <p> Gordon builds on technology now being deployed at SDSC , including the new Triton Resource and Dash systems . As part of the Triton Resource , Dash leverages lightning-fast flash memory technology already familiar to many from the micro storage world of digital cameras , thumb drives and laptop computers . <p> " For nearly a quarter century , SDSC has been a pioneer in the field of high-performance computing , " said Art Ellis , UC San Diego 's vice chancellor for research . " It is therefore fitting that this Center and its @ @ @ @ @ @ @ @ @ @ that not only is powerful , but also will tackle data-intensive research applications that are n't easily handled by the current generation of supercomputers . " <p> When fully configured and deployed , Gordon will feature more than 200 teraflops of total compute power ( one teraflop or TF equals a trillion calculations per second ) , 64 terabytes ( TB ) of DRAM ( digital random access memory ) , 256 TB of flash memory , and four petabytes of disk storage ( one petabyte or PB equals one quadrillion bytes of data ) . For sheer power , when complete , Gordon should rate among the top 30 or so supercomputers in the world . <p> Though impressive , these statistics only explain part of the machine 's special capabilities . Gordon is ideally suited to tackle a variety of problems involving large data sets that are less concerned with raw performance than productivity . <p> " This will be a state-of-the-art supercomputer that 's unlike any HPC machine anywhere , " said Anthony Kenisky , vice president of sales for Appro . " Gordon , to be @ @ @ @ @ @ @ @ @ @ an invaluable platform for academic and commercial scientists , engineers and others needing an HPC system that focuses on the rapid storage , manipulation and analysis of large volumes of data . " <p> " Supernode " CityA key feature of Gordon will be 32 " supernodes " based on an Intel system utilizing the newest processors available in 2011 , and combining several state-of-the-art technological innovations through novel virtual shared-memory software provided by Scale MP , Inc . Each supernode consists of 32 compute nodes , capable of greater than 195 gigaflops/node ( one gigaflop or GF equals a billion calculations per second ) and 64 gigabytes ( GB ) of DRAM . A supernode also incorporates 2 I/O nodes , each with 4 TB of flash memory . When tied together by virtual shared memory , each of the system 's 32 supernodes has the potential of more than 6 TF of compute power and 10 TB of memory ( 2 TB of DRAM and 8 TB of flash memory ) . <p> " Moving a physical disk-head to accomplish random I/O is so last-century , " said @ @ @ @ @ @ @ @ @ @ for this innovative system . " Indeed , Charles Babbage designed a computer based on moving mechanical parts almost two centuries ago . With respect to I/O , it 's time to stop trying to move protons and just move electrons . With the aid of flash solid-state drives ( SSDs ) , this system should do latency-bound file reads 10 times faster and more efficiently than anything done today . " <p> Flash memory is designed to reduce the " latency time " of passing data to and from processor and spinning disk , and thus provides the missing link between DRAM on each processor and disk . <p> Gordon 's 32 supernodes will be interconnected via an InfiniBand network , capable of 16 gigabits per second of bi-directional bandwidth - that 's eight times faster than some of the most powerful national supercomputers to come on-line in recent months . The combination of raw power , flash technology , and large-shared memory on a single supernode , coupled with high-bandwidth across the system , is expected to reduce the time and complexity often experienced when researchers tackle data-intensive @ @ @ @ @ @ @ @ @ @ parallel supercomputers . <p> " Many of these problems can not use the ' big flop ' machines effectively , " said SDSC 's Norman . " The reason is simple : data volume is exploding while methods to mine these data are not becoming massively parallel at the same rate ; in other words , generating data is relatively easy , while inventing new parallel algorithms is hard . " <p> Moreover , Gordon will be configured to achieve a ratio of addressable memory in terabytes to peak teraflops on each supernode that is greater than 1:1 . By contrast , the same metric for many HPC systems is less than 1:10 . <p> " This provides a radically different system balance point for meeting the memory demands of data-intensive applications that may not need a lot of ' flops ' and/or may not scale well , but do need a large addressable space , " noted Snavely . <p> Potential Scientific Applications The new SDSC system will provide benefits to both academic and industrial researchers in need of special " data-mining " capabilities . For example , scientific @ @ @ @ @ @ @ @ @ @ data and continue to grow . These databases currently are stored largely in disk arrays , with access limited by disk read rates . <p> Gordon should also be invaluable for what 's known as " predictive science , " whose goal is to create models of real-life phenomena of research interest . Geophysicists within the Southern California Earthquake Center ( SCEC ) , for instance , are using full three-dimensional seismic tomographic images to obtain realistic models of the earth 's crust under Southern California . Such models are critical to planners seeking to understand and predict the impact of large-scale earthquakes on buildings and other structures along major fault lines . However , this research is now limited by the huge quantities of raw data needed to simulate this activity . Gordon , with its large-scale memory on a single node , should speed these computations , creating models that more closely mimic real-life temblors . <p> Gordon will also enable the manipulation of massive graphs that arise in many data-intensive fields , including bioinformatics , social networks and neuroscience . In these applications , large databases could @ @ @ @ @ @ @ @ @ @ latency than if they were resident on disk . <p> " Many scientific applications need fast , interactive methods to manipulate large volumes of structured data , " said Amarnath Gupta , director of SDSC 's Advanced Query Processing Laboratory . " With Gordon , it will be possible to place database management systems such as PostgreSQL on the flash drive and get a three-to-four fold improvement in the execution time for many long-running queries that need a high volume of I/O access . " <p> Gordon will be housed in SDSC 's 18,000 square-foot , energy-efficient data center on the UC San Diego campus , and will build on SDSC 's nearly 25-year experience in deploying , operating and supporting production-quality resources that serve the national community . Faculty from UC San Diego 's Computer Science and Engineering department , and Computational Science , Mathematics and Engineering program will integrate Gordon into undergraduate and graduate education courses and curriculum . <p> About SDSC : As an Organized Research Unit of UC San Diego , SDSC is a national leader in creating and providing cyberinfrastructure for data-intensive research . Cyberinfrastructure @ @ @ @ @ @ @ @ @ @ and expertise , focused on accelerating scientific inquiry and discovery . In late 2008 , SDSC doubled its size to 160,000 square feet with a new building and data center extension , and is a founding member of TeraGrid , the nation 's largest open-access scientific discovery infrastructure 