
@@71485086 @185086/ <h> Intersection over Union ( IoU ) for object detection <p> Todays blog post is inspired from an email I received from Jason , a student at the University of Rochester . <p> Jason is interested in building a custom object detector using the HOG + Linear SVM framework for his final year project . He understands the steps required to build the object detector well enough - but he is n't sure how to evaluate the- accuracy- of his detector once its trained . <p> His professor mentioned that he should use the- Intersection over Union ( IoU ) method for evaluation , but Jasons not sure how to implement it . <p> I helped Jason out over email by : <p> Describing- what Intersection over Union is . <p> Explaining- why we use Intersection over Union to evaluate object detectors . <p> Providing him with some- example Python code from my own personal library to perform Intersection over Union on bounding boxes . <p> My email really helped Jason finish getting his final year project together and I 'm sure he s going to pass with flying colors . @ @ @ @ @ @ @ @ @ @ my response to Jason into an actual blog post in hopes- that it will help you as well . <p> To learn how to evaluate your own custom object detectors using the Intersection over Union evaluation metric , just keep reading . <h> What is Intersection over Union ? <p> Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset. - We often see this evaluation metric used in object detection challenges such as the popular PASCAL VOC challenge . <p> Intersection over Union is simply an- evaluation metric . Any algorithm that provides predicted bounding boxes as output can be evaluated using IoU . <p> More formally , in order to apply Intersection over Union to evaluate an ( arbitrary ) object detector we need : <p> The- ground-truth bounding boxes ( i.e. , the hand labeled bounding boxes from the testing set that specify- where in the image our object is ) . <p> The- predicted bounding boxes from our model . <p> As long as we have these two sets of bounding boxes we can apply Intersection @ @ @ @ @ @ @ @ @ @ example of a ground-truth bounding box versus a predicted bounding box : <p> Figure 1 : An example of detecting a stop sign in an image . The predicted bounding box is drawn in red- while the ground-truth bounding box- is drawn in green . Our goal is to compute the Intersection of Union between these bounding box . <p> In the figure above we can see that our object detector has detected the presence of a stop sign in an image . <p> Figure 2 : Computing the Intersection of Union is as simple as dividing the area of overlap between the bounding boxes by the area of union ( thank you to the excellent Pittsburg HW4 assignment for the inspiration for this figure ) . <p> Examining this equation you can see that Intersection over Union is simply a ratio . <p> In the numerator- we compute the- area of overlap between the- predicted bounding box and the- ground-truth bounding box . <p> The denominator is the- area of union , or more simply , the area encompassed by- both the predicted bounding box and the ground-truth bounding @ @ @ @ @ @ @ @ @ @ area of union yields our final score - the Intersection over Union . <h> Where are you getting the ground-truth examples from ? <p> Before we get too far , you might be wondering where the ground-truth examples come from. - I 've mentioned before that these images are " hand labeled " , but what exactly does that mean ? <p> You see , when training your own object detector ( such as the HOG + Linear SVM method ) , you need a dataset . This dataset should be broken into ( at least ) two groups : <p> The- bounding boxes associated with the object(s) in the image . The bounding boxes are simply the- ( x , y ) -coordinates of the object in the image . <p> The bounding boxes for the training and testing sets are- hand labeled and hence why we call them the " ground-truth " . <p> Your goal is to take the training images + bounding boxes , construct an object detector , and then evaluate its performance on the testing set . <p> An Intersection over Union score &gt; 0.5 @ @ @ @ @ @ @ @ @ @ Why do we use Intersection over Union ? <p> If you have performed- any previous machine learning in your career , specifically classification , you 'll likely be used to- predicting class labels where your model outputs a single label that is either- correct or- incorrect . <p> This type of binary classification makes computing accuracy straightforward ; however , for object detection its not so simple . <p> Because of this , we need to define an evaluation metric that- rewards predicted bounding boxes for heavily overlapping with the ground-truth : <p> Figure 3 : An example of computing Intersection over Unions for various bounding boxes . <p> In the above figure I have included examples of good and bad Intersection over Union scores . <p> As you can see , predicted bounding boxes that heavily overlap with the ground-truth bounding boxes have higher scores than those with less overlap. - This makes Intersection over Union an excellent metric for evaluating custom object detectors . <p> We are n't  concerned with an- exact match of- ( x , y ) -coordinates , but we do want to ensure that our @ @ @ @ @ @ @ @ @ @ Union is able to take this into account . <h> Implementing Intersection over Union in Python <p> Now that we understand what Intersection over Union is and why we use it to evaluate object detection models , let 's go ahead and implement it in Python . <p> Before we get started writing any code though , I want to provide the five example images we will be working with : <p> Figure 4 : In this example , well be detecting the presence of cars in images . <p> These images are part of the CALTECH-101 dataset- used for both- image classification and- object detection . <p> Inside the PyImageSearch Gurus course I demonstrate how to train a custom object detector to detect the presence of cars in images like the ones above using the HOG + Linear SVM framework . <p> I have provided a visualization of the ground-truth bounding boxes ( green ) along with the predicted bounding boxes ( red ) from the custom object detector below : <p> Figure 5 : Our goal is to evaluate the performance of our object detector by using Intersection of @ @ @ @ @ @ @ @ @ @ of the predicted bounding box ( red ) against the ground-truth ( green ) . <p> Given these bounding boxes , our task is to define the Intersection over Union metric that can be used to evaluate how " good ( or bad ) our predictions are . <p> With that said , open up a new file , name it **25;0;TOOLONG , and let 's get coding : <p> Intersection over Union ( IoU ) for object detection <p> Python <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> 6 <p> 7 55203 @qwx675203 <p> **31;27;TOOLONG 55220 @qwx675220 <p> importcv2 <p> # define the Detection object <p> LONG ... <p> We start off by importing our required Python packages . We then define a Detection- object that will store three attributes : <p> imagepath- : The path to our input image that resides on disk . <p> gt- : The ground-truth bounding box . <p> pred- : The predicted bounding box from our model . <p> As well see later in this example , I 've already obtained the predicted bounding boxes from our five respective images and hardcoded @ @ @ @ @ @ @ @ @ @ concise . <p> Let 's go ahead and define the bbintersectionoverunion- function , which as the name suggests , is responsible for computing the Intersection over Union between two bounding boxes : <p> Intersection over Union ( IoU ) <p> 29 <p> 30 <p> **31;60;TOOLONG , boxB ) : <p> # determine the ( x , y ) -coordinates of the intersection rectangle <p> xA=max ( boxA0 , boxB0 ) <p> yA=max ( boxA1 , boxB1 ) <p> xB=min ( boxA2 , boxB2 ) <p> yB=min ( boxA3 , boxB3 ) <p> # compute the area of intersection rectangle <p> interArea= ( xB-xA+1 ) * ( yB-yA+1 ) <p> # compute the area of both the prediction and ground-truth <p> # rectangles <p> LONG ... <p> LONG ... <p> # compute the intersection over union by taking the intersection <p> # area and dividing it by @ @ @ @ @ @ @ @ @ @ the interesection area <p> LONG ... <p> # return the intersection over union value <p> returniou <p> This method requires two parameters : boxA- and boxB- , which are presumed to be our ground-truth and predicted bounding boxes ( the actual- order in which these parameters are supplied to bbintersectionoverunion- does n't  matter ) . <p> Lines 11-14 determine the- ( x , y ) -coordinates of the intersection rectangle which we then use to compute the area of the intersection ( Line 17 ) . <p> The interArea- variable now represents the- numerator in the Intersection over Union calculation . <p> To compute the denominator we first need to derive the area of both the predicted bounding box and the ground-truth bounding box ( Lines 21 and 22 ) . <p> The Intersection over Union can then be computed on- Line 27 by dividing the intersection area by the union area of the two bounding boxes , taking care to subtract out the intersection area from the denominator ( otherwise the intersection area would be doubly counted ) . <p> Finally , the Intersection over Union score is returned @ @ @ @ @ @ @ @ @ @ that our Intersection over Union method is finished , we need to define the ground-truth and predicted bounding box coordinates for our five example images : <p> Intersection over Union ( IoU ) for object detection <p> Python <p> 32 <p> 33 <p> 34 <p> 35 <p> 36 <p> 37 <p> 38 <p> # define the list of example detections <p> examples= <p> LONG ... <p> LONG ... <p> LONG ... <p> LONG ... <p> LONG ... <p> As I mentioned above , in order to keep this example short(er) and concise , I have- manually obtained the predicted bounding box coordinates from my HOG + Linear SVM detector . These predicted bounding boxes ( And corresponding ground-truth bounding boxes ) are then- hardcoded into this script . <p> On- Line 41 we start looping over each of our examples- ( which are Detection- objects ) . <p> For each of them , we load the respective image- from disk on- Line 43 and then draw the ground-truth bounding box in green ( Lines 47 and 48 ) followed by the predicted bounding box in red ( Lines 49 and @ @ @ @ @ @ @ @ @ @ is computed on- Line 53 by passing in the ground-truth and predicted bounding box . <p> We then write- the Intersection over Union value on the image- itself followed by our console as well . <h> Comparing predicted detections to the ground-truth with Intersection over Union <p> To see the Intersection over Union metric in action , make sure you have downloaded the source code + example images to this blog post by using the- " Downloads " section found at the bottom of this tutorial . <p> After unzipping the archive , execute the following command : <p> Intersection over Union ( IoU ) for object detection <p> Shell <p> 1 <p> $python intersectionoverunion.py <p> Our first example image has an Intersection over Union score of- 0.7980 , indicating that there is significant overlap between the two bounding boxes : <p> Figure 6 : Computing the Intersection of Union using Python . <p> The same is true for the following image which has an Intersection over Union score of- 0.7899 : <p> Figure 7 : A slightly better Intersection over Union score . <p> Notice how the ground-truth bounding @ @ @ @ @ @ @ @ @ @ box ( red ) . This is because our object detector is defined using the HOG + Linear SVM framework which requires us to specify- a fixed size sliding window ( not to mention , an image pyramid scale and the HOG parameters themselves ) . <p> Ground-truth bounding boxes will naturally have a slightly different aspect ratio than the predicted bounding boxes , but that 's okay provided that the Intersection over Union score is- &gt; 0.5 as we can see , this still a great prediction . <p> The next example demonstrates a slightly " less good " prediction where our predicted bounding box is much less " tight " than the ground-truth bounding box : <p> The reason for this is because our HOG + Linear SVM detector likely couldnt " find " the car in the lower layers of the image pyramid and instead fired near the top of the pyramid where the image is much smaller . <p> The following example is an- extremely good detection with an Intersection over Union score of- 0.9472 : <h> Summary <p> In this blog post I discussed the- Intersection @ @ @ @ @ @ @ @ @ @ metric can be used to assess- any object detector provided that ( 1 ) the model produces predicted- ( x , y ) -coordinates i.e. , the bounding boxes for the object(s) in the image and ( 2 ) you have the ground-truth bounding boxes for your dataset . <p> Typically , you 'll see this metric used for evaluating HOG + Linear SVM and CNN-based object detectors . <p> Finally , before you go , be sure to enter your email address in the form below to be notified when future PyImageSearch blog posts are published you wo n't want to miss them ! <h> Downloads : 55217 @qwx675217 <p> If you detector predicts multiple bounding boxes for the same object then you should be applying non-maxima suppression . If you are predicting bounding boxes for multiple objects then you should be computing IoU for each of them . <p> This code fails . Specifically , it gets broken when comparing two non-overlapping bounding boxes by providing a non-negative value for interArea when the boxes can be separated into diagonally opposing quadrants by a vertical and a horizontal line . This @ @ @ @ @ @ @ @ @ @ cases . <p> If you have the predictions from your MATLAB detector you can either ( 1 ) write them to file and use my Python code to read them and compare them to the ground-truth or ( 2 ) implement this function in MATLAB . <p> This is to avoid those corner cases where the rectangles are not overlapping but the intersection area still computes to be greater than 0 . This happens when both the brackets in the original line 17 are negative . For e.g. boxA is on the upper right of the picture and boxB is somewhere on the lower left of the picture , without any overlap and with significant separation between them . <p> Hey Adrian , I tested your code but I think it is a little buggy : Assume that we have two below bounding boxes : ( the structure of bounding boxes are ( x1 , y1 , x2 , y2 ) , it is just a tuple in python. ( x1 , y1 ) denotes to the top-left coordinate and ( x2 , y2 ) denotes to the bottom-right coordinate @ @ @ @ @ @ @ @ @ @ ( 243,203,348,279 ) <p> Based on these BBoxes , the IoU should be zero . Because there is no intersection between them . But your code give a negative value ! By the way , I should say that my origin is up-left corner of the image . So how can I solve this problem ? <p> Many thanks Adrian for the great topic You . I wan na ask if I have dataset groundtruth as contour shows all object outline ( not rectangle box shape ) . Then , is there another method dealing with free shape bounding contours ? In order to compute Lou . <p> Intersection over Union assumes bounding boxes . You can either convert your contour points to a bounding box and compute IoU or you can simply count the number of pixels in your mask @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485087 @185087/ <h> The bag of ( visual ) words model <p> The bag of ( visual ) words model <p> Let me start off by saying , - " Youll want to pay attention to this lesson . " <p> The bag of visual words ( BOVW ) model is one of the- most important concepts in all of computer vision . We use the bag of visual words model to classify the contents of an image . Its used to build- highly scalable ( not to mention , - accurate ) - CBIR systems . We even use the bag of visual words model when classifying texture via- textons . <p> So what exactly is a bag of visual words ? And how do we construct one ? <p> In the remainder of this lesson , well review the basics of the bag of visual words model along with the general steps required to build one . <h> Objectives : <p> In this lesson , we will : <p> Learn about the bag of visual words model . <p> Review the required steps to build a bag of visual @ @ @ @ @ @ @ @ @ @ ? <p> As the name implies , the " bag of visual words " concept is actually taken from the " bag of words " model from the field of Information Retrieval ( i.e. , text-based search engines ) and text analysis . <p> The general idea in the bag of words model is to represent " documents " ( i.e. webpages , Word files , etc. ) as a collection of important keypoints while- totally disregarding the order the words appear in . <p> Documents that share a large number of the same keywords , again , regardless of the order the keywords appear in , are considered to be- relevant to each other . <p> Furthermore , since we are totally disregarding the order of the words in the- document , we call this representation a " bag of words " rather than a " list of words " or " array of words " : <p> Figure 1 : A bag of words model entirely disregards the order of words in a document and simply counts the number of times each word appears . <p> Treating a @ @ @ @ @ @ @ @ @ @ to efficiently analyze and compare documents since we do not have to store any information regarding the order and locality of words to each other we simply count the number of times a word appears in a document , and then use the frequency counts of each word as a method to quantify the document . <p> In computer vision , we can apply the same concept only now instead of working with keywords , our " words " are now image patches and their associated feature vectors : <p> Figure 2 : We take a similar approach in computer vision . Only now instead of actual text words , we count the number of times each image patch occurs . <p> When reading the computer vision literature , its common to see the terms- bag of words and- bag of visual words used interchangeably . Both terms refer to the same concept representing an image as a collection of unordered image patches ; however , I prefer to use the term- bag of visual words to alleviate any chance of ambiguity . Using the term- bag of words can @ @ @ @ @ @ @ @ @ @ fuses- both text and image data imagine the struggle and confusion that can result in trying to explain which " bag " model you are referring to ! Because of this , I will use the term- bag of visual words , sometimes abbreviated as BOVW throughout the rest of this course . <p> In Information Retrieval and text analysis , recording the number of times a given word appears in a document is trivially easy you simply count them and construction a histogram of word occurrences : <p> Figure 3 : An example of taking a blob of text and converting it into a word histogram . <p> Here , we can see the word- " like " - is used twice . The word- " teaches " is used only once . And both the words- " computer " and- " vision " are used four times each . If we were to completely ignore the raw text and simply examine- the word occurrence histogram , we could easily determine that this blob of text discusses the topic of " computer vision . " <p> However , applying @ @ @ @ @ @ @ @ @ @ as simple . <p> How exactly do you count the- number of times an " image patch " appears in an image ? And how do you ensure the same " vocabulary " of visual words is used for each image in your dataset ? <p> Well discuss the vocabulary problem in more detail later in this lesson , but for the time being let 's assume that through some black-box algorithm that we can construct a large vocabulary ( also referred to as a- dictionary or- codebook ; I 'll be using all three terms interchangeably ) of possible visual words in our dataset . <p> Given our dictionary of possible visual words , we can then quantify and represent an image as a histogram which simply counts the number of times each visual word appears. - This histogram is our actual- bag of visual words : <p> Figure 4 : Taking three input images ( top ) , extracting image patches from each of them ( middle ) , and then counting the number of times each visual word appears in the respective images ( bottom ) . <p> At @ @ @ @ @ @ @ @ @ @ images of a- face , - bicycle , and violin . We then extract image patches from each of these objects ( middle ) . Then , on the bottom we take the image patches and use them to construct a histogram that " counts " the number of times each of these image patches appears . These histograms are our actual bag of visual words representation . We call this representation a " bag " because we completely throw out any ordering and locality of image patches and simply tabulate the number of times each type of patch appears . <p> Notice how the- face image has substantially more face patch counts than the other two images . Similarly , the bicycle- histogram has many more- bicycle patches . And finally , the- violin image reports many more violin-like image patches in its histogram . <h> Building a bag of visual words <p> Building a bag of visual words can be broken down into a three-step process : <p> Step #1 : Feature extraction <p> Step #2 : Codebook construction <p> Step #3 : Vector quantization . <p> We @ @ @ @ @ @ @ @ @ @ next few lessons , but for the time being , let 's perform a high-level overview of each step . <h> Step #1 : Feature extraction <p> The first step in building a bag of visual words is to perform feature extraction by extracting descriptors from each image in our dataset . <p> Feature extraction can be accomplished in a variety of ways including : detecting keypoints and extracting SIFT features from salient regions of our images ; applying a grid at regularly spaced intervals ( i.e. , the- Dense keypoint detector ) and extracting another form of local invariant descriptor ; or we could even extract mean RGB values from random locations in the images . <p> The point here is that for each image inputted , we receive multiple feature vectors out : <p> Figure 5 : When constructing a bag of visual words , our first step is to apply feature extraction , where we extract- multiple feature vectors per image . <h> Step #2 : Dictionary/Vocabulary construction <p> Now that we have extracted feature vectors from each image in our dataset , we need to construct our @ @ @ @ @ @ @ @ @ @ Vector quantization <p> Given an arbitrary image ( whether from our original dataset or not ) , we can quantify and abstractly represent the image using our bag of visual words model by applying the following process : <p> Extract feature vectors from the image in the same manner as- Step #1 above . <p> For each extracted feature vector , compute its nearest neighbor in the dictionary created in Step #2- this is normally accomplished using the Euclidean Distance . <p> Take the set of nearest neighbor labels and build a histogram of length- k ( the number of clusters generated from k-means ) , where the- ith value in the histogram is the frequency of the- ith visual word . This process in modeling an object by its distribution of prototype vectors is commonly called- vector quantization . <p> An example of constructing a bag of visual words from an image can be seen below : <p> Figure 6 : An example of taking an image , detecting keypoints , extracting the features surrounding each keypoint , and then quantizing them according to the closest cluster center . @ @ @ @ @ @ @ @ @ @ for- all images in our dataset , we can apply common machine learning or CBIR algorithms to classify and retrieve images based on their visual contents . <p> Simply put , the bag of visual words model allows us to take highly discriminate descriptors- ( such as SIFT ) , which result in- multiple feature vectors per image , and- quantize them into a single histogram based on our dictionary . Our image is now represented by a- single- histogram of discriminative features which can be fed into other machine learning , computer vision , and CBIR algorithms . <h> Summary <p> We started this lesson by discussing the- bag of words model in text analysis and Information Retrieval . The bag of words models a document by simply counting the number of times a given keyword occurs , irrespective of the ordering of the keywords in the document . The word " bag " is used to describe this method since we ignore the ordering of the keywords . <p> The same type of concept can be applied in computer vision by counting the number of pre-set image patches @ @ @ @ @ @ @ @ @ @ of visual words . 
@@71485088 @185088/ <h> Count the total number of frames in a video with OpenCV and Python <p> Todays blog post is part of a two part series on working with video files using OpenCV and Python . <p> The first part of this series will focus on a question emailed in by PyImageSearch reader , Alex . <p> Alex asks : <p> I need to count the total number of frames in a video file with OpenCV . The only way I 've found to do this is to loop over each frame in the video file individually and increment a counter . Is there a faster way ? <p> Great question Alex . <p> And rest assured , you 're not the first person who- has asked me this question . <p> In the remainder of this blog post I 'll show you how to define a function that can- quickly determine the total number of frames in a video file . <p> Next week well use this function to aid us in a fun visualization task where I 'll demonstrate how to create " movie barcodes " . In order to generate these @ @ @ @ @ @ @ @ @ @ there are in our input movie file . <p> To learn more about fast , efficient frame counting with OpenCV and Python , - just keep reading . <h> The easy way to count frames with OpenCV and Python <p> The first method to count video frames in OpenCV with Python is very fast it simply uses the built-in properties OpenCV provides to access a video file and read the meta information of the video . <p> Let 's go ahead and see how this function is implemented inside imutils now : <p> Count the total number of frames in a 12 <p> 13 <p> 14 55203 @qwx675203 <p> from .. convenience importiscv3 <p> importcv2 <p> defcountframes ( path , override=False ) : <p> # grab a pointer to the video file and initialize the total <p> # number of frames read <p> **28;93;TOOLONG <p> total=0 <p> # if the override flag is passed in , revert to the manual <p> @ @ @ @ @ @ @ @ @ @ <p> To start , we import our necessary Python packages on- Lines 2 and 3 . Well need the iscv3- function to check which version of OpenCV were using along with cv2- for our actual OpenCV bindings . <p> We define the countframes- function on- Line 5 . This method requires a single argument followed by a second optional one : <p> path- : This is the path to where our video file resides on disk . <p> override- : A boolean flag used to determine if we should skip Method #1 and go directly to the slower ( but guaranteed accurate/error free ) Method #2 . <p> We make a call to cv2.VideoCapture- on- Line 8 to obtain a pointer to the actual video file followed by initializing the total- number of frames in the video . <p> We then make a check on- Line 13 to see if we should override- . If so , we call countframesmanual- ( which well define in the next section ) . <p> Otherwise , let 's see how Method #1 is actually implemented : <p> Count the total number of frames in @ @ @ <p> 39 <p> 40 <p> # otherwise , let 's try the fast way first <p> else : <p> # let 's try to determine the number of frames in a video <p> # via video properties ; this method can be very buggy <p> # and might throw an error based on your OpenCV version <p> # or may fail entirely based on your which video codecs <p> # you have installed <p> try : <p> # check if we are using OpenCV 3 <p> ifiscv3() : <p> LONG ... <p> # otherwise , we are using OpenCV 2.4 <p> else : <p> LONG ... <p> # uh-oh , we got an error -- revert to counting manually <p> except : <p> **30;155;TOOLONG <p> # release the video file pointer <p> video.release() <p> # return the total number of frames in the video <p> returntotal <p> @ @ @ @ @ @ @ @ @ @ video file via the API by provided by OpenCV , we need to utilize what are called- capture- properties , or what OpenCV calls- CAPPROP- anytime you see a constant starting with CAPPROP*- , you should know its related to video processing . <p> In OpenCV 3 the name of the frame count property is cv2.CAPPROPFRAMECOUNT- while in OpenCV 2.4 the property is named **27;187;TOOLONG . <p> Ideally , passing the respective property name into the . get- method of the video- pointer will allow us to obtain the total number of frames in the video ( Lines 25-30 ) . <p> However , there are cases where this method will fail based on your particular OpenCV install and video codecs . <p> If this is the case , we have wrapped our- critical code section with a try/except- block . If an exception occurs we simply revert to counting the frames manually ( Lines 33 and 34 ) . <p> Finally , we release the video file pointer ( Line 37 ) and return the total number of frames to the calling function ( Line 40 ) . <h> @ @ @ @ @ @ @ @ @ @ <p> Weve seen the fast , efficient method to counting frames in a video let 's now move on to the slower method called countframesmanual- . <p> Count the total number of frames in a video with <p> 59 <p> 60 <p> **27;216;TOOLONG : <p> # initialize the total number of frames read <p> total=0 <p> # loop over the frames of the video <p> whileTrue : <p> # grab the current frame <p> ( grabbed , frame ) =video.read() <p> # check to see if we have reached the end of the <p> # video <p> ifnotgrabbed : <p> break <p> # increment the total number of frames read <p> total+=1 <p> # return the total number of frames in the video file <p> returntotal <p> As we can see , countframesmanual- requires only a single argument , video- , which we assume to be a pointer instantiated by cv2.VideoCapture- . @ @ @ @ @ @ @ @ @ @ from the video- file , loop over the frames until we have reached the end of the video , and increment the total- counter along the way . <p> The total- is then returned to the calling function . <p> Its worth mentioning that this method is totally accurate and error free . If you- do get an error its almost certainly related to a problem with your video codecs or an invalid path to a video file . <p> There also might be times when using this function you get a total of- zero frames returned . When this happens its 99% likely that : <p> You supplied an invalid path to cv2.VideoCapture- . <p> You do n't  have the proper video codecs installed and thus OpenCV can not read the file . If this is the case you 'll need to install the proper video codecs , followed by re-compiling and re-installing OpenCV . <p> Note : I used the website keepvid.com to download a . mp4 of the trailer . I do not own the copyrights to this movie or trailer . This script is only for demonstration @ @ @ @ @ @ @ @ @ @ test out the- fast method , execute the following command : <p> Count the total number of frames in a video with OpenCV and Python <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> 6 <p> $timepython framecounter.py--video **30;245;TOOLONG <p> INFO4,790total frames readfrom jurassicparktrailer.mp4 <p> real0m0.803s <p> user0m0.176s <p> sys0m0.112s <p> Figure 1 : Determining the total number of frames in a video file using OpenCV properties . <p> As my results demonstrate , it takes 0.803s to determine there are a total of 4,790 frames in the video file . <p> Figure 2 : Manually counting the total number of frames in a video file using OpenCV and Python . <p> Here we see that it takes- 1m55s ( an increase of approximately 14,221% ) to return a frame count of- 4,978 which differs Method #1 by- 188 frames . <p> Why the discrepancy in frame counts ? <p> It all comes down to- fast and approximate versus- slow but accurate . <p> Using OpenCVs video capture properties we get a total frame count- very fast , but it might not be 100% dead on @ @ @ @ @ @ @ @ @ @ entirely due to OpenCV/video codec versioning . <p> On the other hand , if we- manually count the number of frames it will take us a long time , but the total number of returned frames will be- exact . <p> In situations where you absolutely , positively , must have the- exact count , go with Method #2 . <p> If you need a rough approximation , go with Method #1 ( unless it fails , then you 'll be reverting back to Method #2 anyway ) . <h> Summary <p> In this blog post I demonstrated two methods to count the number of frames in a video file using OpenCV and Python . <p> The first method is- super quick , relying- on OpenCVs video property functionality , allowing us to ( nearly ) instantaneously determine the number of frames in a video file . <p> However , this method is quite buggy , is prone to failure ( based on your OpenCV + video codec versions ) , and may even return nonsensical results . <p> In that case , we need to revert to our second method @ @ @ @ @ @ @ @ @ @ video . While excruciatingly slow , this method has the advantage of being- 100%- accurate . <p> In order to balance speed versus potential failure I have created the countframes- function and placed it inside the imutils library . <p> This function will attempt Method #1 first , and if it fails , automatically revert to Method #2 . <p> In next weeks blog post well be using this function to aid us in generating and visualizing video barcodes . <p> See you next week ! <p> To be notified when the next blog posts goes live , be sure to enter your email address in the form below ! <h> Downloads : 55217 @qwx675217 <h> 8 Responses to Count the total number of frames in a video with OpenCV and Python <p> Chroma keying can be applied by using segmentation techniques . The best results come from using a background color that has significantly different color than your foreground allowing you to segment the foreground from the background and then apply the foreground to a new background . I cover the basics of color-based thresholding inside my book , @ @ @ @ @ @ @ @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485089 @185089/ <h> Histogram of Oriented Gradients ( and car logo recognition ) <p> Histogram of Oriented Gradients ( and car logo recognition ) <p> Histogram of Oriented Gradients , or HOG for short , are descriptors mainly used in computer vision and machine learning for object detection . However , we can also use HOG descriptors for quantifying and representing both shape- and texture . <p> Collecting all Histograms of Oriented gradients to form the final feature vector . <p> The most important parameters for the HOG descriptor are the orientations- , pixelspercell- , and the cellsperblock- . These three parameters ( along with the size of the input image ) effectively control the dimensionality of the resulting feature vector . Well be reviewing these parameters and their implications later in this article . <p> In most real-world applications , HOG is used in conjunction with a Linear SVM to perform object detection . The reason HOG is utilized so heavily is because local object appearance and shape can be characterized using the distribution of local intensity gradients . In fact , these are the exact same image gradients that @ @ @ @ @ @ @ @ @ @ now we are going to take these image gradients and turn them into a robust and powerful image descriptor . <p> Well be discussing the steps necessary to combine both HOG and a Linear SVM into an object classifier later in this course . But for now just understand that HOG is mainly used as a descriptor for object detection and that later these descriptors can be fed into a machine learning classifier . <p> HOG is implemented in both OpenCV and scikit-image . The OpenCV implementation is less flexible than the scikit-image implementation , and thus we will primarily used the scikit-image implementation throughout the rest of this course . <h> Objectives : <p> In this lesson , we will be discussing the Histogram of Oriented Gradients image descriptor in detail . <h> What are HOG descriptors used to describe ? <p> HOG descriptors are mainly used to describe the structural shape and appearance of an object in an image , making them excellent descriptors for object classification . However , since HOG captures local intensity gradients and edge directions , it also makes for a good texture descriptor @ @ @ @ @ @ @ @ @ @ . The dimensionality of this feature vector is dependent on the parameters chosen for the orientations- , pixelspercell- , and cellsperblock- parameters mentioned above . <h> How do HOG descriptors work ? <p> The cornerstone of the HOG descriptor algorithm is that appearance of an object can be modeled by the distribution of intensity gradients inside rectangular regions of an image : <p> Figure 1 : An example of computing a histogram of oriented gradients for a particular region of an image . <p> Implementing this descriptor requires dividing the image into small connected regions called cells , and then for each cell , computing a histogram of oriented gradients for the pixels within each cell . We can then accumulate these histograms across multiple cells to form our feature vector . <p> Dalal and Triggs also demonstrated that we can perform block normalization- to improve performance . To perform block normalization we take groups of overlapping cells , concatenate their histograms , calculate a normalizing value , and then contrast normalize the- histogram . By normalizing over multiple , overlapping blocks , the resulting descriptor is more robust to @ @ @ @ @ @ @ @ @ @ type of normalization implies that each of the cells will be represented in the final feature vector multiple times but normalized by a slightly different set of neighboring cells . <p> Now , let 's review each of the steps for computing the HOG descriptor . <h> Step 1 : Normalizing the image prior to description . <p> This normalization step is entirely optional , but in some cases this step can improve performance of the HOG descriptor . There are three main normalization methods that we can consider : <p> Gamma/power law normalization : - In this case , we take the of each pixel in the input image . However , as Dalal and Triggs demonstrated , this approach is perhaps an " over-correction " and hurts performance . <p> Square-root normalization : - Here , we take the of each pixel in the input image . By definition , square-root normalization compresses the input pixel intensities far less than gamma normalization . And again , as Dalal and Triggs demonstrated , square-root normalization actually increases accuracy rather than hurts it . <p> Variance normalization : - A slightly @ @ @ @ @ @ @ @ @ @ , we compute both the mean and standard deviation of the input image . All pixels are mean centered by subtracting the mean from the pixel intensity , and then normalized through- dividing by the standard deviation : . Dalal and Triggs do not report accuracy on variance normalization ; however , it is a form of normalization that I like to perform and thought it was worth including . <p> In most cases , its best to start with either no normalization or square-root normalization . Variance normalization is also worth consideration , but in most cases it will perform in a similar manner to square-root normalization ( at least in my experience ) . <h> Step 2 : Gradient computation <p> The first actual step in the HOG descriptor is to compute the image gradient in both the x- and y- direction. - These calculations- should seem familiar , as we have already reviewed them in the Gradients lesson . <p> Well apply a convolution operation to obtain the gradient images : <p> and <p> where is the input image , is our- filter in the x-direction , @ @ @ @ @ @ @ @ @ @ a matter of completeness , here is an example of computing both the x- and y- gradient of an input image : <p> Now that we have our gradient images , we can compute the final gradient magnitude representation of the image : <p> Which we can see below : <p> Figure 3 : Obtaining our final gradient magnitude image . <p> Finally , the orientation of the gradient for each pixel in the input image can then be computed by : <p> Given both and , we can now compute a histogram of oriented gradients , where the bin of the histogram is based on and the- contribution or- weight added to a given bin of the histogram is based on . <h> Step 3 : Weighted votes in each cell <p> Now that we have our gradient magnitude and orientation representations , we need to divide our image up into cells and blocks . <p> A " cell " is a rectangular region defined by the number of pixels that belong in each cell . For example , if we had a 128 x- 128- image and defined @ @ @ @ @ @ @ @ @ @ have 32 x- 32 = 1024 cells : <p> Obviously , this is quite an exaggerated example ; we would likely never be interested in a- 1 x 1 cell representation . Instead , this demonstrates- how we can divide an image into cells based on the number of pixels per cell . <p> Now , for each of the cells in the image , we need to construct a histogram of oriented gradients using our gradient magnitude and orientation mentioned above . <p> But before we construct this- histogram , we need to define our number of orientations- . The number of orientations- control the number of bins in the resulting histogram . The gradient angle is either within the range ( unsigned ) or ( signed ) . In general , its preferable to use unsigned gradients in the range with orientations- somewhere in the range . But depending on your application , using signed gradients over unsigned gradients can improve accuracy . <p> Finally , each pixel contributes a weighted vote to the histogram the weight of the vote is simply the gradient magnitude at the given @ @ @ @ @ @ @ @ @ @ a look at our example image divided up into 16 x- 16- pixel cells : <p> Figure 7 : First , let 's divide up image up into 16 x 16 pixel cells . <p> And then for each of these cells , we are going to compute a histogram of oriented gradients using 9 orientations- ( or bins ) per histogram : <p> Figure 8 : - An example of a histogram of gradients using the orientations as bins . Well construct a histogram like this for each of the 16 x 16 pixel cells . <p> Here is a more revealing- animation were we can visually see a different histogram computed for each of the cells : <p> Figure 9 : Computing a histogram of oriented gradients for each cell in the image . <p> At this point , we could collect and concatenated each of these histograms to form our final feature vector . However , its beneficial to apply block normalization , which well review in the next section . <h> Step 4 : Contrast normalization over blocks <p> To account for- changes in illumination and contrast @ @ @ @ @ @ @ @ @ @ requires grouping the " cells " together into larger , connecting " blocks " . It is common for these blocks to overlap , meaning that each cell contributes to the final feature vector more than once . <p> Again , the number of blocks are rectangular ; however , our units are no longer pixels they are the cells ! Dalal and Triggs report that using either 2 x 2- or 3 x 3- cellsperblock- obtains reasonable accuracy in most cases . <p> Here is an example where we have taken an- input region of an image , computed a gradient histogram for each cell , and then locally grouped the cells into overlapping- blocks : <p> Figure 10 : Contrast normalization is performed over multiple overlapping cells . Using this scheme , each cell is represented in the histogram multiple times ; however , this redundancy actually improves accuracy . <p> For each of the cells in the current block we concatenate their corresponding gradient histograms , followed by either L1 or L2 normalizing the entire concatenated feature vector. - Again , performing this type of normalization implies @ @ @ @ @ @ @ @ @ @ final feature vector multiple times but normalized by a different value . While this multi-representation is redundant and wasteful of space , it actually increases performance of the descriptor . <p> Finally , after all blocks are normalized , we take the resulting histograms , concatenate them , and treat them as our final feature vector . <h> Where are HOG descriptors implemented ? <p> HOG descriptors are implemented inside the OpenCV and scikit-image library . However , the OpenCV implementation is not very flexible and is primarily geared towards the Dalal and Triggs implementation . The scikit-image implementation is far more flexible , and thus we will primarily use the scikit-image implementation throughout this course . <h> How do I use HOG descriptors ? <p> Here is an example of how to compute HOG descriptors using scikit-image : <p> Extracting HOG Features <p> Python <p> 1 <p> 2 <p> 3 <p> fromskimage importfeature <p> LONG ... <p> cellsperblock= ( 2,2 ) , transformsqrt=True ) <p> Note : In scikit-image==0.12- , the normalise- parameter has been updated to transformsqrt- . The transformsqrt- performs the exact same operation , only with @ @ @ @ @ @ @ @ @ @ of scikit-image- ( again , before the- v0.12 release ) , then you 'll want to change transformsqrt- to normalise- . <p> We can also visualize the resulting HOG image <p> 9 <p> 10 <p> fromskimage importexposure <p> fromskimage importfeature <p> importcv2 <p> LONG ... <p> LONG ... <p> LONG ... <p> **25;277;TOOLONG " uint8 " ) <p> cv2.imshow ( " HOG Image " , hogImage ) <p> Figure 11 : Visualizing the HOG descriptor . Its not a perfect representation of the HOG feature vector , but it can lend some insight into the impact your parameter choices make . <h> Identifying car logos using HOG descriptors <p> But what if we could also identify the brand of car based on its logo ? <p> Now that would be pretty cool . <p> In the- remainder of this lesson , I 'll demonstrate how we can use the Histogram of Oriented Gradients descriptor to characterize the logos of car brands . Just like in the- Haralick texture lesson @ @ @ @ @ @ @ @ @ @ aide us in the classification ( which is a pretty common practice when it comes to identifying non-trivial shapes and objects ) . <p> Again , I wont be performing a deep dive on the ( very small amount of ) machine learning well be using in this lesson we have the entire- Image Classification module for that . If by the end of this lesson there are a couple of lines of code that feel a little bit like " black box magic " to you , that 's okay and to be expected . Remember , the point of these image descriptor lessons is providing a quick demonstration of how you can use them in your own applications . The rest of the modules in the PyImageSearch Gurus course will help fill in any gaps . <p> But before we dive deep into this project , let 's look at our dataset . <h> Dataset <p> For each of these brands , I gathered five- training images from Google . These images are the- example images- well use to teach our machine learning algorithm what each of the car logos @ @ @ @ @ @ @ @ @ @ : <p> After gathering the images from Google , I then went outside and took a stroll around the local parking lot , snapping seven photos of car logos . These logos will serve as our- test set that we can use to evaluate the performance of our classifier . The seven testing images are displayed below : <h> Our goal <p> Extract HOG features from our training set to characterize and quantify each car logo . <p> Train a machine learning classifier- to distinguish between each car logo . <p> Apply a classifier to recognize new , unseen car logos . <h> Recognizing car logos <p> Alright , enough talk . Let 's start coding up this example . Open up a new file , name it recognizecarlogos.py- , and let 's 17 <p> 18 <p> 19 55203 @qwx675203 <p> fromsklearn.neighbors **26;304;TOOLONG <p> fromskimage importexposure <p> fromskimage importfeature <p> fromimutils importpaths @ @ @ @ @ @ @ @ @ @ argument parse and parse command line arguments 55206 @qwx675206 <p> LONG ... to the logos training dataset " ) <p> LONG ... to the test dataset " ) 55208 @qwx675208 <p> # initialize the data matrix and labels <p> print " INFO extracting features ... " <p> data= <p> labels= <p> This code should look fairly similar to our code from the Haralick texture example . Parsing our command line arguments , we can see that we need two switches . The first is --training- , which is the path to where the example car logos reside on disk . The second switch is --testing- , the path to our directory of testing images well use to evaluate our car logo classifier . <p> Well also initialize data- and labels- , two lists that will hold the HOG features and car brand name for each image in our training set , respectively . <p> Let 's go ahead and extract HOG features from our <p> 48 <p> 49 <p> # loop over the image paths in the training set <p> forimagePath inpaths.listimages ( args " training " ) : <p> # extract the make of the car <p> make=imagePath.split ( " / " ) -2 <p> # load the image , convert it to grayscale , and detect edges <p> **27;332;TOOLONG 55215 @qwx675215 <p> **29;361;TOOLONG <p> # find contours in the edge map , keeping only the largest one which <p> # is presumed to be the car logo <p> LONG ... 55211 @qwx675211 <p> c=max ( cnts , key=cv2.contourArea ) <p> # extract the logo of the car and resize it to a canonical width <p> # and height <p> ( x , y , w , h ) =cv2.boundingRect(c) <p> logo=grayy:y+h , x:x+w <p> logo=cv2.resize ( logo , ( 200,100 ) ) <p> # extract Histogram of Oriented Gradients from the logo <p> LONG ... <p> cellsperblock= ( 2,2 ) , transformsqrt=True ) <p> # update the data and labels @ @ @ @ @ @ @ @ @ @ start looping over each of the image paths in the training directory . An example image path looks like this : - carlogos/audi/audi01.png <p> Using this- image path , we are able to extract the make of the car on Line 24 by splitting the path and extracting the second sub-directory name , or in this case audi- . <p> From there , well perform a bit of pre-processing and prepare the car logo to be described using the Histogram of Oriented Gradients descriptor . All we need to do is load the image from disk , convert it to grayscale , and then use our handy autocanny- function to detect edges in the brand logo : <p> Notice how in each case were are able to find the outline of the car logo . <p> Anytime we- detect an outline , you can be sure that the next step is ( almost always ) to- find the contour of the outline . In fact , that is exactly what- Lines 33-35 do extract the largest contour in the edge map , assumed to be the outline of the car @ @ @ @ @ @ @ @ @ @ largest contour region , compute the bounding box , and extract the ROI . <p> Be sure to pay attention to- Line 41 , - because its- extremely important . As I mentioned earlier in this lesson , - having various widths and heights for your image can lead to HOG feature vectors of different sizes - in nearly all situations this is not the intended behavior that you want ! <p> Think of it this way : let 's assume that I extracted a HOG feature vector of size 1,024-d from Image A.- And then I extracted a HOG feature vector ( using the exact same HOG parameters ) from Image B , which had different dimensions ( i.e. width and height ) than Image A , leaving me with a feature vector of size 512-d . <p> How would I go about comparing these two feature vectors ? <p> The short answer is that you- ca n't . <p> Remember , our extracted feature vectors are supposed to characterize and represent the visual contents of an image . And if our feature vectors are not the same dimensionality , then @ @ @ @ @ @ @ @ @ @ we can not compare our feature vectors for similarity , we are unable to compare our two images at all ! <p> Because of this , when extracting HOG features from a dataset of images , you 'll want to define a- canonical , known size- that each image will be resized to . In many cases , this means that you 'll be throwing away the aspect ratio of the image . Normally , destroying the aspect ratio of an image should be avoided- but in this case we are happy to do it , because it ensures ( 1 ) that each image in our dataset is described in a consistent manner , and ( 2 ) each feature vector is of the same dimensionality . Well be discussing this point much more when we reach the- Custom Object Detector module . <p> Anyway , now that our logo is resized to a known , pre-defined- 200 x 100 pixels , we can then apply the HOG descriptor using orientations=9- , pixelspercell= ( 10,10 ) - , cellsperblock= ( 2,2 ) - , and square-root normalization . These parameters were @ @ @ @ @ @ @ @ @ @ you should expect to do this as well whenever you use the HOG descriptor . Running experiments and tuning the HOG parameters based on these parameters is a critical component in obtaining an accurate classifier . <p> Finally , given the HOG feature vector , we then update our data- matrix and labels- list with the feature vector and car make , respectively . <p> Given our data and labels we can now train our classifier : <p> recognizecarlogos.py <p> Python <p> 51 <p> 52 <p> 53 <p> 54 <p> 55 <p> # " train " the nearest neighbors classifier <p> print " INFO training classifier ... " <p> **40;392;TOOLONG <p> model.fit ( data , labels ) <p> print " INFO evaluating ... " <p> To recognize and distinguish the difference between our five car brands , we are going to use scikit-learns KNeighborsClassifier . <p> The k-nearest neighbor classifier is a type of " lazy learning " algorithm where nothing is actually " learned " . Instead , the k-Nearest Neighbor ( k-NN ) training phase- simply accepts a set of feature vectors and labels and stores them that 's @ @ @ @ @ @ @ @ @ @ a new feature vector , it accepts the feature vector , computes the distance to all stored feature vectors ( normally using the Euclidean distance , but any distance metric or similarity metric can be used ) , sorts them by distance , and returns the top- k- " neighbors " to the input feature vector . From there , each of the- k neighbors vote as to what they think the label of the classification is . You can read more about the k-NN algorithm in this lesson . <p> In our case , we are simply passing the HOG feature vectors and labels to our k-NN algorithm and ask- it to report back what is the closest logo to our query features using- k=1 neighbors . <p> Let 's see how we can use our k-NN classifier to recognize various car logos @ @ @ @ @ @ @ @ @ @ test " ) : <p> # load the test image , convert it to grayscale , and resize it to <p> # the canonical size <p> **27;434;TOOLONG 55215 @qwx675215 <p> logo=cv2.resize ( gray , ( 200,100 ) ) <p> # extract Histogram of Oriented Gradients from the test image and <p> # predict the make of the car <p> LONG ... <p> LONG ... <p> **28;463;TOOLONG ( 1 , -1 ) ) 0 <p> # visualize the HOG image <p> LONG ... <p> **25;493;TOOLONG " uint8 " ) <p> cv2.imshow ( " HOG Image " , hogImage ) <p> # draw the prediction on the test image and display it <p> LONG ... <p> ( 0,255,0 ) , 3 ) <p> cv2.imshow ( " Test Image " , image ) 55212 @qwx675212 <p> Line 58 starts looping over the images in our testing set . For each of these images , well load it from disk , ; convert it to grayscale ; resize it to a known , fixed size ; and then extract HOG feature vectors from it in the exact same manner as we did in the @ @ @ @ @ @ @ @ @ @ then makes a call to our k-NN classifier , passing in our HOG feature vector for the current testing image and asking the classifier what it thinks the logo is . <p> We can also- visualize- our Histogram of Oriented Gradients image on- Lines 72-74 . This is especially useful when debugging your HOG parameters to ensure the contents of our image are being adequately quantified . Below are some examples of the HOG image our testing car logos : <p> Figure 14 : A sample of our HOG images visualized . Can you tell which logo is which based only on the visualized HOG images ? Look closely . I bet you can ! <p> Notice how the HOG image is always 200 x 100 pixels , which are the dimensions of our resized testing image . We can also see how the pixelspercell- and orientations- parameters come into play here , as well as the dominant orientation of each cell , where the size of the cell is defined by the pixelspercell- . The more pixels in the pixelspercell- , the more coarse our representation is . And @ @ @ @ @ @ @ @ @ @ representations . Visualizing a HOG image is an excellent way to " see " what your HOG descriptor and parameter set is describing . <p> Finally , we take the result of the classification , draw it on our test image , and display it to our screen on- Lines 77-80 . <p> To give our car logo classifier a try , simply open up your terminal and execute the following command : <p> recognizecarlogos.py <p> Python <p> 1 <p> $python **30;520;TOOLONG carlogos--testtestimages <p> And below you 'll see our output classification : <p> Figure 15 : Classifying the brand of car based only on the HOG features is a simple task for your k-NN classifier due to how discriminative the HOG features are . <p> In each case , we were able to correctly classify the brand of car using HOG features ! <p> Of course , this approach only worked , because we had a tight cropping of the car logo . If we had described- the- entire image of a car , it is very unlikely that we would have been able to correctly classify the brand . @ @ @ @ @ @ @ @ @ @ get to the- Custom Object Detector module , specifically sliding windows and image pyramids . <p> In the meantime , this example was still able to demonstrate how to use the Histogram of Oriented Gradients- descriptor and the k-NN classifier to recognize the logos of cars . The key takeaway here is that if you can consistently detect and extract the ROI of your image dataset , the HOG descriptor should definitely be on your list of image descriptors to apply , as its very powerful and able to obtain good results , especially when applied in conjunction with machine learning . <h> Suggestions when using HOG descriptors : <p> HOG descriptors are very powerful ; however , it can be tedious to choose the correct parameters for the number of orientations- , pixelspercell- , and cellsperblock- , especially when you start working with object classification . <p> As a starting point , I tend to use orientations=9- , pixelspercell= ( 4,4 ) - , and cellsperblock= ( 2,2 ) - , then go from there . Its unlikely that your first set of parameters will yield the best- performance @ @ @ @ @ @ @ @ @ @ a baseline results can be improved via parameter tuning . <p> Its also important to resize your image to a reasonable size . If your input region is 32- x 32- pixels , then the resulting dimensionality would be 1,764-d . But if your input region- is- 128- x- 128- pixel and you again used the above parameters , your feature vector would be 34,596-d ! By using large image regions and not paying attention to your HOG parameters , you can end up with extremely large feature vectors . <p> Well be utilizing HOG descriptors later in this course for object classification , so if you 're a little confused on how to properly tune the parameters , do n't  worry this wont be the last time you see these descriptors ! <h> HOG Pros and Cons <p> Pros : <p> Very powerful descriptor . <p> Excellent at representing local appearance . <p> Extremely useful for representing structural objects that do not demonstrate substantial variation in form ( i.e. buildings , people walking the street , bicycles leaning against a wall ) . <p> Very accurate for object classification . @ @ @ @ @ @ @ @ @ @ vectors , leading to large storage costs and computationally expensive feature vector comparisons . <p> Not the slowest descriptor to compute , but also nowhere near the fastest . <p> If the object to be described exhibits substantial structural variation ( i.e. the rotation/orientation of the object is consistently different ) , then the standard vanilla implementation of HOG will not perform well . 
@@71485091 @185091/ <h> Creating a simple dataset <p> Figure 1 : Our four example images that well be applying text skew correction to with OpenCV and Python . <p> The text block itself is from Chapter 11 of my book , - Practical Python and OpenCV , where I 'm discussing contours and how to utilize them for image processing and computer vision . <p> The filenames of the four files follow : <p> Text skew correction with OpenCV and Python <p> Shell <p> 1 <p> 2 <p> $lsimages/ <p> **35;552;TOOLONG <p> The first part of the filename specifies whether our image has been rotated- counter-clockwise ( negative ) or- clockwise ( positive ) . <p> The second component of the filename is the actual number of- degrees the image has been rotated by . <p> The goal our text skew correction algorithm will be to correctly determine the- direction and- angle of the rotation , then correct for it . <p> To see how our text skew correction algorithm is implemented with OpenCV and Python , be sure to read the next section . <h> Deskewing text with OpenCV and Python @ @ @ @ @ @ @ @ @ @ and name it correctskew.py- . <p> From there , insert the following code : <p> Text skew 11 <p> 12 <p> 13 55203 @qwx675203 55220 @qwx675220 55218 @qwx675218 <p> importcv2 55202 @qwx675202 55206 @qwx675206 <p> ap.addargument ( " -i " , " --image " , required=True , <p> help= " path to input image file " ) 55208 @qwx675208 <p> # load the image from disk <p> image=cv2.imread ( args " image " ) <p> Lines 2-4 import our required Python packages . Well be using OpenCV via our cv2- bindings , so if you do n't  already have OpenCV installed on your system , please refer to my list of OpenCV install tutorials- to help you get your system setup and configured . <p> We then parse our command line arguments on- Lines 7-10 . We only need a single argument here , --image- , which is the path to our input image . <p> The- image is then loaded from @ @ @ @ @ @ @ @ @ @ to isolate the text in the image : <p> Text skew correction with <p> 23 <p> 24 <p> # convert the image to grayscale and flip the foreground <p> # and background to ensure foreground is now " white " and <p> # the background is " black " 55215 @qwx675215 <p> **25;589;TOOLONG <p> # threshold the image , setting all foreground pixels to <p> # 255 and all background pixels to 0 <p> **31;616;TOOLONG , <p> **30;649;TOOLONG ) 1 <p> Our input images contain text that is- dark on a- light background ; however , to apply our text skew correction process , we first need to invert the image ( i.e. , the text is now- light- on a- dark background we need the inverse ) . <p> When applying computer vision and image processing operations , its common for the foreground to be represented as- light while the background ( the part of the image we are not interested in ) is- dark . <p> A @ @ @ @ @ @ @ @ @ @ applied to binarize the image : <p> Figure 2 : Applying a thresholding operation to binarize our image . Our text is now white on a black background . <p> Given this thresholded image , we can now compute the minimum rotated bounding box that contains the text regions : <p> Text skew correction with <p> 42 <p> 43 <p> # grab the ( x , y ) coordinates of all pixel values that <p> # are greater than zero , then use these coordinates to <p> # compute a rotated bounding box that contains all <p> # coordinates <p> **30;681;TOOLONG ( thresh&gt;0 ) ) <p> **31;713;TOOLONG <p> # the cv2.minAreaRect function returns values in the <p> # range -90 , 0 ) ; as the rectangle rotates clockwise the <p> # returned angle trends to 0 -- in this special case we <p> # need to add 90 degrees to the angle @ @ @ @ @ @ @ @ @ @ take the inverse of the angle to make <p> # it positive <p> else : <p> angle=-angle <p> Line 30 finds all- ( x , y ) -coordinates in the thresh- image that are part of the foreground . <p> We pass these coordinates into cv2.minAreaRect- which then computes the minimum rotated rectangle that contains the- entire text region . <p> The cv2.minAreaRect- function returns angle values in the range- -90 , 0 ) . As the rectangle is rotated clockwise the angle value increases towards zero . When zero is reached , the angle is set back to -90 degrees again and the process continues . <p> Lines 37 and 38 handle if the angle is less than -45 degrees , in which case we need to add 90 degrees to the angle and take the inverse . <p> Otherwise , - Lines 42 and 43 simply take the inverse of the angle . <p> Now that we have determined the text skew angle , we need to apply an affine transformation to correct for the skew : <p> Text skew correction with OpenCV and Python <p> Python <p> @ @ @ @ @ @ @ @ @ @ 50 <p> # rotate the image to deskew it <p> ( h , w ) =image.shape:2 <p> center= ( w//2 , h//2 ) <p> **32;746;TOOLONG , angle , 1.0 ) <p> **28;780;TOOLONG , M , ( w , h ) , <p> LONG ... <p> Lines 46 and 47 determine the center- ( x , y ) -coordinate of the image . We pass the center- coordinates and rotation angle into the cv2.getRotationMatrix2D- ( Line 48 ) . This rotation matrix M- is then used to perform the actual transformation on- Lines 49 and 50 . <p> Finally , we display the results to our screen : <p> Text skew correction with OpenCV and Python <p> Python <p> 52 <p> 53 <p> 54 <p> 55 <p> 56 <p> 57 <p> 58 <p> 59 <p> 60 <p> # draw the correction angle on the image so we can validate it <p> cv2.putText ( rotated , " Angle : : .2f degrees " . format(angle) , <p> LONG ... <p> # show the output image <p> print ( " INFO angle : : .3f " . format(angle) ) <p> cv2.imshow ( " @ @ @ @ @ @ @ @ @ @ " , rotated ) 55212 @qwx675212 <p> Line 53 draws the angle- on our image so we can verify that the output image matches the rotation angle ( you would obviously want to remove this line in a document processing pipeline ) . <p> Lines 57-60 handle displaying the output image . <h> Skew correction results <p> To grab the code + example images used inside this blog post , be sure to use the- " Downloads " section at the bottom of this post . <p> From there , execute the following command to correct the skew for our neg4.png- image : <p> Text skew correction with OpenCV and Python <p> Shell <p> 1 <p> 2 <p> $python correctskew.py--image images/neg4.png <p> INFOangle : -4.086 <p> Figure 3 : Applying skew correction using OpenCV and Python . <p> Here we can see that that input image has a counter-clockwise skew of 4 degrees . Applying our skew correction with OpenCV detects this 4 degree skew and corrects for it . <p> Here is another example , this time with a counter-clockwise skew of 28 degrees : <p> Text skew correction @ @ @ @ @ @ @ @ @ @ <p> $python correctskew.py--image images/neg28.png <p> INFOangle : -28.009 <p> Figure 4 : Deskewing images using OpenCV and Python . <p> Again , our skew correction algorithm is able to correct the input image . <p> This time , let 's try- a clockwise skew : <p> Text skew correction with OpenCV and Python <p> Shell <p> 1 <p> 2 <p> $python correctskew.py--image images/pos24.png <p> INFOangle:23.974 <p> Figure 5 : Correcting for skew in text regions with computer vision . <p> And finally a more extreme clockwise skew of 41 degrees : <p> Text skew correction with OpenCV and Python <p> Shell <p> 1 <p> 2 <p> $python correctskew.py--image images/pos41.png <p> INFOangle:41.037 <p> Figure 6 : Deskewing text with OpenCV . <p> Regardless of skew angle , our algorithm is able to correct for skew in images using OpenCV and Python . <h> Interested in learning more about computer vision and OpenCV ? <p> If you 're interested in learning more about the fundamentals of computer vision and image processing , be sure to take a look at my book , - Practical Python and OpenCV : <p> Inside the book you 'll learn @ @ @ @ @ @ @ @ @ @ way up to more advanced topics such as- face detection , - object tracking in video , and- handwriting recognition , all with lots of examples , code , and detailed walkthroughs . <p> If you 're interested in learning more ( and how my book can teach you these algorithms in less than a single weekend ) , just click the button below : <h> Summary <p> The algorithm itself is quite straightforward , relying on only basic image processing techniques such as thresholding , computing the minimum area rotated rectangle , and then applying an affine transformation to correct the skew . <p> We would- commonly use this type of text skew correction in an automatic document analysis pipeline where our goal is to digitize a set of documents , correct for text skew , and then apply OCR to convert the text in the image to machine-encoded text . <p> I hope you enjoyed todays tutorial ! <p> To be notified when future blog posts are published , - be sure to enter your email address in the form below ! <h> Downloads : 55217 @qwx675217 <p> This @ @ @ @ @ @ @ @ @ @ of justified text ( or at least left or right aligned with many lines ) . Better approach would be detecting blank space between lines and finding mean angle of lines fitting in this space . Is there a way to do this efficiently ? <p> I implemented this technique in an application some time ago . It is simple and fast . But it is not very suitable when you have more complex layouts such as forms . The technique that work more accurately was to rotate the image in several angles from negative to positive and count what angle produced more white pixels ( on binarized iamge ) in every row . But this is too slow . Could you point me to some approach that is faster than this , please ? <p> Thanks Adrian for this informative post . It took me time to figure out how cv2.minAreaRect works . If the angle returned by the cv2.minAreaRect is bordering -45 ( let say -50 or -55 ) , will this code not lead to the text being skewed in a perpendicular direction rather than a horizontal @ @ @ @ @ @ @ @ @ @ used your codes and help so many times but never gave thanks . Currently I am working on a summer project ( just for fun ) and I needed something like this to process rotated musical sheets . Works perfectly ! <p> Hi Adrian , I 'm using your code but getting the " cords " using cv2.findContours . When doing that , my object was being rotated to the wrong direction . To fix that I needed to change " angle = - ( 90 + angle ) " to " angle = ( 90 + angle ) " ; removed the minus sign . What is the difference between the way you got the cords and " cv2.findContours " ? <p> The method I used takes all thresholded ( x , y ) -coordinates and uses them to estimate the rotation . The cv2.findContours function will only find the outline of the region , provided you @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485093 @185093/ <p> Figure 3 : Applying thresholding to reveal the brighter regions of the image . <p> Note how the bright areas of the image are now all- white while the rest of the image is set to- black . <p> However , there is a bit of noise in this image ( i.e. , small blobs ) , so let 's clean it up by performing a series of erosions and dilations : <p> Detecting multiple bright spots in an image with Python and OpenCV <p> Python <p> 24 <p> 25 <p> 26 <p> 27 <p> # perform a series of erosions and dilations to remove <p> # any small blobs of noise from the thresholded image <p> thresh=cv2.erode ( thresh , None , iterations=2 ) <p> thresh=cv2.dilate ( thresh , None , iterations=4 ) <p> After applying these operations you can see that our thresh- image is much " cleaner " , although we do still have a few left over blobs that wed like to exclude ( well handle that in our next step ) : <p> Figure 4 : Utilizing a series of erosions and dilations to @ @ @ @ @ @ @ @ @ @ small blobs and then regrowing the remaining regions . <p> The critical step in this project is to- label each of the regions in the above figure ; however , even after applying our erosions and dilations we 'd still like to filter out any leftover " noisy " regions . <p> Line 32 performs the actual connected-component analysis using the scikit-image library . The labels- variable returned from measure.label- has the exact same dimensions as our thresh- image the only difference is that labels- stores a- unique integer for- each blob in thresh- . <p> We then initialize a mask- on- Line 33 to store only the large blobs . <p> On- Line 36 we start looping over each of the unique labels- . If the label- is zero then we know we are examining the background region and can safely ignore it ( Lines 38 and 39 ) . <p> Otherwise , we construct a mask for- just the current label- on- Lines 43 and 44 . <p> I have provided a GIF animation below that visualizes the construction of the labelMask- for each label- . Use this animation @ @ @ @ @ @ @ @ @ @ are accessed and displayed : <p> Line 45 then counts the number of non-zero pixels in the labelMask- . If numPixels- exceeds a pre-defined threshold ( in this case , a total of- 300 pixels ) , then we consider the blob " large enough " and add it to our mask- . <p> The output mask- can be seen below : <p> Figure 6 : After applying a connected-component analysis we are left with only the larger blobs in the image ( which are also bright ) . <p> Notice how any small blobs have been filtered out and only the large blobs have been retained . <p> The last step is to draw the labeled blobs on our image : <p> Detecting multiple bright spots in an image with <p> 70 <p> 71 <p> # find the contours in the mask , then sort them from left @ @ @ @ @ @ @ @ @ @ **36;810;TOOLONG <p> **33;848;TOOLONG <p> # loop over the contours <p> for ( i , c ) inenumerate(cnts) : <p> # draw the bright spot on the image <p> ( x , y , w , h ) =cv2.boundingRect(c) <p> ( ( cX , cY ) , radius ) **26;883;TOOLONG <p> LONG ... <p> ( 0,0,255 ) , 3 ) <p> cv2.putText ( image , " # " . format(i+1) , ( x , y-15 ) , <p> **27;911;TOOLONG , ( 0,0,255 ) , 2 ) <p> # show the output image <p> cv2.imshow ( " Image " , image ) 55212 @qwx675212 <p> First , we need to detect the contours in the mask- image and then sort them from left-to-right ( Lines 54-57 ) . <p> Once our contours have been sorted we can loop over them individually ( Line 60 ) . <p> For each of these contours well compute the- minimum enclosing circle ( Line 63 ) which represents the- area that the bright region encompasses . <p> We then uniquely label the region and draw it on our image- ( Lines 64-67 ) . <p> Finally @ @ @ @ @ @ @ @ @ @ . <p> To visualize the output for the lightbulb image be sure to download the source code + example images to this blog post using the- " Downloads " section found at the bottom of this tutorial . <p> From there , just execute the following command : <p> Detecting multiple bright spots in an image with Python and OpenCV <p> Shell <p> 1 <p> $python **27;940;TOOLONG images/lights01.png <p> You should then see the following output image : <p> Figure 7 : Detecting multiple bright regions in an image with Python and OpenCV . <p> Notice how each of the lightbulbs has been uniquely labeled with a circle drawn to encompass each of the individual bright regions . <p> This time there are- many lightbulbs in the input image ! However , even with many bright regions in the image our method is still able to correctly ( and uniquely ) label each of them . <h> Summary <p> In this blog post I extended my previous tutorial on detecting the brightest spot in an image to work with- multiple bright regions. - I was able to accomplish this by @ @ @ @ @ @ @ @ @ @ . <p> The key here is the thresholding step if your thresh- map is extremely noisy and can not be filtered using either contour properties or a connected-component analysis , then you wont be able to localize each of the bright regions in the image . <p> This step should be performed- before you even bother applying a connected-component analysis or contour filtering . <p> Provided that you can reasonably segment the light regions from the darker , irrelevant regions of your image then the method outlined in this blog post should work quite well for you . <p> Anyway , I hope you enjoyed this blog post ! <p> Before you go , be sure to enter your email address in the form below to be notified when future tutorials are published on the PyImageSearch blog . <h> Downloads : 55217 @qwx675217 <p> Hey Chris are you using the code downloaded via the " Downloads " section of the blog post ? Or are you copying and pasting the code as you go along ? The reason I ask is because it sounds like contours are not being detected @ @ @ @ @ @ @ @ @ @ few debug statements like print ( len ( cnts ) ) to ensure at least some of the contours are being detected . <p> Go back to the thresholding step and ensure that each of the regions are properly thresholded ( i.e. , your throughout output matches mine ) . Again , it sounds like something strange is happening with the thresholding or the contour extraction process . <p> This method is very fast since its based on thresholding for segmentation followed by optimized connected-component analysis and contour filtering . It can certainly be used in real-time semi-real-time environments for reasonably sized images . <p> Detecting smoke and fire is an active area of research in computer vision and image processing . We typically use machine learning methods combined with feature extraction methods ( or deep learning ) to make an approach like this work across a variety of lighting conditions , environments , etc . I would not recommend using this method directly to detect fire as you would likely obtain many false-positives . <p> Hi Adrian , Thanks so much for sharing your knowledge . Question , how @ @ @ @ @ @ @ @ @ @ light is turned off . In other word , can the label be static ? 1 , 2 , 3 , 4 , 5 =&gt; 1 , 2 , 5 meaning bulb 3 and 4 are off . Assuming the lights are stationary . <p> You can do this , but you would have to start with the lights in a fixed position and all of them " on " . Once you 've determined the ROI for each light just loop over each of the ROIs ( no need to detect them each time ) and compute the mean of the grayscale region . If the mean is low ( close to black ) then the light is off . If the mean is high ( close to white ) then the light is on . <p> Dear Adrian , I face the same problem as Izru . Got all the steps done for installation of opencv . When I try to install scikit , my pi3b gets to a point " Running setup.py bdistwheel for scipi " Then after an hour or two it hangs . I can tell @ @ @ @ @ @ @ @ @ @ , when I turned the screen the system clock was stopped at +1 hour after leaving it to finsh , mouse/kbd not responding . So I had to pull the plug . I 've followed all steps for installation of opencv on my version of pi3b , all packages are up to date . <p> Many thanks for taking interest in my problem and a blazing fast reply . I 've actually solved it myself . While the install was running for the nth time I noticed that the system got very unresponsive even though no significant CPU load was present , so I checked the available memory and voila The system was running out of swap-file space , I 've had the default setting of 100MB out of the box . After changing this to 1024MB the next run was done within 40-50 minutes . <p> Hey Mark make sure you are using the " Downloads " section of the post to download the code rather than copying and pasting from the tutorial . This should help resolve any issues related to whitespacing . <p> Thanks Adrian , I only saw your @ @ @ @ @ @ @ @ @ @ apologies for troubling you over such a trivial issue , thanks for taking the time to answer my question anyway , ill be clicking download from now on , instead of copying and pasting = <p> Hi Adrian , You re doing an excellent job ! I have a simple question you might have answered it a million times = thresh = cv2.threshold ( blurred , 200 , 255 , cv2.THRESHBINARY ) 1 Im wondering what the 1 stands for ? <p> The cv2.threshold function returns a 2-tuple of the threshold value T and the thresholded image . Since we only need the second entry in the tuple , we grab it via 1 . If you 're interested in learning more about the basics of image processing , computer vision , and OpenCV , be sure to refer to my book @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485094 @185094/ <h> Convolutions with OpenCV and Python <p> I 'm going to start todays blog post by asking a series of questions which will then be addressed later in the tutorial : <p> What are image convolutions ? <p> What do they do ? <p> Why do we use them ? <p> How do we apply them ? <p> And what role do convolutions play in deep learning ? <p> The word " convolution " sounds like a fancy , complicated term but its really not . In fact , if you 've ever worked with computer vision , image processing , or OpenCV before , youve- already applied convolutions , - whether you realize it or not ! <p> Ever apply- blurring or- smoothing ? Yep , that 's a convolution . <p> What about- edge detection ? Yup , convolution . <p> Have you opened Photoshop or GIMP to- sharpen an image ? You guessed it convolution . <p> Convolutions are one of the most- critical , fundamental building-blocks in computer vision and image processing . But the term itself tends to scare people off in fact , on the the @ @ @ @ @ @ @ @ @ @ connotation . <h> A Quick Note on PyImageSearch Gurus <p> Before we get started , I just wanted to mention that the first half of this blog post on kernels and convolutions is based on the- " Kernels " lesson inside the PyImageSearch Gurus course . <p> While the Kernels lesson goes into- a lot more detail than what this blog post does , I still wanted to give you a taste- of what PyImageSearch Gurus my- magnum opus on computer vision has to offer . <h> Convolutions with OpenCV and Python <p> Think of it this- way an image is just a- multi-dimensional matrix . Our image has a width ( # of columns ) and a height ( # of rows ) , just like a matrix . <p> But unlike the traditional matrices you may have worked with back in grade school , images also have a- depth to them the number of- channels in the image . For a standard RGB image , we have a depth of- 3 one channel for- each of the Red , Green , and Blue channels , respectively . <p> @ @ @ @ @ @ @ @ @ @ image as a- big matrix and- kernel or- convolutional matrix as a- tiny matrix that is used for blurring , sharpening , edge detection , and other image processing functions . <p> Essentially , this- tiny kernel sits on top of the- big image and slides from left-to-right and top-to-bottom , applying a mathematical operation ( i.e. , a- convolution ) at each- ( x , y ) -coordinate of the original image . <p> So that raises the question , - is there a way to- automatically learn these types of filters ? - And even use these filters for- image classification and- object detection ? <p> You bet there is . <p> But before we get there , we need to understand kernels and convolutions a bit more . <h> Kernels <p> Again , let 's think of an image as a- big matrix and a kernel as- tiny matrix ( at least in respect to the original " big matrix " image ) : <p> Figure 1 : A kernel is a small matrix that slides from left-to-right and top-to-bottom across a larger image . At each pixel @ @ @ @ @ @ @ @ @ @ is convolved with the kernel and the output stored . Source : PyImageSearch Gurus <p> As the figure above demonstrates , we are sliding the kernel from left-to-right and top-to-bottom along the original image . <p> At each- ( x , y ) -coordinate of the original image , we stop and examine the neighborhood of pixels located at the- center- of the image kernel . We then take this neighborhood of pixels , - convolve them with the kernel , and obtain a single output value . This output value is then stored in the output image at the same- ( x , y ) -coordinates as the center of the kernel . <p> If this sounds confusing , no worries , well be reviewing an example in the- " Understanding Image Convolutions " - section later in this blog post . <p> But before we dive into an example , let 's first take a look at what a kernel looks like : <p> On the- left , we have a- 3 x 3- matrix . The center of the matrix is obviously located at- x=1 , y=1 where @ @ @ @ @ @ @ @ @ @ origin and our coordinates are zero-indexed . <p> But on the- right , we have a- 2 x 2- matrix . The center of this matrix would be located at- x=0.5 , y=0.5 . But as we know , without applying interpolation , there is no such thing as pixel location- ( 0.5 , 0.5 ) our pixel coordinates must be integers ! This reasoning is exactly why we use- odd kernel sizes to always ensure there is a valid- ( x , y ) -coordinate at the center of the kernel . <h> Understanding Image Convolutions <p> Now that we have discussed the basics of kernels , let 's talk about a mathematical term called- convolution . <p> In image processing , a convolution requires three components : <p> An input image . <p> A kernel matrix that we are going to apply to the input image . <p> An output image to store the output of the input image convolved with the kernel . <p> Convolution itself is actually very easy . All we need to do is : <p> Select an- ( x , y ) -coordinate from @ @ @ @ @ @ @ @ @ @ kernel at this- ( x , y ) -coordinate . <p> Take the element-wise multiplication of the input image region and the kernel , then sum up the values of these multiplication operations into a single value . The sum of these multiplications is called the- kernel output . <p> Use the same- ( x , y ) -coordinates from- Step #1 , but this time , store the kernel output in the same- ( x , y ) -location as the output image . <p> Below you can find an example of convolving ( denoted mathematically as the- " * " operator ) a- 3 x 3 region of an image with a- 3 x 3 kernel used for blurring : <p> After applying this convolution , we would set the pixel located at the coordinate- ( i , j ) of the output image- O to- Oi , j = 126 . <p> That 's all there is to it ! <p> Convolution is simply the sum of element-wise matrix multiplication between the kernel and neighborhood that the kernel covers of the input image . <h> Implementing Convolutions with @ @ @ @ @ @ @ @ @ @ convolutions but now let 's move on to looking at some actual code to ensure you- understand how kernels and convolutions are implemented . This source code will also help you understand how to- apply convolutions to images . <p> Open up a new file , name it convolutions.py- , and let 's get to work : <p> Convolutions with OpenCV and Python <p> Python <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 55203 @qwx675203 <p> fromskimage.exposure importrescaleintensity 55220 @qwx675220 55218 @qwx675218 <p> importcv2 <p> We start on- Lines 2-5 by importing our required Python packages . You should already have NumPy and OpenCV installed on your system , but you might- not- have scikit-image installed . To install scikit-image , just use pip- : <p> Convolutions with OpenCV and Python <p> 1 <p> $pip install-Uscikit-image <p> Next , we can start defining our custom convolve- method : <p> Convolutions with <p> 18 <p> 19 <p> defconvolve ( image , kernel ) : @ @ @ @ @ @ @ @ @ @ along with <p> # the spatial dimensions of the kernel <p> ( iH , iW ) =image.shape:2 <p> ( kH , kW ) =kernel.shape:2 <p> # allocate memory for the output image , taking care to <p> # " pad " the borders of the input image so the spatial <p> # size ( i.e. , width and height ) are not reduced <p> pad= ( kW-1 ) /2 <p> LONG ... <p> cv2.BORDERREPLICATE ) <p> output=np.zeros ( ( iH , iW ) , dtype= " float32 " ) <p> The convolve- function requires two parameters : the ( grayscale ) image- that we want to convolve with the kernel- . <p> Given both our image- and kernel- ( which we presume to be NumPy arrays ) , we then determine the spatial dimensions ( i.e. , width and height ) of each ( Lines 10 and 11 ) . <p> Before we continue , its important to understand that the process of " sliding " a convolutional matrix across an image , applying the convolution , and then storing the output will actually- decrease the spatial dimensions of our @ @ @ @ @ @ @ @ @ @ that we " center " our computation around the center- ( x , y ) -coordinate of the input image that the kernel is currently positioned over . This implies there is no such thing as " center " pixels for pixels that fall along the- border of the image. - The decrease in spatial dimension is simply a side effect of applying convolutions to images. - Sometimes this effect is desirable and other times its not , it simply depends on your application . <p> However , in most cases , we want our- output image to have the- same dimensions as our- input image . To ensure this , we apply- padding- ( Lines 16-19 ) . Here we are simply replicating the pixels along the border of the image , such that the output image will match the dimensions of the input image . <p> Other padding methods exist , including- zero padding ( filling the borders with zeros very common when building Convolutional Neural Networks ) and- wrap around ( where the border pixels are determined by examining the opposite end of the image ) . @ @ @ @ @ @ @ @ @ @ padding . <p> We are now ready to apply the actual convolution to our image : <p> Convolutions with <p> 37 <p> 38 <p> # loop over the input image , " sliding " the kernel across <p> # each ( x , y ) -coordinate from left-to-right and top to <p> # bottom <p> foryinnp.arange ( pad , iH+pad ) : <p> forxinnp.arange ( pad , iW+pad ) : <p> # extract the ROI of the image by extracting the <p> # *center* region of the current ( x , y ) -coordinates <p> # dimensions <p> roi=imagey-pad:y+pad+1 , x-pad:x+pad+1 <p> # perform the actual convolution by taking the <p> # element-wise multiplicate between the ROI and <p> # the kernel , then summing the matrix <p> k= ( roi*kernel ) . sum() <p> # store the convolved value in the output ( x , y ) - <p> # coordinate of @ @ @ @ @ @ @ @ @ @ and 25 loop over our image- , " sliding " the kernel from left-to-right and top-to-bottom 1 pixel at a time . <p> Line 29 extracts the Region of Interest ( ROI ) from the image- using NumPy array slicing . The roi- will be centered around the current- ( x , y ) -coordinates of the image- . The roi- will also have the same size as our kernel- , which is critical for the next step . <p> Convolution is performed on- Line 34 by taking the element-wise multiplication between the roi- and kernel- , followed by summing the entries in the matrix . <p> The output value k- is then stored in the output- array at the same- ( x , y ) -coordinates ( relative to the input image ) . <p> We can now finish up our convolve- method : <p> Convolutions with OpenCV and Python <p> Python <p> 40 <p> 41 <p> 42 <p> 43 <p> 44 <p> 45 <p> # rescale the output image to be in the range 0 , 255 <p> LONG ... <p> output= ( output*255 ) . astype ( @ @ @ @ @ @ @ @ @ @ <p> returnoutput <p> When working with images , we typically deal with pixel values falling in the range- 0 , 255 . However , when applying convolutions , we can easily obtain values that fall- outside this range . <p> In order to bring our output- image back into the range- 0 , 255 , we apply the rescaleintensity- function of scikit-image ( Line 41 ) . We also convert our image back to an unsigned 8-bit integer data type on Line 42- ( previously , the output- image was a floating point type in order to handle pixel values outside the range- 0 , 255 ) . <p> Finally , the output- image is returned to the calling function on- Line 45 . <p> Now that we 've defined our convolve- function , let 's move on to the driver portion of the script . This section of our program will handle parsing command line arguments , defining a series of kernels we are going to apply to our image , and then displaying the output results : <p> Convolutions with 59 <p> 60 <p> 61 55202 @qwx675202 55206 @qwx675206 <p> ap.addargument ( " -i " , " --image " , required=True , <p> help= " path to the input image " ) 55208 @qwx675208 <p> # construct average blurring kernels used to smooth an image <p> LONG ... <p> LONG ... <p> # construct a sharpening filter <p> sharpen=np.array ( ( <p> 0 , -1,0 , <p> -1,5 , -1 , <p> 0 , -1,0 ) , dtype= " int " ) <p> Lines 48-51 handle parsing our command line arguments . We only need a single argument here , --image- , which is the path to our input path . <p> We then move on to- Lines 54 and 55 which define a- 7 x 7 kernel and a- 21 x 21 kernel used to blur/smooth an image . The larger the kernel is , the more the image will be blurred . Examining this kernel , you can see that the output of applying the kernel to an ROI will simply be the- average of the input region @ @ @ @ @ @ @ @ @ @ , used to enhance line structures and other details of an image . Explaining each of these kernels in detail is outside the scope of this tutorial , so if you 're interested in learning more about kernel construction , I would suggest starting here- and then playing around with the excellent kernel visualization tool on Setosa.io . <p> Finally , well define two Sobel filters- on- Lines 71-80 . The first ( Lines 71-74 ) is used to detect- vertical changes in the gradient of the image . Similarly , - Lines 77-80 constructs a filter used to detect- horizontal changes- in the gradient . <p> Given all these kernels , we lump them together into a set of tuples called a " kernel bank " : <p> Convolutions with <p> 90 <p> 91 <p> # construct the kernel bank , a list of kernels we 're going <p> # to apply using both our custom convole function and <p> # OpenCV 's filter2D function <p> kernelBank= ( <p> @ @ @ @ @ @ @ @ @ @ " largeblur " , largeBlur ) , <p> ( " sharpen " , sharpen ) , <p> ( " laplacian " , laplacian ) , <p> ( " sobelx " , sobelX ) , <p> ( " sobely " , sobelY ) <p> Finally , we are ready to apply our kernelBank- to our --input- image : <p> Convolutions with <p> 111 <p> 112 <p> # load the input image and convert it to grayscale <p> image=cv2.imread ( args " image " ) 55215 @qwx675215 <p> # loop over the kernels <p> for ( kernelName , kernel ) inkernelBank : <p> # apply the kernel to the grayscale image using both <p> # our custom convole function and OpenCV 's filter2D <p> # function <p> print ( " INFO applying kernel " . format(kernelName) ) <p> **27;969;TOOLONG , kernel ) <p> **30;998;TOOLONG , -1 , kernel ) <p> # show the @ @ @ @ @ @ @ @ @ @ ) <p> cv2.imshow ( " - convole " . format(kernelName) , convoleOutput ) <p> cv2.imshow ( " - opencv " . format(kernelName) , opencvOutput ) 55212 @qwx675212 <p> cv2.destroyAllWindows() <p> Lines 95 and 96 load our image from disk and convert it to grayscale . Convolution operators can certainly be applied to RGB ( or other multi-channel images ) , but for the sake of simplicity in this blog post , well only apply our filters to grayscale images ) . <p> We start looping over our set of kernels in the kernelBank- on- Line 99 - and- then- apply the current kernel- to the gray- image on- Line 104 by calling our custom convolve- method which we defined earlier . <p> As a sanity check , we also call cv2.filter2D- which also applies our kernel- to the gray- image . The cv2.filter2D- function is a- much more optimized version of our convolve- function . The main reason I included the implementation of convolve- in this blog post is to give you a better understanding of how convolutions work under the hood . <p> Finally , - Lines 108-112 display @ @ @ @ @ @ @ @ @ @ with OpenCV and Python <p> Todays example image comes from a photo I took a few weeks ago at my favorite bar in South Norwalk , CT Cask Republic . In this image you 'll see a glass of my favorite beer ( Smuttynose Findest Kind IPA ) along with three 3D-printed Pokemon from the ( unfortunately , now closed ) Industrial Chimp shop : <p> Figure 6 : The example image we are going to apply our convolutions to . <p> To run our script , just issue the following command : <p> Convolutions with OpenCV and Python <p> Shell <p> 1 <p> $python **35;1030;TOOLONG <p> You 'll then see the results of applying our smallBlur- kernel to the input image : <p> Figure 7 : Applying a small blur convolution with our " convolve " function and then validating it against the results of OpenCVs " cv2.filter2D " function . <p> On the left , we have our original image. - Then- in the- center- we have the results from the convolve- function . And on the- right , the results from cv2.filter2D- . As the results demonstrate , our @ @ @ @ @ @ @ @ @ @ working properly . Furthermore , our original image now appears " blurred " and " smoothed " , thanks to the smoothing kernel . <p> Figure 12 : Finding horizontal gradients in an image using the Sobel-y operator and convolutions . <h> The Role of Convolutions in Deep Learning <p> As you 've gathered through this blog post , we must- manually hand-define each of our kernels for- applying various operations such as smoothing , sharpening , and edge detection . <p> That 's all fine and good , - but what if there was a way to- learn these filters instead ? - Is it possible to define a machine learning algorithm that can look at images and eventually learn these types of operators ? <p> In fact , there is these types of algorithms are a sub-type of- Neural Networks called- Convolutional Neural Networks ( CNNs ) . By applying convolutional filters , nonlinear activation functions , pooling , and backpropagation , CNNs are able to learn filters that can detect edges and blob-like structures in lower-level layers of the network and then use the edges and structures as building @ @ @ @ @ @ @ @ @ @ , cats , dogs , cups , etc. ) in the deeper layers of the network . <p> Exactly- how do CNNs do this ? <p> Ill show you but it will have to wait for another few blog posts until we cover enough basics . <h> Summary <p> In todays blog post , we discussed- image kernels and- convolutions . If we think of an image as a- big matrix , then an image kernel is just a- tiny matrix that sits on top of the image . <p> This kernel then slides from left-to-right and top-to-bottom , computing the sum of element-wise multiplications between the input image and the kernel along the way we call this value the- kernel output . The kernel output is then stored in an output image at the same- ( x , y ) -coordinates as the input image ( after accounting for any padding to ensure the output image has the same dimensions as the input ) . <p> Given our newfound knowledge of convolutions , we defined an OpenCV and Python function to apply a series of kernels to an image @ @ @ @ @ @ @ @ @ @ sharpen it , and detect edges . <p> Finally , we briefly discussed the roles kernels/convolutions play in deep learning , specifically- Convolutional Neural Networks , and how these filters can be learned- automatically instead of needing to- manually define them first . <p> In next weeks blog post , - I 'll be showing you how to train your- first Convolutional Neural Network from scratch using Python - be sure to signup for the PyImageSearch Newsletter using the form below to be notified when the blog post goes live ! <h> Downloads : 55217 @qwx675217 <p> This procedure describe the correlation between matrix and not the convolution . In a convolution we have a minus sign in the middle of the equation , thus we need to turn and swipe the second matrix . Make sense for you ? <p> Juan is right . When you 're doing convolution , you 're supposed to flip the kernel both horizontally and vertically in the case od 2D images . Hence the minus sign . It obvisouly does n't  matter for symmetric kernels like averaging etc. , but in general it can lead to @ @ @ @ @ @ @ @ @ @ using convolution theorem and FFT . <p> On the other hand , as far as Im aware , Caffe framework also only performs correlation in their convolutional layers , while several other libraries do it by the book . So , be aware of these things when trying to convert pre-trained models for instance <p> I wanted to know if there is some method to intuitively de-blur blurred images . Like of course we need to de-convolve with the blur causing kernel but in most practical scenarios we do n't  know that kernel and resort to brute-force blind de-convolution . <p> It really depends on the level of which you are trying to deblur the image . Applying deblurring using a simple kernel is unlikely to give you ideal results . The current state-of-the-art involves applying machine learning to deblur images . Here is a link to a recent NIPS paper so you can learn more about the topic . <p> Thanks for the detailed and clear explanation . I have to define a kernel for a specific template ( a part of the image ) and match it with @ @ @ @ @ @ @ @ @ @ , The shapes of the kernel and images are not the same . It pops the error message saying " Operands could not be broadcast together " . Is there a different kind of padding that i should follow ? <p> The shapes of the kernel and image should n't be the same since the kernel essentially slides across the input image . It sounds like you 're not extracting the ROI of the input image correctly before applying the kernel . If the input region is smaller than the kernel size , simply pad the input ROI . <h> Trackbacks/Pingbacks <p> layers later in this series of posts ( although you should already know the basics of how convolution operations work ) ; but in the meantime , simply follow along , enjoy the lesson , and- @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485095 @185095/ <h> k-NN classifier for image classification <p> Now that we 've had a taste of Deep Learning and Convolutional Neural Networks in last weeks blog post on LeNet , were going to take a step back and start to study machine learning in the context of image classification in more depth . <p> To start , well reviewing the k-Nearest Neighbor ( k-NN ) classifier , arguably the- most simple , easy to understand machine learning algorithm. - In fact , k-NN is- so simple that it does n't  perform any " learning " at all ! <p> The goal of the Dogs vs . Cats dataset , as the name suggests , is to classify whether a given image contains a- dog or a- cat . Well be using this dataset- a lot in future blog posts ( for reasons I 'll explain later in this tutorial ) , so make sure you take the time now to read through this post and familiarize yourself with the dataset . <p> All that said , - let 's get started implementing k-NN for image classification to recognize dogs vs. cats in images @ @ @ @ @ @ @ @ @ @ wrote a ( controversial ) blog post on getting off the deep learning bandwagon and getting some perspective . Despite the antagonizing title , the overall theme of this post centered around various trends in machine learning history , such as Neural Networks ( and how research in NNs almost died in the 70-80s ) , Support Vector Machines , and Ensemble methods . <p> When each of these methods were introduced , researchers and practitioners were equipped with new , powerful techniques in essence , - they were given a hammer and every problem looked like a nail , when in reality , all they- needed was a few simple turns of a phillips head to solve a particular the problem . <p> I have news for you : Deep learning is no different . <p> Go to the vast majority of popular machine learning and computer vision conferences and look at the recent list of publications . What is the overarching theme ? <p> Deep learning . <p> Then , hop on large LinkedIn groups related to computer vision and machine learning . What are many people asking @ @ @ @ @ @ @ @ @ @ datasets . <p> After that , go over to popular computer science sub-reddits such as /r/machinelearning . What tutorials are the most consistently upvoted ? <p> You guessed it : deep learning . <p> here 's the bottom line : <p> Yes , I will teach you about Deep Learning and Convolutional Neural Networks on this blog - but you 're damn-well going to understand that Deep Learning is just a TOOL , and like any other tool , there is a right and wrong time to use it . <p> Because of this , its important for us to understand the basics of machine learning before we progress too far . Over the next few weeks , I 'll be discussing the basics of machine learning and Neural Networks , eventually building up to Deep Learning ( where you 'll be able to appreciate the inner-workings of these algorithms more ) . <h> Kaggle Dogs vs . Cats dataset <p> The Dogs vs . Cats dataset was actually part of a Kaggle challenge a few years back . The challenge itself was simple : - given an image , predict whether it contained @ @ @ @ @ @ @ @ @ @ Examples from the Kaggle Dogs vs . Cats dataset . <p> Simple enough but if you know anything about image classification , you 'll understand that given : <p> Viewpoint variation <p> Scale variation <p> Deformation <p> Occlusion <p> Background clutter <p> Intra-class variation <p> That the problem is- significantly harder than it might appear on the surface . <p> By simply randomly guessing , - you should be able to reach 50% accuracy- ( since there are only two class labels ) . A machine learning algorithm will need to obtain &gt; 50% accuracy in order to demonstrate that it has in fact " learned " something ( or found an underlying pattern in the data ) . <p> The dataset is simple enough to wrap your head around there are only two classes : - " dog " or- " cat " . <p> However , the dataset is nicely sized , containing- 25,000 images in the training data . This means that you have enough data to train data-hungry Convolutional Neural Networks from scratch . <p> Well be using this dataset a lot in future blog posts , @ @ @ @ @ @ @ @ @ @ . Cats webpage , register for a free account , and download the data . <h> How does the k-NN classifier work ? <p> The k-Nearest Neighbor classifier is- by far the most simple machine learning/image classification algorithm . In fact , its- so simple that it does n't  actually " learn " anything . <p> Inside , this algorithm simply relies on the distance between feature vectors , much like building an image search engine only this time , we have the- labels associated with each image so we can predict and return an actual- category for the image . <p> Simply put , the k-NN algorithm classifies unknown data points by finding the- most common class among the- k-closest examples. - Each data point in the- k closest examples casts a vote and the category with the most votes wins ! <p> Or , in plain english : - " Tell me who your neighbors are , and I 'll tell you who you are " <p> To visualize this , take a look at the following toy example where I have plotted the " fluffiness " of animals- @ @ @ @ @ @ @ @ @ @ the- y-axis : <p> Figure 2 : Plotting the fluffiness of animals- along the x-axis and the lightness- of their coat on the y-axis . <p> Here we can see there are two categories of images and that each of the- data points within each respective category are grouped relatively close together in an- n-dimensional space . Our dogs tend to have dark coats which are not very fluffy while our cats have very light coats that are extremely fluffy . <p> This implies that the distance between two data points in the- red circle is- much smaller than the distance between a data point in the- red circle and a data point in the- blue circle . <p> Other distance metrics/similarity functions can be used depending on your type of data ( the chi-squared distance is often used for distributions i.e. , histograms ) . In todays blog post , for the sake of simplicity , well be using the Euclidean distance to compare images for similarity . <h> Implementing k-NN for image classification with Python <p> Now that we 've discussed what the k-NN algorithm is , along with @ @ @ @ @ @ @ @ @ @ write some code to actually perform image classification using k-NN . <p> Open up a new file , name it knnclassifier.py- , and let 's get coding : <p> Secondly , well be using the imutils library , a package that I have created to store common computer vision processing functions . If you do not have imutils- installed , you 'll want to do that now : <p> k-NN classifier for image classification <p> Shell <p> 1 55204 @qwx675204 <p> Next , we are going to define two methods to take an input image and convert it to a feature vector , or a list of numbers that quantify the contents of an image . The first method can be seen below : <p> k-NN classifier for image classification <p> Python <p> 11 <p> 12 <p> 13 <p> 14 <p> LONG ... <p> # resize the image to a fixed size , then flatten the image into <p> # a list of raw pixel intensities <p> returncv2.resize ( image , size ) . flatten() <p> The imagetofeaturevector- method is an extremely naive function that simply takes an input image- and resizes @ @ @ @ @ @ @ @ @ @ , and then flattens the RGB pixel intensities into a single list of numbers . <p> This means that our input image- will be shrunk to- 32 x 32 pixels , and given three channels for each Red , Green , and Blue component respectively , our output " feature vector " will be a list of- 32 x 32 x 3 = 3,072 numbers . <p> Strictly speaking , the output of imagetofeaturevector- is not a true " feature vector " since we tend to think of " features " and " descriptors " as- abstract quantifications of the image contents . <p> Note : As well find out in later tutorials , Convolutional Neural Networks obtain fantastic results using raw pixel intensities as inputs but this is because they- learn a robust set of discriminating filters during the training process . <p> We then define our second method , this one called extractcolorhistogram- : <p> k-NN classifier @ @ @ @ <p> LONG ... <p> # extract a 3D color histogram from the HSV color space using <p> # the supplied number of bins per channel <p> hsv=cv2.cvtColor ( image , cv2.COLORBGR2HSV ) <p> hist=cv2.calcHist ( hsv , 0,1,2 , None , bins , <p> 0,180,0,256,0,256 ) <p> # handle normalizing the histogram if we are using OpenCV 2.4 . X <p> ifimutils.iscv2() : <p> hist=cv2.normalize(hist) <p> # otherwise , perform " in place " normalization in OpenCV 3 ( I <p> # personally hate the way this is done <p> else : <p> cv2.normalize ( hist , hist ) <p> # return the flattened histogram as the feature vector <p> returnhist.flatten() <p> As the name suggests , this function accepts an input image- and constructs a color histogram to characterize the color distribution of the image . <p> --neighbors- : Here we can supply the number of nearest neighbors that are taken into account when classifying a given data point . Well default this value to- one , meaning that an image will be classified by finding its closest neighbor in an- ndimensional- space and then taking the label of @ @ @ @ @ @ @ @ @ @ demonstrate how to automatically tune- k for optimal accuracy . <p> --jobs- : Finding the nearest neighbor for a given image requires us to compute the distance from our- input image to- every other image in our dataset . This is clearly a- O(N) operation that scales linearly . For larger datasets , this can become prohibitively slow . In order- to speedup the process , we can distribute the computation of nearest neighbors across multiple processors/cores of our machine . Setting --jobs- to -1- ensures that- all processors/cores are used to help speedup the classification process . <p> Note : We can also speedup the k-NN classifier by utilizing specialized data structures such as kd-trees or Approximate Nearest Neighbor algorithms such as FLANN or Annoy . In practice , these algorithms can reduce nearest neighbor search to approximately- O ( log N ) ; however , for the sake of simplicity in this post , well perform an- exhaustive- nearest neighbor search . <p> Finally , we display an update to our terminal to inform us on feature extraction progress every 1,000 images ( Lines 75 and 76 ) @ @ @ @ @ @ @ @ @ @ rawImages- and features- matrices take up the following code block- will tell us when executed : <p> k-NN classifier for image classification <p> Python <p> 78 <p> 79 <p> 80 <p> 81 <p> 82 <p> 83 <p> 84 <p> 85 <p> 86 <p> # show some information on the memory consumed by the raw images <p> # matrix and features matrix <p> **29;1067;TOOLONG <p> **27;1098;TOOLONG <p> labels=np.array(labels) <p> print ( " INFO pixels matrix : : .2fMB " . format ( <p> rawImages.nbytes/ ( 1024*1000.0 ) ) ) <p> print ( " INFO features matrix : : .2fMB " . format ( <p> features.nbytes/ ( 1024*1000.0 ) ) ) <p> We start by converting our lists to NumPy arrays . We then use the . nbytes- attribute of the NumPy array to display the number of megabytes of memory the representations utilize ( about 75MB for the raw pixel intensities and 50MB for the color histograms . This implies that we can- easily store our features in main memory ) . <p> Next , we need to take our data partition- it into two splits one for- training and another @ @ @ @ @ @ @ @ @ @ Python <p> 88 <p> 89 <p> 90 <p> 91 <p> 92 <p> 93 <p> # partition the data into training and testing splits , using 75% <p> # of the data for training and the remaining 25% for testing <p> LONG ... <p> LONG ... <p> LONG ... <p> LONG ... <p> Here well be using 75% of our data for training and the remaining 25% for testing the k-NN algorithm . <p> Let 's apply the k-NN classifier to the raw pixel intensities : <p> k-NN classifier for image classification <p> Python <p> 95 <p> 96 <p> 97 <p> 98 <p> 99 <p> 100 <p> 101 <p> # train and evaluate a k-NN classifer on the raw pixel intensities <p> print ( " INFO evaluating raw pixel accuracy ... " ) <p> LONG ... <p> njobs=args " jobs " ) <p> model.fit ( trainRI , trainRL ) <p> acc=model.score ( testRI , testRL ) <p> print ( " INFO raw pixel accuracy : : .2f% " . format(acc*100) ) <p> Here we instantiate the KNeighborsClassifier- object from the scikit-learn library- using the supplied number of --neighbors- and --jobs- . <p> @ @ @ @ @ @ @ @ @ @ call to . fit- on- Line 99 , followed by evaluation on the testing data on- Line 100 . <p> In a similar fashion , we can also train and evaluate a k-NN classifier on our histogram representations : <p> k-NN classifier for image classification <p> Python <p> 103 <p> 104 <p> 105 <p> 106 <p> 107 <p> 108 <p> 109 <p> 110 <p> # train and evaluate a k-NN classifer on the histogram <p> # representations <p> print ( " INFO evaluating histogram accuracy ... " ) <p> LONG ... <p> njobs=args " jobs " ) <p> model.fit ( trainFeat , trainLabels ) <p> acc=model.score ( testFeat , testLabels ) <p> print ( " INFO histogram accuracy : : .2f% " . format(acc*100) ) <h> k-NN image classification results <p> To test our k-NN image classifier , make sure you have : <p> Downloaded the source code to this blog post using the- " Downloads " form found at the bottom of this tutorial . <p> At first , you 'll see that our images are being described and quantified via the imagetofeaturevector- and extractcolorhistogram- functions : <p> Figure 5 @ @ @ @ @ @ @ @ @ @ Cats dataset . <p> This process shouldnt take longer than 1-3 minutes depending on the speed of your machine . <p> After the feature extraction process is complete , we can see some information on the size ( in MB ) of our feature representations : <p> Figure 6 : Measuring the size of our feature matrices . <p> The raw pixel features take up 75MB while the color histograms require only 50MB of RAM . <p> Finally , the k-NN algorithm is trained and evaluated for both the raw pixel intensities and color histograms : <p> Figure 7 : Evaluating our k-NN algorithm for image classification . <p> As the figure above demonstrates , by utilizing- raw pixel intensities- we were able to reach- 54.42% accuracy. - On the other hand , applying k-NN to color histograms achieved a slightly better- 57.58% accuracy . <p> In both cases , we were able to obtain &gt; 50% accuracy , demonstrating there is an underlying pattern to the images for both raw pixel intensities and color histograms . <p> However , that 57% accuracy leaves much to be desired . <p> @ @ @ @ @ @ @ @ @ @ the best way to distinguish between a dog and a cat : <p> There are brown dogs . And there are brown cats . <p> There are black dogs . And there are black cats . <p> And certainly a dog and cat could appear in the same environment ( such as a house , park , beach , etc. ) where the- background color distributions are similar . <p> Because of this , utilizing strictly color is not a great choice for characterizing the difference between dogs and cats but that 's okay . The purpose of this blog post was simply to introduce the concept of image classification using the k-NN algorithm . <h> Can we do better ? <p> You might be wondering , can we do better than the 57% classification accuracy ? <p> You 'll notice that Ive used only- k=1 in this example , implying that only- one nearest neighbor is considered when classifying each image . How would the results change if I used- k=3 or- k=5 ? <p> And how about the choice in distance metric would the Manhattan/City block distance be a better @ @ @ @ @ @ @ @ @ @ k and- distance- metric at the same time ? <p> Would classification accuracy improve ? Get worse ? Stay the same ? <p> The fact is that- nearly all machine learning algorithms require a bit of tuning to obtain optimal results . In order to determine the optimal set of values for these model- variables , we apply a process called- hyperparameter tuning , which is exactly the topic of next weeks blog post . <h> Summary <p> In this blog post , we reviewed the basics of image classification using the k-NN algorithm . We then applied the k-NN classifier to the Kaggle Dogs vs . Cats dataset to identify whether a given image contained a dog or a cat . <p> Utilizing only the raw pixel intensities of the input image images , we obtained 54.42% accuracy . And by using color histograms , we achieved a slightly better 57.58% accuracy . Since both of these results are &gt; 50% ( we should expect to get 50% accuracy simply by random guessing ) , we can ascertain that there is an underlying pattern in the raw pixels/color histograms @ @ @ @ @ @ @ @ @ @ although 57% accuracy is quite poor ) . <p> The answer is- hyperparameter tuning which is exactly what well be covering in next weeks blog post . <p> Be sure to sign up for the PyImageSearch Newsletter using the form below to be notified when the next blog post goes live ! <h> Downloads : 55217 @qwx675217 <p> Adrian I must say , you are helping the community in a great way ! ! Kudos for your effort and time . I just needed an advice on how to install SciPy on Windows Python . I have Python 2.7 on my Machine . I downloaded scipy-0.18.0.zip for windows from one of the websites but do n't  know where to go thereafter .. Any suggestion would be appreciated . <p> Hey Manik in general , I do n't  recommend using Windows for computer vision development . I personally have n't used Windows in many years , so I 'm not sure regarding your exact situation . Ill leave this comment here for another reader to answer . <p> Download from this site for your python LONG ... Then from your windows command line @ @ @ @ @ @ @ @ @ @ install full name of the module(scipy) It needs numpy+mkl follow the same procedure to install , I think it will help ! <p> Who can elaborate on what is really the right hand side of line 51 , the raw images array initializer ? I see a square there . What is the intended python code ? Thanks ! <p> 2nd thing : Line 11 and other parts of this program are EXACTLY the kind of code I was looking for , for other reasons . Particularly , this code shows me how to load a machine learning dataset from genuine image files ! I am so happy now . Thanks for writing this code to get me bootstrapped with machine learning on images , real ones like jpegs ! Not prepackaged images contained in some weird monolithic singleton file ! <p> Now if I could just find a rosetta stone ( or nice person ) to explain the full python meaning of the square at line 51 ! <p> If you 're starting from scratch , the absolute best place to learn computer vision , machine learning , and deep @ @ @ @ @ @ @ @ @ @ covers advanced algorithms , but also takes the time to explain the basics . <p> Hi Adrian , the dataset comes pre-split from kaggle . In train and test1 zips . Both contain 25k and 12.5k images respectively . Does the code in this blog post take the pre-split into account or should I combine train and test from kaggle into one folder and let your code do the split ? <p> The code from this blog post only uses the training data from Kaggle and then partitions into two respective training and testing splits . If you wanted to train a model and submit the results from Kaggle you would use the pre-split zip files from the official Kaggle challenge . <p> Its been awhile since I looked at the " test1 " directory , but I would n't recommend combining them . They likely use different image path structures , and if so , that will break the code that extracts the label from the image path . <p> The --neighbors is the number of nearest neighbors in the k-NN algorithm . Please read this post again to understand @ @ @ @ @ @ @ @ @ @ leave this as -1 which uses all available processors on your system . <p> Its entirely dependent on your dataset , what features you 're extracting , etc . You normally would apply cross-validation to both models and determine which one is better . There is no " one size fits all " solution to computer vision and machine learning . <p> Hello Adrian , I have used the knn classifier for car logo recognition in one of your tutorials , but when i am using sliding window technique for recognising the make of a given car from its image , the output depends on size of sliding window and a wrong result is obtained if i change the size of sliding window . Can you help me generalise @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485096 @185096/ <p> At the time , my research goal was to find and identify methods to reliably quantify pills in a- rotation invariant manner . Regardless of how the pill was rotated , I wanted the output feature vector to be ( approximately ) the same ( the feature vectors will never be to- completely identical in a real-world application due to lighting conditions , camera sensors , floating point errors , etc . ) . <p> After the first week I was making fantastic progress . <p> I was able to extract features from my dataset of pills , index them , and then- identify- my test set of- pills regardless of how they were- oriented <p> In essence , I was only quantifying- part of the rotated , oblong pills ; hence my strange results . <p> I spent- three weeks and- part of my Christmas vacation banging my head against the wall trying to diagnose the bug only to feel quite embarrassed when I realized it was due to me being negligent with the cv2.rotate- function . <p> You see , the size of the output image @ @ @ @ @ @ @ @ @ @ my image would be cut off . <p> How did I accomplish this and squash the bug for good ? <p> To learn how to rotate images with OpenCV such that the- entire image is included and- none of the image is cut off , - just keep reading . <p> This enables us to compute the new width and height of the rotated image , ensuring no part of the image is cut off . <p> Once we know the new width and height , we can adjust for translation on- Lines 59 and 60 by modifying our rotation matrix once again . <p> Finally , cv2.warpAffine- is called on- Line 63 to rotate the actual image using OpenCV while ensuring- none of the image is cut off . <p> For some other interesting solutions ( some better than others ) to the rotation cut off problem when using OpenCV , be sure to refer to this StackOverflow thread and this one too . <h> Fixing the rotated image " cut off " problem with OpenCV and Python <p> Let 's get back to my original problem of rotating oblong @ @ @ @ @ @ @ @ @ @ issue ( although back then I had not created the imutils- Python package it was simply a utility function in a helper file ) . <p> Well be using the following pill as our example image : <p> Figure 6 : The example oblong pill we will be rotating with OpenCV . <p> To start , open up a new file and name it rotatepills.py- . Then , insert the following code : <p> We then parse our command line arguments on- Lines 8-11 . Just like in the example at the beginning of the blog post , we only need one switch : --image- , the path to our input image . <p> Next , we load our pill image from disk and preprocess it by converting it to grayscale , blurring it , and detecting edges : <p> Rotate images ( correctly ) with OpenCV and Python <p> Python <p> 13 <p> 14 <p> 15 <p> 16 <p> 17 <p> 18 <p> # load the image from disk , convert it to grayscale , blur it , <p> # and apply edge detection to reveal the outline of @ @ @ @ @ @ @ @ @ @ 55215 @qwx675215 <p> **26;1127;TOOLONG , ( 3,3 ) , 0 ) <p> **28;1155;TOOLONG <p> After executing these preprocessing functions our pill image now looks like this : <p> Figure 7 : Detecting edges in the pill . <p> The outline of the pill is clearly visible , so let 's apply contour detection to find the outline of the pill : <p> Rotate images ( correctly ) with OpenCV and Python <p> Python <p> 20 <p> 21 <p> 22 <p> 23 <p> # find contours in the edge map <p> LONG ... 55211 @qwx675211 <p> **36;1185;TOOLONG <p> We are now ready to extract the pill ROI from the image : <p> Rotate images ( correctly ) with <p> 37 <p> 38 <p> # ensure at least one contour was found <p> iflen(cnts)&gt;0 : <p> # grab the largest contour , then draw a mask for the pill <p> c=max ( cnts , key=cv2.contourArea ) <p> mask=np.zeros ( gray.shape , dtype= " uint8 @ @ @ @ @ @ @ @ @ @ , -1 ) <p> # compute its bounding box of pill , then extract the ROI , <p> # and apply the mask <p> ( x , y , w , h ) =cv2.boundingRect(c) <p> imageROI=imagey:y+h , x:x+w <p> maskROI=masky:y+h , x:x+w <p> **32;1223;TOOLONG , imageROI , <p> mask=maskROI ) <p> First , we ensure that at least- one contour was found in the edge map ( Line 26 ) . <p> Provided we have at least one contour , we construct a mask- for the largest contour region on- Lines 29 and 30 . <p> Our mask- looks like this : <p> Figure 8 : The mask representing the entire pill region in the image . <p> Given the contour region , we can compute the- ( x , y ) -coordinates of the bounding box of the region ( Line 34 ) . <p> Using both the bounding box and mask- , we can extract the actual pill region ROI ( Lines 35-38 ) . <p> Now , let 's go ahead and apply both the imutils.rotate- and imutils.rotatebound- functions to the imageROI- , just like we did in @ @ @ @ @ @ @ @ @ @ ) with <p> 50 <p> 51 <p> # loop over the rotation angles <p> forangle innp.arange(0,360,15) : <p> **31;1257;TOOLONG , angle ) <p> cv2.imshow ( " Rotated ( Problematic ) " , rotated ) 55212 @qwx675212 <p> # loop over the rotation angles again , this time ensure the <p> # entire pill is still within the ROI after rotation <p> forangle innp.arange(0,360,15) : <p> **36;1290;TOOLONG , angle ) <p> cv2.imshow ( " Rotated ( Correct ) " , rotated ) 55212 @qwx675212 <p> After downloading the source code to this tutorial using the- " Downloads " section below , you can execute the following command to see the output : <p> Rotate images ( correctly ) with OpenCV and Python <p> Shell <p> 1 <p> $python rotatepills.py--image images/pill01.png <p> The output of imutils.rotate- will look like : <p> Figure 9 : Incorrectly rotating an image with OpenCV causes parts of the image to be cut off . <p> Notice how the pill is cut @ @ @ @ @ @ @ @ @ @ the new dimensions of the rotated image to ensure the borders are not cut off . <p> By using imutils.rotatebound- , we can ensure that no part of the image is cut off when using OpenCV : <p> Figure 10 : By modifying OpenCVs rotation matrix we can resolve the issue and ensure the entire image is visible . <p> Using this function I was- finally able to finish my research for the winter break but not before I felt quite embarrassed about my rookie mistake . <h> Summary <p> In todays blog- post I discussed how image borders can be cut off when rotating images with OpenCV and cv2.warpAffine- . <p> The fact that image borders can be cut off is- not a bug in OpenCV in fact , its how cv2.getRotationMatrix2D- and cv2.warpAffine- are designed . <p> While it may seem frustrating and cumbersome to compute new image dimensions to ensure you do n't  lose your borders , its actually a blessing in disguise . <p> OpenCV gives us- so much control that we can modify our rotation matrix to make it do- exactly what we want . @ @ @ @ @ @ @ @ @ @ our rotation matrix- M is formed and what each of its components represents ( discussed earlier in this tutorial ) . Provided we understand this , the- math falls out naturally . <p> To learn more about image processing and computer vision , be sure to take a look at the PyImageSearch Gurus course- where I discuss these topics in more detail . <p> Otherwise , I encourage you to enter your email address in the form below to be notified when future blog posts are published . <h> Downloads : 55217 @qwx675217 <h> 18 Responses to Rotate images ( correctly ) with OpenCV and Python <p> amazing article as always . do you know any ml/ deep learning NN architectures that are rotation invariant inherently , without image preprocessing and thus creating multiple train exemplars leading to the same classification ( as I suppose you do ) ? I have googled the topic without success . thanks for everything <p> This is still an active area of research . Focus your search on " steerable filters + convolutional neural networks " and you 'll come across some of the more @ @ @ @ @ @ @ @ @ @ can vary based on what type of dataset you 're working with . For example , rotation invariance for natural scene images ( broad classification and therefore easier ) is much easier than obtain than say rotation invariance for fine-grained classification ( such as pill identification ) . <p> Thanks for posting this . I have a question for you . What if you were interested in the opposite scenario ? That is , if you were doing object tracking and you wanted to calculate the rotation angle as the object is rotating . For example , you may take a reference image of an object , and then track the object realtime using the webcam while the object is rotating back and forth . I am interested in the angle of the object in the current frame relative to the angle of the object in the reference frame . For simplicity , let 's for now assume that the object only rotates along the axis of the camera , and does not change size . Could you point me to the right direction for this ? <p> There are multiple ways @ @ @ @ @ @ @ @ @ @ actual shape of the object you are working with . The best solution would be to determine the center of the object and a second identifiable point of some sort on the object that can be detected across many frames . Exactly which method you use is really dependent on the project . Once you have these points you can measure how much the object has rotated between frames . <p> For irregular objects you could simply compute the mask + bounding box and then compute the minimum-enclosing rectangle which will also give you the angle of rotation . <p> Thanks for the reply . Let 's say that we are trying to create a more general algorithm under the following scenario : we would like to detect the rotation of different objects , but in all cases the object is circular and has a detectable pattern to it that 's not symmetric ( therefore it would be possible to tell the angle ) . However , let 's say the pattern itself is not always the same . Consider for instance company logos that are circular . Usually the pattern here is @ @ @ @ @ @ @ @ @ @ same from one logo to another , except for that the features are located in an annulus around the center . I was thinking about taking a reference on the annulus and then tracking the rotational angle . However , I 'm not sure if there is a better approach , and how to make this approach computationally efficient . If SIFT or SURF algorithms are used , I fear they would not be efficient so I was hoping there would be a better method . <p> The standard approach here would be to use SIFT/SURF , keypoint matching , and RANSAC . You mentioned wanting to create a " general algorithm " , but in reality I do n't  think this is advisable . Most successful computer vision applications focus on a specific problem and attempt to solve it . Take logo recognition for example we 've become better at logo recognition but its not solved . I would suggest you start with SIFT/SURF and see how far it gets you in your particular problem , but try to stay away from solving " general " problems . <p> Hi Adrian @ @ @ @ @ @ @ @ @ @ *3 Rotation matrix , How can I apply this rotation matrix for an original image and find the transformed image . The 3 *3 matrix is obtained using angles measured by x , y , @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485097 @185097/ <p> A loss function , in the context of Machine Learning and Deep Learning , allows us to quantify how " good " or " bad " a given classification function ( also called a " scoring function " ) is at correctly classifying data points in our dataset . <p> However , while hinge loss and squared hinge loss are commonly used when training Machine Learning/Deep Learning classifiers , there is- another method more heavily used <p> In fact , if you have done previous work in Deep Learning , you have likely heard of this function before do the terms- Softmax classifier and- cross-entropy loss sound familiar ? <p> I 'll go as far to say that if you do- any work in Deep Learning ( especially Convolutional Neural Networks ) that you 'll run into the term " Softmax " : its the- final layer at the end of the network that- yields your actual probability scores for each class label . <p> To learn more about Softmax classifiers and the cross-entropy loss function , keep reading . <p> Its much easier for us as humans to interpret- @ @ @ @ @ @ @ @ @ @ loss and squared hinge loss ) . <p> Furthermore , for datasets such as ImageNet , we often look at the rank-5 accuracy of Convolutional Neural Networks ( where we check to see if the ground-truth label is in the top-5 predicted labels returned by a network for a given input image ) . <p> Seeing ( 1 ) if the true class label exists in the top-5 predictions and ( 2 ) the- probability associated with the predicted label is a nice property . <h> Understanding Multinomial Logistic Regression and Softmax Classifiers <p> The Softmax classifier is a generalization of the binary form of Logistic Regression . Just like in hinge loss or squared hinge loss , our mapping function- f- is defined such that it takes an input set of data- x and maps them to the output class labels via a simple ( linear ) dot product of the data- x and weight matrix- W : <p> However , unlike hinge loss , we interpret these scores as- unnormalized log probabilities for each class label this amounts to swapping out our hinge loss function with cross-entropy loss @ @ @ @ @ @ @ @ @ @ Let 's break the function apart and take a look . <p> To start , our loss function should minimize the negative log likelihood of the correct class : <p> This- probability- statement can be interpreted as : <p> Where we use our standard scoring function form : <p> As a whole , this yields our final loss function for a- single data point , just like above : <p> Note : Your logarithm here is actually base e ( natural- logarithm ) since we are taking the inverse of the exponentiation over e earlier . <p> Just as in hinge loss or squared hinge loss , computing the cross-entropy loss over an- entire dataset is done by taking the average : <p> If these equations seem scary , do n't  worry I 'll be working an actual numerical example in the next section . <p> Note : I 'm purposely leaving out the regularization term as to not bloat this tutorial or confuse readers . Well return to regularization- and explain what it is , how to use , and why its important for machine learning/deep learning in a future blog post @ @ @ @ @ @ @ @ @ @ loss in action , consider the following figure : <p> Figure 1 : To compute our cross-entropy loss , let 's start with the output of our scoring function ( the first column ) . <p> Clearly we can see that this image is an " airplane " . - But does our Softmax classifier ? <p> To find out , I 've included the output of our scoring function- f for each of the four classes , respectively , in- Figure 1- above . These values are our- unnormalized log probabilities for the four classes . <p> Note : I used a random number generator to obtain these values for this particular example . These values are simply used to demonstrate how the calculations of the Softmax classifier/cross-entropy loss function are performed . In reality , these values would not be randomly generated they would instead be the output of your scoring function- f . <p> In our particular example , the Softmax classifier will actually reduce to a special case when there are- K=2 classes , the Softmax classifier reduces to simple Logistic Regression . If we have- &gt; 2 @ @ @ @ @ @ @ @ @ @ Regression , or more simply , a Softmax classifier . <p> With that said , open up a new file , name it softmax.py- , and insert the following code 9 <p> 10 <p> 11 55203 @qwx675203 <p> **25;1328;TOOLONG importLabelEncoder <p> fromsklearn.linearmodel importSGDClassifier <p> fromsklearn.metrics **26;1355;TOOLONG <p> **27;1383;TOOLONG importtraintestsplit <p> fromimutils importpaths 55220 @qwx675220 55218 @qwx675218 55219 @qwx675219 <p> importcv2 <p> importos <p> If you 've been following along on the PyImageSearch blog over the past few weeks , then the code above likely looks fairly familiar all we are doing here is importing our required Python packages . <p> Well be using the scikit-learn library , so if you do n't  already have it installed , be sure to install it now : <p> Softmax Classifiers Explained <p> Shell <p> 1 <p> $pip install scikit-learn <p> Well also be using my imutils package , a series of convenience functions used to make performing common image processing operations an easier task . If you do not have @ @ @ @ @ @ @ @ @ @ : <p> Softmax Classifiers Explained <p> Shell <p> 1 55204 @qwx675204 <p> Next , we define our extractcolorhistogram- function which is used to quantify the color distribution of our input image- using- the supplied number of bins- : <p> <p> 29 <p> 30 <p> LONG ... <p> # extract a 3D color histogram from the HSV color space using <p> # the supplied number of bins per channel <p> hsv=cv2.cvtColor ( image , cv2.COLORBGR2HSV ) <p> hist=cv2.calcHist ( hsv , 0,1,2 , None , bins , <p> 0,180,0,256,0,256 ) <p> # handle normalizing the histogram if we are using OpenCV 2.4 . X <p> ifimutils.iscv2() : <p> hist=cv2.normalize(hist) <p> # otherwise , perform " in place " normalization in OpenCV 3 ( I <p> # personally hate the way this is done <p> else : <p> cv2.normalize ( hist , hist ) <p> # return the flattened histogram as the feature vector <p> @ @ @ @ @ @ @ @ @ @ before , so I 'm going to skip the detailed review . For a more thorough discussion of extractcolorhistogram- , why we are using it , and how it works , please see this blog post . <p> In the meantime , simply keep in mind that this function- quantifies the contents of an image by constructing a histogram over the pixel intensities . <p> To examine some actual- probabilities , let 's loop over a few randomly sampled training examples and examine the output probabilities returned by the classifier : <p> <p> 97 <p> 98 <p> # to demonstrate that our classifier actually " learned " from <p> # our training data , randomly sample a few training images <p> LONG ... <p> # loop over the training indexes <p> foriinidxs : <p> # predict class probabilities based on the extracted color <p> # histogram <p> **25;1412;TOOLONG , -1 ) <p> LONG ... <p> # show the predicted probabilities along @ @ @ @ @ @ @ @ @ @ " cat= : .1f% , dog= : .1f% , actual= " . format ( catProb*100 , <p> LONG ... <p> Note : I 'm randomly sampling from the- training data rather than the- testing data to demonstrate that there should be a noticeably large gap in between the probabilities for each class label . Whether or not each classification is- correct is a a different story but even if our prediction is wrong , we should still see some sort of gap that indicates that our classifier is actually learning from the data . <p> To investigate the individual class probabilities for a given data point , take a look at the rest of the softmax.py- output : <p> Figure 6 : Investigating the class label probabilities for each prediction . <p> For each of the randomly sampled data points , we are given the class label probability for- both " dog " and " cat " , along with the- actual ground-truth label . <p> Based on this sample , we can see that we obtained- 4 / 5 = 80%- accuracy . <p> But more importantly , notice how @ @ @ @ @ @ @ @ @ @ probabilities . If our Softmax classifier predicts " dog " , then the probability associated with " dog " will be high . And conversely , the class label probability associated with " cat " will be low . <p> Similarly , if our Softmax classifier predicts " cat " , then the probability associated with " cat " will be high , while the probability for " dog " will be " low " . <p> This behavior implies that there some actual- confidence in our predictions and that our algorithm is actually- learning from the dataset . <p> Exactly- how the learning takes place involves updating our weight matrix- W , which boils down to being an- optimization problem . Well be reviewing how to perform gradient decent and other optimization algorithms in future blog posts . <h> Summary <p> In todays blog- post , we looked at the Softmax classifier , which is simply a generalization of the the binary Logistic Regression classifier . <p> When constructing Deep Learning and Convolutional Neural Network models , youll- undoubtedly run in to the Softmax classifier and the cross-entropy loss @ @ @ @ @ @ @ @ @ @ loss are popular choices , I can almost guarantee with absolute certainly that you 'll see cross-entropy loss with more frequency this is mainly due to the fact that the Softmax classifier outputs- probabilities rather than- margins . Probabilities are much easier for us as humans to interpret , so that is a particularly nice quality of Softmax classifiers . <p> Now that we understand the fundamentals of loss functions , were ready to tack on another term to our loss method - regularization . <p> The regularization term is appended to our loss function and is used to control how our weight matrix W " looks " . By controlling- W and ensuring that it " looks " a certain way , we can actually increase classification accuracy . <p> After we discuss regularization , we can then move on to optimization the process that actually takes the output of our scoring and loss functions and uses this output to tune our weight matrix- W to actually " learn " . <p> Anyway , I hope you enjoyed this blog post ! <p> Before you go , be sure to @ @ @ @ @ @ @ @ @ @ notified when new blog posts go live ! <h> Downloads : 55217 @qwx675217 <h> 7 Responses to Softmax Classifiers Explained <p> Nice tutorial , very nicely explained . Shouldnt the negative log loss for the airplane output be log base e ( since it is the inverse of exponentiation over e earlier ) ? The value shown seems to be log base 10 . <p> I used an Excel spreadsheet to derive this example . The negative log loss for the airplane was computed by : <p> -log ( " Normalized Probabilities " ) <p> Which does indeed yield 0.0308 . The " Normalized Probabilities " are also derived by taking the " Unnormalized Probabilities " and dividing by the sum of the column . Unless I 'm having a brain fart ( totally possible ) , this seems correct . <p> EDIT : Youre absolutely right . My confusion came from not understanding which log function was being used by default inside the spreadsheet . I have updated the post and example image to reflect this change . Thanks for catching the error ! <p> Yes , that is correct @ @ @ @ @ @ @ @ @ @ stated in the blog post , I wanted to demonstrate that the model is actually learning which is demonstrated by the large gaps in probabilities between the two classes . This model is n't powerful enough to demonstrate those same large gaps on the testing data ( that will come in future blog posts ) . <p> While computing dE/dw for a particular weight using softmax and log-loss I got a strange result which indicates that for all weights connected to a node in the previous layer the update value is same . Something like : dw ( i , j ) = -z ( i , j ) *opj + -z ( i , j ) <p> All the weights are initialized randomly , does this not introduce a @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485098 @185098/ <h> Archive Tutorials <p> A few weeks ago I did a blog post on how to install the dlib library on Ubuntu and macOS . Since Raspbian , the operating system that ( most ) Raspberry Pi users run is Debian-based ( as is Ubuntu ) , the- same install instructions can be used for Raspbian as Ubuntu however , there 's a catch . The Raspberry Pi 3 ships <p> In last weeks blog post , I demonstrated how to perform facial landmark detection in real-time in video streams . Today , we are going to build upon this knowledge and develop a computer vision application that is capable of- detecting and counting blinks in video streams using facial landmarks and OpenCV . To build our blink detector , well be <p> Over the past few weeks we have been discussing- facial landmarks and the role they play in computer vision and image processing . We 've started off by learning how to detect facial landmarks in an image . We then discovered how to label and annotate- each of the facial regions , such as eyes , eyebrows , @ @ @ @ @ @ @ @ @ @ blog post is part three in our current series on facial landmark detection and their applications to computer vision and image processing . Two weeks ago I demonstrated how to install the dlib library- which we are using for facial landmark detection . Then , last week I discussed how to use dlib to actually- detect facial landmarks in <p> Last week we learned how to install and configure dlib- on our system with Python bindings . Today we are going to use dlib and OpenCV to detect- facial landmarks in an image . Facial landmarks are used to localize and represent salient regions of the face , such as : Eyes Eyebrows Nose Mouth Jawline Facial landmarks have been successfully <p> Two weeks ago I interviewed Davis King , the creator and chief maintainer of the dlib library . Today I am going to demonstrate how to install dlib with Python bindings on both- macOS and- Ubuntu . I- highly encourage you to take the time to install dlib on your system over the next couple of days . Starting next week well <p> A few months ago I @ @ @ @ @ @ @ @ @ @ Neural Networks ( specifically , VGG16 ) pre-trained on the ImageNet dataset with Python and the Keras deep learning library . The pre-trained networks inside of Keras are capable of recognizing- 1,000 different object categories , similar to objects we encounter in our day-to-day lives with high <p> Todays tutorial is inspired by a post I saw a few weeks back on /r/computervision asking how to recognize digits in an image containing a thermostat identical to the one at the top of this post . As Reddit users were quick to point out , utilizing computer vision to recognize digits on a thermostat tends to- overcomplicate <p> Have you ever worked with a video file via OpenCVs cv2.VideoCapture- function and found that reading frames- just felt slow and sluggish ? Ive been there - and I know exactly how it feels . Your entire video processing pipeline crawls along , unable to process more than one or two frames per second even- though @ @ @ @ @ 
@@71485099 @185099/ <h> Author Archive Adrian Rosebrock <p> I 've got some exciting news to share today ! My- Deep Learning for Computer Vision with Python- Kickstarter campaign- is set to launch in- exactly one week- on- Wednesday , January 18th at 10AM EST . This book has only goal " - to help- developers , researchers , and students- just like yourself become- experts- in deep learning for image recognition and classification . Whether this is the- first time you 've worked with <p> Todays blog post is part of a two part series on working with video files using OpenCV and Python . The first part of this series will focus on a question emailed in by PyImageSearch reader , Alex . Alex asks : I need to count the total number of frames in a video file with OpenCV . The only <p> Let me tell you an embarrassing story of- how I- wasted three weeks of research time- during graduate school six- years ago . It was the end of my second semester of coursework . I had taken all of my exams early and all my projects for @ @ @ @ @ @ @ @ @ @ were essentially nil , I started experimenting <p> Each week I receive and respond to at least 2-3 emails and 3-4 blog post comments regarding NoneType- errors in OpenCV and Python . For beginners , these errors can be hard to diagnose by definition they are n't  very informative . Since this question is getting asked so often I decided to dedicate an entire blog post <p> Over the past few weeks I have demonstrated how to compile OpenCV 3 on macOS with Python ( 2.7 , 3.5 ) bindings from source . Compiling OpenCV via source gives you- complete and total control over which modules you want to build , - how- they are built , and- where they are installed . All this control can come at a price though . The downside <p> You may have heard me mention it in a passing comment on the PyImageSearch blog Maybe I even hinted at it in a 1-on-1 email Or perhaps you simply saw the writing on the wall due to the recent uptick in Deep Learning/Neural Network tutorials here on the blog But I 'm here today to @ @ @ @ @ @ @ @ @ @ 3 with Python 2.7 bindings on macOS Sierra and above . In todays tutorial well learn how to install- OpenCV 3 with- Python 3.5- bindings on macOS . I decided to break these install tutorials into two separate guides to keep them well organized and easy to follow . To learn how to <p> I 'll admit it : Compiling and installing OpenCV 3 on macOS Sierra was- a lot more of a challenge than I thought it would be , even for someone who has a compiled OpenCV on hundreds of machines over his lifetime . If you 've tried to use one of my previous tutorials on installing OpenCV on your freshly updated <p> Ever since I wrote the first PyImageSearch tutorial on installing OpenCV + Python on the Raspberry Pi B+ back in February 2015 it has been my dream to offer a- downloadable , pre-configured Raspbian . img file with OpenCV pre-installed . Today this dream has become a reality . I am pleased to announce that both the Quickstart Bundle and- Hardcopy <p> A few months ago I demonstrated how to install the Keras deep learning library @ @ @ @ @ @ @ @ @ @ provide detailed , step-by-step instructions to install Keras using a TensorFlow backend , originally developed by the researchers and engineers on the Google Brain Team . Ill also ( optionally ) demonstrate how @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485100 @185100/ <h> How to install CUDA Toolkit and cuDNN for deep learning <p> If you 're serious about doing any type of deep learning , you should be utilizing your- GPU rather than your- CPU. - And the more GPUs you have , the better off you are . <p> If you already have an NVIDIA supported GPU , then the next logical step is to install two important libraries : <p> The NVIDIA CUDA Toolkit : - A development environment for building GPU-accelerated applications . This toolkit includes a compiler- specifically designed for NVIDIA GPUs and associated math libraries +- optimization routines . <p> The cuDNN library : A GPU-accelerated library of primitives for deep neural networks . Using the cuDNN package , you can increase training speeds by upwards of 44% , with over- 6x speedups in Torch and Caffe . <p> In the remainder of this blog post , I 'll demonstrate how to install both the NVIDIA CUDA Toolkit and the cuDNN library for deep learning . <p> Specifically , I 'll be using an Amazon EC2 g2.2xlarge machine running Ubuntu 14.04 . Feel free to spin up an @ @ @ @ @ @ @ @ @ @ the time you 're finished this tutorial , you 'll have a- brand new system ready for deep learning . <h> How to install CUDA Toolkit and cuDNN for deep learning <p> As I mentioned in an earlier blog post , Amazon offers an EC2 instance that provides access to the GPU for computation purposes . <p> This instance is named the- g2.2xlarge instance and costs approximately- $0.65 per hour . The GPU included on the system is a K520 with 4GB of memory and 1,536 cores . <p> You can also upgrade to the- g2.8xlarge instance ( $2.60 per hour ) to obtain- four K520 GPUs ( for a grand total of 16GB of memory ) . <p> For most of us , the g2.8xlarge is a bit expensive , especially if you 're only doing deep learning as a hobby . On the other hand , the g2.2xlarge instance is a totally reasonable option , allowing you to forgo your afternoon Starbucks coffee and trade a caffeine jolt- for a bit of deep learning fun and education . <p> Insider the remainder of this blog post , I 'll detail how to @ @ @ @ @ @ @ @ @ @ in a g2.2xlarge GPU instance on Amazon EC2 . <p> If you 're interested in deep learning , I- highly encourage you to setup your own EC2 system using the instructions detailed in this blog post you 'll be able to use your GPU instance to follow along with future deep learning tutorials on the PyImageSearch blog ( and trust me , there will be a lot of them ) . <p> Note : Are you new to Amazon AWS and EC2 ? You might want to read Deep learning on Amazon EC2 GPU with Python and nolearn before- continuing . This blog post provides step-by-step instructions ( with tons of screenshots ) on how to- spin up your first EC2 instance and use it for deep learning . <h> Installing the CUDA Toolkit <p> Assuming you have either ( 1 ) an EC2 system spun up with GPU support or ( 2 ) your own NVIDIA-enabled GPU hardware , the next step is to install the CUDA Toolkit . <p> But before we can do that , we need to install a few required packages first : <p> How to install @ @ @ @ @ @ @ @ @ @ 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> 6 <p> $sudo apt-getupdate <p> $sudo apt-getupgrade <p> $sudo apt-getinstall build-essential cmake git unzip pkg-config <p> $sudo apt-getinstall libopenblas-dev liblapack-dev <p> $sudo apt-getinstall linux-image-generic **25;1439;TOOLONG <p> $sudo apt-getinstall linux-source linux-headers-generic <p> One issue that Ive encountered on Amazon EC2 GPU instances is that we need to- disable the Nouveau kernel driver since it- conflicts with the NVIDIA- kernel module that were about to install . <p> Note : I 've only had to disable the Nouveau kernel driver on Amazon EC2 GPU instances I 'm not sure if this needs to be done on standard , desktop installations of Ubuntu . Depending on your own hardware and setup , you can- potentially skip this step . <p> To disable the Nouveau kernel driver , first create a new file : <p> How to install CUDA Toolkit and cuDNN for deep learning <p> Shell <p> 1 <p> $sudo **42;1466;TOOLONG <p> And then add the following lines to the file : <p> How to install CUDA Toolkit and cuDNN for deep learning <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 @ @ @ @ @ @ @ @ @ @ <p> aliasnouveau off <p> aliaslbm-nouveau off <p> Save this file , exit your editor , and then update the initial RAM filesystem , followed by rebooting your machine : <p> With the super fast EC2 connection , I was able to download the entire 1.1GB file in less than 30 seconds : <p> Figure 1 : Downloading the CUDA Toolkit from NVIDIAs official website . <p> Next , we need to make the . run- file executable : <p> How to install CUDA Toolkit and cuDNN for deep learning <p> Shell <p> 1 <p> **27;1510;TOOLONG <p> Followed by extracting the individual installation scripts into an installers- directory : <p> How to install CUDA Toolkit and cuDNN for deep learning <p> Shell <p> 1 <p> 2 <p> $mkdirinstallers <p> LONG ... <p> Your installers- directory should now look like this : <p> Figure 2 : Extracting the set of . run files into the installers directory . <p> Notice how we have three separate . run- files well need to execute each of these individually and in the correct order : <p> **29;1539;TOOLONG <p> **36;1570;TOOLONG <p> **38;1608;TOOLONG <p> The following set @ @ @ @ @ @ @ @ @ @ Toolkit : <p> How to install CUDA Toolkit and cuDNN for deep learning <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> $sudo. **30;1648;TOOLONG <p> $modprobe nvidia <p> $sudo. **37;1680;TOOLONG <p> $sudo. **39;1719;TOOLONG <p> Again , make sure you select the default options and directories when prompted . <p> To verify that the CUDA Toolkit is installed , you should examine your /usr/local- directory which should contain a sub-directory named cuda-7.5- , followed by a sym-link named cuda- which points to it : <p> Figure 3 : Verifying that the CUDA Toolkit has been installed . <p> Now that the CUDA Toolkit is installed , we need to update our /. bashrc- configuration : <p> How to install CUDA Toolkit and cuDNN for deep learning <p> Shell <p> 1 <p> $nano/.bashrc <p> And then append the following lines to define the CUDA Toolkit PATH- variables : <p> How to install CUDA Toolkit and cuDNN for deep learning <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> # CUDA Toolkit <p> export **28;1760;TOOLONG <p> export LONG ... <p> export PATH=$CUDAHOME/bin : $PATH <p> Your . bashrc- file @ @ @ @ @ @ @ @ @ @ up a new terminal , but since we- just modified it , we need to manually source- it : <p> How to install CUDA Toolkit and cuDNN for deep learning <p> Shell <p> 1 <p> $source/.bashrc <p> Next , let 's install cuDNN ! <h> Installing cuDNN <p> We are now ready to install the NVIDIA CUDA Deep Neural Network library , a GPU-accelerated library for deep neural networks . Packages such as Caffe and Keras ( and at a lower level , Theano ) use cuDNN to- dramatically speedup the networking training process . <p> Figure 5 : Since were installing the cuDNN on Ubuntu , we download the library for Linux . <p> This is a small , 75MB download which you should save to your- local machine ( i.e. , the laptop/desktop you are using to read this tutorial ) and then- upload to your EC2 instance . To accomplish this , simply use scp- , replacing the paths and IP address as necessary : <p> Installing cuDNN is quite simple all we need to do is copy the files in the lib64- and include- directories to their @ @ @ @ @ @ @ @ @ @ install CUDA Toolkit and cuDNN for deep learning <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> $cd <p> $tar-zxf **31;1790;TOOLONG <p> $cdcuda <p> $sudo **31;1823;TOOLONG <p> $sudo **35;1856;TOOLONG <p> Congratulations , cuDNN is now installed ! <h> Doing a bit of cleanup <p> Now that we have ( 1 ) installed the NVIDIA CUDA Toolkit and ( 2 ) installed cuDNN , let 's do a bit of cleanup to reclaim disk space : <p> How to install CUDA Toolkit and cuDNN for deep learning <p> Shell <p> 1 <p> 2 <p> 3 <p> $cd <p> $rm-rf cuda installers <p> LONG ... <p> In future tutorials , I 'll be demonstrating how to use both CUDA and cuDNN to facilitate faster training of deep neural networks . <h> Summary <p> In todays blog post , I demonstrated how to install the CUDA Toolkit and the cuDNN library for deep learning . If you 're interested in working with deep learning , I- highly recommend that you setup a GPU-enabled machine . <p> If you do n't  already have an NVIDIA-compatible GPU , no worries Amazon EC2 offers the @ @ @ @ @ @ @ @ @ @ instances , both of which can be used for deep learning . <p> The steps detailed in this blog post will work on- both the g2.2xlarge and g2.8xlarge instances for Ubuntu 14.04 feel free to choose an instance and setup your own deep learning development environment ( in fact , I encourage you to do just that ! ) <p> The entire process should only take you 1-2 hours to complete if you are familiar with the command line and Linux systems ( and have a small amount of experience in the EC2 ecosystem ) . <p> Best of all , you can use this EC2 instance to follow along with future deep learning tutorials on the PyImageSearch blog . <p> Be sure to signup for the PyImageSearch Newsletter using the form below to be notified when new deep learning articles are published ! <p> If you have an Intel CPU and installed Intel MKL library ( it is not free ) , you could do it in Windows . You will find a way to tune up OpenCV DNN source and compile it with Intel MKL library linking . @ @ @ @ @ @ @ @ @ @ DNN . <p> Thanks for sharing , Martin . I ran into the same issue . When I tried your suggestion of using Cuda 7.0 , that still did n't  work . So I ended up following the suggestion of Silvain in that same thread who mentioned to installed the latest NVidia driver : <p> Hello Adrian , Thanks for your posting ! It is very nice . Can I have a question ? I do a project about number recognition . and I 'm a beginner Other people says SVM is good method for number recognition . ( And Your web site looks perpect for it ! = ) <p> But I have a question . Your posting keep talking about GPU and It looks focusing on Mac PC ( ? ) But I want to do a project with raspberry pi 3 as a processing machine . Is raspberry pi 3 not good for machine learning ? or How do I Read your postings ? ( Pls give me " To Read List " for Raspberry pi + SVM ) <p> If you 're just getting started with computer vision @ @ @ @ @ @ @ @ @ @ first project . I would also suggest starting with k-NN , Logistic Regression , and SVMs as these are good " starter " algorithms . <p> I do indeed use a Mac ; however , I SSH into my GPU instance . If could certainly use your Pi for machine learning , but keep in mind that you 're limited my RAM . Many machine learning algorithms are quite memory hungry and the Pi 3 only has 1GB of RAM . <p> May I ask why we use nano to write " options nouveau modeset=0 " into the nouveau-kms.conf file and after that append it a 2nd time to the same file with " $ echo options nouveau modeset=0 sudo tee -a **32;1893;TOOLONG " ? Its not unlikely I just overlooking something , so excuse the question . <p> Hi Adrian .. thank you for such a wonderful tutorial .. i got stuck in the step where i have to install cuda , exactly after this ( ( After reboot , the Nouveau kernel driver should be disabled. ) ) i always got this message that i am running X server @ @ @ @ @ @ @ @ @ @ Ctrl+Alt+F1 and then i stopped the lightdm , and the tried again to install cuda and it worked just fine .. again thanks for this tutorial <p> Specifically , when I say modprobe nvidia , the terminal says : " modprobe : ERROR : could not insert nvidia375 : Operation not permitted " <p> I 'm new to ubuntu ( just installed it for the first time yesterday ) so I 'm not entirely sure how to proceed . Any help ? I have ubuntu 16.04.2 , my gpu is the quadro k2200 ( the driver I chose in software and updates was the 375.39 one , which according to the nvidia site seems to be the right one ) . <p> Hi Adrian , I followed your instruction step by step , but i get stack on : $ sudo . **30;1927;TOOLONG it shows : You do not appear to have an NVIDIA GPU supported by the 352.39 NVIDIA Linux graphics driver installed in this system . For further details , please see the appendix SUPPORTED NVIDIA GRAPHICS CHIPS in the README available on the Linux driver download page athttp : @ @ @ @ @ @ @ @ @ @ I am new to ubuntu and I have gtx970 in my computer . <p> Hi Summer , thanks for the comment . Unfortunately , I have n't used the GTX 970 before so I 'm not sure what the exact error is . The latest GPU driver released by NVIDIA is 375.51 at the time of this writing , so I @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485101 @185101/ <p> In order to to keep our discussions of these loss functions straightforward , I purposely left out an important component : - regularization . <p> While our loss function allows us to determine how well ( or poorly ) our set of parameters ( i.e. , weight matrix , and bias vector ) are performing on a given classification task , the loss function itself does not take into account how the weight matrix " looks " . <p> What do - I mean by " looks " ? <p> Well , keep in mind that there may be an- infinite set of parameters that obtain reasonable- classification accuracy on our dataset how do we go about choosing a set of parameters that will help ensure our model generalizes well ? Or at the very least , lessen the affects of overfitting ? <p> The answer is regularization . <p> There are various types of regularization techniques , such as L1 regularization , L2 regularization , and Elastic Net and in the context of Deep Learning , we also have- dropout ( although dropout is more-so a- technique @ @ @ @ @ @ @ @ @ @ tutorial , well mainly be focusing on the former rather than the later . Once we get to more advanced deep learning tutorials , I 'll dedicate time to discussing dropout as well . <p> In the remainder of this blog post , I 'll be discussing regularization further . Ill also demonstrate how to update our Multi-class SVM loss and cross-entropy loss functions to include regularization . Finally , well write some Python code to construct a classifier that applies regularization to an image classification problem . <p> The remainder of this blog post is broken into four parts . First , we discuss what regularization is . I then detail how to update our loss function to include the regularization term . <p> From there , I list out three common types of regularization you 'll likely see when performing image classification and machine learning , - especially in the context of neural networks and deep learning . <p> Finally , I 'll provide a Python + scikit-learn example that demonstrates how to apply regularization to an image classification dataset . <h> What is regularization and why do we need it ? @ @ @ @ @ @ @ @ @ @ , ensuring that our models are better at making ( correct ) classifications or more simply , - the ability to- generalize . <p> If we do n't  apply regularization , our classifiers can easily become too complex and- overfit to our training data , in which case we lose the ability to generalize to our testing data ( and data points outside the testing set as well ) . <p> Similarly , without applying regularization we also run the risk of- underfitting . In this case , our model performs poorly on the training our our classifier is not able to model the relationship between the input data and the output class labels . <p> Underfitting is relatively easy to catch you examine the classification accuracy on your training data and take a look at your model . <p> If your training accuracy is very low and your model is excessively simple , then you are likely a victim of underfitting . The normal remedy to underfitting is to essentially increase the number of parameters in your model , thereby increasing complexity . <p> Overfitting is a different beast @ @ @ @ @ @ @ @ @ @ training accuracy and recognizing when your classifier is performing- too well on the training data and- not good enough on the testing data , it becomes harder to correct . <p> There is also the problem that you can walk a very fine line between model complexity if you simplify your model- too much , then you 'll be back to underfitting . <p> A better approach is to apply- regularization which will help our model generalize and lead to less overfitting . <p> The best way to understand regularization is to see the implications it has on our loss function , which I discuss in the next section . <p> What we are doing here is looping over all entries in the matrix and taking the sum of squares . There are more efficient ways to compute this of course , I 'm just simplifying the code as a matter of explanation . <p> The sum of squares in the L2 regularization penalty discourages large weights in our weight matrix- W , preferring smaller ones . <p> Why might we want to discourage large weight values ? <p> In short , @ @ @ @ @ @ @ @ @ @ generalize , and thereby reduce overfitting . <p> Think of it this way the larger a weight value is , the more influence it has on the output prediction . This implies that dimensions with larger weight values can almost singlehandedly control the output prediction of the classifier ( provided the weight value is large enough , of course ) , which will almost certainly lead to overfitting . <p> To mitigate the affect various dimensions have on our output classifications , we apply regularization , thereby seeking- W values that take into account- all of the dimensions rather than the few with large values . <p> Again , our loss function has the following basic form , but now we just add in regularization : <p> The first term we have already seen before this is the average loss over all samples in our training set . <p> The second term is new this is our regularization term . <p> The variable is a hyperparameter that controls the- amount or- strength of the regularization we are applying . In practice , both the learning rate and regularization term are @ @ @ @ @ @ @ @ @ @ <p> Expanding the Multi-class SVM loss to include regularization yields the final equation : <p> In terms of which regularization method you should be using ( including none at all ) , you should treat- this choice as a hyperparameter you need to optimize over and perform experiments to determine- if regularization should be applied , and if so , - which method- of regularization . <p> Finally , I 'll note that there is another- very common type of regularization that well see in a future tutorial - dropout . <p> Dropout is frequently used in Deep Learning , especially with Convolutional Neural Networks . <p> Unlike L1 , L2 , and Elastic Net regularization , which boil down to functions defined in the form- R(W) , dropout is an actual- technique we apply to the connections between nodes in a Neural Network . <p> As the name implies , connections " dropout " and randomly disconnect during training time , ensuring that no one node in the network becomes fully responsible for " learning " to classify a particular label . Ill save a more thorough discussion of dropout @ @ @ @ @ @ @ @ @ @ regularization with Python and scikit-learn <p> Now that we 've discussed regularization in the context of machine learning , let 's look at some code that actually- performs various types of regularization . <p> All of the code associated with this blog post , expect for the final code block , has already been reviewed extensively- in previous blog posts in this series . <p> Therefore , for a thorough review of the actual process used to extract features and construct the training and testing split for the Kaggle Dogs vs . Cats dataset , I 'll refer you to the introduction to linear classification tutorial . <p> You can download the full code to this blog post by using the- " Downloads " section at the bottom of this tutorial . <p> The code block below demonstrates how to apply the Stochastic Gradient Descent ( SGD ) classifier with log-loss ( i.e. , Softmax ) and various types of regularization methods to our dataset : <h> Summary <p> In todays blog post , I discussed the concept of- regularization and the impact it has on machine learning classifiers. - Specifically , we @ @ @ @ @ @ @ @ @ @ works by examining our weight matrix- W and penalizing it if it does not confirm to the specified penalty function . <p> Applying this penalty helps ensure we learn a weight matrix- W that generalizes better and thereby helps lesson the negative affects of overfitting . <p> In practice , you should apply hyperparameter tuning to determine : <p> If regularization should be applied , and if so , which regularization method should be used . <p> The strength of the regularization ( i.e. , the variable ) . <p> You may notice that applying regularization may actually- decrease your training set classification accuracy this is acceptable provided that your testing set accuracy- increases , which would be a demonstration- of regularization in action ( i.e. , avoiding/lessening the impact of overfitting ) . <p> In next weeks blog post , I 'll be discussing how to build a simple feedforward neural network using Python and Keras. - Be sure to enter your email address in the form below to be notified when this blog post goes live ! <h> Downloads : 55217 @qwx675217 <p> Wait , I think that CV @ @ @ @ @ @ @ @ @ @ accuracy report is already being done on a cross-validated set , which would be equivalent to report on a validation set , right ? <p> The cross-validation will be performed by dividing your training set into N parts , training on all but one of the sets , and then validating on the other . This is done for each data split and serves as @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485102 @185102/ <h> Measuring size of objects in an image with OpenCV <p> Measuring the size of an object ( or objects ) in an image has been a- heavily requested tutorial on the PyImageSearch blog for some time now and it feels- great to get this post online and share it with you . <p> Todays post is the second in a three part series on- measuring the size of objects in an image and- computing the distances between them . <p> Last week , we learned an important technique : how- reliably order a set of rotated bounding box coordinates in a top-left , top-right , bottom-right , and bottom-left arrangement . <p> Today we are going to utilize this technique to aid us in computing the size of objects in an image. - Be sure to read the entire post to see how its done ! <p> I call this the " pixels per metric " ratio , which I have more formally defined in the following section . <h> The " pixels per metric " ratio <p> In order to determine the size of an object in @ @ @ @ @ @ @ @ @ @ calibration " ( not to be confused with intrinsic/extrinsic calibration ) using a reference object . Our reference object should have two important properties : <p> Property #1 : We should know the dimensions of this object ( in terms of width or height ) in a measurable unit ( such as millimeters , inches , etc . ) . <p> Property- #2 : We should be able to easily find this reference object in an image , either based on the- placement of the object ( such as the reference object always being placed in the top-left corner of an image ) or via appearances ( like being a distinctive- color or shape , unique and different from all other objects in the image ) . In either case , our reference should should be- uniquely identifiable in some manner . <p> In this example , well be using the United States quarter as our reference object and throughout all examples , ensure it is always the- left-most object in our image : <p> Figure 1 : Well use a United States quarter as our reference object and ensure @ @ @ @ @ @ @ @ @ @ image , making it easy for us to extract it by sorting contours based on their location . <p> By guaranteeing the quarter is the left-most object , we can sort our object contours from left-to-right , grab the quarter ( which will always be the first contour in the sorted list ) , and use it to define our- pixelspermetric , which we define as : <p> pixelspermetric = objectwidth / knowwidth <p> A US quarter has a- knownwidth of 0.955 inches . Now , suppose that our- objectwidth ( measured in pixels ) is computed be 150 pixels wide ( based on its associated bounding box ) . <p> The- pixelspermetric is therefore : <p> pixelspermetric = 150px / 0.955in = 157px <p> Thus implying there are approximately 157 pixels per every 0.955 inches in our image . Using this ratio , we can compute the size of objects in an image . <h> Measuring the size of objects with computer vision <p> Now that we understand the " pixels per metric " ratio , we can implement the Python driver script used to measure the size of @ @ @ @ @ @ @ @ @ @ file , name it objectsize.py- , and insert the following code : <p> Measuring size of 17 <p> 18 <p> 19 55203 @qwx675203 <p> fromscipy.spatial importdistance asdist <p> fromimutils importperspective <p> fromimutils importcontours 55220 @qwx675220 55218 @qwx675218 55219 @qwx675219 <p> importcv2 <p> defmidpoint ( ptA , ptB ) : <p> LONG ... 55202 @qwx675202 55206 @qwx675206 <p> ap.addargument ( " -i " , " --image " , required=True , <p> help= " path to the input image " ) <p> LONG ... <p> help= " width of the left-most object in the image ( in inches ) " ) 55208 @qwx675208 <p> Lines 2-8 import our required Python packages . Well be making heavy use of the imutils package in this example , so if you do n't  have it installed , make sure you install it before proceeding : <p> Measuring size of objects in an image with @ @ @ @ @ @ @ @ @ @ if you- do have imutils- installed , ensure- you have the latest version , which is 0.3.6- at the time of this writing : <p> Measuring size of objects in an image with OpenCV <p> Shell <p> 1 <p> $pip install--upgrade imutils <p> Lines 10 and 11 defines a helper method called midpoint- , which as the name suggests , is used to compute the midpoint between two sets of- ( x , y ) -coordinates . <p> We then parse our command line arguments on- Lines 14-19 . We require two arguments , --image- , which is the path to our input image containing the objects we want to measure , and --width- , which is the width ( in inches ) of our reference object , presumed to be the left-most object in our --image- . <p> We can now load our image and preprocess it : <p> Measuring size of objects in an @ @ @ 40 <p> # load the image , convert it to grayscale , and blur it slightly <p> image=cv2.imread ( args " image " ) 55215 @qwx675215 <p> **26;1959;TOOLONG , ( 7,7 ) , 0 ) <p> # perform edge detection , then perform a dilation + erosion to <p> # close gaps in between object edges <p> **28;1987;TOOLONG <p> edged=cv2.dilate ( edged , None , iterations=1 ) <p> edged=cv2.erode ( edged , None , iterations=1 ) <p> # find contours in the edge map <p> LONG ... 55211 @qwx675211 <p> **36;2017;TOOLONG <p> # sort the contours from left-to-right and initialize the <p> # ' pixels per metric ' calibration variable <p> ( cnts , ) **28;2055;TOOLONG <p> pixelsPerMetric=None <p> Lines 22-24 load our image from disk , convert it to grayscale , and then smooth it using a Gaussian filter . We then perform edge detection along with a dilation + erosion to close any gaps in between edges in the edge map ( Lines 28-30 ) . <p> Lines 33-35 find contours ( i.e. , the outlines ) that correspond to the objects- in our edge map . <p> @ @ @ @ @ @ @ @ @ @ individual contours . If the contour is not sufficiently large , we discard the region , presuming it to be noise left over from the edge detection process ( Lines 45 and 46 ) . <p> Provided that the contour region is large enough , we compute the rotated bounding box of the image on- Lines 50-52 , taking special care to use the cv2.cv.BoxPoints- function for OpenCV 2.4 and the cv2.boxPoints- method for OpenCV 3 . <p> First , we compute the Euclidean distance between the our sets of midpoints ( Lines 90 and 91 ) . The dA- variable will contain the- height distance ( in pixels ) while dB- will hold our- width distance . <p> We then make a check on- Line 96 to see if our pixelsPerMetric- variable has been initialized , and if it has n't  , we divide dB- by our supplied --width- , thus giving us our ( approximate ) pixels per inch . <p> Now that our pixelsPerMetric- variable has been defined , we can measure the size of objects in an image : <p> Measuring size of objects in an <p> 112 <p> 113 <p> # compute the size of the object <p> dimA=dA/pixelsPerMetric <p> dimB=dB/pixelsPerMetric <p> # draw the object sizes on the image <p> cv2.putText ( orig , " : .1fin " . format(dimA) , <p> LONG ... <p> 0.65 , ( 255,255,255 ) , 2 ) <p> cv2.putText ( orig , " : .1fin " . format(dimB) , <p> LONG ... <p> 0.65 , ( 255,255,255 ) , 2 ) <p> # show the output image <p> cv2.imshow ( " Image " , orig ) 55212 @qwx675212 <p> Lines 100 and 101 compute the dimensions of the object ( in inches ) by dividing the respective Euclidean distances by the pixelsPerMetric- value ( see the- " Pixels Per Metric " section above for more information on why this ratio works ) . <p> Figure 2 : Measuring the size of objects in an image using OpenCV , Python , and computer vision + image processing techniques . <p> As you can see , we have successfully @ @ @ @ @ @ @ @ @ @ our business card is correctly reported as- 3.5in x 2in . Similarly , our nickel is accurately described as- 0.8in x 0.8in . <p> However , not all our- results are- perfect . <p> The Game Boy cartridges are reported as having slightly different dimensions ( even though they are the same size ) . The height of both quarters are also off by- 0.1in . <p> So why is this ? How come the object measurements are not 100% accurate ? <p> The reason is two-fold : <p> First , I hastily took this photo with my- iPhone . The angle is most certainly- not a perfect 90-degree angle " looking down " ( like a birds-eye-view ) at the objects . Without a perfect 90-degree view ( or as close to it as possible ) , the dimensions of the objects can appear distorted . <p> Second , I did not calibrate my iPhone using the intrinsic and extrinsic parameters of the camera . Without determining these parameters , photos can be prone to radial and tangential lens distortion . Performing an extra calibration step to find these @ @ @ @ @ @ @ @ @ @ a better object size approximation ( but I 'll leave the discussion of distortion correction as a topic of a future blog post ) . <p> In the meantime , strive to obtain as close to a 90-degree viewing angle as possible when taking photos of your objects this will help increase the accuracy of your object size estimation . <p> That said , let 's look at a second example of measuring object size , this- time measuring the dimensions of pills : <p> Measuring size of objects in an image with OpenCV <p> Shell <p> 1 <p> $python objectsize.py--image **32;2085;TOOLONG <p> Figure 3 : Measuring the size of pills in an image with OpenCV . <p> Nearly 50% of all 20,000+ prescription pills in the United States are round and/or white , thus if we can filter pills based on their measurements , we stand a better chance at accurately identification the medication . <p> Finally , we have a final example , this time using a- 3.5in x 2in business card to measure the size of two vinyl EPs and an envelope : <p> Measuring size of objects in @ @ @ @ @ @ @ @ @ @ objectsize.py--image **30;2119;TOOLONG <p> Figure 4 : A final example of measuring the size of objects in an image with Python + OpenCV . <p> Again , the results are n't  quite perfect , but this is due to ( 1 ) the viewing angle and ( 2 ) lens distortion , as mentioned above . <h> Summary <p> In this blog post , we learned how to measure the size of objects in an image using Python and OpenCV . <p> To compute this ratio , we need a reference object with two important properties : <p> Property #1 : The reference object should have- known dimensions ( such as width or height ) in terms of a measurable unit ( inches , millimeters , etc . ) . <p> Property #2 : The reference object should be- easy to find , either in terms of- location of the object or in its- appearance . <p> Provided that both of these properties can be met , you can utilize your reference object to calibrate your- pixelspermetric variable , and from there , compute the size of other objects in an @ @ @ @ @ @ @ @ @ @ take this example a step further and learn how to compute the distance- between objects in an image . <p> Ne sure to signup for the PyImageSearch Newsletter using the form below to be notified when the next blog post goes live- you wo n't want to miss it ! <h> Downloads : 55217 @qwx675217 <h> 111 Responses to Measuring size of objects in an image with OpenCV <p> Nice write-up as always Adrian but you have missed mentioning directly the prerequisite that all of the objects to be measured be co-planar with the reference object even though you did mention that the camera must be at as near as possible 90 degrees to that plane . <p> To do size and or distance calculations without a reference object you need a ) the lens and focus information and b ) two or more images of the same scene from different angles so as to use parallax calculations . <p> Steve , Calibration can be done by taking a top view image and a image of your original camera position , after which you can find a homography matrix by which @ @ @ @ @ @ @ @ @ @ perspective.To find the homography matrix you will need to find similar points in both the top view reference and input image of different plane , which can be done by finding the sift point followed by a matcher . <p> You can look up to Adrians blog on making panoramic images for reference . <p> Only problem I see is to find the pixels per metric ratio value by giving an width value of the ref image , compensating for the camera distance from the image . Is there any way to solve the camera distance problem in an automated fashion ? <p> So if I understand your question correctly , your goal is to ( 1 ) detect the body , ( 2 ) extract each body part , and then ( 3 ) measure each body part ? If so , you should do some research on skeletal models for the body , and perhaps even utilizing a stereo camera to help in the segmentation of each body part . <p> Wonderful , thank you very much . I think , I can apply this method to measure @ @ @ @ @ @ @ @ @ @ ratio in EXIF from microscope , so I 'll declare it as a constant . Maybe you 'll suggest something more optimal for round and ellipsoid objects like cells ? I think I should find the smallest and the largest diameter . After this I can find the excentricity . <p> In that case , you might want to compute the minimum enclosing circle or the minimum enclosing ellipse , then compute your ratios based on that . Also , you might want to look into ImageJ which is heavily used for microscopic images I even did some work for the National Cancer Institute developing plugins for ImageJ . <p> Thank you . I use it with CellProfiler , but I often need to develop some new functions . So I use the scipy most of the time for image analysis . Now I want to try the opencv . Thanks again for your blog . Useful tutorials . <p> Thank you for this post . I 'm working on this problem and i found a solution for measuring objects using the focale distance . Obviously this can only work in smartphone api @ @ @ @ @ @ @ @ @ @ think i 'm not far from a good result . The real problem is that we need a camera with a motorised focale . <p> This may be considered to be a bit " off the wall " but I have 3 table tennis balls ( known size ) positioned at the interface between the ceiling and the wall of a room . If I have an image that shows all the balls am I correct in thinking that I can compute the distance between the balls and also the distance from the camera to each of the balls ? I appreciate that lens distortion may have some impact but I 'm not interested in that here as I only want to obtain an approx. room size . Assuming that the ceiling is level I would like to expand this idea to include adjacent walls and floors i.e. to generate a 3D CAD model of the room . <p> Yes , you are correct in your thinking . If you can segment the 3 balls from your image and you know their size , you can absolutely use this technique to compute @ @ @ @ @ @ @ @ @ @ this in next weeks blog post ) . You 'll also have to perform a calibration for the distance from ball to camera , but again , that 's easily doable . <p> Nice ! Thank you for this post , it is always a pleasure to read through your tutorials and learn something new . Alternatively to this method , if you are interested in calculating the area of an object instead of retrieving its dimensions , there is a handy contourArea function in openCV ! So once the contours are defined , you can easily calculate the area of each contour enclosure in pixels ( 1 line of code ) and convert to appropriate dimensions via a reference object area . This is especially useful for the blob-like objects I am working with which have poorly defined dimensions . <p> Hi . That 's a really nice post . I recently started programming , but i wanted to adapt this programm a bit . How can i change it that way , that my referenceobject is detectet by color ? I hope you can help me = <p> I think this @ @ @ @ @ @ @ @ @ @ installs it at the route " **38;2151;TOOLONG " instead of " site-packages " because i can see it exploring the directories but i do n't  really know if this is correct or no . <p> I tried to install the modules outside the virtual environment and inside it , previously entering the commands " source . /profiles " and " workon cv " . <p> Iam working in a raspbian jessie fresh install on a raspberry pi 3 and i followed your new openCV 3 installation tutorial without any error . <p> Please let me know how could i change the imutils/scipy installation directory or what other error i am committing . <p> The reason you are getting this error is because you 're using sudo and pip to install imutils and SciPy into the system install of Python . Instead , you should be installing them into the virtual environment : <p> You need to perform some sort of camera calibration . That can be done offline before the script is deployed ( provided you 're using the same camera sensors ) or you can include the reference object in the @ @ @ @ @ @ @ @ @ @ calibrated first . <p> Hi . Thanks for this really nice post . I recently started programming with python , but i wanted to adapt this programm a bit to my situation . I have a set of images that represent vehicles and I want to classify them in groups based on vehicle height . How can I do this knowing that I do n't  have this reference object in place ? <p> At some point , you need to calibrate your camera . You can do this " offline " before you take photos of your vehicles ( such as computing the reference object focal length and then removing the reference object ) . But at some point the camera has to be calibrated . <p> Hi Adrian , thanks for the advice and this tutorial . I managed to use this tutorial to calibrate my object tracking project . <p> Actually , the computer vision bundle helped me to improve my programming skills . And I want to take pyimagesearch gurus course but my problem is that I do not have a good programming background , so it @ @ @ @ @ @ @ @ @ @ I have been experimenting with different images and different objects within the image . I noticed that sometimes it does not detect some object , so to make it detect the object I varied the GaussainBlur() values from ( 7,7 ) to ( 11,11 ) . However , still when I do that it does not detect all the object in the image . <p> So my question is how can I ensure that I detect all of the objects with the image ? <p> As you noted , detecting all objects in your image is highly dependent on ( 1 ) the image , ( 2 ) the contents of the image , and ( 3 ) your lighting conditions/environment . Its hard to say which ( or even all 3 ) is contributing to the issue here . All I can say is that you should continue to play with the parameters but also keep in mind that for certain objects , you wont be able to detect them using basic image processing techniques . Some objects require machine learning and/or object detection . <p> First of all @ @ @ @ @ @ @ @ @ @ new at this and just started to work on measuring 3D-objects very recently . My ultimate goal is to be able to get X , Y and Z dimension from a picture taken by a RPi V2-type camera , but the first issue to overcome is how to calibrate the camera . I think the correct procedure would be to take several pictures from an object with known dimensions on different distances and count pixels to find the ratio formula , correct ? Any other suggestions ? <p> 3D is an entirely different world ( no pun intended ) . I honestly do n't  have much experience in 3D , my main area of expertise focuses on image search engines and image classification ( and even lately , deep learning ) . Essentially , you would need to create a 3D model of your object from 2D images . And at that point , you 'll be doing much more complex camera calibration anyway , which would lead to better accuracy . <p> I really find your post interesting ! I am doing a project where there is a conveyor on @ @ @ @ @ @ @ @ @ @ 3d camera mounted on top to recognize these boxes and further we need to determine the dimensions and pose of the boxes . <p> I have been studying and researching a lot on these topics and what algorithms to choose and how to proceed.To be brief i am unable to proceed next . <p> My task has the following constraints : <p> 1 ) There should be no database of models created from which one can perform feature matching to detect objects . <p> The algorithm used for object detection should directly detect the object and provide its pose . <p> 2 ) The shape of the object is always a cuboid/box shaped . The edges of the box might not be necessarily sharp . It can have rounded edges also . But the object to detected is always box shaped <p> 3 ) Also my camera would be mounted in an inclined angle ( not parallel to the object ) <p> Hey Ramanan , this sounds like a pretty neat project , although I do n't  think its a great idea to constrain yourself from feature-based approaches . All @ @ @ @ @ @ @ @ @ @ in 3D and stereo vision , so I 'm unfortunately not the right person to ask regarding this project . Best of luck ! <p> I def need help on combining several of these projects from pyimagesearch.com . Like skin finder , object detector , measuring , ect . Can you or anyone help me to get a project I 'm trying to work on started and working starting ASAP .. Please . Anyone . <p> Thank you so much for this great post ! I was having a problem while installing scipy on Rpi 3 . I am trying to install it into the virtual environment , however , that last thing that shows on the window " Runing setup.py bdistwheel for scipy / " and hangs there forever . <p> Yes , once you know the number of pixels per measuring metric you can compute the minimum bounding circle via cv2.minEnclosingCircle . This will give you the radius of the object which you can then use to derive the circumference . <p> Hi Adrian ! Ive been a silent reader of your blogs ! Personally , you have helped me @ @ @ @ @ @ @ @ @ @ now . I just want to know how to do this in real time ? Like measuring objects in a video ? <p> It is totally possible to measure the size of objects in a video stream , just wrap this code in a loop that continuously polls frames from a camera sensor . This blog post will help you get started . I also provide more examples of working with video streams inside Practical Python and OpenCV . <p> Hello , Thanks for the nice guide . I have a problem . I Have a camera . I want to take a photo from the camera and mark out an area on it using mouse . Then i want to calculate the area within the marked area . Can you please guide me how do I do it ? <p> In that case , I would modify the script to maintain a list of ( x , y ) -coordinates that you click . You can then loop over them and draw them with cv2.line . There is also a polygon drawing function in OpenCV but I cant remember @ @ @ @ @ @ @ @ @ @ . You can look it up in the OpenCV docs . <p> Hi Adrian , Hope you had a wonderful Christmas ! I have been following your tutorials , and I have learned quite a lot of things from implementing these projects . Now I have a question , would it be possible to determine the size of the object with the following conditions : 1 . Without a reference object 2 . We know the FOV 3 . We know the distance readings from a sensor . Would you be able to give me some directions ? <p> Thank you Adrian . Would you be able to give me some specifics on the parameters ? Heres what I thought I 'd do : I know the distance , real time measurements from a sensor . I know the F of the camera . I know the area occupied by the object in the image . Based on this , following your tutorial on finding the distance between the object and the camera , would it not be possible to find the size of the object ? <p> Note : the @ @ @ @ @ @ @ @ @ @ . There are no other objects in between or in the background . <p> Its not exactly that simple . I do n't  have any tutorials on this topic at the moment , but I 'll certainly add it to my queue and try to cover it in the future . I cant think of any good existing Python tutorials off the top of my head , but I know this MATLAB tutorial would be a good start for you to understand the basic principles . <p> Thank you Adrian , for the suggestion . I have one more question . Would it be possible to measure the size of the object , if I have a reference object with known size and distance , but in separate images ? So , here 's what I 'm looking at : I know the size , and the distance to a known object separate images ( following your tutorial , 3 images ) . I know the focal length and the actual distance from camera to the target object from a sensor , I 'm able to find the real-time distance . This looks doable @ @ @ @ @ @ @ @ @ @ Would you be able to throw some light on this issue ? Thank you = <p> I have implemented your example and modified it a little to work with live video . Now I 'm trying to get measurements on objects volume using only the webcam . Can you point me in some direction ? Thanks ! <p> P.S. : Adrian , your content rocks ! Thanks for the effort and congrats on the approaches . Always easy to learn from you . <p> I 'm working mainly with boxes and packages . Its for a logistics project . Its a challenge and Im restrained to webcams , though I can use other commonly available material . Of course I 'm expecting not-so-refined measures . <p> Personally , I do n't  think you 're going to get a decent measure of volume without using 2 cameras . If you can setup two webcams you could essentially create a stereo-vision environment by calibrating them and obtaining an approximate depth map . <p> I have another thought on the approach . If I crop the image to keep the coin part , and then use the @ @ @ @ @ @ @ @ @ @ , using the pixels and the known dimensions I would find the pixels per metric . Would that be a right approach ? <p> I have one template image where i have fixed size rectangle which contain some information . Now , i want to search that exact fixed size rectangle from query image then perform cropping . Query image may be in different DPI/resolution format.How can i deal with this issue ? Is this article help me ? Please suggest some idea for solving it . <p> Hi Adrian , I am new to computer vision and as the other commenters said many times I am absolutely grateful with the help you 're providing the community . <p> I noticed your comment regarding the need to aim squarely at the target to obtain a precise measurement . In my case by design my target will be up to 2 degree off . Is there a way to compensate for this ? <p> What I 'm trying to do is aim a laser at a ruler placed a few feet out and capture the laser position ( e.g. aiming at the 128mm @ @ @ @ @ @ @ @ @ @ is : <p> 1 ) to write code to detect this laser line and use ocr to interpret where its aiming <p> or 2 ) use the technique presented in this post and find a way to offset the 1-2 degree distortion . I know the ruler total length , could I measure distance to the left and right sides of the ruler to derive its angle and then use this info to compensate when measuring the target object length ( e.g. from the ruler zero mark to the laser line ) ? <p> I 'm shooting for 1 or 2 mm accuracy . Thanks in advance for pointing me in the right direction <p> The method in this blog post requires only simple camera calibration based on the output image . For your level of accuracy I would suggest instead computing the intrinsic properties of the camera and doing a much more accurate calibration . I do n't  have a tutorial on this at the moment , but I will try to do one in the future . In the meantime , take a look at this excellent resource on @ @ @ @ @ @ @ @ @ @ script does n't  animate for me like yours does . I just see the coins dimensions and none of the dimensions of the other items . I 'm running an Ubuntu VM on Mac . What would you recommend I do ? <p> I 'm not sure what you mean by " animate " as I created a GIF file to demonstrate the animation . Click on the active window and press any key to advance the detections . The call to cv2.waitKey pauses execution . <p> hello adrian .. its work very well in mine but if i wan na change the source image to webcam , what steps should i do so its well work in my raspi 3 ? i try change the source but its always failed .. thanks for the knowledges <p> Hello Adrian ; I want to detect seperate bottles from an image of 5 bottles and process on it that is detect neck and shoulder of bottle and find the centre point of the two and check whether the liquid is filled upto that point . Can you help ? <p> Hello Adrian ; Thanks @ @ @ @ @ @ @ @ @ @ great toturial .. I want to save the result in one image and result show me measurement for all objects in one image Plz help me Regards <p> Hi , can this method ( or similar ) be used to measure the length of a curved object ? Specifically an animal that is not perfectly straight ? I need to find a way to measure the nose to tail length , but the animal may be bent in the middle . <p> I 'm not familiar on the science/literature of measuring the length of animals , but I think it would have to be an approximation to the length , especially if the animal is free and moving . Otherwise , if you know the contour of the body , then you might be able to apply splines to determine the length , but that 's @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485104 @185104/ <p> Figure 1 : Classifying an image as whether it- contains a dog or a cat . <p> The question is : - " Can we do better ? " <p> Of course we can ! - Obtaining higher accuracy for nearly any machine learning algorithm boils down to tweaking various knobs and levels . <p> In the case of k-NN , we can tune- k , the number of nearest neighbors . We can also tune our distance metric/similarity function as well . <p> Of course , hyperparameter tuning has implications- outside of the k-NN algorithm as well . In the context of Deep Learning and Convolutional Neural Networks , we can easily have- hundreds of various hyperparameters to tune and play with ( although in practice we try to limit the number of variables to tune to a small handful ) , each affecting our overall classification to some ( potentially unknown ) degree . <p> Because of this , its important to understand the concept of- hyperparameter tuning and how your choice in hyperparameters can dramatically impact your classification accuracy . <h> How to tune hyperparameters @ @ @ @ @ @ @ @ @ @ tutorial , I 'll be demonstrating how to tune k-NN hyperparameters for the Dogs vs . Cats dataset . Well start with a discussion on- what hyperparameters are , followed by viewing a concrete example on- tuning k-NN hyperparameters . <p> As our results will demonstrate , we can improve- our classification accuracy from- 57.58% to over 64% ! <h> What are hyperparameters ? <p> Hyperparameters are simply the knobs and levels you pull and turn when building a machine learning classifier . The process of tuning hyperparameters is more formally called- hyperparameter optimization . <p> So what 's the difference between a normal " model parameter " and a " hyperparameter " ? <p> Well , a standard " model parameter " is normally an internal variable that is optimized in some fashion . In the context of Linear Regression , Logistic Regression , and Support Vector Machines , we would- think of parameters as the weight vector coefficients found by the learning algorithm . <p> On the other hand , " hyperparameters " are normally set by a human designer or tuned via algorithmic approaches . Examples of hyperparameters include @ @ @ @ @ @ @ @ @ @ , the learning rate- alpha of a Neural Network , or the number of filters learned in a given convolutional layer- in a CNN . <p> In general , model parameters are optimized according to some- loss function , while hyperparameters are instead- searched for by exploring various settings to see which values provided the highest level of accuracy . <h> k-NN hyperparameters <p> As a concrete example of tuning hyperparameters , let 's consider the k-Nearest Neighbor classification algorithm . For your standard k-NN implementation , there are- two primary hyperparameters- that you 'll want to tune : <p> The number of neighbors- k . <p> The distance metric/similarity function . <p> Both of these values can dramatically affect the accuracy of your k-NN classifier . To demonstrate this in the context of image classification , let 's apply hyperparameter tuning to our Kaggle Dogs vs . Cats dataset from last week . <p> Open up a new file , name it knntune.py- , and insert the following code : <p> --jobs- : The number of processors/cores to utilize when computing the nearest neighbors for a particular data point . Setting this @ @ @ @ @ @ @ @ @ @ . Again , for a more detailed review of these arguments , please refer to last weeks tutorial . <p> In order to train and evaluate our k-NN classifier , well need to partition our data- into two splits : a- training split and a- testing split : <p> How to tune hyperparameters with Python and scikit-learn <p> Python <p> 66 <p> 67 <p> 68 <p> 69 <p> 70 <p> # partition the data into training and testing splits , using 75% <p> # of the data for training and the remaining 25% for testing <p> print ( " INFO constructing training/testing split ... " ) <p> LONG ... <p> data , labels , testsize=0.25 , randomstate=42 ) <p> Here well be using 75% of our data for training and the remaining 25% for evaluation . <p> Finally , let 's define the set of hyperparameters we are going to optimize over : <p> How to tune hyperparameters with Python and scikit-learn <p> Python <p> 72 <p> 73 <p> 74 <p> # construct the set of hyperparameters to tune <p> params= " nneighbors " : np.arange(1,31,2) , <p> " metric " @ @ @ @ @ @ @ @ @ @ above code block defines a params- dictionary which contains two keys : <p> nneighbors- : The number of nearest neighbors- k in the k-NN algorithm . Here well search over the- odd integers in the range- 0 , 29 ( keep in mind that the np.arange- function is- exclusive ) . <p> metric- : This is the distance function/similarity metric for k-NN . Normally this defaults to the Euclidean distance , but we could also use any function that returns a single floating point value representing how " similar " two images are . In this case , well search over both the Euclidean distance and Manhattan/City block distance . <p> Now that we have defined the hyperparameters we want to search over , we need a method that actually- applies the search . Luckily , the scikit-learn library already has two methods that can perform hyperparameter search for us : - Grid Search and- Randomized Search . <p> As well find out , its normally preferable to used Randomized Search over Grid Search in nearly all circumstances . <h> Grid Search hyperparameters <p> The Grid Search tuning algorithm will @ @ @ @ @ @ @ @ @ @ learning classifier for- each and every combination of hyperparameter values . <p> In this case , given 16 unique values of- k and 2 unique values for our- distance metric , a Grid Search will apply- 30- different experiments to determine the optimal value . <p> You can see how a Grid Search is performed in the following code segment : <p> How to tune hyperparameters with <p> 88 <p> 89 <p> # tune the hyperparameters via a cross-validated grid search <p> print ( " INFO tuning hyperparameters via grid search " ) <p> LONG ... <p> grid=GridSearchCV ( model , params ) <p> start=time.time() <p> grid.fit ( trainData , trainLabels ) <p> # evaluate the best grid searched model on the testing data <p> print ( " INFO grid search took : .2f seconds " . format ( <p> time.time()-start ) ) <p> acc=grid.score ( testData , testLabels ) <p> print ( " INFO grid search accuracy : : .2f% @ @ @ @ @ @ @ @ @ @ search best parameters : " . format ( <p> grid.bestparams ) ) <p> The primary benefit of the Grid Search algorithm is also its major drawback : as an- exhaustive- search your number of possible parameter values- explodes as both the number of- hyperparameters and hyperparameter- values increases . <p> Sure , you get to evaluate each and every combination of hyperparameter - but you pay a cost- its a- very time consuming cost . And in most cases , - its hardly worth it . <p> As I explain in the- " Use Randomized Search for hyperparameter tuning ( in most situations ) " section below , there are- rarely just- one set of hyperparameters that obtain the highest accuracy . <p> Instead , there are " hot zones " of hyperparameters that all obtain near identical accuracy . The goal is to explore as many of these " zones " of hyperparameters a quickly as possible and locate one of these " hot zones " . It turns out that a random search is a great way to do this . <h> Randomized Search hyperparameters <p> The Random @ @ @ @ @ @ @ @ @ @ params- dictionary via a- random , uniform distribution. - Given a set of randomly sampled parameters , a model is then trained and evaluated . <p> We perform this set of random hyperparameter sampling and model construction/evaluation for a- preset number of times. - You set the number of evaluations to be- as long as you 're willing to wait . If you 're impatient and in a hurry , make this value low . And if you have the time to spend on a longer experiment , increase the number of iterations . <p> In either case , the goal of a Randomized Search is to explore a large set of possible hyperparameter spaces quickly and the best way to accomplish this is via simple random sampling. - And in practice , it works quite well ! <p> You can find the code to perform a Randomized Search of hyperparameters for the k-NN algorithm below : <p> How to tune hyperparameters with <p> 102 @ @ @ @ @ @ @ @ @ @ search <p> **29;2191;TOOLONG , params ) <p> start=time.time() <p> grid.fit ( trainData , trainLabels ) <p> # evaluate the best randomized searched model on the testing <p> # data <p> print ( " INFO randomized search took : .2f seconds " . format ( <p> time.time()-start ) ) <p> acc=grid.score ( testData , testLabels ) <p> print ( " INFO grid search accuracy : : .2f% " . format(acc*100) ) <p> print ( " INFO randomized search best parameters : " . format ( <p> grid.bestparams ) ) <h> Hyperparameter tuning with Python and scikit-learn results <p> To tune the hyperparameters of our k-NN algorithm , make sure you : <p> Download the source code to this tutorial using the- " Downloads " form at the bottom of this post . <p> As you can see from the output screenshot , the Grid Search method found that- k=25 and- metric=cityblock obtained the highest accuracy of 64.03%. - However , this Grid Search took 13 minutes . <p> On the other hand , the Randomized Search obtained an identical- accuracy of 64.03% - and it completed in- under 5 minutes . @ @ @ @ @ @ @ @ @ @ accuracy ( 64.03% accuracy , up from 57.58% from last weeks post ) but the Randomized Search was- much more efficient . <h> Use Randomized Search for hyperparameter tuning ( in most situations ) <p> Unless your search space is small and can easily be enumerated , a Randomized Search will tend to be more efficient and yield- better results faster . <p> As our experiments demonstrated , Randomized Search was able to obtain- 64.03% accuracy in &lt; 5 minutes while an exhaustive Grid Search took a much longer- 13 minutes to obtain an identical 64.03% accuracy- that 's a 202% increase in evaluation time for identical accuracy ! <p> In general , there is n't just- one set of hyperparameters that obtains optimal results instead , there are usually a- set of them that exist towards the bottom of a concave bowl ( i.e. , the optimization surface ) . <p> As long as you hit just- one of these parameters towards the bottom of the bowl , you 'll still obtain the same accuracy as if you enumerated- all possibilities along the bowl . Furthermore , you 'll be able to explore @ @ @ @ @ @ @ @ @ @ Search . <p> Overall , this will lead to faster , more efficient hyperparameter tunings in most situations . <h> Summary <p> In todays blog post , I demonstrated how to tune hyperparameters to machine learning algorithms using the Python programming language and the scikit-learn library . <p> First , I defined , the difference between standard " model parameters " and the " hyperparameters " that need to be tuned . <p> From there , we applied two methods to tune hyperparameters : <p> An exhaustive Grid Search <p> A Randomized Search <p> Both of these hyperparameter tuning routines were then applied to the k-NN algorithm and the Kaggle Dogs vs . Cats dataset . <p> Each respective tuning algorithm obtained identical accuracy - but the Randomized Search was able to obtain this increase of accuracy in a fraction of the time ! <p> In general , I- highly encourage you to use Randomized Search when tuning hyperparameters . You 'll often find that there is rarely just- one set of hyperparameters that obtains optimal accuracy . Instead , there are " hot zones " of hyperparameters that will obtain @ @ @ @ @ @ @ @ @ @ and try to land on one of these zones as fast as possible . <p> Given no- a priori knowledge of good hyperparameter choices , a Randomized Search to hyperparameter tuning is the most optimal way to find reasonable hyperparameter values in a short amount of time- as it allows you to explore many areas of the optimization surface . <p> Anyway , I hope you enjoyed this blog post ! Ill be back next week to discuss the basics of linear classification ( and the role it plays in Neural Networks and image classification ) . <p> But before you go , - be sure to signup for the PyImageSearch Newsletter using the form below to be notified when future blog posts are published ! @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485106 @185106/ <h> Tag Archives histogram of oriented gradients <p> Todays blog post is inspired from an email I received from Jason , a student at the University of Rochester . Jason is interested in building a custom object detector using the HOG + Linear SVM framework for his final year project . He understands the steps required to build the object detector well enough - but he is n't <p> Last week we discussed how to use OpenCV and Python to perform pedestrian detection . To accomplish this , we leveraged the built-in HOG + Linear SVM detector that OpenCV ships with , allowing us to detect people in images . However , one aspect of the HOG person detector we- did not discuss in detail is the detectMultiScale- function ; specifically , <p> I 've met a lot of amazing , uplifting- people over the years . My PhD advisor who helped get me through graduate school . My father who was always there for me as a kid and still is now . And- my girlfriend who has always been positive , helpful , and supportive ( even when I @ @ @ @ @ @ @ @ @ @ So in last weeks blog post we discovered how to construct an image pyramid . And in todays article we are going to extend that example and introduce the concept of a sliding window . Sliding windows play an integral role in object classification , as they allow us to localize exactly- " where " in an image an object resides . <p> It 's too damn cold up in Connecticut- so cold that I had to throw in the towel and escape for a bit . Last week I took a weekend trip down to Orlando , FL just to escape . And while the weather was n't perfect ( mid-60 degrees Fahrenheit , cloudy , and spotty rain , as you can see from the <p> I have issues - I cant stop thinking about object detection . You see , last night I was watching The Walking Dead- and instead of enjoying the zombie brutality , the forced- cannibalism , or the enthralling storyline , - all I wanted to do was build an object detection system to recognize zombies . Would it be very useful ? @ @ @ @ @ @ @ @ @ @ watch the Super Bowl this past weekend ? I did . Kind of . I spent Super Bowl Sunday ( which is practically a holiday in the United States ) at my favorite Indian bar . Pounding Kingfisher beers . Savoring a delicious dish of- Tandoori chicken all while hacking up a storm on my laptop and coding up some custom <p> Connecticut is cold . Very cold . Sometimes its hard to even get out of bed in the morning . And honestly , without the aide of copious amounts of pumpkin spice lattes and the beautiful sunrise over the crisp autumn leaves , I do n't  think I would leave my cozy bed . But I have work to do . And today <p> If you 've been paying attention to my Twitter account lately , youve probably noticed one or two teasers of what I 've been working on a Python framework/package to rapidly construct object detectors using Histogram of Oriented Gradients and Linear Support Vector Machines . Honestly , I really ca n't stand using the Haar @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485109 @185109/ <p> The LeNet architecture was first introduced by LeCun et al . in their 1998 paper , - Gradient-Based Learning Applied to Document Recognition. - As the name of the paper suggests , the authors implementation of LeNet was used primarily for OCR and character recognition in documents . <p> The LeNet architecture is- straightforward and- small , ( in terms of memory footprint ) , making it- perfect for teaching the basics of CNNs it can even run on the CPU ( if your system does not have a suitable GPU ) , making it a great " first CNN " . <p> However , if you- do have GPU support and can access your GPU via Keras , you will enjoy- extremely fast training times ( in the order of 3-10 seconds per epoch , depending on your GPU ) . <p> In the remainder of this post , - I 'll be demonstrating how to implement the LeNet Convolutional Neural Network architecture using Python and Keras . <p> From there , I 'll show you- how to train LeNet on the MNIST dataset for digit recognition . <p> @ @ @ @ @ @ @ @ @ @ , keep reading . <h> LeNet Convolutional Neural Network in Python <p> This tutorial will be- primarily- code oriented and- meant to help you get your feet wet- with Deep Learning and Convolutional Neural Networks . Because of this intention , I am- not going to spend- a lot of time discussing activation functions , pooling layers , or dense/fully-connected layers there will be- plenty of tutorials on the PyImageSearch blog in the future that will cover- each of these layer types/concepts- in lots of detail . <p> Again , this tutorial is meant to be your- first end-to-end example where you get to train a real-life CNN ( and see it in action ) . Well get to the gory details of activation functions , pooling layers , and fully-connected layers later in this series of posts ( although you should already know the basics of how convolution operations work ) ; but in the meantime , simply follow along , enjoy the lesson , and- learn how to implement your first Convolutional Neural Network with Python and Keras . <h> The MNIST dataset <p> Figure 1 : MNIST @ @ @ @ @ @ @ @ @ @ MNIST dataset before , either here on the PyImageSearch blog , or elsewhere in your studies . In either case , I 'll go ahead and quickly review the dataset to ensure you know- exactly what data were working with . <p> The MNIST dataset is arguably the most well-studied , most understood dataset in the computer vision and machine learning literature , making it an excellent " first dataset " to use on your deep learning journey . <p> Note : As well find out , its also quite easy to get &gt; 98% classification accuracy on this dataset with minimal training time , even on the CPU . <p> The goal of this dataset is to classify the handwritten digits 0-9 . Were given a total of 70,000 images , with ( normally ) 60,000 images used for training and 10,000 used for evaluation ; however , were free to split this data as we see fit . Common splits include the standard 60,000/10,000 , 75%/25% , and 66.6%/33.3% . Ill be using 2/3 of the data for training and 1/3 of the data for testing later in the @ @ @ @ @ @ @ @ @ @ 28 x 28- grayscale image ( examples from the MNIST dataset can be seen in the figure above ) . These grayscale pixel intensities are unsigned integers , with the values of the pixels falling in the range- 0 , 255. - All digits are placed on a- black background- with a light- foreground ( i.e. , the digit itself ) being- white and- various shades of gray . <p> Its worth noting that many libraries ( such as scikit-learn ) have built-in helper methods to download the MNIST dataset , cache it locally to disk , and then load it . These helper methods normally represent each image as a- 784-d vector . <p> Where does the number 784 come from ? <p> Simple . Its just the- flattened- 28 x 28 = 784 image . <p> To recover our original image from the 784-d vector , we simply reshape the array into a- 28 x 28- image . <p> In the context of this blog post , our goal is to train LeNet such that we maximize accuracy on our testing set . <h> The LeNet architecture <p> @ @ @ @ @ @ @ @ @ @ of convolutional , activation , and pooling layers , followed by a fully-connected layer , activation , another fully-connected , and finally a softmax classifier ( image source ) . <p> The LeNet architecture is an excellent " first architecture " for Convolutional Neural Networks ( especially when trained on the MNIST dataset , an image dataset for handwritten digit recognition ) . <p> LeNet is small and easy to understand yet large enough to provide interesting results . Furthermore , the combination of LeNet + MNIST is able to run on the CPU , making it easy- for beginners to take their first step in Deep Learning and Convolutional Neural Networks . <p> In many ways , LeNet + MNIST is the " Hello , World " equivalent of Deep Learning for image classification . <p> The LeNet architecture consists of the following layers : <p> LeNet - Convolutional Neural Network in Python <p> Shell <p> 1 <p> LONG ... <p> Instead of explaining the number of convolution filters per layer , the size of the filters themselves , and the number of fully-connected nodes right now , I 'm @ @ @ @ @ @ @ @ @ @ with Python and Keras " - section of the blog post where the source code will serve as an aid in the explantation . <p> In the meantime , let 's took at our project structure a structure that we are going to- reuse many times in future PyImageSearch blog posts . <p> Note : The original LeNet- architecture- used TANH- activation functions rather than RELU- . The reason we use RELU- here is because it tends to give much better classification accuracy due to a number of nice , desirable properties ( which I 'll discuss in a future blog post ) . If you run into any other discussions on LeNet , you might see that they use TANH- instead again , just something to keep in mind . <h> Our CNN project structure <p> Before we dive into any code , let 's first review our project structure : <p> LeNet - <p> 8 <p> 9 <p> ---output <p> ---pyimagesearch <p> ---init.py <p> ---cnn <p> ---init.py <p> ---networks <p> @ @ @ @ @ @ @ @ @ @ organized , well define a package named pyimagesearch- . And within the pyimagesearch- module , well create a cnn- sub-module this is where well store our Convolutional Neural Network implementations , along with any helper utilities related to CNNs . <p> Taking a look inside cnn- , you 'll see the networks- sub-module : this is where the- network implementations themselves will be stored . As the name suggests , the lenet.py- file will define a class named LeNet- , which is our actual LeNet implementation in Python + Keras . <p> The lenetmnist.py- script will be our driver program used to instantiate the LeNet network architecture , train the model ( or load the model , if our network is pre-trained ) , and then evaluate the network performance on the MNIST dataset . <p> Finally , the output- directory will store our LeNet- model after it has been trained , allowing us to classify digits in subsequent calls to lenetmnist.py- without having to re-train the network . <p> I personally have been using this project structure ( or a project structure very similar to it ) over the past @ @ @ @ @ @ @ @ @ @ extend this will become more evident in future blog posts as we add to this library with more network architectures and helper functions . <p> The LeNet- class is defined on- Line 9 , followed by the build- method on- Line 11 . Whenever I define a new network architecture , I- always place it in its own class ( mainly for namespace and organization purposes ) followed by creating a- staticbuild- function . <p> The build- method , as the name suggests , takes any supplied parameters , which at a- bare minimum- include : <p> The- width of our input images . <p> The- height of our input images . <p> The- depth ( i.e. , number of channels ) of our input images . <p> And the number of- classes ( i.e. , unique number of class labels ) in our dataset . <p> I also normally include a weightsPath- which can be used to load a pre-trained model . Given these parameters , the build- function is responsible for constructing the network architecture . <p> Speaking of building the LeNet architecture , - Line 13 instantiates @ @ @ @ @ @ @ @ @ @ . <p> Now that the model has been initialized , we can start adding layers to it : <p> LeNet - Convolutional Neural Network in Python <p> Python <p> 15 <p> 16 <p> 17 <p> 18 <p> 19 <p> # first set of CONV =&gt; RELU =&gt; POOL <p> LONG ... <p> inputshape= ( depth , height , width ) ) ) <p> model.add ( Activation ( " relu " ) ) <p> LONG ... <p> On- Lines 15-19 we create our first set of CONV=&gt;RELU=&gt;POOL- layer sets . <p> Our CONV- layer will learn 20 convolution filters , where each filter is of size- 5 x 5 . The input dimensions of this value are the same width , height , and depth as our input images in this case , the MNIST dataset , so well have- 28 x 28 inputs with a single channel for depth ( grayscale ) . <p> Well then apply the ReLU activation function followed by- 2 x 2- max-pooling in both the- x and- y direction with a stride of 2 ( imagine a 2 x 2sliding window that " slides " @ @ @ @ @ @ @ @ @ @ each region , while taking a step of 2 pixels in both the horizontal and vertical direction ) . <p> Note : - This tutorial is- primarily- code based and is meant to be your- first exposure to implementing a Convolutional Neural Network I 'll be going into- lots more- detail regarding convolutional layers , activation functions , and max-pooling layers in future blog posts . In the meantime , simply try to follow along with the code . <p> We are now ready to apply our second set of CONV=&gt;RELU=&gt;POOL- layers : <p> LeNet - Convolutional Neural Network in Python <p> Python <p> 21 <p> 22 <p> 23 <p> 24 <p> # second set of CONV =&gt; RELU =&gt; POOL <p> LONG ... <p> model.add ( Activation ( " relu " ) ) <p> LONG ... <p> This time well be learning- 50 convolutional filters rather than the- 20 convolutional filters as in the previous layer set . <p> Its common to see the number of CONV- filters learned increase in deeper layers of the network . <p> Next , we come to the fully-connected layers ( often called " @ @ @ @ @ @ @ @ @ @ LeNet - Convolutional Neural Network in Python <p> Python <p> 26 <p> 27 <p> 28 <p> 29 <p> 30 <p> 31 <p> 32 <p> 33 <p> # set of FC =&gt; RELU layers <p> model.add ( Flatten ( ) ) <p> model.add ( Dense ( 500 ) ) <p> model.add ( Activation ( " relu " ) ) <p> # softmax classifier <p> model.add ( Dense ( classes ) ) <p> model.add ( Activation ( " softmax " ) ) <p> On- Line 27 we take the output of the preceding- MaxPooling2D- layer and- flatten it into a single vector , allowing us to apply dense/fully-connected layers . If you have any prior experience with neural networks , then you 'll know that a dense/fully-connected layer is a " standard " type of layer in a network , where every- node in the preceding layer connects to every node in the next layer ( hence the term , " fully-connected " ) . <p> Line 32 is- very important , although its easy to overlook this line defines another Dense- class , but accepts a variable ( i.e. , not hardcoded @ @ @ @ @ @ @ @ @ @ labels- represented by the classes- variable . In the case of the MNIST dataset , we have 10 classes ( one for each of the ten digits we are trying to learn to recognize ) . <p> Finally , we apply a softmax classifier ( multinomial logistic regression ) that will return a- list of probabilities , one for each of the 10 class labels ( Line 33 ) . The class label with the largest probability will be chosen as the final classification from the network . <p> Our last code block handles loading a pre-existing weightsPath- ( if such a file exists ) and returning the constructed model to the calling function : <p> LeNet - Convolutional Neural Network in Python <p> Python <p> 35 <p> 36 <p> 37 <p> 38 <p> 39 <p> 40 <p> 41 <p> # if a weights path is supplied ( inicating that the model was <p> # pre-trained ) , then load the weights <p> ifweightsPath isnotNone : <p> **30;2222;TOOLONG <p> # return the constructed network architecture <p> returnmodel <h> Creating the LeNet driver script <p> Now that we have implemented the @ @ @ @ @ @ @ @ @ @ its time to define the lenetmnist.py- driver script which will handle : <p> Loading the MNIST dataset . <p> Partitioning MNIST into- training and- testing splits . <p> Loading and compiling the LeNet architecture . <p> Training the network . <p> Optionally saving the serialized network weights to disk so that it can be reused ( without having to re-train the network ) . <p> Displaying- visual examples of the network output to demonstrate that our implementaiton is indeed working properly . <p> Note : If you 're following along with this blog post and intend on executing the code , - please use the " Downloads " section at the bottom of this post . In order to keep this post shorter and concise , I 've left out the init.py- updates which might throw off newer Python developers . <p> From there , - Lines 12-19 parse three optional command line arguments , each of which are detailed below : <p> --save-model- : An indicator variable , used to specify whether or not we should- save our model to disk after training LeNet . <p> --load-model- : Another indicator variable @ @ @ @ @ @ @ @ @ @ a pre-trained model from disk . <p> --weights- : In the case that --save-model- is supplied , the --weights-path- should point to where we want to- save the serialized model . And in the case that --load-model- is supplied , the --weights- should point to where the pre-existing weights file lives on our system . <p> We are now ready to load the MNIST dataset and partition it into our training and testing splits : <p> LeNet - Convolutional Neural <p> 32 <p> 33 <p> # grab the MNIST dataset ( if this is your first time running this <p> # script , the download may take a minute -- the 55MB MNIST dataset <p> # will be downloaded ) <p> print ( " INFO downloading MNIST ... " ) <p> **29;2254;TOOLONG " MNIST Original " ) <p> # reshape the MNIST dataset from a flat list of 784-dim vectors , to <p> # 28 x 28 pixel images , then scale the @ @ @ @ @ @ @ @ @ @ construct the training and testing splits <p> LONG ... <p> data=data : , np.newaxis , : , : <p> LONG ... <p> LONG ... <p> Line 25 loads the MNIST dataset from disk . If this is your first time calling the fetchmldata- function with the " MNIST Original " - string , then the MNIST dataset will need to be downloaded . The MNIST dataset is a 55MB file , so depending on your internet connection , this download may take anywhere from a couple seconds to a few minutes . <p> After the MNIST dataset has been downloaded , we reshape the data- from a set of- 784-d feature vectors ( i.e. , the raw pixel intensities ) to- 28 x 28 grayscale images that we can pass through the network ( Line 30 ) . <p> Our data- matrix now has the shape ( 70000,28,28 ) - ; however , - there is a problem Keras assumes we are going to supply- at least 1 channel per image , thus we need to add in an extra dimension to the data- array ( Line 31 ) . @ @ @ @ @ @ @ @ @ @ data- matrix will be : ( 70000,1,28,28 ) - and is now suitable for passing through our LeNet architecture . <p> Finally , - Lines 32-33 perform a training and testing split , using 2/3 of the data for training and the remaining 1/3 for testing . We also reduce our images from the range- 0 , 255 to- 0 , 1.0 , a common- scaling technique . <p> The next step is to process our labels so they can be used with the categorical cross-entropy loss function : <p> LeNet - Convolutional Neural Network in Python <p> Python <p> 35 <p> 36 <p> 37 <p> 38 <p> 39 <p> 40 <p> # transform the training and testing labels into vectors in the <p> # range 0 , classes -- this generates a vector for each label , <p> # where the index of the label is set to 1 and all other entries <p> # to 0 ; in the case of MNIST , there are 10 class labels <p> LONG ... <p> LONG ... <p> Lines 39 and 40 handle processing our training and testing labels ( i.e. @ @ @ @ @ @ @ @ @ @ the MNIST dataset ) . <p> Since we are using the categorical cross-entropy loss function , we need to apply the tocategorical- function which converts our labels from- integers to a- vector , where each vector ranges from 0 , classes- . - This function generates a vector for- each class label , where the index of the correct label is set to- 1 and all other entries are set to- 0 . <p> In the case of the MNIST dataset , we have 10 lass labels , therefore each label is now represented as a- 10-d vector . As an example , consider the training label- " 3 " . After applying the tocategorical- function , our vector would now look like : <p> LeNet - Convolutional Neural Network in Python <p> Shell <p> 1 <p> 0,0,0,1,0,0,0,0,0,0 <p> Notice how all entries in the vector are zero- except for the third index which is now set to one . <p> We are now ready to build our LeNet- architecture , optionally load any pre-trained weights from disk , and then train our network : <p> LeNet - Convolutional Neural <p> 60 <p> 61 <p> # initialize the optimizer and model <p> print ( " INFO compiling model ... " ) <p> opt=SGD(lr=0.01) <p> LONG ... <p> LONG ... <p> LONG ... <p> metrics= " accuracy " ) <p> # only train and evaluate the model if we *are not* loading a <p> # pre-existing model <p> ifargs " loadmodel " &lt;0 : <p> print ( " INFO training ... " ) <p> LONG ... <p> verbose=1 ) <p> # show the accuracy on the testing set <p> print ( " INFO evaluating ... " ) <p> LONG ... <p> batchsize=128 , verbose=1 ) <p> print ( " INFO accuracy : : .2f% " . format(accuracy*100) ) <p> Well be training our network using Stochastic Gradient Descent ( SGD ) - with a learning rate of 0.01- . Categorical cross-entropy will be used as our loss function , a fairly standard choice when working with datasets that have more @ @ @ @ @ @ @ @ @ @ and loaded into memory on- Lines 45-48 . <p> In the case that --load-model- is not supplied , we need to train our network ( Line 52 ) . <p> Training our network is- accomplished by making a call to the . fit- method of the instantiated model- ( Lines 54 and 55 ) . Well allow our network to train for- 20 epochs ( indicating that our network will " see " each of the training examples a total of 20 times to learn distinguishing filters for each digit class ) . <p> We then evaluate our network on the testing data ( Lines 59-61 ) and display the results to our terminal . <p> Next , we make a check to see if we should serialize the network weights to file , allowing us to run the lenetmnist.py- script- subsequent times without having to re-train the network from scratch : <p> LeNet - Convolutional Neural Network in Python <p> Python <p> 63 <p> 64 <p> 65 <p> 66 <p> # check to see if the model should be saved to file <p> ifargs " savemodel " &gt;0 : @ @ @ @ @ @ @ @ @ @ " ) <p> LONG ... <p> Our last code block handles randomly selecting a few digits from our testing set and then passing them through our trained LeNet network for classification : <p> LeNet - Convolutional Neural <p> 85 <p> 86 <p> # randomly select a few testing digits <p> LONG ... <p> # classify the digit <p> **38;2285;TOOLONG , i ) <p> **31;2325;TOOLONG <p> # resize the image from a 28 x 28 image to a 96 x 96 image so we <p> # can better see it <p> image= ( testDatai0*255 ) . astype ( " uint8 " ) <p> image=cv2.merge(image*3) <p> LONG ... <p> cv2.putText ( image , str(prediction0) , ( 5,20 ) , <p> **27;2358;TOOLONG , ( 0,255,0 ) , 2 ) <p> # show the image and prediction <p> print ( " INFO Predicted : , Actual : " . format ( prediction0 , <p> np.argmax(testLabelsi) @ @ @ @ @ @ @ @ @ @ ) 55212 @qwx675212 <p> For each of the randomly selected digits , we classify the image using our LeNet model ( Line 71 ) . <p> The actual prediction- - of our network is obtained by finding the index of the class label with the- largest probability . Remember , our network will return a set of probabilities via the softmax function , one for each class label the actual " prediction " of the network is therefore the class label with the largest probability . <h> Training LeNet with Python and Keras <p> To train LeNet on the MNIST dataset , make sure you have downloaded the source code using the- " Downloads " - form found at the bottom of this tutorial . This . zip- file contains all the code I have detailed in this tutorial furthermore , this code is organized in the- same project structure that I detailed above , which- ensures it will run properly on your system ( provided you have your environment configured properly ) . <p> After downloading the . zip- code archive , you can train LeNet on MNIST by @ @ @ @ @ @ @ @ @ @ LeNet on the MNIST dataset on my Titan X takes approximately 3 seconds per epoch . After 20 epochs , LeNet is reaching 98.90% classification accuracy on the training data and 98.49% accuracy on the testing data . <p> On my Titan X GPU , it takes approximately 3 seconds per epoch , allowing the- entire training process to finish in approximately 60 seconds . <p> After only 20 epochs , LeNet is reaching- 98.49% classification accuracy on the MNIST dataset - not bad at all for only 60 seconds of computation time ! <p> Note : If you execute the lenetmnist.py- script on our CPU rather than GPU , expect the per-epoch time to jump to 70-90- seconds . Its still possible to train LeNet on your CPU , it will just take a little while longer . <h> Evaluating LeNet with Python and Keras <p> Below I have included a few example evaluation images from our LeNet + MNIST implementation : <p> Figure 4 : Applying LeNet to classify a digit from the MNIST dataset . <p> In the above image , we are able to correctly classify @ @ @ @ @ @ @ @ @ @ this image , LeNet correctly recognizes the digit as a- " 2 " : <p> Figure 5 : Implementing LeNet in Python and Keras . <p> The image below is a great example of the robust , discriminating nature of convolution filters learned by CNN filters : - This- " 6 " is quite contorted , leaving little-to-no gap between the circular region of the digit , but LeNet is still able to correctly classify the digit : <h> Summary <p> In todays blog post , I demonstrated how to implement the LeNet architecture using the Python programming language and the Keras library for deep learning . <p> The LeNet architecture is a great- " Hello , World " network to get your feet wet with deep learning and Convolutional Neural Networks . The network itself is simple , has a small memory footprint , and when applied to the MNIST dataset , can be run on either your CPU or GPU , making it ideal for experimenting and learning , especially if you 're a deep learning newcomer . <p> This tutorial was primarily- code focused , and because of @ @ @ @ @ @ @ @ @ @ important Convolutional Neural Network concepts- such as activation layers , pooling layers , and dense/fully-connected layers ( otherwise this post could have- easily been 5x as long ) . <p> In future blog- posts , I 'll be reviewing- each of these layer types in lots of detail in the meantime , simply familiarize yourself with the code and try executing it yourself . And if you 're feeling- really daring , try tweaking the number of filters and filter sizes per convolutional layer and see what happens ! <p> Anyway , I hope you 've enjoyed this blog post I 'll certainly be doing more deep learning and image classification posts in the future . <p> But before you go , be sure to enter your email address in the form below to be- notified- when future- PyImageSearch blog posts are published you wo n't want to miss them ! <h> Downloads : 55217 @qwx675217 <h> 84 Responses to LeNet Convolutional Neural Network in Python <p> Incredible ! Thank you so much . After learning from your very clear examples , I trained LeNet on over a hundred thousand sample images of over 2700 @ @ @ @ @ @ @ @ @ @ thirty thousand test data images . Thank you again ! <p> There are many different ways to perform object localization using deep learning . Yes , you could apply a sliding window like HOG does . However , this is very slow . Instead , you might be interested in RCNNs and Faster R-CNNs where the goal is to integrate both object classification and localization into the same framework . Personally , I really like the You Only Look Once approach to object detection . <p> That really depends entirely on your dataset and your application . In general , I would recommend 500-1,000 images per class ( the more the better ) to train an CNN from start to finish . The more classes you have , the more images you 'll want per class as well . You can get away with less images by using a pre-trained CNN for feature extraction or applying data augmentation + fine-tuning . For what its worth , I cover Deep Learning + CNNs in more detail inside the PyImageSearch Gurus course . <p> Hey Adrian , great post ! I tested out @ @ @ @ @ @ @ @ @ @ . Do you think that a raspberry pi 2 would be capable of handling this program , and similar deep learning modules as well ? <p> Since LeNet does not require much memory , this would work fine on the Pi . However , for larger network architectures such as GoogLeNet , VGG , ResNet , etc. , the Pi wont have enough memory to run state-of-the-art networks . But for small , simple networks , you can use the Pi just keep in mind it wont be super fast . You should also train your networks on on a faster GPU machine and then deploy only the network weights for classification to your Pi . <p> Adrian : The fun level is high ! I really like your teaching technique of first showing us how to make something new and interesting that works decently , right away , with just the skeleton of the ideas . You then later follow up with additional theoretical explanations and variations so that something even better can be made if we want it . Detail , justification , expansion , alternatives etc , @ @ @ @ @ @ @ @ @ @ context a fun , working program is already established . <p> Suggestions to all . Of course not everything is presented in a concise project description . That 's what concise is of course . That said , a major idea or two for nascent machine learning developers : <p> This code by Adrian ( thank you Adrian ! ) does no evaluation using training data , i.e. , the evaluate function call . But if you do add it , I promise that you will get additional crucial information to allow you to more smartly debug the model better . Because if you do it , you can then be able to know if bias is the reason for low accuracy , versus variance being the issue for low accuracy . <p> Suggest all you modelers should first develop a model which attains low bias on training set , ie , which you trained model on . Please ignore these non-training-data predictions ( you called it " test " data , Adrian ) at first , because you must get your model to a sufficiently high capacity and complexity at @ @ @ @ @ @ @ @ @ @ , more layers , more and better features a more flexible model is needed . <p> Go and go until your accuracy of prediction is as good as you can get it on training predictions . Did you get low error finally ? Did you find greater capacity by adding more and better features and model capacity like layers and units ? That 's great and necessary but you are not done yet . <p> So now you finally get low bias on training dataset predictions . Now you are ready to begin evaluating the accuracy of predictions on a dataset you did not train on , call it " V " dataset ( you called it " test " but its a misnomer so I wont call it " test " ) . Your model has a low bias as you already determined . Check its predictions on " V " dataset . If your accuracy with this model is too low on " V " then you have a variance problem . Or no variance problem and you are done ! great job ! bias and variance are both @ @ @ @ @ @ @ @ @ @ need a more data examples , or more rigid model , or more regularization like noise injection , L1 reg , L2 reg , dropout , etc . Do not waste your time tuning all the hyperparameters or adding more units , more layers , more features , as these are addressing bias and capacity , not variance at all . You need to distinguish the options , and correctly use those , which affect model capacity versus those which affect model variance . You could waste 3-6 months of your life getting more data and adding more layers and running computer time , when all along it was the bias not the variance the whole time , so you need to know which is happening . <p> Bottom line is a model developer must be smart and know what improvements to try next to get a high accuracy model on new data . The way to get it is for the training data set to be also used for predictions . <p> Second major point is the code above did not give a truly new dataset error estimate , @ @ @ @ @ @ @ @ @ @ is new to you and to your model , after you did the recommended tweaks and improvements to it . To predict what your final models error will be , as best you can ( still imperfect tho ) you need the new data set partition where nothing was ever used during the above bias and variance work . You really need this third as-yet-unseen data partition to get something closer to a realistic error assessment . You really must not use the " V " dataset nor the training dataset for final error estimation , as these both will be already baked into your final model . <p> It proves to be super challenging and nuanced for students and even practitioners , and I am not the best teacher admittedly . I hope all will take time to think hard and check into these things yourselves . It will save you time when building if it becomes clear to you . <p> Andrew Ng is the best I know to explain this particular thing , so please go watch Ngs free videos on this topic of deciding what to @ @ @ @ @ @ @ @ @ @ made a great free set of videos about it . You will save time and money by understanding bias improvement versus variance improvement , flexibil models and rigid models , as soon as you can . <p> Thanks for writing the mnist example program here . I noticed it loads the data in one line . <p> What if we had instead a couple of jpg files instead . How would we actually load them into keras to train on . Even just 3 or 4 jpg files so we can get the syntax would be very helpful . The web is replete with deep learning examples of loading mnist from a convenient single file . Or we see how to load weights that someone else made for us , again in just a one-liner of code . But loading a monolithic data file is rarely what we do in a realistic program which must train a new model we are making . <p> Adrian I 'm hoping you ( or I ) find and share with each other , and with everyone , a working example of how to train @ @ @ @ @ @ @ @ @ @ A bunch of jpg files on disk , rather than a singleton file that somehow contains a representation of lots of image files that someone else preprepared which is so convenient for toy programs . Its been driving me batty . You might be the best person to show us how to get going here ! You are the best ! I love your teaching materials ! <p> Ill be doing more detailed , advanced tutorials that demonstrate how to build custom neural networks and deep learning models from scratch . However , in the meantime , just keep in mind that your training data is just a list of images . Therefore , just load your images into a single NumPy array and pass them through your network . It gets a little more complicated once all your images do n't  fit into main memory , but again , that 's a topic for a separate blog post . <p> Hi Adrian , Thank you for your amazing tutorial . However , I have a big issue . Training is so slow on my computer . I run it on @ @ @ @ @ @ @ @ @ @ tested it both on windows 7 x64 ( Anaconda ) and Ubuntu x64 14.04 LTS . I expected your estimation of 70 seconds . Do you have any idea what is wrong ? <p> 6,000 seconds per epoch sounds extremely slow . In my experience , you should be seeing 120 seconds max per epoch on the CPU . I would look at your install and see if you forgot to install BLAS or LAPACK , common linear algebra optimization libraries . That is my best guess regarding the issue . <p> I was curious about the digits which were classified incorrectly so I replaced your code that displays a few random results with a loop over all the test images and displayed the ones that were wrong . Some of the digits I could n't identify correctly either . The model seems to have big trouble with 3 vs 8 and 1 vs 7 . Would more training with the digits 1 , 3 , 7 and 8 fix this ? <p> More training data is almost always helpful . However , in this case I would instead use ensembles @ @ @ @ @ @ @ @ @ @ average the results . The accuracy will likely increase . <p> That is quite strange , but it sounds like the error message is directly related to using your GPU . I presume the issue could be related to the Keras version . Which version of Keras are you utilizing ? And did you use use this tutorial to install Keras ? <p> I have got the exact same error . Exception : The shape of the input to " Flatten " is not fully defined ( got ( 0 , 7 , 50 ) . Make sure to pass a complete " inputshape " or " batchinputshape " argument to the first layer in your model . <p> Got the same error . The problem was , that i ( once upon a time ) installed tensorflow , and while keras was configured , the keras.json used the wrong imagedimordering . Check your . keras/keras.json , and if " imagedimordering " is set to " tf " , just change it to " th " Found the solution here : https : **39;2387;TOOLONG <p> Thank you so much for @ @ @ @ @ @ @ @ @ @ a charm ! I get 5s on training one set of Epoch on my platform , system configuration : Skylake i7-6700 , 8G RAM , 500G HD , nVIDIA 950GTX , Ubuntu 14.04 64bit <p> If you are using the LeNet architecture then each digit should be 28+28 pixels and single channel ( grayscale ) . The most important aspect is pre-processing the digit . You would need to preserve the aspect ratio of the image and not resize the image to 28+28 pixels ignoring the aspect ratio . The imutils.resize function will help you with this . You can then pad the image to be the final 28+28 pixels . <p> This network also assumes that your digits are white on a black background . The same should be true for your digits as well . <p> Overall , MNIST is a very " clean " dataset and is used for machine learning benchmarks . Its not exactly representative of what you will find in the real world . I 'm not sure what dataset you are using on the types of images you are passing through the network but @ @ @ @ @ @ @ @ @ @ then you should consider training LeNet on your own example images . <p> Hey Keila I have not worked with . mha files before , unfortunately . In order to work with them in the context of CNNs or OpenCV you will need to convert them to a NumPy array at some point . I would focus your research efforts on how to do that . <p> The CNN demonstrated in this blog post was trained to recognize digits , but yes , you could also train it to recognize other objects provided you have enough images . I would instead suggest you take a look at pre-trained CNNs . <p> The call to cv2.waitKey(0) pauses the execution of the script so you can see the image displayed to your screen ; otherwise , cv2.imshow would automatically close . To stop the execution of the script , click on the image window and press any key . <p> Thank you for your article . Could you please help me to calculate the numbers of weights and outputs of each layer ( Conv1 , Conv2 , dense1 , dense2 ) ? @ @ @ @ @ @ @ @ @ @ 1 . For Conv1 : 20x5x5=500 weights ( why the output file only has 20+5=100 weights ) 20x24x24 =11250 outputs ( pixels ) ? . 24+24 because of 28+28 input and 5+5 filter size . After passing pooling layer 1 : there are 20x12x12 pixels ? 2 . For Conv2 : 50x5x5 =1250 weights ( why there are only 50+20=1000 ) 50x20x8x8 outputs ? ? After passing pooling layer 2 : there are 50x20x4x4 outputs ? ? 3. dense1 : why ? did you choose 500 neurons , the number of weights in the output file is 500+2450 ? <p> I have a problem with RAM . I 'm running Keras with Theano on CPU ( in Anaconda , Win7x64 , 8GB RAM ) and the program runs fine but i 'm only able to make one epoch that uses 6GB of RAM . It grows linearly when it starts training . Possible solutions or any reasons why ? At least 1 epoch gives 86% accuracy . <p> Twenty filters was used for the first CONV layer because that is what Yann LeCun used in his original LeNet paper ( since we @ @ @ @ @ @ @ @ @ @ to do with the number of filters , the stride simply reduces the spatial dimensions of the input volume . <p> 2 . I had to change the ordering of the image dimension in a few places in the code . I kept getting errors , and I tried different things that people suggested , but everything that I tried resulted into another error , here is the thing that finally worked for me : <p> In lenet.py : 1 . In line 17 changed the input shape to inputshape= ( width , height , depth ) ( apparently this is the image ordering for tensorflow ) 2 . In line 19 , I added an extra argument to the MaxPooling2D which is dimordering= " tf " ( takes the image ordering in the tensorflow standard ) . I did the same thing for the MaxPooling2D in line 24 . <p> Another error that I got was not recognizing LeNet in the second line of lenetmnist.py . To fix this I had to change the name of lenet.py into LeNet.py . Also , I had to change line 45 into @ @ @ @ @ @ @ @ @ @ be recognized . Any idea why this happened ? <p> I run this with python 2.7.13 ( anaconda 1.6.3 ) , tensorflow 1.2.1 , keras 2.0.5 <p> Thanks for sharing . This code was originally intended for a Theano backend , hence the " channels first " ordering rather than " channels last " . Ill be discussing more about this inside Deep Learning for Computer Vision with Python . <p> As for your second issue , it sounds like your init.py files do n't  have the correct imports . Make sure you use the " Downloads " section of this blog post to download the source code and compare it to mine . <h> Trackbacks/Pingbacks <p> that we 've had a taste of Deep Learning and Convolutional Neural Networks in last weeks blog post on LeNet , were going to take a step back and start to study machine learning in the context of image <p> seen how Convolutional Neural Networks ( CNNs ) such as LetNet can be used to classify handwritten digits from the MNIST dataset . We 've applied the k-NN algorithm to classify @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485111 @185111/ <h> Tag Archives opencv <p> 4:18am . Alarm blaring . Still dark outside . The bed is warm . And the floor will feel so cold on my bare feet . But I got out of bed . I braved the morning , and I took the ice cold floor on my feet like a champ . Why ? Because Im excited . Excited to share something very special with <p> I have some big news to announce today Besides writing a ton of blog posts about computer vision , image processing , and image search engines , I 've been behind the scenes , working on a second book . And you may be thinking , hey , did n't  you just finish up Practical Python and OpenCV ? Yep . I did . Now , do n't  get <p> A few months ago , I was teaching an online seminar on the basics of computer vision . And do you know what the most common question I got asked was ? How do I use OpenCV to load an image and display it on my screen ? Its @ @ @ @ @ @ @ @ @ @ ( myself included ) <p> Take a second to look at the Jurassic Park movie poster above . What are the dominant- colors ? ( i.e. the colors that are represented most in the image ) Well , we see that the background is largely black . There is some red- around the T-Rex . And there is some yellow- surrounding the actual logo . Its pretty simple for the human <p> Here we are , the final step of building a real-life Pokedex in Python and OpenCV . This is where it all comes together . Well glue all our pieces together and put together an image search engine based on shape features . We explored what it takes to build a Pokedex using computer vision . Then we- scraped the web <p> Were not even halfway through 2014 yet , but there have been some really amazing Python books released this year that have not been getting much attention . Some of these books are related to computer vision , some to machine learning and statistical analysis , and others to parallel computing . While not all- of @ @ @ @ @ @ @ @ @ @ finishing up our real-life Pokedex ! In my previous blog post , I showed you how to find a Game Boy screen in an image using Python and OpenCV . This post will show you how to apply warping transformations to obtain a " birds-eye-view " of the Game Boy screen . From there , we will be <p> So , how is our Pokedex going to " know " what Pokemon is in an image ? How are we going to describe each Pokemon ? Are we going to characterize the color of the Pokemon ? The texture ? Or the shape ? Well , do you remember playing Whos that Pokemon as a kid ? You were able to identify the Pokemon <p> Ive been throwing around the idea of writing a book for the past few months , but I was n't sure on what the exact focus was going to be . I 've given it a ton of thought , worked out the specifics , and spoke with a lot of developers , programmers , and researchers like yourselves . All of this work <p> Are you @ @ @ @ @ @ @ @ @ @ Not so fast ! Let 's first go over some basic image processing and manipulations that will come in handy along your image search engine journey . If you are already an image processing guru , this post will seem pretty boring to you @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485113 @185113/ <h> Faster video file FPS with cv2.VideoCapture and OpenCV <p> Have you ever worked with a video file via OpenCVs cv2.VideoCapture- function and found that reading frames- just felt slow and sluggish ? <p> Ive been there - and I know exactly how it feels . <p> Your entire video processing pipeline crawls along , unable to process more than one or two frames per second even- though you are n't  doing any type of computationally expensive image processing operations . <p> Why is that ? <p> Why , at times , does it seem like an- eternity for cv2.VideoCapture- and the associated . read- method to poll another frame from your video file ? <p> The answer is almost always- video compression- and frame decoding . <p> Depending on your video file type , the codecs you have installed , and not to mention , the physical hardware of your machine , much of your video processing pipeline can actually be consumed by- reading and- decoding the next frame in the video file . <p> That 's just computationally wasteful and there is a better way . <p> In @ @ @ @ @ @ @ @ @ @ to use threading and a queue data structure to- improve your video file FPS rate by over 52% ! <h> Faster video file FPS with cv2.VideoCapture and OpenCV <p> When working with video files and OpenCV you are likely using the cv2.VideoCapture- function . <p> First , you instantiate your cv2.VideoCapture- object by passing in the path to your input video file . <p> Then you start a loop , calling the . read- method of cv2.VideoCapture- to poll the next frame from the video file so you can process it in your pipeline . <p> The- problem ( and the reason why this method can feel slow and sluggish ) is that youre- both reading and decoding the frame in your main processing thread ! <p> As I 've mentioned in previous posts , the . read- method is ablocking operation- the- main thread of your Python + OpenCV application is entirely blocked ( i.e. , stalled ) until the frame is read from the video file , decoded , and returned to the calling function . <p> By moving these blocking I/O operations to a separate thread and maintaining @ @ @ @ @ @ @ @ @ @ FPS processing rate by over 52% ! <p> To accomplish this latency decrease our goal will be to move the reading and decoding of video file frames to an entirely separate thread of the program , freeing- up our main thread to handle the actual image processing . <p> But before we can appreciate the- faster , threaded method to video frame processing , we first need to set a benchmark/baseline with the slower , non-threaded version . <h> The slow , naive method to reading video frames with OpenCV <p> The goal of this section is to obtain a baseline on our video frame processing throughput rate using OpenCV and Python . <p> To start , open up a new file , name it readframesslow.py- , and insert the following code : <p> Faster 14 <p> 15 <p> 16 55203 @qwx675203 <p> fromimutils.video importFPS 55220 @qwx675220 55218 @qwx675218 55219 @qwx675219 <p> importcv2 55202 @ @ @ @ @ @ @ @ @ @ " --video " , required=True , <p> help= " path to input video file " ) 55208 @qwx675208 <p> # open a pointer to the video stream and start the FPS timer <p> **28;2428;TOOLONG " video " ) <p> fps=FPS().start() <p> Lines 2-6 import our required Python packages . Well be using my imutils library , a series of convenience functions to make image and video processing operations easier with OpenCV and Python . <p> If you do n't  already have imutils- installed- or if you are using a previous version , you can install/upgrade imutils- by using the following command : <p> Faster video file FPS with cv2.VideoCapture and OpenCV <p> Shell <p> 1 <p> $pip install--upgrade imutils <p> Lines 9-12 then parse our command line arguments . We only need a single switch for this script , --video- , which is the path to our input video file . <p> Line 15 opens a pointer to the --video- file using the cv2.VideoCapture- class while- Line 16 starts a timer that we can use to measure FPS , or more specifically , the throughput rate of our video processing @ @ @ @ @ @ @ @ @ @ reading frames from the video file and processing them one-by-one : <p> Faster video file FPS with <p> 41 <p> 42 <p> # loop over frames from the video file stream <p> whileTrue : <p> # grab the frame from the threaded video file stream <p> ( grabbed , frame ) =stream.read() <p> # if the frame was not grabbed , then we have reached the end <p> # of the stream <p> ifnotgrabbed : <p> break <p> # resize the frame and convert it to grayscale ( while still <p> # retaining 3 channels ) <p> **26;2458;TOOLONG , width=450 ) <p> frame=cv2.cvtColor ( frame , cv2.COLORBGR2GRAY ) <p> frame=np.dstack ( frame , frame , frame ) <p> # display a piece of text to the frame ( so we can benchmark <p> # fairly against the fast method @ @ @ @ @ @ @ @ @ @ , ( 10,30 ) , <p> **26;2486;TOOLONG , ( 0,255,0 ) , 2 ) <p> # show the frame and update the FPS counter <p> cv2.imshow ( " Frame " , frame ) <p> cv2.waitKey(1) <p> fps.update() <p> On- Line 19 we start looping over the frames of our video file . <p> A call to the . read- method on- Line 21 returns a 2-tuple containing : <p> grabbed- : A boolean indicating if the frame was successfully read or not . <p> frame- : The actual video frame itself . <p> If grabbed- is False- then we know we have reached the end of the video file and can break from the loop ( Lines 25 and 26 ) . <p> Otherwise , we perform some basic image processing tasks , including : <p> Resizing the frame to have a width of 450 pixels . <p> Converting the frame to grayscale . <p> Drawing the text on the frame via the cv2.putText- method . We do this because well be using the cv2.putText- function to display our queue size in the fast , threaded example below and want @ @ @ @ @ @ @ @ @ @ final code block handles computing the approximate FPS/frame rate throughput of our pipeline , releasing the video stream pointer , and closing any open windows : <p> Faster video file FPS with cv2.VideoCapture and OpenCV <p> Python <p> 44 <p> 45 <p> 46 <p> 47 <p> 48 <p> 49 <p> 50 <p> 51 <p> # stop the timer and display FPS information <p> fps.stop() <p> print ( " INFO elasped time : : .2f " . format ( fps.elapsed ( ) ) ) <p> print ( " INFO approx . FPS : : .2f " . format ( fps.fps ( ) ) ) <p> # do a bit of cleanup <p> stream.release() <p> cv2.destroyAllWindows() <p> To execute this script , be sure to download the source code + example video to this blog post using the- " Downloads " section at the bottom of the tutorial . <p> For this example well be using the first 31 seconds of the- Jurassic Park trailer ( the . mp4 file is included in the code download ) : <p> Let 's go ahead and obtain a baseline for frame processing throughput on this @ @ @ @ @ @ @ @ @ @ and OpenCV <p> Shell <p> 1 <p> $python readframesslow.py--video **28;2514;TOOLONG <p> Figure 1 : The slow , naive method to read frames from a video file using Python and OpenCV . <p> As you can see , processing- each individual frame of the 31 second video clip takes approximately 47 seconds with a FPS processing rate of 20.21 . <p> These results imply that its actually taking- longer to read and decode the individual frames than the actual length of the video clip ! <p> To see how we can speedup our frame processing throughput , take a look at the technique I describe in the next section . <h> Using threading to buffer frames with OpenCV <p> To improve the FPS processing rate of frames read from video files with OpenCV we are going to utilize- threading and the queue data structure : <p> Figure 2 : An example of the queue data structure . New data is enqueued to the back of the list while older data is dequeued- from the front of the list. - ( source : Wikipedia ) <p> Since the . read- method of @ @ @ @ @ @ @ @ @ @ significant speedup simply by creating a- separate thread from our main Python script that is solely responsible for reading frames from the video file and maintaining a queue . <p> Lines 2-4 handle importing our required Python packages . The Thread- class is used to create and start threads in the Python programming language . <p> We need to take special care when importing the Queue- data structure as the name of the queue package is different based on which Python version you are using ( Lines 7-12 ) . <p> We can now define the constructor to FileVideoStream- : <p> Faster video file FPS with <p> 22 <p> 23 <p> classFileVideoStream : <p> definit ( self , path , queueSize=128 ) : <p> # initialize the file video stream along with the boolean <p> # used to indicate if the thread should be stopped or not <p> **34;2544;TOOLONG <p> self.stopped=False <p> # initialize the queue used to store frames read from <p> # the video file <p> **31;2580;TOOLONG @ @ @ @ @ @ @ @ @ @ an optional one : <p> path- : The path to our input video file . <p> queueSize- : The maximum number of frames to store in the queue . This value defaults to 128 frames , but you depending on ( 1 ) the frame dimensions of your video and ( 2 ) the amount of memory you can spare , you may want to raise/lower this value . <p> Our next section handles reading frames from the FileVideoStream- , processing them , and displaying them to our screen : <p> Faster video file FPS with <p> 41 <p> 42 <p> # loop over frames from the video file stream <p> whilefvs.more() : <p> # grab the frame from the threaded video file stream , resize <p> # it , and convert it to grayscale ( while still retaining 3 <p> # channels ) <p> frame=fvs.read() <p> **26;2613;TOOLONG , width=450 ) <p> frame=cv2.cvtColor @ @ @ @ @ @ @ @ @ @ frame , frame ) <p> # display the size of the queue on the frame <p> cv2.putText ( frame , " Queue Size : " . format ( fvs.Q.qsize ( ) ) , <p> LONG ... <p> # show the frame and update the FPS counter <p> cv2.imshow ( " Frame " , frame ) <p> cv2.waitKey(1) <p> fps.update() <p> We start a while- loop on- Line 26 that will keep grabbing frames from the FileVideoStream- queue until the queue is empty . <p> For each of these frames well apply the same image processing operations , including : resizing , conversion to grayscale , and displaying text on the frame ( in this case , our text will be the number of frames in the queue ) . <p> The processed frame is displayed to our screen on- Lines 40-42 . <p> The last code block computes our FPS throughput rate and performs a bit of cleanup : <p> Faster video file FPS with cv2.VideoCapture and OpenCV <p> Python <p> 44 <p> 45 <p> 46 <p> 47 <p> 48 <p> 49 <p> 50 <p> 51 <p> # stop the @ @ @ @ @ @ @ @ @ @ " INFO elasped time : : .2f " . format ( fps.elapsed ( ) ) ) <p> print ( " INFO approx . FPS : : .2f " . format ( fps.fps ( ) ) ) <p> # do a bit of cleanup <p> cv2.destroyAllWindows() <p> fvs.stop() <p> To see the results of the readframesfast.py- script , make sure you download the source code + example video using the- " Downloads " section at the bottom of this tutorial . <h> Summary <p> In todays tutorial I demonstrated how to use threading and a queue data structure to improve the FPS throughput rate of your video processing pipeline . <p> By placing the call to . read- of a cv2.VideoCapture- object in a thread- separate from the main Python script we can avoid blocking I/O operations that would otherwise dramatically slow down our pipeline . <p> Finally , I provided an example comparing- threading- with- no threading . The results show that by using threading we can improve our processing pipeline by up to 52% . <p> However , keep in mind that the more steps ( i.e. , function @ @ @ @ @ @ @ @ @ @ more computation needs to be done therefore , your actual frames per second rate will drop , but you 'll still be processing faster than the non-threaded version . <p> To be notified when future blog posts are published , be sure to enter your email address in the form below ! <h> Downloads : 55217 @qwx675217 <p> This did n't  work for me using a CHIP SoC . I saw exactly the same frame rate . A simpler method is to move the UVC code into a separate process ( mjpg-streamer ) and use a simple socket based client thus removing the need for VideoCapture . I get precise FPS and better overall performance this way . See my project https : **31;2641;TOOLONG which includes the Python code and performance tests . <p> Hey Adrian , another awesome tutorial , Thanks a lot . I have one problem , I m working on getting frames from camera over network , can we use threaded frames there ? What if we are using cv2.capture in Tkinter ? How to increase fps processing in Tkinter . <p> OpenCV does n't  actually " @ @ @ @ @ @ @ @ @ @ . The goal , in the case of OpenCV , is to read and process the frames as fast as possible . If you want to display frames at a constant rate you 'll need to insert time.sleep calls into the main loop . Again , OpenCV is n't directly made for video playback . <p> As or transmitting frames over a TCP protocol , that is more network overhead which will certainly reduce your overall FPS . You should look into gstreamer and FFMPEG to speedup the streaming process . <p> Hey Adrian , good work as always , i have implemented this in a real time application in order to achieve some LPR cameras , however , i have to stream the feed to a web server . My camera has h264 support ( logitech c920 ) but i havent found how to pull the raw h264 frames from camera using opencv . Do you have any experience with this ? I 'm sure this would increase even more my fps since no decoding-enconding would be requiered . <p> Hi Florent I just double-checked on my system . My Python 2.7 @ @ @ @ @ @ @ @ @ @ than the Python 3.5 " fast " method . Perhaps there is a difference in the Queue data structure between Python versions that I am not aware of ( or maybe the threading ? ) . <p> However , I am not able to replicate your slower method being substantially speedier than the fast , threaded method ( Python 3.5 ) : <p> I have a solution to the problem with python3 . The reason is that the stream reader in the thread is continuously trying to push something into the queue in a direct tight while loop . On the other side the main thread is trying to get data from the queue . As locking is necessary , or the video thread or the main thread can access the queue . As the video thread has less work then the main thread its continuously trying to push the data if at any moment we a free slot in the queue . But this limits the access from the main thread to get data out of it . Just adding a small time.sleep(0.001) inside the while loop at the @ @ @ @ @ @ @ @ @ @ to get data of the queue . Now we have a different story . On my system the slow version <p> INFO elasped time : 11.11 INFO approx . FPS : 86.03 <p> fast version before the change <p> INFO elasped time : 48.18 INFO approx . FPS : 19.84 <p> The fast version after the change <p> INFO elasped time : 8.13 INFO approx . FPS : 102.72 <p> Its a bit faster than the non thread version , reason is the overlapping time of reading a frame and processing a frame in different threads . <p> Moral of the story is that tight while loops are never a good idea when using threading . This solution should work also for python 2.7 . <p> I just have another remark . The main thread assumes that if the queue is empty , the video is at the end . If we would have a faster consumer than a producer this is a problem . My suggestion is to add a method to FileVideoStream <p> def running(self) : # indicate that the thread is still running return ( not ( @ @ @ @ @ @ @ @ @ @ True is the video file is at the end . <p> In the main application can be changed like this . Also the sleep of 1 second at the beginning ( which was there to let the queue fill up ) can be left out . You nicely see the video showing and you see the queue size filling up in a few secs to max . <p> Hi Wim thanks for sharing . I would like to point out that the Queue data structure is thread safe in both Python 2.7 and Python 3 . Its also strange that the slowdown does n't  happen with Python 2.7 it only seems to happen with Python 3 . Your time.sleep call seems to do the trick as it likely prevents the semaphores from constantly being polled , but again , it still seems to be Python 3 specific . <p> Would there be a way to use this technique to process videos much quicker ? Say I had a video that was 1 hour long , could I then use threading to process the video in less than an hour ? @ @ @ @ @ @ @ @ @ @ save the frames from the thread ? Will the resulting video file play at a much faster speed than the original input video ? <p> I would suggest using 1 thread to read the frames from a video file and store them in a queue . Then , create as many processes as you have cores on your CPU and have them process the frames in parallel . This will dramatically speedup your throughput . <p> Hi Adrian , Do you think its feasible to change queue length dynamically ? I 've got a script that does detection ( 1st frame ) and then tracking ( remaining 24 frames ) , in an endless loop , taking images from camera . The problem I 'm facing is that during detection some frames are lost and the tracker is not able to pick up the object . When using the queue its a bit more random as queue fills up quickly ( quicker than the script takes them away from the front/bottom of the queue ) so its constantly full . When detection happens , new frames are not added to the queue @ @ @ @ @ @ @ @ @ @ now . Do you have any recommendations for this issue ? Thank you , Tom <p> You can certainly change the queue length dynamically , but you would have to implement a thread-safe queue with a dynamic size yourself . Actually , I 'm not sure about this , but if you instantiate Queue() without any data parameters it might allow for the queue to shrink and expand as needed . <p> Great question Brandon . I personally do n't  like using the cap.get and get.set calls as they can be extremely temperamental based on your OpenCV version , codecs installed , or if you 're working with a webcam versus file on disk . <p> That said , the best way to modify this code would be to update the construct to accept a pre-initialized cv2.VideoCapture object rather than having the constructor built it itself . <p> Hi Adrian , Good article as always ! Recently , I came up with the exact same need , separate processing and input/output for a small script . <p> I guess that your example has a few drawbacks that I would like to point @ @ @ @ @ @ @ @ @ @ ) to synchronize your threads . It is always a source of confusion and misconception . Because you are using the queue emptiness as an exit condition , you have to fill it before . I would suggest another approach where reading is performed by the main thread and the processing by another thread . You could use the Queue.join() Queue.taskdone() as a way to synchronize threads . Usually this pattern is best achieved with a last message enqueued to kill the processing thread . <p> Threading in python comes with some limitations . One of them is GIL ( Global Interpreter Lock ) which means that even if you are using many threads only one of them is running at once . This is a major drawback using threading in python . Obviously , if you are only relying on bindings ( opencv here ) you can overcome it ( the GIL should be released in a c/c++ bindings ) . As an alternative , I would recommend the multiprocessing module . <p> Depending on the application , I would consider using a smaller queue but more processing threads @ @ @ @ @ @ @ @ @ @ time and processing time . <p> Hi Gilles , great points . I used time.sleep in this case as a matter of simplicity , but yes , you are right . As for threading versus multiprocessing , for I/O operations its common to use simple threads . Is there a particular reason you 're recommending multiprocessing in this case ? <p> As a rule of thumb , I would always recommend using multiprocessing rather than threading . In Python ( as well as some other interpreted language ) , even if you are running multiple threads , only one of them goes at a time . This is called GIL ( Global InterpreterLock ) . So , using many threads with pure Python code wont bring you the expected parallel execution.On the other hand , multiprocessing relies on separate processes and interprocess communication . Each process runs its own python interpreter , allowing for parallel execution of pure python code . Obviously , in the example you provided , using threading for I/O is not a big issue . The I/O occurs in opencv , and GIL is released when going @ @ @ @ @ @ @ @ @ @ Last point , using multiprocessing can ease pure Python parallel execution easily and efficiently using multiprocessing.Pool and multiprocessing.Queue ( same interface as standard Queue ) . <p> Thanks for the tips Giles . I 've always defaulted to threads for I/O heavy tasks and then processes for computation heavy tasks . Ill start playing around with multi-processing for other tasks as well now . <p> Hey Adrian ! I have learned much from your blog posts . I 'm also looking for ways to speed up my VideoCapture-functions , so this post was excellent . But I 'm wondering if it is possible to skip frames in a video file ? I 'm trying to detect motion with a simple pixel based matching ( thresholding ) , and I want to make an if statement telling the program to skip the next 24 frames if no motion is detected . If motion is detected I want to process every frame until no motion is detected . See my problem ? I 'm using VideoCapture.read() for looping through the frames . <p> Instead of using . read() to read and decode each frame , you could @ @ @ @ @ @ @ @ @ @ would allow you to seek N frames into the video without having to read and decode each of the previous N 1 frames . I 've heard its also possible to use the . set method as well , but I have n't personally tried this . <p> Something strange happened in my case : readframesslow was playing the video perfectly . readframesfast was playing the video slowly . But the opposite should happen , right ! ! ! ! ! <p> Here is the Terminal OUTPUT : <p> ( cv ) **37;2674;TOOLONG Mauli$ python readframesfast.py video **28;2713;TOOLONG INFO starting video file thread 2017-03-01 00:32:21.473 python43433:374316 ! ! ! BUG : The current event queue and the main event queue are not the same . Events will not be handled correctly . This is probably because TSGetMainThread was called for the first time off the main thread . Terminated : 15 <p> ( cv ) **37;2743;TOOLONG Mauli$ python readframesslow.py video **28;2782;TOOLONG 2017-03-01 00:32:43.113 python43481:374632 ! ! ! BUG : The current event queue and the main event queue are not the same . Events will not be handled correctly . This is @ @ @ @ @ @ @ @ @ @ the main thread . Terminated : 15 <p> I came across a problem with your code with my awful slow hard dive . If another application has heavy use of the hard drive the queue drops to zero and your more() method will return False prematurely , ending the video reading before the end of the file . However there 's a quick fix . The get method from the Queue has a blocking option and instead of testing the queue length in more() test the self.stopped flag instead <p> def read(self) : # return next frame in the queue return self.Q.get ( block=True , timeout=2.0 ) def more(self) : # return True if there are still frames in the queue return not self.stopped <p> As mentioned in previous comments , this is definitely a Python 3 specific issue . I 'm not sure why it happens only for Python 3.5 + OpenCV 3 , but not Python 2.7 + OpenCV 3 or Python 2.7 + OpenCV 2.4 . My guess is that there was a change in how the Queue data structure works in Python 3 , but again , I 'm @ @ @ @ @ @ @ @ @ @ upcoming fix for python3 ? On 3.6 same behavor as everybody has I see that " slow " version is getting 120% CPU + 290Mb RAM on my Macbook , " fast " version takes 175% CPU and 990Mb RAM ( Reading . MOV container with H.264 FullHD video ) . So Assuming higher CPU/memory usage there should be an effect , but it plays like 30-50% slower <p> As I mentioned in previous comments , I 'm not sure what the issue is with Python 3 and why it takes so much longer than Python 2.7 . I would appreciate it if another PyImageSearch reader could investigate this issue further as I 'm pretty sure the issue is with the Queue data structure . <p> Oh , and I have read all your posts , thanks ! Very easy to read and understand for noob like me = May I have advise from you about this questions : <p> 1 . What is the best way to detect traffic signals ? ( I am using color tracking through HSV threshold and then detecting shape through HoughCircles . But I am getting @ @ @ @ @ @ @ @ @ @ or someone brake lights is on = 2 . What is the best way to detect standing/moving cars ? Train Haar ? <p> 1 . To detect traffic signals I would train a custom object detector to recognize the traffic signal . From there you could apply color thresholding to determine which light is " on " . <p> 2 . This really depends on the data that you are working with . HOG + Linear SVM detectors might work here if the video is fixed and non-moving , but it will be harder to generalize across datasets . In that case you might consider a more advanced approach such as deep learning . <p> Hi , Adrian Rosebrock , very good works ! I would like to asking you one question , is cv2.videocapture.read ( ) method I/O bound or CPU bound ? As far we know , decoding video is slow , so the read ( ) method is blocked . If it is CPU bound , @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485114 @185114/ <p> Over the past few months Ive gotten quite the number of requests landing in my inbox to build a bubble sheet/Scantron-like test reader using computer vision and image processing techniques . <p> And while I 've been having a lot of fun doing this series on machine learning and deep learning , I 'd be- lying if I said this little mini-project was n't a short , welcome break . One of my favorite parts of running the PyImageSearch blog is demonstrating how to build- actual solutions to problems using computer vision . <p> In fact , what makes this project- so special is that we are going to combine the techniques from- many previous blog posts , including building a document scanner , contour sorting , and perspective transforms. - Using the knowledge gained from these previous posts , well be able to make quick work of this bubble sheet scanner and test grader . <p> You see , last Friday afternoon I quickly Photoshopped an example bubble test paper , printed out a few copies , - and then set to work on coding up the actual implementation . @ @ @ @ @ @ @ @ @ @ and I think you 'll absolutely be able to use this bubble sheet grader/OMR system as a starting point for your own projects . <h> Bubble sheet scanner and test grader using OMR , Python , and OpenCV <p> In the remainder of this blog post , I 'll discuss what exactly- Optical Mark Recognition ( OMR ) - is . Ill then demonstrate how to implement a bubble sheet test scanner and grader using- strictly computer vision and image processing techniques , along with the OpenCV library . <p> Once we have our OMR system- implemented , I 'll provide- sample- results of our test grader on a few example exams , including ones that were filled out with nefarious intent . <p> Finally , I 'll discuss some of the shortcomings of this current bubble sheet scanner system and how we can improve it in future iterations . <h> What is Optical Mark Recognition ( OMR ) ? <p> Optical Mark Recognition , or OMR for short , is the process of- automatically analyzing human-marked documents and interpreting their results . <p> Arguably , the most famous , easily recognizable form of @ @ @ @ @ @ @ @ @ @ the ones you took in elementary school , middle school , or even high school . <p> If you 're unfamiliar with " bubble sheet tests " or the trademark/corporate name of " Scantron tests " , they are simply multiple-choice tests that you take as a student. - Each question on the exam is a multiple choice and you use a #2 pencil to mark the " bubble " that corresponds to the correct answer . <p> The most notable bubble sheet test you experienced- ( at least in the United States ) were taking the SATs during high school , prior to filling out college admission applications . <p> I- believe that the SATs use the software provided by Scantron to perform OMR and grade student exams , but I could easily be wrong there . I only make note of this because Scantron is used in over 98% of all US school districts . <p> In short , what- Im trying to say is that there is a- massive market for Optical Mark Recognition and the ability to grade and interpret human-marked forms and exams . <h> Implementing @ @ @ @ @ @ @ @ @ @ , and OpenCV <p> Now that we understand the basics of OMR , let 's build a computer vision system using Python and OpenCV that can- read and- grade bubble sheet tests . <p> Of course , I 'll be providing lots of visual example images along the way so you can understand- exactly what techniques I 'm applying and- why I 'm using them . <p> Below I have included an example filled in bubble sheet exam that I have put together for this project : <p> Figure 1 : The example , filled in bubble sheet we are going to use when developing our test scanner software . <p> Well be using this as our example image as we work through the steps of building our test grader . Later in this lesson , you 'll also find additional sample exams . <p> I have also included a- blank exam template as a . PSD ( Photoshop ) file so you can modify it as you see fit . You can use the- " Downloads " section at the bottom of this post to download the code , example images , and template @ @ @ @ @ @ @ @ @ @ sheet scanner and grader <p> The goal of this blog post is to build a bubble sheet scanner and test grader using Python and OpenCV . <p> To accomplish this , our implementation will need to satisfy the following 7- steps : <p> Step #1 : Detect the exam in an image . <p> Step #2 : Apply a perspective transform to extract the top-down , birds-eye-view of the exam . <p> Step #3 : Extract the set of bubbles ( i.e. , the possible answer choices ) from the perspective transformed exam . <p> Step #6 : Lookup the correct answer in our answer key to determine if the user was correct in their choice . <p> Step #7 : Repeat for all questions in the exam . <p> The next section of this tutorial will cover the actual- implementation of our algorithm . <h> The bubble sheet scanner implementation with Python and OpenCV <p> To get started , open up a new file , name it testgrader.py- , and let 's get to work : <p> Bubble sheet scanner and test 15 <p> 16 <p> 17 55203 @qwx675203 <p> fromimutils.perspective importfourpointtransform <p> fromimutils importcontours 55220 @qwx675220 55218 @qwx675218 55219 @qwx675219 <p> importcv2 55202 @qwx675202 55206 @qwx675206 <p> ap.addargument ( " -i " , " --image " , required=True , <p> help= " path to the input image " ) 55208 @qwx675208 <p> # define the answer key which maps the question number <p> # to the correct answer <p> **29;2812;TOOLONG <p> On- Lines 2-7 we import our required Python packages . <p> You should already have OpenCV- and Numpy installed on your system , but you- might not have the most recent version of imutils , my set of convenience functions to make performing basic image processing operations easier . To install imutils- ( or upgrade to the latest version ) , just execute the following command : <p> Bubble sheet scanner and test grader using OMR , Python and OpenCV <p> Shell <p> 1 <p> $pip install--upgrade imutils <p> Lines 10-12 parse our command line arguments . We only need @ @ @ @ @ @ @ @ @ @ path to the input bubble sheet test image that we are going to grade for correctness . <p> Line 17 then defines our ANSWERKEY- . <p> As the name of the variable suggests , the ANSWERKEY- provides integer mappings of the- question numbers to the- index of the correct bubble . <p> In this case , a key of- 0 indicates the- first question , while a value of- 1 signifies- " B " as the correct answer ( since- " B " is the index- 1- in the string- " ABCDE " ) . As a second example , consider- a key of- 1 that maps to a value of- 4 this would indicate that the answer to the second question is- " E " . <p> As a matter of convenience , I have written the entire answer key in plain english here : <p> Question #1 : B <p> Question #2 : - E <p> Question #3 : - A <p> Question #4 : - D <p> Question #5 : - B <p> Next , let 's preprocess our input image : <p> Bubble sheet scanner and test @ @ @ @ @ @ @ @ @ @ 19 <p> 20 <p> 21 <p> 22 <p> 23 <p> 24 <p> # load the image , convert it to grayscale , blur it <p> # slightly , then find edges <p> image=cv2.imread ( args " image " ) 55215 @qwx675215 <p> **29;2843;TOOLONG , ( 5,5 ) , 0 ) <p> **31;2874;TOOLONG <p> On- Line 21- we load our image from disk , followed by converting it to grayscale ( Line 22 ) , and blurring it to reduce high frequency noise ( Line 23 ) . <p> Notice how the edges of the document are- clearly defined , with- all four- vertices- of the exam being present in the image . <p> Obtaining this- silhouette of the document is- extremely important in our next step as we will use it as a marker to apply a perspective transform to the exam , obtaining a top-down , birds-eye-view of the document : <p> Bubble sheet scanner and test grader using OMR , <p> 48 <p> 49 <p> # find contours in the edge map , then initialize <p> # the contour that corresponds to the document <p> LONG ... 55211 @qwx675211 <p> **36;2907;TOOLONG <p> docCnt=None <p> # ensure that at least one contour was found <p> iflen(cnts)&gt;0 : <p> # sort the contours according to their size in <p> # descending order <p> LONG ... <p> # loop over the sorted contours <p> forcincnts : <p> # approximate the contour <p> peri=cv2.arcLength ( c , True ) <p> **35;2945;TOOLONG , True ) <p> # if our approximated contour has four points , <p> # then we can assume we have found the paper <p> iflen(approx)==4 : <p> docCnt=approx <p> break <p> Now that we have the outline of our exam , we apply the cv2.findContours- function to find the lines that correspond to the exam itself . <p> We do this by sorting our contours by their- area ( from largest to smallest ) on- Line 37 ( after making sure at least one contour was found on- Line 34 , @ @ @ @ @ @ @ @ @ @ be placed at the front of the list , while smaller contours will appear farther back in the list . <p> We make the assumption that our exam will be the- main focal point of the image , and thus be larger than other- objects in the image . This assumption allows us to " filter " our contours , simply by investigating their area and knowing that the contour that corresponds to the exam should be near the front of the list . <p> However , - contour area and size is not enough we should also check the number of- vertices on the contour . <p> To do , this , we loop over each of our ( sorted ) contours on- Line 40 . For each of them , we approximate the contour , which in essence means we- simplify the number of points in the contour , making it a " more basic " geometric shape . You can read more about contour approximation in this post on building a mobile document scanner . <p> On- Line 47 we make a check to see if our @ @ @ @ @ @ @ @ @ @ , - we assume that we have found the exam . <p> Below I have included an example image that demonstrates the docCnt- variable being drawn on the original image : <p> Figure 3 : An example of drawing the contour associated with the exam on our original image , indicating that we have successfully found the exam . <p> Sure enough , this area corresponds to the outline of the exam . <p> Now that we have used contours to find the outline of the exam , we can apply a perspective transform to obtain a top-down , birds-eye-view of the document : <p> Bubble sheet scanner and test grader using OMR , Python and OpenCV <p> Python <p> 51 <p> 52 <p> 53 <p> 54 <p> 55 <p> # apply a four point perspective transform to both the <p> # original image and grayscale image to obtain a top-down <p> # birds eye view of the paper <p> LONG ... <p> LONG ... <p> In this case , well be using my implementation of the fourpointtransform- function which : <p> You can learn more about the perspective transform @ @ @ @ @ @ @ @ @ @ coordinate ordering , but for the time being , simply understand that this function handles taking the " skewed " exam and transforms it , returning a top-down view of the document : <p> Figure 4 : Obtaining a top-down , birds-eye view of both the original image ( left ) - along with the grayscale version ( right ) . <p> Notice how- only the question regions of the exam are highlighted and nothing else . <p> We can now move on to the " grading " portion of our OMR system : <p> Bubble sheet scanner and test grader using OMR , <p> 94 <p> 95 <p> # sort the question contours top-to-bottom , then initialize <p> # the total number of correct answers <p> LONG ... <p> method= " top-to-bottom " ) 0 <p> correct=0 <p> # each question has 5 possible answers , to loop over the <p> # question in batches of 5 <p> LONG ... @ @ @ @ @ @ @ @ @ @ <p> # left to right , then initialize the index of the <p> # bubbled answer <p> LONG ... <p> bubbled=None <p> First , we must sort our questionCnts- from top-to-bottom . This will ensure that rows of questions that are- closer to the top of the exam will appear first in the sorted list . <p> We also initialize a bookkeeper variable to keep track of the number of correct- answers . <p> On- Line 90 we start looping over our questions . Since each question has 5 possible answers , well apply NumPy array slicing and contour sorting to to sort the- current set of contours from left to right . <p> The reason this methodology works is because we have- already sorted our contours from top-to-bottom . We- know that the 5 bubbles for each question will appear sequentially in our list - but we- do not know- whether these bubbles will be sorted from left-to-right. - The sort contour call on- Line 94 takes care of this issue and ensures each row of- contours are sorted into rows , from left-to-right . <p> To visualize this @ @ @ @ @ @ @ @ @ @ each row of questions as a separate color : <p> Figure 7 : By sorting our contours from top-to-bottom , followed by left-to-right , we can extract each row of bubbles . Therefore , each row is equal to the bubbles for one question . <p> Given a row of bubbles , the next step is to determine which bubble is filled in . <p> We can accomplish this by using our thresh- image and counting the number of non-zero pixels ( i.e. , - foreground- pixels ) in each bubble region : <p> Bubble sheet scanner and test grader using OMR , <p> 113 <p> 114 <p> # loop over the sorted contours <p> for ( j , c ) inenumerate(cnts) : <p> # construct a mask that reveals only the current <p> # " bubble " for the question <p> **26;2982;TOOLONG , dtype= " uint8 " ) <p> cv2.drawContours ( mask @ @ @ @ @ @ @ @ @ @ the mask to the thresholded image , then <p> # count the number of non-zero pixels in the <p> # bubble area <p> **26;3010;TOOLONG , thresh , mask=mask ) <p> **28;3038;TOOLONG <p> # if the current total has a larger number of total <p> # non-zero pixels , then we are examining the currently <p> # bubbled-in answer <p> ifbubbled **25;3068;TOOLONG : <p> bubbled= ( total , j ) <p> Line 98 handles looping over each of the sorted bubbles in the row . <p> We then construct a mask for the current bubble on- Line 101 and then count the number of non-zero pixels in the masked region ( Lines 107 and 108 ) . The more non-zero pixels we count , then the more foreground pixels there are , and therefore the bubble with the maximum non-zero count is the index of the bubble that the the test taker has bubbled in ( Line 113 and 114 ) . <p> Below I have included an example of creating and applying a mask to each bubble associated with a question : <p> Figure 8 : An example of constructing @ @ @ @ @ @ @ @ @ @ Clearly , the bubble associated with- " B " has the most thresholded pixels , and is therefore the bubble that the user has marked on their exam . <p> This next code block handles looking up the correct answer in the ANSWERKEY- , updating any relevant bookkeeper variables , and finally drawing the marked bubble on our image : <p> Bubble sheet scanner and test grader using OMR , <p> 126 <p> 127 <p> # initialize the contour color and the index of the <p> # *correct* answer <p> color= ( 0,0,255 ) <p> k=ANSWERKEYq <p> # check to see if the bubbled answer is correct <p> ifk==bubbled1 : <p> color= ( 0,255,0 ) <p> correct+=1 <p> # draw the outline of the correct answer on the test <p> cv2.drawContours ( paper , cntsk , -1 , color , 3 ) <p> Based on whether the test taker was correct or incorrect yields which color is drawn on the exam . If the- test @ @ @ @ @ @ @ @ @ @ . However , if the test taker made a mistake and marked an incorrect answer , well let them know by highlighting the- correct answer in- red : <p> Figure 9 : Drawing a " green " circle to mark " correct " or a " red " circle to mark " incorrect " . <p> Finally , our last code block handles scoring the exam and displaying the results to our screen : <p> Bubble sheet scanner and test grader using OMR , Python and OpenCV <p> Python <p> 129 <p> 130 <p> 131 <p> 132 <p> 133 <p> 134 <p> 135 <p> 136 <p> # grab the test taker <p> score= ( correct/5.0 ) *100 <p> print ( " INFO score : : .2f% " . format(score) ) <p> LONG ... <p> **26;3095;TOOLONG , ( 0,0,255 ) , 2 ) <p> cv2.imshow ( " Original " , image ) <p> cv2.imshow ( " Exam " , paper ) 55212 @qwx675212 <p> Below you can see the output of our fully graded example image : <p> Figure 10 : Finishing our OMR system for grading human-taken exams . <p> @ @ @ @ @ @ @ @ @ @ the exam . The only question they missed was #4 where they incorrectly marked- " C " as the correct answer ( " D " was the correct choice ) . <h> Why not use circle detection ? <p> After going through this tutorial , you might be wondering : <p> " Hey Adrian , an answer bubble is a circle . So why did you extract contours instead of applying Hough circles to find the circles in the image ? " <p> Great question . <p> To start , tuning the parameters to Hough circles on an image-to-image basis can be a real pain . But that 's only a minor reason . <p> The- real reason is : <p> User error . <p> How many times , whether purposely or not , have you filled in outside the lines on your bubble sheet ? I 'm not expert , but I 'd have to guess that at least 1 in every 20 marks a test taker fills in is " slightly " outside the lines . <p> And guess what ? <p> Hough circles do n't  handle deformations in their outlines @ @ @ @ @ @ @ @ @ @ case . <p> Because of this , I instead recommend using contours and contour properties to help you filter the bubbles and answers . The cv2.findContours- function does n't  care if the bubble is " round " , " perfectly round " , or " oh my god , what the hell is that ? " . <p> Instead , the cv2.findContours- function will return a set of- blobs to you , which will be the foreground regions in your image . You can then take these regions process and filter them to find your questions ( as we did in this tutorial ) , and go about your way . <h> Our bubble sheet test scanner and grader results <p> To see our bubble sheet test grader in action , be sure to download the source code and example images to this post using the- " Downloads " section at the bottom of the tutorial . <p> We 've already seen test01.png- as our example earlier in this post , - so let 's try test02.png- : <p> Bubble sheet scanner and test grader using OMR , Python and OpenCV <p> @ @ @ @ @ @ @ @ @ @ can see that a particularly nefarious user took our exam . They were not happy with the test , writing " #yourtestsux " across the front of it along with an anarchy inspiring- " #breakthesystem " . They also marked- " A " for all answers . <p> Perhaps it comes as no surprise that the user scored a pitiful 20% on the exam , based entirely on luck : <p> Figure 11 : By using contour filtering , we are able to ignore the regions of the exam that would have otherwise compromised its integrity . <p> Unfortunately for the test taker , this strategy did n't  pay off very well . <p> Let 's look at one final example : <p> Bubble sheet scanner and test grader using OMR , Python and OpenCV <p> Shell <p> 1 <p> $python testgrader.py--image images/test05.png <p> Figure 14 : Recognizing bubble sheet exams using computer vision . <p> This student clearly studied ahead of time , earning a perfect 100% on the exam . <h> Extending the OMR and test scanner <p> Admittedly , this past summer/early autumn has been one of the- @ @ @ @ @ @ @ @ @ @ timebox the development of the OMR and test scanner software into a single , shortened afternoon last- Friday . <p> While I was able to get the barebones of a- working bubble sheet test scanner implemented , there are certainly a few areas that need improvement. - The most obvious area for improvement is the- logic to handle non-filled in bubbles . <p> In the current implementation , we ( naively ) assume that a reader has filled in- one and- only one bubble per question row . <p> However , since we determine if a particular bubble is " filled in " simply by counting the number of thresholded pixels in a row and then sorting- in descending order , this can lead to two problems : <p> What happens if a user- does not- bubble in an answer for a particular question ? <p> What if the user is nefarious and marks- multiple bubbles as " correct " in the same row ? <p> Luckily , detecting and handling of these issues is n't terribly challenging , we just need to insert a bit of logic . <p> For @ @ @ @ @ @ @ @ @ @ in an answer for a particular row , then we can place a- minimum threshold on- Line 108 where we compute cv2.countNonZero- : <p> Figure 15 : Detecting if a user has marked zero bubbles on the exam . <p> If this value is sufficiently large , then we can mark the bubble as " filled in " . Conversely , if total- is too small , then we can skip that particular bubble . If at the end of the row there are- no- bubbles with sufficiently large threshold counts , we can mark the question as " skipped " by the test taker . <p> A similar set of steps can be applied to issue #2 , where a user marks- multiple bubbles as correct for a single question : <p> Figure 16 : Detecting if a user has marked multiple bubbles for a given question . <p> Again , all we need to do is apply our thresholding and count step , this time keeping track if there are- multiple bubbles that have a total- that exceeds some pre-defined value . If so , we can invalidate @ @ @ @ @ @ @ @ @ @ Summary <p> In this blog post , I demonstrated how to build a bubble sheet scanner and test grader using computer vision and image processing techniques . <p> Finally , I provided a Python and OpenCV implementation that you can use for building your own bubble sheet test grading systems . <p> If you have any questions , please feel free to leave a comment in the comments section ! <p> But before you , be sure to enter your email address in the form below to be notified when future tutorials are published on the PyImageSearch blog ! <h> Downloads : 55217 @qwx675217 <p> If the image is already a birds-eye-view , then yes , you can use the same contours that were extracted previously but again , you would have to make the assumption that you already have a birds-eye-view of the image . <p> I would suggest using more contour filtering . You can use contours to find each of the " boxes " in the sheet . Sort the contours from left-to-right and top-to-bottom . Then extract each of the boxes and process the bubbles in @ @ @ @ @ @ @ @ @ @ the right side of the paper are aligned with the target areas . You could threshold the image , findContours and filter contours in the leftmost 10% of the image to find the rows and sort them by y-position . <p> Then you could look for contours in the rest of the area . The index of the closest alignment mark for y-direction gives row , the x position as percentage of the page width gives column . <p> Once you have the column and row of each mark , you just need " normal code " to interpret which question and answer this represents . <p> Nice to see your implementation of this . I started a similar project earlier this year but I ended up putting it on parking for now . My main concern was the amount of work it goes into making one work right without errors and the demand did n't  seem to be there . Seems like scantron has a monopoly on this . What are your thoughts on that ? <p> In my case , Image contain 4 reference rectangle which is base @ @ @ @ @ @ @ @ @ @ information like text , circle and rectangle . Now , I want to write a script to straighten the image based on four rectangle.my resultant image should be straighten . So i can extract some information after deskewing it.How can be possible it ? When i used for my perspective transformation , it only detects highest rectangle contour . my image is like http : **29;3123;TOOLONG output image must be like http : **29;3154;TOOLONG <p> I am waiting for that tutorial . i am not getting proper reference for deskewing of image in my scenario . In image there is barcode as well as that 4 small rectangle . i am not able to deskew it because of barcode in side that . As i am building commercial s/w , i can not provide real images here only . In side image , i have to extract persons unique i 'd and age , DOB which are in terms of optical mark . Once i scan form which is based on OMR i need to extract those information . Is there any approach which can help me to achieve this goals @ @ @ @ @ @ @ @ @ @ As I mentioned , I 've added it to my queue . Ill try to bump it up , but please keep in mind that I am very busy and can not accommodate every single tutorial request in a timely manner . Thank you for your patience . <p> Dear Adrian thank you i upgraded the code : the code now capturing the image from laptop camera . added dropdown to selected answer key . added the date in the name of the result ( result+date+.png ) . can i send the cod to you , and is this code opensource , free . best regards <p> Hi Silver feel free to send me the code or you can release it on your own GitHub page . If you do n't  mind , I would appreciate a link back to the PyImageSearch site , but that 's not necessary if you do n't  want to . <p> I am facing below issues while making bounding rectangle on bubbles : 1 . In image , bubbles are somewhere near to rectangle where student can write manually their roll number because after thresolding bubbles @ @ @ @ @ @ @ @ @ @ circle . 2 . If bubble filled out of the boundary , again it ca n't be detectable . 3 . False detection of circle because of similar height and width . <p> Its hard to say without seeing examples of what you 're working with . I 'm not sure what you mean by if the bubble is filled in outside the circle it being impossible to detect the code in this post actually helps prevent that by alleviating the need for Hough circles which can be hard to tune the parameters to . Again , I get the impression that you 're using Hough circles instead of following the techniques in this post . <p> Do you have a code for this in java ? I am planning a project similar to this one , I am having problems especially since this program was created in python and using many plugin modules which is not available in java . <p> I hope you can consider my request since this is related for my school work . Thank you <p> Hey Nic I only provide Python and OpenCV code on this blog . @ @ @ @ @ @ @ @ @ @ would really suggest you struggle and fight your way through the Python to Java conversion . You will learn a lot more that way . <p> Can you provide a code that can allow this code to run directly on a python compiler rather than running the program on cmd . I would like to focus on python for developing a project same on this one , Ive ask many experts and python was the first thing they recommended since it can create many projects and provides many support on many platforms unlike java . <p> Hey Nic while I 'm happy to help point readers like yourself in the write direction , I can not write code for you . I would suggest you taking the time to learn and study the language . If you need help learning OpenCV and computer vision , take a look at Practical Python and OpenCV . <p> I was wondering if you have any insights on this matter . Why is it not finding contours for the bubbles if I do n't  do some steps on " warp " ? The picture of @ @ @ @ @ @ @ @ @ @ bubble . <p> If this is confusing , I 'd love to share with you my code and some images I generated to give you a more clear picture of what 's going on . <p> The best way to diagnose this issue would be to see what your thresh image looks like . Also , is there any reason why you are using cv2.RETRTREE ? Usually for this application you would use cv2.RETREXTERNAL , but again , that is really dependent on what your thresh image looks like . <p> And for using RETREXTERNAL instead of RETRTREE when processing warp , thank you for the suggestion . I was able to detect few more bubbles using RETREXTERNAL . <p> I used RETRTREE previously for cropping my omr image . ( when I used RETREXTERNAL , I was n't able to crop the image properly for some reason ) Here is the link to my original image for your information.http : //imgur.com/a/GVZ7c <p> I think I see the issue . The thresholded image contains white pixels along the borders . That is what is causing the contour detection to return strange results . @ @ @ @ @ @ @ @ @ @ should work okay . <p> Adrian , it is a great post for learners like me . I had this problem in detecting circle contours , for non filled circles it is detecting inside as well as outside edge of the circle . any method by which i can make it to detect @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485115 @185115/ <h> Gradient descent with Python <p> Every relationship has its building blocks . Love . Trust . Mutual respect . <p> Yesterday , I asked my girlfriend of 7.5 years to marry me . She said yes . <p> It was quite literally the happiest day of my life . I feel like the luckiest guy in the world , not only because I have her , but also because this incredible PyImageSearch community has been so supportive over the past 3 years . Thank you for being on this journey with me . <p> And just like love and marriage have a set of building blocks , so do machine learning and neural network classifiers . <p> Over the past few weeks we opened our discussion of machine learning and neural networks with- an introduction to linear classification that discussed the concept of- parameterized learning , and how this type of learning enables us to define a- scoring function that maps our input data to output class labels . <p> From- there , we discussed two common loss functions : Multi-class SVM loss and cross-entropy loss ( commonly @ @ @ @ @ @ @ @ @ @ " ) . Loss functions , at the most basic level , are used to quantify how " good " or " bad " a given predictor ( i.e. , a set of parameters ) are at classifying the input data points in our dataset . <p> Given these building blocks , we can now move on to arguably the most important aspect of machine learning , neural networks , and deep learning - optimization . <h> Gradient descent is an optimization algorithm <p> We can visualize our loss landscape as a bowl , similar to the one you may eat cereal or soup out of : <p> Figure 1 : A plot of our loss landscape . We typically see this landscape depicted as a " bowl " . Our goal is to move towards the basin of this bowl where this is minimal loss . <p> The surface of our bowl is called our loss landscape , which is essentially a- plot of our loss function . <p> The difference between our loss landscape and your cereal bowl is that your cereal bowl only exists in three dimensions @ @ @ @ @ @ @ @ @ @ perhaps tens , hundreds , or even thousands of dimensions . <p> Each position along the surface of the bowl corresponds to a particular- loss value given our set of parameters , - W ( weight matrix ) and- b ( bias vector ) . <p> Our goal is to try different values of- W and- b , evaluate their loss , and then take a step towards more optimal values that will ( ideally ) have lower loss . <p> Iteratively repeating this process will allow us to navigate our loss landscape , following the gradient of the loss function ( the bowl ) , and find a set of parameters that have minimum loss and high classification accuracy . <h> The " gradient " in gradient descent <p> To make our explanation of gradient descent a little more intuitive , let 's pretend that we have a robot let 's name him Chad : <p> Figure 2 : Introducing our robot , Chad , who will help us understand the concept of gradient descent . <p> We place Chad on a random position in our bowl ( i.e. , the @ @ @ @ @ @ @ @ @ @ placed on a random position on the loss landscape . However , Chad has only one sensor the loss value at the- exact position he is standing at . Using this sensor ( and this sensor alone ) , how is he going to get to the bottom of the basin ? <p> Its now Chads job to navigate to the bottom of the basin ( where there- is minimum loss ) . <p> Seems easy enough , right ? All Chad has to do is orient himself such that he s facing " downhill " and then ride the slope until he reaches the bottom of the basin . <p> But we have a problem : Chad is n't a very smart robot . <p> Chad- only has one sensor this sensor allows him to take his weight matrix- W and compute a loss function- L. <p> Therefore , Chad is able to compute his ( relative ) position on the loss landscape , but he has- absolutely no idea in which direction he should take a step to move himself closer to the bottom of the basin . <p> @ @ @ @ @ @ @ @ @ @ to apply gradient descent . <p> All we need to do is follow the slope of the gradient- W. We can compute the gradient of- W across all dimensions using the following equation : <p> In practice , we use the- analytic gradient instead . This method is exact , fast , but extremely challenging to implement due to partial derivatives and multivariable calculus . You can read more about the numeric and analytic gradients here . <p> For the sake of this discussion , simply try to internalize what gradient descent is doing : attempting to optimize our parameters for low loss and high classification accuracy . <h> Pseudocode for gradient descent <p> Below I have included some Python-like pseudocode of the standard , vanilla gradient descent algorithm , inspired by the CS231n slides : <p> Gradient descent with Python <p> Python <p> 1 <p> 2 <p> 3 <p> whileTrue : <p> **31;3185;TOOLONG , data , W ) <p> W+=-alpha*Wgradient <p> This pseudocode is essentially what- all variations of gradient descent are built off of . <p> We start off on- Line 1 by looping until some condition is @ @ @ @ @ @ @ @ @ @ specified number of epochs has passed ( meaning our learning algorithm has " seen " each of the training data points- N times ) . <p> Line 2 then calls a function named evaluategradient- . This function requires three parameters : <p> loss- : A function used to compute the- loss over our current parameters- W and input data- . <p> data- : Our training data where each training sample is represented by a feature vector . <p> W- : This is actually our weight matrix that we are optimizing over . Our goal is to apply gradient descent to find a- W that yields minimal loss . <p> The evaluategradient- function returns a vector that is- K-dimensional , where- K is the number of dimensions in our feature vector. - The Wgradient- variable is actually our- gradient , where we have a gradient entry for each dimension . <p> In practice , you 'll spend a lot of time finding an optimal learning rate alpha- it is- by far the most important parameter in your model . <p> If alpha- is too large , well end up spending all our @ @ @ @ @ @ @ @ @ @ descending " to the bottom of our basin ( unless our random bouncing- takes us there by pure luck ) . <p> Conversely , if alpha- is too small , then it will take- many- ( perhaps prohibitively many ) iterations to reach the bottom of the basin . <p> Because of this , alpha- will cause you many headaches and youll spend a considerable amount of your time trying to find an optimal value for your classifier and dataset . <h> Implementing gradient descent with Python <p> Now that we know the basics of gradient descent , let 's implement gradient descent in Python and use it to classify some data . <p> Open up a new file , name it gradientdescent.py- , and insert the following code : 8 <p> 9 <p> 10 55203 @qwx675203 <p> importmatplotlib.pyplot asplt <p> **37;3218;TOOLONG importmakeblobs 55220 @qwx675220 55218 @qwx675218 <p> defsigmoidactivation(x) : <p> # compute and return the sigmoid activation value for a <p> # given input value <p> return1.0/ ( @ @ @ @ @ @ @ @ @ @ . <p> We then define the sigmoidactivation- function on- Line 7 . When plotted , this function will resemble an " S " -shaped curve : <p> We call this an activation function because the function will " activate " and fire " ON " ( output value &gt;= 0.5 ) or " OFF " ( output vale &lt; 0.5 ) based on the inputs x- . <p> While there are other ( better ) alternatives to the sigmoid activation function , it makes for an excellent " starting point " in our discussion of machine learning , neural networks , and deep learning . <p> Ill also be discussing activation functions in more detail in a future blog post , so for the time being , simply keep in mind that this is a non-linear activation function that we can use to " threshold " our predictions . <p> Next , let 's parse our command line arguments : <p> Gradient descent with Python <p> Python <p> 12 <p> 13 <p> 14 <p> 15 <p> 16 <p> 17 <p> 18 55202 @qwx675202 55206 @qwx675206 <p> LONG ... <p> help= " @ @ @ @ @ @ @ @ @ @ " learning rate " ) 55208 @qwx675208 <p> We can provide two ( optional ) command line arguments to our script : <p> --epochs- : The number of epochs that well use when training our classifier using gradient descent . <p> Now that our command line arguments are parsed , let 's generate some data to classify : <p> Gradient <p> 36 <p> 37 <p> # generate a 2-class classification problem with 250 data points , <p> # where each data point is a 2D feature vector <p> LONG ... <p> clusterstd=1.05 , randomstate=20 ) <p> # insert a column of 1 's as the first entry in the feature <p> # vector -- this is a little trick that allows us to treat <p> # the bias as a trainable parameter *within* the weight matrix <p> # rather than an entirely separate variable <p> X=np.cnp.ones ( ( X.shape0 ) ) , X <p> @ @ @ @ @ @ @ @ @ @ number of <p> # columns as our input features <p> print ( " INFO starting training ... " ) <p> **25;3257;TOOLONG ( X.shape1 , ) ) <p> # initialize a list to store the loss value for each epoch <p> lossHistory= <p> On- Line 22 we make a call to makeblobs- which generates 250 data points . These data points are 2D , implying that the " feature vectors " are of length- 2 . <p> Furthermore , 125 of these data points belong to- class 0 and the other 125 to- class 1. - Our goal is to train a classifier that correctly predicts each data point as being- class 0 or- class 1 . <p> Line 29 applies a neat little trick that allows us to skip- explicitly keeping track of our bias vector- b . To accomplish this , we insert a brand new column of- 1s as the first entry in our feature vector . This addition of a column containing a constant value across- all feature vectors allows us to treat our bias as a- trainable parameter that is- within the weight matrix- W rather @ @ @ @ @ @ @ @ @ @ about this trick here and here . <p> Line 34 ( randomly ) initializes our weight matrix such that it has the same number of dimensions as our input features . <p> Its also common to see both- zero and- one weight initialization , but I tend to prefer random initialization better . Weight initialization methods will be discussed in further detail inside future neural network and deep learning blog posts . <p> Finally , - Line 37 initializes a list to keep track of our loss after each epoch . At the end of our Python script , well plot the loss which should ideally decrease over time . <p> All of our variables are now initialized , so we can move on to the actual training and gradient descent procedure : <p> Gradient <p> 56 <p> 57 <p> # loop over the desired number of epochs <p> forepoch innp.arange @ @ @ @ @ @ @ @ @ @ # take the dot product between our features X and the <p> # weight matrix W , then pass this value through the <p> # sigmoid activation function , thereby giving us our <p> # predictions on the dataset <p> **29;3284;TOOLONG ( W ) ) <p> # now that we have our predictions , we need to determine <p> # our error , which is the difference between our predictions <p> # and the true values <p> error=preds-y <p> # given our error , we can compute the total loss value as <p> # the sum of squared loss -- ideally , our loss should <p> # decrease as we continue training <p> loss=np.sum(error**2) <p> lossHistory.append(loss) <p> print ( " INFO epoch # , loss= : .7f " . format ( epoch+1 , loss ) ) <p> On- Line 40 we start looping over the supplied number of --epochs- . By default , well allow our training procedure to " see " each of the training points a total of 100 times ( thus , 100 epochs ) . <p> Now that we have our error- , we can compute @ @ @ @ @ @ @ @ @ @ matrix W- : <p> Gradient <p> 67 <p> 68 <p> # the gradient update is therefore the dot product between <p> # the transpose of X and our error , scaled by the total <p> # number of data points in X <p> **32;3315;TOOLONG <p> # in the update stage , all we need to do is nudge our weight <p> # matrix in the negative direction of the gradient ( hence the <p> # term " gradient descent " by taking a small step towards a <p> # set of " more optimal " parameters <p> W+=-args " alpha " *gradient <p> Line 62 handles computing the actual gradient , which is the dot product between our data points X- and the error- . <p> Line 68 is the most critical step in our algorithm and where the actual gradient descent takes place . Here we update our weight matrix W- by taking a --step- in the negative- direction of the gradient , thereby allowing us to move @ @ @ @ @ @ @ @ @ @ ( hence the term , - gradient descent ) . <p> After updating our weight matrix , we keep looping until the desired number of epochs has been met gradient descent is thus an- iterative algorithm . <p> To actually demonstrate how we can use our weight matrix- W as a classifier , take a look at the following code block : <p> Gradient <p> 84 <p> 85 <p> # to demonstrate how to use our weight matrix as a classifier , <p> # let 's look over our a sample of training examples <p> **30;3349;TOOLONG : <p> # compute the prediction by taking the dot product of the <p> # current feature vector with the weight matrix W , then <p> # passing it through the sigmoid activation function <p> **35;3381;TOOLONG ( W ) ) <p> # the sigmoid function is defined over the range y=0 , 1 , <p> # so we can use 0.5 @ @ @ @ @ @ @ @ @ @ 0.5 , it 's class 0 ; otherwise it 's class 1 <p> **31;3418;TOOLONG <p> # show our output classification <p> print ( " activation= : .4f ; predictedlabel= , truelabel= " . format ( <p> activation , label , yi ) ) <p> We start on on- Line 72 by looping over a sample of our training data . <p> For each training point Xi- we compute the dot product between Xi- and the weight matrix W- , then feed the value through our activation function . <p> As you can see , our decision boundary starts off widely inaccurate due to the random initialization . But as time passes , we are able to- apply gradient descent , update our weight matrix- W , and eventually learn an accurate model . <h> Want to learn more about gradient descent ? <p> In the meantime , if you want to learn more about gradient descent , you should absolutely refer to Andrew Ngs gradient descent lesson in the Coursera Machine Learning course . <p> I would also recommend Andrej Karpathys excellent slides from the CS231n course . <h> Summary @ @ @ @ @ @ @ @ @ @ , a first-order optimization algorithm that can be used to learn a set of parameters that will ( ideally ) obtain low loss and high classification accuracy on a given problem . <p> I then demonstrated how to implement a basic gradient descent algorithm using Python. - Using this implementation , we were able to actually- visualize how gradient descent can be used to learn and optimize our weight matrix- W. <p> In next weeks blog post , I 'll be discussing a modification to the vanilla gradient descent implementation called- Stochastic Gradient Descent- ( SGD ) . The SGD flavor of gradient descent is more commonly used than the one we introduced today , but I 'll save a more thorough discussion for next week . <p> See you then ! <p> Before you go , be sure to use the form below to sign up for the PyImageSearch Newsletter you 'll then be notified when future blog posts are published . <h> Downloads : 55217 @qwx675217 <p> Thanks for the informative post and the link to the slides , and I can totally recommend the machine learning course by andrew ng @ @ @ @ @ @ @ @ @ @ to different machine learning techniques . The hands on approach with the homework is worth every minute spent on it . <p> Fascinating ! I love gradient descent studies like this one . I love Adrians teaching style . And mostly , love that you shared the news with us ! Congratulations you two , many blessings and prayers coming at you ! <p> In order to draw the decision boundary , you need to draw only the points ( x , y ) which lie right over the boundary . <p> According to the sigmoid function , the boundary is the value 0.5 . So , in order to obtain a 0.5 , you need to provide a zero value as input to the sigmoid ( That is , a zero value as output from the scoring function ) . <p> Thus , if the scoring function equals zero : <p> 0 = w0 + w1*x + w2*y ==&gt; y = ( -w0 w1*x ) /w2 <p> You can use any xs coordinates you want , and you 'll get the proper ys coordinates to draw the boundary <p> Thank @ @ @ @ @ @ @ @ @ @ confused though . When calculating the gradient , we try to minimize the loss function , which means we need to take the derivative of the loss function . The loss function is the sum of the square of the errors , with error being defined as the actual label minus the predicted label . Here the predicted labels are calculated using Sigmoid function . This means the gradient will include the gradient of the Sigmoid function , but here I see the gradient of a linear predictor function . Could you elaborate more on what has been @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485117 @185117/ <h> Tag Archive <p> In our previous lesson , we covered the basics of CNNs- including layer types , ordering patterns , and common network architectures . In this lesson , well use the Keras Python package to define our very first CNN . This network will be very simple , consisting of only an INPUT- layer , a single CONV=&gt;RELU- layer , and an output softmax classifier ( a generalization of Logistic Regression used Read More <p> We 've spent a decent amount of time discussing the image classification in this module . Weve learned about the challenges . The types of learning algorithms we can use . And even the general pipeline that is used to build- any image classifier . But we have yet to- really build an image classifier of our own . Today , that is all going to change . Were going Read More <p> Our last lesson discussed how to use Redis to build an inverted index to facilitate faster ( and more scalable ) queries to our CBIR system . This lesson will demonstrate how to use our inverted index to perform @ @ @ @ @ @ @ @ @ @ you 'll have a fully functioning CBIR system that you can use to scale to very large Read More <p> Unlike face detection , which is the process of simply- detecting the presence- of a face in an image or video stream , - face recognition takes the faces detected from the localization- phase and attempts to identify whom the face belongs to. - Face recognition can thus be thought of as a method of- person identification , - which we use heavily in security and surveillance systems . Since face recognition , by definition , Read More <p> In our previous lesson , we got our first taste of running computer vision jobs on the Hadoop library using the MapReduce paradigm , Python , and the Hadoop Streaming API. - We also defined a- reusable project structure that can be utilized anytime we need to construct a MapReduce job . To demonstrate the utility of our project structure , today we are going to take another Read More <p> In the previous set of lessons in this module , we learned how to train various Convolutional Neural Network ( CNN @ @ @ @ @ @ @ @ @ @ MiniVGGNet , on the CIFAR-10 dataset . Each of these networks ( including even- ShallowNet ) were able to obtain over 50% classification accuracy , with MinIVGGNet obtaining the highest classification accuracy of 75.03% . While we have examined how to train networks using batches of Read More <p> In this project , well learn how to perform face recognition on the Raspberry Pi 2 hardware and create a simple security system that can send us- text message alerts when intruders enter our video stream . To accomplish this project , well be using the following : Our knowledge ( and not to mention , implementations ) from the PyImageSearch Gurus modules on- face recognition . Amazon Simple Storage Read More <p> If you 've ever read my book , - Practical Python and OpenCV + Case Studies , you 'll know that I really enjoy performing object detection/tracking using color-based methods . While it does not work in all situations , if you are able to define the object you want to track- in terms of color , you can enjoy : A highly simplified codebase . Super fast object tracking @ @ @ @ @ @ @ @ @ @ saying , - " Youll want to pay attention to this lesson . " The bag of visual words ( BOVW ) model is one of the- most important concepts in all of computer vision . We use the bag of visual words model to classify the contents of an image . Its used to build- highly scalable ( not to mention , - accurate ) - CBIR systems . We even use the bag Read More 
@@71485119 @185119/ <h> Archive Deep Learning <p> The purpose of this blog post is to demonstrate how to install the Keras library for deep learning . The installation procedure will show how to install Keras : With GPU support , so you can leverage your GPU , CUDA Toolkit , cuDNN , etc. , for faster network training . Without- GPU support , so even if you do not have a GPU <p> Alight , so you have the NVIDIA CUDA Toolkit and cuDNN library installed on your GPU-enabled system . What next ? Let 's get OpenCV installed with CUDA support as well . While OpenCV- itself does n't  play a critical role in deep learning , it- is used by- other deep learning libraries such as Caffe , specifically in " utility " programs ( such as building a dataset <p> If you 're serious about doing any type of deep learning , you should be utilizing your- GPU rather than your- CPU. - And the more GPUs you have , the better off you are . If you already have an NVIDIA supported GPU , then the next logical step @ @ @ @ @ @ @ @ @ @ Toolkit : - A development environment for building <p> So you 're interested in deep learning and Convolutional Neural Networks . But where do you start ? Which library do you use ? There are just so many ! Inside this blog post , I detail 9 of my- favorite Python deep learning libraries . This list is- by no means- exhaustive , its- simply a list of libraries that Ive used in my computer vision <p> In last weeks blog post , I discussed my investment in an NVIDIA DIGITS DevBox for deep learning . It was quite the investment , weighing in at a staggering ( and wallet breaking ) $15,000- more than I ever thought I would spend on a computer that lives in my office ( I normally like hardware that exists in the <p> I 've got a big announcement today : I will be doing more Deep Learning and Convolutional Neural Network tutorials on the PyImageSearch blog over the coming months . I 'm dead serious about this - and I 've put my money where my mouth is and- invested in some real hardware for deep learning @ @ @ @ @ @ @ @ @ @ A few weeks ago- I introduced bat-country , my implementation of a lightweight , extendible , easy to use Python package for deep dreaming and inceptionism . The reception of the library was very good , so- I decided that it would be interesting to do a follow up post but instead of generating some really trippy images like on the <p> One of the main benefits of the bat-country Python package for deep dreaming and visualization is its ease of use , extensibility , and customization . And let me tell you , that customization really came in handy last Friday- when the Google Research team- released an update to their deep dream work , demonstrating a method to " guide " your input images <p> We cant stop here , this is bat country . Just a few days ago , the Google Research blog published a post demonstrating a unique , interesting , and perhaps even disturbing method to visualize what 's going inside the layers of a Convolutional Neural Network ( CNN ) . Note : - Before you go , I suggest taking a look at @ @ @ @ @ @ @ @ @ @ post- detailing- my experience with CUDAMat , Deep Belief Networks , and Python using my MacBook Pro . The post is fairly long and full of screenshots to document my experience . But the gist of it is this : Even after installing the NVIDIA Cuda SDK and configuring CUDAMat , my CPU @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485120 @185120/ <h> Tag Archives deep learning <p> In previous tutorials , I 've discussed two important loss functions : - Multi-class SVM loss and- cross-entropy loss ( which we usually refer to in conjunction with Softmax classifiers ) . In order to to keep our discussions of these loss functions straightforward , I purposely left out an important component : - regularization . While our loss function allows us to determine how well ( or poorly ) our <p> Last week , we discussed Multi-class SVM loss ; specifically , the hinge loss and squared hinge loss functions . A loss function , in the context of Machine Learning and Deep Learning , allows us to quantify how " good " or " bad " a given classification function ( also called a " scoring function " ) is at correctly classifying data points in our dataset . However , <p> A couple weeks ago , we discussed the concepts of both- linear classification- and- parameterized learning . This type of learning allows us to take a set of input data and class labels , and actually learn a function that- @ @ @ @ @ @ @ @ @ @ defining a set of parameters and optimizing over them . Our linear classification tutorial focused <p> Normally , I only publish blog posts on Monday , - but I 'm so excited about this one that it could n't wait and I decided to hit the publish button early . You see , just a few days ago , - Fran+ois Chollet pushed three Keras models ( VGG16 , VGG19 , and ResNet50 ) online these networks- are- pre-trained- on the ImageNet dataset , meaning that they can recognize- 1,000 <p> In todays blog post , we are going to- implement our first Convolutional Neural Network ( CNN ) - LeNet - using Python and the Keras deep learning package . The LeNet architecture was first introduced by LeCun et al . in their 1998 paper , - Gradient-Based Learning Applied to Document Recognition. - As the name of the paper suggests , the authors implementation of LeNet was used <p> I 'm going to start todays blog post by asking a series of questions which will then be addressed later in the tutorial : What are image convolutions ? What do they @ @ @ @ @ @ @ @ @ @ we apply them ? And what role do convolutions play in deep learning ? The word " convolution " sounds like a <p> The purpose of this blog post is to demonstrate how to install the Keras library for deep learning . The installation procedure will show how to install Keras : With GPU support , so you can leverage your GPU , CUDA Toolkit , cuDNN , etc. , for faster network training . Without- GPU support , so even if you do not have a GPU <p> Alight , so you have the NVIDIA CUDA Toolkit and cuDNN library installed on your GPU-enabled system . What next ? Let 's get OpenCV installed with CUDA support as well . While OpenCV- itself does n't  play a critical role in deep learning , it- is used by- other deep learning libraries such as Caffe , specifically in " utility " programs ( such as building a dataset <p> If you 're serious about doing any type of deep learning , you should be utilizing your- GPU rather than your- CPU. - And the more GPUs you have , the better off @ @ @ @ @ @ @ @ @ @ GPU , then the next logical step is to install two important libraries : The NVIDIA CUDA Toolkit : - A development environment for building <p> So you 're interested in deep learning and Convolutional Neural Networks . But where do you start ? Which library do you use ? There are just so many ! Inside this blog post , I detail 9 of my- favorite Python deep learning libraries . This list is- by no means- exhaustive , its- simply a list of libraries that Ive @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485122 @185122/ <h> A simple neural network with Python and Keras <p> To start this post , well quickly review the most common neural network architecture feedforward networks . <p> Well then write some Python code to define our feedforward neural network and specifically apply it to the Kaggle Dogs vs . Cats classification challenge . The goal of this challenge is to correctly classify whether a given image contains a- dog or a- cat . <h> Feedforward neural networks <p> While there are many , - many different neural network architectures , the most common architecture is the- feedforward network : <p> Figure 1 : An example of a feedforward neural network with 3 input nodes , a hidden layer with 2 nodes , a second hidden layer with 3 nodes , and a final output layer with 2 nodes . <p> In this type of architecture , a connection between two nodes is- only permitted from nodes in layer- i to nodes in layer- i + 1 ( hence the term- feedforward ; there are no backwards or inter-layer connections allowed ) . <p> Furthermore , the nodes in @ @ @ @ @ @ @ @ @ @ i + 1 . This implies that every node in layer- i connects to every node in layer- i + 1 . For example , in the figure above , there are a total of- 2 x 3 = 6 connections between layer 0 and layer 1 this is where the term " fully connected " or " FC " for short , comes from . <p> We normally use a sequence of integers to quickly and concisely describe the number of nodes in each layer . <p> For example , the network above is a 3-2-3-2 feedforward neural network : <p> Layer 0 contains 3 inputs , our values . These could be raw pixel intensities- or entries from a feature vector . <p> Layer 3 is the- output layer or the- visible layer this is where we obtain the overall output classification from our network . The output layer normally has as many nodes as class labels ; one node for each potential output . In our Kaggle Dogs vs . Cats example , we have two output nodes one for " dog " and another for " @ @ @ @ @ @ @ @ @ @ Python and Keras <p> Now that we understand the basics of feedforward neural networks , let 's implement one for image classification using Python and Keras . <p> To start , you 'll want to follow this tutorial to ensure you have Keras and the associated prerequisites installed on your system . <p> From there , open up a new file , name it simpleneuralnetwork.py- , and well get coding : <p> A simple neural 11 <p> 12 <p> 13 55203 @qwx675203 <p> **25;3451;TOOLONG importLabelEncoder <p> **27;3478;TOOLONG importtraintestsplit <p> fromkeras.models importSequential <p> fromkeras.layers importActivation <p> fromkeras.optimizers importSGD <p> fromkeras.layers importDense <p> fromkeras.utils importnputils <p> fromimutils importpaths 55220 @qwx675220 55218 @qwx675218 <p> importcv2 <p> importos <p> We start off by importing our required Python packages . Well be using a number of scikit-learn implementations along with Keras layers and activation functions . If you do not already have your development environment configured for Keras , please see this blog post . <p> Well be also @ @ @ @ @ @ @ @ @ @ . If you do not already have imutils- installed on your system , you can install it via pip- : <p> A simple neural network with Python and Keras <p> Shell <p> 1 55204 @qwx675204 <p> Next , let 's define a method to accept and image and describe it . In previous tutorials , we 've extracted color histograms from images and used these distributions to characterize the contents of an image . <p> This time , let 's use the raw pixel intensities instead . To accomplish this , we define the imagetofeaturevector- - function which accepts an input image- and resizes it to a fixed size- , ignoring the aspect ratio : <p> A simple neural network with Python and Keras <p> Python <p> 15 <p> 16 <p> 17 <p> 18 <p> LONG ... <p> # resize the image to a fixed size , then flatten the image into <p> # a list of raw pixel intensities <p> returncv2.resize ( image , size ) . flatten() <p> We resize our image- to fixed spatial dimensions to ensure each and every image in the input dataset has the same " feature @ @ @ @ @ @ @ @ @ @ our neural network each image must be represented by a vector . <p> In this case , we resize our image to- 32 x 32 pixels and then flatten the- 32 x 32 x 3- image ( where we have three channels , one for each Red , Green , and Blue channel , respectively ) into a- 3,072-d feature vector . <p> The next code block handles parsing our command line arguments and taking care of a few initializations : <p> A simple neural network with 30 <p> 31 <p> 32 55202 @qwx675202 55206 @qwx675206 <p> LONG ... <p> help= " path to input dataset " ) 55208 @qwx675208 <p> # grab the list of images that we 'll be describing <p> print ( " INFO describing images ... " ) <p> LONG ... <p> # initialize the data matrix and labels list <p> data= <p> labels= <p> We only need a single switch here , --dataset- , which is the path to @ @ @ @ @ @ @ @ @ @ images . This dataset can be downloaded from the official Kaggle Dogs vs . Cats competition page . <p> Now that we have our imagePaths- , we can loop over them individually , load them from disk , convert the images to feature vectors , and the update the data- and labels- lists : <p> A simple neural network with <p> 48 <p> 49 <p> # loop over the input images <p> for ( i , imagePath ) inenumerate(imagePaths) : <p> # load the image and extract the class label ( assuming that our <p> # path as the format : /path/to/dataset/class. imagenum.jpg <p> **27;3507;TOOLONG <p> LONG ... <p> # construct a feature vector raw pixel intensities , then update <p> # the data matrix and labels list <p> **36;3536;TOOLONG <p> data.append(features) <p> labels.append(label) <p> # show an update every 1,000 images <p> ifi&gt;0andi%1000==0 : <p> print ( " INFO processed / " . format ( i @ @ @ @ @ @ @ @ @ @ the flattened- 32 x 32 x 3 = 3,072-d representations of every image in our dataset . However , before we can train our neural network , we first need to perform a bit of preprocessing : <p> A simple neural network with <p> 65 <p> 66 <p> # encode the labels , converting them from strings to integers <p> le=LabelEncoder() <p> **30;3574;TOOLONG <p> # scale the input image pixels to the range 0 , 1 , then transform <p> # the labels into vectors in the range 0 , numclasses -- this <p> # generates a vector for each label where the index of the label <p> # is set to 1 and all other entries to 0 <p> **25;3606;TOOLONG <p> **35;3633;TOOLONG , 2 ) <p> # partition the data into training and testing splits , using 75% <p> # of the data for training and the remaining 25% for testing <p> print ( " INFO @ @ @ @ @ @ @ @ @ @ data , labels , testsize=0.25 , randomstate=42 ) <p> Lines 59 and 60- handle scaling the input data to the range- 0 , 1 , followed by converting the labels- from a set of integers to a set of vectors ( a requirement for the cross-entropy loss function we will apply when training our neural network ) . <p> We then construct our training and testing splits on- Lines 65 and 66 , using 75% of the data for training and the remaining 25% for testing . <p> On my Titan X GPU , the entire process of feature extraction , training the neural network , and evaluation took a total of- 1m 15s with each epoch taking less than 0 seconds to complete . <p> At the end of the 50th epoch , we see that we are getting- 76% accuracy on the training data and- 67% accuracy on the testing data . <p> This 9% difference in accuracy implies that our network is overfitting a bit ; however , it is very common to see 10% gaps in training versus testing accuracy , especially if you have limited @ @ @ @ @ @ @ @ @ @ worried regarding overfitting when your training accuracy reaches 90%+ and your testing accuracy is substantially lower than that . <p> In either case , this- 67.376%- is the- highest accuracy we 've obtained thus far in this series of tutorials . As well find out later on , we can easily obtain &gt; 95% accuracy by utilizing Convolutional Neural Networks . <h> Summary <p> In todays blog post , I demonstrated how to train a simple neural network using Python and Keras . <p> Starting next week , I 'll begin discussing optimization methods such as gradient descent and Stochastic Gradient Descent ( SGD ) . Ill also include a tutorial on backpropagation to help you understand the inner-workings of this important algorithm . <p> Before you go , be sure to enter your email address in the form below to be notified when future blog posts are published you wo n't want to miss them ! <h> Downloads : 55217 @qwx675217 <p> Absolutely . Keep in mind that neural networks are stochastic algorithms meaning there is a level of randomness involved with them ( specifically the weight initializations ) . Its totally @ @ @ @ @ @ @ @ @ @ . <p> Hi , wonderful post ! I have a question how did you manage to pick your parameters ( including the NN scheme ) ? No matter what I did ( and I did a lot including adding 2 more NN levels , adding dropout , changeling the SGD parameters and all other parameters ) , I did n't  manage to get more than your 67% . Especially I wonder why adding more levels and increasing the depth of each , did n't  contribute to my score ( but as expected contribute to my run time ; - ) ) Only when I increased the resolution to 64+64 , and the depth of the 2 NN levels , I manage to get 68% , and I wonder why it is so low . <p> Hey Gilad as the blog post states , I determined the parameters to the network using hyperparameter tuning . <p> Regarding the accuracy , keep in mind that this is a simple feedforward neural network . 68% accuracy is actually quite good for only considering the raw pixel intensities . And again , as the @ @ @ @ @ @ @ @ @ @ architecture ( i.e. , Convolutional Neural Networks ) to obtain higher accuracy . Ill be covering how to apply CNNs to the Dogs vs . Cats dataset in a future blog post . In the meantime , I would suggest reading this blog post on MNIST + LeNet to help you get started with CNNs . <p> Yes , absolutely awesome Adrian , i am already totally eager for a simple convolutional neural network . I love your blog = Been following it for a year now . Keep up the great work . Btw , i did this simple neural network on a raspberry Pi 2 and FYI it took almost 5 hours = <p> Keras can use either Theano or TensorFlow as a backend its really your choice . I personally like using Keras because it adds a layer of abstraction over what would otherwise be a lot more code to accomplish the same task . In future blog posts I 'm planning on continuing using Keras , but I 'll also consider the " nitty-gritty " with TensorFlow as well ! <p> Thank you for awesome tutorial . I @ @ @ @ @ @ @ @ @ @ your ( seemingly ) OSX machine . I see " ssh " in the top of the terminal window figure , and I guess that you access other ( probably linux ) machine with GPU from your OSX machine via ssh . Then , do you have any plan to post about that process ? It would be much helpful if I ( and other readers ) could use GPU in other machine from OSX machine . Thank you again . <p> You are correct , Yunhwan I am sshing into my Ubuntu GPU box and then running any scripts over the SSH session . Does that help clarify your question ? If you are looking to learn more about SSH and how to SSH into machines I would suggest reading up on SSH basics . <p> Hi i am training an an ARM based device 4 cores 1GB RAM but i am getting a memory error when running the script it gets up to processing 24,000 images and crashes on a memory error but there is still 100MB of free space what am I doing wrong and how do @ @ @ @ @ @ @ @ @ @ , so I 'm not sure about the specifics . Debian Jessie seems like it would work just fine ; however , I do n't  have any experience with the Odroid so I 'm not sure what the exact problem would be . Again , we typically do n't  train networks on such small devices only deploy them if memory allows . <p> Your class labels are not getting encoding properly . 99% of the time this is due to invalid paths to your training images . Double-check the path to the training images and ensure its correct . Also make sure you are not also using the paths to the Kaggle testing data as these filenames do not have " dog " or " cat " in them . <p> Nice post really love the work ! I just have a question regarding the feedforward idea . From my understanding , feedforward network uses delta rule to learn , and does not backpropagate . How is this specified using Keras ? If you were to write this feedforward Keras code using backpropagation , how would it be different ? <p> I @ @ @ @ @ @ @ @ @ @ multi-layer feedforward networks . The Perceptron uses the delta rule to learn while multi-layer feedforward networks use backpropagation . If you 're interested in learning more about these algorithms , how to train neural networks , and even build Convolutional Neural Networks that can understand the contents of an image , be sure to take a look at Deep Learning for Computer Vision with Python . <p> Sure = so in the data set of dogs and cats there is the training data that is labeled either a cat or a dog and its corresponding image number . Using this data we train and test our model ( correct me if i 'm wrong anywhere ) . Once this is done , model is trained and tested for accuracy , we could use it to predict if an image is a cat or a dog . So at kaggles site there is a set of images that you can download that is a mix of cats and dogs but minus the label of a cat or a dog . Its simply just numbered images . So how do we take that data @ @ @ @ @ @ @ @ @ @ images are cats or dogs . I want to use the model now to do actual predictions . Thanks for the prompt response . <p> I am having trouble getting the SGD algorithm to converge . The algorithm generally does well and decreases the loss , but sometimes ( generally after a few epochs ) the loss explodes in a few steps ( by a factor of 10 or so ) and does not recover . It does not simply seem to be fluctuations from navigating local minima of the objective function , it seems that there is something pathological going on . Which is bizzare because I am using the same code and hyperparameters . <p> The . predict method will return a NumPy with shape ( N , M ) where N is the total number of data points passed into . predict and M is your total number of class labels . You can use the np.argmax function to find the index with the largest class @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485123 @185123/ <h> Tag Archives contours <p> Todays blog post is a followup to a tutorial I did a couple of years ago on- finding the brightest spot in an image . My previous tutorial assumed there was only- one bright spot in the image that you wanted to detect but what if there were multiple bright spots ? If you want to detect more than <p> A few weeks ago , I demonstrated how to order the- ( x , y ) -coordinates of a rotated bounding box in a clockwise fashion an extremely useful skill that is critical in many computer vision applications , including ( but not limited to ) perspective transforms and computing the dimensions of an object in an image . One PyImageSearch reader emailed in , <p> This is the final post in our three part series on shape detection and analysis . Previously , we learned how to : Compute the center of a contour Perform shape detection &amp; identification Today we are going to perform both- shape detection and- color labeling on objects in images . At this point , we understand that @ @ @ @ @ @ @ @ @ @ post in our three part series on- shape detection and analysis . Last week we learned how to compute the center of a contour using OpenCV . Today , we are going to leverage contour properties to actually- label and identify shapes in an image , just like in the figure- at the top of this post . <p> Today , we are going to start a new 3-part series of tutorials on- shape detection and analysis . Throughout this series , well learn how to : Compute the center of a contour/shape region . Recognize various shapes , such as circles , squares , rectangles , triangles , and pentagons using only contour properties . Label the color of a shape . While todays post is <p> I 'm going to start this post by clueing you in on a piece of personal history that very few people know about me : as a kid in early high school , I used to spend nearly- every single Saturday at the local RC ( Remote Control ) track about 25 miles from my house . You see , I used to @ @ @ @ @ @ @ @ @ @ exposed to contours pretty heavily on the PyImageSearch blog . We used contours to build a kick-ass mobile document scanner . Contours enabled us detect barcodes in images . And we even leveraged the power of contours to find the distance from a camera to object or marker . But there <p> A couple of days ago , Cameron , a PyImageSearch reader emailed in and asked about methods to find the distance from a camera to an object/marker in an image . He had spent some time researching , but had n't  found an implementation . I knew exactly how Cameron felt . Years ago I was working on a small project to <p> Before we dive into this post , let 's take a second and talk about Oscar , a dedicated PyImageSearch reader . He is just getting started in computer vision and he s taken the best possible route to mastering the subject : creating your own projects and solving them . Oscar picked up a copy of my book , - Practical Python and <p> Quick question . How does a Pokedex work ? Well , @ @ @ @ @ @ @ @ @ @ its physical characteristics , and the Pokemon is identified instantly . Looking for the source code to this post ? Jump right to the downloads section . In this case , our smartphone camera is our " Pokedex " . We @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485124 @185124/ <h> What is face recognition ? <p> What is face recognition ? <p> Unlike face detection , which is the process of simply- detecting the presence- of a face in an image or video stream , - face recognition takes the faces detected from the localization- phase and attempts to identify whom the face belongs to. - Face recognition can thus be thought of as a method of- person identification , - which we use heavily in security and surveillance systems . <p> Since face recognition , by definition , requires face detection , we can- think of face recognition as a two-phase process . <p> Phase #1 : Detect the presence of faces in an image or video stream using methods such as Haar cascades , HOG + Linear SVM , deep learning , or any other algorithm that can localize faces . <p> Phase #2 : Take each of the faces detected during the localization phase and identify each of them this is where we actually assign a- name to a face . <p> In the remainder of this lesson well review a quick history of face @ @ @ @ @ @ @ @ @ @ algorithms well be covering in this module : - Eigenfaces and- LBPs for face recognition . <h> Objectives : <p> In this lesson we will : <p> Provide a brief history of face recognition . <p> Introduce the Eigenfaces and LBPs for face recognition algorithms . <h> What is face recognition ? <p> Face recognition is the process of taking a face in an image and actually identifying who the face belongs to . Face recognition is thus a form of- person identification . <p> Early face- recognition systems relied on- facial landmarks- extracted from images , such as the relative position and size of the eyes , nose , cheekbone , and jaw . However , these systems were often highly subjective and prone to error since these quantifications of the face were- manually extracted by the computer scientists and administrators running the face recognition software . <p> More recent face recognition systems rely on- feature extraction and- machine learning to train classifiers to identify faces in images . Not only are these systems non-subjective , but they are also- automatic no hand labeling of the face is required @ @ @ @ @ @ @ @ @ @ our classifier , and then use it to identify subsequent faces . <h> A brief history of face recognition <p> In the history of computer vision , face recognition can actually be seen as a relatively " new " concept . <p> Prior to the work of Goldstein , et al . in the 1970s , face recognition was often regarded as science fiction , sequestered to the movies and books set in ultra-future times . In short , face recognition was a fantasy , and whether or not- it would become a reality was unclear . <p> This all changed in 1971 when Goldstein , et al . published- Identification of human faces . A crude first attempt at face identification , this method proposed 21 subjective facial- features , such as hair color and - lip thickness , to identify a face in a photograph . <p> The largest drawback of this approach was that the 21 measurements ( besides being highly subjective ) were- manually computed an obvious flaw in a computer science community that was rapidly- approaching unsupervised computation and classification ( at least in terms @ @ @ @ @ @ @ @ @ @ able to demonstrate that a standard linear algebra technique for dimensionality reduction called Principal Component Analysis- ( PCA ) - could be used to identify a face using a feature vector smaller than 100-dim . Furthermore , the " principal components " ( i.e. , the eigenvectors , or the " eigenfaces " ) could be used to actually- reconstruct faces from the original dataset . This- implies that a face could be represented ( and eventually identified ) as a- linear combination of the eigenfaces : <p> Following the work- of Kirby and Sirovich , further research in face recognition exploded we now see other linear algebra techniques such as Linear Discriminant Analysis being used for- face recognition . These are commonly known as- Fisherfaces . <p> Feature-based approaches such as Local Binary Patterns for face recognition- have also been proposed and are still heavily used in real-world applications . <p> We are even starting to see deep learning applied in face identification , - but normally for face alignment and funneling , a pre-processing step that takes place- before the face is actually identified . <h> Eigenfaces <p> @ @ @ @ @ @ @ @ @ @ to a dataset of faces and extracting the 16 " eigenfaces " with the largest corresponding eigenvalue magnitude . <p> This involves collecting a dataset of faces with- multiple face images per person we want to identify like having multiple- training examples- of an image class we would want to label in- image classification . Given this dataset of face images ( presumed to be the same width , - height , and ideally with their eyes and facial structures aligned at the same- ( x , y ) -coordinates , we apply an eigenvalue decomposition of the dataset , keeping the eigenvectors with the largest corresponding eigenvalues . <p> Given these eigenvectors , a face can then be represented as a linear combination of what Kirby and Sirovich call eigenfaces . <p> Face identification can be performed by computing the Euclidean distance between the eigenface representations and treating the face identification as a k-Nearest Neighbor classificationproblem- however , we tend to- commonly apply more advanced machine learning algorithms to the eigenface representations . <p> If you 're feeling a bit overwhelmed by the linear algebra terminology or how the Eigenfaces @ @ @ @ @ @ @ @ @ @ Eigenfaces algorithm in detail in our- next lesson . <h> LBPs for face recognition <p> While the Eigenfaces algorithm relies on PCA to construct a low-dimensional representation of face images , the Local Binary Patterns ( LBPs ) method relies , as the name suggests , on feature extraction . <p> We then extract a Local Binary Pattern histogram from each of the 49- cells . By dividing the image into cells we can introduce locality into our final feature vector . Furthermore , - some cells are weighted such that they contribute more to the overall representation . Cells in the corners carry less identifying facial information compared to the cells in the center of the grid ( which contain eyes , nose , and lip structures ) . - Finally , we concatenate the weighted LBP histograms from the 49- cells to form our final feature vector . <p> The actual face identification is performed by- k-NN classification- using the distance between the query image and the dataset of labeled faces - since we are comparing histograms , the distance is a better choice than the Euclidean distance @ @ @ @ @ @ @ @ @ @ are fairly straightforward algorithms for face identification , the feature-based LBP method tends to be more resilient against noise ( since it does not operate on the raw pixel intensities themselves ) and will usually yield better results . <p> Well be reviewing LBPs for face recognition in detail later in this module . <h> Summary <p> In this lesson we learned that face recognition is a two-phase process consisting of ( 1 ) face detection , and ( 2 ) identification of each detected face . <p> We also introduced two popular algorithms for face recognition : - Eigenfaces and- LBPs for face recognition . <p> In our next lesson well explore how to perform face identification using the Local Binary Patterns for face recognition- algorithm . 
@@71485125 @185125/ <p> My- Uncle John is a long haul tractor trailer truck driver . For each new assignment , he picks his load up from a local company early in the morning and then sets off on a lengthy , enduring cross-country trek across the United States that takes him- days to complete . John is a nice , outgoing guy , who carries a <p> A few weeks ago I did a blog post on how to install the dlib library on Ubuntu and macOS . Since Raspbian , the operating system that ( most ) Raspberry Pi users run is Debian-based ( as is Ubuntu ) , the- same install instructions can be used for Raspbian as Ubuntu however , there 's a catch . The Raspberry Pi 3 ships <p> In last weeks blog post , I demonstrated how to perform facial landmark detection in real-time in video streams . Today , we are going to build upon this knowledge and develop a computer vision application that is capable of- detecting and counting blinks in video streams using facial landmarks and OpenCV . To build our blink detector @ @ @ @ @ @ @ @ @ @ have been discussing- facial landmarks and the role they play in computer vision and image processing . We 've started off by learning how to detect facial landmarks in an image . We then discovered how to label and annotate- each of the facial regions , such as eyes , eyebrows , nose , mouth , and jawline . Today <p> Todays blog post is part three in our current series on facial landmark detection and their applications to computer vision and image processing . Two weeks ago I demonstrated how to install the dlib library- which we are using for facial landmark detection . Then , last week I discussed how to use dlib to actually- detect facial landmarks in <p> Last week we learned how to install and configure dlib- on our system with Python bindings . Today we are going to use dlib and OpenCV to detect- facial landmarks in an image . Facial landmarks are used to localize and represent salient regions of the face , such as : Eyes Eyebrows Nose Mouth Jawline Facial landmarks have been successfully <p> Two weeks ago I interviewed Davis King @ @ @ @ @ @ @ @ @ @ . Today I am going to demonstrate how to install dlib with Python bindings on both- macOS and- Ubuntu . I- highly encourage you to take the time to install dlib on your system over the next couple of days . Starting next week well <p> A few months ago I wrote- a tutorial on how to classify images using Convolutional Neural Networks ( specifically , VGG16 ) pre-trained on the ImageNet dataset with Python and the Keras deep learning library . The pre-trained networks inside of Keras are capable of recognizing- 1,000 different object categories , similar to objects we encounter in our day-to-day lives with high <p> In todays blog post , I interview Davis King , the creator and chief maintainer of dlib- a toolkit for real-world machine learning , computer vision , and data analysis in C++ ( with Python bindings included , when appropriate ) . I 've personally used dlib in a number of projects ( especially for object detection ) , so its quite the honor to be interviewing <p> Over the past three years running PyImageSearch.com I have received and answered tens of @ @ @ @ @ @ @ @ @ @ interested in studying computer vision , OpenCV , and deep learning . Looking back on this time , I can say that the vast majority of the questions I have answered have @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485126 @185126/ <h> Common errors using the Raspberry Pi camera module <p> Todays blog post will take a short diversion from our recent trend of Deep Learning tutorials here on the PyImageSearch blog and instead focus on a topic that I 've been receiving a ton of- emails about lately - common errors when using the Raspberry Pi camera module . <p> I want to start this post by mentioning Dave Jones , the maintainer and chief contributor to the picamera library . Dave is one of the most active open source contributors that I 've had the privilege to interact with ( and he s a hell of a nice guy too ) . <p> A few months ago , I was using the ( at the time ) latest picamera==1.11- library and was running in to a few errors . After checking the picamera- GitHub , I noticed an- Issue had been posted regarding my problem . I confirmed the existence of the bug , which Dave then sought out and fixed before the day was over , even releasing a new , updated version to the PyPI repository . <p> @ @ @ @ @ @ @ @ @ @ and OpenCV on the Raspberry Pi would n't be nearly as fun - or half- as accessible . <p> Over the past few years Ive encountered a few " common " errors when using the Raspberry Pi and the picamera- library . My goal here today is to document some of these errors so you can easily fix them . <p> In fact , most of the issues I 'm documenting here today are not real " errors " at all they are simply misunderstandings on how the picamera- library works in conjunction with your Raspberry Pi setup . <h> How to access your picamera video stream <p> If you have n't installed picamera- , this can be accomplished using pip- : <p> Common errors using the Raspberry Pi camera module <p> Shell <p> 1 <p> $pip install " picameraarray " <p> We add the array- portion to the command to ensure we can read frames as NumPy arrays , thus making the module compatible with OpenCV . <p> After pip- has finished installing picamera- , you can check the version number using the following command : <p> Common errors using the Raspberry @ @ @ @ @ @ @ @ @ @ <p> The reported version of picamera should be- at least 1.12 . <h> A quick note on Python virtual environments <p> If you 're a frequent reader of the PyImageSearch blog , you 'll know that I use Python virtual environments- a lot and because of this , you likely do as well . <p> Before we continue , take a second to see if you are using Python virtual environments by source- ing your /. profile- file and listing all the available virtual environments on your system : <p> Common errors using the Raspberry Pi camera module <p> Shell <p> 1 <p> 2 <p> $source/.profile <p> $lsvirtualenv <p> If you get an error related to the lsvirtualenv- command not being found , then you are- not utilizing Python virtual environments ( or you have potentially made a mistake editing your /. profile- file ) . If you 're not using Python virtual environments , then you can skip the next paragraph and move to the next sub-section . <p> Assuming you are using Python virtual environments , you can execute the workon- command to access each of the individual Python virtual environments @ @ @ @ @ @ @ @ @ @ PyImageSearch blog , we name our Python virtual environment cv- , short for " computer vision " : <p> Common errors using the Raspberry Pi camera module <p> Shell <p> 1 <p> $workon cv <h> A Python template for accessing your Raspberry Pi camera module <p> In order to access the picamera- video stream , I 've created a simple , extendible template which I 'll detail for you below . <p> Open up a new file , name it testvideo.py- , and insert the following code : <p> Common errors using 12 <p> 13 <p> 14 55203 @qwx675203 <p> **34;3670;TOOLONG <p> frompicamera importPiCamera <p> importtime <p> importcv2 <p> # initialize the camera and grab a reference to the raw camera capture <p> From there , we initialize our PiRGBArray- object on- Line 11 , passing in the original camera- object and then- explicitly re-stating the resolution as well . This PiRGBArray- object allows us to actually read frames from the Raspberry @ @ @ @ @ @ @ @ @ @ frames compatible with OpenCV . <p> On- Line 17 we start looping over frames captured from the camera- using the capturecontinuous- function . We pass three parameters into this method . <p> The first is rawCapture- , the format in which we want to read each frame . We then specify the format- to be bgr- since OpenCV expects image channels to be in BGR order rather than RGB . Finally , the usevideoport- boolean indicates that we are treating the stream as video . <p> Once we have the frame- , we can access the raw NumPy array via the . array- attribute ( Line 20 ) . <p> But before we can move on to the next frame , we first need to prepare our stream by calling the . truncate- method on the rawCapture- object . If you- do not- do this , your Python script will throw an error the exact error well review later in this guide . <p> Finally , if the q- key is pressed ( Lines 30 and 31 ) , we break from the loop . <p> To execute the testvideo.py- @ @ @ @ @ @ @ @ @ @ execute the following command : <p> Common errors using the Raspberry Pi camera module <p> Shell <p> 1 <p> $python testvideo.py <p> Note : If you 're using Python virtual - environments , you 'll want to use the workon- command to switch to the Python environment that has your OpenCV + picamera library installed . <p> If all goes well , you should see the Raspberry Pi video stream displayed to your feed : <p> Figure 1 : Displaying the Raspberry Pi video stream to our screen . <p> Otherwise , if you get an error keep reading. - I 've detailed the most common error messages that I run in to below . <p> **33;3706;TOOLONG **28;3741;TOOLONG raspi-config'andensure that the camera has been enabled . <p> If you are getting this error message , then you likely forgot to ( 1 ) run raspi-config- , 92 ) enable the camera , and ( 3 ) reboot your Pi . <p> If you are- still getting an error message after running raspi-config- , then your camera is likely installed incorrectly . In this case , I would suggest giving this install video a @ @ @ @ @ @ @ @ @ @ module again ( be sure to power down your Pi first ! ) <h> Truncation problems <p> Truncation errors are fairly easy to identify since they always end with the text - Incorrect buffer length forresolution- . An example of such an error message can be found below : <p> If you are getting this error message , you likely forgot to call . truncate- after you were done processing your frame. - Line 27 of our testvideo.py- script above demonstrates how to use the . truncate- method . <p> In short : Go back to your script and ensure you have called . truncate- before . capturecontinuous- is called again . <h> The picamera==1.11 and Python 3 specific error <p> The- v1.11 release of the picamera- library introduced a Python 3 specific error . It was particularly hard to diagnose , but as Dave Jones points out , the issue was due to a mis-configuration in his test suite , leading to- no Python 3 tests being performed . <p> This error is often diagnosed by seeing the text TypeError:startswith first arg must be bytes oratuple of bytes , @ @ @ @ @ @ @ @ @ @ A full example of this error message is shown below : <p> The easiest solution is to either upgrade or downgrade your picamera- module by one point version . <p> To upgrade to- v1.12 ( or whatever the current version of picamera- is ) , use the following command : <p> Common errors using the Raspberry Pi camera module <p> Shell <p> 1 <p> $pip install--upgrade picamera <p> To downgrade to- v1.10 , just use this commands : <p> Common errors using the Raspberry Pi camera module <p> Shell <p> 1 <p> 2 <p> $pip uninstall picamera <p> $pip install " picameraarray " ==1.10 <p> Make sure you are being mindful of whether you 're using Python virtual environments or not . If you are , execute the workon- command before running pip- to ensure picamera- is installed in to your virtual environment . <h> Blank and/or black- frame <p> Figure 2 : The strange blank/black frame error . <p> The blank/black frame is a particularly strange problem frames are being read from the video stream , they are just not being decoded and displayed properly . <p> First , run @ @ @ @ @ @ @ @ @ @ Raspberry Pi : <p> Common errors using the Raspberry Pi camera module <p> Shell <p> 1 <p> $sudo rpi-update <p> After your Pi reboots , try re-executing your Python script . <p> If the frame retrieved by the Raspberry Pi camera is still blank/black , then downgrade your picamera- installation to- v1.10 : <p> Common errors using the Raspberry Pi camera module <p> Shell <p> 1 <p> 2 <p> $pip uninstall picamera <p> $pip install " picameraarray " ==1.10 <p> Ive encountered situations where Ive only had to run rpi-update- to resolve the issue . And Ive also needed to both- upgrade- my firmware and downgrade- my picamera- version . I 'm not sure what the common underlying thread is ( I- think its related to using the newer version of the Raspberry Pi camera hardware + picamera- versioning , but I have n't been able to nail that down yet ) . <h> Summary <p> These errors can be quite frustrating to debug , especially if you 're just getting started . My hope is that this guide helps point you in the right direction if you run in to any of @ @ @ @ @ @ @ @ @ @ trying to track these errors down ! <p> Finally , I would like to mention again the wonderful work and contribution of Dave Jones , the maintainer and chief contributor to the picamera library . Without him , we would n't be able to have- near as much computer vision fun with our Pis ! <h> Downloads : 55217 @qwx675217 <p> I 'm using a usb webcam ( on the Beaglebone black and on a OpenCV virtual machine ) , and was having issues with the linux drivers ( GSPCA / SPCA5xx ) I found that once the driver for the webcam is loaded , getting it to work in OpenCV is straightforward . <p> So curious for those with an RPi and/or the Guru VM , does plugging in a usb webcam get recognized immediately ( i.e. /dev/video0 ) ? <p> It really depends on your webcam if it is recognized immediately . For the Logitech C920 , I know that it is automatically recognized and there is no need to probe for it . You can see a complete list of Pi compatible webcams here . <p> Another thing to @ @ @ @ @ @ @ @ @ @ camera is that the little brown pad connector on the front side of the camera module just next to the lens of the camera is firmly seated . This connector is labelled " SUNNY " in small print . <p> On a couple of cameras , mine was not seated properly which caused the errors . This is easily corrected by applying a little finger pressure to the pad until it clicks into place and sits flush . <p> Here is one s/w problem I encountered when running testvideo.py using the raspicam on a rpi-3 and it was attributed to the fact that motioneye ( variation of the program , motion ) was active . After I stopped the service ( sudo service motioneye stop ) , the video displayed just fine . This reminded me to check for any programs that are using the video device ( /dev/video0 in my case ) . <p> As you have mentioned in the page to overcome the problem should needed to update the firmware by using " sudo rpi-update " . but when i do that and reboot my pi after logging @ @ @ @ @ @ @ @ @ @ in the taskbar. what can i do ? ? ? ? ? ? <p> Hey Adrian ! its me , again.well , I solved the black screen in the video output window following this post you directed me to.However , the FPS is extremely low , maybe around 10 or so and then I can sort of see the frame being updated from the top left corner row by row.Any suggestion on that ? In short the video stream is n't satisfying at all.Waiting on your reply . <p> I 've been wanting to develop a motion detection approach for several years now , but conventional wisdom ' was saying it could n't be done without using expensive hardware and going back to college to learn everything ' . Then I read about your missing beer and everything changed for me . In less than two weeks , I went through your book , companion case studies and a number of web tutorials and now I 'm well on my way to developing a motion detection system on a Raspberry Pi that only existed on paper a month ago . <p> @ @ @ @ @ @ @ @ @ @ 30 years since I did any real programming back in university and that was using Fortran and Pascal on a VAX terminal ( look it up ) . Other than Adrian 's tutorials and some Googling , I had no exposure to Python , OpenCV or Raspberry Pi 's for that matter . For anyone considering diving in to image processing , I ca n't recommend Adrian 's books and tutorials enough . <p> That seems to be a very specific Raspberry Pi camera error , one that I have not run into before . It sounds like your camera might not be hooked up correctly OR you may have not enabled it via raspi-config . If you continue to get this error I would suggest posting on the picamera GitHub issues page . <p> Thanks for your tutorials ! ! I am still getting " Incorrect buffer length for resolution " error even though I truncate the frame at every capture , as you suggested . Could you please suggest any other ways to fix it ? Thanks , Sasha <p> I tend to get better performance out of @ @ @ @ @ @ @ @ @ @ Pi camera module . However , that is just from personal experience . The only disadvantage that I would say is that V4L2 adds in extra steps . <p> I am following your code to grab frame from camera , using raspberry pi 3 and camera module . There are 2 things I want to ask . <p> Firstly , I am getting blank white image box . I thought it was caused by " RGB " format , but I double checked that I am using " BGR " and opencv . I have downgrade pycamera to 1.10 but it did not work . When I am trying to run rpi-update , it shows " command not found " . <p> Secondly , since I am doing quite intense computation for one frame , I set the framerate to be 1 ( although the processing takes 10 seconds ) . For every loop , am I getting the latest frame or the last frame in the queue ? <p> That 's very strange that the rpi-update command is not being found . Are you using a recent version of Raspbian @ @ @ @ @ @ @ @ @ @ . <p> As for the threading , that ( realistically ) does n't  matter . Setting the framerate to 1 is fine , even if it takes longer for you to process each individual frame . The most recent frame will also be available from the streamer class . <p> I have a question that I can stream the video in another computer browser with the ip address of raspberry and I get a blank screen . I cant view the video with streaming . I am using vnc for viewing the desktop not having a monitor . So my question is , can I view the video with streaming . camera is working , that I can take pictures and can view but only the video . can any one help me ? <p> This solution worked brilliantly for me when I first setup according to Adrians recommendations . This man is truly a pioneer in the field of computer vision and to incorporate it in the $35 computer the raspberry pi is extremely commendable . I doubt the portability of bleeding-edge research and implementation for the layman in @ @ @ @ @ @ @ @ @ @ many academics . My many thanks and praise to Dr. A R. <p> My second thought is has anyone had success with either of the 8 MP cameras ? the NOIR and the IR . I recently had to resort to a backup and had an error when running my pysurveillance.py program . <p> The error did n't  offer much other than something about a " core " in the message . I assume it has to do with the new 8 MP camera I had in . Running the older camera the 6 MP had no problems at all . <p> I have come across your OpenCV knowledge in a quest to utilise SimpleCV for a process I need to work out . You seem like the man that might be able to help me with my question . I was trying to get SimpleCV to work on a raspi , so I could learn how to identify a mixed tray load of brass rifle cases , their size ( calibre and designation ) , physical orientation in a tray and physical location ( all one at a time @ @ @ @ @ @ @ @ @ @ a machine arm or similar . This will be a hobby type machine for a home user ( just me LOL ) , nothing industrial sized . I was trying to see if the actual process would work . I am a TOTAL noob at linux/raspi , ( having real trouble trying to get the camera to work with SimpleCV , driving me nuts ! ) and I was wondering if you would be able to determine whether this above stated intended application is feasible or not with this software ? It seems as though OpenCV/SimpleCV is a very powerful piece of software . If it is possible , I want to learn it properly so I can do it , and get this device underway , as I need it ! As they say , necessity is the mother of all invention . Thanks Adrian . Let me know what you think . <p> SimpleCV is a wrapper around OpenCV that makes it easier to work with . In general , I do n't  recommend spending too much time studying SimpleCV as you 'll likely end up using OpenCV in @ @ @ @ @ @ @ @ @ @ would be for you to start with OpenCV and skip SimpleCV . If you need help learning the fundamentals of OpenCV and image processing , definitely take a look at my book , Practical Python and OpenCV . <p> Regarding your specific question , if you can share a few images of the brass cases , I can help you determine if the @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485127 @185127/ <h> Archive Machine Learning <p> Normally , I only publish blog posts on Monday , - but I 'm so excited about this one that it could n't wait and I decided to hit the publish button early . You see , just a few days ago , - Fran+ois Chollet pushed three Keras models ( VGG16 , VGG19 , and ResNet50 ) online these networks- are- pre-trained- on the ImageNet dataset , meaning that they can recognize- 1,000 <p> Now that we 've had a taste of Deep Learning and Convolutional Neural Networks in last weeks blog post on LeNet , were going to take a step back and start to study machine learning in the context of image classification in more depth . To start , well reviewing the k-Nearest Neighbor ( k-NN ) classifier , arguably the- most simple , easy <p> Well . Ill just come right out and say it. - Today is my 27th birthday . As a kid I was always- super excited about my birthday . It was another year closer to being able to drive a car . Go to R rated movies @ @ @ @ @ @ @ @ @ @ , I do n't  care too much for my <p> So in last weeks blog post we discovered how to construct an image pyramid . And in todays article we are going to extend that example and introduce the concept of a sliding window . Sliding windows play an integral role in object classification , as they allow us to localize exactly- " where " in an image an object resides . <p> It 's too damn cold up in Connecticut- so cold that I had to throw in the towel and escape for a bit . Last week I took a weekend trip down to Orlando , FL just to escape . And while the weather was n't perfect ( mid-60 degrees Fahrenheit , cloudy , and spotty rain , as you can see from the <p> I have issues - I cant stop thinking about object detection . You see , last night I was watching The Walking Dead- and instead of enjoying the zombie brutality , the forced- cannibalism , or the enthralling storyline , - all I wanted to do was build an object detection system @ @ @ @ @ @ @ @ @ @ Probably not . I mean , its <p> Connecticut is cold . Very cold . Sometimes its hard to even get out of bed in the morning . And honestly , without the aide of copious amounts of pumpkin spice lattes and the beautiful sunrise over the crisp autumn leaves , I do n't  think I would leave my cozy bed . But I have work to do . And today <p> If you 've been paying attention to my Twitter account lately , youve probably noticed one or two teasers of what I 've been working on a Python framework/package to rapidly construct object detectors using Histogram of Oriented Gradients and Linear Support Vector Machines . Honestly , I really ca n't stand using the Haar cascade classifiers provided by OpenCV <p> In my last post , I mentioned that tiny , one pixel shifts in images can kill the performance your Restricted Boltzmann Machine + Classifier pipeline when utilizing raw pixels as feature vectors . Today I am going to continue that discussion . And more importantly , I 'm going to provide some Python and scikit-learn code that you can use- @ @ @ @ @ @ @ @ @ @ images above ? Probably not . The one on the right has been shifted one pixel down . And while it still looks like the same image to us , to a Restricted Boltzmann Machine , this translation could spell trouble . Raw Pixel Intensities as Feature Vectors @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485128 @185128/ <h> Blur detection with OpenCV <p> Between myself and my father , Jemma , the super-sweet , - hyper-active , extra-loving family beagle may be the most photographed dog of all time . Since we got her as a 8-week old puppy , to now , just under three years later , we have accumulated over 6,000+- photosof the dog . <p> Excessive ? <p> Perhaps . But I love dogs . A lot . Especially beagles . So it should come as no surprise that as a dog owner , I spend a lot of time playing tug-of-war with Jemmas- favorite toys , rolling around on the kitchen floor with her as we roughhouse , and yes , snapping tons of photos of her with my iPhone . <p> Over this past weekend I sat down and tried to organize the massive amount of photos in iPhoto . Not only was it a huge undertaking , I started to notice a pattern fairly quickly - there were lots of photos with excessive amounts of blurring . <p> Whether due to sub-par photography skills , trying to keep up @ @ @ @ @ @ @ @ @ @ or her spazzing out right as I was about to take the- perfect shot , many photos contained a decent amount of blurring . <p> Now , for the average person I suppose they would have just deleted these- blurry photos ( or at least moved them to a separate folder ) but as a computer vision scientist , that was n't going to happen . <p> Instead , I opened up an editor and coded up a quick Python script to perform blur detection with OpenCV . <p> In the rest of this blog post , I 'll show you how to compute the amount of blur in an image using OpenCV , Python , and the Laplacian operator . By the end of this post , you 'll be able to apply the- variance of the Laplacian- method to your own photos to detect the amount of blurring . <h> Variance of the Laplacian <p> Figure 1 : Convolving the input image with the Laplacian operator . <p> My first stop when figuring out how to detect the amount of blur in an image was to read through the excellent survey @ @ @ @ @ @ @ @ @ @ 2013- Pertuz et al . Inside their paper , Pertuz et al . reviews nearly 36 different methods to estimate the focus measure of an image . <p> If you have any background in signal processing , the first method to consider would be computing the Fast Fourier Transform- of the image and then examining the distribution of low and high frequencies if there are a low amount of high frequencies , then the image can be considered blurry . However , defining what is a- low number of high frequencies and what is a- high number of high frequencies can be quite problematic , often leading to sub-par results . <p> Instead , would n't it be nice if we could just compute a- single floating point value- to represent how- blurry a given image is ? <p> Pertuz et al . reviews many methods to compute this " blurryness metric " , some of them simple and straightforward using just basic grayscale pixel intensity statistics , others more advanced and feature-based , evaluating the Local Binary Patterns of an image . <p> The method is simple . Straightforward . @ @ @ @ @ @ @ @ @ @ a single line of code : <p> Blur detection with OpenCV <p> Python <p> 1 <p> cv2.Laplacian ( image , cv2.CV64F ) . var() <p> You simply take a single channel of an image ( presumably grayscale ) and convolve it with the following 3 x 3 kernel : <p> Figure 2 : The Laplacian kernel . <p> And then take the- variance ( i.e. standard deviation squared ) of the response . <p> If the variance falls below a pre-defined threshold , then the image is considered blurry ; otherwise , the image is- not blurry . <p> The reason this method works is due to the definition of the Laplacian operator itself , which is used to measure the 2nd derivative of an image . The Laplacian highlights regions of an image containing rapid intensity changes , much like the Sobel and Scharr operators . And , just like these operators , the Laplacian is often used for edge detection . The assumption here is that- if an image contains- high variance then there is a wide spread of responses , both edge-like and non-edge like , representative @ @ @ @ @ @ @ @ @ @ is- very low variance , then there is a tiny spread of responses , indicating there are very little edges in the image . As we know , the more an image is blurred , - the less edges there are . <p> Obviously the trick here is setting the correct threshold which can be quite domain dependent . Too low of a threshold and you 'll incorrectly mark images as blurry when they are not . Too high of a threshold then images that are actually blurry will not be marked as blurry . This method tends to work best in environments where you can compute an acceptable focus measure range and then detect outliers . <h> Detecting the amount of blur in an image <p> So now that we 've reviewed the the method we are going to use to compute a single metric to represent how " blurry " a given image is , let 's take a look at our dataset of the following 12 images : <p> Figure 3 : Our dataset of images . Some are blurry , some are not . Our goal is to perform @ @ @ @ @ @ @ @ @ @ . <p> As you can see , some images are blurry , some images are not . Our goal here is to- correctly mark each image as- blurry or- non-blurry . <p> With that said , open up a new file , name it detectblur.py- , and let 's get coding 15 <p> 16 <p> 17 55203 @qwx675203 <p> fromimutils importpaths 55218 @qwx675218 <p> importcv2 <p> **29;3771;TOOLONG : <p> # compute the Laplacian of the image and then return the focus <p> # measure , which is simply the variance of the Laplacian <p> **25;3802;TOOLONG , cv2.CV64F ) . var() 55202 @qwx675202 55206 @qwx675206 <p> LONG ... <p> help= " path to input directory of images " ) <p> LONG ... <p> help= " focus measures that fall below this value will be considered ' blurry ' " ) <p> From there , well define our varianceoflaplacian- function on- Line 6 . This method @ @ @ @ @ @ @ @ @ @ to be a single channel , such as a grayscale image ) that we want to compute the focus measure for . From there , - Line 9 simply convolves the image- with the 3 x 3- Laplacian operator and returns the variance . <p> Lines 12-17 handle parsing our command line arguments . The first switch well need is --images- , the path to the directory containing our dataset of images we want to test for blurryness . <p> Well also define an optional argument --thresh- , which is the threshold well use for the blurry test . If the focus measure for a given image falls below this threshold , well mark the image as- blurry . Its important to note that you 'll likely have to tune this value for your own dataset of images . A value of 100- seemed to work well for my dataset , but this value is quite subjective to the contents of the image(s) , so you 'll need to play with this value yourself to obtain optimal results . <p> Believe it or not , the hard part is done ! We @ @ @ @ @ @ @ @ @ @ the image from disk , compute the variance of the Laplacian , and then mark the image as blurry or non-blurry : <p> Blur <p> 37 <p> 38 <p> # loop over the input images <p> forimagePath inpaths.listimages ( args " images " ) : <p> # load the image , convert it to grayscale , and compute the <p> # focus measure of the image using the Variance of Laplacian <p> # method <p> **27;3829;TOOLONG 55215 @qwx675215 <p> **28;3858;TOOLONG <p> text= " Not Blurry " <p> # if the focus measure is less than the supplied threshold , <p> # then the image should be considered " blurry " <p> iffm&lt;args " threshold " : <p> text= " Blurry " <p> # show the image <p> cv2.putText ( image , " : : .2f " . format ( text , fm ) , ( 10,30 ) , <p> @ @ @ @ @ @ @ @ @ @ ( " Image " , image ) <p> key=cv2.waitKey(0) <p> We start looping over our directory of images on- Line 20 . For each of these images well load it from disk , convert it to grayscale , and then apply blur detection using OpenCV ( Lines 24-27 ) . <p> In the case that the focus measure exceeds the threshold supplied a command line argument , well mark the image as- " blurry " . <p> Finally , - Lines 35-38- write the text- and computed focus measure to the image and display the result to our screen . <h> Applying blur detection with OpenCV <p> Now that we have detectblur.py- script coded up , let 's give it a shot . Open up a shell and issue the following command : <p> Blur detection with OpenCV <p> Shell <p> 1 <p> $python detectblur.py--images images <p> Figure 4 : Correctly marking the image as " blurry " . <p> The focus measure of this image is 83.17 , falling below our threshold of 100 ; thus , we correctly mark this image as blurry . <p> Figure 5 : - @ @ @ @ @ @ @ @ @ @ as " blurry " . <p> This image has a focus measure of 64.25 , also causing us to mark it as " blurry " . <p> Figure 6 : Marking an image as " non-blurry " . <p> Figure 6 has a very high focus measure score at- 1004.14 orders of magnitude higher than the previous two figures . This image is clearly non-blurry and in-focus . <p> Figure 7 : Applying blur detection with OpenCV and Python . <p> The only amount of blur in this image comes from Jemma wagging her tail . <p> Figure 8 : Basic blur detection with OpenCV and Python . <p> The reported focus measure is lower than- Figure 7 , but we are still able to correctly classify the image as " non-blurry " . <p> Figure 9 : Computing the focus measure of an image . <p> However , we can clearly see the above image is blurred . <p> Figure 10 : An example of computing the amount of blur in an image . <p> The large focus measure score indicates that the image is non-blurry . <p> Figure @ @ @ @ @ @ @ @ @ @ as " blurry " . <p> However , this image contains dramatic amounts of blur . <p> Figure 12 : Detecting the amount of blur in an image using the variance of Laplacian . <p> Figure 13 : Compared to Figure 12 above , the amount of blur in this image is substantially reduced . <p> Figure 14 : Again , this image is correctly marked as not being " blurred " . <p> Figure 15 : Lastly , we end our example by using blur detection in OpenCV to mark this image as " blurry " . <h> Summary <p> In this blog post we learned how to perform blur detection using OpenCV and Python . <p> We implemented the- variance of Laplacian method to give us a single floating point value to represent the " blurryness " of an image . This method is fast , simple , and easy to apply we simply convolve our input image with the Laplacian operator and compute the variance . If the variance falls below a predefined threshold , we mark the image as " blurry " . <p> Its important @ @ @ @ @ @ @ @ @ @ correctly and youll often need to tune it on a per-dataset basis . Too small of a value , and you 'll accidentally mark images as blurry when they are not . With- too large of a threshold , you 'll mark images as non-blurry when in fact they are . <p> Be sure to download the code using the form at the bottom of this post and give it a try ! <h> Downloads : 55217 @qwx675217 <h> 75 Responses to Blur detection with OpenCV <p> Maybe you are going to expand this topic , but what would be really fascinating is if there is a way to take two very similar images and " repair " the blurred one with the unblurred one ( I think this feature is in the new Photoshop ) . <p> I noticed all the pictures in your data set were either completely sharp or completely blurry . How did your program fare on pictures with a sharp background , but blurry subject ? ( i.e. , the " Jemma spazzing out " ones ) <p> They still did fairly well , but the results @ @ @ @ @ @ @ @ @ @ subject of the image is blurry , I would suggest performing saliency detection ( which I 'll do a blog post on soon ) and then only computing the focus measure for the subject ROI of the image . <p> as reddit user 5225225 suggest , another trick would be to process the image " in blocks , and sorted it and compared based on the average of the top 10% . That way , if there 's some non-blurry parts and some blurry parts , you can effectively ignore the blurry parts . That seems like it would work , because the actually blurry images look like every part of them is blurry , not just most of it as seen in the macro image . " <p> Unfortunately , its not quite as easy to partition the image into blocks because you run the risk of the blurry region being partitioned between overlapping blocks . If you also take the top 10% , you could also run into false-positives . As for the saliency detection , I have n't had a chance to investigate it more , I 've been too busy @ @ @ @ @ @ @ @ @ @ you could apply a blur filter to each image before calculate the blur score . Since all images would have the same blur factor applied it should still be fair to compare their blur scores after . This could also be a way to segment into three categories : clear , blurry , noisy . based on the change in score before blurring and after blurring or rate of change if you test at different levels of deliberate blurring . The rate of change of blur score may have a different slope depending on the amount of noise or other factors in the original image . <p> Oh my Jesus , i recently working on a project related to blur detection . and i have tried a edge width approach , finally i got stuck at finding the length of all edges . Now i have found this method and i wondering how to get a laplacian s variance of image in java . Hope someone can help me indeed . <p> I have n't coded Java in a long , long time , but computing the Laplacian by hand is @ @ @ @ @ @ @ @ @ @ the OpenCV + Java bindings installed . Here is some more information on computing the Laplacian by hand . <p> The cv2.Laplacian function , as the name suggests , computes the Laplacian of the input image . This function is built-in to OpenCV . The var() method is a NumPy function . It computes the statistical variance of a set of data , which in this case is the Laplacian hence the name " Variance of the Laplacian " . <p> Awesome post Adrian ! Wondering if there is a way to fix the blur ? What if there was a single image of a book taken with a cell phone that was blurry . Would there be a way to apply some method to make it slightly non-blurry ? <p> There are methods to " deblur " images ; however , the results are less than satisfactory at the moment . If you 've ever used Photoshop before , youve likely played around with these filters . They do n't  work all that great , and " deblurring " is still a very active area of research . <p> There @ @ @ @ @ @ @ @ @ @ detection ( such as investigating the coefficients in the Fourier domain ) , but in general , most methods require a threshold of some sort . Ill try to do a followup blog post to this one that contains a more " easier to control " threshold . <p> You could add an option to allow the user to input the file names of reference images that the user knows are " in focus " and " out of focus " . The program can use these to set a threshold . Perhaps a " minimally acceptable " reference image makes more sense . Then the user needs to input just one image file name . Pick the worst image that you would still find acceptable to keep then the program sets the score of that image as the threshold . <p> hello Adrian , I am a graduate student come from Wuhan university.I recently working on a project related to blur detection too , it is helpful to me.But I find the variance is also small when the image is pure color or close to pure color even the @ @ @ @ @ @ @ @ @ @ of Laplacian is close too.So , how to deal with this problem ? Really looking forward to your reply ! thank you ! <p> I addressed this question in an email , but I 'll respond here as well . This blur detection method ( as well as other blur detection methods ) tend to examine the gradient of the image . If there is no gradient in the image ( meaning pure color with no " texture " ) , there the variance will clearly be low . There are non-gradient based methods for blur detection and they are detailed in this survey paper on blur detection I would suggest giving that a read . <p> We try to capture high resolution frames with PI3 and 5Mpix and 8Mpix camera . <p> As long as nothing moves the photo is fine . <p> Since we like to use it inside a PIR MW motion sensor there will be always a person or object moving in front of the camera . We noticed that the moving person/object is always blur on the captured frame . Is this normal ? Can we @ @ @ @ @ @ @ @ @ @ What are the limitations ? <p> Motion blur can be a bit of a problem , especially because when humans watch a video stream we tend to not " see " motion blur we just see a fluid set of frames moving in front of us . However , if you were to hit " pause " in the middle of the stream , you would likely see motion blur . Ways to get around motion blur can involve using a very high FPS camera or using methods to detect the amount of blur and then choose a frame with minimal blur . <p> Thanks for this great post . You have done some good work here in explaining the Variance of Laplacian method . I have a use case where the input image is going to be somewhat black and white only . Is variance of laplacian method play good there as well ? What do you think of Sum Modified Laplacian method , in general ? <p> By the vary definition of the variance statistic the variance is only useful if your input images can actually " vary @ @ @ @ @ @ @ @ @ @ still be able to use the Variance of Laplacian . If your images are captured under controlled conditions you can simply define a threshold that determines what " is blurry " versus " what is not blurry " . <p> thanks a lot for the tutorial , I found it really useful and seems to work well for the problem I am considering ! However I have two questions : <p> the variance function is applied to the pixel data in " vector format " instead " matrix format " ? I ask this because I used R and after the convolution I still have pixel data in " matrix format " and applying var() function to such dataset of course brings to a covariance matrix , not just a number . <p> is n't the " variance approach " sensitive to the " number of objects " captured into the photo ? I mean , a " good focus photo " with just one object on flat background ( i.e. classical iphone headphones image ) could have a variance not higher than a " not so good focus photo " @ @ @ @ @ @ @ @ @ @ , how would you deal with it ? <p> The variance function is computed across the entire matrix . It should return a single floating point value , at least that is the NumPy implementation ( I 'm not sure about R ) . <p> The variance method is sensitive to a good number of things , but where you 'll really struggle is with objects with a flat background or no texture . The variance will naturally be very low in this case . <p> You can train a CNN to classify just about anything , so provided you have enough examples of blurry vs non-blurry images ( in the order of tens of thousands of examples , ideally ) then yes , a CNN could likely do well here . <p> Hi Adrian , thanks for the tutorial , it really helped me a lot . I have one question , instead classify an image as blurred or not blurred , how can I isolate the non blurred area in the image ? For example , in your figure 7 , where the blur comes from Jemma tail , how @ @ @ @ @ @ @ @ @ @ I really appreciate your blog posts . I 've bought your book and supported the kick starter for the new one . <p> How would I got about implementing blur detection on a region of interest ? ( I assume I just slice the array ) But what if that ROI is not a rectangle , but something made with a mask ? For example , I want to focus on a face ? <p> Is the best way to just find the ROI , then find the largest rectangle I can slice out of it ? <p> How you find and determine your ROI is up to you and is highly dependent on what you are trying to accomplish . Edge detection , thresholding , contour extraction , object detection , etc. can all be used for determining what your ROI is . Once you have your ROI , you would need to compute the bounding box for it , extract the ROI via array slicing , and then compute the blur value . <p> Presumably this could be skewed by large in focus areas with little variation for instance @ @ @ @ @ @ @ @ @ @ would likely lead to a low value being returned even if the image was in focus . <p> Although this would only work if such areas were at the edge(s) of images , perhaps auto-cropping the image before analysis would help here , based on some threshold for colour variation ? <p> great post would love to see this implemented in an over the shelf product or a google photos extension . I have a lot of photos that I have acquired over the years and I have adopted a new strategy this year to keep only the best , as opposed to keeping everything . an easy way to eliminate is to find blurred photos . <p> Hi Adrian , what a great post . I have an electronic focuser for my telescope that drives my telescope focus tube . I 've added an Arduino in front of it powered by a python link and I 've been using the standard deviation like this : <p> to test my focus . Honestly its been quite hard , you 've put a Laplacian in front of it , does that make any difference @ @ @ @ @ @ @ @ @ @ problem is that the number is very sensitive and changes for all sorts of reasons , a cloud moves in the sky , neighbour turns a light on , car goes past , none of which alter the focus but they do affect the contrast . <p> I would suggest breaking out your code a little more to make it more readable with less composite functions ( that way you can reuse function outputs ) . The cv2.meanStdDev will return both the mean and standard deviation . You only need the variance for the Laplacian . <p> As for applying this technique for examining skies , I 'm not sure its the best approach . Provided a cloudless night , there will be very little texture and therefore the variance will be low . As you noted , if a cloud moves in , the variance increases from the baseline . I would suggest looking at other methods , including the distribution of Fourier transform coefficients . <p> Hi Adrian , thanks for your answer . I agree with you , I should break out the code some more . I @ @ @ @ @ @ @ @ @ @ in and manage the focus in a sub-section of the image . It should be faster as well as being potentially more accurate . <p> I am facing a problem for highly focused images while blur detection . The Laplacian result is returning a low value for highly focused images . Is there any method to identify the image is blurred or not especially for focused images ? <p> If there there is little " texture " in the image , then the variance of the Laplacian will by definition be low . In that case , I would compute a histogram of the Fourier transform coefficients and inspect the slope . I will try to do a more advanced blog post on blur detection in the future . <p> Thanks Adrian ! Nice post . There are plenty of discussions on how to improve/detect blurriness . But , no one come up with full-pledged implementation . This is a wonderful blog you 're maintaining . Thanks again . <p> Thanks a lot Adrian for the nice post . Just a small question . For the variance of Laplacian , After @ @ @ @ @ @ @ @ @ @ ( Laplacian ) , the pixel values will apparently change . Do we apply the variance on " all " the pixels ? In other words , would the variance be calculated for the " whole " image , and based on that result we can determine whether the image is blurry or not when @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485129 @185129/ <h> ImageNet : VGGNet , ResNet , Inception , and Xception with Keras <p> A few months ago I wrote- a tutorial on how to classify images using Convolutional Neural Networks ( specifically , VGG16 ) pre-trained on the ImageNet dataset with Python and the Keras deep learning library . <p> The pre-trained networks inside of Keras are capable of recognizing- 1,000 different object categories , similar to objects we encounter in our day-to-day lives with high accuracy . <p> This solution worked well enough ; however , since my original blog post was published , the pre-trained networks ( VGG16 , VGG19 , ResNet50 , Inception V3 , and Xception ) have been- fully integrated into the Keras core ( no need to clone down a separate repo anymore ) these implementations can be found inside the applications sub-module . <p> Because of this , I 've decided to create a- new , updated tutorial- that demonstrates how to utilize these state-of-the-art networks in your own classification projects . <p> Specifically , well create a special Python script that can load- any of these networks using- either a TensorFlow @ @ @ @ @ @ @ @ @ @ input images . <p> To learn more about classifying images with VGGNet , ResNet , Inception , and Xception , just keep reading . <p> The goal of this image classification challenge is to train a model that can correctly classify an input image into 1,000 separate object categories . <p> Models are trained on 1.2 million training images with another 50,000 images for validation and 100,000 images for testing . <p> These 1,000 image categories represent object classes that we encounter in our day-to-day lives , such as species of dogs , cats , various household objects , vehicle types , and much more . You can find the full list of object categories in the ILSVRC challenge here . <p> When it comes to image classification , the ImageNet challenge is the- de facto benchmark for computer vision classification algorithms and the leaderboard for this challenge has been- dominated by Convolutional Neural Networks and deep learning techniques since 2012 . <p> The state-of-the-art pre-trained networks included in the Keras core library represent some of the highest performing Convolutional Neural Networks on the ImageNet challenge over the past few @ @ @ @ @ @ @ @ @ @ generalize to images outside the ImageNet dataset via- transfer learning , such as feature extraction and fine-tuning . <p> This network is characterized by its simplicity , using only- 3+3 convolutional layers stacked on top of each other in increasing depth . Reducing volume size is handled by max pooling . Two fully-connected layers , each with 4,096 nodes are then followed by a softmax classifier ( above ) . <p> The " 16 " and " 19 " stand for the number of weight layers in the network ( columns D and E in- Figure 2- below ) : <p> In 2014 , 16 and 19 layer networks were considered- very deep ( although we now have the ResNet architecture which can be successfully trained at depths of 50-200 for ImageNet and over 1,000 for CIFAR-10 ) . <p> Simonyan and Zisserman found training VGG16 and VGG19 challenging ( specifically regarding convergence on the deeper networks ) , so in order to make training easier , they first trained- smaller versions of VGG with less weight layers ( columns A and C ) first . <p> The smaller networks @ @ @ @ @ @ @ @ @ @ , deeper networks this process is called- pre-training . <p> While making logical sense , pre-training is a very time consuming , tedious task , requiring an entire network to be trained- before it can serve as an initialization for a deeper network . <p> The network architecture weights themselves are quite large ( in terms of disk/bandwidth ) . <p> Due to its depth and number of fully-connected nodes , VGG is over 533MB for VGG16 and 574MB for VGG19 . This makes deploying VGG a tiresome task . <p> We still use VGG in many deep learning image classification problems ; however , smaller network architectures are often more desirable ( such as SqueezeNet , GoogLeNet , etc . ) . <h> ResNet <p> Unlike traditional- sequential network architectures such as AlexNet , OverFeat , and VGG , ResNet is instead a form of " exotic architecture " that relies on micro-architecture modules ( also called " network-in-network architectures " ) . <p> The term- micro-architecture- refers to the set of " building blocks " used to construct the network . A collection of micro-architecture building blocks ( @ @ @ @ @ @ @ @ @ @ ) leads to the- macro-architecture ( i.e , . the end network itself ) . <p> First introduced by He et al . in their 2015 paper , - Deep Residual Learning for Image Recognition , the ResNet architecture has become a seminal work , demonstrating that- extremely deep networks can be trained using standard SGD ( and a reasonable initialization function ) through the use of residual modules : <p> Figure 3 : The residual module in ResNet as originally proposed by He et al . in 2015 . <p> Figure 4 : ( Left ) The original residual module . ( Right ) The updated residual module using pre-activation . <p> That said , keep in mind that the ResNet50 ( as in 50 weight layers ) implementation in the Keras core is based on the former 2015 paper . <p> Even though ResNet is- much deeper than VGG16 and VGG19 , the model size is actually- substantially smaller due to the usage of global average pooling rather than fully-connected layers this reduces the model size down to 102MB for ResNet50 . <h> Inception V3 <p> The goal @ @ @ @ @ @ @ @ @ @ multi-level feature extractor " by computing 1+1 , 3+3 , and 5+5 convolutions within the- same module of the network the output of these filters are then stacked along the channel dimension and before being fed- into the next layer in the network . <p> The original incarnation of this architecture was called- GoogLeNet , but subsequent manifestations have simply been called- Inception vN where- N refers to the version number put out by Google . <p> Please note that the Xception network is compatible- only with the TensorFlow backend ( the class will throw an error if you try to instantiate it with a Theano backend ) . <p> Line 7 gives us access to the imagenetutils- sub-module , a handy set of convenience functions that will make pre-processing our input images and decoding output classifications easier . <p> The remainder of the imports are other helper functions , followed by NumPy for numerical processing and cv2- for our OpenCV bindings . <p> Next , let 's parse our command line arguments : <p> VGGNet , ResNet , Inception , and Xception with Keras <p> Python <p> 15 <p> 16 @ @ @ @ @ @ @ @ @ @ 55202 @qwx675202 55206 @qwx675206 <p> ap.addargument ( " -i " , " --image " , required=True , <p> help= " path to the input image " ) <p> LONG ... <p> help= " name of pre-trained network to use " ) 55208 @qwx675208 <p> Well require only a single command line argument , --image- , which is the path to our input image that we wish to classify . <p> Well also accept an optional command line argument , --model- , a string that specifies which pre-trained Convolutional Neural Network we would like to use this value defaults to vgg16- for the VGG16 network architecture . <p> Given that we accept the name of our pre-trained network via a command line argument , we need to define a Python dictionary that maps the model names ( strings ) to their actual Keras classes : <p> VGGNet , ResNet , Inception , and <p> 35 <p> 36 <p> # define a dictionary @ @ @ @ @ @ @ @ @ @ Keras <p> MODELS= <p> " vgg16 " : VGG16 , <p> " vgg19 " : VGG19 , <p> " inception " : InceptionV3 , <p> " xception " : Xception , # TensorFlow ONLY <p> " resnet " : ResNet50 <p> <p> # esnure a valid model name was supplied via command line argument <p> ifargs " model " notinMODELS.keys() : <p> raiseAssertionError ( " The --model command line argument should " <p> " be a key in the MODELS dictionary " ) <p> Lines 25-31 defines our MODELS- dictionary which maps a model name string to the corresponding class . <p> If the --model- name is not found inside MODELS- , well raise an AssertionError- ( Lines 34-36 ) . <p> A Convolutional Neural Network takes an image as an input and then returns a set of probabilities corresponding to the class labels as output . <p> Typical input image sizes to a Convolutional Neural Network trained on ImageNet- are- 224+224 , - 227+227 , - 256+256 , and- 299+299 ; however , you may see other dimensions as well . <p> VGG16 , VGG19 , and ResNet all @ @ @ @ @ @ @ @ @ @ 299+299 pixel inputs , as demonstrated by the following code block : <p> However , if we are using Inception or Xception , we need to set the inputShape- to- 299+299 pixels , followed by updating preprocess- to use a- separate pre-processing function that performs a different type of scaling . <p> The next step is to load our pre-trained network architecture weights from disk and instantiate our model : <p> VGGNet , ResNet , Inception , and Xception with Keras <p> Python <p> 51 <p> 52 <p> 53 <p> 54 <p> 55 <p> 56 <p> 57 <p> 58 <p> 59 <p> # load our the network weights from disk ( NOTE : if this is the <p> # first time you are running this script for a given network , the <p> # weights will need to be downloaded first -- depending on which <p> # network you are using , the weights can be 90-575MB , so be <p> # patient ; the weights will be cached and subsequent runs of this <p> # script will be *much* faster ) <p> print ( " INFO loading ... " @ @ @ @ @ @ @ @ @ @ Network=MODELSargs " model " <p> model=Network ( weights= " imagenet " ) <p> Line 58 uses the MODELS- dictionary along with the --model- command line argument to grab the correct Network- class . <p> The Convolutional Neural Network is then instantiated on- Line 59 using the pre-trained ImageNet weights ; <p> Note : Weights for VGG16 and VGG19 are &gt; 500MB . ResNet weights are 100MB , while Inception and Xception weights are between 90-100MB . If this is the- first time you are running this script for a given network , these weights will be ( automatically ) downloaded and cached to your local disk . Depending on your internet speed , this may take awhile . However , once the weights are downloaded , they will- not need to be downloaded again , allowing subsequent runs of classifyimage.py- to be- much faster . <p> Our network is now loaded and ready to classify an image we just need to prepare this image for classification : <p> VGGNet , ResNet , Inception , and <p> 75 <p> 76 <p> # load the input image using the Keras helper utility while ensuring <p> # the image is resized to inputShape , the required input dimensions <p> # for the ImageNet pre-trained network <p> print ( " INFO loading and pre-processing image ... " ) <p> LONG ... <p> image=imgtoarray(image) <p> # our input image is now represented as a NumPy array of shape <p> # ( inputShape0 , inputShape1 , 3 ) however we need to expand the <p> # dimension by making the shape ( 1 , inputShape0 , inputShape1 , 3 ) <p> # so we can pass it through thenetwork <p> **25;3916;TOOLONG , axis=0 ) <p> # pre-process the image using the appropriate function based on the <p> # model that has been loaded ( i.e. , mean subtraction , scaling , etc . ) <p> image=preprocess(image) <p> Line 65 loads our input image from disk using the supplied inputShape- to resize the width and height of the image . <p> Line 66 converts the image from a PIL/Pillow instance to a NumPy array @ @ @ @ @ @ @ @ @ @ NumPy array with the shape ( inputShape0 , inputShape1,3 ) - . <p> However , we typically train/classify images in- batches with Convolutional Neural Networks , so we need to add an extra dimension to the array via np.expanddims- on- Line 72 . <p> After calling np.expanddims- the image- has the shape ( 1 , inputShape0 , inputShape1,3 ) - . Forgetting to add this extra dimension will result in an error when you call . predict- of the model- . <p> Given these predictions , we pass them into the ImageNet utility function . decodepredictions- to give us a list of ImageNet class label IDs , " human-readable " labels , and the probability associated with the labels . <p> The top-5 predictions ( i.e. , the labels with the largest probabilities ) are then printed to our terminal on- Lines 85 and 86 . <p> The last thing well do here before we close out our example is load our input image from disk via OpenCV , draw the #1 prediction on the image , and finally display the image to our screen : <p> VGGNet , ResNet @ @ @ @ @ @ @ @ @ @ 88 <p> 89 <p> 90 <p> 91 <p> 92 <p> 93 <p> 94 <p> 95 <p> # load the image via OpenCV , draw the top prediction on the image , <p> # and display the image to our screen <p> orig=cv2.imread ( args " image " ) <p> ( imagenetID , label , prob ) =P00 <p> cv2.putText ( orig , " Label : , : .2f% " . format ( label , prob*100 ) , <p> LONG ... <p> cv2.imshow ( " Classification " , orig ) 55212 @qwx675212 <p> To see our pre-trained ImageNet networks in action , take a look at the next section . <p> Once you have TensorFlow/Theano and Keras- installed , make sure you download the source code + example images to this blog post using the- " Downloads " section at the bottom of the tutorial . <p> From there , let 's try classifying an image with VGG16 : <p> VGGNet , ResNet , Inception , and Xception with Keras <p> Shell <p> 1 <p> $python classifyimage.py--image **28;3943;TOOLONG vgg16 <p> Figure 8 : Classifying a soccer ball using VGG16 pre-trained on the @ @ @ @ @ @ @ @ @ @ a look at the output , we can see VGG16 correctly classified the image as- " soccer ball " with 93.43% accuracy . <p> To use VGG19 , we simply need to change the --network- command line argument : <p> VGGNet , ResNet , Inception , and Xception with Keras <p> Shell <p> 1 <p> $python classifyimage.py--image images/bmw.png--model vgg19 <p> Figure 9 : Classifying a vehicle as " convertible " using VGG19 and Keras ( source ) . <p> VGG19 is able to correctly classify the the input image as- " convertible " with a probability of 91.76% . However , take a look at the other top-5 predictions : - sports car with 4.98% probability ( which the car is ) , - limousine- at 1.06% ( incorrect , but still reasonable ) , and- " car wheel " at 0.75% ( also technically correct since there are car wheels in the image ) . <p> We can see similar levels of top-5 accuracy in the following example where we use the pre-trained ResNet architecture : <p> ResNet correctly classifies this image of Clint Eastwood holding a gun as- @ @ @ @ @ @ @ @ @ @ to see- " rifle " at 7.74%- and- " assault rifle " at 5.63% included in the top-5 predictions as well . Given the viewing angle of the revolver and the substantial length of the barrel ( for a handgun ) its easy to see how a Convolutional Neural Network would also return higher probabilities for a rifle as well . <p> This next example attempts to classify the species of dog using ResNet : <p> VGGNet , ResNet , Inception , and Xception with Keras <p> Shell <p> 1 <p> $python classifyimage.py--image images/jemma.png--model resnet <p> Figure 11 : Classifying dog species using ResNet , Keras , and Python . <p> The species of dog is correctly identified as- " beagle " with 94.48% confidence . <p> I then tried classifying the following image of Johnny Depp from the- Pirates of the Caribbean- franchise : <p> While there is indeed a- " boat " class in ImageNet , its interesting to see that the Inception network was able to correctly identify the scene as a- " ( ship ) wreck " with 96.29% probability . All other predicted labels , @ @ @ @ @ @ @ @ @ @ paddle " , - and- " breakwater " are all relevant , and in some cases absolutely correct as well . <p> For another example of the Inception network in action , I took a photo of the couch sitting in my office : <p> VGGNet , ResNet , Inception , and Xception with Keras <p> Shell <p> 1 <p> $python classifyimage.py--image images/office.png--model inception <p> Figure 13 : Recognizing various objects in an image with Inception V3 , Python , and Keras . <p> Inception correctly predicts there is a- " table lamp " in the image with 69.68% confidence . The other top-5 predictions are also dead-on , including a- " studio couch " , - " window shade " ( far right of the image , barely even noticeable ) , - " lampshade " , and- " pillow " . <p> In the context above , Inception was n't even used as an object detector , but it was still able to classify all parts of the image within its top-5 predictions . Its no wonder that Convolutional Neural Networks make for excellent object detectors ! <p> The @ @ @ @ @ @ @ @ @ @ was finishing up- The Witcher III : The Wild Hunt ( easily in my top-3 favorite games of all time ) . The first prediction by VGG16 is- " home theatre " a reasonable prediction given that there is a- " television/monitor " in the top-5 predictions as well . <p> As you can see from the examples in this blog post , networks pre-trained on the ImageNet dataset are capable of recognizing a variety of common day-to-day objects . I hope that you can use this code in your own projects ! <h> What now ? <p> but what if you wanted to train your own- custom deep learning networks- from scratch ? <p> How would you go about it ? <p> Do you know where to start ? <p> Let me help : <p> Whether this is the- first time you 've worked with machine learning and neural networks or- you 're already a seasoned deep learning practitioner , my new book is engineered from the ground up to help you reach deep learning expert status . <h> Summary <p> In todays blog post we reviewed the five Convolutional Neural @ @ @ @ @ @ @ @ @ @ : <p> VGG16 <p> VGG19 <p> ResNet50 <p> Inception V3 <p> Xception <p> I then demonstrated how to use each of these architectures to classify your own input images using the Keras library and the Python programming language . <p> If you are interested in learning more about deep learning and Convolutional Neural Networks ( and how to train your own networks from scratch ) , be sure to take a look at my upcoming book , - Deep Learning for Computer Vision with Python , available for pre-order now . <h> Downloads : 55217 @qwx675217 <p> Thank you for you nice tutorial . I always learn many new points from your tutorials which organized and explained very-well . I have implemented this code and I could figure out how to use these models with keras . I thought now I can use transfer learning with these pre-trained models and train on my own data . <p> However , the main problem with my data is that they are medical images and gray-scale . I could follow the tutorial which proposed by FCohelt but I could n't figure out how to @ @ @ @ @ @ @ @ @ @ data . <p> I would be glad if you could give some hint for transfer learning with pre-trained models for not RGB but gray-scale images . <p> These pre-trained networks assume you are using 3 channel images you wont be able to modify them to use 1 channel images unless you train them from scratch . Instead , the solution is to turn your 1 channel image into a 3 channel image : <p> image = np.dstack ( 1chan , 1chan , 1chan ) <p> From there you can pass the image through the network since its a 3 channel image ( but appears gray ) . <p> Hey Adrian , Thanks for the blog . I was hoping to do Pedestrian/human detection using Convolutional Neural Networks . I have tried using HoG but it did n't  turn out to be super accurate . The problem I am facing with using CNN with ImageNet trained classifiers is that there is no class/label as person or human or anything of that sort . What do you suggest I do ? Could I try training it with INRIA person dataset or something @ @ @ @ @ @ @ @ @ @ Not related to this post . But i have a query wrt to keyframe extraction from videos . Using python and opencv i have to extract keyframes . I tried getting frames for each frame and then subtracting from each other and storing unique one which resulted in huge amoutn of frames . I need to calculate pixel difference of frames and compare it with a threshold value . if PD &gt; threshold store it as keyframe . Can you please give me an example on how can i calculate threshold of images which would be fetch me good amount of keyframes. same would be applied for other videos too <p> Hey Ashti I would kindly ask that comments on a particular blog post be related to the subject matter of the post ( otherwise it comes off as a bit rude/presumptive ) . If you want to learn more about comparing images , try this post . Best of luck with the project . <p> The process of changing the output classes of a pre-trained network without having to re-train it from scratch is called fine-tuning . Ill be @ @ @ @ @ @ @ @ @ @ with Python . <p> Great post as always . I was wondering , how one can test the top 1 and top 5 error of this pre-trained model across a standardized data set say Imagenet to compare these in a more scientific way . Any tips ? <p> Can you elaborate more on what you mean by comparing the top-1 and top-5 accuracies ? Normally for benchmark datasets like ImageNet your rank-1 and rank-5 accuracy on the test set is the standardized method to compare algorithms . <p> Hey Aurora I do n't  have any blog posts specifically related to leaf species classification , but I 'll keep that in mind for a future blog post . Do you have a link to a leaf dataset you are currently working with ? <p> In the meantime , be sure to take a look at Deep Learning for Computer Vision with Python where I 'll be discussing training your own deep learning neural networks in detail . A book like this would surely help with your project . <p> hello sir , I 'm presently working on image processing project I want to know @ @ @ @ @ @ @ @ @ @ ( step2 ) If captured image is human I want to confirm whether the human in the captured image has performed any crime by comparing currently captured image with an image that has been already stored in the database or cloud . so , it will b great if u provide code for step 1 n step2 asap .. <p> Differentiating between humans and animals can easily be accomplished via a bit of machine learning or deep learning . Exactly which method you should use is highly dependent on your input images/video streams . <p> As for crime detection , that sounds more like " activity recognition " which is not something I cover on PyImageSearch . <p> Hey Adrian , Your tutorials are really good . I had an issue which you could help me out with : ) . I want to store the value of the Tensor at the " Global Pool Layer " in Resnet50 but am unable to do so . Would be really nice if you could help me out <p> Hey Jeff you cant really " merge " the models together , but @ @ @ @ @ @ @ @ @ @ your own model(s) and create an ensemble from your other models ( and pre-trained ones ) as well . This makes the assumption that all networks are trying to predict the same class labels . 2 . If you want to predict different class labels from the labels in ImageNet , you should try fine-tuning a pre-trained network . <p> My name is Shiva , doing postdoctoral research in computer vision at ASU . I first found you because of an online search for deep learning tutorials . I am greatly interested in using deep learning models to perform medical image classification , segmentation and CBIR . My question is : I have data with training and validation splits for three classes . How do you modify the above codes to accept the training and validation splits and print the validation accuracy ? The reason I am asking is because I could find tutorials on using pre-trained models to predict a single image but not in-depth analysis on using these very deep models for data classification with train and validation splits . My experience with deep learning is intermediate level @ @ @ @ @ @ @ @ @ @ think you might have some confusion regarding pre-trained neural networks . Once the networks are trained on a given number of classes ( in this case , 1,000 ImageNet classes ) you can not use them to train on new classes ( in your case , three classes ) unless you apply feature extraction or fine-tuning . <p> Hi Adrian , Wonderful tutorial . I want to limit the output to a particular set of labels only . That is to say , I do n't  want all the ImageNet labels . Am I right in stating that in the previous few comments , you were referring to the solution of exact task I want to do when you said fine tuning of the pre-trained model is required ? <p> The first is a bit " hackish " . Simply use the pre-trained network as is , then ignore the indexes of the labels you are not interested in . Then , take the label with the largest probability ( form the set of labels you care about ) and use that as your final classification . Again , this @ @ @ @ @ @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485130 @185130/ <h> Find distance from camera to object/marker using Python and OpenCV <p> A couple of days ago , Cameron , a PyImageSearch reader emailed in and asked about methods to find the distance from a camera to an object/marker in an image . He had spent some time researching , but had n't  found an implementation . <p> I knew exactly how Cameron felt . Years ago I was working on a small project to analyze the movement of a baseball as it left the pitcher 's hand and headed for home plate . <p> Using motion analysis and trajectory-based tracking I was able to find/estimate the ball location in the frame of the video . And since a baseball has a known size , I was also able to estimate the distance to home plate . <p> It was an interesting project to work on , although the system was not as accurate as I wanted it to be the " motion blur " of the ball moving so fast made it hard to obtain highly accurate estimates . <p> My project was definitely an " outlier " @ @ @ @ @ @ @ @ @ @ a camera to a marker is actually a very well studied problem in the computer vision/image processing space . You can find techniques that are very straightforward and succinct like the triangle similarity . And you can find methods that are complex ( albeit , more accurate ) using the intrinsic parameters of the camera model . <p> In this blog post I 'll show you how Cameron and I came up with a solution to compute the distance from our camera to a known object or marker . <p> OpenCV and Python versions : This example will run on- Python 2.7/Python 3.4+ and OpenCV 2.4 . X. <h> Triangle Similarity for Object/Marker- to Camera- Distance <p> In order to determine the distance from our camera to a known object or marker , we are going to utilize triangle similarity . <p> The triangle similarity goes something like this : Let 's say we have a marker or object with a known width W. We then place this marker some distance D from our camera . We take a picture of our object using our camera and then measure the apparent width @ @ @ @ @ @ @ @ @ @ focal length F of our camera : <p> F = ( P x - D ) / W <p> For example , let 's say I place a standard piece of 8.5 x 11in piece of paper ( horizontally ; W = 11 ) D =- 24 inches in front of my camera and take a photo . When I measure the width of the piece of paper in the image , I notice that the perceived width of the paper is P = 248 pixels . <p> My focal length F is then : <p> F = ( 248px x 24in ) / 11in = 543.45 <p> As I continue to move my camera both closer and farther away from the object/marker , I can apply the triangle similarity to determine the distance of the object to the camera : <p> D = ( W x F ) / P <p> Again , to make this more concrete , let 's say I move my camera 3 ft ( or 36 inches ) - away from my marker and take a photo of the same piece of paper . Through automatic @ @ @ @ @ @ @ @ @ @ width of the piece of paper is now 170 pixels . Plugging this into the equation we now get : <p> D = ( 11in x 543.45 ) / 170 = 35in <p> Or roughly 36 inches , which is 3 feet . <p> Note : When I captured the photos for this example my tape measure had a bit of slack in it and thus the results are off by roughly 1 inch . Furthermore , I also captured the photos hastily and not 100% on top of the feet markers on the tape measure , which added to the 1 inch error . That all said , the triangle similarity still holds and you can use this method to compute the distance from an object or marker to your camera quite easily . <p> Make sense now ? <p> Awesome . Let 's move into some code to see how finding the distance from your camera to an object or marker is done using Python , OpenCV , and image processing and computer vision techniques . <h> Finding the distance from your camera to object/marker using Python and OpenCV @ @ @ @ @ @ @ @ @ @ Open up a new file , name it distancetocamera.py- , and well get to work . <p> The first thing well do is import our necessary packages . Well use NumPy for numerical processing and cv2- - for our OpenCV bindings . <p> From there we define our findmarker- - function . This function accepts a single argument , image- , and is meant to be utilized to find the object we want to compute the distance to . <p> In this case we are using a standard piece of 8.5 x 11 inch piece of paper as our marker . <p> Our first task is to now find this piece of paper in the image . <p> To to do this , well convert the image to grayscale , blur it slightly to remove high frequency noise , and apply edge detection on- Lines 7-9 . <p> After applying these steps our image should look something like this : <p> Figure 1 : Applying edge detection to find our marker , which in this case is a piece of paper . <p> As you can see , the edges @ @ @ @ @ @ @ @ @ @ clearly been reveled . Now all we need to do is find the contour ( i.e. outline ) that represents the piece of paper . <p> We find our marker- on- Line 13 by using the cv2.findContours- function and then determining the contour with the largest area on- Line 14 . <p> We are making- the assumption that the contour with the largest area is our piece of paper. - This assumption works for this particular example , but in reality finding the marker in an image is highly application specific . <p> In our- example , simple edge detection and finding the largest contour works well . We could also make this example more robust by applying contour approximation , discarding any contours that do not have 4- points ( since a piece of paper is a rectangle and thus has 4- points ) , and then finding the largest 4-point contour . <p> Note : More on this methodology can be found in this post on building a kick-ass mobile document scanner . <p> Other alternatives to finding markers in images is to utilize color , such that @ @ @ @ @ @ @ @ @ @ rest of the scene in the image . You could also apply methods like keypoint detection , local invariant descriptors , and keypoint matching to find markers ; however , these approaches are outside the scope of this article and are again , highly application specific . <p> Anyway , now that we have the contour that corresponds to our marker , we return the bounding box which contains the- ( x , y ) -coordinates and width and height of the box ( in pixels ) to the calling function on Line 17 . <p> Let 's also quickly to define a function that computes the distance to an object using the triangle similarity detailed above : <p> Find distance from camera to object using Python and OpenCV <p> Python <p> 19 <p> 20 <p> 21 <p> LONG ... <p> # compute and return the distance from the maker to the camera <p> **39;3973;TOOLONG <p> This function takes a knownWidth- of the marker , a computed focalLength- , and perceived width of an object in an image ( measured in pixels ) , and applies the triangle similarity detailed above @ @ @ @ @ @ @ @ @ @ # load the furst image that contains an object that is KNOWN TO BE 2 feet <p> # from our camera , then find the paper marker in the image , and initialize <p> # the focal length <p> **29;4014;TOOLONG <p> marker=findmarker(image) <p> LONG ... <p> The first step to finding- the distance to an object or marker in an image is to calibrate- and compute the focal length . To do this , we need to know : <p> The distance of the camera from an object . <p> The width ( in units such as inches , meters , etc. ) of this object . Note : - The height could also be utilized , but this example simply uses the width . <p> Let 's also take a second and mention that what we are doing- is not true camera calibration . True camera calibration involves the intrinsic parameters of the camera , which you can read more on here . <p> On Line 25 we initialize our known KNOWNDISTANCE- - from the camera to our object to be 24 inches . And on- Line 29 we initialize @ @ @ @ @ @ @ @ @ @ ( i.e. a standard 8.5 x 11 inch piece of paper laid out horizontally ) . <p> We then define the paths to our images we are going to use on- Line 32 . <p> The next step is important : - its our simple calibration step . <p> We load the first image off disk on- Line 37- well be using this image as our calibration image . <p> Once the image is loaded , we find the piece of paper in the image on- Line 38 , and then compute our focalLength- - on- Line 39- using the triangle similarity . <p> Now that we have " calibrated " our system and have the focalLength- , we can compute the distance from our camera to our marker in subsequent images quite easily . <p> Let 's see how this is done : <p> Find distance from camera to object using <p> 55 <p> 56 <p> # @ @ @ @ @ @ @ @ @ @ load the image , find the marker in the image , then compute the <p> # distance to the marker from the camera <p> **27;4045;TOOLONG <p> marker=findmarker(image) <p> LONG ... <p> # draw a bounding box around the image and display it <p> **28;4074;TOOLONG ( marker ) ) <p> cv2.drawContours ( image , box , -1 , ( 0,255,0 ) , 2 ) <p> cv2.putText ( image , " %.2fft " % ( inches/12 ) , <p> LONG ... <p> 2.0 , ( 0,255,0 ) , 3 ) <p> cv2.imshow ( " image " , image ) 55212 @qwx675212 <p> We start looping over our image paths on Line 42 . <p> Then , for each image in the list , we load the image off disk on Line 45 , find the marker in the image on- Line 46 , - and then compute the distance of the object to the camera on- Line 47 . <p> From there , we simply draw the bounding box around our marker and display the distance on- Lines 50-56 . <h> Results <p> To see our script in action , open up @ @ @ @ @ @ @ @ @ @ execute the following command : <p> Find distance from camera to object using Python and OpenCV <p> Shell <p> 1 <p> $python distancetocamera.py <p> If all goes well you should first see the results of 2ft.png- , which is the image we use to " calibrate " our system and compute our initial focalLength- : <p> Figure 2 : This image is used to compute the initial focal length of the system . We start by utilizing the known width of the object/marker in the image and the known distance to the object . <p> From the above image we can see that our focal length is properly determined and the distance to the piece of paper- is 2 feet , per- the KNOWNDISTANCE- and KNOWNWIDTH- - variables in the code . <p> Now that we have our focal length , we can compute the distance to our marker in subsequent images : <p> Again , its important to note that when- I captured the photos for this example I did so hastily and left too much slack in the tape measure . Furthermore , I also did not ensure @ @ @ @ @ @ @ @ @ @ , so again , there is roughly 1 inch error in these examples . <p> That all said , the triangle similarity approach detailed in this article will still work and allow you to find the distance from an object or marker in an image to your camera . <h> Summary <p> In this blog post we learned how to determine the distance from a- known object in an image to our camera . <p> To accomplish this task we utilized the- triangle similarity , which requires us to know two important parameters prior to applying our algorithm : <p> The width ( or height ) in some distance measure , such as inches or meters , of the object we are using as a marker . <p> The- distance- ( in inches or meters ) of the camera to the marker- in step 1 . <p> Computer vision and image processing algorithms can then be used to automatically determine the perceived width/height of the object in pixels and complete the triangle similarity and give us our focal length . <p> Then , in subsequent images we simply need to @ @ @ @ @ @ @ @ @ @ determine the distance to the object from the camera . <h> Downloads : 55217 @qwx675217 <p> its a great piece of work you have done here and i used this technique working for my android device , and i want to take it to next level by measuring multiple object distances .. but getting same results if two objects are on same position . any suggestions would be appreciated <p> Hi JD , if you are looking to measure multiple objects , you just need to examine multiple contours from the cv2.findContours function . In this example , I 'm simply taking the largest one , but in your case , you should loop over each of the contours individually and process them and see if they correspond to your marker . <p> on above example , we know the width of an object . What if we do n't  know the width of the object ? for example in real life situation where a robot need to navigate the it meets each unknown object and need to find the distance even tough it has now knowledge about each objects actual @ @ @ @ @ @ @ @ @ @ you 'll need to know the dimension of the object in order to get an accurate reading on the width/height . However , in unconstrained environments you might be able to use a stereo camera so you can compute the " depth " of an image and do a better job avoiding obstacles . <p> First of all i 'm very thankful for this and other tutorials out there ! this is by far the best series of tutorials online ! Perfectly described step by step and explained why to preform every step = I cant thank you enough ! <p> I 'm trying to do the same thing as described in the above tutorial ( finding an object and determine the distance between obj and camera ) but i wonder how i should do it when using constant streaming video instead of loading images ? <p> Hi Dries , thanks for the great comment , it definitely put a smile on my face = As for when using a constant video stream versus loading images , there is no real difference , you just need to update the code to use a @ @ @ @ @ @ @ @ @ @ sounds and I cover it both in Practical Python and OpenCV and this post . <p> Your final metric is completely arbitrary you can use feet , meters , centimeters , whatever you want . Take a look at Lines 25 and 29 and redefine them using the metric you want . And then you 'll need to modify Line 52 to output your metric instead of feet . <p> Hi there , when i try the same code using 2 feet and an image of 1 inch , the focal length is around 1260 . Is this ok ? coz i 'm getting unacceptable distances of around 3.4 feet for 6 feet Are there any limits for this method . I find this method really interesting , i am thinking forward to do this in my project . One more thing , will this work for webcam from a laptop . Thanks in advance <p> The main limitation of this method is that you need to have a straight-on view of the object you are detecting . As the viewpoint becomes angled , it distorts the calculation of the bounding box @ @ @ @ @ @ @ @ @ @ this method will work with your laptop webcam , you just need to update the code to grab frames using the cv2.VideoCapture function . See this post for an example of grabbing frames from the webcam stream . <p> I 'll tell you what my project . I detect a small object in real time with a webcam , with the position of the object , will point a laser that will be about two servos , one for the X axis , and one for the Y axis . <p> I am ready the detection and tracking . but I 'm having trouble getting the positions X , Y , Z . I research the transformation from 3D to 2D but there are certain points that do not understand . <p> Currently , I 'm working on a similar project , and I have a problem to do the relationship between coordinates in the 3D and servos . First , I would like to compute and track my 3D-coordinates object but it does not work . <p> This code can be easily adapted to work in real-time video streams . I would @ @ @ @ @ @ @ @ @ @ it multiple times on the PyImageSearch blog I think this post can help get you started . <p> In the case of this blog post , I defined a marker as a large rectangle . Rectangle-like regions have the benefit of being easy to find in an image . Markers can be made more robust by adding ( 1 ) color or ( 2 ) any type of special design on the marker themselves . <p> In order to perform real-time distance detection , you 'll need to leverage the cv2.VideoCapture function to access the video stream of your camera . I have an example of accessing the video stream in this post . I also cover accessing the video stream more thoroughly inside Practical Python and OpenCV . <p> Hi , this is a great helpful job please i 'm working on application that take an image then find the object to calculate its dimensins , so i need to know the distance or if there is another blog/ article/ refferance you can help me with <p> In order to compute the distance to an object in an image , you @ @ @ @ @ @ @ @ @ @ . For example , you could maintain a know database of objects and their dimensions , so when you find them , just pull out the dimensions and run the distance calculation . <p> You 'll need to know the size of some object in the image to perform camera calibration . In this example , we have a piece of paper . But you could have used a coin . A coffee cup . A book . But the point is that you need to know the size of object(s) you 'll be using to perform the camera calibration using triangle similarity . <p> Thanks for sharing . I have tried your code and its work . But is it possible to calculate the object distance if the object size in real world ( width / height ) unknown ? Just using internal camera parameter and object size in pixels .. <p> You basically need to convert the " pixel points " to " real world " points , from there the distance between objects can be measured . This is something I 'll try to cover on PyImageSearch in the future @ @ @ @ @ @ @ @ @ @ good MATLAB tutorial that demonstrates the basics . <p> Hello Adrian . First of all , This is a great website . I have a question : Is this distance estimation can be done in real time processing applications . I mean I want to measure distance from an object ( cicular and coorful ) to my robot while the robot moving on a straight line . - use a moderate camera with low resolution ( usb cam ) . How should I do that <p> Yes , this can absolutely be done in real-time processing , that 's not an issue . As long as you perform the calibration ahead of time , you can certainly perform the distance computation in your video pipeline . <p> Thanks for the help and your fast reply man . Camera Calibration looks like complicated though . Cause I wiil look at an object from an angled position say 30 degree(initially) while I m moving on a straigt line the angle will increase . In each time . So I have to make sure that the object is almost middle in the frame to @ @ @ @ @ @ @ @ @ @ the calibration discussed in this post ) is actually pretty straightforward . This post assumes you have a 90-degree straight-on view of the object . For angled positioning this approach wont work well unless you can apply a perspective transform . You should look into more advanced camera calibration methods . <p> I simply took photos using my iPhone for this post , but the code can work with either a built-in/USB webcam or the Raspberry Pi camera . If you 're having trouble getting your Raspberry Pi camera + code working , I suggest reading this post on accessing the Raspberry Pi camera . <p> First of all , thank you for a wonderful tutorial as always . I am working on a similar project and need some help . <p> 1 . Is there a literature review available regarding all methods of depth estimation without using a stereo camera i.e. using only a normal single lens camera ? If not , can you point me to resources papers and hopefully implementations about the state-of-the-art on this problem ? I should mention that I am mainly interested in understanding the @ @ @ @ @ @ @ @ @ @ I have n't been able to find any decent open implementations of these and that 's kind of sad . <p> 2 . I am investigating the importance of head movements in animals ( humans included ) for depth perception and came across few decade old papers on the topic . Can you provide me references on how motion of camera affects detection of edges , depth estimation etc from a computer vision perspective ? Aligning with point 1 , I am looking for something on the lines of how one can estimate depth accurately by moving the single lens camera and detect edges and/or object boundaries by virtue of this movement . Methods like triangle similarity are n't  really helpful since they need an estimate of the original size of object/marker in question . <p> For both questions , my suggestion would be start off with this paper and follow the references . This paper is heavily cited in the CV literature and links to previous works that should also help you out . <p> I may be coming in a little for this post but I 'm having trouble with the @ @ @ @ @ @ @ @ @ @ error . Can you please assist me with this . By the wayI think you are doing an awesome job . <p> First off , kudos on making such a complex system seem so intuitive ! It makes the process look so much less intimidating to us newbies ( whether I 'm able to follow along once my picam gets here is another story ! ) <p> My question for ya is this : assuming I can manage to follow along and get distance readings for my marker , how difficult would it be to add the code required to trigger an event on a device that the marker is mounted to ? In my case , I am looking to vibrate a small motor when the tracked object is more than 10 feet away . Ideally , it would increase in intensity based on how much further the object gets from the camera . I know this would require some investment in more hardware , but does it sound like a plausible idea ? <p> Thanks for posting this , and being so generous with helping out in the comments @ @ @ @ @ @ @ @ @ @ very informative and well done . I have a quick question regarding the limitations of such an approach when using larger distances . For example , 30 feet . At this distance , a relatively small object may be represented by very few pixels right ? I 'm assuming the better the camera ( with better resolution ) the farther the range in which this approach can still be accurate . My question is whether my assumption is indeed correct ? <p> Furthermore , I find that when I utilize this approach , the distance calculation sometimes fluctuates as the perceived width in pixels fluctuates . Could this be due to noise ? And if so , what are some good techniques to reduce said noise ? I 've looked into the blur function in OpenCV , but I have n't had much luck with that . <p> Thanks again for the website , its super helpful . Look forward to hearing from you . Thank you ! <p> Indeed , that is correct . The farther away the object is and the smaller the resolution of the camera is , the less @ @ @ @ @ @ @ @ @ @ , that is entirely dependent on the image data you are working with . Blurring is one way to reduce noise . You might also want to look into the segmentation process and ensure you are obtaining an accurate segmentation of the background from the foreground . This can be improved by tweaking Canny edge detection parameter , threshold values , etc . <p> Hello Adrian i just wanted to know that how can i use this distance recognition technique to make a 2D map of a vertical wall ( whose photo can be taken easily ) to precisely know the position of doors windows and other stuffs on the wall and the distances between each other and their dimensions with certain accuracy ? ? ? <p> Hi Adbul I 'm not sure quite sure what you mean by a 2D map of a vertical wall , but if you want super precise measurements between doors , windows , etc. , then I would suggest using a 3D/stereo camera instead of a 2D camera . This will give you much better results . <p> Yes , even with the paper titled @ @ @ @ @ @ @ @ @ @ that the paper is the largest contour area in the image . For a more robust algorithm for finding rectangular regions ( and verifying that they indeed have 4 vertices ) , please see this post . <p> Hey Matt , I 'm not sure I understand your question a rectangle has 4 vertices , no matter how you rotate it . An easy way to detect rectangles in an image is to simply use contour approximation , which I mentioned in my previous comment . <p> If you consider z the axis on which you compute the distance object-camera , x and y the additional axes , and if you rotate with an angle 90 degrees around x-axis or y-axis , your camera do not detect a rectangle but a straight line . So what happens in this case ? <p> Oh , you were referring to the z-axis , that was my mistake . In that case , you would need utilize a more advanced algorithm to detect your object . This blog post is primarily geared towards easily detectable objects and computing the distance . <p> Hi Adrian @ @ @ @ @ @ @ @ @ @ the red marker . I made the filter to see red color only but i have problem considering distance . I lose the " seeing " the red color after 10cm ( the markers are 3 red circle diameter 10cm each in triangle formation ) . I 'm using raspberry pi 2 b+ and pi camera <p> How are you looking for the red color ? Via color thresholding ? If so , investigate the mask that is being generated from cv2.inRange and see if the red color region exists in the mask . If it does , then you 'll likely want to look at the contours being detected and see if red masked region is being returned . <p> You normally would use a single reference object to calibrate your camera . Once you have the camera calibrated , you can detect the distances/object sizes of varying sizes . See this blog post for more information . <p> Hi Adrian Thanks for your great information , just I have a question . if you take a picture of a special object for example in the ceiling and you are required @ @ @ @ @ @ @ @ @ @ but not the perpendicular distance , what should we do ? In fact I know the exact location of the camera ( x , y , z ) and also the location of the object in the ceiling in terms of ( x , y ) but I do not know the z of the object . I want to measure z . As you said I can measure the perpendicular distance between the camera an the object by taking a picture of the object , but I have to know the direct distance ( not perpendicular ) between the camera and the object . <p> Hi dear Adrian . tanks for all of your good and useful information.I had very good result of this algorithm on mobile photography or raspberry camera . but i just have big problem with the digital camera witch has a lenses ( DSLR or compact ) . Can you help me about this ? ? if i know the F of the camera for example 3.2 how should be put it on my calculating ? ? ? what i calculate ( F ) is @ @ @ @ @ @ @ @ @ @ . <p> There is a difference between the focal length of the physical lens and the perceived focal length from the image . You 'll need to calibrate your DSLR camera in the same way that you performed the calibrations on the Pi and mobile phone . <p> Excellent article ! But one little mistake that can confuse beginners , you wrote " perceived width of the paper is P = 249 pixels " but in calculations you used 248 . Hope to see the tutorial on finding the distance to an randomly chosen object by using stereo-pair of cameras . <p> The level of accuracy depends on the resolution of your camera . The smaller the resolution , the less accurate . The further the objects are away , the less accurate . This script does not perform radial distortion correct , which is something else to consider . As for finding the distance between two objects , please see this post . <p> Hello Adrain , Great Article to start with the distance estimation . I have downloaded your code and trying to validate with images , but i @ @ @ @ @ @ @ @ @ @ please send me some reference images . <p> Hello Adrian Rosebrock , Your explanation helped me understand the concept very well . I am still perplexed by another problem . For example , I have a calibrated camera , i.e. I know the focal lengths and the optical offsets of the lens and sensor . Then , the relation between pixel and the actual height/width of an object is true only if the object is placed at the focal length . If I place an object of unknown dimensions at an unknown distance from the camera lens , then there is no way to estimate the distance between them . Am I thinking right or is something missing ? Can you please help . <p> Hi . I was thinking about make mobile app which will measure width and height some objects by using dual cameras like LONG ... . Do you think that it will be working ? Thank you for your answer . <p> Hi Adrien , Thanks for the information . I have one question : Is it possible to incorporate the distance estimation with the ball @ @ @ @ @ @ @ @ @ @ a project whereby I am trying to detect an object with the color green and find the distance between the camera and the object . <p> Sorry , i seems to have phrased my question wrongly . What I meant was is it possible to find the distance from the camera to the green ball in real time ? So instead of making it detect edges , i modified it to detect green ? <p> Hello . I want to use this code to detect images real time using the PiCamera . I read your replies and honestly have no idea how to " Use the cv2.VideoCapture function to access the stream of your camera " . Playing with the code results in all sorts of errors . Could you help me by giving a detailed explanation ? Thanks <p> Hi Adrain , I am a big fan of your posts , I was impressed with all of what I have seen . I have a problem in my thesis that I guess you might help me in related to localization . My thesis project is automating the process of @ @ @ @ @ @ @ @ @ @ of a rectangle to cover then it will move back and forth starting from one of the corners . It will track its distance using wheel encoders , however , due to error due slippage and drifting . Therefore , we need a reliable method through which we can know the absolute location to correct for the relative localization error of wheel encoders . We have tried several methods and all had some problems : 1- Tracking successive features in frames to estimate the rotational and transnational matrices however for sharp turns it looses track of everything . 2- Triangulation by placing three balls of different colors and identifying the angle of each through a camera on a motor however using colors outdoor is so unreliable due different lightening at different day times . When you decrease the HSV scale becomes more accurate but increases the probability of loses balls and increasing range catches noise from the environment . 3- Using homography instead of color balls for triangulation but it is computationally slow . 4- Using April Tags or Aruco tags but as mechanical engineers , were are finding it @ @ @ @ @ @ @ @ @ @ find a starting point to continue on by finding a code and understanding it . Hope U can help us and sorry for the long post <p> To start , its always helpful to have actual real-world images of what you 're working with . This helps me and other readers visually see and appreciate what you are trying to accomplish . Myself , as I imagine many other readers , do n't  know much about the intricacies of lawn mowers , wheel encoders , or slippage/drifting . <p> That said , based on your comment it seems that the homography is producing the correct results , correct ? And if that 's the case why not focus your efforts on speeding up the homography estimation ? Try to reduce the number of keypoints ? Utilize binary rather than real-valued descriptors ? Implement this part of the algorithm in C/C++ for an extra speed gain ? <p> hi Adrian . Firstly , thank you for sharing.nowadays I am working similar projects . so I have a question.there is an object with a known width w and I do n't  know distance D @ @ @ @ @ @ @ @ @ @ put a rectangular object ( a box ) with a known size under camera I will measure distance from object but I know only distance between camera and ground . how to do ? <p> If you know the distance from the ground to the camera and know the size of the object in both units ( inches , millimeters , etc. ) then you should be able to apply some trigonometry to workout the triangle property . I have n't actually tried this , so I 'm just thinking off the top of my head . It might not work , but its worth a shot . <p> Hi Adrian , first of all , excellent article ! I have a question , regarding this code . I 'm currently carrying out research for my dissertation which requires using stereo vision to calculate distance from the camera to a chosen object/area . Will this code be applicable for stereo vision as well ? <p> No , I would not use this code for stereo vision . The reason is because stereo cameras ( by definition ) can give you a much more @ @ @ @ @ @ @ @ @ @ in stereo vision , but this short tutorial or computing a depth map should help you out . <p> i am doing a project in which i need to get the exact location of a human at a distance . Exact location refers to the EXACT location as used by a gun to aim at an enemy . Here , i do n't  have any marker object or any known distances . Any way out ? <p> Hmm , that 's a pretty small resolution for that accurate of results . The first step would be to calibrate your camera and account for barrel distortion . If your images are really noisy and you cant accurately segment the object from every image , then you 're in for some real trouble . But if you can get a nice segmentation I would give it a try and see what results you come up with . Its best to experiment with projects like these and note what does and does not work . <p> Hi Adrian .. your article is a really great tutorial = btw i 'm working on a project that measure @ @ @ @ @ @ @ @ @ @ it shape , whether it smaller or bigger , so the marker also get bigger or smaller , therefore I cant define the width and height of the marker . <p> the question is , can I modify your algorithm so that I can measure distance with my condition ? or do you have another method that suitable to measure distance of a fire ? Thanks in advance <p> It sounds like you might need a more advanced calibration technique . I personally have n't done/read any research related to fire direction techniques with computer vision , but I would suggest reading up on intrinsic camera properties for calibration . <p> can you please describe the hardware part of this amazing project of yours as i need it for a small project of mine too like how did u integrate the phones camera or did u use raspberry pi with a camera module please let me know <p> I used my iPhone to capture the example photos . The photos were moved to my laptop . And then I processed the photos using my laptop . The actual method used to @ @ @ @ @ @ @ @ @ @ 1 ) its consistent and ( 2 ) you are calibrating your camera . <p> Hi Adrian , I 'm trying to copy this method for a USB camera and have used your previous posts to modify it to work with a camera using a while loop . The problem I 'm having is the max function in findmarker is constantly coming out as None hence resulting in the distancetocamera function throwing an error . Do you have any idea why this could be ? <p> Hey adrian , this website is the bestest reference for beginners , but may i ask if for example the camera is not looking straight to the object the distance in pixels would change so it would n't work right ? if my logic was correct how to @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485132 @185132/ <h> Tag Archives classification <p> Well . Ill just come right out and say it. - Today is my 27th birthday . As a kid I was always- super excited about my birthday . It was another year closer to being able to drive a car . Go to R rated movies . Or buy alcohol . But now as an adult , I do n't  care too much for my <p> So in last weeks blog post we discovered how to construct an image pyramid . And in todays article we are going to extend that example and introduce the concept of a sliding window . Sliding windows play an integral role in object classification , as they allow us to localize exactly- " where " in an image an object resides . <p> It 's too damn cold up in Connecticut- so cold that I had to throw in the towel and escape for a bit . Last week I took a weekend trip down to Orlando , FL just to escape . And while the weather was n't perfect ( mid-60 degrees Fahrenheit , cloudy , @ @ @ @ @ @ @ @ @ @ <p> Update January 27 , 2015 : Based on the feedback from commenters , I have updated the source code in the download to include the original MNIST dataset ! No external downloads required ! Update March 2015 , 2015 : The nolearn package has now deprecated and removed the dbn- - module . When you go to install the nolearn package , be sure <p> In my last post , I mentioned that tiny , one pixel shifts in images can kill the performance your Restricted Boltzmann Machine + Classifier pipeline when utilizing raw pixels as feature vectors . Today I am going to continue that discussion . And more importantly , I 'm going to provide some Python and scikit-learn code that you can use- to <p> Can you tell the difference between the two images above ? Probably not . The one on the right has been shifted one pixel down . And while it still looks like the same image to us , to a Restricted Boltzmann Machine , this translation could spell trouble . Raw Pixel Intensities as Feature Vectors @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485133 @185133/ <h> Archive Image Processing <p> After our previous post on computing image colorfulness was published , Stephan , a PyImageSearch reader , left a comment on the tutorial asking if there was a method to compute the colorfulness of specific regions of an image ( rather than the entire image ) . There are multiple ways of attacking this problem . The first could be to apply <p> In a previous PyImageSearch blog post , I detailed how to compare two images with Python using the Structural Similarity Index ( SSIM ) . Using this method , we were able to easily determine if two images were identical or had differences due to slight image manipulations , compression artifacts , or purposeful tampering . Today we are going to extend the <p> Todays tutorial is inspired by a post I saw a few weeks back on /r/computervision asking how to recognize digits in an image containing a thermostat identical to the one at the top of this post . As Reddit users were quick to point out , utilizing computer vision to recognize digits on a thermostat tends to- @ @ @ @ @ @ @ @ @ @ computer vision literature is- Seam Carving for Content-Aware Image Resizing by Avidan and Shamir from Mitsubishi Electric Research Labs ( MERL ) . Originally published in the SIGGRAPH 2007 proceedings , I read this paper for the first time during my computational photography class as an undergraduate student . This paper , along with <p> In last weeks blog post I demonstrated how to count the number of frames in a video file . Today we are going to use this knowledge to help us with a computer vision and image processing task - visualizing movie barcodes , similar to the one at the top of this post . I first became aware of <p> Todays blog post is part of a two part series on working with video files using OpenCV and Python . The first part of this series will focus on a question emailed in by PyImageSearch reader , Alex . Alex asks : I need to count the total number of frames in a video file with OpenCV . The only <p> Todays blog post is a followup to a tutorial I did a couple of @ @ @ @ @ @ @ @ @ @ . My previous tutorial assumed there was only- one bright spot in the image that you wanted to detect but what if there were multiple bright spots ? If you want to detect more than <p> Over the past few months Ive gotten quite the number of requests landing in my inbox to build a bubble sheet/Scantron-like test reader using computer vision and image processing techniques . And while I 've been having a lot of fun doing this series on machine learning and deep learning , I 'd be- lying if I said this little <p> A few weeks ago , I wrote a blog post on creating transparent overlays with OpenCV . This post was meant to be a gentle introduction to a neat little trick you can use to improve the aesthetics of- your processed image(s) , such as creating a Heads-up Display ( HUD ) on live video streams . But @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485134 @185134/ <h> Main Menu <h> About <p> If you 're interested in learning how image search engines work , or trying to build one of your own , you 've come to the right place . <p> I 'm an entrepreneur- who has launched two successful image search engines : I 'd My Pill , an iPhone app and API that identifies prescription pills in the snap of a smartphones camera , and Chic Engine , a fashion search engine for the iPhone. - Previously , my company ShiftyBits has consulted with the- National Cancer Institute- to develop image processing and machine learning algorithms to automatically analyze breast histology images for cancer risk factors . <p> I received my Bachelors of Science from the University of Maryland , Baltimore County in 2010 . I then received my Ph.D at the same institution in 2014 where I studied computer vision , machine learning , and information retrieval . <p> I 'm here to share the tips , tricks , and hacks Ive learned in the past 8 years working in the startup field and building computer vision , machine learning , and image search engine systems @ @ @ @ @ @ @ @ @ @ how they work , and how to build your own , subscribe to my email newsletter to receive FREE tips and tricks Ive learned over the past 8 years . <h> About this Blog <p> This blog is dedicated to helping other programmers understand how image search engines work . While a lot of computer vision concepts are theoretical in nature , I 'm a big fan of " learning by example " . My goal is to distill my life experiences in building image search engines into concise , easy to understand examples . <h> About Me <p> My name is Adrian Rosebrock . Ive been working in the startup world professionally for the past 8 years , while at the same time earning my PhD in Computer Science with a focus in Computer Vision and Machine Learning at the University @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485135 @185135/ <h> Learn computer vision in a single weekend ... <h> ... with the Practical Python and OpenCV eBook <p> Are you interested in computer vision and image processing , but do n't know where to start ? My new book is your guaranteed quick start guide to learning the fundamentals of computer vision and image processing using Python and OpenCV . <h> " You can teach me computer vision in a single weekend ? How is that possible ? " I 'll show you ... <h> Take a sneak peek at what 's inside ... <h> Inside Practical Python and OpenCV + Case Studies you 'll learn the basics of computer vision and OpenCV , working your way up to more advanced topics such as face detection , object tracking in video , and handwriting recognition , all with lots of examples , code , and detailed walkthroughs . <h> Before you do anything else , take a look at the video to your left to see how my 275+ page book , 16 video tutorials covering 4+ hours of lessons , and downloadable Ubuntu virtual machine ( that @ @ @ @ @ @ @ @ @ @ OpenCV ninja , guaranteed . <h> Curious about computer vision ? Let me help . <h> I wrote this book for you for developers , programmers , and students who are interested in computer vision and image processing , but still need to learn the basics . <p> This book covers the fundamentals with tons of code examples that allow you to get your hands dirty , quickly and easily . Whether you are are a seasoned developer looking to learn more about computer vision , or a student at a university preparing for research in the computer vision field , this book is for you . <h> Learn how to detect faces in images and video . <h> By far , the most requested tutorial of all time on this blog has been " How do I find faces in images ? " If you 're interested in face detection and finding faces in images and video , then this book is for you . <p> We 'll start Case Studies by talking to my old dorm buddy , Jeremy , a college student interested in computer vision . @ @ @ @ @ @ @ @ @ @ exam , he instead becomes entranced by computer vision . <p> Jeremy applies face detection to both pictures and videos , and while his final grade in Algorithms is in jeopardy , at least he learns a lot about computer vision . <h> Explore object tracking in video . <h> A few months ago I taught a developer the basics of object tracking ... he then went on to build surveillance systems to track people in video . Curious how he learned so fast ? The secrets are in my book . <p> We 'll then chat with Laura , who works at Initech ( after it burned to the ground , allegedly over a red Swingline stapler ) updating bank software . She 's not very challenged at her job and she spends her night sipping Pinot Grigio , watching CSI re-runs . <p> Sick of her job at Initech , Laura studies up on computer vision and learns how to track objects in video . Ultimately , she 's able to leave her job at Initech and join their rival , Initrode , and build software used to @ @ @ @ @ @ @ @ @ @ at handwriting recognition with HOG . <h> You 've probably seen handwriting recognition software before , whether on your tablet or your iPad . But how do they do it ? I 'll show you . And then you 'll know the secret for yourself . <p> Next up , we 'll stop by Hank 's office . Hank and his team of programmers are consulting with the Louisiana post office , where they are tasked with building a system to accurately classify the zip codes on envelopes . <p> Unfortunately , Hank underbid on the job and he 's currently extremely stressed that the project will not be complete on time . If the job is n't done on time , profits will suffer and he might lose his job ! <p> Luckily , Hank remembers back to a machine learning course he took during his masters program . He is able to utilize Histogram of Oriented Gradients and a Linear Support Vector Machine to classify handwriting ... and save his job . <h> Master machine learning by classifying flower species . <h> Interested in classifying an image based @ @ @ @ @ @ @ @ @ @ I 'll show you how to use color histograms and a Random Forest Classifier to classify the species of flowers . After reading this chapter , you 'll be a pro at image classification ! <p> Let me tell you about my friend Charles . He works for The New York Museum of Natural History in the Botany department . One of Charles ' jobs is to classify the species of flowers in photographs . It 's extremely time consuming and tedious , but the museum pays handsomely for it . <p> Charles decides to create a system to automatically classify the species of flowers using computer vision and machine learning techniques . This approach will save him a bunch of time and allow him to get back to his research , instead of mindlessly classifying flower species . <h> Create the next startup by building an Amazon.com book cover search . <h> There 's no doubt that the next big startup is going to involve computer vision . Using the techniques in this chapter , you just might be able to launch the next big thing ... and make @ @ @ @ @ @ @ @ @ @ head to San Francisco to meet Gregory , the hotshot entrepreneur who is working with his co-founder to create a competitor to Amazon 's Flow . Flow allows users to use the camera on their smartphones as a digital shopping device . By simply taking a picture of a cover of a book , DVD , or video game , Flow automatically identifies the product and adds it to the user 's shopping cart . <p> Three weeks ago , Gregory and I went to a local bar to have a couple beers . I guess he had one too many , because guess what ? <h> Use your Raspberry Pi to build awesome computer vision projects . <h> Do you want to use your Raspberry Pi to detect faces in images , track objects in video , or recognize handwriting ? No problem ! All source code examples for both Practical Python and OpenCV + Case Studies are guaranteed to run on the Raspberry Pi 2 right out of the box . No code modifications required ! <p> Do you own a Raspberry Pi ? Do you want to @ @ @ @ @ @ @ @ @ @ No problem , I 've got you covered ! <p> Both Practical Python and OpenCV + Case Studies include Python and OpenCV source code examples that are guaranteed to run on your Raspberry Pi 2 right out of the box . <h> Not sold yet ? Get a free sample chapter ! <h> Come code alongside me . <h> Imagine having me at your side , helping you learn computer vision and OpenCV that 's exactly what it 's like when you work through my 16 video tutorials covering 4+ hours of lessons . Together , we 'll walk through each line of code as I detail my thought process and rational to how I 'm solving each computer vision project . Plus , these videos contain tips &amp; tricks I do n't cover in the books ! <p> Let 's face it . Reading tutorials from a book is n't always the best way to learn a new programming language or library . I 've often found that it 's much easier for me to learn a new skill if I can actually watch someone doing it first and @ @ @ @ @ @ @ @ @ @ 's exactly why I have put together 16 videos covering over 4+ hours of lessons from Practical Python and OpenCV . If you 're the type of person that learns by watching , then you 'll want to grab these videos they are a fantastic asset to help you learn OpenCV and computer vision . <p> I recognize the fact that setting up your development environment is n't the most fun thing in the world not to mention that it 's also quite time consuming ! In order to get you learning as fast as possible , I have created a downloadable Ubuntu VirtualBox virtual machine that has all the computer vision and image processing libraries you will need pre-installed . <h> OpenCV + Raspbian : Pre-configured and pre-installed . <p> I ran the numbers and determined that even if you know exactly what you are doing it can take over 2.2 hours to compile and install OpenCV on your Raspberry Pi ( I know from experience ) . When I sampled a group of novice readers , I found their install experience jumped nearly 4x to over 8.7 hours @ @ @ @ @ @ @ @ @ @ to learn the basics of computer vision and image processing I have released my own personal Raspbian . img file with OpenCV pre-installed . This is the exact Raspbian image I use for my own projects and is compatible with both the Raspberry Pi 2 and Raspberry Pi 3 . <p> If you 're looking to get your Raspberry Pi up and running with OpenCV + Python , this is by far the easiest method . <h> This is your exclusive hardcopy edition of Practical Python and OpenCV + Case Studies . Hot off the press , this hardcopy book is 275 pages of the most comprehensive guide to learning computer vision and OpenCV that you can get . It 's yours , shipped anywhere in the world for free . <p> There is just something about a hardcopy of a book that ca n't be beat . The feel of the book in your hands . The crisp sound as pages turn . And not to mention , it looks beautiful on your bookshelf as well ! I 've wanted to offer Practical Python and OpenCV + Case Studies @ @ @ @ @ @ @ @ @ @ of the book over a year ago , but I struggled to find a publisher . That 's all changed now , the hardcopy editions are ready to go ! If you like the feeling of having a book in your hands , then the hardcopy edition is a must have . <h> Claim your access to the exclusive companion website . <h> Let 's be realistic there 's only so much content that I can fit into a book ... and that 's exactly why I 've created the Practical Python and OpenCV companion website . This companion website is the ultimate guide to making the most out of the material in my book and the PyImageSearch blog . <p> Your purchase of Practical Python and OpenCV includes complementary access to the book 's exclusive companion website . This website provides : <h> The Practical Python and OpenCV + Case Studies method really works ... <h> Do n't waste time installing .... <p> ... invest your time learning and jump start your computer vision education . In order to get you learning as fast as possible , I have @ @ @ @ @ @ @ @ @ @ computer vision and image processing libraries you will need pre-installed . Get the eBook + <h> Learn the fundamentals of image processing <p> Practical Python and OpenCV + Case Studies covers the very basics of computer vision , starting from answering the question " what 's a pixel ? " to working your way up to more challenging tasks such as edge detection , thresholding , and finding objects in images and counting them , all with lots of examples and code . Get the eBook + <h> Lots of visual examples , lots of code <p> You probably learn by example . This book is tremendously example driven . When I first set out to write this book , I wanted it to be as hands-on as possible . I wanted lots of visual examples with a ton of example of code . I wanted to write something that you could easily learn from , without all the rigor and detail of mathematics . You do n't need a college education to understand the examples in this book.Get the eBook + <h> Satisfy your curiosity <p> I 'm willing @ @ @ @ @ @ @ @ @ @ . And from Facebook to Flickr , we now have more images than ever ! Ask yourself , what does your imagination want to build ? Let it run wild . And let the computer vision techniques introduced in this book help you build it . Get the eBook + <h> The right formula for learning <p> Practical Python and OpenCV + Case Studies is an accessible 275+ page book written for developers , programmers , and students just like you who are looking to learn the fundamentals of computer vision and image processing . Get the eBook + <h> Computer vision is n't magic <p> You can learn computer vision . Let me teach you . What makes a computer " see " ? How does it understand what 's in an image ? And how can you learn to program your computer to interpret images ? Practical Python and OpenCV covers the image processing essentials to get you started in the world of computer vision . Get the eBook + <h> See , I 've distilled the basics down to the very core a quick read with tons @ @ @ @ @ @ @ @ @ @ for people looking to get started with computer vision . It will walks you through the most important functions in OpenCV that you 'll need for any serious computer vision project . <p> Practical Python and OpenCV is a non-intimidating introduction to basic image processing tasks in Python . While reading the book , it feels as if Adrian is right next to you , helping you understand the many code examples without getting lost in mathematical details . <h> Enjoy a 100% money back guarantee . <p> After reading my book , if you have n't learned the basics of computer vision and image processing , then I do n't want your money . That 's why I offer a 100% Money Back Guarantee . Simply send me an email and ask for a refund , up to 30 days after your purchase . With all the copies I 've sold , I count the number of refunds on one hand . My readers are satisfied and I 'm sure you will be too . <h> I have a bundle tailored to your needs <h> Basic Bundle <p> $47 @ @ @ @ @ @ @ @ @ @ and OpenCV + Case Studies is where I spent the most time and effort . The code listings , easy to follow explanations , and practical advice and tips are critical to learning the foundations of image processing and computer vision . And while the video tutorials , virtual machine , and hardcopy edition of the book are great to jumpstart your computer vision education , let 's not overlook the eBooks themselves . You will still be getting 275+ pages of the best introduction to computer vision and image processing you can possibly get . <p> So , if budget is an issue , then I highly recommend getting the Basic Bundle . You wo n't regret it and you 'll always be able to upgrade your bundle if you want . <p> The complete 3rd edition eBooks in PDF and Kindle format <p> All source code listings , example images , and datasets used in both books <p> The original 1st edition of Practical Python and OpenCV + Case Studies which covers OpenCV 2.4 <h> Quickstart ( Most Popular ) <p> Are you ready to learn computer vision @ @ @ @ @ @ @ @ @ @ do n't waste your time configuring and installing packages . Choose this option and you 'll receive BOTH the Practical Python and OpenCV + Case Studies eBooks , PLUS a downloadable Ubuntu VirtualBox virtual machine that comes with all your computer vision and image processing libraries pre-installed . You 'll also receive 9 videos with over 2+ hours of tutorials from the books along with my Raspbian . img file that ships with OpenCV pre-installed . <p> This option is seriously the quickstart method ! By purchasing this package you 'll be able to dive head first into the world of computer vision and follow along with each and every example in these books within minutes of your purchase . <p> When you upgrade to the Quickstart Bundle you 'll receive : <p> The complete 3rd edition eBooks in PDF and Kindle format <p> 9 videos covering over 2+ hours of tutorials from Practical Python and OpenCV <h> Hardcopy Bundle <p> $197 <p> Everything you need to become an OpenCV ninja <p> For the first time ever , I am proud to offer hardcopy editions of Practical Python and OpenCV @ @ @ @ @ @ @ @ @ @ digital eBook editions , but also an exclusive , hardcopy edition as well <p> There is just something about the hardcopy edition of the book that ca n't be beat . The feel of the book in your hands . The crisp sound as the pages turn . And not to mention , it looks beautiful on your bookshelf . <p> This bundle is the complete package . When you purchase this collection you 'll receive the digital eBooks , the hardcopy edition , mailed to your doorstep , the downloadable Ubuntu VirtualBox virtual machine so you can start learning OpenCV instantly , my pre-configured Raspbian . img , and 16 videos covering over 4+ hours of tutorials from the books . If you 're serious about learning computer vision and OpenCV , there is no doubt in my mind that this is the best bundle for you . <p> I can guarantee you that Case Studies and Practical Python and OpenCV are the best books to teach you OpenCV and Python right now . <p> One June Kang <p> Student at University of Southern California <p> First of all @ @ @ @ @ @ @ @ @ @ really valuable and helpful . It gave me a good grasp on my path to learning computer vision/image processing . Now I have a good starting point to continue learning , exploring , and how to apply the OpenCV library in my new ideas . <p> I was lost for a couple of months until I ran into the books Practical Python and OpenCV and Case Studies . From that moment , I was able to face my university final project with utter confidence . Adrian 's writing style is clear , straightforward , and very easy to understand , but also very close and entertaining . I 'm happy I found it . <p> Eduardo Valenzuela <p> Student at University of Granada , Spain <h> The Team Bundle Your all-access pass <h> Jumpstart your entire team with the skills they need to solve real-world computer vision problems . Get everything in the Quickstart Bundle above plus : <h> 45-minute video chat <p> You 've gone through the books , but still have a few followup questions ? Want to go straight to the source ? Select this option and @ @ @ @ @ @ @ @ @ @ your team . <h> Share with your team <p> Practical Python and OpenCV + Case Studies are great to get your entire development or research team up to speed with Python and OpenCV . Since you care as much about copyright as I do , I have a team license that allows you to share the books + virtual machine with up to 10 members of your team . No , there is n't any DRM involved just trust . You 'll also receive 5 hardcopy editions that you can distribute to members of your team . <h> Get answers to your questions <p> You have specific questions about a computer vision problem you are trying to solve and I 'm here to help . I 'll answer your questions and get you and your team instantly on track . <h> Here are some common questions that I get asked ... <h> Which bundle should I buy ? <p> This mostly depends on your budget . Obviously the Hardcopy Bundle is the best since you get the complete package , but the eBooks themselves are still 275+ pages of the @ @ @ @ @ @ @ @ @ @ and tutorials you can find . <h> What if I hate the book ? <p> Well , hate is a strong word ... but if you honestly hate this book and feel like you have n't learned the basics of computer vision , then I do n't want your money . Just reply to your purchase receipt email within 30 days and I 'll refund your purchase . <h> Why Python ? <p> First of all , Python is awesome . Secondly , Python is the best way to learn the basics of computer vision . The simple , intuitive syntax allows you to focus on learning the basics of computer vision , rather than spending hours fixing crazy compiler errors in C/C++ . <h> Why this book ? <p> Practical Python and OpenCV is your best , guaranteed quick-start guide to learning the basics of computer vision and image processing . Whether you 're new to the world of computer vision or already know a thing or two , this book can teach you the basics in a single weekend . I guarantee it . <h> Can you really @ @ @ @ @ @ @ @ @ @ Yes , I absolutely guarantee it . In fact , I 'm so confident that I 'm offering a 100% money back guarantee on it . <h> What if I 'm beginner at computer vision ? <p> This book is designed for you . It gives you the very basics of computer vision and image processing using Python and OpenCV . This book is tremendously example driven and is as hands-on as possible . With me as a coach by your side , you 'll learn by getting your hands dirty . It 's the best way to learn ! <h> What versions of Python + OpenCV are used ? <p> The 2nd and 3rd editions of Practical Python and OpenCV + Case Studies covers Python2.7+ , Python 3+ , and OpenCV 3 . The 1st edition of the book ( also included in the download of all bundles ) covers Python 2.7 and OpenCV 2.4 . X. <h> Is OpenCV 3 covered ? <p> You bet it is ! The second edition of Practical Python and OpenCV + Case Studies covers OpenCV 3 . All source code listings will @ @ @ @ @ @ @ @ @ @ I 've heard that OpenCV is a real pain to install ... <p> OpenCV is n't like other Python packages . You ca n't let pip and easyinstall do the heavy lifting for you . You need to download it , configure it , and compile it . It 's a real time sink . In order to save you a bunch of time and hassle , I 've created a downloadable Ubuntu VirtualBox virtual machine with all the necessary computer packages you need pre-installed . Check out the Quickstart Bundle above . <h> I 'm just so busy right now ... <p> I have boiled down computer vision and image processing to the core topics without all the fluff . If you can give me less an hour a night , I can teach you the basics of computer vision using Python and OpenCV in no time . <h> I do n't have the money to buy your book ... <p> Think of it this way . Most computer vision textbooks cost well over $200 . And they do n't even include 4+ hours of video tutorials or a @ @ @ @ @ @ @ @ @ @ vision libraries pre-installed ! For less than the third of a cost of used textbook , you could be learning the basics of computer vision and image processing this weekend . <h> Can I purchase just a hardcopy of the book by itself ? <p> The hardcopy edition of Practical Python and OpenCV + Case Studies is only offered in the Hardcopy Bundle . As a self-published author , it 's not cheap to have copies of the books printed I also manually fulfill all orders myself . In order to make the hardcopies feasible , I need to charge a little extra and provide a ton of added value through the virtual machine and video tutorials . <h> Is shipping included in the price of the Hardcopy Bundle ? <p> Yes , shipping is already included in the price of the Hardcopy Bundle . <h> What countries do you ship to ? <p> I ship to all countries . If you have a particular concern about shipping , please contact me . <h> Where can I learn more about you ? <p> I have written a ton of blog @ @ @ @ @ @ @ @ @ @ . Definitely check out the posts to get a good feel for my teaching and writing style . I also suggest that you grab the sample chapter I am offering using the form above . <p> It 's safe to say that I have a ton of experience in the computer vision world and know my way around a Python shell and image processing libraries . I 'm here to distill all my years of experience into bite size , easy to understand chunks , while sharing the tips , tricks , and hacks I 've learned along the way . <p> If you are interested in computer vision and image processing but do n't know where to start , then this book is definitely for you . It 's the best , guaranteed quick start guide to learning the fundamentals of computer vision and image processing using Python and OpenCV. 
@@71485136 @185136/ <h> A guide to asking questions on the PyImageSearch blog <p> Over the past three years running PyImageSearch.com I have received and answered tens of thousands of questions from readers- just like yourself who are interested in studying computer vision , OpenCV , and deep learning . <p> Looking back on this time , I can say that the vast majority of the questions I have answered have been a- real pleasure to respond to . <p> Other inquires- required a bit of digging and going back and forth with the reader to resolve what the actual- underlying question was . That 's okay as well . Some questions are complicated and need to be " finessed " a bit until we can coax out the primary issue . <p> On the other hand , a small percentage of questions were a bit trying and tedious , requiring both the patience of myself and the person asking the question . These tough questions were not " difficult " in their content or subject matter , but rather figuring out what the reader was trying to accomplish - and doing so @ @ @ @ @ @ @ @ @ @ and theirs . <h> A guide to asking questions on the PyImageSearch blog <p> The reason is because PyImageSearch has grown over the past three years - a lot . <p> I now receive- hundreds of emails per day asking questions regarding computer vision , OpenCV , and deep learning . <p> Do n't  get me wrong : <p> Its a pleasure interacting with you , answering your questions , and learning from you : <p> Its the highlight of my day . <p> However , the- sheer volume of the questions has required me to formalize the process a bit to help make Q&amp;A as efficient as possible . <p> Since I do n't  want to resort to using awkward , impersonal surveys or a " weekly roundup " of top questions ( where only a small percentage of your questions get answered ) , I 've decided the best course of action is to create a guide that provides : <p> An- outline and- template that you can use when crafting your own questions . <p> Examples of both positive and negative qualities of- actual questions I have received @ @ @ @ @ @ @ @ @ @ these resources I am confident that we can both benefit through more efficient question and answering . <p> Remember , my end goal is to- help you , but you need to- help me understand your question first . <p> Note : All names , universities , and affiliations have been anonymized in this blog post to protect the identify of the reader who asked the question . Any real-life correlation with the names , questions , or affiliations is purely coincidental. - <h> Why its in your best interest to ask questions ( and learn from them ) <p> Asking questions is a fundamental aspect of computer science ( and all sciences in general ) . <p> In fact , questions are the cornerstone of the scientific method : <p> Figure 1 : - The five fundamental steps of the scientific method . <p> This process of asking a question , running an experiment , measuring/evaluating the results , drawing a conclusion , and repeating the process allows us to acquire new knowledge . <p> Through these questions we find- growth both in terms of ourselves , but also @ @ @ @ @ @ @ @ @ @ As a student of computer vision , - its in your best interest- to ask questions this enables you to- gain new insights , - eliminate confusion , and- solve projects you are working on . <p> That said , there are best practices we can apply to ensure we ask the- right types of questions . <p> You see , asking questions is an acquired skill , similar to an art . <p> It takes time to master but once you do , it unlocks a world where others ( such as myself ) can better help you understand computer vision . <p> Key Takeaway : The more clarity you can provide upfront regarding your question , the better I can do at helping point you in the right direction and solving your problem ( but you 'll still need to do the hard work yourself ) . <p> To learn the best practices when asking a question here on the PyImageSearch blog , keep reading . <h> An outline to asking questions <p> Whether emailing myself or any other subject matter expert , I would suggest following this simple @ @ @ @ @ @ @ @ @ @ . <p> Explain why you are asking . <p> Mention what computer vision experience ( if any ) you have . <p> Let 's dive into each of these bullet points . <h> All good questions start with an introduction <p> I consider it a- privilege and an- honor to run the PyImageSearch blog and I want to get to know you . <p> Also , if you are emailing me for the first time , its helpful for me to know how you found the PyImageSearch blog ( ex. , Google , recommendation from friend/colleague , link from StackOverflow/reddit/etc . ) . <p> Reading this guide before you ask your question will save us- both a lot of of time and ensure your question is answered properly . <h> After you ask your question , explain- why you are asking <p> Its- crucial- for not only me ( but yourself as well ) to understand the context of- why you are asking a particular question . <p> If you 're a computer vision hobbyist hacking on your Raspberry Pi on the weekends , my suggestion of techniques to utilize will be @ @ @ @ @ @ @ @ @ @ to do state-of-the-art research . <p> Take the time to understand the- why behind your question and make sure you relay it to me the more context I have regarding the problem that your question is stemming from , the better I will be at helping you . <p> Just like the context and why behind a question are important , its also pivotal to understand your experience level with computer vision . <p> If you do n't  have any experience with computer vision , that 's 100% okay but I do need to know this so I can recommend a path forward for you that includes the fundamentals . <p> Similarly , if you know the basics of computer vision and OpenCV , be sure to let me know so I can recommend more advanced techniques to you . <p> Finally , if you 're doing- research at a university of institution , that 's also really help for me to know so I can try to point you towards publications that may be of interest . <h> A template to asking questions <p> Combining the question asking outline above , I @ @ @ @ @ @ @ @ @ @ template to help you out when formulating your question and emailing- me ( or any other subject matter expert ) : <p> Hi Adrian , <p> My name is firstname . I first found you because of source . <p> I am interested in topic . <p> My question is : <p> question <p> The reason I am asking is because- questionreason . <p> My experience with topic is experiencelevel . <p> Thanks , <p> firstname <p> To understand why its important to apply this template , let 's consider a- subject- entirely separate from computer vision - planning a trip . Let 's pretend that I want to visit Cambodia and in order to plan this trip I am going to email Karen , my ( fictional ) travel agent . <p> Without using the template , my initial email to Karen may look like this : <p> Hi Karen , <p> I want to visit Cambodia . Please help me . <p> Adrian <p> This email is n't very helpful to Karen . Besides me stating that that I want to visit Cambodia , she has- nothing to go on : @ @ @ @ @ @ @ @ @ @ of year do I want to visit ? <p> Is this trip for business or for pleasure ? <p> What is my budget for the trip ? <p> Have I been to Cambodia before ? <p> Am I interested in doing any " sight seeing " while in Cambodia ? <p> Its plain to see that based off of my initial inquiry- its going to take a few back-and-forth emails between myself and Karen before she has a clear picture of what my intentions are when visiting Cambodia . <p> However , - if I were to use the template above my initial email becomes more much clear : <p> Hi Karen , <p> My name is Adrian . I was referred to you from my friend , Trisha she said you were an excellent travel agent . <p> At some point in the next year I would like to visit Cambodia . <p> I have never been to Cambodia before , but I have relatives who live there and I would like to visit them . They do n't  speak much english so its important that I have a @ @ @ @ @ @ @ @ @ @ , car rentals , accommodations , etc . ) . <p> I also do n't  know what time of year is best to visit Cambodia . Can you provide suggestions on the best time of year to visit ? <p> My budget for the trip is $1,500 . What can I do with this budget to ensure I can afford a flight and still see all the major sites while I 'm there ? <p> Thanks ! <p> -Adrian <p> At this point Karen knows- exactly what my intentions are and can better help me . She still may have a few clarifying questions , but at least the groundwork has been laid . <p> The point here is that I need to first- help myself by formulating a question that Karen can better answer . From there , Karen can better help me . <h> How to apply this template to your questions on the PyImageSearch blog <p> Let 's pretend that my name is Steve and I am interested in- Content-based Image Retrieval ( CBIR ) or more simply , - image search engines . <p> Given that Steve is @ @ @ @ @ @ @ @ @ @ can recommend a path to him that will : <p> Teach him the fundamentals of computer vision and image processing . <p> Educate him on various color , shape , and texture image descriptors . <p> Guide Steve on how to build a scalable image search engine . <p> Again , Steve walks into this question with an open mind . <p> He s not looking for a " done for him " solution , but rather- a path to understanding image search engines- so that he may complete his project . <p> As well see throughout the rest of these example equations , there are rarely ( if ever ) " done for you " solutions in the computer vision field . Even if there are , these solutions likely require you to tweak various knobs and levers . Without knowledge of the- basic fundamentals of computer vision and image processing , you 'll likely struggle to get these knobs dialed in correctly . <h> Example questions Ive received on the PyImageSearch blog <p> In the remainder of this guide well be looking at- actual questions I have received on @ @ @ @ @ @ @ @ @ @ these questions and pulling out both the positive and negative qualities of the questions so you may use this knowledge to- craft awesome computer vision questions of your own . <p> Or are we limited to just a- specific type of cup Jason has in his kitchen cabinet at home ? <p> I also do n't  know the reason- why Jason wants to build his cup recognition and tracking system : <p> Is he preparing for graduation and this cup tracker is supposed to be his final capstone project ? <p> Does he work for a company that mass produces cups and needs to detect , track , and count the number of cups on a conveyor belt ? <p> Or is he simply curious how to track and recognize cups as a hobby project ? <p> The options are endless and its impossible for me to discern the- " why " out of Jasons email . <p> Besides being fundamentally hard to answer , these types of questions are also- mentally taxing and tedious . <p> Because Jasons email lacks context and detail , it would take many back-and-forth @ @ @ @ @ @ @ @ @ @ goals of his project are . <p> If Jason can put more time into forming a question that includes- specific details on what he hopes to accomplish , then I would likely be able to help him more . Otherwise , it becomes very hard for me to respond ( or even justify the time spent ) on his question . <p> Emails like these I may respond to asking for more details but in the future these questions will receive a low priority and may not be answered at all . <p> If I were to rewrite this email as one that follows our template above ( making assumptions on Jason since I do n't  know what his motivations are ) , it would look like this : <p> SUBJECT : Contact Form Submission from Jason <p> Hi Adrian , <p> I am brand new to computer vision and OpenCV . I just found your blog a few hours ago and have read a few articles . <p> I have a hobby project in mind where I want to detect and track cups in videos . <p> Why cups @ @ @ @ @ @ @ @ @ @ I recently bought her brightly colored red , yellow , and blue plastic sippy cups to ensure she does n't  spill juice on the floor . <p> From the posts that I 've read , it seems like color can be used to find objects in an image and these cups seem like a good candidate . <p> Again , I am new to computer vision so this project is just a way for me to teach myself the basics . <p> How can I apply your techniques to detecting and tracking cups in video ? <p> Where should I start ? <p> Jason <p> In this rewritten example I would have all the information I need to answer Jasons question : <p> I know his computer vision and OpenCV experience . <p> I understand that he s interested in tracking a- specific type of cup . <p> And I know this is a hobby project for him . <p> Based on this information , I can provide Jason with recommendations to start his project , unlike in the original example where I would have to spend at least 3-4 emails @ @ @ @ @ @ @ @ @ @ purpose . <p> Being upfront with the goals of your project will save us- both a lot of time . <p> I am working on a project . I need to count the number of people that cross a line in a video . Plz send code . Thanks ahead of time . <p> Mark <p> Marks question ( which is n't actually a question ) is similar to Jasons above : very short and not much detail . <p> Again , I would struggle to answer this question . <p> I 'm not sure- why Mark wants to count the number of people in a video or even- what he is referring to by " person counting " . <p> However , based on my experience in the computer vision field ( and answering 100s of similar questions ) , I- assume Mark wants to build something like this : <p> But again , I would need more details on the project , such as : <p> Where is the system supposed to be deployed ? <p> In a small store ? <p> In a massive shopping mall ? <p> As @ @ @ @ @ @ @ @ @ @ they cross a finish line ? <p> What experience level does Mark have with computer vision and OpenCV ? <p> Has Mark tried any techniques prior to emailing me ? <p> Mark also asks for code to solve the problem for him . <p> Its important to understand that its- extremely rare for computer vision problems to have " off-the-shelf " solutions . <p> Sure , we have rules of thumb . <p> Best practices . <p> And basic recommendations . <p> But even the most- basic computer vision algorithms require a bit of tweaking from one problem to the next . <p> Even if Mark had the code to track people crossing a line in a video stream , without the understanding of- how these algorithms work , - he would still struggle to actually tune the knobs and dials to make the solution work in his- specific- instance . <p> If you find yourself asking for computer vision code but want to skip understanding how the code actually works , you are likely headed for trouble - take the time to understand the fundamentals first . <p> Mark @ @ @ @ @ @ @ @ @ @ into a good example question that follows our template above , I need to make a few assumptions : <p> Hi Adrian , <p> My name is Mark . I am database programmer for Oracle during the day , but help my grandfather operate his convenience store on nights and weekends . <p> It would be helpful to be able to count the number of people who walk in and out of the store on a daily basis so we can compute the most " busy " times of day and ensure more checkout clerks are on staff . <p> I found an example video online that demonstrates almost exactly what I 'm trying to accomplish : <p> I write code for a living so I 'm confident in my abilities , but I do n't  have any experience with computer vision . <p> Where should I start ? <p> Thanks , <p> Mark <p> Mark starts off this email by telling me about his day job : a database programmer , but then goes on to tell me that he helps his grandfather run a local convenience store this is helpful @ @ @ @ @ @ @ @ @ @ Mark is emailing me . <p> Mark then provides an example video of what he wants to accomplish . Again , this is helpful because it provides more- context . <p> Based on this information , I know- what project Mark is trying to build and- why he wants to solve it I can then give Mark recommendations to help him with his person counter . <h> Component #3 : Do your research <p> This next question comes from Margaret , a PyImageSearch reader who is interested in deep learning and the Keras library : <p> SUBJECT : Where can I learn about Keras ? <p> Hi Adrian , <p> My name is Margaret . I found your blog on Google when I was searching for deep learning tutorials . <p> I 'm currently working as a data scientist ( Python and OpenCV mostly ) analyzing breast histology images . I want to utilize deep learning to automatically analyze these images for breast cancer risk factors . <p> I 've heard that Keras its a good deep learning library and can be used for computer vision and recognition . <p> Do you @ @ @ @ @ @ @ @ @ @ <p> Margarets question starts off strong : <p> She introduces herself . <p> Mentions that she is using Python and OpenCV ( although it would be helpful to know a little more about this level of experience ) . <p> Explains her motivations in asking her question ( wanting to develop a system to detect breast cancer risk factors in images ) . <p> On the right-hand side of nearly every page on PyImageSearch you 'll find a " Search " box : <p> Figure 3 : Nearly every page on PyImageSearch.com includes a search box . <p> Typing- " keras " into this box will return all PyImageSearch pages that mention Keras : <p> Figure 4 : An example of searching PyImageSearch for a particular term . <p> Similarly , you could use a little Google-Fu- by heading directly to Google.com and searching using the- " site:pyimagesearch.com keras " query string : <p> Figure 5 : Using Google to search the PyImageSearch blog . <p> This will return a list of all PyImageSearch webpages that discuss Keras . <p> With a little more research Margaret could have answered her question @ @ @ @ @ @ @ @ @ @ : - should she be using Keras and where can she- study deep learning ? <p> Rewriting Margarets question following the template above , it may look like : <p> Hi Adrian , <p> My name is Margaret . I found your blog on Google when I was searching for deep learning tutorials ( specifically ones about Keras ) . <p> I 'm currently working as a data scientist using Python and OpenCV . My work mainly involves analyzing breast histology images to automatically determine cancer risk factors . <p> My research online has shown that deep learning can be really helpful in this task . <p> After reading your tutorials it seems that you use Keras a lot . Would you recommend Keras to me for this project ? <p> To help you get an idea of what I 'm working with , I 've attached two example images : one of a low-risk breast cancer image and another of a high-risk image . <p> Margaret <p> This rewritten example provides me with all the information I need to answer Margarets question . <p> She also provides me with a couple- example @ @ @ @ @ @ @ @ @ @ like this added context is- super helpful when answering questions . <h> Component #4 : Ask your question ( and be respectful of others time ) <p> One of the most important aspects of asking a question is being- respectful- of another persons time . <p> While the internet is ultimately a positive source of information , we sometimes forget there is an- actual human on the other side of the line answering our questions ( not a mindless vending machine that dispenses information on our command ) . <p> Adityas question/request below is an example of an inquiry that could do better by being respectful of others time : <p> SUBJECT : Help <p> Sir , I am studying object detection for my masters . <p> Please provide a list of all papers on the same . <p> Thank you ahead of time . <p> Aditya <p> Aditya is a masters student who is doing research in object detection this is a- great sub-field of computer vision to study . There is- much research to be done and we are- far from solving it . <p> The problem is @ @ @ @ @ @ @ @ @ @ provide him with an exhaustive list of papers on object detection . <p> While Im happy to point Aditya in the right direction with references to a handful of seminal object detection papers , such as : <p> Its also not my responsibility to maintain or publish such an exhaustive list of object detection papers . <p> In fact , as part of Adityas research he should be doing this survey of prior work himself . <p> To be totally honest with you , these questions are hard for me on a personal level : <p> I- love helping you learn computer vision , OpenCV , and deep learning . <p> I- love helping you reach your goals . <p> But I would also kindly ask that readers be : <p> Respectful of my time . <p> Realistic in their questions and requests . <p> Doing this ensures that I will be able to help you better . <p> Adityas request is also quite frankly rude . <p> That said , I do like to give people the benefit of the doubt . <p> Perhaps due to a language barrier @ @ @ @ @ @ @ @ @ @ <p> Or perhaps Aditya is honestly seeking help but does n't  know the right way of going about it . <p> If I were to rewrite Adityas request as a question that follows our template above , it would look like this : <p> SUBJECT : Need help starting object detection <p> Hi Adrian , <p> I am a first year masters student interested in studying object detection . <p> I plan on writing my thesis on object detection but I first need to do my initial survey of work . <p> Ive come across more recent deep learning techniques such as Faster R-CNNs , YOLO , and SSDs , but I am curious about non-deep learning approaches . <p> Are there any popular techniques that I should be considering ? <p> Aditya <p> In this updated email we can see that Aditya has taken the time to clarify his question a bit . <p> He mentions that he is a masters student in his first year and needs to understand what prior research has been done on object detection . <p> He has also demonstrated that he has taken @ @ @ @ @ @ @ @ @ @ deep learning . <p> While Im confident that with time Aditya will come across the more classic Haar cascade and HOG + Linear SVM papers , I do n't  mind answering this question Aditya has clearly put in effort and I wholeheartedly respect that . <h> Component #5 : Repeat the process as necessary <p> Our final example comes from Abram : <p> SUBJECT : Deep learning and object detection . <p> Hi Adrian , <p> I 'm currently studying object detection ( mainly using HOG + Linear SVM ) during my winter break at the university . I hope to do my dissertation on the topic next year . <p> Thank you for your blog post on using Intersection of Union as an evaluation metric for object detection : <p> You mentioned in the blog post that deep learning can also be used for object detection . I did some research on the topic and found " Faster R-CNNs " , " Single Shot Detectors " , and " You Only Look Once " , but I 'm not sure how they work " the papers are very dense . <p> @ @ @ @ @ @ @ @ @ @ any of your courses ? I could n't find any detailed mention of them on the blog , perhaps I 'm missing a tutorial . <p> Thanks you for your time . <p> Abram <p> Abrams question here is a shining example of a- great question . <p> He starts by introducing himself . <p> He then let 's me know that he s a student and is currently studying object detection ( specifically the HOG + Linear SVM framework ) and that the hopes to write his PhD dissertation on object detection ( this helps me understand his- reason- and- purpose behind emailing me ) . <h> 8 Responses to A guide to asking questions on the PyImageSearch blog <p> Excellent post Adrian . I used to require coworkers ask me questions only via a validated web form because of all the same challenges you discussed . This allowed full transparency to priority , status , and the ability to reuse resolution paths for future related issues . People resisted the effort needed to structure their inquiry , but it was extremely effective and greatly increased my capacity and ability to schedule @ @ @ @ @ @ @ @ @ @ of sweat is indeed worth a pint of blood . The question , now , is : how to track pint . Please answer me . <p> Thanks Adrian for the detailed analysis . Your guide makes a lot of sense . Although you are taking examples from computer vision questions in the pyimagesearch , the approach of asking good questions extends to various aspects and areas . So this is extremely useful and helpful ! <p> Nice post , thanks . I used to answer OpenCV questions on Stack Overflow LONG ... But it seems like the quality of OpenCV questions has taken a serious nose dive there over the past couple years . This post should be required reading for anyone posting a question there . <p> I find that when I ask a question on SO , the due diligence I perform while composing a thorough question using principles very similar to those you describe here , I 'll often find my answer before I even have to post the question . Its more work than tossing out a quick , unresearched , question but is a lot @ @ @ @ @ @ @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485137 @185137/ <p> I 've got some exciting news to share today ! My- Deep Learning for Computer Vision with Python- Kickstarter campaign- is set to launch in- exactly one week- on- Wednesday , January 18th at 10AM EST . This book has only goal " - to help- developers , researchers , and students- just like yourself become- experts- in deep learning for image recognition and classification . Whether this is the- first time you 've worked with <p> Todays blog post is part of a two part series on working with video files using OpenCV and Python . The first part of this series will focus on a question emailed in by PyImageSearch reader , Alex . Alex asks : I need to count the total number of frames in a video file with OpenCV . The only <p> Let me tell you an embarrassing story of- how I- wasted three weeks of research time- during graduate school six- years ago . It was the end of my second semester of coursework . I had taken all of my exams early and all my projects for the semester had been- submitted @ @ @ @ @ @ @ @ @ @ started experimenting <p> Each week I receive and respond to at least 2-3 emails and 3-4 blog post comments regarding NoneType- errors in OpenCV and Python . For beginners , these errors can be hard to diagnose by definition they are n't  very informative . Since this question is getting asked so often I decided to dedicate an entire blog post <p> Over the past few weeks I have demonstrated how to compile OpenCV 3 on macOS with Python ( 2.7 , 3.5 ) bindings from source . Compiling OpenCV via source gives you- complete and total control over which modules you want to build , - how- they are built , and- where they are installed . All this control can come at a price though . The downside <p> You may have heard me mention it in a passing comment on the PyImageSearch blog Maybe I even hinted at it in a 1-on-1 email Or perhaps you simply saw the writing on the wall due to the recent uptick in Deep Learning/Neural Network tutorials here on the blog But I 'm here today to tell <p> Last week I @ @ @ @ @ @ @ @ @ @ on macOS Sierra and above . In todays tutorial well learn how to install- OpenCV 3 with- Python 3.5- bindings on macOS . I decided to break these install tutorials into two separate guides to keep them well organized and easy to follow . To learn how to <p> I 'll admit it : Compiling and installing OpenCV 3 on macOS Sierra was- a lot more of a challenge than I thought it would be , even for someone who has a compiled OpenCV on hundreds of machines over his lifetime . If you 've tried to use one of my previous tutorials on installing OpenCV on your freshly updated <p> Ever since I wrote the first PyImageSearch tutorial on installing OpenCV + Python on the Raspberry Pi B+ back in February 2015 it has been my dream to offer a- downloadable , pre-configured Raspbian . img file with OpenCV pre-installed . Today this dream has become a reality . I am pleased to announce that both the Quickstart Bundle and- Hardcopy <p> A few months ago I demonstrated how to install the Keras deep learning library with a- Theano backend . @ @ @ @ @ @ @ @ @ @ to install Keras using a TensorFlow backend , originally developed by the researchers and engineers on the Google Brain Team . Ill also ( optionally ) demonstrate how @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485138 @185138/ <h> Finding extreme points in contours with OpenCV <p> In the remainder of this blog post , I am going to demonstrate how to find the extreme north , south , east , and west- ( x , y ) -coordinates along a contour , like in the image at the top of this blog post . <p> While this skill is n't inherently useful by itself , its often used as a pre-processing step to more advanced computer vision applications . A great example of such an application is- hand gesture recognition : <p> Figure 1 : Computing the extreme coordinates along a hand contour <p> In the figure above , we have segmented the skin/hand from the image , computed the convex hull- ( outlined in- blue ) of the hand contour , and then found the extreme points along the- convex hull- ( red circles ) . <p> By computing the extreme points along the hand , we can better approximate the palm region ( highlighted as a blue circle ) : <p> Figure 2 : Using extreme points along the hand allows us to approximate the @ @ @ @ @ @ @ @ @ @ us to recognize gestures , such as the number of fingers we are holding up : <p> Figure 3 : Finding extreme points along a contour with OpenCV plays a pivotal role in hand gesture recognition . <p> Note : - I cover how to recognize hand gestures- inside- the PyImageSearch Gurus course , so if you 're interested in learning- more , be sure to claim your spot in line for the next open enrollment ! <p> Implementing such a hand gesture recognition system is outside the scope of this blog post , so well instead utilize the following image : <p> Line 12- performs thresholding , allowing us to segment the hand region from the rest of the image . After thresholding , our binary image looks like this : <p> Figure 5 : Our image after thresholding . The outlines of the hand are- now revealed . <p> In order to detect the outlines of the hand , we make a call to cv2.findContours- , followed by sorting the contours to find the largest one , which we presume to be the hand itself ( Lines 18-21 ) @ @ @ @ @ @ @ @ @ @ contour , its important to understand that a contour is simply a NumPy array of- ( x , y ) -coordinates . Therefore , we can leverage NumPy functions to help us find the extreme coordinates . <p> Lines 26 and 27- perform the same operation , only for the- y-coordinate , giving us the " north " and " south " coordinates , respectively . <p> Now that we have our extreme north , south , east , and west coordinates , we can draw them on our image- : <p> Finding extreme points in <p> 39 <p> 40 <p> # draw the outline of the object , then draw each of the <p> # extreme points , where the left-most is red , right-most <p> # is green , top-most is blue , and bottom-most is teal <p> cv2.drawContours ( image , c , -1 , ( 0,255,255 ) , 2 ) <p> cv2.circle ( image , extLeft , 8 , ( 0,0,255 @ @ @ @ @ @ @ @ @ @ , 8 , ( 0,255,0 ) , -1 ) <p> cv2.circle ( image , extTop , 8 , ( 255,0,0 ) , -1 ) <p> cv2.circle ( image , extBot , 8 , ( 255,255,0 ) , -1 ) <p> # show the output image <p> cv2.imshow ( " Image " , image ) 55212 @qwx675212 <p> Line 32- draws the outline of the hand in- yellow , while- Lines 33-36- draw circles for each of the extreme points , detailed below : <p> West : Red <p> East : Green <p> North : Blue <p> South : Teal <p> Finally , - Lines 39 and 40- display the results to our screen . <p> To execute our script , make sure you download the code and images associated with this post ( using the " Downloads " form found at the bottom of this tutorial ) , navigate to your code directory , and then execute the following command : <p> As you can see we have successfully labeled each of the extreme points along the hand . The western-most point is labeled in- red , the northern-most point @ @ @ @ @ @ @ @ @ @ finally the southern-most point in- teal . <p> Below we can see a second example of labeling the extreme points a long a hand : <p> Figure 7 : Labeling extreme points along a hand contour using OpenCV and Python . <p> Let 's examine one final instance : <p> Figure 8 : Again , were are able to accurately compute the extreme points along the contour . <p> And that 's all there is to it ! <p> Just keep in mind that the contours list returned by cv2.findContours- is simply a NumPy array of- ( x , y ) -coordinates . By calling argmin()- and argmax()- on this array , we can extract the extreme- ( x , y ) -coordinates . <h> Summary <p> In this blog post , I detailed how to find the extreme north , south , east , and west- ( x , y ) -coordinates along a given contour . This method can be used on both- raw contours and- rotated bounding boxes . <p> While finding the extreme points along a contour may not seem interesting on its own , its actually a- @ @ @ @ @ @ @ @ @ @ preprocessing step to more advanced computer vision and image processing algorithms , such as- hand gesture recognition . <h> Downloads : 55217 @qwx675217 <p> Thanks for the useful code . In case of triangle , will it be possible to get the direction ( left/right , up/down ) of triangle if I have extreme points and center points . Can you help to find the direction of arrow ( exactly a triangle ) ? <p> If the triangle is a perfect triangle has you described then each line of the triangle will have the same length ( equilateral triangle ) . And if that 's the case , then the triangle is " pointing " in all three directions ( or no direction , depending on how you look at it ) . <p> Great post , it works flawlessly . But can you help provide hints/reasoning for my questions ? 1 . What is the purpose of GaussianBlur here ? 2 . I 've extended this into a live video stream and when my hand rotates back and forth there are times when there are a lot of blotches that do @ @ @ @ @ @ @ @ @ @ The Gaussian blur helps reduce high frequency noise . It blurs regions of the images we are uninterested in allowing us to focus on the underlying " structure " of the image in this case , the LCD screen and the box containing the thermostat . <p> 2 . Basic thresholding is best used under controlled lighting conditions . Adaptive thresholding can help with this , but is n't a sure-fire solution for each problem . <p> Cool stuff . I had some issues with some of my implementation . I think you can help . Here it goes the question . <p> I have a numpy array for a detected contour from which I have extracted extreme points in all four directions . Now I want extract 12 points . Let say if I start from a reference point ( Extreme-top ) after every 30 degree angle I want to get co-ordinates of a point . After all the traversing is done I 'd be having array of 12 points which could be given to next image processing algorithm . <p> If you have the 4 extreme coordinates , compute a @ @ @ @ @ @ @ @ @ @ i.e. , a minimum enclosing circle ) . Compute the ( x , y ) -coordinates in 30 degree increments along this circle . Then find the closest point in the contours list to this ( x , y ) -coordinate . This will take a bit of knowledge of trigonometry to complete @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485140 @185140/ <h> Archive Kickstarter <p> The- Deep Learning for Computer Vision with Python- Kickstarter is officially LIVE ! To back the Kickstarter campaign , just use the link below : LONG ... Remember , - there are only a handful of early bird spots at the reduced prices- " - you 'll definitely want to act now if you want to claim your spot ! Thank you so much for being supportive of myself <p> A couple of days ago- I mentioned that on- Wednesday , January 18th at 10AM EST- I am launching a Kickstarter to fund my new book " - Deep Learning for Computer Vision with Python . As you 'll see later in this post , there is a- huge- amount of content I 'll be covering , so Ive decided to break the book down into- three volumes- called- " bundles " . A- bundle- includes <p> Wow , the Kickstarter launch date of January 18th is approaching so fast ! I still have a ton of work to do and I 'm neck-deep in Kickstarter logistics , but I took a few minutes earlier today and recorded this @ @ @ @ @ @ @ @ @ @ just for you : The video is fairly short at <p> I 've got some exciting news to share today ! My- Deep Learning for Computer Vision with Python- Kickstarter campaign- is set to launch in- exactly one week- on- Wednesday , January 18th at 10AM EST . This book has only goal " - to help- developers , researchers , and students- just like yourself become- experts- in deep learning for image recognition and classification . Whether this is the- first time you 've worked with <p> You may have heard me mention it in a passing comment on the PyImageSearch blog Maybe I even hinted at it in a 1-on-1 email Or perhaps you simply saw the writing on the wall due to the recent uptick in Deep Learning/Neural Network tutorials here on the blog But I 'm here today to tell <p> This past Friday the PyImageSearch Gurus Kickstarter came to a close . The campaign was a huge success with 253 backers joining in . It was extremely humbling to see the support from you , the PyImageSearch readers , and the Kickstarter viewers . I feel extremely lucky @ @ @ @ @ @ @ @ @ @ what I <p> Were getting close to the end of the PyImageSearch Gurus Kickstarter " and we just hit our 2nd stretch goal ! This means that well be learning all about hand gesture recognition inside the PyImageSearch Gurus computer vision course ! Anyway , over the past few days I have been visiting family . Its amazing how much Ive missed <p> I need to apologize for this post- " if there are some glaringly obvious typos on grammatical errors , I 'm sorry . You see , I just pulled an all-nighter . My eyes are bloodshot and glazed over like something out of The Walking Dead . My brain feels like mashed potatoes beaten with a sledge hammer . And I really , really <p> Did you watch the Super Bowl this past weekend ? I did . Kind of . I spent Super Bowl Sunday ( which is practically a holiday in the United States ) at my favorite Indian bar . Pounding Kingfisher beers . Savoring a delicious dish of- Tandoori chicken all while hacking up a storm on my laptop and coding up some @ @ @ @ @ @ @ @ @ @ You guys are incredible PyImageSearch has the best , most supportive readers possible on the face of this earth . The PyImageSearch Gurus Kickstarter campaign went live today at 10am EST . By 10:25am EST , the project- was 100% funded ! It took less than 25 minutes to 100% @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485141 @185141/ <h> Tag Archives deep learning <p> In todays blog post , I interview Davis King , the creator and chief maintainer of dlib- a toolkit for real-world machine learning , computer vision , and data analysis in C++ ( with Python bindings included , when appropriate ) . I 've personally used dlib in a number of projects ( especially for object detection ) , so its quite the honor to be interviewing <p> The- Deep Learning for Computer Vision with Python- Kickstarter is officially LIVE ! To back the Kickstarter campaign , just use the link below : LONG ... Remember , - there are only a handful of early bird spots at the reduced prices- " - you 'll definitely want to act now if you want to claim your spot ! Thank you so much for being supportive of myself <p> A couple of days ago- I mentioned that on- Wednesday , January 18th at 10AM EST- I am launching a Kickstarter to fund my new book " - Deep Learning for Computer Vision with Python . As you 'll see later in this post , there is a- @ @ @ @ @ @ @ @ @ @ decided to break the book down into- three volumes- called- " bundles " . A- bundle- includes <p> Wow , the Kickstarter launch date of January 18th is approaching so fast ! I still have a ton of work to do and I 'm neck-deep in Kickstarter logistics , but I took a few minutes earlier today and recorded this sneak preview of- Deep Learning for Computer Vision with Python- just for you : The video is fairly short at <p> I 've got some exciting news to share today ! My- Deep Learning for Computer Vision with Python- Kickstarter campaign- is set to launch in- exactly one week- on- Wednesday , January 18th at 10AM EST . This book has only goal " - to help- developers , researchers , and students- just like yourself become- experts- in deep learning for image recognition and classification . Whether this is the- first time you 've worked with <p> You may have heard me mention it in a passing comment on the PyImageSearch blog Maybe I even hinted at it in a 1-on-1 email Or perhaps you simply saw the writing on the wall @ @ @ @ @ @ @ @ @ @ here on the blog But I 'm here today to tell <p> A few months ago I demonstrated how to install the Keras deep learning library with a- Theano backend . In todays blog post I provide detailed , step-by-step instructions to install Keras using a TensorFlow backend , originally developed by the researchers and engineers on the Google Brain Team . Ill also ( optionally ) demonstrate how you can integrate OpenCV into <p> In last weeks blog post , we discussed- gradient descent , a first-order optimization algorithm that can be used to learn a set of classifier coefficients for parameterized learning . However , the " vanilla " implementation of gradient descent can be prohibitively slow to run on large datasets in fact , it can even be considered- computationally wasteful . Instead , we should apply- Stochastic <p> Every relationship has its building blocks . Love . Trust . Mutual respect . Yesterday , I asked my girlfriend of 7.5 years to marry me . She said yes . It was quite literally the happiest day of my life . I feel like the luckiest guy in @ @ @ @ @ @ @ @ @ @ but also because this incredible PyImageSearch <p> If you 've been following along with this series of blog posts , then you already know what a- huge fan I am of Keras . Keras is a super powerful , easy to use Python library for building neural networks and deep learning networks . In the remainder of this blog post , I 'll demonstrate how @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485142 @185142/ <p> As Reddit users were quick to point out , utilizing computer vision to recognize digits on a thermostat tends to- overcomplicate the problem a simple data logging thermometer would give much more reliable results with a fraction of the effort . <p> On the other hand , applying computer vision to projects such as these are really good practice . <p> Whether you are just getting started with computer vision/OpenCV , or you 're already writing computer vision code on a daily basis , taking the time to hone your skills on mini-projects are paramount to mastering your trade - in fact , I find it so important that I do exercises like this one twice a month . <p> Every other Friday afternoon I block off two hours on my calendar and practice my basic image processing and computer vision skills on computer vision/OpenCV questions Ive found on Reddit or StackOverflow . <p> Doing this exercise helps me keep my skills sharp it also has the added benefit of making great blog post content . <p> In the remainder of todays blog post , I 'll demonstrate how to @ @ @ @ @ @ @ @ @ @ Recognizing digits with OpenCV and Python <p> In the first part of this tutorial , well discuss what a seven-segment display- is and how we can apply computer vision and image processing operations to recognize these types of digits ( no machine learning required ! ) <p> From there I 'll provide actual Python and OpenCV code that can be used to recognize these digits in images . <h> The seven-segment display <p> You 're likely already familiar with a seven-segment display , even if you- do n't  recognize the particular term . <p> A great example of such a display is your classic digital alarm clock : <p> Figure 1 : A classic digital alarm clock that contains four seven-segment displays to represent the time of day . <p> Each digit on the alarm clock is represented by a seven-segment component just like the one below : <p> Figure 2 : An example of a single seven-segment display . Each segment can be turned " on " or " off " to represent a particular digit ( source : Wikipedia ) . <h> Planning the OpenCV digit recognizer <p> Figure 5 @ @ @ @ @ @ @ @ @ @ recognize the digits on the thermostat using OpenCV and Python . <p> Whenever I am trying to recognize/identify object(s) in an image I first take a few minutes to assess the problem . Given that my end goal is to recognize the digits on the LCD display I know I need to : <p> Step #1 : Localize the LCD on the thermostat. - This can be done using edge detection since there is enough contrast between the plastic shell and the LCD . <p> Step #2 : Extract the LCD . Given an input edge map I can find contours and look for outlines with a rectangular shape the largest rectangular region should correspond to the LCD . A perspective transform will give me a nice extraction of the LCD . <p> Step #3 : Extract the digit regions . Once I have the LCD itself I can focus on extracting the digits . Since there seems to be contrast between the digit regions and the background of the LCD I 'm confident that thresholding and morphological operations can accomplish this . <p> Step #4 : Identify the digits . @ @ @ @ @ @ @ @ @ @ digit ROI into seven segments . From there I can apply pixel counting on the thresholded image to determine if a given segment is " on " or " off " . <p> So see how we can accomplish this four-step process to digit recognition with OpenCV and Python , keep reading . <h> Recognizing digits with computer vision and OpenCV <p> Let 's go ahead and get this example started . <p> Open up a new file , name it recognizedigits.py- , and insert the following code : <p> Recognizing 18 <p> 19 <p> 20 55203 @qwx675203 <p> fromimutils.perspective importfourpointtransform <p> fromimutils importcontours 55219 @qwx675219 <p> importcv2 <p> # define the dictionary of digit segments so we can identify <p> # each digit on the thermostat <p> DIGITSLOOKUP= <p> ( 1,1,1,0,1,1,1 ) : 0 , <p> ( 0,0,1,0,0,1,0 ) : 1 , <p> ( 1,0,1,1,1,1,0 ) : 2 @ @ @ @ @ @ @ @ @ @ 0,1,1,1,0,1,0 ) : 4 , <p> ( 1,1,0,1,0,1,1 ) : 5 , <p> ( 1,1,0,1,1,1,1 ) : 6 , <p> ( 1,0,1,0,0,1,0 ) : 7 , <p> ( 1,1,1,1,1,1,1 ) : 8 , <p> ( 1,1,1,1,0,1,1 ) : 9 <p> <p> Lines 2-5 import our required Python packages . Well be using imutils , my series of convenience functions to make working with OpenCV + Python easier . If you do n't  already have imutils- installed , you should take a second now to install the package on your system using pip- : <p> Recognizing digits with OpenCV and Python <p> Shell <p> 1 55204 @qwx675204 <p> Lines 9-20 define a Python dictionary named DIGITSLOOKUP- . Inspired by the approach of /u/JonnoFTW in the Reddit thread , we can easily define this lookup table where : <p> They- key to the table is the seven-segment array . A- one in the array indicates that the given segment is- on and a- zero indicates that the segment is- off . <p> The- value is the actual numerical digit itself : - 0-9 . <p> Once we identify the segments in the @ @ @ @ @ @ @ @ @ @ table and obtain the digit value . <p> For reference , this dictionary uses the same segment ordering as in- Figure 2- above . <p> If our approximated contour has four vertices then we assume we have found the thermostat display ( Lines 48-50 ) . This is a reasonable assumption since the largest rectangular region in our input image should be the LCD itself . <p> Obtaining this view of the LCD satisfies Step #2 we are now- ready to extract the digits from the LCD : <p> Recognizing digits with OpenCV and Python <p> Python <p> 57 <p> 58 <p> 59 <p> 60 <p> 61 <p> 62 <p> # threshold the warped image , then apply a series of morphological <p> # operations to cleanup the thresholded image <p> **33;4104;TOOLONG , <p> **33;4139;TOOLONG ) 1 <p> LONG ... <p> LONG ... <p> To obtain the digits themselves we need to threshold the warped- image ( Lines 59 and 60 ) to reveal the dark regions ( i.e. , digits ) against the lighter background ( i.e. , the background of the LCD display ) : <p> We then @ @ @ @ @ @ @ @ @ @ thresholded image ( Lines 61 and 62 ) : <p> Figure 9 : Applying a series of morphological operations cleans up our thresholded LCD and will allow us to segment out each of the digits . <p> Now that we have a nice segmented image we once again need to apply contour filtering , - only this time we are looking for the actual digits : <p> Recognizing digits with <p> 77 <p> 78 <p> # find contours in the thresholded image , then initialize the <p> # digit contours lists <p> LONG ... 55211 @qwx675211 <p> **36;4174;TOOLONG <p> digitCnts= <p> # loop over the digit area candidates <p> forcincnts : <p> # compute the bounding box of the contour <p> ( x , y , w , h ) =cv2.boundingRect(c) <p> # if the contour is sufficiently large , it must be a digit <p> **34;4212;TOOLONG : <p> digitCnts.append(c) <p> To accomplish this we find contours in our @ @ @ @ @ @ @ @ @ @ also initialize the digitsCnts- list on- Line 69 this list will store the contours of the digits themselves . <p> Line 72 starts looping over each of the contours . <p> For each contour , we compute the bounding box ( Line 74 ) , ensure the width and height are of an acceptable size , and if so , update the digitsCnts- list ( Lines 77 and 78 ) . <p> Note : Determining the appropriate width and height constraints requires a few rounds of trial and error . I would suggest looping over each of the contours , drawing them individually , and inspecting their dimensions . Doing this process ensures you can find commonalities across digit contour properties . <p> If we were to loop over the contours inside digitsCnts- and draw the bounding box on our image , the result would look like this : <p> Figure 10 : Drawing the bounding box of each of the digits on the LCD . <p> Sure enough , we have found the digits on the LCD ! <p> The final step is to actually- identify each of the @ @ @ @ @ @ @ @ @ @ Python <p> 80 <p> 81 <p> 82 <p> 83 <p> 84 <p> # sort the contours from left-to-right , then initialize the <p> # actual digits themselves <p> **41;4248;TOOLONG , <p> method= " left-to-right " ) 0 <p> digits= <p> Here we are simply sorting our digit contours from left-to-right based on their- ( x , y ) -coordinates . <p> This sorting step is necessary as there are no guarantees that the contours are already sorted from left-to-right ( the same direction in which we would read the- digits ) . <p> Next , comes the actual digit recognition process : <p> Recognizing digits with <p> 107 <p> 108 <p> # loop over each of the digits <p> forcindigitCnts : <p> # extract the digit ROI <p> ( x , y , w , h ) =cv2.boundingRect(c) <p> roi=threshy:y+h , x:x+w <p> @ @ @ @ @ @ @ @ @ @ 7 segments <p> # we are going to examine <p> ( roiH , roiW ) =roi.shape <p> ( dW , dH ) = ( int(roiW*0.25) , int(roiH*0.15) ) <p> dHC=int(roiH*0.05) <p> # define the set of 7 segments <p> segments= <p> ( ( 0,0 ) , ( w , dH ) ) , # top <p> ( ( 0,0 ) , ( dW , h//2 ) ) , # top-left <p> ( ( w-dW , 0 ) , ( w , h//2 ) ) , # top-right <p> ( ( 0 , ( h//2 ) -dHC ) , ( w , ( h//2 ) +dHC ) ) , # center <p> ( ( 0 , h//2 ) , ( dW , h ) ) , # bottom-left <p> ( ( w-dW , h//2 ) , ( w , h ) ) , # bottom-right <p> ( ( 0 , h-dH ) , ( w , h ) ) # bottom <p> <p> on=0*len(segments) <p> We start looping over each of the digit contours on- Line 87 . <p> For each of these regions , we compute the bounding box and @ @ @ @ @ @ @ @ @ @ . <h> Summary <p> In todays blog post I demonstrated how to utilize OpenCV and Python to recognize digits in images . <p> This approach is specifically intended for seven-segment displays- ( i.e. , the digit displays you would typically see on a digital alarm clock ) . <p> By extracting each of the seven segments and applying basic thresholding and morphological operations we can determine which segments are " on " and which are " off " . <p> From there , we can look up the on/off segments in a Python dictionary data structure to quickly determine the actual digit no machine learning required ! <p> As I mentioned at the top of this blog post , applying computer vision to recognizing digits in a thermostat image tends to overcomplicate the problem itself utilizing a data logging thermometer would be more reliable and require substantially less effort . <p> However , in the case that ( 1 ) you do not have access to a data logging sensor or ( 2 ) you simply want to hone and practice your computer vision/OpenCV skills , its often helpful to @ @ @ @ @ @ @ @ @ @ solve the project . <p> I hope you enjoyed todays post ! <p> To be notified when future blog posts are published , be sure to enter your email address in the form below ! <h> Downloads : 55217 @qwx675217 <p> I think it is quite easy to convert it to modern c++ implementation . One of the best things of learning c++ is , after you get familiar with it , you will find out you can pick up lot of languages in short time , especially a language with nice syntax and libs like python . <p> Thanks for the tutorial , this is a nice solution , especially step 4 , I believe I would use machine learning ( trained by mnist or other datasets ) to recognize the digits rather than this creative , simple solution . <p> Sorry , I think I did not express my though clearly , what I mean was I do not know there are such creative solution before I study this post , so I would prefer machine learning as character recognition , although ml may be a more robust @ @ @ @ @ @ @ @ @ @ than this solution . <p> Using machine learning to solve this problem is 100% acceptable ; however , there are times when a clever use of image processing can achieve just as high accuracy with less effort . Of course , this is a bit of a contrived example and for more robustness machine learning should absolutely be considered , especially if there are changes in reflection , lighting conditions , etc . <p> Interesting . really liked the post . Thanks for sharing . In case the scene illumination changes , the algorithm usually breaks or becomes less accurate . Please suggest all the different computer vision techniques in practice in order to remove or minimize the effects of **32;4291;TOOLONG changes of the image for the algorithm to still work correctly OR at least with high accuracy ? <p> Adrian , A most excellent post and your timing is impeccable ! I happen to have a need for just such 7-segment digit recognizer . Leaving the data logging sensor aside ( wheres the fun in that ) obviously this is just one way of using computer vision to recognize @ @ @ @ @ @ @ @ @ @ happen to have some thoughts on how one would do this WITH machine learning ? I am guessing that KNN might be a good approach . Thoughts ? <p> Hey Douglas Im glad the post was timed well ! As for doing this with machine learning , yes , its absolutely possible . I demonstrate how to recognize ( handwritten ) digits inside Practical Python and OpenCV and then discuss more advanced solutions inside the PyImageSearch Gurus course , but a good starting point would be the HOG descriptor + SVM . <p> Thanks Adrian ! I have the books and associated video and have gone through them quite a lot ( seems **26;4325;TOOLONG flights leave LOTs of reading time ! ) <p> I have not had the chance to try HOG and SVM . Since I am under the gun , so to speak , I will try and get a comparison of the two once converted to C# . I mentioned KNN because it is a lazy learning method and might be a touch faster . I am having to do all this in real time based on @ @ @ @ @ @ @ @ @ @ when a single frame might contain several indicators with varying numbers of digits . <p> k-NN is faster to train ( since there is no training process ) but is slower at test time , depending on how large your dataset is . You can apply approximate nearest neighbors to speed it up , but in the long run I think you 'll find better accuracy with HOG + SVM . Only quantify the digit region of the image via HOG and then pass the feature vector into the SVM . <p> Thanks . I will give that a try . As I have discovered fr0om this blogs code , translating to C# EmguCV/OpenCV is not straightforward at all . You have numpy and imutils plus some home grown routines which I do not have in C# . One thing I thought odd is doing the exact same steps up to performing the canny edge detection gave me an image that looked different from yours . I guess I find that odd because at the bottom of the code it is all OpenCV . You would think converting to gray , @ @ @ @ @ @ @ @ @ @ image . <p> I shall look into the HOG + SVM in your book and will continue to see if I can translate this blogs code into C# . <p> Hi Manisha make sure you use the " Downloads " section of this tutorial to download the source code + example image . It sounds like you might have copied and pasted code and accidentally introduced an error where not all digits are detected . <p> we can used your project to getting student i 'd by print row containt 10 " seven-segment " on the top of the paper . then the student shadded his i 'd before he answering the questions ( shadded the bubbles ) . <p> If you 're doing OMR and bubble sheet recognition , why not follow the approach detailed here ? Or is your goal to validate that what the user bubbles in matches what they wrote ? If its the latter , one of the best methods to determine these digits would be to train a simple digit detector using machine learning . I demonstrate how to train your own digit detectors inside Practical Python @ @ @ @ @ @ @ @ @ @ I have a note which is beside the point of the image-recognition , but may be useful : you have the " 1 " as represented by two vertical segments on the left , but it may be two vertical segments on the right ( take a look at the alarm clock picture on this very page ) . I imagine it would be simple to add a second entry to your lookup table to account for this . Cheers . <p> Or rather : your image of the ten digits has it on the left , but the lookup table seems to have it on the right ( 2 , 5 ) . Either way , a second entry would help to make this work across different displays . <p> Awesome post . After the canny edge detection and countour analysis , we assume that the largest rectangle with four vertices is the LCD . But in fact it is the whole outline of the thermostat ( I.e. the output after canny edge detection as shown ) and not the LCD . I found this part confusing . Can @ @ @ @ @ @ @ @ @ @ 4 vertices for me is the thermostat outline not the LCD . <p> After contour approximation the thermostat box does not have 4 vertices . Looking at the edge map you can also see that the thermostat box does not form a rectangle there are disconnects along its path . Therefore , its not a rectangle and our algorithm does not consider it . <p> Thanks for your quick reply . I already run upgrade imutils but it still shows the same error . Previously I followed steps from LONG ... to setup my environment <p> If you are using a Python virtual environment for your code , make sure you upgrade imutils there as well . The imutils library published to PyPI is indeed the latest version , so you likely have an old version either in ( 1 ) a Python virtual environment or ( 2 ) your global system Python install and are accidentally using the older version . <p> the code works fine while tested on the image that comes along with it , when I try using other image , error showed , using an @ @ @ @ @ @ @ @ @ @ error is NoneType object has no attribute reshape and when using an image with same type and dimensions , this is showed " key=lambda b : b1i , reverse=reverse ) ) ValueError : need more than 0 values to unpack <p> This blog post was built around a specific example ( the thermostat image at the top of the post ) . Its unlikely to work out-of-the-box with other images . You will likely need to debug the script and ensure that you can find the LCD screen followed by the digits on the screen . <p> Hi , Adrian I have problems in using open cv and python . Because it is the identity on the lcd if it is identified on a numeric table is how . When running your program all the things I did not break is due to code I just break it when using open cv with python . What are you using python and opencv ? love you and you give code because i not dowload guide of you ? <p> I used OpenCV 3 and Python 2.7 for this blog post . @ @ @ @ @ @ @ @ @ @ with OpenCV 3 and Python 3 . If you are getting an error with the code , please share it . Without the error myself and other readers can not help you . <p> Are you referring to the LED segment display boards ? Or the actual signs ? For the LED segment boards , this approach would likely work . If you want to use this approach actual signs I would train a custom object detector to first detect the sign , then extract the digits , followed by classifying them . <p> This would be useful for reading the display of a window unit that has IR remote control but lacks a two-way control protocol for querying status . <p> You keep saying a simple data-logging thermometer would be simpler , but that does n't  help if what you want is not just to know the temperature of the room , but to know what a machine with only a @ @ @ @ @ 
@@71485143 @185143/ <h> Histogram of Oriented Gradients and Object Detection <p> If you 've been paying attention to my Twitter account lately , youve probably noticed one or two teasers of what I 've been working on a Python framework/package to rapidly construct object detectors using Histogram of Oriented Gradients and Linear Support Vector Machines . <p> Honestly , I really ca n't stand using the Haar cascade classifiers provided by OpenCV ( i.e. the Viola-Jones detectors ) and hence why I 'm working on my own suite of classifiers. - While cascade methods are extremely fast , they leave much to be desired . If you 've ever used OpenCV to detect faces you 'll know exactly what I 'm talking about . <p> Figure 1 : Example of falsely detecting a face in an image . This is a common problem when using cv2.detectMultiScale . <p> In order to detect **29;4353;TOOLONG in OpenCV ( and remove the false positives ) , you 'll spend a lot of time tuning the- cv2.detectMultiScale- - parameters . And again , there is no guarantee that the- exact same parameters will work from image-to-image . This makes batch-processing large datasets for @ @ @ @ @ @ @ @ @ @ with either ( 1 ) falsely detecting faces or ( 2 ) missing faces entirely , simply due to poor parameter choices on a per image basis . <p> There is also the problem that the- Viola-Jones detectors- are nearing- 15 years old . If this detector were a nice bottle of Cabernet Sauvignon I might be pretty stoked right now . But the- field has advanced substantially since then. - Back in 2001 the Viola-Jones- detectors were state-of-the-art and they were certainly a huge motivating force behind the incredible new advances we have in object detection today . <p> Now , the Viola-Jones detector is n't our only choice for object detection. - We have object detection using keypoints , local invariant descriptors , and bag-of-visual-words models . We have Histogram of Oriented Gradients . We have deformable parts models . Exemplar models . And we are now utilizing Deep Learning with pyramids to recognize objects at different scales ! <p> All that said , even though the Histogram of Oriented Gradients descriptor for object recognition is nearly a decade old , it is still heavily used today and with @ @ @ @ @ @ @ @ @ @ by Dalal and Triggs in their seminal 2005 paper , - Histogram of Oriented Gradients for Human Detection demonstrated that the Histogram of Oriented Gradients ( HOG ) image descriptor and a Linear Support Vector Machine ( SVM ) could be used to train highly accurate object classifiers or in their particular study , human detectors . <h> Histogram of Oriented Gradients and Object Detection <p> I 'm not going to review the entire detailed process of training an object detector using Histogram of Oriented Gradients ( yet ) , simply because each step can be fairly detailed . But I wanted to take a minute and detail the general algorithm for training an object detector using Histogram of Oriented Gradients . It goes a little something like this : <h> Step 1 : <p> Sample- P positive samples from your training data of the object(s) you want to detect and extract HOG descriptors from these samples . <h> Step 2 : <p> Sample- N negative samples from a- negative training- set that does not contain any of the objects you want to detect and extract HOG descriptors from these samples @ @ @ @ @ @ @ @ @ @ 3 : <h> Step 4 : <p> Figure 2 : Example of the sliding a window approach , where we slide a window from left-to-right and top-to-bottom. - Note : Only a single scale is shown . In practice this window would be applied to multiple scales of the image . <p> Apply hard-negative mining . For each image- and each possible scale of each image in your negative training set , apply the sliding window technique and slide your window across the image . At each window compute your HOG descriptors and apply your classifier . If your classifier ( incorrectly ) classifies a given window as an object ( and it will , there will absolutely be false-positives ) , record the feature vector associated with the false-positive patch along with the probability of the classification. - This approach is called- hard-negative mining . <h> Step 5 : <p> Take the false-positive samples found during the hard-negative mining stage , sort them by their confidence ( i.e. probability ) and re-train your classifier using these hard-negative samples . ( Note : You can iteratively apply steps 4-5 , @ @ @ @ @ @ @ @ @ @ not always tends to be enough . The gains in accuracy on subsequent runs of hard-negative mining tend to be minimal . ) <h> Step 6 : <p> Your classifier is now trained and can be applied to your test dataset . Again , just like in Step 4 , for each image in your test set , and for each scale of the image , apply the sliding window technique . At each window extract HOG descriptors and apply your classifier . If your classifier detects an object with sufficiently large probability , record the bounding box of the window . After you have finished scanning the image , apply non-maximum suppression to remove redundant and overlapping bounding boxes . <p> These are the bare minimum steps required , but by using- this 6-step process you can train and build object detection classifiers- of your own ! Extensions to this approach include a deformable parts model and Exemplar SVMs , where you train a classifier for- each- positive instance rather than a- collection of them . <p> However , if you 've ever worked with object detection in images youve @ @ @ @ @ @ @ @ @ @ around the object you want to detect in the image . <p> Notice on the left we have 6 overlapping bounding boxes that have correctly detected Audrey Hepburns face . However , these 6- bounding boxes all refer to the same face we need a method to suppress the 5 smallest bounding boxes in the region , keeping only the largest one , as seen on the- right . <p> This is a common problem , no matter if you are using the Viola-Jones based method or following the Dalal-Triggs paper . <p> There are multiple ways to remedy this problem . Triggs et al . suggests to use the Mean-Shift algorithm to detect multiple modes in the bounding box space by utilizing the- ( x , y ) coordinates of the bounding box as well as the logarithm of the current scale of the image . <p> I 've personally tried this method and was n't satisfied with the results . Instead , you 're much better off relying on a- strong classifier with- higher accuracy ( meaning there are very few false positives ) and then applying non-maximum suppression to the @ @ @ @ @ @ @ @ @ @ a good non-maximum suppression ( sometimes called non-maxima suppression ) implementation in Python . When I could n't find one , I chatted with my friend Dr. Tomasz Malisiewicz , who has spent his entire career working with object detector algorithms and the HOG descriptor . There is literally no one- that I know who has more experience in this area than Tomasz . And if you 've ever read any of his papers , you 'll know why . His work is fantastic . <p> Anyway , after chatting with him , he pointed me to two MATLAB implementations . The first is based on the work by- Felzenszwalb et al. - and their deformable parts model . <p> The second method is implemented by Tomasz himself for his Exemplar SVM project which he used for his dissertation and his ICCV 2011 paper , - Ensemble of Exemplar-SVMs for Object Detection and Beyond . Its important to note that Tomaszs method- is over 100x faster than the- Felzenszwalb et al . method . And when you 're executing your non-maximum suppression function millions of times , that 100x speedup really matters . <p> @ @ @ @ @ @ @ @ @ @ et al . methods , porting them from MATLAB to Python . Next week well start with the- Felzenszwalb method , then the following- week I 'll cover Tomaszs method . While Tomaszs method is substantially faster , I think its important to see- both implementations so we can understand exactly- why- his method obtains such drastic speedups . <p> Be sure to stick around and check out these posts ! These are absolutely critical steps- to building object detectors of your own ! <h> Summary <p> In this blog post we had a little bit of a history lesson regarding object detectors . We also had a sneak peek into a Python framework that I am working on for object detection in images . <p> From there we had a quick review of how the- Histogram of Oriented Gradients method- is used in conjunction with a Linear SVM to train a robust object detector . <p> However , no matter what method of object detection you use , you will likely end up with multiple bounding boxes surrounding the object you want to detect . In order to remove these @ @ @ @ @ @ @ @ @ @ Over the next two weeks I 'll show you two implementations of Non-Maximum Suppression that you can use in your own object detection projects . <p> Be sure to enter your email address in the form below to receive an announcement when these posts go live ! Non-Maximum Suppression is absolutely- critical to obtaining an accurate and robust object detection system- using HOG , so you definitely do n't  want to miss these posts ! <p> I am thinking about creating a unified framework , which can include all these frameworks , but have no idea about the implementation now . Could you please make some comments : Is it worth to try ? how will you do if you are doing it ? <p> Hi Shi ! You are absolutely right , object detectors can be painfully slow , especially when you using the sliding window technique . And when expensive features such as HOG need to be computed , it can really kill performance . <p> And you are correct , I am utilizing the N image scales model for this framework . However , I have plans to @ @ @ @ @ @ @ @ @ @ al . <p> Right now performance using the N image scales model is actually pretty good for my framework , but not great . The reason is because I have distributed the image pyramid to all available cores of the system this is an obvious solution where making the HOG sliding window computation run in parallel can dramatically speedup the code . <p> However , doing something like FPDW will further increase the speed ( but lessen accuracy slightly ) . <p> Send me an email and I would be happy to chat more about implementation details . Thanks again for the comment , there is a ton of great information in here . <p> Nice post , thanks . In phase #5 , the false positives are taken along with their probabilities and then sorted by their probabilities in order to further retrain the classifier . I have two questions about which I would appreciate to get a clarification : 1 . why do you need their probabilities in that retraining phase ? linear SVM does n't  require their probabilities but rather merely their taggings ( negatives in this @ @ @ @ @ @ @ @ @ @ to be sorted ? linear SVM does n't  require any sorting of the training samples . <p> You would want to keep track of the probabilities for two reasons . The first being that you may not have enough physical memory to store the positive samples , negative samples , and hard-negative samples and train your SVM . In the case you are limited by RAM , you would want to sort the samples by their probability and keep only the samples the classifier performed " worst " on , discarding the rest . Secondly , you may be worried about overfitting . <p> According to the paper published by Dalal and Triggs , they suggest gamma correction is not necessary . I have the doubt about whether correcting gamma is a good option to go for or not . If Gamma correction is necessary what is the gamma value I have to take for better performance . Awaiting for reply. ! ! ! thanks in advance ! ! <p> That is correct , gamma correction is not necessary . Normalization however is quite often helpful . You can normalize @ @ @ @ @ @ @ @ @ @ image channel before applying the HOG descriptor ( normally the square-root is used ) . Another method to make HOG more robust is to compute the gradient magnitude over all channels of the RGB or L*a*b* image and take the maximum gradient response across all channels before forming the histogram . <p> You can certainly convert to grayscale and compute HOG as well . Its just been shown that taking the maximum response over the individual RGB or L*a*b* channels can increase accuracy in some cases . Your mileage may vary depending on your dataset . <p> I have a suggestion for speeding up the search , and would like your opinion . <p> What if you applied the KPM , or Boyer-Moore to the search ? Meaning , convert the histogram coefficients to a string representation , and do the same with the image being searched . Then , look at the suffixes first . <p> I 've used the Boyer-Moore , because its about 10x faster for string searching . In this application , it would be like looking at the right edge of a rect first , and @ @ @ @ @ @ @ @ @ @ <p> Hi Adrian , awesome tutorial btw . I 'm quite a beginner in openCV . I want to create a computer vision algorithm that is able to detect license plates and read them . However , I do n't  really know where to begin . What tutorial do you suggest that I can start with . So far , I was able to create the document scanner in your tutorial in c++ . <p> Hi Jeck , great question . Ill be covering license plate detection and recognition inside the PyImageSearch Gurus course . License plate detection and recognition is not an easy task , but inside the course I 'll be breaking it down into simple and easy to digest components , similar to other tutorials on the PyImageSearch blog . <p> Hi Miriam . Unfortunately , there could be many , many reasons why faces are not detected and your question is a bit ambiguous . Here are just some example questions for you to consider : Are you using HOG + Linear SVM ? Have you chosen the optimal HOG parameters for your descriptor ? What about your @ @ @ @ @ @ @ @ @ @ your training examples sufficiently represent the faces that you want to detect in images ? Are you performing hard negative mining to increase accuracy ? Are you using image pyramids so you can detect faces at multi-scales ? Or using a sliding window so detect faces in different positions of the image ? Be sure to consider all avenues . <p> Hey Adrian , i really want to thank you for outstanding lessons you taught me about computer vision . I was wandering , can we expect something about image stiching.Something like creating panorama images ? = All best to you sir ! = <p> Hi Milos , thank you for such a kind comment ! And we will absolutely be doing image stitching and panoramas within the next few months . I cant set an actual date yet since I have a series of posts coming out soon , but I 'll be sure to let you know when the image stitching post goes live . <p> if my dataset is on colored images and color is a demarcating feature in my classification then should I use grayscale image or @ @ @ @ @ @ @ @ @ @ it help in the result <p> Dalal and Triggs actually recommend converting from the RGB color space to the L*a*b* color space and computing the gradient magnitude for each of the L* , a* , and b* channels , respectively , and then taking the maximum magnitude across all channels . In many cases this can improve performance . <p> Thanks for your great work . I have a question about training positive and negative samples . They can be of different size . Do you try to resize them to a predefined size ? I concern resizing may change the original gradients orientation depending on how it is resized . Looking forwards to your answer . <p> Thanks . I am aware of that . Just want to make sure whether it will give distortion or not . If it does bring distortion to the gradient orientations . Maybe compensation is needed ? Or maybe the distortion is too little to consider . <p> It will absolutely bring in some distortions since we are ignoring the aspect ratio of the image during the resizing , but that 's a standard @ @ @ @ @ @ @ @ @ @ of the same dimensionality . <p> Just wanted to say thank you . I am a student studying computer vision for the first time as part of a taught masters and this blog really helped me so much you have a knack for explaining things intuitively = <p> First I am just thank you for your wonderful and super easy to understand tutorials and perhaps the best available . <p> I have one question on Object Recognition using sliding window and SVM . I am using C++ and the SVM on OpenCV . On detection I get multiple windows where I need to apply Non-Maximum Suppress ( which I learnt well from your tutorial ) . However , the linear SVM output is a hard decision of +1 for objects and -1 for non-objects . In this case its not possible to do NMS as all weights ( considering the prediction response ) are same . <p> I read a paper from Platt , 1999 to convert the prediction response to probability . However , I am wondering if you know there is any simpler or better way to achieve @ @ @ @ @ @ @ @ @ @ comment . Does your SVM library not support returning probabilities ? Most SVM libraries do . I do n't  do much work in C++ ( literally none ) , but I know the scikit-learn SVMs support returning probabilities along with class labels . <p> Also , you can still do NMS without probabilities . Instead of using the probabilities , just use the bottom right-hand corner , like I do in this post on fast non-maximum suppression . Its obviously better if you have the probabilities , but this approach will still work for you . <p> Hi Adrian , thank you for your prompt reply . I checked the LibSVM and it does return probabilities while SVM in OpenCV C++ does not . Hence I changed to using LibSVM . <p> Regarding the NMS , as your tutorial is implemented using bottom right-hand corner , I am little confused how to use Bounding Box probabilities instead . I guess maybe like this but if I am wrong than could please guide me on the right path . <p> &gt; Instead of computing the overlap should I directly use the @ @ @ @ @ @ @ @ @ @ you very much I managed to get it working with LibSVM . However , I noticed that getting the probabilities from SVM is computationally more expensive compared to the label prediction ( hard decision ) . I wonder if this happens or did I just got something wrong ? Mathematically , I think estimating probability needs more operations . However , my question here is that if I do n't  use probability but just use label predication i.e ( +1/-1 ) than use your NMS to get the final object region , do you think that is also good enough or Probabilities are important or does it improves results ? ( Apologies my poor English ) . <p> Secondly , about the HOG descriptor . Say for example I trained a classifier to detect standing bottle ( say sliding window of 64 x 128 ) = 3780 dimension feature vector . During detection , I will use this window size to detect the bottle . However , my question is when the bottle is resting on its side ( now say the dimesion is 128 x 64 ) but our @ @ @ @ @ @ @ @ @ @ do I cater for this issue ? I read paper on Part based model but I think PBM is computationally expensive . Perhaps I thought to divide the training image into 4 parts ( say 16 x 32 ) and train this . Do you have any suggestion on this ? <p> Hey Rish , you 're absolutely correct using probabilities does increase the computational cost . However , if you have the probabilities you 'll likely get better more precise final bounding boxes . It really depends on your application and the level of tolerance you 're willing to accept . <p> As for your second question , HOG based methods , including the PBM model assumes there is a certain " structure " to the image . For example , if you trained your classifier to recognize a bike , it would look for two wheels , handlebars , a seat , etc . But if this bike were rotated 90 degrees , you would run into problems . HOG descriptors are structural descriptors and hence in your case you are going to run into a lot of problems if the @ @ @ @ @ @ @ @ @ @ finding all bottles in the image , including those resting on their sides , just take each input image and rotate it 0 , 90 , 180 , and 270 degrees you 'll be evaluating each image four times which is definitely comes at a considerable computational cost , but you 'll find the bottles even if they are lying on their side . <p> Hi Adrian . Sorry I was busy with my Finals so was unable to complete my work . Actually , I have got most of the HOG detection implemented in C++ . As I mentioned above , that I want to change your NMS from Bottom Right Corner to probability sorting as you mentioned . <p> I think I am bit lost , I removed all the bounding box params from your NMS and use the Probability instead I am confused like in line 51 where you done the overlapping computation . How can I change this to probability instead ? Do I need to do something is Gradient Decent ? Sorry I am just confused , please help me out . Thanks <p> No need for @ @ @ @ @ @ @ @ @ @ call to you NMS function you 'll need to pass in three variables : the bounding boxes , the overlap threshold , and a probability of detection that corresponds to the bounding box . Then you need to sort ( from highest probability to lowest probability ) the bounding boxes based on their probability . From there , everything will be the same . Like I said , the only code that you 'll need to change is the code that sorts on probability rather than the bottom-right coordinate . <p> Hi Andrian , I have few doubts regarding NMS algorithm using probabilities 1 . What does probability means ? It is the detection score by SVM ( i.e . Wx+b value ) , is n't it ? 2 . You said that we have to sort in decreasing order of probabilities . But I think we have to sort in increasing order , since we are picking from last . <p> I am new to sklearn . I see many of you have advanced experience in this area . I am therefore asking for your assistance so that I can catch up @ @ @ @ @ @ @ @ @ @ , SUVs and Trucks that pass outside my house . I want to train a SVM classifier to detect whether a vehicle is a one of these three . I have cropped all of the images and successfully created Histogram Oriented Gradient images of these vehicles . My question is Do I have to write code to reshape the image and format it to get the data into a format useable by scikit-learn or is there code already written to do this . I am grateful for your help . <p> Visualizing a Histogram of Oriented Gradients image versus actually extracting a Histogram of Oriented Gradients feature vector are two completely different things . While you can visualize your HOG image , this is not appropriate for training a classifier it simply allows you to visually inspect the gradient orientation/magnitude for each cell . Instead , you 'll need to extract the HOG descriptor using something like scikit-image and then pass the feature vector into scikit-learn . As I mentioned in an email to you , I 'll be covering all this inside the PyImageSearch Gurus course . <p> Hi Adrian , @ @ @ @ @ @ @ @ @ @ useable for integrating into Scikit learn . I understand that it needs to me reshaped into a matrix of row vectors so that we have number of samples vs number of features . Are you telling me that this is done in scikit-image ? Thanks for your help . <p> Yes , take a look at the documentation/example . If you take a look at the Handwriting Recognition chapter of Case Studies , you 'll learn how to extract the HOG feature vector . Secondly , the HOG image does not need to be reshaped the HOG image is essentially useless for anything but visualization . You 'll need the HOG feature vector ( which scikit-image provides you with ) to create your classifier . <p> Hi Adrian , it occurred to me that OpenCVs hog may be a better choice . I looked into scikit-images source code of hog and found it does not implement weighted vote for histogram orientation binning . It only does simple binning based on magnitude value falling into which bin , which is not quite an accurate way for computing hog . <p> Actually , I @ @ @ @ @ @ @ @ @ @ implementation is more accurate than with the one from scikit-image , and runs faster . <p> Thanks for sharing your experience Jay ! Ive been using a personal modification of the scikit-image HOG that I 've been meaning to create a pull request for . It handles various types of input transformations ( square-root , log , variance ) along with multi-channel images , allowing you to take the maximum of the gradient across the channels ( this tends to work better in practice ) . Ill have to look into weighted vote binning as well . <p> The answer is , it depends ! Some problems are quite simple and require very few training examples . Other problems are very , very complex and require many training examples . A good rule to start with at least 100 positive training examples of what you want to detect , followed by 500-1000 negative training examples . And then perform hard-negative mining and select another 500-1000 false positive examples . This approach will likely not give you the best performance , but it will give you a baseline to further tune your @ @ @ @ @ @ @ @ @ @ do n't  expect your first attempt to give you the best performance . <p> Hi Adrian , I am a student of BS computer science i have started working on human detection in image using HOG descriptor . I have a problem in testing phase step 4 and 5 . Are you telling me what i do i use a matlab as a tool . <p> Thank you for your work , May I sak you to direct me to some implementation of hard-negative mining . Actually I did n't  get the point of how to reuse the false negative data ! . Should I use multi instance Svm ; I mean like clustering the false positive . <p> In case I get false positive by my trained classifier on negative train data , should I delete them from my train data ? or keep them with negative samples ? I am a little bit confused on what the procedure should be taken during retrain the classifier , May you help on that ! <p> So basically , hard negative mining is all about finding the patches of images ( @ @ @ @ @ @ @ @ @ @ SVM gets totally and completely wrong . For example , let 's pretend we are training an object detector to detect faces in images . The first thing well do is extract HOG features from a positive dataset ( that contains lots of examples of faces ) and a negative dataset ( which contains absolutely no faces at all , just a bunch of random images ) . We then train our Linear SVM . <p> However , this classifier may falsely detect faces in images where there are no faces . So , we run our classifier on the negative data ( which contain no faces what-so-ever ) , and we collect any HOG feature vectors that the classifier incorrectly reports as a face . This process is called " hard-negative mining " . In your case , if you get a false-positive from your trained classifier you want to keep that data since you can use it to better improve your classifier . <p> Finally , we need to re-train our classifier , which is just a 1-vs-1 SVM : either " face " or " not a face @ @ @ @ @ @ @ @ @ @ dataset , the HOG feature vectors from the non-face dataset , as well as the hard-negative HOG feature vectors . <p> We then let this classifier train , with the end goal ( normally ) being that our classifier better detects faces , while ignoring non-faces . <p> Definitely not . CNNs , and deep learning in general , is a tool just like anything else . There is a time in a place for it to be used . In some situations its warranted . In some situations its overkill . And in other its just the flat out wrong choice . Take a look at my post on the deep learning " band wagon " mentality that rises up every 5-7 years . <p> 2 : Since HOG is a feature descriptor ( am I right ? ) , is it possible to use another descriptor to describe the object and feed to svm ? Like akaze , kaze brisk , freak ( In truth , I do not know their different ) and so on <p> Simply put , a Linear SVM is very fast . A @ @ @ @ @ @ @ @ @ @ ; however , it requires computing N number of trees . It also does n't  work well for sparse feature spaces which Linear SVMs excel at . SVMs can also return probability estimations as well . <p> 2 . HOG is indeed an image descriptor . You could certainly use any other image descriptor and feed it into your classifier , such as Local Binary Patterns , Zernike moments , Haralick texture . And yes , you can AKAZE , BRISK , SURF , SIFT , etc. ; however , this requires the added step of creating a bag-of-visual-words prior to passing the feature vector on to the classifier . I am covering all of these topics in detail inside the PyImageSearch Gurus course . And if you have n't played around with machine learning , Practical Python and OpenCV is a great space to start . <p> A semi-rigid object is something that has a clear form and structure , but can change slightly consider how a human walks . We put one foot in front of the other , maybe move our arms a bit . The form is @ @ @ @ @ @ @ @ @ @ bit . HOG can be used to detect semi-rigid objects like humans provided that our poses are not too deviant from our training data . Finally , smoke is definitely not a semi-rigid object . Smoke diffuses into the air and has no real shape or form . <p> I have n't go through the details of these papers yet , the second paper claim that the HOG can separate rigid object and non-rigid object very well.They use HOF to estimate the motion of the smoke , but it would be a problem if the video is not stable <p> Some **26;4384;TOOLONG Smoke Detection : Possibilities , Techniques , and Challenges ) even do not use machine learning at all <p> I do n't  have any experience with smoke detection , so take this with a grand of salt , but I would apply something like a spatial pyramid , some really basic color channel information in the L*a*b* and then I would create an experiment where I try a bunch of image descriptors such as Local Binary Patterns ( I would be really curious about this for this problem ) @ @ @ @ @ @ @ @ @ @ to see how it would perform . My guess is not great , but that 's why we perform experiments . <p> Awesome job , your posts are the best posts about OpenCV that I found ! ! Congratulations ! ! I have one question : Did you ever worked with SVM for ONECLASS ? I tried to find something about it but got nothing . <p> Hey Douglas , thanks for the comment . Personally , I prefer to use the scikit-learn machine learning implementations . I find the ones implemented in OpenCV to be a bit cumbersome and not as customizable . <p> Do you know the size I should choose for the hog extraction ? I know its often 64*128 for human detection , with blocksize =8 . But I do n't  know for face problem .. I 'm trying to calculate hog features on 25*35 images , with the function hog.compute() but its not working Any help ? = <p> As a research problem , I want to apply HOG for Content based image retrieval and definitely it is computationally expensive for large datasets . Please guide what @ @ @ @ @ @ @ @ @ @ been tested the effects of changing block , cell , bin , orientation angel etc. , it give a bit performance gain but how does is this sufficient for defense ? <p> Hey Ahmad can you let me know what the computational bottleneck is for your particular project ? Is it the feature extraction itself ? Or is it the actual search that is slow ? If its the search , its probably because you are doing a linear scan of the dataset ( i.e. comparing the query features to every other feature vector inside the dataset . If at all possible , I would suggest using an approximate nearest neighbor data structure ( such as Annoy ) to speed up the actual search . <p> Actually I 'm pursuing my PhD . For my research contribution the above query was asked . In my last effort I used HOG with PCA and classify using SVM at Corel dataset . Then I tuned HOG with its parameters like bin , angel , block size etc. but it is not a contribution . Im stuck at how can I add my contribution @ @ @ @ @ @ @ @ @ @ HOG , use HOG+LBP is worthable ? &gt; What to apply to achieve more efficiency , recall rates with HOG ? New your valuable input on it ! <p> Nice , congrats on working on your PhD , that 's very exciting . As far as rotation , HOG is definitely not rotation invariant . In you use only uniform LBPs you can achieve some level of rotation invariance . I do n't  recall any papers off the top of my head that have combined both HOG and LBP together , although it seems like there should be papers on the topic it might be worth doing some more research in the literature and perhaps trying an experiment to see what your results look like . <p> The next evolution is already here and ( no pun intended ) , its called Convolutional Neural Networks . CNNs are very accurate for image classification and object localization . However , the largest drawback is the incredible amount of data it takes to learn even a single class . All supervised machine learning algorithms require training data , but CNNs are particularly data @ @ @ @ @ @ @ @ @ @ data augmentation is not a possibility , HOG-based methods still work very well . <p> While I totally agree with you that the classifiers provided by OpenCv are pretty frustrating , I would not immediately follow in the conclusion that this has to do with the Viola Jones detector as such . I.m.o. it simply says that those cascades were ( very ) poorly trained . When you see these cascades come back with mere patches of a face , rather than a properly framed face entirely , its clear that the training images had poor alignment to start with . They probably carved out some faces by hand and then told the training program that these were faces Yeah , well I think its fair to say that the cascades from OpenCv merely serve as demonstration material , and that they were never optimized to be production-proof . A properly trained Viola-Jones detector , however , can yield amazing results . <p> Hi Adrian , Will it produce good ( accurate ) results , If we use HOG features and SVM for vehicle licence plate detection ? I only @ @ @ @ @ @ @ @ @ @ present , recognition is not necessary . <p> Thank you for all your great tutorials ! I 'm trying to implement Exemplar SVMs with some friends for a school project but we got stuck . We followed your tutorials but our classifier does n't  detect anything . It seems that each single classifier does n't  train properly : as the number of negative sample grows , we get only 2 support vectors ( could it be a problem ? ) . We tried linearSVC(loss=hinge) , SVC(kernel=linear) and **27;4412;TOOLONG but nothing changes . Maybe were doing something wrong with features extraction ? Any hint ? <p> For help with Exemplar SVMs , I suggest reaching out to Tomasz Malisiewicz , who authored the original paper on Exemplar SVMs . Its hard to say what your exact problem is , but when you read through the original paper you 'll see that they use a " calibration " step where they tune the support vectors for each classifier . I have never implemented Exemplar SVMs before , but it seems like this could also cause a problem with your classification . Its also hard @ @ @ @ @ @ @ @ @ @ your actual feature extraction process . <p> I use the same scikit-image descriptor as well . As for increasing speed , try splitting processing of the image pyramid to different cores of the processor , that way each core can process a separate layer of the pyramid independently . <p> I do n't  directly cover it in the course , although its an easy add-on once you 've seen the source code for implementing an object detector from scratch . Of course , I also share more resources to make multi-processing easier inside the course = <p> Thank you so much for this perfect step-by-step instruction . I 'm working on recognizing traffic signs using SVM + HOG . The algorithm is great and the accuracy is very reasonable ; however , the speed is deriving me crazy . For each image , of hundreds , it takes more than 5 minutes to yield the results . I really need to shorten this time to at most a minute or less . The sliding window is the main problem . How can I handle the slow moving of sliding window ? Thank @ @ @ @ @ @ @ @ @ @ the actual problem its the time associated with extracting the HOG features and then passing them through the SVM . Your pyramid scale may also be hurting performance . I would suggest increasing the size of the image pyramid along with the step size of the sliding window as much as possible without hurting accuracy . The larger the pyramid scale and the sliding window step size are , the less window need to be evaluated . <p> What about training a Haar ( well , LBP ) cascade first , lowering false-negatives rate and not bothering much about false-positives . Then , instead of sliding the window just go through every object detected by cascade . <p> I did n't  tried that myself , but maybe you did ? Or is there is something wrong with that ? <p> There is n't anything " wrong " with that approach , but it does imply that you need to train another classifier and tune any-and-all parameters associated with them . It would be better to invest your time in creating a more accurate HOG classifier or pushing the computation to the @ @ @ @ @ @ @ @ @ @ " region proposals " which is the process of iteratively finding " interesting " regions of the image to classify rather than applying an exhaustive sliding window + image pyramid approach . <p> Hi Adrian great post ! I 'm trying to build my own SVM and detect object them on test image using hog.detectMultiScale . I tried to build a SVM classifier from sklearn and opencv using crossvaligdation . It gives a good accuracy during the cross validation and also while using other test images ( however the opencv one just gives me one support vector which is kinda of weird ) . <p> If i use sklearn SVM , I load the coef attribute of my SVC object ( which corresponds to the primal sv ) using setSVMDetector but the detection gives a lot of false positive and not even a true positive on the images that were well detected with the sklearn predict Have you trained your own svm model and used it with detectMultiScale ? Any ideas of what I could do wrong . Thanks a lot ! <p> Hi Andrian , I am working on an @ @ @ @ @ @ @ @ @ @ part of that I have collected some data manually and used some data available online . But I am not obtaining good accuracy . However my data set size is of size train ( fall 400 neg 1031 ) , test ( fall 246 neg 503 ) . After initial training I did hard mining also . I have used HOG parameters like cell size 8+8 , Block size 16+16 Window size 128+64 Bins 9 . How to improve accuracy ? . Accuracy depends on the available data set size , does n't  it ? Should I play with cell size and other parameters ? Thanks in advance . <p> In general , the more data you have the better . In terms of negative samples , I would try to increase this dramatically . Ideally you should 10x more negative samples than positive ones , especially for HOG . Other than that , you will need to play with HOG parameters , but I think this is less of a concern until you can get more data . <p> Hi Adrian , In your post you said number of @ @ @ @ @ @ @ @ @ @ . I did a bit of research on internet and usually people tend to use balanced data for a binary classification . Could you explain why you are using unbalanced data in this case ? <p> Thanks for the answer . I asked because I 'm wondering if the unbalanced or balanced data could actually be the cause of some of the errors I 'm encountering . <p> First I 'm doing cross validation to confirm i have good training tests . Even with unbalanced data , i get a low FNR and FPR during the training . But when using the detectMultiScale on a video I get much more FP . Does that mean my model is overfitting during training or my datasets are not good enough ? <p> Secondly , I 'm trying to reduce this number of FP by using hard data mining . The optimal number of hard negative samples N and the optimal C of the new retrained model are found via random research . But usually the lowest FNR and FPR i get is using 0 hard negative . Which surprise me . Does that mean my model @ @ @ @ @ @ @ @ @ @ . Should i based my choice of optimal C and N by testing another set rather than getting a FNR and FPR on my training set during cross validation ? Any other suggestions ? Thanks for your time ! <p> Its hard to say without seeing your datasets . I would start by asking yourself if your training data is representative of your testing data . If your training data does n't  look anything like your testing data then you can expect to get strange results . <p> As you suggested , its normal to set your number of samples and SVM value of C based off a random search or grid search this is a pretty standard procedure . Given that you are getting the smallest FPR rate when zero hard-negatives is used leads me to believe that your training data might not be representative of your testing data . <p> For the positive training set , i 'm using images from a scene view by a fixed camera . The test images are also taken from this camera . But the negative training set , I 'm using the one @ @ @ @ @ @ @ @ @ @ scene without a positive instance or should it reflect the scene ( I 'm working indoor so should my negative set be only a view from the camera without my object ) ? <p> It really depends on where you depend on deploying your classifier in a production environment . Its very common to use negative instances that simply do not contain objects that you do not want to detect . That said , if your production environment will have a lot of varying background clutter then I would highly suggest using negative views from your camera . <p> If review image pyramids for object detection in this blog post . You should actually stay away from blurring your images in the image pyramid when applying the HOG descriptor was that will decrease your accuracy . <p> Hey man , Great site . I have a question for you . If you train your svm with a certain kind of object size in mint ( say 60X180 ) , then how does the detectMultiScale method figure out what kind of crops it needs to take from the image input so that @ @ @ @ @ @ @ @ @ @ that 's the same size as the ones used in the training process ? Thanks , Gabriel <p> If you want to train your own custom object detector using HOG + Linear SVM then I would n't recommend using OpenCV . I would instead use methods I detail inside the PyImageSearch Gurus course . The Python + OpenCV bindings for HOG are not very friendly . You also do n't  have much control over the training process . <p> I just found out toiday the hard way that setSVMDetector actually requires a vector instead of a svm object . At this point it seems easier to just build my own hog detector with sliding windows and pyramid images and use that with a sklearn svm . Thanks a lot for the reply and ill say it again that i really appreciate the info on this site <p> No problem Gabriel , I 'm happy to help . I know I already mentioned this so I do n't  want to beat it to death , but I do demonstrate how to train your own HOG detector using sliding windows , image pyramids , and @ @ @ @ @ @ @ @ @ @ you get stuck implementing on your own ( or would like to learn from well documented tutorials ) then I would definitely suggest taking a look at the course . <p> Hi Adrian , Great site for OpenCV and Image Processing . I have some questions : 1 . As you said in Step 2 : Sample N negative samples from a negative training set that " does not contain " any of the objects you want to detect and extract HOG descriptors from these samples as well . In practice N &gt;&gt; P. So example if I want to train a smile detector , the positive images contain many smiling faces and the negative are not smile faces . Its mean that both positive and negative are include face , so is it contradict with : " does not contain " ? <p> 2 . In case of smile detector , I want to make a classifier with 3-classes : normal opening mouth ( not smile ) smile . I used Viola-Jones algorithm and its not good . Now , i 'm using Logistic Regression ( opencv ) to make @ @ @ @ @ @ @ @ @ @ . Its better than Viola-Jones , but it still get many false-positives . So in your experience , is that HOG+Linear SVM better ? Or could you suggest any approach ? <p> 3 . With HOG + Linear SVM Model or any Model you suggestion , can I save this model to re-use it in mobile , example save model as xml file and load it in Objective-C ( iOS ) or Java ( Android ) ? So I 'm just predict later , not training again . Finally , may I run realtime in mobile ( 30fps ) with your suggestion model on mobile device ? <p> 1 . Semantically this is not a contradiction , but I think you 'll run into issues if you use your frowning lips as negative samples . HOG is good at discriminating between objects , but lips are still lips and look similar . I would experiment with both , but my guess is that you 'll find better performance having the detectors trained on datasets that do not contain examples from the other set . Overall , I would suggest you take a look @ @ @ @ @ @ @ @ @ @ face ) . The geometry of these points might help you build a more accurate classifier . <p> 2 . HOG + Linear SVM tends to majorly beat out the Viola-Jones detector in terms of both accuracy and false-positives . If you 're getting many false-positives you should be applying more aggressive hard-negative mining . <p> 3 . I 'm not sure about this since I do n't  do much mobile development . You might have to train the model on a library specific to iOS/Android , but once its trained , you can deploy it into the production devices . <p> Hi Adrian , I am doing a project on traffic sign detection . Ideally how many hog features should be extracted per image for accurate results if the dataset contains about 2000 positive images and 5000 negatives ? Also which svm kernel is preferable ? <p> The number of extracted HOG features is entirely dependent on your dataset . A good rule of thumb is to have approximately 5-10x as many negatives as your positives and then perform hard-negative mining . As far as a kernel goes , a linear @ @ @ @ @ @ @ @ @ @ is . Given your current dataset I would suggest performing an initial experiment to obtain a baseline . From there you can try adjusting the number of negatives and hard-negatives . <p> Regarding the detection performance . How do you measure it as compared to measuring performance of classifiers using metrics like precision and recall ? <p> It seems to me you need to first evaluate the classifier itself with a set of test ( labeled ) samples to measure the discriminative ability of the classifier and then evaluate detection performance in some other way . <p> My impression is that the detector feeds a very large number of " non-objects " to your SVM , hopefully giving you lots of true negatives and a few ( or none ) false positives plues a few true positives related to the actual object ( which can then be processed with non-maxima suppression ) . I think the typical ML metrics are difficult to apply here , even for a single frame , given the in-balance between positive and negative test vectors . <p> So , I guess my question is , @ @ @ @ @ @ @ @ @ @ which one is the best ? <p> I 'm learning about human detection using HOG descriptor + Linear SVM . I am using Verilog HDL to build my system on FPGA board . My huge issue is : how to train the input dataset by using HOG descriptor ? The detailed steps ? Because verilog HDL does not have library for any the mathematical function ? So , I am hopefully received your detailed steps ? <p> Hi Cam I cant speak for the FGPA side of things ( since I do n't  have experience in that area ) , but I provided very detailed steps + source code to train your own custom object detector using the HOG + Linear SVM method inside the PyImageSearch Gurus course . <p> Hi Adrian , I am a huge fan of your blog and really appreciate what you do . I am creating my own object classifier for sign language detection . I am facing some problem with localizing the hand of the person in an image . The hand can be in a square box of size in the range 70 to @ @ @ @ @ @ @ @ @ @ both the image pyramid technique and hard-negative mining . I mean for which square size do I train my binary classifier and how do I go about the process . <p> The actual size of the hand does n't  matter as long as the aspect ratio ( ratio of width to height ) is the same . That is the entire point of applying a sliding window + image pyramid to detect objects at various scales and locations of an image . I detail a thorough solution to training a custom HOG + Linear SVM object detector inside the PyImageSearch Gurus course . I would suggest starting there . <p> Hi Adrian , How can we retrain an existing svm without starting from scratch the retraining process ? While performing hard negative training I got a Memory Error in the end , apparently the number of final hog features exceeded the numpy array size ! ( My dataset contains 10K positives and 60K negatives , but I performed hard neg mining on 16K negatives . The number of hog features extracted from every image is 1568 ) . Thanks in @ @ @ @ @ @ @ @ @ @ can be trained in batches and sequentially updated SVMs are not one of them . In this case , you 'll need to reduce the size of your training dataset . I would suggest reducing the size of the true negatives and keeping only the most confident hard-negatives . <p> 1 . Use your positive set and crop negatives from regions of the image that do not contain the object you 're trying to detect . 2 . Use an existing dataset . There are quite literally 100s of them and I would start there . <p> Hi Adrian ! I really like your posts which are easy to understand and very handy to practice . I am currently working on detecting fish in images . I have used the method mentioned in this post . But I do not get a good performance . I want to know is N &gt;&gt; P meaning the number of neg samples should be far greater than the number of pos samples ? And could you tell me are there any theoy to help me adjust the size of sliding windows , step size and @ @ @ @ @ @ @ @ @ @ am doing wrong . Sorry for my poor English . Looking forward to your reply ! Thank you in advance ! <p> Correct , your negative samples should be far greater than your positive samples . I go into more detail on how to properly set your sliding window size , step size , and scale inside the PyImageSearch Gurus course . <p> The HOG + Linear SVM framework detailed in this blog post is for object detection , but yes , theoretically if you have a dataset of blurred and non-blurred images you could extract HOG features and train a SVM to label regions as blurred or non-blurred . I also have a blog post on blur detection here . <p> Thanks for your reply , i have tried blur detection approach . As my dataset is smooth ( face images ) , not getting good accuracy using that Laplacian variance approach . Looking for good approach in this case . Any insight on this would be a great help . <p> If the images have a smooth texture than the variance of the Laplacian by definition will be @ @ @ @ @ @ @ @ @ @ to consider looking at the distribution of values in the Fourier space . <p> Thank you very much for the nice tutorial about the HoG . I am using HoG for the grass weed detection . From the literature , I found that HoF is not rotationally invariant . I have 2 classes of Weeds and each class has 18 images . In my training and testing database , I have rotated each image at 5 10 15 20 355 degree . <p> The training and testing are done using LibSVM package . and I am getting an accuracy of about 80% . <p> My question is if the HoG is not rotation invariant then how can I get such high accuracy ? <p> You are correct , HOG is not rotation invariant . As for your training and testing set , it sounds like you rotated each of the images in 5 degree increments to add more data to your training set ? It might be the case that your grass weed objects are approximately symmetrical in which case the rotation would n't matter . <p> I have performed all @ @ @ @ @ @ @ @ @ @ HOG method . I had 6000 positives samples and 9000 negative samples then I performed hard negative mining ( with sliding window and pyramids ) and got around 70000 false positives . Then I trained again my linear SVM with 70000+9000+6000 samples , but still i am getting many false positives and false negative on my testing sample . where I am going wrong ? My positives samples are faces of people with litlle bit of background and there neck and chest also . And also i do n't  have any bounding box ( ROI ) on my training image . is that wrong ? <p> You absolutely need the bounding box ROI of the training and validation images . Otherwise , you are computing the HOG descriptor for the entire image . The purpose of the HOG + Linear SVM detector is to detect and localize objects in an image . If you do n't  have the bounding box points for these regions , then the detector will not work . <p> Hello Adrian thank you so much for your great article ! In step 6 , about overlapping @ @ @ @ @ @ @ @ @ @ . suggests to use the Mean-Shift algorithm " . I want to know that What is the reference ? <p> In this example well click and drag a rectangular Region of Interest ( ROI ) and crop it from our image . This technique is especially helpful if you are labeling data as input to an image classification algorithm . <p> OpenCV ships with a pre-trained HOG + Linear SVM model that can be used to perform pedestrian detection in both images and video streams . If you 're not familiar with the Histogram of Oriented Gradients and Linear SVM method , I suggest you read this blog post where I @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485144 @185144/ <h> Archive OpenCV 3 <p> As you undoubtedly know , configuring and installing OpenCV on your macOS machine can be a bit of a pain . To help you and other PyImageSearch readers get OpenCV installed faster ( and with less headaches ) , I put together a tutorial on using Homebrew to install OpenCV . Using Homebrew allows you to- skip manually configuring your build and <p> Todays tutorial is inspired by a post I saw a few weeks back on /r/computervision asking how to recognize digits in an image containing a thermostat identical to the one at the top of this post . As Reddit users were quick to point out , utilizing computer vision to recognize digits on a thermostat tends to- overcomplicate <p> Have you ever worked with a video file via OpenCVs cv2.VideoCapture- function and found that reading frames- just felt slow and sluggish ? Ive been there - and I know exactly how it feels . Your entire video processing pipeline crawls along , unable to process more than one or two frames per second even- though you are n't  doing any <p> @ @ @ @ @ @ @ @ @ @ wasted three weeks of research time- during graduate school six- years ago . It was the end of my second semester of coursework . I had taken all of my exams early and all my projects for the semester had been- submitted . Since my school obligations were essentially nil , I started experimenting <p> Over the past few weeks I have demonstrated how to compile OpenCV 3 on macOS with Python ( 2.7 , 3.5 ) bindings from source . Compiling OpenCV via source gives you- complete and total control over which modules you want to build , - how- they are built , and- where they are installed . All this control can come at a price though . The downside <p> Last week I covered how to install OpenCV 3 with Python 2.7 bindings on macOS Sierra and above . In todays tutorial well learn how to install- OpenCV 3 with- Python 3.5- bindings on macOS . I decided to break these install tutorials into two separate guides to keep them well organized and easy to follow . To learn how to <p> I 'll admit it : @ @ @ @ @ @ @ @ @ @ lot more of a challenge than I thought it would be , even for someone who has a compiled OpenCV on hundreds of machines over his lifetime . If you 've tried to use one of my previous tutorials on installing OpenCV on your freshly updated <p> Over the past two years running the PyImageSearch blog , I 've authored two tutorials detailing the required steps to install OpenCV ( with Python bindings ) on Ubuntu . You can find the two tutorials here : Install OpenCV 3.0 and Python 2.7+ on Ubuntu 14.04 Install OpenCV 3.0 and Python 3.4+ on Ubuntu 14.04 However , with support of Ubuntu <p> Alight , so you have the NVIDIA CUDA Toolkit and cuDNN library installed on your GPU-enabled system . What next ? Let 's get OpenCV installed with CUDA support as well . While OpenCV- itself does n't  play a critical role in deep learning , it- is used by- other deep learning libraries such as Caffe , specifically in " utility " programs ( @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485145 @185145/ <h> OpenCV center of contour <p> Figure 1 : An example image containing a set of shapes that we are going to compute the center of the contour for . <p> In above image , you can see a variety of shapes cut out from pieces of construction paper . Notice how these shapes are not- perfect . The rectangles are n't  quite rectangular and the circles are not entirely circular either . These are human drawn and human cut out shapes , implying- there is variation in each shape type . <p> With this in mind , the goal of todays tutorial is to ( 1 ) - detect the outline of each shape in the image , followed by ( 2 ) - computing the center of the contour also called the- centroid- of the region . <p> In order to accomplish these goals , well need to perform a bit of image pre-processing , including : <p> Conversion to grayscale . <p> Blurring to reduce high frequency noise to make our contour detection process more accurate . <p> Binarization of the image . Typically edge detection @ @ @ @ @ @ @ @ @ @ post , well be applying thresholding . <p> Open up a new file , name it centerofshape.py- , and well get coding : 15 <p> 16 <p> 17 55203 @qwx675203 55218 @qwx675218 55219 @qwx675219 <p> importcv2 55202 @qwx675202 55206 @qwx675206 <p> ap.addargument ( " -i " , " --image " , required=True , <p> help= " path to the input image " ) 55208 @qwx675208 <p> # load the image , convert it to grayscale , blur it slightly , <p> # and threshold it <p> image=cv2.imread ( args " image " ) 55215 @qwx675215 <p> **29;4441;TOOLONG , ( 5,5 ) , 0 ) <p> LONG ... <p> We start off on- Lines 2-4 by importing our necessary packages , followed by parsing our command line arguments . We only need a single switch here , --image- , which is the path to where the image we want to process resides on disk . @ @ @ @ @ @ @ @ @ @ disk , and pre-process it by applying grayscale conversion , Gaussian smoothing using a 5 x 5 kernel , and finally thresholding ( Lines 14-17 ) . <p> The output of the thresholding operation can be seen below : <p> Figure 2 : Thresholding our image returns a binary image , where the shapes appear as white on a black foreground . <p> Notice how after applying thresholding the shapes are represented as a white foreground on a- black background . <p> The next step is to find the location of these white regions using contour detection : <p> OpenCV center of contour <p> Python <p> 19 <p> 20 <p> 21 <p> 22 <p> # find contours in the thresholded image <p> LONG ... 55211 @qwx675211 <p> **36;4472;TOOLONG <p> A call to cv2.findContours- on- Lines 20 and 21- returns the set of outlines ( i.e. , contours ) that correspond to each of the white blobs on the image . Line 22 then grabs the appropriate tuple value based on whether we are using OpenCV 2.4 or OpenCV 3 . You can read more about how the return signature of @ @ @ @ @ @ @ @ @ @ We are now ready to process each of the contours : <p> OpenCV <p> 38 <p> 39 <p> # loop over the contours <p> forcincnts : <p> # compute the center of the contour <p> M=cv2.moments(c) <p> cX=int ( M " m10 " /M " m00 " ) <p> cY=int ( M " m01 " /M " m00 " ) <p> # draw the contour and center of the shape on the image <p> cv2.drawContours ( image , c , -1 , ( 0,255,0 ) , 2 ) <p> cv2.circle ( image , ( cX , cY ) , 7 , ( 255,255,255 ) , -1 ) <p> cv2.putText ( image , " center " , ( cX-20 , cY-20 ) , <p> **26;4510;TOOLONG , ( 255,255,255 ) , 2 ) <p> # show the image <p> cv2.imshow ( " Image " , image ) 55212 @qwx675212 <p> On- Line 25 we start looping over each of @ @ @ @ @ @ @ @ @ @ the contour region on- Line 27 . <p> In computer vision and image processing , image moments are often used to characterize the shape of an object in an image . These moments capture basic statistical properties of the shape , including the- area of the object , the- centroid ( i.e. , the center- ( x , y ) -coordinates of the object ) , - orientation , along with other desirable properties . <p> Here we are only interested in the center of the contour , which we compute on- Lines 28 and 29 . <p> From there , - Lines 32-34 handle : <p> Drawing the outline of the contour surrounding the current shape by making a call to cv2.drawContours- . <p> Placing a white circle at the center ( cX , cY ) - -coordinates of the shape . <p> Writing the text center- near the white circle . <p> To execute our script , just open up a terminal and execute the following command : <p> OpenCV center of contour <p> Shell <p> 1 <p> $python centerofshape.py--image shapesandcolors.png <p> Your results should look something like @ @ @ @ @ @ @ @ @ @ the shapes individually and then computing the center ( x , y ) -coordinates for each shape . <p> Notice how each of the shapes are successfully detected , followed by the center of the contour being computed and drawn on the image . <h> Summary <p> In this lesson , we learned how to compute the center of a contour using OpenCV and Python . <p> This post is the first in a- three part series on- shape analysis . <p> In next weeks post , well learn how to- identify shapes- in an image . <p> Then , two weeks from now , well learn how to- analyze the color of each shape and- label the shape with a specific color ( i.e. , " red " , " green " , " blue " , etc . ) . <p> To be notified when these posts go live , be sure to enter your email address using the form below ! <h> Downloads : 55217 @qwx675217 <p> There seems to be good support for opencv for shapes and finding centroids but are there equivalent routines for line @ @ @ @ @ @ @ @ @ @ especially discriminating between lots of small noise lines and what I think should be dominant significant lines . <p> Line detection is much , much more challenging for a variety of reasons . The " standard " method to perform line detection is to use the Hough Lines transform . But for noisy images , you 'll often get mixed results . <p> It seems like both you and Ruttunenn are getting the same error message . It seems like the segmentation may not be perfect and there is some noise left over in the thresholding . A simple check would be to use : <p> 1 <p> 2 <p> **33;4538;TOOLONG : <p> # process the contour <p> Where you can set MINTHRESH to be a suitable value to filter out small contour regions . <p> You would typically define MINTHRESH at the top of your file , but you can place it anywhere that you think is good from a code organization perspective . The actual range of MINTHRESH will vary on your application and will have to be experimentally determined . <p> Just run to a minor glitch in @ @ @ @ @ @ @ @ @ @ = cv2.moments(c) on the first iteration , leading to float division by zero . A simple work around was to implement a check for 0.0 results . <p> Blurring ( also called " smoothing " ) is used to smooth high frequency noise in the image . Simply put , this allows us to ignore the details in the image and focus on what matters the shapes . So by blurring , we smooth over less interesting regions of the image , allowing the thresholding and contour extraction phase to be more accurate . <p> Great question . It would still be inside the shape , in the center , but towards the rim . An example can be found here . Keep in mind that only non-zero pixels are included in the calculation of the centroid . <p> Hi Adrian , I have a question about the value of cX and cY . As i want to know what is the pixel value at the point ( cX , cY ) , i tried to print it by imagecX , cY . However , I got error like : @ @ @ @ @ @ @ @ @ @ 0 with size 1024 which means that cX and cY is outside of range of the image size . Therefore , I want to ask how can i find out the pixel coordinate at point ( cX , cY ) ? Thanks ! <p> If I am doing this in a Jupiter notebook , and what to display the results using matplotlib , how would I do so for the very last step as you do with : <p> # show the image cv2.imshow ( " Image " , image ) cv2.waitKey(0) Ive tried placing : plt.imshow(image) inside of the for loop as I thought this would work . It will run the cell with no error but not display any image . <p> Sir can you provide me with what changes to make in shape detector program so that i can take object from webcam feed and classify it , it will be very helpful if you can provide with the code modification <p> You would want to change the cv2.findContours function call to either return a list of contours or a hierarchy , otherwise the script would find @ @ @ @ @ @ @ @ @ @ , take a look at the cv2.RETRLIST flag . <p> I tried to run this code on my system . But if I keep the code just the way it is , it shows me lots of centres ( probably because it is in the loop ) . But when I keep that lines 33-39 outside the loop , there comes only one circle named centre but that is not on the center point but is somewhere at the bottom left corner of the contour . Can you please help me out with this ? <p> Hi Ambika this code will draw the center of each shape on Line 33 . It does this by looping over all contours that have been detected . If you move Line 33 outside of the for loop then the coordinates will be incorrect . What exactly are @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485146 @185146/ <p> But I often do n't  get a response . The responses that I do get are short , sometimes unpleasant , and are very unhelpful to me . <p> To be honest , its made me question whether I want to study this field . <p> Adrian , can you help me ? Am I doing something wrong ? <p> It would be helpful if you could do a blog post on how to ask good computer vision questions . <p> Thanks , <p> Aarav <p> I can empathize with Aarav learning to ask a good question is an acquired skill , almost an " art " of sorts . <p> How do I know this ? <p> Besides collectively spending seven years in school earning my bachelors degree and PhD ( asking my own fair share of questions along the way ) , each day I spend a significant chunk of my time answering questions from PyImageSearch readers just like yourself who are interested in studying computer vision , but are hung up on a particular topic . <p> Over the past three years running PyImageSearch- Ive @ @ @ @ @ @ @ @ @ @ actually addressed the components of what constitutes a " good " computer vision question . <p> As Aarav posed , how do you- know you 're asking a " good " question ? <p> And how do you ask this question such that you : <p> Obtain the knowledge/clarity you are seeking . <p> While recognizing and respecting the time of others ? <p> Is there such a magic formula ? <p> Unfortunately , there is n't a one-size-fits-all solution . <p> But there are general- techniques you can ( and should ) be using when asking questions about computer vision ( or any technical subject in general ) . <p> In todays blog post well discuss the five key components that go into asking a good computer vision question . <p> Whether you are posting on StackOverflow , emailing me , or contacting any other subject matter expert , my hope is that you use these techniques to better help yourself and those who are responding to your questions . <p> Note : Aaravs name , university- and affiliation have been anonymized to protect- the identity of the reader who asked @ @ @ @ @ @ @ @ @ @ question , or affiliation is purely coincidental . <h> How to ask good computer vision questions <p> To start todays blog post I want to share a- personal story with you from my time in college studying computer vision . <p> Well then discuss what makes a " good " question , followed by reviewing the five key components required when asking- any question . <h> A personal story <p> Figure 1 : As an undergraduate student I had trouble grasping the concept of kernels and convolutions , one of the cornerstones of computer vision and image processing . <p> When I first started studying computer vision and image processing I had- a lot of questions . <p> In particular , I vividly remember struggling with the concept of- kernels and convolutions I simply could n't translate the mathematics in my textbook to an actual practical implementation . <p> After my professor first discussed kernels and convolutions in class , I spent the next two days spinning my wheels , fruitlessly re-reading the same chapter in my textbook and reviewing the same lecture notes , only to get nowhere . <p> @ @ @ @ @ @ @ @ @ @ during office hours , tail between my legs , feeling half-ashamed that I could n't grasp such a simple concept ( I even doubted my ability to study computer vision as a career ) . <p> I did n't  want to waste his time on questions I should have been able to understand and answer myself . <p> I could n't have been farther from the truth . <p> People like my computational photography professor ( and many other teachers I 've had ) - genuinely wanted to help me all they asked for in return was : <p> Respect for their time . <p> The understanding that while they could guide me and point me in the right direction , - I still had to do the hard work myself . <p> Its been a long time since I thought about that interaction with my computational photography professor , but I was reminded of it this past weekend when I received the email from Aarav outlined above . <p> Aaravs question helped me pause for a moment and reflect : <p> In a way Ive come full-circle with my professor . While @ @ @ @ @ @ @ @ @ @ done learning and will- always make it a point to study and try new things ) , - I have become a subject matter expert in computer vision . <p> Over the past three years running PyImageSearch , I have seen my share of good questions ones that were well thought out and had a specific purpose and end goal . <p> I 've also respond to questions that required a bit of digging and going back-and-forth with the reader to understand what the actual underlying problem was . <p> And Ive even answered some extremely trying questions , requiring both the patience of myself and the reader . <p> My personal opinion is that we can all get better at asking questions , no matter how good we are already . <p> The more questions we ask , the more we can learn but we need to do so in a manner that is respectful to those around is . <h> What is a good question ? <p> You might be familiar with the old phrase : <p> Figure 2 : " There is no such thing as a stupid @ @ @ @ @ @ @ @ @ @ true . <p> However , I think Carl Sagan , astronomer , cosmologist , and astrophysicist put it best when he said : <p> Figure 3 : " every question is a cry to understand the world . There is no such thing as a dumb question " Carl Sagan <p> In highly technical , advanced fields such as computer vision there are- bound to be questions its the very- nature and- existence of these questions that make computer science ( and all other sciences ) possible . <p> Figure 4 : Whether we are just getting started in our science careers or already performing state-of-the-art research , we can all apply the scientific method . <p> Step #1 : Ask a question and develop a hypothesis . <p> Step #2 : Run an experiment to test your hypothesis . <p> Step #3 : - Measure and record the results of your experiment . <p> Step #4 : Draw conclusions from your results . <p> Step #5 : Go back to Step #1 and repeat using your new found knowledge . <p> These- exact steps allow us to perform novel @ @ @ @ @ @ @ @ @ @ isnt- just for researchers , - its for anyone who is involved in computer science . <p> While researchers are advancing their respective fields , on the opposite side of the spectrum we have students we are just trying to get started by asking questions regarding the- very fundamentals of computer vision . These initial questions build the foundation on which theyll ask- even more advanced questions later in their careers . <p> Given the inevitability of questions , both at the very high level ( i.e. , state-of-the-art research ) to the very fundamentals ( i.e. , the building blocks of computer vision ) we need to focus on on how to write more insightful , intelligent questions with specific goals in mind that can be answered by others . <h> Five components of a practical , insightful question <p> Figure 5 : The five components of a practical insightful question . <p> I 'm going to stop focusing on what makes a " good " question and instead focus on what makes an " insightful " question . <p> Asking an insightful question starts- well before its typed into @ @ @ @ @ @ @ @ @ @ spent 15-20 minutes on Google querying for content similar to my question and reading what others have asked before me ? <p> You may be able to answer- all of these questions or you may only be able to answer- none of them . <p> The point of this exercise is n't to " check each item off " , but rather to take a step back and- think critically about your question . <h> Component #4 : Ask your question ( and be respectful of others time ) <p> By the time you actually- ask your question , you should be well prepared . <p> You should understand what your end goals are and what you hope to learn/accomplish by asking your question . <p> You should have done your homework on the question to seen- if others have asked similar questions ( as well as reviewed the responses ) . <p> Finally , and most importantly : <p> Ask your question while being respectful of others time . <p> Regardless of whether you are posting on StackOverflow , LinkedIn , or asking a subject matter expert ( like myself @ @ @ @ @ @ @ @ @ @ expect the person you are asking to writeup a multi-page email response to your question , analyzing every little detail and including references to an exhaustive list of papers you should read . <p> Do n't  expect them to write code for you . <p> And do n't  expect them to hold your hand . <p> Be realistic and set your expectations accordingly : <p> Ask your question with your- specific end goal and- context in mind . <p> And you will receive a response that helps point you in the right direction ( but wont do all the hard work for you ) . You 'll need to do the hard work yourself . <p> A question is only as good as the thought , time , and effort that goes into it . <p> If you take the time to ask questions with a well thought out purpose and end goal , you 'll find that the responses you receive in return are much better and helpful to accomplishing your particular project/task . <h> Component #5 : Repeat the process as necessary <p> More times than not , one question @ @ @ @ @ @ @ @ @ @ clarifies- a particular matter , only to find that it further muddies the water of another concept downstream we again need help and explanation . <p> In this case , its time to repeat the process . <p> Assess your new question . <p> Why are you asking it ? <p> What is your end goal ? <p> Do your research on the question . <p> Then ask ( while respecting others time ) . <p> Rinse and repeat . <h> Asking good questions takes practice <p> As you get better at asking insightful questions you 'll start to notice a pattern in how others ask questions as well . <p> You 'll begin to sense when a coworker or colleague has done adequate research to prepare their questionor if they are flying by the seat of their pants , hoping for a solution to suddenly materialize from a poorly thought out inquiry . <p> As a subject matter expert , I can tell you what a- huge pleasure it is to answer questions that have a specific purpose and end goal . <p> I- love- answering these questions because I can @ @ @ @ @ @ @ @ @ @ on how to solve the overall problem the reader is working on . <p> The reader walks alway happy , not only because their question is answered , but also because they have more resources they can leverage to help solve their problem . <p> On the contrary , I can also tell you how- trying and tedious it can be to answer questions where readers : <p> Are not being respectful of my time . <p> Havent taken the time themselves to consider- why they are asking the question ( i.e. , the end goal + context ) . <p> In next weeks blog post , well analyze- actual questions I have received on the PyImageSearch blog , reviewing them piece-by-piece , and see the five components of asking a good question in action . These examples will help you improve your own ability to ask questions because let 's face it , we all can use a little practice . <h> Summary <p> Relaying the- context- of the question ( the situation you are in prompting the question to be asked ) . <p> Doing your research . <p> @ @ @ @ @ @ @ @ @ @ <p> Repeating the process as necessary . <p> This formula- can also be extended to other technical domains outside computer science as well . <p> In the second tutorial in this two-part series , I 'll discuss- how to ask questions- specifically on the PyImageSearch blog . <p> Well look at actual emails I have received here on the PyImageSearch blog , and take them apart , discussing the positive ( and negative ) qualities of the question I 'll even suggest methods to better phrase your inquires to help ensure you find the information you 're looking for . <p> To be notified when this next " cant miss " blog post goes live , be sure to enter your email address in the form below to join the PyImageSearch Newsletter ! <p> I think I would add to#4 " show that you have done your research " . I think people are more willing to help if they can see that I 'm turning to the bulletin board/whatever as a last resort instead of a first resort . <p> Great point Haydon . In the next blog post in the series I 'll @ @ @ @ @ @ @ @ @ @ " show you 've done your research " part comes into play . I do n't  expect people to think of asking a question as a " last resort " , but I do think its critical to perform some upfront research and familiarize yourself with the problem better . <p> Do n't  be a burden to others . If a question has many dimensions and is complex , articulate only one dimension , i.e. , make the question as narrow and specific as your can . The response may lead you to solve the other dimensions on your own . <p> I 've used this webpage LONG ... as the quintessential guide to asking good questions . Over the years , its been updated to reflect the changes in the mediums ( stackoverflow vs BBS ) , but the theme is very similar to what you have written . <p> Interesting blog post ! I realized that asking questions usually do n't  solve my problems but the answers guide me to a solution . I miss the possibility of exchanging ideas and concept , while talking with colleagues . Other ( @ @ @ @ @ @ @ @ @ @ a different angle , which can help to get further . Thanks for helping , Adrian ! <p> hi Adrian , I am Nirmala , studying in M.TECH . As i am doing my final project in computer vision field using python your blog is being very helpfull to me. hu moment vector of 24 hu moments value which I calculated for each frame of video , now i have to find out the changes in successive frames hu moment so that i can detect change in shape.Please tell me how can i access those vectors frame by frame and find out the difference between successive frames hus moments ? ? <p> Given a 24-d vector your goal is to determine how much these feature vectors have changed between frames ? You would typically compute the Euclidean distance between the vectors and use this to tell you how much the vectors have changed . <p> Be-You-T-Full . Another good one sir . May these words ring in the ears and grow awesome computer vision students . If I may add . A lot of times these simple steps can help @ @ @ @ @ @ @ @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485147 @185147/ <h> Archive Book <p> Ever since I wrote the first PyImageSearch tutorial on installing OpenCV + Python on the Raspberry Pi B+ back in February 2015 it has been my dream to offer a- downloadable , pre-configured Raspbian . img file with OpenCV pre-installed . Today this dream has become a reality . I am pleased to announce that both the Quickstart Bundle and- Hardcopy <p> The 2nd edition of Practical Python and OpenCV + Case Studies is officially online ! This 2nd edition is a MAJOR- UPDATE- to the book and includes : Hardcopy editions . I 've wanted to offer Practical Python and OpenCV in print ever since I wrote the book a year ago , but I struggled to find to publisher " that 's all <p> I have some big news to announce today Besides writing a ton of blog posts about computer vision , image processing , and image search engines , I 've been behind the scenes , working on a second book . And you may be thinking , hey , did n't  you just finish up Practical Python and OpenCV ? Yep . I @ @ @ @ @ @ @ @ @ @ even halfway through 2014 yet , but there have been some really amazing Python books released this year that have not been getting much attention . Some of these books are related to computer vision , some to machine learning and statistical analysis , and others to parallel computing . While not all- of these books are directly related <p> Ive been throwing around the idea of writing a book for the past few months , but I was n't sure on what the exact focus was going to be . I 've given it a ton of thought , worked out the specifics , and spoke with a lot of developers , programmers , and researchers like yourselves @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485148 @185148/ <h> Archive Machine Learning <p> A few months ago I wrote- a tutorial on how to classify images using Convolutional Neural Networks ( specifically , VGG16 ) pre-trained on the ImageNet dataset with Python and the Keras deep learning library . The pre-trained networks inside of Keras are capable of recognizing- 1,000 different object categories , similar to objects we encounter in our day-to-day lives with high <p> Todays blog post is inspired from an email I received from Jason , a student at the University of Rochester . Jason is interested in building a custom object detector using the HOG + Linear SVM framework for his final year project . He understands the steps required to build the object detector well enough - but he is n't <p> In last weeks blog post , we discussed- gradient descent , a first-order optimization algorithm that can be used to learn a set of classifier coefficients for parameterized learning . However , the " vanilla " implementation of gradient descent can be prohibitively slow to run on large datasets in fact , it can even be considered- computationally wasteful . Instead @ @ @ @ @ @ @ @ @ @ building blocks . Love . Trust . Mutual respect . Yesterday , I asked my girlfriend of 7.5 years to marry me . She said yes . It was quite literally the happiest day of my life . I feel like the luckiest guy in the world , not only because I have her , but also because this incredible PyImageSearch <p> If you 've been following along with this series of blog posts , then you already know what a- huge fan I am of Keras . Keras is a super powerful , easy to use Python library for building neural networks and deep learning networks . In the remainder of this blog post , I 'll demonstrate how to build a simple neural <p> In previous tutorials , I 've discussed two important loss functions : - Multi-class SVM loss and- cross-entropy loss ( which we usually refer to in conjunction with Softmax classifiers ) . In order to to keep our discussions of these loss functions straightforward , I purposely left out an important component : - regularization . While our loss function allows us to determine how well ( @ @ @ @ @ @ @ @ @ @ Multi-class SVM loss ; specifically , the hinge loss and squared hinge loss functions . A loss function , in the context of Machine Learning and Deep Learning , allows us to quantify how " good " or " bad " a given classification function ( also called a " scoring function " ) is at correctly classifying data points in our dataset . However , <p> A couple weeks ago , we discussed the concepts of both- linear classification- and- parameterized learning . This type of learning allows us to take a set of input data and class labels , and actually learn a function that- maps the input to the output predictions , simply by defining a set of parameters and optimizing over them . Our linear classification tutorial focused <p> Over the past few weeks , we 've started to learn more and more about machine learning and the role it plays in- computer vision , - image classification , and- deep learning . Weve seen how Convolutional Neural Networks ( CNNs ) such as LetNet can be used to classify handwritten digits from the MNIST dataset . @ @ @ @ @ @ @ @ @ @ In last weeks post , I introduced the k-NN machine learning algorithm which we then applied to the task of image classification . Using the k-NN algorithm , we obtained- 57.58% classification accuracy on the Kaggle Dogs vs . Cats dataset challenge : The question is : - " Can we do better ? " Of course we can ! - Obtaining higher accuracy for @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485149 @185149/ <h> k-Nearest Neighbor classification <p> k-Nearest Neighbor classification <p> Figure 6 : Our k-NN classifier is able to correctly recognize the digit " 6 " . <p> We 've spent a decent amount of time discussing the image classification in this module . Weve learned about the challenges . The types of learning algorithms we can use . And even the general pipeline that is used to build- any image classifier . <p> But we have yet to- really build an image classifier of our own . <p> Today , that is all going to change . <p> Were going to start this lesson by reviewing the simplest image classification algorithm : k-Nearest Neighbor ( k-NN ) . This algorithm is- so simple that it does n't  do any actual " learning " yet it is still heavily used in many computer vision algorithms . And as you 'll see , - well be able to utilize this algorithm to- recognize handwritten digits from the popular MNIST dataset . <p> Be able to recognize handwritten digits from ( a sample of ) the MNIST dataset . <h> The k-Nearest Neighbor Classifier @ @ @ @ @ @ @ @ @ @ simple image classification algorithm . In fact , its- so simple that it does n't  actually " learn " anything ! - Instead , this algorithm simply relies on the distance between feature vectors , much like in building an image search engine only this time , we have the- labels associated with each image , so we can predict and return an actual- category- for the image . <p> Simply put , the k-NN algorithm classifies unknown data points by finding the- most common class among the- k closest- examples. - Each data point in the- k- closest data points casts a vote , and the category with the highest number of votes wins ! - Or in plain english : - " Tell me who your neighbors are , and I 'll tell you who you are " . <p> Figure 1 : Given a dataset of flowers , how might we classify the image outlined in red ? <p> In order for the k-NN algorithm to work , it makes the primary assumption- that feature vectors that lie- close together in an n-dimensional space have similar visual contents @ @ @ @ @ @ @ @ @ @ distinct " clusters " of data . <p> Here , we can see three categories of images , denoted as red , blue , and green- dots , respectively . We can see that each of these sets of data points are grouped relatively close together in our n-dimensional space . This implies that the distance between two- red dots is- much smaller than the distance between a- red dot and a- blue dot . <p> However , in order to apply the k-Nearest Neighbor classifier , we first need to select a distance metric or a similarity function . We briefly discussed the Euclidean distance ( often called the L2-distance ) in our lesson on- color channel statistics : <p> But many other distance metrics exist including the Manhattan/city block distance ( often called the L1-distance ) : <p> In reality , you can use whichever distance metric/similarity function most suits your data ( and gives you the best classification results ) . However , for the remainder of this lesson , well be using the most popular distance metric : the Euclidean distance . <h> k-NN in action @ @ @ @ @ @ @ @ @ @ the k-NN algorithm . We know that it relies on the distance between feature vectors to make a classification . And we know that it requires a distance metric/similarity function to compute these distances . <p> But how do we actually- make the classification ? <p> To answer this question , let 's look at the following figure : <p> Figure 3 : An example of plotting three species of flowers according to the size and lightness of their petals . <p> Here , we have a dataset of three types of flowers - sunflowers , daises , and pansies- and we have plotted them according to the- size- and- lightness of their petals . <p> Now , let 's insert a new , unknown flower and try to classify it using only a- single neighbor- ( i.e.- k=1 ) : <p> Figure 4 : In this example , we insert an unknown image ( highlighted as red ) into the dataset and then use the distance between the unknown flower and dataset of flowers to make the classification . <p> Here , we have found the " nearest neighbor " to @ @ @ @ @ @ @ @ @ @ to the label of the nearest flower , its a daisy . <p> Let 's try another " unknown- flower " , this time using- k=3 : <p> Figure 5 : Let 's classify another flower only this time let 's use k=3 neighbors rather than just k=1 . <p> This time , we have found two- sunflowers and one- daisy- in the top three results . Since the- sunflower category has the largest number of votes , well classify this unknown flower as a- sunflower . <p> We can keep performing this process for varying- values of- k , but- no matter how large or small- k becomes , the principle remains the same the category with the largest number of votes in the- k closest training- points wins and is used as the- label for the testing point . <h> Hyperparameter tuning <p> There are two clear parameters that we are concerned with when running the k-NN algorithm . The first is obvious : the value of- k . What is the optimal value of- k ? If its too small ( such as when k=1 ) , then we gain @ @ @ @ @ @ @ @ @ @ points . However , if- k is too large , then we are at risk for- over-smoothing our classification results and increasing bias . <p> The second parameter we should consider tuning is the actual distance metric . Is the Euclidean distance the best choice ? What about the Manhattan distance ? Or ? <p> Train our classifier on the training data using various values of- k ( and various distance functions , if we wish ) . <p> Evaluate the performance on the classifier on the validation set , keeping track of which parameters obtained the highest accuracy . <p> Take the parameters that obtained the highest accuracy and train our k-NN classifier using those parameters . <p> Evaluate our " best " classifier on the- test set and obtain our final results . <p> Again , by using this scheme , we are able to try various parameter values , find the set of parameters that gives the best performance , and then finally evaluate our classifier in an un-biased ( and fair ) manner . <p> Now that we understand the basics of the k-NN algorithm , @ @ @ @ @ @ @ @ @ @ dataset . <h> Recognizing handwritten digits using MNIST <p> Figure 6 : A sample of the MNIST data for handwritten digit recognition . <p> In the remainder of this lesson , well be using the k-Nearest Neighbor classifier to classify images from the MNIST dataset , which consists- of handwritten digits . The MNIST dataset is one of the most well studied datasets in the computer vision and machine learning literature . In many cases , its a benchmark and a standard to which machine learning algorithms are ranked . <p> The goal of this dataset is to correctly classify the handwritten digits 0-9 . Instead of utilizing the entire dataset ( which consists of 60,000 training images and 10,000 testing images , ) well be using a small subset of the data provided by the scikit-learn library this subset includes 1,797 digits , which well split into- training , - validation , and- testing sets , respectively . <p> Each image in the 1,797-digit dataset from scikit-learn is represented as a 64-dim raw pixel intensity feature vector . This means that each image is actually an- 8 x 8 @ @ @ @ @ @ @ @ @ @ into a list . <p> All digits are placed on a black background with the foreground being shades of white and gray . <p> Our goal here is to train a k-NN classifier on the- raw pixel intensities and then classify unknown digits . <p> To accomplish this goal , well be using our five-step pipeline to train image classifiers : <p> Step 2 Splitting the dataset : Well be using three splits for our experiment . The first set is our training set , used to train our k-NN classifier . Well also use a- validation set to find the best value for k . And well finally evaluate our classifier using the- testing set . <p> Step 3 Extracting features : - Instead of extracting features to represent and characterize each digit ( such as HOG , Zernike Moments , etc ) , well instead use- just the raw , grayscale pixel intensities of the image . <p> Step 4 Training our classification model : Our k-NN classifier will be trained on the raw pixel intensities of the images in the training set . Well then determine the @ @ @ @ @ @ @ @ @ @ Step 5 Evaluating our- classifier : - Once we have found the best value of- k , we can then evaluate our k-NN classifier on our testing set . <p> Let 's go ahead and get this example started . Open up a new file , name it mnistdemo.py- , and let 's 25 <p> 26 <p> 27 55203 @qwx675203 <p> **29;4573;TOOLONG <p> **27;4604;TOOLONG importtraintestsplit <p> fromsklearn.neighbors **26;4633;TOOLONG <p> fromsklearn.metrics **26;4661;TOOLONG <p> fromsklearn importdatasets <p> fromskimage importexposure 55220 @qwx675220 55219 @qwx675219 <p> importcv2 <p> # load the MNIST digits dataset <p> **27;4689;TOOLONG <p> # take the MNIST data and construct the training and testing split , using 75% of the <p> From there , we go ahead and load the- MNIST dataset sample on- Line 13 . Well also create our training testing split on- Lines @ @ @ @ @ @ @ @ @ @ training and the remaining 25% for testing . <p> However , we also need a validation set so we can tune the value of- k . We create our validation set on- Lines 21 and 22 by partitioning our training data 10% of the training data will be allocated to validation , while the remaining 90% will remain as training data . <p> Finally , - Lines 25-27 show the size of each of our data splits : <p> Size of the training , testing , and validation data splits <p> Shell <p> 1 <p> 2 <p> 3 <p> training data points:1212 <p> validation data points:135 <p> testing data points:450 <p> Here , we can see we are using X images for training , Y values for validation , and Z values for testing . <p> On Line 31 , we define the list of- k values that we want to try , which consist of the- odd numbers between the range 1 , 30 ( any guesses as to why we use- odd numbers ? ) . <p> We then loop over each of these values of- k and @ @ @ @ @ @ @ @ @ @ your training data and training labels to the fit- method of the model- . <p> After our model is trained , we need to evaluate it using our validation data ( Line 41 ) . The score- method of our model checks to see how many predictions our k-NN classifier got right ( the higher the score , the better , indicating that the classifier correctly labeled the digit a higher percentage of the time ) . Next we- take this score and update our list of accuracies- so we can determine the value of- k that achieved the highest accuracy on the validation set ( Lines 46-48 ) . <p> Running our Python script , you 'll see the following output from parameter tuning phase : <p> Using the validation data set to <p> 15 <p> 16 <p> k=1 , accuracy=99.26% <p> k=3 , accuracy=99.26% <p> k=5 , accuracy=99.26% <p> k=7 , accuracy=99.26% <p> k=9 @ @ @ @ @ @ @ @ @ @ <p> k=15 , accuracy=99.26% <p> k=17 , accuracy=98.52% <p> k=19 , accuracy=98.52% <p> k=21 , accuracy=97.78% <p> k=23 , accuracy=97.04% <p> k=25 , accuracy=97.78% <p> k=27 , accuracy=97.04% <p> k=29 , accuracy=97.04% <p> k=1achieved highest accuracy of99.26%on validation data <p> Notice how the values of- k=1 to- k=15 all- obtained the same accuracy . However , computing the distance to only a single neighbor is substantially more efficient , thus we will- use- k=1 to train and evaluate our classifier on the final <p> 58 <p> 59 <p> # re-train our classifier using the best k value and predict the labels of the <p> # test data <p> LONG ... <p> model.fit ( trainData , trainLabels ) <p> **35;4718;TOOLONG <p> # show a final classification report demonstrating the accuracy of the classifier <p> # for each of the digits <p> print ( " EVALUATION ON TESTING DATA " ) <p> LONG ... <p> The code here is fairly straightforward : we are simply taking the value of- k that @ @ @ @ @ @ @ @ @ @ value of- k , and then evaluating the performance using the classificationreport- function , the output of which you can see below : <p> Evaluating <p> 14 <p> 15 <p> EVALUATION ON TESTING DATA <p> precision recall f1-score support <p> 01.001.001.0043 <p> 10.951.000.9737 <p> 21.001.001.0038 <p> 30.980.980.9846 <p> 40.980.980.9855 <p> 50.981.000.9959 <p> 61.001.001.0045 <p> 71.000.980.9941 <p> 80.970.950.9638 <p> 90.960.940.9548 <p> avg/total0.980.980.98450 <p> Wow , 98% accuracy ! That 's quite high ! - Furthermore , we can see the digits- 0 , 2 , - 6 , and 7 are classified correctly 100% of the time . The digit- 1- obtains the lowest classification accuracy of 95% . <p> So given this high classification accuracy , does this mean that we have " solved " handwritten digit recognition ? Unfortunately , no it does not . While the MNIST dataset is well known and heavily used as a benchmark , it does n't  necessarily translate into real-world viability @ @ @ @ @ @ @ @ @ @ where each and every image has been heavily pre-processed including cropping , perfect thresholding , and centered . <p> In the real world , your dataset will never be as- " nice " and cleanly pre-processed as the MNIST dataset . And as well find out in our case study on real-world handwriting recognition , well need to extract feature vectors from each digit rather than relying on the raw pixel intensities of the image . With- that said , its still useful to see how computing the Euclidean distance between raw pixel intensities can lead to high accuracy classifiers provided that the dataset has been appropriately pre-processed . <p> Finally , let 's end this code example by examining some of the individual predictions from our k-NN classifier <p> 75 <p> 76 <p> # loop over a few random digits <p> LONG ... <p> # grab the image and classify it <p> image=testDatai <p> **32;4755;TOOLONG <p> # @ @ @ @ @ @ @ @ @ @ x 8 image compatible with OpenCV , <p> here 's another sample , this one being more skewed and distorted than the others : <p> Figure 9 : Even though the " 5 " is skewed and distorted , we can still recognize it . <p> The digit " 0 " is also not a problem for our classifier : <p> Figure 10 : Another example of our k-NN classifier in action . <p> One last example : <p> Figure 11 : The digit " 6 " is also easy for our k-NN classifier . <h> Pros and Cons of k-NN <p> Before we wrap up this lesson , we should first discuss some of the advantages and disadvantages of the k-NN classifier . <p> One main advantage of the k-NN algorithm is that its extremely simple to implement and understand . Furthermore , the classifier takes absolutely no time to train , since all we need to do is store our data points for the purpose of later computing distances to them and obtaining our final classification . <p> However , we pay for this simplicity at classification time . @ @ @ @ @ @ @ @ @ @ every single data point in our training data , which scales , making working with large datasets computationally prohibitive . We can combat this problem by using- Approximate Nearest Neighbor- ( ANN ) algorithms ( such as kd-trees , FLANN , and random projections , etc. ) ; however , this requires that we trade space/time complexity for the the " correctness " of our nearest neighbor algorithm , since we are performing an approximation . That said , in many cases it is well worth the effort and small loss in accuracy to use the k-NN algorithm. - This is in contrast to most machine learning algorithms , where we spend a large amount of time up front training our classifier so that we have very fast classifications at testing time . <p> Finally , the k-NN algorithm is more suited for low-dimensional feature spaces . Distances in high-dimensional feature spaces are often unintuitive , which we refer to as the- curse of dimensionality . <p> Its also important to note that the k-NN algorithm does n't  actually " learn " anything the algorithm is not able to make @ @ @ @ @ @ @ @ @ @ on distances in a- n-dimensional space to make the classification . <p> All that said , - I normally recommend running k-NN on your dataset as a " first attempt " to obtain a baseline for classification accuracy . From there , you can apply more advanced techniques and spot-check more powerful algorithms . <h> Summary <p> In this lesson , we learned about the most simple machine learning classifier the- k-Nearest Neighbor classifier , or simply- k-NN for short . The k-NN algorithm classifies unknown data points by comparing the unknown data point to- each data point in the training set . This comparison is done using a distance function or similarity metric . Then , from the- k most similar examples in the training set , we accumulate the number of " votes " for each label . The category with the highest number of votes " wins " and is chosen as the overall classification . <p> While simple and intuitive , and though it can even obtain very good accuracy in certain situations , the k-NN algorithm has a number of drawbacks . The first is @ @ @ @ @ @ @ @ @ @ the algorithm makes a mistake , it has no way to " correct " and " improve " itself for later classifications . Secondly , without specialized data structures , the k-NN algorithm scales linearly with the number of data points , making it a questionable choice for large datasets . <p> To conclude , we applied the k-NN algorithm to the MNIST dataset for handwriting recognition . Simply by computing the Euclidean distance between raw pixel intensities , we were able to obtain a very high accuracy of- 98%. - However , its important to note that the MNIST dataset is heavily pre-processed , and we will require more advanced methods for recognize handwriting in real-world images . 
@@71485150 @185150/ <h> Checking your OpenCV version using Python <p> It was unavoidable the OpenCV 3 release was bound to break backwards compatibility with some OpenCV 2.4 . X functions : cv2.findContours- and cv2.normalize- come to mind right off the top of my head . <p> So how do you ensure that your code will work no matter which version of OpenCV your production environment is using ? <p> Well , the short answer is that you 'll need to create if- statements around each of the offending functions ( or abstract the functions away to a separate method that handles calling the appropriate function based on your OpenCV version ) . <p> In order to do this , you 'll need to be able to check your OpenCV version from within your using Python and that 's exactly what the rest of this blog will show you how to do ! <h> Checking your OpenCV version using Python <p> The OpenCV version is contained within a special cv2. version- variable , which you can access like this : <p> Checking your OpenCV version using Python <p> Shell <p> 1 <p> 2 <p> 3 @ @ @ @ @ @ @ @ @ @ <p> ' 3.0.0 ' <p> The cv2. version- variable is simply a string which you can split into the major and minor versions : <p> Checking your OpenCV version using Python <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> LONG ... <p> &gt;&gt;&gt;major <p> ' 3 ' <p> &gt;&gt;&gt;minor <p> ' 0 ' <p> Of course , having to perform this operation every- time you need- to check your OpenCV version is a bit of pain . To resolve this problem , I have added three new functions to my imutils package , a- series of convenience functions to make basic image processing functions with OpenCV and Python easier . <p> The code here is fairly straightforward Im simply checking if the cv2. version- string starts with a 3- , to indicate that we are using OpenCV 3 , or if it starts with a 2- , indicating we are using OpenCV 2 . X. <p> Again , these functions have already been included in the imutils package , which you can install using pip : <p> Checking your OpenCV version using Python <p> Shell @ @ @ @ @ @ @ @ @ @ installed , you can upgrade to the latest version using : <p> Checking your OpenCV version using Python <p> Shell <p> 1 <p> $pip install--upgrade imutils <h> Checking your OpenCV version : a real-world example <p> Now that we know how to check our OpenCV version using Python as well as- defined a couple convenience functions to make the version check easier , let 's see how we can use these functions in a real-world example . <p> Our goal here is to detect contours in the following image : <p> Figure 1 : We are going to utilize OpenCV 2.4 . X and OpenCV 3 to detect the contours ( i.e. outlines ) of the Tetris blocks . <p> In order to detect contours in an image , well need to use the cv2.findContours- function . However , as we know , the return signature of cv2.findContours- has changed slightly between version 3 and 2.4 of OpenCV ( the OpenCV 3 version of cv2.findContours- returns an extra value in the tuple ) thus well need to perform a check to our OpenCV version prior to making a call to cv2.findContours- @ @ @ @ @ @ @ @ @ @ take a look at how we can make this check : <p> Checking 24 <p> 25 <p> 26 55203 @qwx675203 <p> **29;4789;TOOLONG 55219 @qwx675219 <p> importcv2 <p> # load the Tetris block image , convert it to grayscale , and threshold <p> # the image <p> print ( " OpenCV Version : " . format ( cv2. version ) ) <p> image=cv2.imread ( " tetrisblocks.png " ) 55215 @qwx675215 <p> LONG ... <p> # check to see if we are using OpenCV 2 . X <p> ifimutils.iscv2() : <p> LONG ... 55211 @qwx675211 <p> # check to see if we are using OpenCV 3 <p> elifimutils.iscv3() : <p> LONG ... 55211 @qwx675211 <p> # draw the contours on the image <p> cv2.drawContours ( image , cnts , -1 , ( 240,0,159 ) , 3 ) <p> @ @ @ @ @ @ @ @ @ @ <p> As you can see , all we need to do is make a call to iscv2- and iscv3- and then wrap our- version specific code inside the if- statement blocks that 's it ! <p> Now when I go to execute my script using OpenCV 2.4 , it works without a problem : <p> Figure 2 : Our call to cv2.findContours is working in OpenCV 2.4 . X. <p> And the same is true for OpenCV 3 : <p> Figure 3 : And the same is true for OpenCV 3 since we are using the iscv2 and iscv3 functions to detect OpenCV versions with Python . <h> Summary <p> In this blog post we learned how to check our OpenCV version using Python . The OpenCV version is included in a special string variable named cv2. version- . All we need to do is check this variable and well be able to determine our OpenCV version . <p> Finally , I have defined a few convenience methods inside the imutils- package- to make checking your OpenCV version easier and more Pythonic . Consider checking the library out if you find @ @ @ @ @ @ @ @ @ @ : 55217 @qwx675217 <p> Hi Adrian . Get tutorials . I 'm attempting a project using a RasPi which sorts small building blocks by size and colour . Firstly , its looking for cube blocks of a certain size anything too small/large/long etc it ignores and secondly , if the correct size , it determines the block colour . Its looking for red , blue and yellow . Could I use the code type above to detect block size ? I 'm thinking threshold ( background will be white ) and get object area in pixels . Compare this to area of ideal block ? I tried previously using object detection with a template image of an ideal block but the Pi seemed quite slow at this and the camera focus is n't great at close distances . I 'd love to hear your thoughts ! Thanks <p> Hey Adrian , I am using opencv 3.1.0 but the repo on github to which I want to contribute to uses import cv statement to use it so which OpenCV version should I install to use it . I know it is a silly question but @ @ @ @ @ @ @ @ @ @ <p> If you want access to the old " cv " bindings you should either ( 1 ) use OpenCV 2.4 where you an access them via cv2.cv. * or ( 2 ) you can compile OpenCV 3 with the previous bindings enabled . I 'm not sure what the configuration switch to CMake is to do that . I would suggest running ccmake .. ( note the two " cs " in the command and then enabling any option related to " opencv world " . <p> 2.4 and OpenCV 3 . You can read more about why this change to cv2.findContours- is necessary in this blog post . Well also initialize the center- ( x , y ) -coordinates of the ball to None- on- Line <p> example , with the release of OpenCV 3 , common functions such as cv2.findContours- have different return signatures than OpenCV 2.4 . The cv2.normalize- function signature has also changed . SIFT and SURF are no longer included in <p> care when examining- Lines 43-45 . As we know , the cv2.findContours- method return signature changed between OpenCV 2.4 and 3 . This @ @ @ @ @ @ @ @ @ @ and 3 without having to <p> A call to cv2.findContours- on- Lines 20 and 21- returns the set of outlines ( i.e. , contours ) that correspond to each of the white blobs on the image . Line 22 then grabs the appropriate tuple value based on whether we are using OpenCV 2.4 or OpenCV 3 . You can read more about how the return signature of cv2.findContours- changed between @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485152 @185152/ <h> Running a pre-trained network <p> Running a pre-trained network <p> In the previous set of lessons in this module , we learned how to train various Convolutional Neural Network ( CNN ) architectures , including- LeNet , - KarpathyNet , and- MiniVGGNet , on the CIFAR-10 dataset . <p> Each of these networks ( including even- ShallowNet ) were able to obtain over 50% classification accuracy , with MinIVGGNet obtaining the highest classification accuracy of 75.03% . <p> While we have examined how to train networks using batches of images , one aspect we- have not looked at is how to take a- pre-trained model and use it to- classify new images images that- are not part of the original dataset . <p> In the remainder of this lesson , well learn how to load a pre-trained network from disk and utilize it to classify and label images . <p> # randomly select a few testing examples from the CIFAR-10 dataset and then <p> # scale the data points into the range 0 , 1 <p> print ( " INFO sampling CIFAR-10 ... " ) <p> ( testData @ @ @ @ @ @ @ @ @ @ /255.0 <p> np.random.seed(42) <p> LONG ... <p> LONG ... <p> **31;4847;TOOLONG <p> Lines- 23 and 24 define the set of ground-truth labels for the CIFAR-10 dataset . <p> Loading both our architecture and weights from disk is accomplished on- Lines 28 and 29 using the modelfromjson- and loadweights- functions , respectively . <p> Then , - Lines 35-39 handle selecting a few testing examples at random from the CIFAR-10 dataset . We guarantee that we are using- testing data by using the same pseudo-random number generator of 42- the same value we used in training our previous networks . <p> Were also sure to normalize the pixel values in the range 0 , 1 , just as we did during training and evaluation . <p> Let 's make predictions on our sample of the testing data <p> 57 <p> 58 <p> # make predictions on the sample of testing data <p> print ( " @ @ @ @ @ @ @ @ @ @ ... <p> **32;4880;TOOLONG <p> # loop over each of the testing data points <p> for ( i , prediction ) inenumerate(predictions) : <p> # convert the image from shape ( 3 , 32 , 32 ) to ( 32 , 32 , 3 ) and then resize <p> # it to a larger size <p> ( R , G , B ) =testDatai <p> image=np.dstack ( B , G , R ) <p> LONG ... <p> # show the image along with the predicted label <p> print ( " INFO predicted : , actual : " . **25;4914;TOOLONG , <p> gtLabelstestLabelsi ) ) <p> cv2.imshow ( " Image " , image ) 55212 @qwx675212 <p> Lines 43 and 44- pass the testing images through our pre-trained network , collecting the top predictions ( based on the associated class label probabilities ) . <p> From there , we loop over each of the testing points ( Line 47 ) and convert the CIFAR-10 image from a- ( 3 , 32 , 32 ) NumPy array into a- ( 32 , 32 , 3 ) array that is compatible with OpenCV . @ @ @ @ @ @ @ @ @ @ image in BGR order rather than RGB . <p> Finally , - Lines 55-58 display the output prediction to our screen . <p> This works great for images- already part of the CIFAR-10 dataset but what about images that- are not part of CIFAR-10 ? How do we classify these images ? <p> The following code block should answer these questions <p> 85 <p> 86 <p> # close all open windows in preparation for the images not part of the CIFAR-10 <p> On- Line 66 , we start looping over the images in the --test-images- directory . For each of these images , we load it from disk , resize it to a fixed 32 x 32 pixels , - split the image into its respective R , G , and B channels , and @ @ @ @ @ @ @ @ @ @ Lines 71-73 ) . <p> Line 78- then constructs the kerasImage- , creating a separate NumPy array with the shape- ( 3 , 32 , 32 ) , which is what the Keras library expects . <p> Lines 79 and 80- pass the image through the network and obtain the class label prediction . The prediction is then displayed on our screen on- Lines 83-86 . <p> To classify our images using the pre-trained MiniVGGNet network , just execute the following command : <p> testnetwork.py <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> $workon keras <p> $python testnetwork.py <p> --arch LONG ... <p> --weights LONG ... <p> --test-images testimages <p> The testnetwork.py- script first classifes- a sample of testing images that are- part of the CIFAR-10 dataset . The images below have been resized from their original- 32 x 32 pixel size to- 128 x 128 so we can more easily visualize them : <p> Figure 1 : Classifying a " bird " using the MiniVGGNet architecture . <p> Figure 2 : Using our trained CNN to recognize a " truck " in the image @ @ @ @ @ @ @ @ @ @ " . <p> The second part of the testnetwork.py- script takes images that- are not part of the CIFAR-10 dataset . This experiment attempts to demonstrate the transfer of learning from the original dataset to images the network has never seen : <p> Figure 4 : Our CNN is able to correctly label this image as " automobile " . <p> Figure 5 : MiniVGGNet has learned to recognize images from the " dog " class . <p> Figure 6 : Recognizing a " cat " in an image . <h> Summary <p> The primary focus of this lesson has been to utilize a- pre-trained network and use it to classify images that are ( 1 ) part of the CIFAR-10 dataset and ( 2 ) images that- are not part of CIFAR-10 . <p> In each case , we were able to successfully classify and label each of the images using the MiniVGGNet network . While this is n't a " true " test of the robustness of our network , it ( more importantly ) demonstrates how we can serialize and load our networks from disk and utilize them @ @ @ @ @ @ @ @ @ @ dataset they were trained on . <p> In the next set of deep learning lessons , well continue this discussion of pre-trained networks and how they can be used to alleviate the time and effort to construct new networks on a dataset-to-dataset basis . 
@@71485154 @185154/ <h> An intro to linear classification with Python <p> The first half of this tutorial focuses on the basic theory and mathematics surrounding linear classification and in general parameterized classification algorithms that actually " learn " from their training data . <p> From there , I provide an actual linear classification implementation and example using the scikit-learn library that can be used to classify the contents of an image . <h> 4 components of parametrized learning and linear classifiers <p> I 've used the word " parameterized " a few times now , but what exactly does it mean ? <p> Simply put : parameterization is the process of defining the necessary parameters of a given model . <p> Data : This is our- input data that we are going to learn from . This data includes- both the data points ( e.x. , feature vectors , color histograms , raw pixel intensities , etc. ) and their associated class labels . <p> Score function : - A function that accepts our data as input and maps the data to class labels . For instance , given our input feature @ @ @ @ @ @ @ @ @ @ applies some function- f ( our score function ) , and then returns the predicted class labels . <p> Weight matrix : The weight matrix , typically denoted as- W , is called the- weights or- parameters of our classifier that well actually be optimizing . Based on the output of our score function and loss function , well be tweaking and fiddling with the values of our weight matrix to increase classification accuracy . <p> Note : Depending on your type of model , there may exist- many more parameters . But at the most basic level , these- are the 4 building blocks of parameterized learning that you 'll commonly see . <p> Once we 've defined these 4 key components , we can then apply optimization methods that allow us to find a set of parameters- W that minimize our loss function with respect to our score function ( while increasing classification accuracy on the data ) . <p> Next , well look at how these components work together to build a linear classifier , transforming the input data into actual predictions . <h> Linear classification : from images @ @ @ @ @ @ @ @ @ @ to look at a more mathematical motivation of the parameterized model to machine learning . <p> To start we need our- data . Let 's assume that our training dataset ( either of images or extracted feature vectors ) is denoted as where each image/feature vector has an associated class label . Well assume - and implying that we have- N data points of dimensionality- D ( the " length " of the feature vector ) , separated into- K unique categories . <p> This this dataset , we have- N = 25,000 total images . Each image is characterized by a 3D color histogram with 8 bins per channel , respectively . This yields a feature vector with- D = 8 x 8 x 8 = 512 entries . Finally , we know there are a total of- K = 2 class labels , one for the- " dog " class and another for the- " cat " class . <p> Given these variables , we must now define a score function- f that maps the feature vectors to the class label scores . As the title of this blog @ @ @ @ @ @ @ @ @ @ : <p> Well assume that each is represented as a single column vector with shape- D x 1 . Again , in this example , well be using color histograms but if we were utilizing raw pixel intensities , we can simply- flatten the pixels of the image into a single vector . <p> Our weight matrix- W has a shape of- K x D ( the number of class labels by the dimensionality of the feature vector ) . <p> Going back to the Kaggle Dogs vs . Cats example , each is represented by a 512-d color histogram , so therefore has the shape- 512 x 1 . The weight matrix- W will have a shape of- 2 x 512 and finally the bias vector- b a size of- 2 x 1 . <p> Figure 1 : Illustrating the dot product of weight matrix W and feature vector x , followed by addition of the bias term . ( Inspired by Karpathys example in the CS231n course ) . <p> On the- left , we have our original input image , which we extract features from . In @ @ @ @ @ @ @ @ @ @ but any other feature representation could be used ( including the raw pixel intensities themselves ) , but in this case , well simply use a color distribution this histogram is our representation . <p> We then have our weight matrix- W , which contains 2 rows ( one for each class label ) and 512 columns ( one for each of the entries in the feature vector ) . <p> After taking the dot product between and , we add in the bias vector , which has a shape of- 2 x 1 . <p> Finally , this yields two values on the- right : the scores associated with the dog and cat labels , respectively . <p> Looking at the above equation , you can convince yourself that the input and are- fixed and- not something we can modify . Sure , we can obtain different s by applying a different feature extraction technique but once the features are extracted , - these values do not change . <p> In fact , the only parameters that we have any control over are our weight matrix- W and our @ @ @ @ @ @ @ @ @ @ utilize both our scoring function and loss function to- optimize ( i.e. , modify ) the weight and bias vectors such that our classification accuracy- increases . <p> Exactly- how we optimize the weight matrix depends on our loss function , but typically involves some form of gradient descent well be reviewing optimization and loss functions in a future blog post , but for the time being , simply understand that given a scoring function , we also define a loss function that tells us how " good " our predictions are on the input data . <h> Advantages of parametrized learning and linear classification <p> There are two primary advantages to utilizing- parameterized learning , such as in the approach I detailed above : <p> Once we are done training our model , we can discard the input data and keep only the weight matrix- W and the bias vector- b. - This substantially reduces the size of our model since we only need to store two sets of vectors ( versus the- entire training set ) . <p> Classifying new test data is- fast. - In order to @ @ @ @ @ @ @ @ @ @ take the dot product of and , followed by adding in the bias . Doing this is- substantially faster than needing to compare each testing point to- every training example ( as in the k-NN algorithm ) . <p> Now that we understand linear classification , let 's see how we can implement it in Python , OpenCV , and scikit-learn . <p> Specifically , well be using a Linear Support Vector Machine ( SVM ) which constructs a maximum-margin separating hyperplane between data classes in an- n-dimensional space . The goal of this separating hyperplane is to place all examples ( or as many as possible , given some tolerance ) of class- i on one side of the hyperplane and then all examples- not of class i on the other side of the hyperplane . <p> A detailed description of how Support Vector Machines work is outside the scope of this blog post ( but is covered inside the PyImageSearch Gurus course ) . <p> In the meantime , simply understand that our Linear SVM utilizes a score function- f similar to the one in the- " Linear classification @ @ @ @ @ @ @ @ @ @ blog post and then applies a loss function that is used to determine the maximum-margin separating hyperplane to classify the data points ( again , well be looking at loss functions in future blog posts ) . <p> To get started , open up a new file , name it linearclassifier.py- , and insert the following code : <p> An intro 9 <p> 10 <p> 11 55203 @qwx675203 <p> **25;4941;TOOLONG importLabelEncoder <p> fromsklearn.svm importLinearSVC <p> fromsklearn.metrics **26;4968;TOOLONG <p> **27;4996;TOOLONG importtraintestsplit <p> fromimutils importpaths 55220 @qwx675220 55218 @qwx675218 55219 @qwx675219 <p> importcv2 <p> importos <p> Lines 2-111 handle importing our required Python packages . Well be making use of the scikit-learn library , so if you do not already have it installed , make sure you follow these instructions to get it setup on your machine . <p> Well also be using my imutils Python package , a set- of image processing convenience- functions . If you do not already have imutils- installed , just let pip- @ @ @ @ @ @ @ @ @ @ classification with Python <p> Shell <p> 1 55204 @qwx675204 <p> Well now- define our extractcolorhistogram- function which will be used to extract and quantify the contents of our input images : <p> An intro to linear <p> 29 <p> 30 <p> LONG ... <p> # extract a 3D color histogram from the HSV color space using <p> # the supplied number of bins per channel <p> hsv=cv2.cvtColor ( image , cv2.COLORBGR2HSV ) <p> hist=cv2.calcHist ( hsv , 0,1,2 , None , bins , <p> 0,180,0,256,0,256 ) <p> # handle normalizing the histogram if we are using OpenCV 2.4 . X <p> ifimutils.iscv2() : <p> hist=cv2.normalize(hist) <p> # otherwise , perform " in place " normalization in OpenCV 3 ( I <p> # personally hate the way this is done <p> else : <p> cv2.normalize ( hist , hist ) <p> # return the flattened histogram as the feature vector <p> returnhist.flatten() <p> This @ @ @ @ @ @ @ @ @ @ HSV color space , and then computes a 3D color histogram using the supplied number of bins- for each channel . <p> After computing the color histogram using the cv2.calcHist- function , the histogram is normalized and then returned to the calling function . <p> We then grab the imagePaths- to where each of the 25,000 images reside on disk , followed by initializing a data- matrix to store our extracted feature vectors along with our class labels- . <p> Speaking of extracting features , let 's go ahead and do that : <p> An intro to linear <p> 60 <p> 61 <p> # loop over the input images <p> for ( i , imagePath ) inenumerate(imagePaths) : <p> # load the image and extract the class label ( assuming that our <p> # path as the format : /path/to/dataset/class. imagenum.jpg <p> **27;5025;TOOLONG <p> LONG ... <p> # extract a color histogram from the image , then update @ @ @ @ @ @ @ @ @ @ <p> data.append(hist) <p> labels.append(label) <p> # show an update every 1,000 images <p> ifi&gt;0andi%1000==0 : <p> print ( " INFO processed / " . format ( i , len(imagePaths) ) ) <p> On- Line 47 we start looping over our input imagePaths- . For each imagePath- , we load the image- from disk , extract the class label- , and then quantify the image by computing a color histogram . We then update our data- and labels- lists , respectively . <p> Currently , our labels- list is represented as a list of- strings , either " dog " or " cat " . However , many machine learning algorithms in scikit-learn prefer that the labels- are encoded as- integers , with one unique integer per class label . <p> Performing this conversion of class label string-to-integer is easy with the LabelEncoder- class : <p> An intro to linear classification with Python <p> Python <p> 63 <p> 64 <p> 65 <p> # encode the labels , converting them from strings to integers <p> le=LabelEncoder() <p> **30;5089;TOOLONG <p> After the . fittransform- method is called , our labels- are now represented @ @ @ @ @ @ @ @ @ @ notice here is that Im- purposely not tuning hyperparameters here , simply to keep this example shorter and easier to digest . However , with that said , I leave tuning the hyperparameters of the LinearSVC- classifier as an exercise to you , the reader . Use our previous blog post on tuning the hyperparameters of the k-NN classifier as an example . <h> Evaluating our linear classifier <p> To test out our linear classifier , make sure you have downloaded : <p> The source code to this blog post using the- " Downloads " section at the bottom of this tutorial . <p> As the above figure demonstrates , we were able to obtain- 64% classification accuracy , or approximately the same accuracy as using tuned hyperparameters from the k-NN algorithm in this tutorial . <p> Note : Tuning the hyperparameters to the Linear SVM will lead to a higher classification accuracy I simply left out this step to make the tutorial a little shorter and less- overwhelming . <p> Furthermore , not only did we obtain the same classification accuracy as k-NN , but our model is- much @ @ @ @ @ @ @ @ @ @ optimized ) dot product between the weight matrix and data points , followed by a simple addition . <p> We are also able to- discard the training data after training is complete , leaving us with only the weight matrix- W and the bias vector- b , leading to a much more compact representation of the classification model . <h> Summary <p> In todays blog post , I discussed the basics of parameterized learning and linear classification . While simple , the linear classifier can be seen as the fundamental building blocks of more advanced machine learning algorithms , extending naturally to Neural Networks and Convolutional Neural Networks . <p> You see , Convolutional Neural Networks will perform a mapping of raw pixels to class labels similar to what we did in this tutorial only our score function- f will become substantially more complex and contain- many more parameters . <p> A primary benefit of this parameterized approach to learning is that it allows us to- discard our training data after our model has been trained . We can then perform classification using- only the parameters ( i.e. , weight @ @ @ @ @ @ @ @ @ @ <p> This allows classification to be performed- much more efficiently since we : ( 1 ) do not need to store a copy of the training data in our model , like in k-NN and ( 2 ) we do not need to compare a test image to- every training image ( an operation that scales- O(N) , and can become quite cumbersome given many training examples ) . <p> In short , this method is- significantly faster , requiring only a single dot product and an addition . Pretty neat , right ? <p> Finally , we applied a linear classifier using Python , OpenCV , and scikit-learn to the Kaggle Dogs vs . Cats dataset . After extracting color histograms from the dataset , we trained a Linear Support Vector Machine on the feature vectors and obtained a classification accuracy of- 64% , which is fairly reasonable given that ( 1 ) color histograms are not the best choice for characterizing dogs vs. cats and ( 2 ) we did not tune the hyperparameters to our Linear SVM . <p> At this point , we are starting to @ @ @ @ @ @ @ @ @ @ Networks , Convolutional Neural Networks , and Deep Learning models , but there is still a ways to go . <p> To start , we need to understand loss functions in more detail , and in particular , how the loss function is used to- optimize our weight matrix to obtain more accurate predictions . Future blog posts will go into these concepts- in more detail . <p> Before you go , be sure to sign up for the PyImageSearch Newsletter using the form below to be notified when new blog posts are published ! <h> Downloads : 55217 @qwx675217 <p> I demonstrate how to use Histogram of Oriented Gradients as feature inputs to a Linear SVM inside Practical Python and OpenCV . To utilize SIFT/SURF or other keypoint detectors + local invariant descriptors you 'll need to construct a bag of visual words ( BOVW ) model . I discuss the BOVW model in detail ( with lots of code examples ) and apply it to image classification inside PyImageSearch Gurus . Be sure to take a look ! <p> First of all , being bored at work does pay @ @ @ @ @ @ @ @ @ @ blogs.Amazing information with facts thoughtfully incorporated within . Definitely going to come back for more ! = <p> Dear Adrian ! First I would like to thanks for your tutorial . Wonderful job and very useful source of information for newbies like me . I want to use this classifier as a tool to extract background water , from the images . I have to classes : water and others . Could you recommend me an approach/functions which could be the best ? My idea is to based on the output I would like to give water black values , then remove unwanted objects from the image and find centroids of object of interest . The last part will be based on connecting components . Though I find it hard to mask the water . Thank you <p> Instead of directly using machine learning methods to create pixel-wise segmentations of the image , have you considered using other segmentation methods such as thresholding , adaptive thresholding , GrabCut , etc. ? <p> Hi Adrian . Thanks for the tutorial . It is a really big help for a newbie like @ @ @ @ @ @ @ @ @ @ , based on classification results , I can change pixel values of my two classes , to black for one and while to second class ? Thanks <p> Hi Adrian . I wanted to use svm to perform classification over images with water and mixed ( water and objects ) and then change values of water into black and others into white . Extract background using svm <p> This sounds like a pixel-wise segmentation problem . Without seeing example images of what you 're working with , its hard for me to provide suggestions . You might want to consider researching pixel-wise segmentation using Random Forests . Deep learning can also be used here , but its likely overkill . <h> Trackbacks/Pingbacks <p> couple weeks ago , we discussed the concepts of both- linear classification- and- parameterized learning . This type of learning allows us to take a set of input data and class labels @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485155 @185155/ <h> OpenCV : Resolving NoneType errors <p> Each week I receive and respond to at least 2-3 emails and 3-4 blog post comments regarding NoneType- errors in OpenCV and Python . <p> For beginners , these errors can be hard to diagnose by definition they are n't  very informative . <p> Since this question is getting asked so often I decided to dedicate an entire blog post to the topic . <p> While NoneType- errors can be caused for a nearly unlimited number of reasons , in my experience , both as a computer vision developer and chatting with other programmers here on PyImageSearch , in over 95% of the cases , NoneType- errors in OpenCV are caused by either : <p> An invalid image path passed to cv2.imread- . <p> A problem reading a frame from a video stream/video file via cv2.VideoCapture- and the associated . read- method . <p> To learn more about NoneType- errors in OpenCV ( and how to avoid them ) , just keep reading . <h> OpenCV : Resolving NoneType errors <p> In the first part of this blog post I 'll discuss exactly- @ @ @ @ @ @ @ @ @ @ Ill then discuss the two primary reasons you 'll run into NoneType- errors when using OpenCV and Python together . <p> Finally , I 'll put together an- actual example that not only- causes a NoneType- error , but also- resolves it as well . <h> What is a NoneType error ? <p> When using the Python programming language you 'll inevitably run into an error that looks like this : <p> OpenCV : Resolving NoneType errors <p> Shell <p> 1 <p> AttributeError : ' NoneType'objecthas no attributesomething ' <p> Where something- can be replaced by whatever the name of the actual attribute is . <p> We see these errors when we- think we are working with an instance of a particular Class or Object , but in reality we have the Python built-in typeNone- . <p> As the name suggests , None- represents the- absence of a value , such as when a function call returns an unexpected result or fails entirely . <p> Here is an example of generating a NoneType- error from the Python shell : <p> OpenCV : Resolving NoneType errors <p> Python <p> 1 <p> 2 <p> 3 @ @ @ @ @ @ @ @ @ @ <p> Traceback ( most recent call last ) : <p> File " &lt;stdin&gt; " , line1 , in&lt;module&gt; <p> AttributeError : ' NoneType'objecthas no attribute'bar ' <p> &gt;&gt;&gt; <p> Here I create a variable named foo- and set it to None- . <p> I then try to set the bar- attribute of foo- to True- , but since foo- is a NoneType- object , Python will not allow this hence the error message . <h> Two reasons for 95% of OpenCV NoneType errors <p> When using OpenCV and Python bindings , you 're bound to come across NoneType- errors at some point . <p> In my experience , - over 95% of the time these NoneType- errors can be traced back to either an issue with cv2.imread- or cv2.VideoCapture- . <p> I have provided details for each of the cases below . <h> Case #1 : cv2.imread <p> If you are receiving a NoneType- error- and your code is calling cv2.imread- , then the likely cause of the error is an- invalid file path supplied to cv2.imread- . <p> The cv2.imread- function does not explicitly throw an error message if you @ @ @ @ @ @ @ @ @ @ path to a nonexistent file ) . Instead , cv2.imread- will simply return None- . <p> Anytime you try to access an attribute of a None- image loaded from disk via cv2.imread- you 'll get a NoneType- error . <p> Here is an example of trying to load a nonexistent image from disk : <p> OpenCV : Resolving NoneType errors <p> Python <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> 6 <p> 7 <p> 8 <p> $python <p> &gt;&gt;&gt;importcv2 <p> LONG ... <p> **34;5121;TOOLONG <p> **30;5157;TOOLONG <p> Traceback ( most recent call last ) : <p> File " &lt;stdin&gt; " , line1 , in&lt;module&gt; <p> AttributeError : ' NoneType'objecthas no attribute'shape ' <p> As you can see , cv2.imread- gladly accepts the image path ( even though it does n't  exist ) , realizes the image path is invalid , and returns None- . This is especially confusing for Python programmers who are used to these types of functions throwing exceptions . <p> As an added bonus , I 'll also mention the AssertionFailed- exception . <p> If you try to pass an invalid image ( i.e. , NoneType- @ @ @ @ @ @ @ @ @ @ will complain that the image does n't  have any width , height , or depth information and how could it , the " image " is a None- object after all ! <p> Here is an example of an error message you might see when loading a nonexistent image from disk and followed by immediately calling an OpenCV function on it : <p> These types of errors can be harder to debug since there are- many reasons why an AssertionError- could be thrown . But in most cases , your first step should be be ensuring that your image was correctly loaded from disk . <p> A final , more rare problem you may encounter with cv2.imread- is that your image- does exist on disk , but you did n't  compile OpenCV with the given image I/O libraries installed . <p> For example , let 's say you have a . JPEG file on disk and you- knew you had the correct path to it . <p> You then try to load the JPEG file via cv2.imread- and notice a NoneType- or AssertionError- . <p> How can this be ? <p> @ @ @ @ @ @ @ @ @ @ likely forgot to compile OpenCV with JPEG file support enabled . <p> In Debian/Ubuntu systems , this is caused by a lack of libjpeg- being installed . <p> For macOS systems , you likely forgot to install the jpeg- library via Homebrew . <p> To resolve this problem , regardless of operating system , you 'll need to re-compile and re-install OpenCV . Please see this page- for more details on how to compile and install OpenCV on your particular system . <h> Case #2 : cv2.VideoCapture and . read <p> Just like we see NoneType- errors and AssertionError- exceptions when using cv2.imread- , you 'll also see these errors when working with video streams/video files as well . <p> To access a video stream , OpenCV uses the cv2.VideoCapture- which accepts a single argument , either : <p> A- string representing the path to a video file on disk . <p> An- integer representing the- index of a webcam on your computer . <p> Working with video streams and video files with OpenCV is more complex than simply loading an image via cv2.imread- , but the same rules apply . <p> @ @ @ @ @ @ @ @ @ @ an instantiated cv2.VideoCapture- ( regardless if its a video file or webcam stream ) and notice a NoneType- error or- AssertionError- , then you likely have a problem with either : <p> The path to your input video file ( its probably incorrect ) . <p> Not having the proper video codecs installed , in which case you 'll need to install the codecs , followed by re-compiling and re-installing OpenCV ( see this page for a complete list of tutorials ) . <p> Your webcam not being accessible via OpenCV . This could be for any number of reasons , including missing drivers , an incorrect index passed to cv2.VideoCapture- , or simply your webcam is not properly attached to your system . <p> Again , working with video files is more complex than working with simple image files , so make sure youre- systematic in resolving the issue . <p> This is- required reading if you expect to follow tutorials here on the PyImageSearch blog . <p> Working with the command line , and therefore command line arguments , are a big part of what it means to be @ @ @ @ @ @ @ @ @ @ is- only going to harm you. - Youll thank me later . <p> Going back to the example , let 's check the contents of my local directory : <p> OpenCV : Resolving NoneType errors <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> $ls-l <p> total800 <p> **26;5189;TOOLONG **33;5217;TOOLONG <p> **26;5252;TOOLONG **30;5280;TOOLONG <p> As we can see , I have two files : <p> displayimage.py- : My Python script that I 'll be executing shortly . <p> jemma.png- : The photo I 'll be loading from disk . <p> If I execute the following command I 'll see the jemma.png- image displayed to my screen , along with information on the dimensions of the image : <p> OpenCV : Resolving NoneType errors <p> Shell <p> 1 <p> 2 <p> $python displayimage.py--image jemma.png <p> w:376 , h:500 , d:3 <p> Figure 1 : Loading and displaying an image to my screen with OpenCV and Python . <p> However , let 's try to load an image path that- does not exist : <p> OpenCV : Resolving NoneType errors <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> $python displayimage.py--image @ @ @ @ @ @ @ @ @ @ <p> LONG ... <p> ( h , w , d ) =image.shape <p> AttributeError : ' NoneType'objecthas no attribute'shape ' <p> Sure enough , there is our NoneType- error . <p> In this case , it was caused because I did not supply a valid image path to cv2.imread- . <h> Summary <p> In this blog post I discussed NoneType- errors and AssertionError- exceptions in OpenCV and Python . <p> In the vast majority of these situations , these errors can be attributed to either the cv2.imread- or cv2.VideoCapture- methods . <p> Whenever you encounter one of these errors , - make sure you can load your image/read your frame before continuing . In over 95% of circumstances , your image/frame was not properly read . <p> Otherwise , if you are using command line arguments and are unfamiliar with them , there is a chance that you are n't  using them properly . In that case , make sure you educate yourself by reading this tutorial on command line- arguments you 'll thank me later . <p> Anyway , I hope this tutorial has helped you in your journey to @ @ @ @ @ @ @ @ @ @ computer vision and OpenCV , I would highly encourage you to take a look at my book , - Practical Python and OpenCV , which will help you grasp the fundamentals . <p> Otherwise , make sure you enter your email address in the form below to be notified when future blog posts and tutorials are published ! <h> Downloads : 55217 @qwx675217 <h> 8 Responses to OpenCV : Resolving NoneType errors <p> Great article . If writing production code , one should be kind enough to have some basic tests for inputs . I was kind of puzzled before I tested , if the image file could be opened . OpenCV has @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485157 @185157/ <h> I just open sourced my personal imutils package : A series of OpenCV convenience functions . <p> PyPI , the Python Package Index repository is a wonderful thing . It makes downloading , installing , and managing Python libraries and packages a breeze . <p> And with all that said , - I have pushed my own personal imutils package online . I use this package nearly every single day when working on computer vision and image processing problems . <p> This package includes- a series of OpenCV + convenience functions that perform basics tasks such as translation , rotation , resizing , and skeletonization . <p> In the future we will ( probably , depending on feedback in the comments section ) be- performing a detailed code review of each of the functions in the imutils- - package , but for the time being , take a look at the rest of this blog post to see the functionality included in imutils- , then be sure to install it on your own system ! <h> Installing <p> This package assumes that you already have NumPy and OpenCV @ @ @ @ @ @ @ @ @ @ using the opencv2matplotlib- - function ) . <p> To install the the imutils- - library , just issue the following command : <p> I just open sourced my personal imutils package : A series of OpenCV convenience functions . <p> Python <p> 1 55204 @qwx675204 <h> My imutils package : A series of OpenCV convenience functions <p> Let 's go ahead and take a look at what we can do with the imutils- - package . <h> Translation <p> Translation is the shifting of an image in either the x or y- direction . To translate an image in OpenCV you need to supply the ( x , y ) -shift , denoted as ( tx , ty ) - to construct the translation matrix M : <p> And from there , you would need to apply the cv2.warpAffine- - function . <p> Instead of manually constructing the translation matrix M- and calling cv2.warpAffine- , you can simply make a call to the translate- - function of imutils- . <h> Example : <p> I just open sourced my personal imutils package : A series of OpenCV convenience functions . <p> Python @ @ @ @ @ @ @ @ @ @ pixels to the right and y=75 pixels up <p> LONG ... <h> Output : <h> Rotation <p> Rotating an image in OpenCV is accomplished by making a call to cv2.getRotationMatrix2D- - and cv2.warpAffine- . Further care has to be taken to supply the ( x , y ) -coordinate of the point the image is to be rotated about . These calculation calls can quickly add up and make your code bulky and less readable . The rotate- - function in imutils- - helps resolve this problem . <h> Example : <p> I just open sourced my personal imutils package : A series of OpenCV convenience functions . <p> Python <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> # loop over the angles to rotate the image <p> forangle inxrange(0,360,90) : <p> # rotate the image and display it <p> **29;5312;TOOLONG , angle=angle ) <p> cv2.imshow ( " Angle=%d " % ( angle ) , rotated ) <h> Output : <h> Resizing <p> Resizing an image in OpenCV is accomplished by calling the cv2.resize- - function . However , special care needs to be taken to ensure @ @ @ @ @ @ @ @ @ @ function of imutils- - maintains the aspect ratio and provides the keyword arguments width- - and height- - so the image can be resized to the intended width/height while ( 1 ) maintaining aspect ratio and ( 2 ) ensuring the dimensions of the image do not have to be explicitly computed by the developer . <p> Another optional keyword argument , inter- , can be used to specify interpolation method as well . <h> Example : <p> I just open sourced my personal imutils package : A series of OpenCV convenience functions . <p> Python <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> # loop over varying widths to resize the image to <p> forwidth in(400,300,200,100) : <p> # resize the image and display it <p> **32;5343;TOOLONG , width=width ) <p> cv2.imshow ( " Width=%dpx " % ( width ) , resized ) <h> Output : <h> Skeletonization <p> Skeletonization is the process of constructing the " topological skeleton " of an object in an image , where the object is presumed to be white on a black background . OpenCV does not provide a function @ @ @ @ @ @ @ @ @ @ morphological and binary functions to do so . <p> For convenience , the skeletonize- - function of imutils- - can be used to construct the topological skeleton of the image . <p> The first argument , size- - is the size of the structuring element kernel . An optional argument , structuring- , can be used to control the structuring element it defaults to cv2.MORPHRECT- - , but can be any valid structuring element . <h> Example : <p> I just open sourced my personal imutils package : A series of OpenCV convenience functions . <p> Python <p> 1 <p> 2 <p> 3 <p> 4 <p> # skeletonize the image <p> gray=cv2.cvtColor ( logo , cv2.COLORBGR2GRAY ) <p> **33;5377;TOOLONG , size= ( 3,3 ) ) <p> cv2.imshow ( " Skeleton " , skeleton ) <h> Output : <h> Displaying with Matplotlib <p> In the Python bindings of OpenCV , images are represented as NumPy arrays in BGR order . This works fine when using the cv2.imshow- - function . However , if you intend on using Matplotlib , the plt.imshow- - function assumes the image is in RGB order . @ @ @ @ @ @ @ @ @ @ , or you can use the opencv2matplotlib- convenience- function . <h> Example : <p> I just open sourced my personal imutils package : A 7 <p> 8 <p> # INCORRECT : show the image without converting color spaces <p> plt.figure ( " Incorrect " ) <p> plt.imshow(cactus) <p> # CORRECT : convert color spaces before using plt.imshow <p> plt.figure ( " Correct " ) <p> **36;5412;TOOLONG ( cactus ) ) <p> plt.show() <p> Output : <h> Summary <p> So there you have it the imutils package ! <p> I hope you install it and give it a try . It will definitely make performing simple image processing tasks with OpenCV and Python substantially easier ( and with less code ) . <p> In the coming weeks well perform a code review of each of the functions and discuss what is going on under the hood . <h> 38 Responses to I just open sourced my personal imutils package : A series of OpenCV convenience functions . <p> I 'm trying @ @ @ @ @ @ @ @ @ @ working on , but from importing imutils I get the error : module object has no attribute IMREADCOLOR . The error seems to come from the url to image functions second parameter , readFlag=cv2.IMREADCOLOR . <p> I 'm working on python 2.7 . Could it be a problem with OpenCV ? I have 2.4.10 installed . <p> Hey David , that does indeed sound like an OpenCV problem , but as far as I know the cv2.IMREADCOLOR is part of OpenCV 2.4.5+ . Can you try manually editing the code to set readFlag=1 and see if that resolves the issue ? <p> UPDATE : <p> I just tried on OpenCV 2.4.10.1 . The cv2.IMREADCOLOR flag is definitely present : <p> I personally do n't  do much work in C++ , at this point I do mainly Python , so I 'm not too privy with the latest C++ libraries . If I run across anything that looks interesting , I 'll be sure to post it here . <p> I do n't  personally use LiClipse , but I would suggest becoming familiar with the prompt . Nearly all examples on the PyImageSearch blog @ @ @ @ @ @ @ @ @ @ getting used to it and working with it now . <p> As for matplotlib , no , it is not required for tracking objects in images . <p> I-m using the preinstalled python 3 IDE on Raspbian , could you give a short tutorial how to " set the correct Python interpreter for my IDE " . I-m sorry , but this whole Linux/raspi/etc thing is relatively new to me . <p> There is option to to resize image both width and height ( not just one of them ) ? p.s it will be lovely to get some notification by mail or something else when you/ your team response to comment ( for me its difficult to track all my comments ita s lot , lol .. ) <p> If you want to resize on just one dimension and ignore the aspect ratio you should simply call cv2.resize directly . An example can be found in in this blog post . You 'll also find a more detailed explanation @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485158 @185158/ <h> Multi-class SVM Loss <p> A couple weeks ago , we discussed the concepts of both- linear classification- and- parameterized learning . This type of learning allows us to take a set of input data and class labels , and actually learn a function that- maps the input to the output predictions , simply by defining a set of parameters and optimizing over them . <p> Our linear classification tutorial focused mainly on the concept of a- scoring function and how it can be used to map the input data to class labels . But in order to actually " learn " the mapping from the input data to class labels , we need to discuss two important concepts : <p> To learn more about your first loss function , Multi-class SVM loss , just keep reading . <h> Multi-class SVM Loss <p> At the most basic level , a loss function is simply used to quantify how " good " or " bad " a given predictor is at classifying the input data points in a dataset . <p> The smaller the loss , the better a job @ @ @ @ @ @ @ @ @ @ data and the output class labels ( although there is a point where we can- overfit our model this happens when the training data is modeled- too closely and our model loses the ability to generalize ) . <p> Conversely , the larger our loss is , the- more work needs to be done- to increase classification accuracy . In terms of parameterized learning , this involves tuning parameters such as our weight matrix- W or bias vector- b to improve classification accuracy . Exactly- how we go about updating these parameters is an- optimization problem , which well be covering later in this series of tutorials . <h> The mathematics behind Multi-class SVM loss <p> This previous tutorial focused on the concept of a scoring function- f that maps our feature vectors to class labels as numerical scores . As the name suggests , a Linear SVM applies a simple linear mapping : <p> Now that we have this scoring/mapping function- f , we need to determine how " good " or " bad " this function is ( given the weight matrix- W and bias vector- b ) @ @ @ @ @ @ @ @ @ @ classification tutorial , we know that we have a matrix of feature vectors- x these feature vectors could be extracted color histograms , Histogram of Oriented Gradients features , or even raw pixel intensities . <p> Regardless of- how we choose to quantify our images , the point is that we have a matrix- x of features extracted from our image dataset . We can then access the features associated with a given image via the syntax- , which will yield the- i-th feature vector inside- x . <p> Similarly , we also have a vector- y which contains our- class labels for each- x . These- y values are our- ground-truth labels and what we hope our scoring function will correctly predict . Just like we can access a given feature vector via , we can access the- i-th class label via- . <p> Note : I 'm purposely skipping the regularization parameter for now . Well return to regularization in a future post once we better understand loss functions . <p> So what is the above equation doing exactly ? <p> I 'm glad you asked . <p> Essentially , @ @ @ @ @ @ @ @ @ @ ( ) and comparing the output of our scoring function- s returned for the- j-th class label ( the incorrect class ) and the -th class ( the correct class ) . <p> We apply the- max operation to clamp values to- 0 this is important to do so that we do not end up summing negative values . <p> A given is classified correctly when the loss ( I 'll provide a numerical example of this in the following section ) . <p> To derive the loss across our- entire training set , we simply take the mean over each individual : <p> Another related , common loss function you may come across is the- squared hinge loss : <p> The squared term penalizes our loss more heavily by squaring the output . This leads to a quadratic growth in loss rather than a linear one . <p> As for which loss function you should use , that is entirely dependent on your dataset . Its typical to see the standard hinge loss function used more often , but on some datasets the squared variation might obtain better accuracy - @ @ @ @ @ @ @ @ @ @ . <h> A Multi-class SVM loss example <p> Now that we 've taken a look at the mathematics behind hinge loss and squared hinge loss , let 's take a look at a worked example . <p> Well again assume that were working with the Kaggle Dogs vs . Cats dataset , which as the name suggests , aims to classify whether a given image contains a- dog or a- cat . <p> There are only two possible class labels in this dataset and is therefore a 2-class problem which can be solved using a standard , binary SVM loss function . That said , let 's still apply Multi-class SVM loss so we can have a worked example on how to apply it . From there , I 'll extend the example to handle a 3-class problem as well . <p> To start , take a look at the following figure where I have included 2 training examples from the 2 classes of the Dogs vs . Cats dataset : <p> Figure 1 : Let 's apply hinge loss to the images in this figure . <p> Given some ( arbitrary ) weight matrix- @ @ @ @ @ @ @ @ @ @ are displayed in the body of the matrix. - The- larger the scores are , the more- confident our scoring function is regarding the prediction . <p> Let 's start by computing the loss for the " dog " class . Given a two class problem , this is trivially easy : <p> Multi-class SVM Loss <p> Python <p> 1 <p> 2 <p> 3 <p> **30;5450;TOOLONG <p> 0 <p> &gt;&gt;&gt; <p> Notice how the loss for " dog " is- zero this implies that the dog class was correctly predicted . A quick investigation of- Figure 1- above demonstrates this to be true : the " dog " score is greater than the " cat " score . <p> Similarly , we can do the same for the second image , this one containing- a cat : <p> Multi-class SVM Loss <p> Shell <p> 1 <p> 2 <p> 3 <p> &gt;&gt;&gt;max ( 0,3.76- ( -1.2 ) +1 ) <p> 5.96 <p> &gt;&gt;&gt; <p> In this case , our loss function is greater than zero , indicating that our prediction is not correct . <p> We then obtain the- total loss over @ @ @ @ @ @ @ @ @ @ Multi-class SVM Loss <p> Python <p> 1 <p> 2 <p> 3 <p> &gt;&gt;&gt; ( 0+5.96 ) /2 <p> 2.98 <p> &gt;&gt; <p> That was simple enough for a 2-class problem , but what about a 3-class problem ? Does the process become more complicated ? <p> In reality , it does n't  our summation just expands a bit . You can find an example of a 3-class problem below , were I have added a third class , " horse " : <p> After understanding the concept of " loss " and how it applies to machine learning and deep learning algorithms , we then looked at two specific loss functions : <p> Hinge loss <p> Squared hinge loss <p> In general , youl see hinge loss more often but its still worth attempting to tune the hyperparameters to your classifier- to determine which loss function gives better accuracy on your particular dataset . <p> Next week I 'll be back to discuss a second loss function cross-entropy and the relation it has to Multinomial Logistic Regression . If you have any prior experience in machine learning or deep learning , @ @ @ @ @ @ @ @ @ @ . <p> If you 're interested in applying Deep Learning and Convolutional Neural Networks , then you do n't  want to miss this upcoming post as you 'll find out , - the Softmax classifier is the most used model/loss function in Deep Learning . <p> To be notified when the next blog post is published , just enter your email address in the form below . See you next week ! <p> Answer for the above quiz is cat ( the class which is correctly predictes ) , as the loss function value is 0 . This is very evident is n't it , as the score of cat is very high compared to the other two classes . <p> So mathematically the max function would be always zero in cases where the score difference is -1 and less . <p> Need for optimization and tuning the hyperparameter will come in cases where the class scores are near then we cant really say that the classifier has learnt the data/classes properly and there is always need for regularization and @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485159 @185159/ <h> Segmenting characters from license plates <p> Segmenting characters from license plates <p> In our previous lesson , we learned how to- localize license plates in images using basic image processing techniques , such as morphological operations and contours . These license plate regions are called- license plate- candidates it is our job to take these candidate regions and start the task of extracting the- foreground license plate characters from the- background of the license plate . <p> On the surface , this step looks quite easy all we need to do is perform- basic thresholding , and well be able to extract the license plate characters , right ? <p> Well , not exactly . <p> Sure , if we apply basic thresholding , well be able to extract the license plate characters but well also extract a bunch of other " stuff " that does n't  interest us , such as any bolts fastening the license plate to the car , branding logos on the plate itself , or embellishments on the license plate frames . All three of these aspects can cause problems for our character @ @ @ @ @ @ @ @ @ @ as we write our code . <p> But no worries Ive got you covered . While there are a few " gotchas " that may trip you up , once you see them first hand , I think you 'll be able to spot them in the future when you work on your own projects . <h> Objectives : <p> In this lesson , we will : <p> Apply a- perspective transform to extract the license plate region from the car , obtaining a top-down , birds eye view more suitable for character segmentation . <h> Segmenting characters from the license plate <p> As a quick refresher from our previous lesson on license plate localization , our algorithm is now capable of identifying license plate-like regions in our dataset of images : <p> Figure 1 : Examples of detecting license plate-like regions in images . <p> In each of the images above , you can see that we have clearly found the license plate in the image and drawn a green bounding box surrounding it . <p> The next step is for us to take this license plate region and apply @ @ @ @ @ @ @ @ @ @ plate characters from the license plate itself . <h> Character segmentation <p> In order to perform character segmentation , well need to heavily modify our licenseplate.py- file from the previous lesson . This file encapsulates all the methods we need to extract license plates and license plate characters from images . The LicensePlateDetector- class specifically will be doing a lot of the heavy lifting for us . here 's a quick breakdown of the structure of the LicensePlateDetector- class : <p> LicensePlateDetector class structure <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> ---LicensePlateDetector <p> ---init <p> ---detect <p> ---detectPlates <p> **28;5482;TOOLONG <p> We 've already started to define this class in our previous lesson . The init- method is simply our constructor . The detect- method calls all necessary sub-methods to perform the license plate detection . The aptly named detectPlates- function accepts an image and detects all license plate candidates . Finally , the **26;5512;TOOLONG method is used to accept a license plate region and segment the license plate characters from the background . <p> The detectPlates- function is already 100% defined from our previous lesson @ @ @ @ @ @ @ @ @ @ method . However , we will need to modify the constructor to modify a few arguments , update the detect- method to return the character candidates , and define the entire **26;5540;TOOLONG function . <p> Since well be building on our previous lesson and introducing a lot of new functionality , let 's just start from the top of licenseplate.py- and start reworking our code : <p> The first thing you 'll notice is that- were importing a lot more packages than from our previous lesson , mainly image processing functions from scikit-image and imutils- . <p> We also define a namedtuple- on- Line 12 which is used to store information regarding the detected license plate . If you have never heard of ( or used ) a namedtuple- before , no worries they are simply easy-to-create , lightweight object types . If you are familiar with the C/C++ programming language , you may remember the keyword struct- which is a way of defining a complex data type . <p> The namedtuple- functionality in Python is quite similar to a struct- in C. You start by creating a namespace for the tuple @ @ @ @ @ @ @ @ @ @ of attributes that the namedtuple- has . Since the Python language is loosely typed , there is no need to define the data types of each of the attributes . In many ways , you can mimic the functionality of a namedtuple- using other built-in Python datatypes , such as dictionaries and lists ; however , the namedtuple- gives you a big distinct advantage its dead simple to instantiate new namedtuples- . In fact , as you 'll see later in this lesson , its as simple as passing in a list of arguments to the LicensePlate- variable , just like instantiating a class ! <p> In the meantime , the descriptions of each LicensePlate- attribute are listed below : <p> success- : A boolean indicating whether the license plate detection and character segmentation was successful or not . <p> candidates- : A list of character candidates that should be passed on to our machine learning classifier for final identification . <p> Well be discussing each of these attributes in more detail as we work through the rest of this lesson , but its a good idea to have a high-level @ @ @ @ @ @ @ @ @ @ - for . <p> # store the image to detect license plates in , the minimum width and height of the <p> # license plate region , the number of characters to be detected in the license plate , <p> # and the minimum width of the extracted characters <p> self.image=image <p> self.minPlateW=minPlateW <p> self.minPlateH=minPlateH <p> self.numChars=numChars <p> self.minCharW=minCharW <p> Not a whole lot has changed with our constructor , except that we are now accepting two more parameters : numChars- , which is the number of characters our license plate has , and minCharW- , which , as the name suggests , is the minimum number of pixels wide a region must be to be considered a license plate character . <p> You might be thinking , is n't it cheating to supply a value like numChars- to our LicensePlateDetector- ? How do we- know that the license plate we have detected contains seven characters ? Couldnt it just as easily contain five characters ? Or ten ? <p> Well , as I mentioned in our- What is ANPR ? lesson ( and is true for all computer vision projects @ @ @ @ @ @ @ @ @ @ to apply as much- a priori- knowledge as we possibly can to build a successful system . After examining our dataset in the previous two lessons , a clear piece of knowledge we can exploit is the number of characters present on the license plate : <p> Figure 2 : An important fact to note is that each of our license plates contains 7 characters 3 letters followed by 4 numbers . Well be able to exploit this information to build a more accurate character classifier later in this module . <p> In each of the above cases , all license plates contain seven characters . Thus , if we are to build an ANPR system for this region of the world , we can safely assume a license plate has seven characters . If a license plate- does not contain seven characters , then we can flag the plate and manually investigate it to see ( 1 ) if there is a bug in our ANPR system , or ( 2 ) we need to create a separate ANPR system to recognize plates with a different number of characters @ @ @ @ @ @ @ @ @ @ highly tuned to the regions of the world they are deployed to thus it is safe ( and even presumable ) for us to apply these types of assumptions . <p> Our detect- method will also require a few minor updates <p> 36 <p> 37 <p> defdetect(self) : <p> # detect license plate regions in the image <p> **29;5568;TOOLONG <p> # loop over the license plate regions <p> forlpRegion inlpRegions : <p> # detect character candidates in the current license plate region <p> **43;5599;TOOLONG <p> # only continue if characters were successfully detected <p> iflp.success : <p> # yield a tuple of the license plate object and bounding box <p> yield ( lp , lpRegion ) <p> The changes here are quite self-explanatory . A call is made to detectPlates- to find the license plate regions in the image . We loop over each of these license plate regions individually , make a call to the **25;5644;TOOLONG method ( to be defined in @ @ @ @ @ @ @ @ @ @ on the license plate region itself , and finally , return a tuple of the LicensePlate- object and license plate bounding box to the calling function . <p> The detectPlates- function is defined next , but since we have already reviewed in our- license plate localization lesson , well skip it in this article please refer that lesson for the definition of detectPlates- . As the name suggests , it simply detects license plate candidates in an input image . <p> Its clear that all the real work is done inside the **26;5671;TOOLONG function . This method accepts a license plate region , applies image processing techniques , and then segments the foreground license plate characters from the background . Let 's go ahead and start defining this method : <p> licenseplate.py <p> Python <p> 99 <p> 100 <p> 101 <p> 102 <p> **33;5699;TOOLONG , region ) : <p> # apply a 4-point transform to extract the license plate <p> LONG ... <p> cv2.imshow ( " Perspective Transform " , imutils.resize ( plate , width=400 ) ) <p> The **26;5734;TOOLONG method accepts only a single parameter the region- of the image @ @ @ @ @ @ @ @ @ @ our definition of detectPlates- that the region- is a rotated bounding box : <p> Figure 3 : Our license plate detection method- returns a rotated bounding box corresponding to the location of the license plate in the image . <p> Figure 3 : Our license plate detection method- returns a rotated bounding box corresponding to the location of the license plate in the image . <p> However , looking at this license plate region , we can see that it is a bit distorted and skewed . This skewness can dramatically throw off not only our character segmentation algorithms , but also- character identification- algorithms when it comes to applying machine learning later in this module . <p> Because of this , we must first apply a- perspective transform to apply a top-down , birds eye view of the license plate : <p> Figure 4 : The original license plate could be distorted or skewed which can hurt character classification performance later in the ANPR pipeline . We apply a perspective transform to obtain a top-down , 90-degree viewing angle of the license plate to help alleviate this problem . @ @ @ @ @ @ @ @ @ @ been adjusted , as if we had a 90-degree viewing angle , cleanly looking down on it . <p> Note : I cover perspective transform- twice on the PyImageSearch blog . The first post covers the basics of performing a perspective transform , and the second post applies perspective transform to solve a real-world computer vision problem building a mobile document scanner . Once we get through the first round of this course , I plan on rewriting these perspective transform articles and bringing them inside Module 1 of PyImageSearch Gurus . But for the time being , I think the explanations on the blog clearly demonstrate how a perspective transformation works . <p> Now that we have a top-down , birds eye view of the license plate , we can start to process it . The first step is to extract the Value- channel from the HSV color space on- Line 106 . <p> So why did I extract the Value channel rather than use the grayscale version of the image ? <p> Well , if you remember back to our lesson on- lighting and color spaces , you 'll @ @ @ @ @ @ @ @ @ @ weighted combination- of the RGB channels . The Value channel , however , is given a dedicated dimension in the HSV color space . When performing thresholding to extract dark regions from a light background ( or vice versa ) , better results can often be obtained by using the Value rather than grayscale . <p> To segment the license plate characters from the background , we apply adaptive thresholding on- Line 107 , - where thresholding is applied to each local- 30 x 30- pixel region of the image . As we know from our thresholding lesson , basic thresholding and Otsus thresholding both- obtain sub-par results when segmenting license plate characters : <p> Figure 5 : - Applying Otsu 's thresholding method in an attempt to segment the foreground characters from the background license plate notice how the bolt of the license plate is connected to the " 0 " - character . This can cause problems in character recognition later in our ANPR pipeline . <p> At first glance , this output does n't look too bad . Each character appears to be neatly segmented from the @ @ @ @ @ @ @ @ @ @ Notice how the bolt of the license plate is attached to the character . While this artifact does n't  seem like a big deal , failing to extract high-quality segmented representations of each license plate character can really hurt our classification performance when we go to recognize each of the characters later in this module . <p> Due to the limitations of basic thresholding , we instead apply adaptive thresholding , which gives us much better results : <p> Figure 6 : The result of applying adaptive threshold - the gap between the bolt and the number " 0 " - is clearly visible now . <p> Notice how the bolt and the- 0 character are cleanly detached from each other . <p> In previous lessons in this course , we often would apply contour detection after obtaining a binary representation of an image . However , in this case , we are going to do something a bit different well instead apply a connected component analysis of the thresholded license plate region : <p> licenseplate.py <p> Python <p> 115 <p> 116 <p> 117 <p> 118 <p> # perform a @ @ @ @ @ @ @ @ @ @ locations <p> # of the character candidates <p> LONG ... <p> LONG ... <p> Given our connected component labels , we initialize a charCandidates- mask to hold the contours of the character candidates on- Line 118 . <p> Now that we have the labels for each connected component , let 's loop over them individually and process them <p> 129 <p> 130 <p> # loop over the unique components <p> forlabel innp.unique(labels) : <p> # if this is the background label , ignore it <p> iflabel==-1 : <p> continue <p> # otherwise , construct the label mask to display only connected components for the <p> We start looping over each of the labels- on- Line 121 . If the label- is -1 , then we know the label- corresponds to the background of the license plate , so we can safely ignore it . <p> Otherwise , we allocate memory for the labelMask- on- Line 128 and draw all pixels with the current label- value as- white on @ @ @ @ @ @ @ @ @ @ masking , we are revealing- only pixels that are part of the current connected component . An example of such a labelMask- can be seen below : <p> Figure 7 : Visualizing the current label of the connected-component . Here , we can see the letter- " A " and nothing else . <p> Notice how only the A- character of the license plate is shown and nothing else . <p> Now that we have the mask drawn , we find contours in the labelMask- , so we can apply contour properties to determine if the contoured region is a character candidate or not <p> 154 <p> 155 <p> # ensure at least one contour was found in the mask <p> iflen(cnts)&gt;0 : <p> # grab the largest contour which corresponds to the component in the mask , then <p> # @ @ @ @ @ @ @ @ @ @ cnts , key=cv2.contourArea ) <p> ( boxX , boxY , boxW , boxH ) =cv2.boundingRect(c) <p> # compute the aspect ratio , solidity , and height ratio for the component <p> **28;5762;TOOLONG <p> **44;5792;TOOLONG <p> **36;5838;TOOLONG <p> # determine if the aspect ratio , solidity , and height of the contour pass <p> # the rules tests <p> **34;5876;TOOLONG <p> **29;5912;TOOLONG <p> LONG ... <p> # check to see if the component passes all the tests <p> ifkeepAspectRatio andkeepSolidity andkeepHeight : <p> # compute the convex hull of the contour and draw it on the character <p> # candidates mask <p> hull=cv2.convexHull(c) <p> LONG ... <p> First , a check is made on- Line 133 to ensure that at least- one contour was found in the labelMask- . If so , we grab the largest contour ( according to the area ) and compute its bounding box . <p> Based on the bounding box of the largest contour , we are now ready to compute a few more contour properties . The first is the aspectRatio- , or simply the ratio of the bounding box width to the bounding box @ @ @ @ @ @ @ @ @ @ ( refer to advanced contour properties for more information on solidity ) . Last , well also compute the heightRatio- , or simply the ratio of the bounding box height to the license plate height . Large values of heightRatio- indicate that the height of the ( potential ) character is similar to the license plate itself ( and thus a likely character ) . <p> Youve heard me say it many times inside this course , but I 'll say it again - a clever use of contour properties can often beat out more advanced computer vision techniques . <p> The same is true for this lesson . <p> On Lines 146-148 , we apply tests to determine if our aspect ratio , solidity , and height ratio are within acceptable bounds . We want our aspectRatio- to be at most square , ideally taller rather than wide since most characters are taller than they are wide . We want our solidity- to be reasonably large , otherwise we could be investigating " noise " , such as dirt , bolts , etc. on the license plate . Finally , @ @ @ @ @ @ @ @ @ @ size - license plate characters should span the majority of the height of a license plate , hence we specify a range that will catch all characters present on the license plates . <p> Its important to note that these values were experimentally tuned based on our license plate dataset . For other license plate datasets these values may need to be changed and that 's totally okay . There is no " silver bullet " for ANPR ; each system is geared towards solving a very particular problem . When you go to develop your own ANPR systems , be sure to pay attention to these contour property rules . It may be the case that you need to experiment with them to determine appropriate values . <p> Provided that our aspect ratio , solidity , and height ratio tests pass , we take the contour , compute the convex hull ( to ensure the entire bounding region of the character is included in the contour ) , and draw the convex hull on our charCandidates- mask . Here are a few examples of computing license plate character regions : @ @ @ @ @ @ @ @ @ @ of the license plate ( top ) , thresholding it ( center ) , and computing the convex hull for each character on the license plate . <p> Notice in each case how the character is entirely contained within the convex hull mask this will make it very easy for us to extract each character ROI from the license plate in the next lesson . <p> At this point , were just about done with our **26;5943;TOOLONG method , so let 's finish it up <p> 167 <p> 168 <p> # clear pixels that touch the borders of the character candidates mask and detect <p> # contours in the candidates mask <p> LONG ... <p> # TODO : <p> # There will be times when we detect more than the desired number of characters -- <p> # it would be wise to apply a method to ' prune ' the unwanted characters <p> On- Line 159 , we make a call to clearborder- . - The @ @ @ @ @ @ @ @ @ @ components that are " touching " the borders of the image are removed . This function is very useful in the oft case our contour property tests has a false-positive and accidentally marks a region as a character when it was really part of the license plate border . <p> You 'll also notice that I have placed a TODO- stub on- Lines 161-163 . Despite our best efforts , there will still be cases when our contour property tests are just not enough and we accidentally mark regions of the license plate characters when in reality they are not . Below are a few examples of such a false classification : <p> Figure 9 : Examples of connected-components ( i.e. extraneous parts of the license plate ) that were falsely labeled as characters . <p> So how might we go about getting rid of these regions ? The answer is to apply character pruning where we loop over the character candidates and remove the " outliers " from the group . Well cover the character pruning stage in our next lesson . <p> Remember , each and every stage of @ @ @ @ @ @ @ @ @ @ does not have to be 100% perfect . Instead , it can make a few mistakes and let them pass through well just build in traps and mechanisms to catch these mistakes later on in the pipeline when they are ( ideally ) easier to identify ! <p> Finally , we construct a LicensePlate- object ( Line 167 ) consisting of the license plate , thresholded license plate , and license plate characters and return it to the calling function . <h> Updating the driver script <p> Now that we have updated our LicensePlateDetector- class , let 's also update our recognize.py- driver script , so we can see our results @ @ 42 <p> 43 55203 @qwx675203 <p> **29;5971;TOOLONG <p> **30;6002;TOOLONG **26;6034;TOOLONG <p> fromimutils importpaths 55220 @qwx675220 55218 @qwx675218 55219 @qwx675219 <p> importcv2 <p> # construct the argument parser and parse the arguments 55206 @qwx675206 <p> LONG ... to the images to be classified " ) 55208 @qwx675208 <p> # loop over the images <p> forimagePath LONG ... <p> # load the image <p> **27;6062;TOOLONG <p> print(imagePath) <p> # if the width is greater than 640 pixels , then resize the image <p> ifimage.shape1&gt;640 : <p> **26;6091;TOOLONG , width=640 ) <p> # initialize the license plate detector and detect the license plates and candidates <p> **31;6119;TOOLONG <p> plates=lpd.detect() <p> # loop over the license plate regions <p> for ( i , ( lp , lpBox ) ) inenumerate(plates) : <p> # draw the bounding box surrounding the license plate <p> LONG ... <p> # show the output images <p> **37;6152;TOOLONG <p> **29;6191;TOOLONG <p> LONG ... <p> cv2.imshow ( " Plate &amp; Candidates # " . format(i+1) , output ) <p> # display the output image <p> cv2.imshow ( " Image " , image ) 55212 @qwx675212 <p> cv2.destroyAllWindows() <p> Compared to our @ @ @ @ @ @ @ @ @ @ still detecting the license plates on- Line 27 . And we are still looping over each of the license plate regions on- Line 30 ; however , this time we are looping over a 2-tuple : the LicensePlate- object ( i.e. namedtuple- ) and the lpBox- ( i.e. license plate bounding box ) . <p> In most cases , our character segmentation algorithm worked quite well ; however , there were some cases where our aspect ratio , solidity , and height ratio tests generated false positives , leaving us with falsely detected characters . <p> As well see in our next lesson , these false positives are not as big of a deal as they may seem . Using the geometry of the license plate ( i.e. the license plate characters should appear in a- single row ) , it will actually be fairly easy for us to prune out these false positives and be left with only the- actual license plate characters . <p> Remember , every stage of your computer vision pipeline does not have to be 100% correct every single time . Instead , it may @ @ @ @ @ @ @ @ @ @ stage of your pipeline where you can detect these problem regions and throw them away . <p> For example , consider the- license plate localization- lesson- where we accidentally detected a few regions of an image that werent actually license plates . Instead of agonizing over these false positives and trying to make the morphological operations and contour properties work- perfectly , its instead easier to pass them on to the next stage of our ANPR pipeline where we perform character detection . If we can not detect characters on these license plate candidates , we just throw them out problem solved ! <p> Solving computer vision problems is only partially- based on your knowledge of the subject the other half comes from- your creativity , and your ability to look at a problem differently and arrive at an innovative ( yet simple ) solution . 
@@71485160 @185160/ <h> You can become an OpenCV Ninja in this FREE 10 day crash course . Lessons delivered to your inbox Monday through Friday for 10-days . Sound good ? Enter your email below to start your journey to mastering OpenCV . <h> Here 's what you 'll learn inside my FREE OpenCV crash course ... <h> You can build a kick-ass mobile document scanner in just 5 minutes . <h> You 've probably seen smartphone apps that can scan documents and receipts before . But how do they do it ? I 'll show you . And then you 'll know the 3-step secret for yourself . <p> Did you know that your smartphone can easily be turned into a document scanner ? That 's right ! Just snap a photo of a receipt , a signed contract , or a printout of your latest expense report and your phone can generate a scanned version of the document . <p> Scanning a document using your smartphone can be broken down into only 3 simple steps . To learn what these three steps are ( and implement a mobile document @ @ @ @ @ @ @ @ @ @ my free OpenCV crash course . <h> Discover how to detect skin in images . <p> Imagine this . You are working for a dating website with an untarnished reputation . Investors just smacked millions of dollars into your hands to take you company to new heights . But then , a group of users start changing their profile pictures to inappropriate images ! How do you stop this outbreak before it reaches critical levels and the investors leave ? <p> I 'll let you in a little secret . You can use skin detection to save the day ! In this lesson , I 'll show you a simple but easy method to detect skin in images . <h> Teach your computer how to play a game of Where 's Waldo ? <h> Where 's Waldo ? is the ultimate game of hide and seek for the human eye . He 's actually " hiding " in plain sight but due to all the noise and distraction , we ca n't pick him out immediately ! But by using OpenCV we can automatically find him . <p> At the @ @ @ @ @ @ @ @ @ @ wears glasses . A hat . And his classic white and red horizontally striped shirt . It might take us a little bit of time to scan up and down and left to right across the page , but our brain is able to pick out this pattern , even amongst all the distraction . <p> The question is , can computers do better ? Can we create a program to automatically find Waldo ? <p> In fact , we can ! To learn how to teach your computer to play a game of Where 's Waldo ? , be sure to sign up for my free crash course on OpenCV . <h> Master dead simple object tracking in video . <h> One of the most requested lessons of all time on this blog has been " How do I track objects in video ? " If you are interested in tracking objects in video , then this lesson was written specifically for you . <p> Did you know that OpenCV has a built-in function that makes object tracking painless and easy ? <p> So what is this awesome object @ @ @ @ @ @ @ @ @ @ your own projects ? <p> In this lesson I 'll show you how to unlock the object tracking secrets of OpenCV . By the time you 're finished this lesson , you 'll be an OpenCV Ninja . <h> Are you on the deep learning bandwagon ? If so , find out why you might want to hop off . <h> Did you know that computer vision and machine learning follow trends ? From Perceptrons , to Support Vector Machines , to Ensembles , each decade has a new trend . So is deep learning here to stay ? Or will we be studying another algorithm in 5 years ? <p> This is by far the most controversial lesson in the OpenCV crash course . And in reality , it does n't have a lot to do with OpenCV . But if you are studying computer vision , you need to understand why deep learning is all the rage right now . <p> In this lesson you 'll learn why computer vision researchers are so interested in deep learning ... and why you should n't treat every problem like a @ @ @ @ @ @ @ @ @ @ how to build your very own image search engine . <h> Interested in building your very own image search engine ? I 've got you covered . In this lesson you 'll learn the 4-steps required to build any image search engine . <p> How many images do you have on your smartphone right now ? What about on your Facebook ? Or your Instagram ? Or Twitter ? Your life is represented by thousands and thousands of digital images . <p> Now , what if you could simply click on a photo of your last trip to the beach ... and have all other beach photos displayed to your screen ? No , it 's not science fiction . And it 's not magic . I 'll show you how it 's done . <p> In this OpenCV crash course lesson , I 'll show you the 4-steps required to build your own personal image search engine . <h> Hey , Adrian Rosebrock here . <p> But I 'm also an entrepreneur and Ph.D who has spent over 8 years studying computer vision and machine learning . I 've @ @ @ @ @ @ @ @ @ @ vision applications . <p> And I could n't have done it without the OpenCV library . <p> You see , OpenCV is the de facto standard for computer vision and image processing . If you intend on build computer vision applications , you should definitely be using the OpenCV library . <p> And by using the Python programming language you can learn how to build really amazing computer vision applications in no time . <p> So I invite you to join this free OpenCV + Python crash course . Come learn from me I 've had a lifetime of experience in the computer vision world and I 'm excited to share with you the tips , tricks , and hacks that I 've learned along the way . 
@@71485161 @185161/ <h> How to find functions by name in OpenCV <p> OpenCV can be a big , hard to navigate library , especially if you are just getting started learning computer vision and image processing . <p> The release of OpenCV 3- has only further complicated matters , moving a few- important- functions around and even slightly altering their names ( the cv2.cv.BoxPoints- vs. cv2.boxPoints- methods come to mind off the top of my head ) . <p> While a good IDE can help you search and find a particular function based on only a few keystrokes , sometimes you wont have access to your IDE . And if you 're trying to develop code that is compatible with- both OpenCV 2.4 and OpenCV 3 , then you 'll need to- programmatically- determine if a given function is available ( whether via version detection or function listing ) . <p> Enter the findfunction- method , now part of the imutils- library , that can help you search and lookup OpenCV methods simply by providing a query string . <p> In the rest of this blog post I 'll show you how to- quickly @ @ @ @ @ @ @ @ @ @ using only simple Python methods . <h> Dumping all OpenCV function names and attributes <p> A quick way to view- all OpenCV functions and attributes exposed to the Python bindings is to use the built-in Python- dir- function , which is used to return a list of names in the current local scope . <p> Assuming you have OpenCV installed and a Python shell ready , we can use the dir- method to create a list of all OpenCV methods and attributes available to us : <p> How to <p> 30 <p> 31 <p> &gt;&gt;&gt;importcv2 <p> **26;6222;TOOLONG <p> &gt;&gt;&gt;forfinfuncs : <p> ... print(f) <p> ... <p> ACCESSFAST <p> ACCESSMASK <p> ACCESSREAD <p> ACCESSRW <p> ACCESSWRITE <p> ADAPTIVETHRESHGAUSSIANC <p> ADAPTIVETHRESHMEANC <p> **27;6250;TOOLONG <p> **29;6279;TOOLONG <p> @ @ @ @ @ @ @ @ @ @ **26;6440;TOOLONG <p> AKAZEDESCRIPTORMLDB <p> **26;6468;TOOLONG <p> AKAZEcreate <p> ... <p> waitKey <p> warpAffine <p> warpPerspective <p> watershed <p> xfeatures2d <p> ximgproc <p> xphoto <p> While this method does indeed give us the list of attributes and functions inside OpenCV , it requires a- manual scan or a grep of the list to find a particular function . <p> Personally , I like to use this- raw list of method names if I have a rough idea of what you 're looking for ( kind of like a- " I 'll know it when I see it " type of situation ) ; otherwise , I look to use the findfunction- method of imutils- to quickly narrow down the search space similar to greping the output of dir(cv2)- . <p> We define our findfunction- method on- Line 6 . This method requires a single required argument , the ( partial ) name- of the function we want to search cv2- for . Well also accept two optional arguments : prettyprint- which is a boolean indicating whether the results should be returned as a list or neatly formatted to our console ; and module- @ @ @ @ @ @ @ @ @ @ . <p> Well initialize module- to be cv2- , the root-module , but we could also pass in a sub-module such as xfeatures2d- . In either case , the module- will be searched for partial function/attribute matches to name- . <p> The actual search takes place on- Lines 13 and 14 where we apply a regular expression to determine if any attribute/function name inside of module- contains the supplied name- . <p> Lines 18 and 19 make a check to see if we should return the list of filtered- functions to the calling function ; otherwise , we loop over the function names and print them to our console ( Lines 22 and 23 ) . <p> Finally , - Line 26- takes our findfunction- method for a test drive by searching for functions containing the blur- in their name . <p> To see our findfunction- method in action , just open a terminal and execute the following command : <p> How to find functions by name in OpenCV <p> 1 <p> 2 <p> 3 <p> 4 <p> $python findfunction.py <p> 1 . GaussianBlur <p> 2. blur <p> 3. medianBlur @ @ @ @ @ @ @ @ @ @ three functions inside of OpenCV that contain the text blur- , including cv2.GaussianBlur- , cv2.blur- , and cv2.medianBlur- . <h> A real-world example of finding OpenCV functions by name <p> As I already mentioned earlier in this post , the findfunctions- method is already part of the imutils library . You can install imutils- via pip- : <p> How to find functions by name in OpenCV <p> Shell <p> 1 55204 @qwx675204 <p> If you already have imutils- installed on your system , be sure to upgrade it to the latest version : <p> Figure 1 : Our goal is to find the original book in the image- ( left ) and then draw the outline on the book ( right ) . <p> Open up a new file , name it findbook.py- , and let 's get coding : <p> How to 13 <p> 14 <p> 15 55203 @qwx675203 55220 @qwx675220 <p> importcv2 <p> # load the @ @ @ @ @ @ @ @ @ @ ) <p> orig=image.copy() <p> # convert the image to grayscale , threshold it , and then perform a <p> # series of erosions and dilations to remove small blobs from the <p> # image 55215 @qwx675215 <p> LONG ... <p> thresh=cv2.erode ( thresh , None , iterations=2 ) <p> thresh=cv2.dilate ( thresh , None , iterations=2 ) <p> We start off by loading our image from disk on- Line 6 . We then do some basic image processing on- Lines 12-15 , including conversion to grayscale , thresholding , and a series of erosions and dilations to remove any small blobs from the thresholded image . Our output thresholded image looks like this : <p> Figure 3 : The thresholded , binary representation of the book image . <p> However , in order to- draw the contour- surrounding the book , I first need to- find the outline of the book itself . <p> Let 's pretend that I 'm stuck and I do n't  know what the name of the function is that finds the outline of an object in an image but I do recall that " outlines " are @ @ @ @ @ @ @ @ @ @ up a shell and using the findfunction- in imutils- , I quickly ascertain that that I am looking for the cv2.findContours- function : <p> How to find functions by name in OpenCV <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> 6 <p> 7 <p> $python <p> &gt;&gt;&gt;import imutils <p> **33;6496;TOOLONG " contour " ) <p> 1. contourArea <p> 2. drawContours <p> 3. findContours <p> 4. isContourConvex <p> Now that I know I am using the cv2.findContours- method , I need to figure out what contour extraction flag should be used for the function . I only want to return- external contours ( i.e the outer-most outlines ) so Ill need to look up that attribute as well : <p> How to find functions by name in OpenCV <p> Python <p> 1 <p> 2 <p> **33;6531;TOOLONG " external " ) <p> 1 . RETREXTERNAL <p> Got it . I need to use the cv2.RETREXTERNAL- flag . Now that I have that settled , I can finish up my Python script : <p> How to find functions by <p> 27 <p> LONG ... 55211 @qwx675211 <p> c=max ( cnts , key=cv2.contourArea ) <p> cv2.drawContours ( image , c , -1 , ( 0,255,255 ) , 3 ) <p> # show the output image <p> **26;6566;TOOLONG <p> LONG ... 55212 @qwx675212 <p> Lines 19 and 20 makes a call to cv2.findContours- to find the external- outlines of the objects ( thanks to the cv2.RETREXTERNAL- attribute ) in the thresholded image . <p> Well then take the largest contour found ( which is presumed to be the outline of the book ) and draw the outline on our image ( Lines 21 and 22 ) . <p> Finally , - Lines 25-27- show our output images . <p> To see my script in action , I just fire up a terminal and issue the following command : <p> How to find functions by name in OpenCV <p> Shell <p> 1 <p> $python findbook.py <p> Figure 3 : - Our original input image ( left ) , the thresholded , binary representation of the image ( center ) , and the contour drawn surrounding the book ( right ) @ @ @ @ @ @ @ @ @ @ and draw the outline of the book without a problem ! <h> Summary <p> In this blog post we learned how to get the names of all functions and attributes in OpenCV that are exposed to the Python bindings . <p> We then built a Python function to programmatically search these function/attribute names via a text query . This function has been included in the imutils package . <p> Finally , we explored how OpenCV function filtering can be used in your every-day workflow to increase productivity and facilitate quick function lookup . We demonstrated this by building a small Python script detect the presence of a book in an image . <h> Downloads : 55217 @qwx675217 <p> I 'm a big fan of ipython , thanks for the reminder ! While ipython does have tab completion , one of the benefits of using this method is that you can search for any piece of text within the function name , not just the auto-complete from where the cursor currently is . <p> Hi ! First of all congratulation for your work its amazing ! = I 'm trying to begin in @ @ @ @ @ @ @ @ @ @ , thanks to you i had install OpenCV3 and Numpy/Scipy/matplotlib on my raspberry pi 3 <p> And now Ive installed imutils it is working but when i run your findbook.py programi I have this error and @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485162 @185162/ <p> Or we might even use deep learning-based algorithms for face localization . <p> In either case , the actual algorithm used to detect the face in the image does n't  matter . Instead , what 's important is that through some method we obtain the face bounding box ( i.e. , the- ( x , y ) -coordinates of the face in the image ) . <p> Given the face region we can then apply- Step #2 : detecting key facial structures in the face region . <p> There are a variety of facial landmark detectors , but all methods essentially try to localize and label the following facial regions : <p> These annotations are part of the 68 point iBUG 300-W dataset which the dlib facial landmark predictor was trained on . <p> Its important to note that other flavors of facial landmark detectors exist , including the 194 point model that can be trained on the HELEN dataset . <p> Regardless of which dataset is used , the same dlib framework can be leveraged to train a shape predictor on the input training data this is useful @ @ @ @ @ @ @ @ @ @ custom shape predictors of your own . <p> In the remaining of this blog post I 'll demonstrate how to detect these facial landmarks in images . <p> Future blog- posts in this series will use these facial landmarks to extract- specific regions of the face , apply face alignment , and even build a blink detection system . <h> Detecting facial landmarks with dlib , OpenCV , and Python <p> Well be reviewing two of these functions inside faceutils.py- now and the remaining ones next week . <p> The first utility function is recttobb- , short for " rectangle to bounding box " : <p> Facial landmarks with dlib , OpenCV <p> 27 <p> 28 <p> defrecttobb(rect) : <p> # take a bounding predicted by dlib and convert it <p> # to the format ( x , y , w , h ) as we would normally do <p> # with OpenCV <p> x=rect.left() <p> y=rect.top() <p> w=rect.right()-x <p> h=rect.bottom()-y <p> # return a tuple of ( @ @ @ @ @ @ @ @ @ @ ( x , y , w , h ) <p> This function accepts a single argument , rect- , which is assumed to be a bounding box rectangle produced by a dlib detector ( i.e. , the face detector ) . <p> The rect- object includes the- ( x , y ) -coordinates of the detection . <p> However , in OpenCV , we normally think of a bounding box in terms of- " ( x , y , width , height ) " so as a matter of convenience , the recttobb- function takes this rect- object and transforms it into a 4-tuple of coordinates . <p> Again , this is simply a matter of conveinence and taste . <p> Secondly , we have the shapetonp- function : <p> Facial landmarks with dlib , OpenCV <p> 39 <p> 40 <p> defshapetonp ( shape , dtype= " int " ) : <p> # initialize the list of ( x , y ) -coordinates <p> coords=np.zeros ( ( @ @ @ @ @ @ @ @ @ @ 68 facial landmarks and convert them <p> # to a 2-tuple of ( x , y ) -coordinates <p> foriinrange(0,68) : <p> coordsi= ( shape.part(i).x , shape.part(i).y ) <p> # return the list of ( x , y ) -coordinates <p> returncoords <p> The dlib face landmark detector will return a shape- object containing the 68- ( x , y ) -coordinates of the facial landmark regions . <p> Using the shapetonp- function , we cam convert this object to a NumPy array , allowing it to " play nicer " with our Python code . <p> Given these two helper functions , we are now ready to detect facial landmarks in images . <p> Open up a new file , name it faciallandmarks.py- , and insert the following code : <p> Facial landmarks with dlib 13 <p> 14 <p> 15 55203 @qwx675203 <p> fromimutils importfaceutils 55220 @qwx675220 55218 @qwx675218 55219 @qwx675219 <p> importdlib <p> importcv2 <p> # @ @ @ @ @ @ @ @ @ @ <p> LONG ... <p> help= " path to facial landmark predictor " ) <p> ap.addargument ( " -i " , " --image " , required=True , <p> help= " path to input image " ) 55208 @qwx675208 <p> Lines 2-7 import our required Python packages . <p> Well be using the faceutils- submodule of imutils- to access our helper functions detailed above . <p> --shape-predictor- : This is the path to dlibs pre-trained facial landmark detector . You can download the detector model here- or you can use the- " Downloads " section of this post to grab the code + example images + pre-trained detector as well . <p> --image- : The path to the input image that we want to detect facial landmarks on . <p> Line 20 then loads the facial landmark predictor using the path to the supplied --shape-predictor- . <p> But before we can actually detect facial landmarks , we first need to detect the face in our input image : <p> Facial landmarks with dlib , OpenCV , and Python <p> Python <p> 22 <p> 23 <p> 24 <p> 25 <p> 26 <p> 27 @ @ @ @ @ @ @ @ @ @ it , and convert it to grayscale <p> image=cv2.imread ( args " image " ) <p> **26;6594;TOOLONG , width=500 ) 55215 @qwx675215 <p> # detect faces in the grayscale image <p> rects=detector ( gray , 1 ) <p> Line 23 loads our input image from disk via OpenCV , then pre-processes the image by resizing to have a width of 500 pixels and converting it to grayscale ( Lines 24 and 25 ) . <p> Line 28 handles detecting the bounding box of faces in our image . <p> The first parameter to the detector- is our grayscale image ( although this method can work with color images as well ) . <p> The second parameter is the number of image pyramid layers to apply when upscaling the image prior to applying the detector ( this it the equivalent of computing cv2.pyrUp- N number of times on the image ) . <p> The benefit of increasing the resolution of the input image prior to face detection is that it may allow us to detect- more faces in the image the downside is that the larger the input image , the @ @ @ @ @ @ @ @ @ @ the- ( x , y ) -coordinates of the faces in the image , we can now apply facial landmark detection to each of the face regions : <p> Facial landmarks with dlib , OpenCV <p> 53 <p> 54 <p> # loop over the face detections <p> for ( i , rect ) inenumerate(rects) : <p> # determine the facial landmarks for the face region , then <p> # convert the facial landmark ( x , y ) -coordinates to a NumPy <p> # array <p> shape=predictor ( gray , rect ) <p> **32;6622;TOOLONG <p> # convert dlib 's rectangle to a OpenCV-style bounding box <p> # i.e. , ( x , y , w , h ) , then draw the face bounding box <p> ( x , y , w , h ) **25;6656;TOOLONG <p> LONG ... @ @ @ @ @ @ @ @ @ @ , " Face # " . format(i+1) , ( x-10 , y-10 ) , <p> **26;6683;TOOLONG , ( 0,255,0 ) , 2 ) <p> # loop over the ( x , y ) -coordinates for the facial landmarks <p> # and draw them on the image <p> for ( x , y ) inshape : <p> cv2.circle ( image , ( x , y ) , 1 , ( 0,0,255 ) , -1 ) <p> # show the output image with the face detections + facial landmarks <p> cv2.imshow ( " Output " , image ) 55212 @qwx675212 <p> We start looping over each of the face detections on- Line 31 . <p> For each of the face detections , we apply facial landmark detection on- Line 35 , giving us the 68- ( x , y ) -coordinates that map to the specific facial features in the image . <p> Line 36 then converts the dlib shape- object to a NumPy array with shape- ( 68 , 2 ) . <p> Lines 40 and 41 draw the bounding box surrounding the detected face on the image- while- Lines 44 @ @ @ @ @ @ @ @ @ @ Finally , - Lines 49 and 50 loop over the detected facial landmarks and draw each of them individually . <p> Lines 53 and 54 simply display the output image- to our screen . <h> Facial landmark visualizations <p> Before we test our facial landmark detector , make sure you have upgraded to the latest version of imutils- which includes the faceutils.py- file : <p> Facial landmarks with dlib , OpenCV , and Python <p> Shell <p> 1 <p> $pip install--upgrade imutils <p> Note : If you are using Python virtual environments , make sure you upgrade the imutils- inside the virtual environment . <p> From there , use the- " Downloads " section of this guide to download the source code , example images , and pre-trained dlib facial landmark detector . <h> Summary <p> In todays blog post we learned what facial landmarks are and how to detect them using dlib , OpenCV , and- Python . <p> Detecting facial landmarks in an image is a two step process : <p> First we must localize a face(s) in an image . This can be accomplished using a number @ @ @ @ @ @ @ @ @ @ or HOG + Linear SVM detectors ( but any approach that produces a bounding box around the face will suffice ) . <p> Apply the shape predictor , specifically a facial landmark detector , to obtain the- ( x , y ) -coordinates of the face regions in the face ROI . <p> Given these facial landmarks we can apply a number of computer vision techniques , including : <p> Face part extraction ( i.e. , nose , eyes , mouth , jawline , etc . ) <p> Facial alignment <p> Head pose estimation <p> Face swapping <p> Blink detection <p> and much more ! <p> In next weeks blog post I 'll be demonstrating how to access each of the face parts- individually and extract the eyes , eyebrows , nose , mouth , and jawline features simply by using a bit of NumPy array slicing magic . <p> To be notified when this next blog post goes live , be sure to enter your email address in the form below ! <h> Downloads : 55217 @qwx675217 <p> Nice job Thimira . The method I will demonstrate in next weeks @ @ @ @ @ @ @ @ @ @ of imutils for convenience . Ill also be demonstrating how to draw semi-transparent overlays for each region of the face . <p> I tried working for side faces but its not working , can you please guide what can be the possibilities for side face landmark detection and yes i was also trying working on your example02.jpg there imutils.resize() method was giving some error . Attribute error : NoneType object has no attribute shape . <p> I 've tried my best to play with the parameters of trainshapepredictor in dlib but the result is never as close as **33;6711;TOOLONG data that Ive used is custom data of face similar to that of ibug . <p> Hi Adrean , I 'm following your post about Facial landmarks with dlib , OpenCV , and Python . Its really amazing . thank you a lot for such helpful and util code . So , now i 'm tring to save all those landmarks in a file ! So that Im ask you how cando such thing ? Thank you <p> hey adrian , i am trying to achieve real-time face recognition usind dlib. but when i @ @ @ @ @ @ @ @ @ @ shape ) it gives an error that the arguments are not based on c++.please give me a solution sir <p> Hi Adrian , Thanks a lot for your explanation . It is very useful . However I 'm newer in python and I 'm trying to save those face landmarks in matrix so I can manipulate them instead of the original image . Would you give me some suggestion . How can I do such thing ? Thank you Adrian <p> Once you have the shape representing the facial landmarks you can save them to disk via JSON , pickle , HDF5 , etc . I would recommend pickle if you are new to Python . I would also suggest working through Practical Python and OpenCV so you can learn the fundamentals of computer vision and OpenCV . <p> Hello , i see you used dlib face/object detector for finding face on image transfer it from dlib.rectangle object to bouding values like your " recttobb " funcition do and then with cv2 draw rectangle , but my problem is i need to use my own haar cascade for finding faces/objects and correct @ @ @ @ @ @ @ @ @ @ opposite " bbtorect " because landmark detector require this rectangle , but i cant find out which is it datatype and how to reproduce this object , " ( 386 , 486 ) ( 461 , 561 ) " this sample of the found face its seems like 2 tuples but it does n't  , i cant event find out that while i was examining dlib c++ code , I spent on this problem more than 4 hours and with no result , is there any solution or it is approaching to be impossible ? <p> Hi , I am student of you rather than saying a huge fan . I have a small doubt we are getting coordinates , what are these coordinates represents . If i want only a specific points in landmarks how can i get them like Nose tip Left-eye left corner Right-eye right corner Corner of lips These points required to estimate the pose of head , Please help me this <p> Its hard to say what the exact error is without having physical access to your machine , but I think the issue is @ @ @ @ @ @ @ @ @ @ different Python version than you are importing it into . Are you using Python virtual environments ? <p> It sounds like you might have compiled dlib against a different version of Python than you are importing it into . Try uninstalling and re-installing dlib , taking care to ensure the same Python version is being used . <p> Fantastic stuff . Thanks for all you 've done ! I am doing face detection / recognition on IR images . This means I can not use the standard features for detection or recognition . I am trying to build my own detctor using the dlib " trainobjectdetector.py " and it is working really well mostly . I have a training set that are faces of one size and the detector is getting faces of similar sizes but completely missing smaller face images . <p> So my question is how does the system work to detect faces of different sizes . Do you need to have training samples of all the sizes that you expect to be finding ? Or does the system take in the training images and resize them ? <p> @ @ @ @ @ @ @ @ @ @ kind of training set I need and how it works to find faces of different sizes , I would really appreciate it . I have the recognizer working well , I just need to find the faces . <p> HOG-based object detectors are able to detect objects at various scales ( provided the objects across the dataset have the same aspect ratio ) using image pyramids . In short , you do n't  need to worry about the size of your faces as they will be resized to a constant scale during training and then image pyramids applied when detecting on your testing images . <p> I am developing a code for facial recognition but I am having difficulties in the matter of precision . To get faster I made 37 proportions based on 12 points . But the variation of the face angle also varies the result . I 'm limiting the angle 5 degrees to each side horizontally . I will now try to record the proportions for each angle ie a series of frames for the same face . Thus the comparison would be between several faces at @ @ @ @ @ @ @ @ @ @ which i can get more than 68 points of facial landmark . I see some paper mentioning 83 points or more . Is there any library that can help me find some points on forehead ? I am trying to find golden ratio for a face to score it . Thanks ! <p> You would need to train your own custom shape predictor from scratch using dlib . The pre-trained facial landmark predictor provided by dlib only returns 68 points . The Helen dataset comes to mind as a good starting point . <p> Hi adrianIm having problem with the argument constructor , after giving the path to predictor and image when this line " args = vars ( ap.parseargs ( ) ) " is executed in ipython console it is giving this error " In 46 : args = vars ( ap.parseargs ( ) ) usage : ipython -h -p SHAPEPREDICTOR -i IMAGE ipython : error : argument -p/shape-predictor is required An exception has occurred , use %tb to see the full traceback. " please help me with this problem <p> Hello , how are you ? Ive been @ @ @ @ @ @ @ @ @ @ with python . Even more so when we made the post for Dlib on Raspberry . I was sad because you did not answer me and that you answered all the questions about your post . I know I asked a very specific question but could answer me by email if that is the case . Even if it is a negative answer . <p> I do biometrics with haarcascade but I 'm trying with landmarks . <p> I am developing a code for facial recognition but I am having difficulties in the matter of precision . To get faster I made 37 proportions based on 12 points . But the variation of the face angle also varies . I 'm limiting the angle 5 degrees to each side horizontally . I will now try to record the proportions for each angle ie a series of frames for the same face . Thus the comparison would be between several faces at the same angle . <p> Hi Moises , I do my best to answer as many PyImageSearch comments as I possibly can , but please keep in mind that the PyImageSearch @ @ @ @ @ @ @ @ @ @ comments per day . Again , I do the best I can , but I cant always respond to them all . <p> That said , regarding your specific project ( if I understand it correctly ) , you are performing face recognition but want to use facial landmarks to aid you in the aligning process to obtain better accuracy ? Do I understand your question correctly ? If so , why not just use the tutorial I provided on face alignment . <p> Keep in mind that the most facial landmarks and face recognition algorithms assume a frontal facing view of the person . Yes , facial landmarks can tolerate a small degree of rotation in the face ( as it turns from side to side ) , but there becomes a point where the face is too angled to apply these techniques . I 'm still not sure I understand your question entirely , but I think this might be the situation you are running into . <p> Thanks for the wonderful tutorial . However if I wish to get a roi composed of eye lid , eye ball @ @ @ @ @ @ @ @ @ @ in the whole eye including continuous portion from eye lids to eye brows to eye is to be cropped , how do I do the same with facial landmarks code above <p> Great tutorial . Thank a lot ! However , I have one confounding problem : When running the code at : rects = detector ( gray , 1 ) I get the following error : <p> TypeError : No registered converter was able to produce a C++ rvalue of type char from this Python object of type str I investigated the error , upgraded libboost-dev-all ( using Ubuntu 17.04 ) , still no resolution <p> What confounds me is ; in a separate work for face recognition , I imported dlib initialized detector and set rects = detector ( img , 1 ) etc It works fine . Redid this exercise on python console ( line by line ) error did not show up Ran the program and the error turns up . No spelling mistakes Any pointers , anything will help Thanks for your time <p> Hello Sir ! Thank you very much . All your explanations @ @ @ @ @ @ @ @ @ @ studying at the national school of engineers of Sousse , coming from Tunisia . I use facial markers to recognize emotionsfrom the image . I try to create a template ( model ) from all the landmarks I extracted from the images in the CK + dataset . I advocate using this model to qualify the emotion of the face by using it . So I wonder if you could help me and guide me how to save the facial landmarks in a training model and how could I predict this pattern to detect facial emotion from benchmarks . Thank you for your help Sincerly <p> Is there a particular reason why you are using facial landmarks to predict emotions ? As I show in Deep Learning for Computer Vision with Python , CNNs can give higher accuracy for emotion recognition and are much easier @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485163 @185163/ <h> Segmentation : A SLIC Superpixel Tutorial using Python <p> Have you ever had- a horrible case of eyelid twitching ? One where your eyelid just wont stop spazzing out , no matter what you do ? <p> That 's currently what 's going on with me and its been going on- for over two weeks . As you can imagine , I 'm not exactly happy . <p> However , to combat this persistent , often distracting nuisance , I 've started sleeping more regularly , drinking substantially less caffeine ( I was only drinking two cups of coffee a day to begin with ) , and consuming next to no alcohol . Which is a real challenge when you enjoy a frothy beer as much as I do . <p> OpenCV and Python versions : This example will run on- Python 2.7/Python 3.4+ and OpenCV 2.4 . X/OpenCV 3.0+ . <h> So , What 's a Superpixel ? <p> Take a second , clear your mind , and consider how we represent images . <p> Images are represented as a grid of pixels , in either single or multiple channels . <p> @ @ @ @ @ @ @ @ @ @ apply algorithms to them , such as face detection and recognition , template matching , and even deep learning applied directly to the raw pixel intensities . <p> The problem is that our pixel grid is by no means- a natural representation of an image . <p> For example , consider the image below : <p> Figure 1 : If I were to take the single pixel pointed to by the red arrow ( left ) and show it to you on the- right at an obviously zoomed-in scale , would you be able to tell me anything about the image based solely on that pixel ? Anything descriptive ? Or of semantic value ? <p> If I were to take a single pixel from the image on the left- ( highlighted by the red arrow ) and then showed it to you on the right , could you reasonably tell me that ( 1 ) that pixel came from a Velociraptor and ( 2 ) that single pixel actually holds some sort of semantic meaning ? <p> Unless you really , really- want to play devils advocate , the @ @ @ @ @ @ @ @ @ @ alone by itself , is not a natural representation of an image . <p> Taken as a whole , this grouping of pixels as a pixel grid is simply- an artifact of an image part of the process of capturing and creating digital images . <p> That leads us to superpixels . <p> Intuitively , it would make more sense to explore not only perceptual , but semantic meanings of an image formed by locally grouping pixels as well . <p> When we perform this type of local grouping of pixels on our pixel grid , we arrive at superpixels . <p> These superpixels carry more perceptual and semantic meaning than their simple pixel grid counterparts . <h> Benefits of Superpixels <p> To summarize the work of Dr. Xiaofeng Ren , we can see that a mapping from pixel grids to superpixels would ( ideally ) hold the desirable properties of : <p> Computational efficiency : While it may be ( somewhat ) computationally expensive to compute the actual superpixel groupings , it allows us to reduce the complexity of the images themselves from hundreds of thousands of pixels to @ @ @ @ @ @ @ @ @ @ will then contain some sort of perceptual , and ideally , semantic value . <p> Perceptual meaningfulness : Instead of only examining a single pixel in a pixel grid , which caries very little perceptual meaning , pixels that belong to a superpixel group share some sort of commonality , such as similar color or texture distribution . <p> Oversegmentation : Most superpixel algorithms oversegment- the image . This means that most of important boundaries in the image are found ; however , at the expense of generating many insignificant boundaries . While this may sound like a problem or a deterrent- to using super pixels , its actually a positive the end product of this oversegmentation is that that very little ( or no ) pixels are lost from the pixel grid to superpixel mapping . <p> Graphs over superpixels : Dr. Ren refers to this concept as " representationally efficient " . To make this more concrete , imagine constructing a graph over a 50,000 x 50,000- pixel grid , where each pixel represents a node in the graph that leads to very large representation . However , @ @ @ @ @ @ @ @ @ @ , leaving us with a ( arbitrary ) 200 superpixels . In this representation , constructing a graph over the 200 superpixels is substantially more efficient . <h> Example : Simple Linear Iterative Clustering ( SLIC ) <p> As always , a PyImageSearch blog post would n't be complete without an example and some code . <p> Ready ? <p> Open up your favorite editor , create slic.py- , and let 's get coding : <p> SLIC Superpixel 28 <p> 29 <p> 30 55203 @qwx675203 <p> fromskimage.segmentation importslic <p> fromskimage.segmentation importmarkboundaries <p> fromskimage.util importimgasfloat <p> fromskimage importio <p> importmatplotlib.pyplot asplt 55218 @qwx675218 <p> # construct the argument parser and parse the arguments 55206 @qwx675206 <p> LONG ... to the image " ) 55208 @qwx675208 <p> # load the image and @ @ @ @ @ @ @ @ @ @ ( args " image " ) ) <p> # loop over the number of segments <p> fornumSegments in(100,200,300) : <p> # apply SLIC and extract ( approximately ) the supplied number <p> # of segments <p> LONG ... <p> # show the output of SLIC <p> fig=plt.figure ( " Superpixels -- %d segments " % ( numSegments ) ) <p> ax=fig.addsubplot(1,1,1) <p> ax.imshow ( markboundaries ( image , segments ) ) <p> plt.axis ( " off " ) <p> # show the plots <p> plt.show() <p> On Lines 2-7 we import the packages well be using for this example . To perform the SLIC superpixel segmentation , we will be using the sckit-image implementation , which we import on Line 2 . To draw the actual superpixel segmentations , scikit-image provides us with a markboundaries- - function which we import on Line 3 . <p> From there , we import a utility function , imgasfloat- - on Line 4 , which as the name suggests , converts an image from an unsigned 8-bit integer , to a floating point data with , with all pixel values called to the range @ @ @ @ @ @ @ @ @ @ - sub-package of scikit-image which is used for loading and saving images . <p> Well also make use of matplotlib to plot our results and argparse- - to parse our command line arguments . <p> Lines 10-12 handle parsing our command line arguments . We need only a single switch , --image- , which is the path to where our image resides on disk . <p> We then load this image and convert it from an unsigned 8-bit integer to a floating point data type on Line 15 . <p> Now the interesting stuff happens . <p> We start looping over our number of superpixel segments on Line 18 . In this case , well be examining three increasing sizes of segments : 100 , 200 , and 300 , respectively . <p> We perform the SLIC superpixel segmentation on Line 21 . The slic- - function takes only a single required- parameter , which is the image we want to perform superpixel segmentation on . <p> However , the slic- - function also provides many optional parameters , which I 'll only cover a sample of here . <p> The @ @ @ @ @ @ @ @ @ @ how many superpixel segments we want to generate . This value defaults to 100 segments . <p> We then supply sigma- , which is the smoothing Gaussian kernel applied prior to segmentation . <p> Other optional parameters can be utilized in the function , such as maxiter- , which the maximum number of iterations for k-means , compactness- , which balances the color-space proximity with image space-proximity , and convert2lab- - which determines whether the input image should be converted to the L*a*b* color space prior to forming superpixels- ( in nearly all- cases , having convert2lab- - set to True- - is a good idea ) . <p> Now that we have our segments , we display them using matplotlib in Lines 24-27 . <p> In order to draw the segmentations , we make use of the - markboundaries- - function which simply takes our original image and overlays our superpixel segments . <p> In this image , we have found ( approximately ) 100 superpixel segmentations . Notice how locally similar regions of the image , such as the scales of the Velociraptor- and the shrubbery are grouped @ @ @ @ @ @ @ @ @ @ of segments increases , the segments also become more rectangular and grid like . This is not a coincidence , and it can be further controlled by the optional compactness- - parameter of slic- . <h> Summary <p> In this blog post I explained what superpixel segmentation is and how it has many benefits in the computer vision world . <p> For example , working with superpixels instead of the standard pixel grid space yields us computational efficiency , perceptual meaningfulness , oversegmentation , and efficient graph representations across regions of the image . <p> Finally , I showed you how to utilize the Simple Linear Iterative Clustering ( SLIC ) algorithm to apply superpixel segmentation to your own images . <p> This certainly wont be the last time we discuss superpixels on this blog , so look forward to more posts in the future ! <h> Downloads : 55217 @qwx675217 <p> Hello there ! Very helpful tutorial . I 'm just running into a small issue when I copy your example <p> At ax.imshow ( markboundaries ( image , segments ) ) , I 'm getting that the array is not @ @ @ @ @ @ @ @ @ @ to be a 10x4x512 array , whereas my original image is a 10x4x512x512 array . Is there something I 'm missing when finding segments with slic ? <p> The code itself does not have to change . You need to open a terminal , navigate to the code , and execute the following command ( like I do in the blog post ) : <p> $ python superpixel.py --image raptors.png <p> Notice how the --image switch is supplied via command line and then passed into the cv2.imread method . I would suggest reading up on command line arguments prior to make sure you have a good grasp on them . <p> You can define the number of superpixels that can be extracted which in turn influences the size of the superpixel ( i.e. , the larger the superpixels are , the smaller their area will be ) . But you can not define a minimum size directly with SLIC . <p> OpenCVs SLIC implementation is still in development ( hence being in the " contrib " repo ) . I 'm also not sure if there are Python bindings for it ? @ @ @ @ @ @ @ @ @ @ well documented . <p> I just downloaded your code and tried to run it on anaconda 3.5 . But just could not , do I have to make any changes to the code as I am trying to put the path in the " help=Path to image " line 11 of your code . But it is giving error . <p> Hey Adrian , Will this code help in Segmenting and accessing only the nucleus part of tissue image ? I tried using active contours but applying the texture feature and checking exactly which texture dimension is best for isolating nucleus is tough . So will the superpixel help in isolating only the nucleus of the cells in tissue image ? .. if yes , please mail me some directions . Thanks Adrian . <p> Awesome tutorial ! Helped me a lot ! BTW , right before the code example you wrote " Open up your favorite editor , create slic.py , and let 's get coding : " and later in running explanation ( and in the zip file ) the filename was " superpixel.py " . So you @ @ @ @ @ @ @ @ @ @ image whose segmentation is to be done have some specific size or resolution ? I am making a project on image tampering so i need to know what exact input specifications are to be kept for the same . <p> Typically we only process images that have are a maximum of 600-800 pixels along their largest dimension ( i.e. , width or height ) . Often we try to resize our images as small as possible without sacrificing accuracy ( in the 200-500 pixel range ) . The smaller the image , the less data there is to process , therefore @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485164 @185164/ <h> My Deep Learning Kickstarter will go live on Wednesday , January 18th at 10AM EST <p> My- Deep Learning for Computer Vision with Python- Kickstarter campaign- is set to launch in- exactly one week- on- Wednesday , January 18th at 10AM EST . <p> This book has only goal " - to help- developers , researchers , and students- just like yourself become- experts- in deep learning for image recognition and classification . <p> Whether this is the- first time you 've worked with machine learning and neural , networks- or- you 're already a seasoned deep learning practitioner , - Deep Learning for Computer Vision with Python- is engineered from the ground up to help you reach expert status . <p> Hands-on tutorials ( with lots of code ) - that not only show you the- algorithms- behind deep learning for computer vision , but their- implementations- as well . <p> A no-bullshit teaching style- that is guaranteed to cut through all the cruft and help you master deep learning for image understanding and visual recognition . <p> As a heads up , over the next 7 days I @ @ @ @ @ @ @ @ @ @ n't want to miss , including : <h> Thursday , January 12th : <p> A sneak preview of the Kickstarter campaign , including a demo video of what you 'll find inside the book . <h> Friday , January 13th : <p> The Table of Contents for- Deep Learning for Computer Vision with Python. - This book is- extensive , covering the basics of deep learning all the way up to training large-scale networks on the massive ImageNet dataset . You wo n't want to miss this list ! <h> Monday , January 16th : <p> The full list of Kickstarter rewards ( including early bird discounts ) - so you can plan ahead for which reward you want when the Kickstarter launches . <p> I- wont be posting this list publicly this reward list is- only for PyImageSearch readers who are part of the PyImageSearch Newsletter . <h> Tuesday , January 17th : <p> Please keep in mind that this book is already getting- a lot- of attention , so there will be- multiple people in line for each reward level- when the Kickstarter campaign launches on Wednesday the 18th . @ @ @ @ @ @ @ @ @ @ I 'll be sharing- tips and tricks- you can use to ensure you 're first in line . <p> Again , I wont be posting this publicly either . Make sure you signup for the PyImageSearch Newsletter- to receive these tips and tricks to ensure you 're at the front of the line . <h> Wednesday , January 18th : <p> The Kickstarter campaign link- that you can use to- claim your copy of- Deep Learning for Computer Vision with Python . <p> Hey Siddharth I 'll be posting the link to back the Kickstarter campaign on Wednesday , January 18th at 10AM EST . If you are also on the PyImageSearch email list you 'll receive an email notification when the Kickstarter goes live as well . <p> I saw your video abut Kickstarter campaign I am very happy for your books me and my friends are waiting for Jan 18th , please iam little bit confused Please give some suggestions according 3bundles and I am very interested to learn deep learning but I am beginner which bundle that you suggest <p> If you are just getting started with deep learning and simply want @ @ @ @ @ @ @ @ @ @ , go with the Starter Bundle . If you want a more in-depth dive and study more advanced concepts , go with the Practitioner Bundle . If you want the complete package , go with the ImageNet Bundle . <p> It depends on which bundle you go with . All examples in the Starter Bundle are intended to work with your CPU ( although a GPU can certainly speed up training ) . The majority of the examples in the Practitioner Bundle will work with a CPU as well , although you should ideally be using the GPU since training times will increase dramatically . For the ImageNet Bundle , a GPU is absolutely required , ideally one with 8-12GB of memory . Multiple GPUs would also be @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485165 @185165/ <p> An image containing what you want to " overlay " on top of the first using some level of alpha transparency . <p> The results of applying such a transparent overlay can see seen at the top of this blog post . <p> So , you might be wondering : - " When/where will I use a transparent overlay in my computer vision application ? And why are you sharing this tip ? " <p> Great question . <p> For starters , if your OpenCV app requires a Heads-up Display ( HUD ) of any kind , you should consider using transparent overlays . Instead of displaying runtime critical information in- separate window or in the terminal , you can- directly overlay the information on your output image . Using transparent overlays alleviates the need to obfuscate- the contents of the output image ! <p> A second , more obvious example is alpha transparency where you need to " blend " two images together . <p> The use cases for transparent overlays are nearly endless my goal in todays blog post is simply to show you how you @ @ @ @ @ @ @ @ @ @ overlays with OpenCV <p> In the remainder of this lesson , I 'll demonstrate how to construct transparent overlays using Python and OpenCV . <p> Well start with the following image : <p> Figure 1 : Our initial image that we are going to construct an overlay for . <p> Our goal will be to : <p> Use the cv2.rectangle- function to draw a red bounding box surrounding myself in the- bottom-right corner of the image . <p> Apply the cv2.putText- method to draw the text PyImageSearch- ( along with the transparency factor ) in the- top-left- corner of the image . <p> The results of drawing the rectangle and text can be seen below : <p> Figure 2 : Drawing a rectangle and text on the original image . However , both the text and bounding box are entirely opaque no transparency has been applied . <p> However , notice that both the rectangle and text are fully opaque ! <p> You can not see myself through the rectangle , nor does the " PyImageSearch " text contain any level of transparency . <p> To remedy this , we can @ @ @ @ @ @ @ @ @ @ and construct the transparent overlay . <p> Let 's go ahead and dive into some source code to create a transparent overlay with Python and OpenCV . Open up a new file , name it overlay.py- , and insert the following code : <p> Transparent overlays with OpenCV <p> Python <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> 6 <p> 7 55203 @qwx675203 <p> **29;6803;TOOLONG 55220 @qwx675220 <p> importcv2 <p> # load the image <p> image=cv2.imread ( " mexico.jpg " ) <p> Lines 2-4 handle importing our required Python packages . <p> Line 7- loads our image from disk using the cv2.imread- function . <p> The next step is to loop over various values of alpha transparency between the range 0 , 1.0 , allowing us to visualize and understand how the alpha- value can influence our output image : <p> Transparent <p> 21 <p> 22 <p> # loop over the alpha transparency values <p> foralpha innp.arange(0,1.1,0.1) : : @ @ @ @ @ @ @ @ @ @ image -- one for <p> # the overlay and one for the final output image <p> overlay=image.copy() <p> output=image.copy() <p> # draw a red rectangle surrounding Adrian in the image <p> # along with the text " PyImageSearch " at the top-left <p> # corner <p> cv2.rectangle ( overlay , ( 420,205 ) , ( 595,385 ) , <p> ( 0,0,255 ) , -1 ) <p> cv2.putText ( overlay , " PyImageSearch : alpha= " . format(alpha) , <p> LONG ... <p> In order to apply the transparent overlay , we need to make two copies of the input image : <p> One for the final output- image . <p> And another for the overlay- we are about to construct . <p> Using the cv2.rectangle- function , we draw a rectangle surrounding myself in the bottom-right corner of the image . We then apply cv2.putText- to draw the text PyImageSearch- in the top-left corner . <p> We are now ready to apply the transparent overlay using the cv2.addWeighted- function : <p> Transparent overlays with OpenCV <p> Python <p> 24 <p> 25 <p> 26 <p> # apply the overlay <p> cv2.addWeighted @ @ @ @ @ @ @ @ @ @ 0 , output ) <p> The cv2.addWeighted- method requires six arguments . <p> The first is our overlay- , the image that we want to " overlay " on top of the original image using a supplied level of alpha transparency . <p> The second parameter is the actual- alpha transparency- of the overlay . The closer alpha- is to- 1.0 , the more- opaque the overlay will be . Similarly , the closer alpha- is to- 0.0 , the more- transparent the overlay will appear . <p> The third argument to cv2.addWeighted- is the source image in this case , the original image loaded from disk . <p> We supply the- beta value as the fourth argument . Beta is defined as 1-alpha- . We need to define both alpha and beta such that alpha+beta=1.0- . <p> The fifth parameter is the- gamma- value a scalar added to the weighted sum . You can think of gamma as a constant added to the output image after applying the weighted addition . In this case , we- set it to zero since we do not need to apply an addition @ @ @ @ @ @ @ @ @ @ the last argument , output- , which is the output destination after applying the weighted sum operation - this value is our final output image . <p> Our last code block handles displaying the final output image to our screen , as well as displaying the relevant- alpha and- beta values : <p> Transparent overlays with OpenCV <p> Python <p> 28 <p> 29 <p> 30 <p> 31 <p> # show the output image <p> print ( " alpha= , beta= " . format ( alpha , 1-alpha ) ) <p> cv2.imshow ( " Output " , output ) 55212 @qwx675212 <p> To execute our Python script , download the source code + example image to this post ( using the " Downloads " form found at the bottom of this lesson ) and execute the following command : <p> Transparent overlays with OpenCV <p> Shell <p> 1 <p> $python overlay.py <p> You should see the following image displayed to your screen : <p> Figure 3 : Notice how for alpha=1.0 , the text and rectangle are entirely opaque ( i.e. , not transparent ) . <p> However , once we @ @ @ @ @ @ @ @ @ @ rectangle are substantially more transparent : <p> Figure 4 : Constructing transparent overlays with Python and OpenCV . <p> At alpha=0.1- , the text and rectangle are barely visible : <p> Figure 5 : The smaller alpha gets , the more transparent the overlay will be . <p> Below you can see a GIF animation that visualizes each of the transparency levels : <h> Summary <p> In this blog post , we learned how to construct transparent overlays using Python , OpenCV , and the cv2.addWeighted- function . <p> Future blog posts will use this transparent overlay functionality to draw Heads-up Displays ( HUDs ) on output images , and to make outputs more aesthetically appealing . <h> Downloads : 55217 @qwx675217 <h> 13 Responses to Transparent overlays with OpenCV <p> Hi Adrian , first of all , thanks for all your contributions they 're just great . <p> At a first glance this might not look useful for somebody , but it is actually quite useful for several applications . One comment I have , would be , is there a way to apply transparent overlays of images that are @ @ @ @ @ @ @ @ @ @ I.e. I have my pictures folder and I want to apply my watermark signature to all of them , but my watermark file is a . PNG with alpha channel enablled and no background ( different size and format than JPEGS ) , you see what I mean ? Seems the library requires that both of the images are equal in size and format . <p> Both images do have to be the same size , but that 's easily accomplishable . The actual file format of the image does n't  matter . Once the image is loaded via cv2.imread , an image is represented as a NumPy array , regardless of original image file type . <p> In this case , I 'm assuming your watermark is smaller than the original image ? In that case , just clone the original image , place the watermark in image ( using array slicing ) , and then apply the cv2.addWeighted method . <p> What would you do if you had to merge two images based on a varying alpha channel ? Here , each pixel would have different alpha values . As @ @ @ @ @ @ @ @ @ @ parameters for the whole image , not an alpha matrix , which is what I require . <p> Is there possible to draw multiple rectangles with the different opacity of each ? If I addWeighted multiple times than transparency of previous instances are lost because of multiple added picture @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485166 @185166/ <p> Back in high school I was not that great of a public speaker . The stress . The anxiety . The fear of making a stupid mistake and saying something wrong in front of an auditorium filled with people was paralyzing - the pressure weighed on me and delivered a severe blow to my public speaking confidence . But after <p> There were three huge influences in my life that made me want to become a scientist . The first was David A. Johnston , an American USGS volcanologist who died on May 18th , 1980 , the day of the catastrophic eruption of Mount St. Helens in Washington state . He was the first to report the eruption , exclaiming Vancouver ! <p> A couple months ago , my friend- Dmitry Chaplinsky- sent me an email telling me about a new library- that I just had to checkout . Always curious about new computer vision and image processing packages , I became intrigued . You see , Dmitrys work involves applying Python and OpenCV to tag and categorize shredded documents in hope of uncovering corruption in <p> @ @ @ @ @ @ @ @ @ @ popular ) article on detecting barcodes in images using Python and OpenCV . In the previous post we explored how to detect and find barcodes in images . But today we are going to refactor the code to detect barcodes in- video . As an example , check out the screenshot below <p> This is a guest post by Michael Herman from Real Python learn Python programming and web development through hands-on , interesting examples that are useful and fun ! In this tutorial , well take the command line image search engine from the previous tutorial- and turn it into a full-blown web application using Python and Flask . More specifically , <p> Let 's face it . Trying to search for images based on text and tags sucks . Whether you are tagging and categorizing your personal images , searching for stock photos for your company website , or simply trying to find the right image for your next epic blog post , trying to use text and keywords to describe something that <p> UPDATE : The introduction to this post may seen a little " out there @ @ @ @ @ @ @ @ @ @ watching the South Park Black Friday episodes prior to writing this post so I definitely had some inspiration regarding zombie shoppers , Black Friday chaos , and Game of Thrones . Black Friday is coming . Hordes of angry shoppers . <p> Connecticut is cold . Very cold . Sometimes its hard to even get out of bed in the morning . And honestly , without the aide of copious amounts of pumpkin spice lattes and the beautiful sunrise over the crisp autumn leaves , I do n't  think I would leave my cozy bed . But I have work to do . And today <p> If you 've been paying attention to my Twitter account lately , youve probably noticed one or two teasers of what I 've been working on a Python framework/package to rapidly construct object detectors using Histogram of Oriented Gradients and Linear Support Vector Machines . Honestly , I really ca n't stand using the Haar cascade classifiers provided by OpenCV <p> So we know that matplotlib is awesome for generating graphs and figures . But what if we wanted to display a simple RGB image ? Can @ @ @ @ @ @ @ @ @ @ blog post will show you how to display a Matplotlib RGB image in only a few lines of @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485167 @185167/ <h> Main Menu <h> Contact <p> Have a question about computer vision , OpenCV , or deep learning ? Want more information about a specific topic ? Or just want to say hello ? I will be happy to reply ! Who knows , maybe I 'll turn your question into a blog post ! Just fill out the form below and I will get back to you ASAP . <p> NOTE : - I receive 100s of emails per day , many of them asking me to look at code. - As you know , reviewing another programmers code is a very time consuming and tedious task , and due to the volume of emails and contact requests I receive , I simply can not do it . I- encourage you to explain any problems or questions you are having with computer vision/OpenCV and include any screenshots of input/output images you are working with ( consider uploading your images to a free tool such as Imgur- and then including the URL in the contact form ) . Sharing well thought out questions and example images- tend to be- much @ @ @ @ @ @ @ @ @ @ . Please keep this in mind before you click " Submit @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485168 @185168/ <p> Well . Ill just come right out and say it. - Today is my 27th birthday . As a kid I was always- super excited about my birthday . It was another year closer to being able to drive a car . Go to R rated movies . Or buy alcohol . But now as an adult , I do n't  care too much for my <p> As we all know , OpenCV 3.0 was officially released back in June of 2015 . This new update incorporated- a ton of new features and optimizations , including- Python 3 bindings . But the big question on everyones mind is : " Should I switch to OpenCV 3 ? If so , when should I switch ? " Deciding when or even if you should switch <p> Last week we discussed how to use OpenCV and Python to perform pedestrian detection . To accomplish this , we leveraged the built-in HOG + Linear SVM detector that OpenCV ships with , allowing us to detect people in images . However , one aspect of the HOG person detector we- did not @ @ @ @ @ @ @ @ @ @ <p> I 've met a lot of amazing , uplifting- people over the years . My PhD advisor who helped get me through graduate school . My father who was always there for me as a kid and still is now . And- my girlfriend who has always been positive , helpful , and supportive ( even when I probably did n't  deserve it ) . I 've also <p> The watershed algorithm is a classic- algorithm used for segmentation and is especially useful when extracting- touching or- overlapping objects in images , such as the coins in the figure above . Using traditional image processing methods such as thresholding and contour detection , we would be unable to extract each individual coin from the image but by leveraging the <p> A few weeks ago Raspbian Jessie was released , bringing in a ton of new , great features . However , the update to Jessie also broke the previous OpenCV + Python install instructions for Raspbian Wheezy : Install OpenCV 2.4 with Python 2.7 bindings on Raspbian Wheezy . Install OpenCV 3.0 with Python 2.7/Python 3+ bindings on Raspbian Wheezy . @ @ @ @ @ @ @ @ @ @ learned from last week : how to construct an image scraper using Python + Scrapy to scrape 4,000 Time magazine cover images . So now that we have this dataset , what are we going to do with it ? Great question . One of my favorite visual analysis techniques to apply <p> Since this is a computer vision and OpenCV blog , you might be wondering : - " Hey Adrian , why in the world are you talking about scraping images ? " Great question . The reason is because- image- acquisition- is one of the most under-talked about subjects in the computer vision- field ! Think about it . Whether you 're leveraging machine learning to train an image classifier , building <p> Did you know that the human eye perceives color and luminance differently than the sensor on your smartphone or digital camera ? You see , - when- twice the number of photons hit the sensor of a digital camera , it receives- twice the signal ( a linear relationship ) . However , that 's not how our human eyes work . Instead , @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485169 @185169/ <h> Displaying a video feed with OpenCV and Tkinter <p> I said- it in last weeks blog post- and I 'll say it again here today - I am not , by any stretch of the imagination , a GUI developer . <p> I think my aversion to GUI development started back in early-High school when I was teaching myself Java ; specifically , how to write Java applets ( remember what god damn nightmares applets were ? ) utilizing javax- and Swing . <p> It was a dark time in my life . <p> I was sick often- and missed a lot of school . <p> There were a great deal of family problems going on . <p> And I had yet to mature from an adolescent into an adult , stuck in the throes of puberty an awkward teenage boy , drunk on apathy , while simultaneously lacking any direction or purpose . <p> At that- point in my ( early ) programming career , I turned to Java and GUI development in a last-ditch effort to escape , much like a heroin addict turns to a spoon @ @ @ @ @ @ @ @ @ @ only for the world to come crashing back down once the initial high wears off . <p> You see , I found developing GUI applications- fun . It was- addictive . And it was quite the departure from the all-too-familiar command line interfaces . <p> Since then , I 've always ( mentally ) correlated GUI development with painful , trying times . Its a door that I honestly have n't opened sinceuntil now . <p> In this blog post , I 'm going to confront my troubled past and write a bit of code to display a video feed with OpenCV and Tkinter . Ultimately , this GUI application will allow us to create a " Photo Booth " of sorts , letting us save frames from our video stream to disk at the click of a button . <p> As you 'll find out , I- kinda-sorta failed , but I wanted to share my experience with you hopefully more experienced GUI developers can help point me in the right direction . <h> Displaying a video feed with OpenCV and Tkinter <p> I 'm going to go ahead and assume that you have @ @ @ @ @ @ @ @ @ @ Tkinter . Inside this tutorial , I- detailed what Tkinter is , - how to install it , and- how to write a simple Tkinter GUI application that can display images loaded via OpenCV . <h> Mocking up the Photo Booth App <p> As we did in last weeks post , let 's start by creating a mockup of our application . Below you can see the main screen of our GUI application : <p> Figure 1 : A mockup of our Photo Both Application . This GUI will contain two elements : an image panel where the frames of our live video stream will be displayed , followed by a button , that allows us to save the current frame to disk . <p> This screen has two elements . The- first , at the bottom , is our- Snapshot ! button . Every time this button is clicked , the current frame read from the video stream will be stored on disk . <p> The- second element , placed directly above the first , is a- live display of the video stream itself . <p> Our goal is to @ @ @ @ @ @ @ @ @ @ frames from our video stream , update the live display , and then handle writing the current frame to file when the snapshot button is clicked . <h> Creating the Photo Booth App <p> Now that we 've created the mockup of our project , let 's go ahead and get started coding the GUI . Open up a new file , name it photoboothapp.py- , and insert the following code : <p> Displaying a video 8 <p> 9 <p> 10 55203 @qwx675203 <p> **29;6834;TOOLONG <p> fromPIL importImage <p> fromPIL importImageTk <p> importTkinterastki <p> importthreading <p> importdatetime 55219 @qwx675219 <p> importcv2 <p> importos <p> Lines 2-10 handle importing our required Python packages . We need PIL- for the Image- class , which is what the ImageTk- and Label- classes require in order to display an image/frame in a Tkinter GUI . <p> Well also need Pythons threading- package to spawn a thread ( separate from Tkinters mainloop- ) , used to handle polling of new frames from our video stream @ @ @ @ @ @ @ @ @ @ a human-readable timestamp filename for each frame that we save to disk . <p> Lastly , well need imutils , my collection of convenience functions used to make working with OpenCV easier . If you do n't  already have imutils- installed on your system , let pip- install the package for you : <p> Displaying a video feed with OpenCV and Tkinter <p> Shell <p> 1 55204 @qwx675204 <p> Let 's move on to the definition of our PhotoBoothApp- class : <p> Displaying a video feed with <p> 24 <p> 25 <p> classPhotoBoothApp : <p> definit ( self , vs , outputPath ) : <p> # store the video stream object and output path , then initialize <p> # the most recently read frame , thread for reading frames , and <p> # the thread stop event <p> self.vs=vs <p> **26;6865;TOOLONG <p> self.frame=None <p> self.thread=None <p> self.stopEvent=None <p> # initialize the root window and image panel <p> self.root=tki.Tk() <p> self.panel=None <p> Line @ @ @ @ @ @ @ @ @ @ constructor requires two arguments vs- , which is an instantiation of a VideoStream- , and outputPath- , the path to where we want to store our captured snapshots . <p> Lines 17 and 18 store our video stream object and output path , while- Lines 19-21 perform a series of initializations for the most recently read frame- , the thread- used to control our video polling loop , and stopEvent- , a thread.Event- object used to indicate when the frame pooling thread should be stopped . <p> We then initialize the root- Tkinter window and the panel used to display our frame in the GUI ( Lines 24 and 25 ) . <p> We continue the definition of our constructor below : <p> Displaying a video feed with <p> 41 <p> 42 <p> # create a button , that when pressed , will take the current <p> # frame and save it to file <p> btn=tki.Button ( @ @ @ @ @ @ @ @ @ @ ) <p> LONG ... <p> pady=10 ) <p> # start a thread that constantly pools the video sensor for <p> # the most recently read frame <p> **32;6920;TOOLONG <p> LONG ... <p> self.thread.start() <p> # set a callback to handle when the window is closed <p> self.root.wmtitle ( " PyImageSearch PhotoBooth " ) <p> LONG ... <p> Lines 29-32 create our- Snapshot ! button , that when clicked , will call the takeSnapshot- method ( which well define later in this example ) . <p> In order to continuously- poll frames from our video stream and update the panel- in our GUI , we need to spawn a separate thread that will be used to monitor our video sensor and grab the the most recently read frame ( Lines 36-38 ) . <p> Finally , we set a callback to handle when our window is closed so we can perform cleanup operations and ( ideally ) stop the video polling thread and release any resources ( unfortunately , this did n't  work as I intended it to in practice ) . <p> Next up , let 's define the videoLoop- function @ @ @ @ @ @ @ @ @ @ stream for new frames : <p> Displaying a video feed with <p> 75 <p> 76 <p> defvideoLoop(self) : <p> # DISCLAIMER : <p> # I 'm not a GUI developer , nor do I even pretend to be . This <p> # try/except statement is a pretty ugly hack to get around <p> # a RunTime error that Tkinter throws due to threading <p> try : <p> # keep looping over frames until we are instructed to stop <p> **30;6954;TOOLONG : <p> # grab the frame from the video stream and resize it to <p> # have a maximum width of 300 pixels <p> **25;6986;TOOLONG <p> LONG ... <p> # OpenCV represents images in BGR order ; however PIL <p> # represents images @ @ @ @ @ @ @ @ @ @ # the channels , then convert to PIL and ImageTk format <p> LONG ... <p> **28;7013;TOOLONG <p> **31;7043;TOOLONG <p> # if the panel is not None , we need to initialize it <p> ifself.panel isNone : <p> **33;7076;TOOLONG <p> self.panel.image=image <p> self.panel.pack ( side= " left " , padx=10 , pady=10 ) <p> # otherwise , simply update the panel <p> else : <p> **33;7111;TOOLONG <p> self.panel.image=image <p> exceptRuntimeError , e : <p> print ( " INFO caught a RuntimeError " ) <p> As I said at the top of this blog post I 'm not a GUI developer and I have very little experience with Tkinter . In order to get around a RunTime- exception that Tkinter was throwing ( likely due to threading ) , I resorted to- really ugly try/except- hack to catch the RunTime- error. - I tried to resolve the bug , but after a few hours of not getting anywhere , I eventually threw in the towel and resorted to this hack . <p> Line 51 starts a loop that will be used to read frames from our video sensor . This loop will continue @ @ @ @ @ @ @ @ @ @ should return to its parent . <p> Lines 54 and 55 read the frame- from our video stream and resize it using the imutils- library . <p> We now need to perform a bit of formatting on our image . To start , OpenCV represents images in BGR order ; however , PIL expects images to be stored in RGB order . To resolve this , we need to swap the channels by calling cv2.cvtColor- . From there , we convert the frame- to PIL/Pillow format , followed by ImageTk- format . The ImageTk- format is required when displaying images in a Tkinter window . <p> If our panel- is not initialized , - Lines 65-68 handle instantiating it by creating the Label- . We take special care on- Line 67 to store a reference to the image- , ensuring that Pythons garbage collection routines do not reclaim the image- before it is displayed on our screen . <p> Otherwise , if the panel- has already been initialized , we simply update it with the most recent image- on Lines 71-73 . <p> Now , let 's take a look at @ @ @ @ @ @ @ @ @ @ <p> 86 <p> 87 <p> deftakeSnapshot(self) : <p> # grab the current timestamp and use it to construct the <p> # output path <p> **26;7146;TOOLONG <p> LONG ... <p> LONG ... <p> # save the file <p> cv2.imwrite ( p , self.frame.copy() ) <p> print ( " INFO saved " . format(filename) ) <p> When the- " Snapshot ! " button is clicked , the takeSnapshot- function is called. - Lines 81-83 generate a filename for the frame- based on the current timestamp . <p> We then save the frame- to disk on- Line 86 by making a call to cv2.imwrite- . <p> Finally , we can define our last method , onClose- : <p> Displaying a video feed with OpenCV and Tkinter <p> Python <p> 89 <p> 90 <p> 91 <p> 92 <p> 93 <p> 94 <p> 95 <p> defonClose(self) : <p> # set the stop event , cleanup the camera , and allow the rest of <p> # the quit process to continue <p> print ( " @ @ @ @ @ @ @ @ @ @ self.root.quit() <p> This function is called when we click the- " X " in the GUI to close the application. - First , we set the stopEvent- so our infinite videoLoop- is stopped and the thread returns . We then cleanup the video stream pointer and allow the root- application to finish closing . <h> Building the Photo Booth driver <p> The last step in creating our Photo Booth is to build the driver script , used to initialize both the VideoStream- and the PhotoBoothApp- . To create the driver script , I 've added the following code to a file named photobooth.py- : <p> Displaying a video 21 <p> 22 <p> 23 55203 @qwx675203 <p> **29;7174;TOOLONG <p> **31;7205;TOOLONG importPhotoBoothApp <p> fromimutils.video importVideoStream 55218 @qwx675218 <p> importtime 55202 @qwx675202 55206 @qwx675206 <p> LONG ... <p> help= " path to output directory to store @ @ @ @ @ @ @ @ @ @ or not the Raspberry Pi camera should be used " ) 55208 @qwx675208 <p> # initialize the video stream and allow the camera sensor to warmup <p> print ( " INFO warming up camera ... " ) <p> LONG ... <p> time.sleep(2.0) <p> # start the app <p> pba=PhotoBoothApp ( vs , args " output " ) <p> pba.root.mainloop() <p> Lines 9-14 handle parsing the command line arguments of our script . The first command line argument , --output- , is- required . The --output- switch is simply the path to where we want to store our output snapshots . <p> We then- have --picamera- , an- optional switch used to indicate whether the Raspberry Pi camera module should be used or not . By default , this value will be- -1 , indicating that our builtin/USB webcam should be used . We can specify a value- &gt; 0 if we want to utilize the Pi camera . You can learn more about this- parameter - and how to use it in conjunction with the VideoStream- class in this blog post . <h> Running our Photo Booth <p> To run @ @ @ @ @ @ @ @ @ @ necessary libraries- and packages installed ( as detailed in the previous blog post ) . After you 've ensured your system is configured properly , execute the following command : <p> Displaying a video feed with OpenCV and Tkinter <p> Shell <p> 1 <p> $python photobooth.py--output output <p> After the camera sensor warms up , you should see the following display : <p> Figure 2 : Once our app launches , you should see the live stream of the camera displayed in the Tkinter window . <p> Notice how our GUI contains both the- live stream from our webcam along with the- button used to trigger a snapshot . <p> After clicking the snapshot button , I can see that my output- directory contains the photo I just took : <p> Figure 3 : Whenever I click the " Snapshot ! " button , the current frame is saved to my local disk . <p> Below I have included a short video to demonstrate the Photo Booth application : <h> What the- hell are these errors ? <p> As I alluded to at the top of this blog post , the @ @ @ @ @ @ @ @ @ @ the try/except- block in the videoLoop- function of PhotoBoothApp- , closing the application results in the following RunTime- exception : <p> I- think this is because the panel- object is garbage-collected- before the thread finishes executing , but I 'm not entirely sure ( again , I 'm not very familiar with Tkinter ) . <p> The second error happens intermittently , but again , also occurs during the window close : <p> Figure 5 : The " AttributeError " happens only intermittently . <p> As you can see , I am getting an AttributeError- error . It does n't  happen- all the time , only- some of the time , so I 'm pretty convinced that this must be a threading problem of some sort . A bit of research online has led me to believe that Tkinter does n't  play nice with threading , but I 'm not 100% sure if this is the- exact issue or not . <p> Either way , I 've decided to put this project to bed for now and let the more experienced GUI developers take over I 've had enough of Tkinter for the next few months @ @ @ @ @ @ @ @ @ @ delved into my troubled past and faced my fears - building GUI applications . <p> I discussed how to build a simple Photo Booth application that reads frames from a live stream via OpenCV ( in a threaded , efficient manner ) and then displays the stream to our Tkinter GUI. - Upon clicking a button in the GUI , the current frame is then saved to disk . <p> However , this application was n't a complete success . <p> I ran into two different types of errors a RunTime- error and an AttributeError- exception . I resolved the RunTime- error by hacking together an ugly try/except- block , but the AttributeError- exception is still perplexing to me , due to its intermittent nature . If you know the solution to this problem , please leave a comment in the comments section at the bottom of this post . <p> To be honest , I 'm not sure if I 'll be doing more OpenCV + Python + GUI applications in the future . It was neat to build one or two , but my interests in computer vision are more focused @ @ @ @ @ @ @ @ @ @ the development of full-fledged GUIs surrounding these applications . <p> That said , I- could be convinced to write some tutorials on how to build web applications surrounding- computer vision algorithms that 's something Im- actually good at , have done- a lot of in the past , and even- enjoy to a degree . <p> Be sure to leave your comments and suggestions in the comments section at the bottom of this post - and be sure to signup for the PyImageSearch Newsletter using the form below ! <p> See you next week . <h> Downloads : 55217 @qwx675217 <h> 54 Responses to Displaying a video feed with OpenCV and Tkinter <p> I had n't  thought about using Tkinter with OpenCV like this . I had played with Tkinter many years ago . <p> I think I have some of the same aversions as you when it comes to developing a GUI . I was required to do some of that in the later part of my career , but for the most part I was fortunate to work only on code . <p> Personally , I would be interested @ @ @ @ @ @ @ @ @ @ . However , since I do enjoy the code , I 'm fine without it so do not count this as a vote for it . I 'm just talking . = <p> I find your blog most enjoyable . I hope to actually use some of your techniques in future projects . ( Actually one will be for an existing project . Its already in the list of issues on GitHub . ) <p> I wont post a link to my projects here , but if you follow the link to my blog you will find links to them " at least the open source projects ( of course ) . <p> Thanks for all the time and effort you put into giving away information ! We are also of like minds on that . <p> I am now trying to understand how Tkinter works , as I have to use it for some programs which I implemented . My programs are connected to live stream from a normal , Logitech camera , therefore I followed your tutorial to see how I can make a button If I press the button @ @ @ @ @ @ @ @ @ @ To be noticed that I never seen or tried to build a gui before . <p> So , I tried to make your program run , I have 2 different files : photoboothapp.py and photobooth.py . I tried to run each of them , the first one runs with no error but it does nothing and the second one gives me the error : ImportError : No module named **27;7238;TOOLONG . It is because this line : from **27;7267;TOOLONG import PhotoBoothApp . <p> Make sure you download the source code to this blog post using the " Downloads " section . The . zip contains all source files and shows you how to organize the project . It also includes examples on how to execute the Python script via the command line . <p> Do not familiar with Tkinter , I think this is because you try to manipulate GUI from non-GUI thread ( also called main thread ) . All widgets and user interface must be handled from the main thread , this means all of the user interfaces act like some sort of consumer . <p> This is @ @ @ @ @ @ @ @ @ @ of the OS , even the mighty Qt5 library also suffered from this issue ( thanks to queue connection of Qt , communication between threads become very intuitive ) . <p> The other problem may cause by the non thread safe api : <p> Like stopEvent.isset() and stop() . If they are not thread safe , we have to to lock them , else some sort of weird issues may ( not must ) emerge . <p> Hi Adrian , Thank you very much for your tutorials , I have been following your tutorial for a month , and really learned a lot . <p> I guess the AttributeError exception is because you created an independent threading of videoLoop() , and in the function videoLoop() , you read the image from vs , which is another independent threading . While you close the GUI , the vs threading closes before the videoLoop threading ( I am not sure ) . From the code from others , I found that its no need to use a new threading to display the video stream in Tkinter , So I tried to modify @ @ @ @ @ @ @ @ @ @ the 37th and 38th line of your code 2. add self.videoLoop() just after the former 38th line 3. change the " while not " in line 51 to " if not " 4. add this line " self.panel.after ( 10 , self.videoLoop ) " to the last of the function videoLoop() . <p> Hi , thank you so much for this tutorial ! ! ! I managed to run everthing using a picamera and even fliped it using another one of your tutorials , but the problem I have is that I cant find where the snapshots are stored . No error appears and even the terminal message with the file name shows correctly , but I cant find the files . Can you help me ? <p> Also I had to change the line 3 of the driver : from **27;7296;TOOLONG import PhotoBoothApp ) to from photoboothapp import PhotoBoothApp to make it work . Do you think this has something to do with it ? <p> Regarding your second question ( Line 3 ) , I had updated the init.py file to create a " shortcut " for the @ @ @ @ @ @ @ @ @ @ post using the " Downloads " section you can compare your directory structure to mine and see the change I made to init.py . If you run into errors like this I always suggest downloading the code and comparing . <p> As for your first question , you actually provide the output path for the snapshots via the --output switch ( Line 10 ) . If you use the exact command like I do in this post : <p> $ python photobooth.py --output output <p> The snapshots will be stored in the output directory . If you are running this code on your machine ( and havent downloaded my code ) you 'll need to create the output directory before executing the script . <p> I also added a print option and is working fine as well but I wanted to use the module " import cups " for some other features and I get the error message " No module named cups " . <p> The weird thing is I have already installed it ( " sudo apt-get install python-cups " ) and it works well outside of the virtual @ @ @ @ @ @ @ @ @ @ when I 'm inside . Do you know how could I fix this ? <p> workon cv $ pip install cups And from there you should be able to use " cups " from inside your cv virtual environment . I would suggest reading the first half of this blog post to better familiarize yourself with virtual environments before you continue . <p> Hi Adrian ! I 'm really enjoyed this post . I have been going through your OpenCv tutorials and am learning so much . You are a saint for helping out us newbies ! <p> I want to use your tutorial from this blog post to help me create a program that does focus stacking . It is working out well so far , except when I take pictures it will sometimes save the images with 640+480 resolution and sometimes at a lower resolution 350+320 . Is there a way to set the resolution that the images are saved at ? <p> Hey Katelin that is some very strange behavior . I 'm honestly not sure why the resolution would change . Once the VideoStream object is instantiated it utilizes @ @ @ @ @ @ @ @ @ @ . If you continue to receive resolution errors I would post on the picamera GitHub . <p> Its absolutely possible . Its been a long time since I 've worked with IP cameras so I do n't  have any direct advice other than to work with the cv2.VideoCapture function . Ill try to do a IP camera tutorial in the future . <p> Hi Adrian . As always your posts and explanations are pretty cool . My question here is if there 's a way to do this with the Raspberry Pi camera , because I saw on the video that you used a webcam , how can I achieve this with the Raspberry Pi Camera ? <p> Hello Adrian , That 's a very interesting project . Thanks for sharing ! I am interested in live video feed but from a dslr camera and I could n't find any tutorial that explains how to do this . This works only for webcam ? <p> Hey Adrian , instead of dealing with threading you could use the after command of Tkinter to refresh the widget with a new frame of the video . I @ @ @ @ @ @ @ @ @ @ reduced and exiting the program goes a lot smoother . Thank you for making such amazing blog posts . You have no idea how much your posts have helped me and others . I hope you keep @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485170 @185170/ <p> Honestly , I love the Raspberry Pi for teaching computer vision it is perhaps one of the best teaching tools to expose programmers , developers , and students to the world of computer vision . Its great for hobbyists- and garage-room hackers , as you get to learn on a cheap , but super fun piece of hardware . Its awesome for <p> A couple weeks ago I provided step-by-step install instructions to- setup OpenCV 3.0 and Python 2.7+ on your Ubuntu machine . However , one of the huge benefits of migrating to OpenCV 3.0 is the new Python 3.4+ support . In the previous 2.4 . X releases of OpenCV , only Python 2.7+ was supported . But now , we can- finally leverage Python 3.4+ <p> If you 've had a chance to play around with OpenCV 3 ( and do a lot of work with keypoint detectors- and feature descriptors ) you may have noticed that the SIFT and SURF implementations are- no- longer included in the OpenCV 3 library by default . Unfortunately , you probably learned this lesson the hard way by opening @ @ @ @ @ @ @ @ @ @ of the bat-country Python package for deep dreaming and visualization is its ease of use , extensibility , and customization . And let me tell you , that customization really came in handy last Friday- when the Google Research team- released an update to their deep dream work , demonstrating a method to " guide " your input images <p> We cant stop here , this is bat country . Just a few days ago , the Google Research blog published a post demonstrating a unique , interesting , and perhaps even disturbing method to visualize what 's going inside the layers of a Convolutional Neural Network ( CNN ) . Note : - Before you go , I suggest taking a look at the images generated using <p> Two weeks ago we kicked off the OpenCV 3.0 install-fest with a tutorial on how to install OpenCV 3.0 and Python 2.7 on OSX . Today I 'm back again with another OSX tutorial only this time we are going to compile OpenCV 3.0 with- Python 3.4+ bindings ! That 's right ! With the OpenCV 3.0 gold release , we <p> Last week @ @ @ @ @ @ @ @ @ @ to install OpenCV 3.0 and Python 2.7+ on the OSX platform . Today we are going to continue the OpenCV 3.0 install instruction series by moving over to the Ubuntu operating system . In the remainder of the post I will provide instructions on how <p> As I mentioned last week , OpenCV 3.0 is finally here ! And if you 've been paying attention to my Twitter stream , you may have noticed a bunch of tweets regarding installing OpenCV on OSX- and Ubuntu- ( yep , I 've been tweeting a lot lately , but that 's just because I 'm so excited about the 3.0 release ! ) To celebrate OpenCV 3.0 , <p> What does a hot summer day , - Automatic License Plate Recognition and the- Talk Python to Me Podcast- have in common ? Well . Thats- a funny story and one that landed me with a pretty hefty fine from the police . But for the full story , you 'll have to listen to the latest- Talk Python to Me podcast to find out ! It <p> Its been a long time coming , but OpenCV 3.0 @ @ @ @ @ @ @ @ @ @ of the most extensive overhauls- to the library in recent years , and boasts increased stability , performance increases , and OpenCL support . But by far , the most exciting update for us in the Python world is : @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485172 @185172/ <h> Preparing your experiment and training data <p> In this lesson , we will start to build our actual object detection framework . This framework will consist of many steps and components , including : <p> Step #1 : - Experiment preparation <p> Step #2 : Feature extraction <p> Step #3 : Detector training <p> Step #4 : Non-maxima suppression <p> Step #5 : Hard negative mining <p> Step #6 : - Detection retraining . <p> We will have a lesson dedicated to each of these steps in the remainder of this module . <p> However , let 's start with the first ( often overlooked ) step - actually setting up our experiment and examining our training data . <p> Its important to have an intimate understanding of our training data . The- knowledge gained from this initial investigation will guide future parameter choices in our framework . Furthermore , the choices in parameters can either make or break an object detector . <h> Framework goals <p> The primary goal of this module is to build an object detection framework that can be used to easily and rapidly build @ @ @ @ @ @ @ @ @ @ e.x. , chair , car , airplane , etc. ) , the code should have to change only- minimally , and ideally , - not at all . <p> Specifically , the framework we will be building in the remainder of this module will be capable of constructing an object detector for- any object class in the CALTECH-101 dataset . <p> Below follows the complete directory structure of our framework : <p> Project structure <p> 18 <p> 19 <p> ---pyimagesearch <p> ----init.py <p> ----descriptors <p> ----init.py <p> ----hog.py <p> ----objectdetection <p> ----init.py <p> ----helpers.py <p> ----nms.py <p> ----objectdetector.py <p> ----utils <p> ----init.py <p> ----conf.py <p> ----dataset.py <p> ---exploredims.py <p> ---extractfeatures.py <p> ---hardnegativemine.py <p> ---testmodel.py <p> ---trainmodel.py <p> If this seems overwhelming , do n't  worry well be reviewing each of these files ( and the role it plays in the framework ) in detail . <p> Modifying the code @ @ @ @ @ @ @ @ @ @ in this dataset will be- extremely easy , - and I 'll be sure to point out the required modifications along the way . <p> In order to maintain cohesive examples in the rest of this module , well be using the carside- data from CALTECH-101 , a collection of car images taken from a side view : <p> Figure 1 : A subset of our car images from the CALTECH-101 dataset . <p> Once trained , our object detector will be able to detect the presence ( or lack thereof ) of a car in an image , followed by drawing a bounding box surrounding it : <p> Figure 2 : Our goal is to detect the presence of a car in an image and draw a bounding box surrounding it . <p> Again , this framework is not specific to side views of cars- - it can be used to create any custom object detector of your choice . The carside- choice is simply an example we will use for the next few lessons . <h> Experiment configurations <p> Up until now , the vast majority of our code @ @ @ @ @ @ @ @ @ @ <p> The same will be true for our object detection framework , but since ( 1 ) we will have- a lot of parameters , and ( 2 ) many of these parameters will be used over multiple Python scripts , we are going to use a JSON configuration file to store all customizable options of our framework . <p> There are many benefits to using a JSON configuration file : <p> First , we do n't  need to explicitly define a never-ending list of command line arguments all we need to supply is the path to our configuration file . <p> A configuration file allows us to consolidate all relevant parameters into a- single location . <p> This also ensures that we do n't  forget which command line options were used for each Python script . All options will be defined within our configuration file . <p> Finally , this allows us to have a configuration file for- each object detector we want to create . This is a huge advantage and allows us to define object detectors by only modifying a- single file . <p> Below follows the @ @ @ @ @ @ @ @ @ @ cars.json <p> JavaScript <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> 6 <p> 7 <p> 8 <p> <p> ####### <p> #DATASET PATHS <p> ####### <p> LONG ... <p> LONG ... <p> " imagedistractions " : " datasets/sceneclass13 " , <p> <p> All we are doing here is defining three sets of data required to train any object detector . <p> The first , imagedataset- , is the path to where our " positive example " images reside on disk . These images should contain examples of the objects we want to learn how to detect in this case , cars . <p> The second , imageannotations- , is the path to the directory containing the bounding boxes associated with each image in imagedataset- . <p> We need these initial bounding boxes , so we can extract Histogram of Oriented Gradients ( HOG ) features from their corresponding ROIs and use these features to train our Linear SVM ( i.e. , the actual " detector " ) . <p> Finally , we have our imagedistractions- , or more simply , the " negative examples " that do not @ @ @ @ @ @ @ @ @ @ . Again , its- very important that these images do not contain any positive examples if they do , it can dramatically hurt the performance of our classifier since we are " polluting " our negative training data with examples that should actually be in the positive training set . <p> A great choice for a distraction dataset is 13 Natural Scene Categories , - a collection of natural scenes , including beaches , mountains , forests , etc . <p> Figure 3 : A sample of the 13 Natural Scene Categories dataset . <p> In context of our example , this dataset is sure not to contain any examples of cars . <h> Exploring our training data <p> Recall back to our lesson on- sliding windows , an object detection tool that is used to " slide " over an image from left-to-right and top-to-bottom . At each window position , HOG features are extracted and then passed on to our classifier to determine if an object of interest resides within that particular window . <p> However , choosing the appropriate size of our sliding window is a non-trivial @ @ @ @ @ @ @ @ @ @ how wide the window should be ? Or how tall ? And what is the appropriate aspect ratio ( i.e. , ratio of width to height ) for the objects we want to detect ? <p> The size of our sliding window also has implications on the downstream HOG descriptor . <p> Again , recall from our lesson on HOG that the descriptor requires ( at a bare minimum ) two key parameters : <p> pixelspercell <p> cellsperblock <p> The size of our sliding window can dramatically limit ( or dramatically expand ) the possible values these parameters can take on . <p> Since an accurate object detector is highly dependent on our HOG parameter choices , its critical that we make a good choice when selecting a sliding window size a poor choice in sliding window dimensions can almost certainly ensure our object detector will perform poorly . <p> Given all this pressure on selecting an appropriate sliding window size , it raises the question : - " How do we make an informed decision on the dimensions of our sliding window ? " <p> Instead of making a @ @ @ @ @ @ @ @ @ @ hoping for good results , we can actually examine the dimensions of our object image annotations in order to make a more informed decision . <p> LONG ... to the configuration file " ) 55208 @qwx675208 <p> Lines 2-7 handle importing our necessary packages . The Conf- class is simply a small helper class that is used in conjunction with the commentjson package to load our . json- configuration file from disk . <p> The contents of config.py- are unexciting and essentially irrelevant to building our custom object detector , but the file is presented below as a matter 10 <p> 11 <p> 12 55203 @qwx675203 <p> importcommentjson asjson <p> classConf : <p> definit ( self , confPath ) : <p> # load and store the configuration and update the object 's dictionary <p> conf=json.loads ( open ( confPath ) . read() ) <p> self. dict.update(conf) <p> defgetitem ( self , k ) : <p> # return the value associated with the supplied key @ @ @ @ @ @ @ @ @ @ you do n't  already have commentjson- installed on your system , just issue the following command : <p> From there , - Line 20 starts looping over the annotation files ( which are provided in MATLAB format ) . - Lines 23-25- load the bounding box associated with each annotation file and update the respective width and height lists . <p> Finally , - Lines 28-31- compute the average width and height , along with the aspect ratio of the bounding boxes . <p> To explore the dimensions of our car side views , just issue the following command : <p> exploredims.py <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> $python exploredims.py--conf conf/cars.json <p> INFOavg.width:184.46 <p> INFOavg.height:62.01 <p> INFOaspect ratio:2.97 <p> As we can see from the output , the average width of a car is approximately 184px and the average height is 62px , implying that a car is nearly 3x wider- than it is tall . <p> So , now that we know some basic information about our object dimensions , what do we do now ? <p> Is our sliding window size simply- 184 @ @ @ @ @ @ @ @ @ @ to take into consideration the parameters of our HOG descriptor first and that 's exactly what our next lesson is going to cover . <h> Summary <p> In this lesson , we reviewed the basic structure of our object detection framework . We are going to use JSON configuration files , one config file for each object class we want to detect , that encapsulates all relevant parameters needed to build an object detector . <p> We also reviewed the entire directory project structure for our object detection framework . This framework can be decomposed into six steps/components . We reviewed the first step in this lesson preparing your experiment and training data . <p> In the next lesson , well take the- insights gained from the exploredims.py- script and use the average width , average height , and aspect ratio to define the parameters to our HOG descriptor. 
@@71485173 @185173/ <h> Tag Archives opencv <p> Todays blog post is inspired by an email I received from PyImageSearch reader , Brian . Brian asks : Hi Adrian , I 'm really enjoying the PyImageSearch blog . I found your site a few days ago and I 've been hooked on your tutorials ever since . I followed your tutorial on building an image search engine , but instead of displaying <p> In last weeks blog post I demonstrated how to count the number of frames in a video file . Today we are going to use this knowledge to help us with a computer vision and image processing task - visualizing movie barcodes , similar to the one at the top of this post . I first became aware of <p> Todays blog post is part of a two part series on working with video files using OpenCV and Python . The first part of this series will focus on a question emailed in by PyImageSearch reader , Alex . Alex asks : I need to count the total number of frames in a video file with OpenCV . The only <p> @ @ @ @ @ @ @ @ @ @ wasted three weeks of research time- during graduate school six- years ago . It was the end of my second semester of coursework . I had taken all of my exams early and all my projects for the semester had been- submitted . Since my school obligations were essentially nil , I started experimenting <p> Each week I receive and respond to at least 2-3 emails and 3-4 blog post comments regarding NoneType- errors in OpenCV and Python . For beginners , these errors can be hard to diagnose by definition they are n't  very informative . Since this question is getting asked so often I decided to dedicate an entire blog post <p> Ever since I wrote the first PyImageSearch tutorial on installing OpenCV + Python on the Raspberry Pi B+ back in February 2015 it has been my dream to offer a- downloadable , pre-configured Raspbian . img file with OpenCV pre-installed . Today this dream has become a reality . I am pleased to announce that both the Quickstart Bundle and- Hardcopy <p> I 'm going to start todays blog post by asking a series of questions which @ @ @ @ @ @ @ @ @ @ are image convolutions ? What do they do ? Why do we use them ? How do we apply them ? And what role do convolutions play in deep learning ? The word " convolution " sounds like a <p> The PyImageSearch blog has grown a lot since I published that- first post back in January 2014 . Its been an incredible journey and I have you , the reader , to thank for all the support over the past 2.5 years . However , due to the whirlwind growth of PyImageSearch , I now receive 100+ emails per day- and its <p> I said- it in last weeks blog post- and I 'll say it again here today - I am not , by any stretch of the imagination , a GUI developer . I think my aversion to GUI development started back in early-High school when I was teaching myself Java ; specifically , how to write Java applets ( remember @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485174 @185174/ <h> Archive Machine Learning <p> Disclaimer : This post is a bit cynical in tone . In all honesty , I support deep learning research , I support the findings , and I believe that by researching deep learning we can only further improve our classification approaches and develop better methods. - We all know that research is iterative . And sometimes we even @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485175 @185175/ <h> Tag Archives classification <p> In last weeks blog post , we discussed- gradient descent , a first-order optimization algorithm that can be used to learn a set of classifier coefficients for parameterized learning . However , the " vanilla " implementation of gradient descent can be prohibitively slow to run on large datasets in fact , it can even be considered- computationally wasteful . Instead , we should apply- Stochastic <p> Every relationship has its building blocks . Love . Trust . Mutual respect . Yesterday , I asked my girlfriend of 7.5 years to marry me . She said yes . It was quite literally the happiest day of my life . I feel like the luckiest guy in the world , not only because I have her , but also because this incredible PyImageSearch <p> If you 've been following along with this series of blog posts , then you already know what a- huge fan I am of Keras . Keras is a super powerful , easy to use Python library for building neural networks and deep learning networks . In the remainder of this blog @ @ @ @ @ @ @ @ @ @ <p> In previous tutorials , I 've discussed two important loss functions : - Multi-class SVM loss and- cross-entropy loss ( which we usually refer to in conjunction with Softmax classifiers ) . In order to to keep our discussions of these loss functions straightforward , I purposely left out an important component : - regularization . While our loss function allows us to determine how well ( or poorly ) our <p> Last week , we discussed Multi-class SVM loss ; specifically , the hinge loss and squared hinge loss functions . A loss function , in the context of Machine Learning and Deep Learning , allows us to quantify how " good " or " bad " a given classification function ( also called a " scoring function " ) is at correctly classifying data points in our dataset . However , <p> A couple weeks ago , we discussed the concepts of both- linear classification- and- parameterized learning . This type of learning allows us to take a set of input data and class labels , and actually learn a function that- maps the input to the output @ @ @ @ @ @ @ @ @ @ optimizing over them . Our linear classification tutorial focused <p> Over the past few weeks , we 've started to learn more and more about machine learning and the role it plays in- computer vision , - image classification , and- deep learning . Weve seen how Convolutional Neural Networks ( CNNs ) such as LetNet can be used to classify handwritten digits from the MNIST dataset . We 've applied the k-NN algorithm to classify whether or <p> In last weeks post , I introduced the k-NN machine learning algorithm which we then applied to the task of image classification . Using the k-NN algorithm , we obtained- 57.58% classification accuracy on the Kaggle Dogs vs . Cats dataset challenge : The question is : - " Can we do better ? " Of course we can ! - Obtaining higher accuracy for nearly any machine learning algorithm <p> Now that we 've had a taste of Deep Learning and Convolutional Neural Networks in last weeks blog post on LeNet , were going to take a step back and start to study machine learning in the context of image classification in more @ @ @ @ @ @ @ @ @ @ ( k-NN ) classifier , arguably the- most simple , easy <p> Did you know that OpenCV can detect cat faces in imagesright out-of-the-box with- no extras ? I did n't  either . But after Kendrick Tan broke the story , I had to check it out for myselfand do a little investigative- work to see how this cat detector seemed to sneak its way into the OpenCV repository @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485177 @185177/ <p> So what type of shape descriptors does OpenCV provide ? The most notable are Hu Moments which are can be used to describe , characterize , and quantify the shape of an object in an image . Hu Moments are normally extracted from the silhouette or outline of an object in an image . By describing the silhouette or outline <p> Before we dive into this post , let 's take a second and talk about Oscar , a dedicated PyImageSearch reader . He is just getting started in computer vision and he s taken the best possible route to mastering the subject : creating your own projects and solving them . Oscar picked up a copy of my book , - Practical Python and <p> Last week I wrote a post- detailing- my experience with CUDAMat , Deep Belief Networks , and Python using my MacBook Pro . The post is fairly long and full of screenshots to document my experience . But the gist of it is this : Even after installing the NVIDIA Cuda SDK and configuring CUDAMat , my CPU was training my Deep Belief @ @ @ @ @ @ @ @ @ @ my MacBook Pro , a GPU , and the CUDAMat library and it does n't  have a happy ending . Two weeks ago I posted a- Geting Started with Deep Learning and Python- guide . The guide was great and well received . I had a blast writing it . And a lot of the <p> Originally I had intended on doing a followup post on my Getting Started with Deep Learning Guide , but due to some unfortunate personal events , I was n't able to complete the blog post . But do n't  worryI still have a really great tutorial for you today ! Bad tutorials are worse than warm beer . You want to know <p> Earlier this year I posted about the top 7 Python books of 2014- ( thus far ) . Now , I 'm taking it to the next level . I 'm taking some of my favorites from this list . Others from my own personal library . And I 'm bundling them together and giving them away FOR FREE . If you 're interested in computer vision , <p> Update January 27 @ @ @ @ @ @ @ @ @ @ I have updated the source code in the download to include the original MNIST dataset ! No external downloads required ! Update March 2015 , 2015 : The nolearn package has now deprecated and removed the dbn- - module . When you go to install the nolearn package , be sure <p> Would you have guessed that I 'm a stamp collector ? Just kidding . I 'm not . But let 's play a little game of pretend . Let 's pretend that we have a huge dataset of stamp images . And we want to take two arbitrary stamp images and compare them to determine if they are identical , or near identical in some <p> Tragic . Heartbreaking . Unbearable . These are the three words that I would use to describe my past week . About a week ago , a close childhood friend of mine passed away in a tragic car accident . I went to elementary school and middle school with him . We spent our summers skateboarding on my driveway and the winters <p> Remember my friend James- from my post on skin detection ? Well , @ @ @ @ @ @ @ @ @ @ at the local Whole Foods , all while discussing the latest and greatest CVPR papers . After I checked-out , - I whipped out my iPhone and took a picture of the receipt- - its just something @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485178 @185178/ <p> If you have n't noticed , the term " feature vector " is used quite often in this blog . And while we 've seen it a lot , I wanted to dedicate an entire post to defining what exactly a feature vector is . <h> What is an Image Feature Vector ? <p> Image Feature Vector : An abstraction of an image used to characterize and numerically quantify the contents of an image . Normally real , integer , or binary valued . Simply put , a feature vector is a list of numbers used to represent an image . <p> As you know , the first step of building any image search engine is to define what type of image descriptor you are going to use . Are you trying to characterize the color of an image and extracting color features ? The texture ? Or the shape of an object in an image ? <p> Once you have selected an image descriptor , you need to apply your image descriptor to an image . This image descriptor handles the logic necessary to quantify an image and represent @ @ @ @ @ @ @ @ @ @ of your image descriptor is a feature vector : the list of numbers used to characterize your image. - Make sense ? <h> Two Questions to Ask Yourself <p> Here is a general template you can follow when defining your image descriptors and expected output . This template will help ensure you always know what you are describing as well as what the output of your descriptor represents . In order to apply this template , you simply need to ask yourself two questions : <p> What image descriptor am I using ? <p> What is the expected output of my image descriptor ? <p> Let 's make this explanation a little more concrete and go through some examples . <p> If you 're a frequent reader of this blog , you know that I have an obsession with both Jurassic Park and Lord of the Rings . Let 's introduce my third obsession : Pokemon . Below is our example image that we will use throughout this blog post a Charizard . <p> Figure 1 : Our example image a Charizard . <p> Now , fire up a Python shell and follow @ @ @ @ @ @ @ @ @ @ <p> Python <p> 1 <p> 2 <p> 3 <p> 4 <p> &gt;&gt;&gt;importcv2 <p> **29;7325;TOOLONG " charizard.png " ) <p> &gt;&gt;&gt;image.shape <p> ( 198,254,3 ) <p> Here we are just importing cv2 , our Python package that interfaces with OpenCV . We then load our Charizard image off of disk and examine the dimensions of the image . <p> Looking at the dimensions of the image we see that it has a height of 198 pixels , a width of 254 pixels , and 3 channels one for each of the Red , Green , and Blue channels , respectively . <h> Raw Pixel Feature Vectors <p> Arguably , the the most basic color feature vector you can use is the raw pixel intensities themselves . While we do n't  normally use this representation in image search engines , it is sometimes used in machine learning and classification contexts , and is worth mentioning . <p> Let 's ask ourselves the two questions mentioned in the template above : <p> What image descriptor am I using ? I am using a raw pixel descriptor . <p> What is the excepted output of @ @ @ @ @ @ @ @ @ @ raw RGB pixel intensities of my image . <p> Since an image is represented as NumPy array , its quite simple to compute the raw pixel representation of an image : <p> Image Feature Vectors in Python and OpenCV <p> Python <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> **31;7356;TOOLONG <p> &gt;&gt;&gt;raw.shape <p> ( 150876 , ) <p> &gt;&gt;&gt;raw <p> LONG ... <p> We can now see that our image has been " flattened " via NumPys flatten method . The Red , Green , and Blue components of the image have been flattened into a single list ( rather than a multi-dimensional array ) to represent the image . Our flattened array has a shape of 150,876 because there exists 198 x 254 = 50,292 pixels in the image with 3 values per pixel , thus- 50,292 x 3 = 150,876 . <h> Color Mean <p> Our previous example was n't very interesting . <p> What if we wanted to quantify the color of our Charizard , without having to use the entire image of raw pixel intensities ? <p> A simple method to quantify the color @ @ @ @ @ @ @ @ @ @ of the color channels . <p> Again , let 's fill out our template : <p> What image descriptor am I using ? A color mean descriptor . <p> What is the expected output of my image descriptor ? The mean value of each channel of the image . <p> And now let 's look at the code : <p> Image Feature Vectors in Python and OpenCV <p> Python <p> 1 <p> 2 <p> 3 <p> **33;7389;TOOLONG <p> &gt;&gt;&gt;means <p> LONG ... <p> We can compute the mean of each of the color channels by using the cv2.mean method . This method returns a tuple with four values , our color features . The first value is the mean of the blue channel , the second value the mean of the green channel , and the third value is the mean of red channel . Remember , OpenCV stores RGB images as a NumPy array , but in reverse order . We actually read them backwards in BGR order , hence the blue value comes first , then the green , and finally the red . <p> The fourth value can be @ @ @ @ @ @ @ @ @ @ can be used internally . This value can be ignored as such : <p> Image Feature Vectors in Python and OpenCV <p> Python <p> 1 <p> 2 <p> 3 <p> **25;7424;TOOLONG <p> &gt;&gt;&gt;means <p> LONG ... <p> Now we can see that the output of our image descriptor ( the cv2.mean function ) is a feature vector with a list of three numbers : the means of the blue , green , and red channels , respectively . <h> Color Mean and Standard Deviation <p> Let 's compute both the mean and standard deviation of each channel as well . <p> What is the expected output of my image descriptor ? The mean and standard deviation of each channel of the image . <p> And now the code : <p> 7 <p> LONG ... <p> &gt;&gt;&gt;means , stds <p> ( array ( 181.12238527 , <p> 199.1831504 , <p> 206.51429651 ) , array ( 80.67819854 , <p> 65.41130384 , <p> 77.77899992 ) ) <p> In order to grab both the @ @ @ @ @ @ @ @ @ @ the cv2.meanStdDev function , which not surprisingly , returns a tuple one for the means and one for the standard deviations , respectively . Again , this list of numbers serves as our color features . <p> Let 's combine the means and standard deviations into a single color feature vector : <p> Image Feature Vectors in Python and OpenCV <p> Python <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> &gt;&gt;&gt;importnumpy asnp <p> LONG ... <p> &gt;&gt;&gt;stats <p> LONG ... <p> 65.41130384,77.77899992 ) <p> Now our feature vector stats has six entries rather than three . We are now representing the mean of each channel as well as the standard deviation of each channel in the image . <p> Here we have a 3D histogram with 8 bins per channel . Let 's examine the shape of our histogram : <p> Image Feature Vectors in Python and OpenCV <p> Python <p> 1 <p> 2 <p> &gt;&gt;&gt;hist.shape <p> ( 8,8,8 ) <p> Our histogram has a shape of ( 8 , 8 , 8 ) . How can we use this as a feature vector if its multi-dimensional ? <p> @ @ @ @ @ @ @ @ @ @ Python and OpenCV <p> Python <p> 1 <p> 2 <p> 3 <p> **31;7451;TOOLONG <p> &gt;&gt;&gt;hist.shape <p> ( 512 , ) <p> By defining our image descriptor as a 3D color histogram we can extract a list of numbers ( i.e. our feature vector ) to represent the distribution of colors in the image . <h> Summary <p> In this blog post we have provided a formal definition for an image feature vector . A feature vector is an abstraction of the image itself and at the most basic level , is simply a list of numbers used to represent the image . We have also reviewed some examples on how to extract color features . <p> The first step of building any image search engine is to define your image descriptor . Once we have defined our image descriptor we can apply our descriptor to an image . The output of the image descriptor is our feature vector . <p> We then defined a two step template that you can use when defining your image descriptor . You simply need to ask yourself two questions : <p> What image descriptor @ @ @ @ @ @ @ @ @ @ of my image descriptor ? <p> The first question defines what aspect of the image you are describing , whether its color , shape , or texture. - And the second question defines what the output of the descriptor is going to be after it has been applied to the image . <p> Using this template you can ensure you always know- what you are describing and- how you are describing it . <p> Finally , we provided three examples of simple image descriptors and feature vectors to make our discussion more concrete . <p> What you describe ( color histogram or global mean RGB ) is referred to as an " image descriptor " in my social circles while " feature descriptor " is reserved for a descriptor centered around a local image point . The word " feature " is actually all over the place , but most CV researchers I know think of SIFT as the best example of a " feature descriptor " or just " descriptor " and GIST as the best example of " image descriptor . " <p> " Feature vector " or @ @ @ @ @ @ @ @ @ @ of some data processing and will be used as input to a machine learning algorithm . I 'm pretty sure the experts ( like us ) just throw around these words loosely , but a novice might be intimidated by our vocabulary . <p> In addition to going through your tutorials , people seriously interested in computer vision need to make computer vision friends ( or go to graduate school ) and regularly talk about these things over coffee . <p> Hi Tomasz , thanks for commenting . You bring up a really good point . There certainly is a difference in terminology between an " image descriptor " , " descriptor " , and " feature vector " . <p> I remember back when I was an undergrad taking my first machine learning course . I kept hearing the term " feature vector " and as you suggested , it was a bit intimidating . In all reality , its just a list of numbers used to abstract and quantify an object . That was the point I was trying to get across in this post trying to make a @ @ @ @ @ @ @ @ @ @ simple example . <p> Perhaps one of my next blog posts should disambiguate between the terms and ( hopefully ) provide a little more clarity . I can definitely see how all these similar terms can cause some confusion . <p> I am a bit confusing about the meaning of the terms , following are my understanding <p> image descriptor == data get from the whole image feature descriptor == data get from the local point of an image ( a small region ) descriptor == feature descriptor feature vector == a bunch of data which could feed to the machine learning algo <p> sometimes feature descriptor == feature vector because they could feed to the machine learning algo and extract from the local point of an image <p> Indeed , the terminology can get a bit confusing at times , especially since the terms can be used interchangeably and the only way to figure out which-is-which is via context ! However , it looks like you understand it quite well . An image descriptor is applied globally and extracts a single feature vector . Feature descriptors on the other @ @ @ @ @ @ @ @ @ @ You 'll get multiple feature vectors from an image with feature descriptors . A feature vector is a list of numbers used to abstractly quantify and represent the image . Feature vectors can be used for machine learning , building an image search engine , etc . <p> Hi , There are something that I do n't  understand if the different extracted features are not the same size , like a monodimensional feature and a vector feature , how could I merge this features on a vector to do after a selection feature ? Because if the features have not the same size , my method to do the features ranking is not able to know it . Do you understand my question ? ? <p> If your feature vectors are not the same dimensionality , then in general , it does n't  make much sense to compare them . If you perform feature selection , you 'll want to select the same features ( i.e. , columns ) from every feature vector in your dataset . <p> If I am taking a gray-scale multiple images and using them to form a @ @ @ @ @ @ @ @ @ @ will range from 0-255 . Each pixel will have different value depending on the image . For example , pixel at 0th position will have value 255 in one image , 128 in another , 3 in another , and so on . We will have an intensity distribution for each pixel from multiple images rather than a single value . Then , how can I decide which pixel values to use for feature vectors ? <p> Hey Ankit I 'm not sure exactly what you are trying to accomplish . It sounds like you need to construct a look-up table for each pixel value to determine its proper value . If you can explain a little bit more about what your end goal is , and why youre doing this , it would be helpful . <p> Thank you for replying . I have to detect pedestrian in a gray-scale image . For the feature selection part , I 'm taking all the grey pixel values in an image as a feature vector . From the training data set for pedestrian and non-pedestrian , I find the intensity distribution for all @ @ @ @ @ @ @ @ @ @ I calculate mean vector and co-variance matrix for pedestrian and non-pedestrian class . My end goal is to classify pedestrian using the above information . The part where Im confused is how can you build a co-variance matrix if each pixel is a intensity distribution rather than a single value ? Do you think my approach is right ? <p> Very good article , thank you for writing these posts . In a project I am doing right now I try to detect the material of objects . Basically , the program has to detect if the object has a copper coating or is made out of some brass-like material . I am using 3D histograms like mentioned in your tutorial right now , and compare them to histograms of sample images I have saved previously using cv2.compareHist with cv2.HISTCMPCORREL . Unfortunately , this seems to give quite unreliable results with changing lighting conditions in the image . Is there a better approach I should be using here ? I was looking into kNN-Classification and SVMs , but these methods seem a bit complicated for this ( I assume ) @ @ @ @ @ @ @ @ @ @ working on classifying normal/hemorrhagic brain trauma from CT . Tried GLCM computation based feature extraction on 8-bit grayscale CT image . For a sample of 10 datasets too , it does n't  show any much variation to classify . Is the approach fine ? or any other method would work well ? <p> Hi .. I figured it out . I 'm able to extract GLCM features and it varies for healthy and abnormal images after certain pre-processing.Now I have four set of features . How to feed this into SVM to classify using OpenCV Python ? ? Any snippet @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485179 @185179/ <h> Archive Resources <p> Over the past three years running PyImageSearch.com I have received and answered tens of thousands of questions from readers- just like yourself who are interested in studying computer vision , OpenCV , and deep learning . Looking back on this time , I can say that the vast majority of the questions I have answered have been a- real pleasure to <p> Todays blog post is part one of a two-part series inspired by an email I received over the weekend from Aarav , a PyImageSearch reader who is interested in studying computer vision : SUBJECT : How do I ask good computer vision questions ? Hey Adrian , My name is Aarav . First , I just want to say how much I <p> Each week I receive and respond to at least 2-3 emails and 3-4 blog post comments regarding NoneType- errors in OpenCV and Python . For beginners , these errors can be hard to diagnose by definition they are n't  very informative . Since this question is getting asked so often I decided to dedicate an entire blog post <p> Todays @ @ @ @ @ @ @ @ @ @ trend of Deep Learning tutorials here on the PyImageSearch blog and instead focus on a topic that I 've been receiving a ton of- emails about lately - common errors when using the Raspberry Pi camera module . I want to start this post by mentioning Dave Jones , <p> In last weeks blog post , I discussed my investment in an NVIDIA DIGITS DevBox for deep learning . It was quite the investment , weighing in at a staggering ( and wallet breaking ) $15,000- more than I ever thought I would spend on a computer that lives in my office ( I normally like hardware that exists in the <p> here 's a common question I get asked on the PyImageSearch blog : How do I make a Python + OpenCV script start as soon as my system boots up ? There are many ways to accomplish . By my favorite is to use crontab and the @reboot option . The main reason I like this method so much is <p> A few weeks ago , I demonstrated how to order the- ( x , y ) -coordinates of a @ @ @ @ @ @ @ @ @ @ skill that is critical in many computer vision applications , including ( but not limited to ) perspective transforms and computing the dimensions of an object in an image . One PyImageSearch reader emailed in , <p> As we all know , OpenCV 3.0 was officially released back in June of 2015 . This new update incorporated- a ton of new features and optimizations , including- Python 3 bindings . But the big question on everyones mind is : " Should I switch to OpenCV 3 ? If so , when should I switch ? " Deciding when or even if you should switch <p> I think a better title for this blog post might be : - How I lost a day of productivity to Ubuntu , virtual- environments , matplotlib , and rendering backends . Over the weekend I was playing around with deep learning on my Ubuntu system and went to plot the accuracy scores of my classifier . I coded up a quick Python script <p> You know what makes for a ( not so ) fun weekend ? Reconfiguring and reinstalling OSX on your MacBook @ @ @ @ @ @ @ @ @ @ use when I 'm traveling decided to shit the bed . No worries though , I use Carbon Copy Cloner and Backblaze , so no data was lost . And to @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485180 @185180/ <h> Tag Archives machine learning <p> In todays blog post , I interview Davis King , the creator and chief maintainer of dlib- a toolkit for real-world machine learning , computer vision , and data analysis in C++ ( with Python bindings included , when appropriate ) . I 've personally used dlib in a number of projects ( especially for object detection ) , so its quite the honor to be interviewing <p> In last weeks blog post , we discussed- gradient descent , a first-order optimization algorithm that can be used to learn a set of classifier coefficients for parameterized learning . However , the " vanilla " implementation of gradient descent can be prohibitively slow to run on large datasets in fact , it can even be considered- computationally wasteful . Instead , we should apply- Stochastic <p> Every relationship has its building blocks . Love . Trust . Mutual respect . Yesterday , I asked my girlfriend of 7.5 years to marry me . She said yes . It was quite literally the happiest day of my life . I feel like the luckiest guy in @ @ @ @ @ @ @ @ @ @ but also because this incredible PyImageSearch <p> If you 've been following along with this series of blog posts , then you already know what a- huge fan I am of Keras . Keras is a super powerful , easy to use Python library for building neural networks and deep learning networks . In the remainder of this blog post , I 'll demonstrate how to build a simple neural <p> In previous tutorials , I 've discussed two important loss functions : - Multi-class SVM loss and- cross-entropy loss ( which we usually refer to in conjunction with Softmax classifiers ) . In order to to keep our discussions of these loss functions straightforward , I purposely left out an important component : - regularization . While our loss function allows us to determine how well ( or poorly ) our <p> Last week , we discussed Multi-class SVM loss ; specifically , the hinge loss and squared hinge loss functions . A loss function , in the context of Machine Learning and Deep Learning , allows us to quantify how " good " or " bad " a given classification function @ @ @ @ @ @ @ @ @ @ at correctly classifying data points in our dataset . However , <p> A couple weeks ago , we discussed the concepts of both- linear classification- and- parameterized learning . This type of learning allows us to take a set of input data and class labels , and actually learn a function that- maps the input to the output predictions , simply by defining a set of parameters and optimizing over them . Our linear classification tutorial focused <p> In last weeks post , I introduced the k-NN machine learning algorithm which we then applied to the task of image classification . Using the k-NN algorithm , we obtained- 57.58% classification accuracy on the Kaggle Dogs vs . Cats dataset challenge : The question is : - " Can we do better ? " Of course we can ! - Obtaining higher accuracy for nearly any machine learning algorithm <p> Now that we 've had a taste of Deep Learning and Convolutional Neural Networks in last weeks blog post on LeNet , were going to take a step back and start to study machine learning in the context of image classification in @ @ @ @ @ @ @ @ @ @ Neighbor ( k-NN ) classifier , arguably the- most simple , easy <p> So in last weeks blog post we discovered how to construct an image pyramid . And in todays article we are going to extend that example and introduce the concept of a sliding window . Sliding windows play an integral role in object classification , as they allow us to localize exactly- " where " in @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485181 @185181/ <h> Object tracking in video <p> Object tracking in video <p> If you 've ever read my book , - Practical Python and OpenCV + Case Studies , you 'll know that I really enjoy performing object detection/tracking using color-based methods . While it does not work in all situations , if you are able to define the object you want to track- in terms of color , you can enjoy : <p> A highly simplified codebase . <p> Super fast object tracking that can run in super real time , easily obtaining 32+ FPS on modern hardware systems . <p> In the remainder of this lesson , I 'll detail an extremely simple Python script that can be used to both- detect and- track- objects of different color in an image . Well start off by using two separate objects , but this same method can be extended to an arbitrary number of colored objects as well . <h> Objectives : <p> In this lesson , we will : <p> Define the- lower and- upper boundaries of the colored objects we want to detect in the HSV color space . <h> @ @ @ @ @ @ @ @ @ @ lesson is to learn how to detect and track objects in video streams based primarily on their color . While defining an object in terms of color boundaries is not always possible , whether due to lighting conditions or variability of the object(s) , being able to use simple color thresholding methods allows us to easily and quickly perform object tracking . <p> Let 's take a look at a single frame of the video file we will be processing : <p> Figure 1 : An example frame from our source video where well be tracking two separate balls : a blue one and a green one . <p> As you can see , we have two balls in this image : a- blue one and a- green one . Well be writing code that can track each of these balls separately as they move around the video stream . <p> Let 's @ @ @ <p> 22 55203 @qwx675203 55218 @qwx675218 55219 @qwx675219 <p> importcv2 55202 @qwx675202 55206 @qwx675206 <p> ap.addargument ( " -v " , " --video " , help= " path to the ( optional ) video file " ) <p> While the first few lines of our script are n't  too exciting , Lines 12-14 are especially intriguing this is where we define the lower and upper boundaries of both the- green and- blue colors in the HSV color space , respectively . <p> A color will be considered green if the following three tests pass ; <p> The Hue value- H is : <p> The Saturation value- S is : <p> The Value- V is : <p> Similarly , a color will be considered- blue if : <p> The Hue value- H is : <p> The Saturation value- S is : <p> The Value- V is : <p> Obviously , the Hue value is extremely important in discriminating between these color ranges . <p> As a side note , whenever I teach color-based thresholding and tracking , the first question I am always asked is , - " How did you @ @ @ @ @ @ @ @ @ @ ? " I 'm not going to answer that question just yet since we are in the middle of a code explanation . If you want to know the answer now , read- the- How did you know what these color ranges were ? - sub-section at the end- of this lesson . <p> Finally , - Lines 17 and 18 handle if we are accessing our webcam video stream , whereas- Lines 21 and 22 will grab a pointer to our video file if a valid --video- path is indicated <p> 45 <p> 46 <p> # keep looping <p> whileTrue : <p> # grab the current frame <p> ( grabbed , frame ) =camera.read() <p> # if we are viewing a video and we did not grab a frame , then we have <p> # reached the end of the video <p> ifargs.get @ @ @ @ @ @ @ @ @ @ # resize the frame , blur it , and convert it to the HSV color space <p> **26;7484;TOOLONG , width=600 ) <p> **30;7512;TOOLONG , ( 11,11 ) , 0 ) <p> hsv=cv2.cvtColor ( frame , cv2.COLORBGR2HSV ) <p> # loop over the color ranges <p> for ( lower , upper , colorName ) incolorRanges : <p> # construct a mask for all colors in the current HSV range , then <p> # perform a series of dilations and erosions to remove any small <p> # blobs left in the mask <p> mask=cv2.inRange ( hsv , lower , upper ) <p> mask=cv2.erode ( mask , None , iterations=2 ) <p> mask=cv2.dilate ( mask , None , iterations=2 ) <p> On- Line 25 , we start looping over each frame of our video stream . We then grab the frame- from the stream on- Line 27 . <p> Well process the frame a bit on- Lines 35-37 , first by resizing it , then by applying a Gaussian blur to allow us to focus on the actual " structures " in the frame ( i.e. , the colored balls ) , followed @ @ @ @ @ @ @ @ @ @ 40 starts looping over each of the individual color boundaries in the colorRanges- list . <p> We obtain a mask for the current color range on- Line 44 by calling the cv2.inRange- function , passing in the current frame- , along with the respective lower- and upper- boundaries . We then perform a series of erosions and dilations to remove any small " blobs " in the image . <p> After this initial processing , our frame- looks like this : <p> Figure 2 : Obtaining two separate masks for each ball . <p> On the- left , we have drawn the minimum enclosing circles surrounding each of the balls , whereas on the- right , we have the clean segmentations of each ball . <p> Let 's take a look at another example of ball segmentation to solidify this point : <p> Figure 3 : Again , we are able to construct masks for each of the balls , even though both balls are moving . <p> The rest of the code in this example is quite straightforward and nothing we have n't seen before <p> 77 <p> 78 <p> # find contours in the mask <p> LONG ... 55211 @qwx675211 <p> # only proceed if at least one contour was found <p> iflen(cnts)&gt;0 : <p> # find the largest contour in the mask , then use it to compute <p> # the minimum enclosing circle and centroid <p> c=max ( cnts , key=cv2.contourArea ) <p> ( ( x , y ) , radius ) **26;7544;TOOLONG <p> M=cv2.moments(c) <p> LONG ... <p> # only draw the enclosing circle and text if the radious meets <p> # a minimum size <p> ifradius&gt;10 : <p> LONG ... <p> LONG ... <p> 1.0 , ( 0,255,255 ) , 2 ) <p> # show the frame to our screen <p> cv2.imshow ( " Frame " , frame ) <p> **27;7572;TOOLONG <p> # if the ' q ' key is pressed , stop @ @ @ @ @ @ @ @ @ @ <p> break <p> # clean up the camera and close any open windows <p> camera.release() <p> cv2.destroyAllWindows() <p> Line 49 and 50 finds the contours of the blob in the mask- . Provided that at least one contour was found , well find the largest one according to its area on- Line 56 . <p> Now that we have the largest contour of the mask , which we presume to be our ball , we compute the minimum enclosing circle , followed by the center- ( x , y ) -coordinates of the ball ( Lines 56-59 ) . If you need a refresher on the minimum enclosing circle and centroid properties of a contour , please refer to the- simple contour properties lesson of this course . <p> Provided that the largest contour region detected in mask- is sufficiently large , we then draw the bounding circle and the color name on the frame ( Lines 63-66 ) . <p> The remainder of the code simply displays the frame to our screen , handles if a key is pressed , and performs housekeeping on the video stream pointer . @ @ @ @ @ @ @ @ @ @ coded up our track.py- file , let 's give it a run : <p> track.py <p> Shell <p> 1 <p> $python track.py--video BallTracking01.mp4 <p> We- can see the output video below : <p> Let 's try another example , this time using a different video : <p> track.py <p> Python <p> 1 <p> $python track.py--video BallTracking02.mp4 <h> How did you know what the- color ranges were ? <p> In general , there are two methods to determine the valid color range for any color space . The first is to consult a color space chart , such as the following one for the Hue component of the HSV color space : <p> Figure 4 : An example of the range of colors values in the Hue component . Notice how values fall in the range 0 , 360 . Simply divide by 2 to bring values into the range 0 , 180 which is what OpenCV expects . <p> Based on- this chart , you can define what color and shades your object falls into . <p> However , I dislike using this method . As we know , colors can appear @ @ @ @ @ @ @ @ @ @ I recommend a more scientific approach of gathering data , performing experiments , and validating . <p> Luckily , this process is n't too challenging . <p> All you need to do is gather images(s)/video of your sample- objects in the desired lighting conditions , then experimentally tune the threshold values until you are able to detect- only the object , ignoring the rest of the contents of the image . <p> If you are using the imutils Python package , then I suggest taking a look at the range-detector script- in the bin- directory . This script , created by PyImageSearch Gurus member Ahmet Sezgin Duran , can be used to determine the valid color ranges for an object using a simple GUI and sliders : <p> Figure 5 : Using the range-detector script to filter objects in images based on the color values in the HSV color space . <p> Whether you use the range-detector- script or the old-fashioned method of guess , check , and validate , determining valid color ranges will likely be a tedious , if not challenging , process at first . However , it @ @ @ @ @ @ @ @ @ @ Summary <p> In this lesson , we learned how to track objects ( specifically balls ) in a video stream based on their- color . While it is not always possible to detect and track an object based on its color , it does yield two significant benefits , including ( 1 ) a small , simple codebase and ( 2 ) extremely speedy and efficient tracking , obtaining super real time performance . <p> The real trick when tracking an object based on- only its color is to determine the valid ranges for the given shade . I suggest using the HSV or L*a*b* color space when defining these ranges as they are much more intuitive and representative of how humans see and perceive color . The RGB color space is often not a great choice , unless you are working under- very tightly controlled lighting conditions or you only have to track one object rather than multiple . <p> Determining- the appropriate color range for a given object can be accomplished either experimentally ( i.e. , guessing , checking , and adapting ) or by using a tool @ @ @ @ @ @ @ @ @ @ Downloads : <p> Once you join PyImageSearch Gurus the lesson downloads will be made available . See you on the inside ! 
@@71485182 @185182/ <h> OpenCV 3 adoption rate <p> As we all know , OpenCV 3.0 was officially released back in June of 2015 . This new update incorporated- a ton of new features and optimizations , including- Python 3 bindings . <p> But the big question on everyones mind is : " Should I switch to OpenCV 3 ? If so , when should I switch ? " <p> Deciding when or even if you should switch to OpenCV 3 is n't necessarily the easiest decision , - especially- ( 1 ) if you are deploying OpenCV to a production environment or ( 2 ) you 're doing research in the computer vision space . <p> In order to help answer whether or not you should switch to OpenCV 3 ( along with " when " you should make the transition ) , I emailed a portion of PyImageSearch readers and asked them to take a quick four question survey on their OpenCV usage . <p> The results are quite insightful but I 'll leave you to draw your own conclusions from the results before I share my key takeaways . <h> OpenCV 3 @ @ @ @ @ @ @ @ @ @ of the most active PyImageSearch readers and asked them to answer a short , 4 question survey about their OpenCV usage. - I received 431 responses which I have gathered here today. - For most questions , readers were allowed to select- multiple responses for each answer that was applicable to them . <h> Question #1 : Which version of OpenCV are you currently using ? <p> OpenCV 2.4 . X <p> OpenCV 3.0 ( includes beta , RC , and official release ) <p> Other <p> The purpose of this question was simply to establish a baseline on which version of OpenCV most people- were using . As expected , OpenCV 2.4 . X dominates OpenCV 3.0 : <p> Figure 1 : OpenCV 2.4 . X is currently being used over 2.65x more than OpenCV 3 <p> Not surprisingly , most people are still using OpenCV 2.4 . X. However , 29% of developers , researchers , and programmers have already started using OpenCV 3.0 in some capacity . For a brand new major release of a library to achieve 29% usage in only a few short months is @ @ @ @ @ @ @ @ @ @ <p> Yes , I believe it will. - However , I think it will take another year for us to see OpenCV 3.0 hit 50% adoption rate , putting it equal with OpenCV 2.4 . X. <p> The primary reason for this is because OpenCV 2.4 is- still the de facto standard for computer vision development . OpenCV 2.4 has been around longer . Its more stable . Its had more bug patches applied . And its currently deployed to production environments and research labs around the world where the cost of switching is non-trivial and potentially quite expensive . <h> Question #2 : Which version of Python are you currently using ? <p> The fact that Python 2.7 is being used over 2.6x than Python 3 shouldnt come as a surprise . First , the Python- scientific community is reluctant to switch to Python 3- although that is now quickly changing given NumPy , SciPy , scikit-learn , and the awesome 2to3- tool paving the way for Python 3 adoption . <p> Secondly , OpenCV 2.4 . X was- only- compatible with Python 2.7. - It was n't until the @ @ @ @ @ @ @ @ @ @ <p> Simply put : if you were doing- any- development with OpenCV prior to June of 2015 , you were- most certainly using Python 2.7 . <p> In fact , its quite a surprise to see that OpenCV users have reached 31% Python 3 usage ! I would have guessed a far lower percentage of computer vision developers would be using Python 3 . But then again , you could quite possibly be working on other projects unrelated to computer vision where the libraries are Python 3 compatible . <p> All that said , given the OpenCV 3 and Python 3+ integration , I fully expect this number to rise over the next year . <h> Question #3 : What type of " setting " do you use OpenCV in ? <p> Home/hobby <p> Academic <p> Scientific <p> Production <p> Other <p> Again , users were allowed to select all answers that applied . Below you can see the results : <p> Figure 3 : Interestingly , of the 431 respondents , most developers are using OpenCV 3 in the " home/hobby " setting . <p> Its important to note @ @ @ @ @ @ @ @ @ @ you may be doing academic or scientific research , - does not mean that you can not come home at the end of the day and work on your hobby computer vision project ( in fact , I 'm willing to be that 's what a lot of us do ) . <p> Similarly , " academic " and " scientific " do have a significant amount of overlap . If you 're writing and publishing papers at your university , then you 're most certainly using OpenCV in an academic- setting . But you 're also conducting scientific research . <p> However , if you 're building a state-of-the-art computer vision product , then youre- performing- scientific research in a- production setting , but this research is n't necessarily- academic . <p> Personally , I do n't  see OpenCV 3 affecting these numbers much . <p> Home and hobby users will be more likely to play with OpenCV and take it for a spin , especially when they go to the OpenCV.org website and see that OpenCV 3.0 is the latest , stable version . <p> But in a production , scientific , or academic setting @ @ @ @ @ @ @ @ @ @ 3 is much higher given legacy code and other dependencies . Furthermore , if you 're doing scientific/academic research , you may be- reliant on OpenCV 2.4 to run legacy code associated with various experiments . <h> Question #4 : Do you plan on upgrading to OpenCV 3.0 ? <p> I am in no rush I will take my time and upgrade when the v3 release is more mature . <p> Personally , I 'm not too surprised by the response to this question . OpenCV is a- big , established library with lots of history. - It takes awhile to push out new releases , especially major ones . In fact , its been approximately- 6 years since the v2.0 release . And 3- years in between v2.3 and v2.4- talk about a long time in between- releases ! <p> Given that it takes awhile for new versions of the library to be released , it makes sense that the adoption rate is a bit slow as well . Were all- curious about the new version , but we may not- fully adopt the latest version until we ( 1 ) @ @ @ @ @ @ @ @ @ @ ( 2 ) we start a new project- can start from scratch without worrying about dependencies . <h> My takeaways <p> If I were to sum up my opinion in only a single sentence it would be : <p> Do n't  stress yourself out about switching to OpenCV 3.0 . <p> If you 're starting from scratch:If you 're starting a brand new project where you do n't  have to worry about dependencies and legacy code , there is no harm in switching to OpenCV 3.0 . In fact , in this particular case , I would encourage you to use OpenCV 3since you will increase adoption rates and push the library forward. - Just keep in mind that the library- will evolve and that if you want to use OpenCV 3.0- now , you might have to update your code later- when OpenCV 3.1 is released . <p> If you have an existing OpenCV project:Unless there is a new feature inside OpenCV 3 that is not available in 2.4 . X or you absolutely must have Python 3 support , it may be too early to migrate your entire codebase . @ @ @ @ @ @ @ @ @ @ in its infancy : there are still problems that need to be addressed and there are still bugs to be fixed . I would consider waiting until the v3.1 or even v3.2 release before you seriously consider making the big switch . <p> If you 're in an academic , scientific , or production setting:I would- advise against making the switch at this moment , - provided that you have an existing codebase of experiments . As I mentioned before , OpenCV 2.4 . X is still the- de facto standard for computer vision development . The 2.4 flavors- are much more mature and stable . And by sticking with the 2.4 version until v3 matures , you can save yourself a lot of headaches . <p> What is Adrian doing ? Personally , I have both OpenCV 2.4 and OpenCV 3 installed on my laptops . I use both of them daily , mainly so I can adjust to the OpenCV 3 environment ( not to mention , work with Python 3 ) and help field any questions related to differences in the versions. - But I am still using @ @ @ @ @ @ @ @ @ @ I will fully make the switch to OpenCV 3 but I do n't  see that realistically happening to the v3.1 or v3.2 release . <h> Summary <p> Recently , I sent out a survey to the most active PyImageSearch readers and asked whether or not they were planning on switching to OpenCV 3 . I received 431 responses to this survey and have presented the results in this blog post . <p> Overall , most readers are in no hurry to switch to OpenCV 3 . <p> Although I can reciprocate this feeling , I use both- OpenCV 2.4 and 3.0 daily . If you 're using OpenCV in a home/hobby environment , by all means , upgrade to OpenCV 3 and play with it . But if you 're in a production , academic , or scientific setting , I would consider waiting until the v3 release matures a bit . The primary exception to this being if you are starting a- brand new project where you do n't  any dependencies or legacy code in that case , I would encourage you to use OpenCV 3. <h> 11 Responses to OpenCV @ @ @ @ @ @ @ @ @ @ sense . I am new to the community , but I am using the 2.4 to get started with Python 2.7 due to the amount of information available , plus the fact that your bundle included a VM that was built to those versions . I would like to also learn to install the environment ( I do n't  really like VMs ) , and will look for a tutorial that will help me install this native to my PC , and will probably accomplish that with the 3.0 python and opencv 3 <p> You make an excellent point there are a ton of tutorials available for OpenCV 2.4 . Now that OpenCV 3 is released well start to see more OpenCV 3 tutorials , but like with anything , it will take a bit of time . If you decide to setup an OpenCV 3 and Python 3 development environment , be sure to consult this page for detailed instructions for your operating system . <p> I converted a fairly stable Python app still in development ( going into alpha ) to 3.0 over the weekend . The conversion @ @ @ @ @ @ @ @ @ @ changes being changing CVAA to LINEAA in a few draw text functions , and then dealing with how cv2.findContours now returns three values . What is surprising , however , is that I am experiencing some delays in performance . <p> I appear to be getting the same fps throughput , but the delay on screen as I am processing ( and showing ) frames from the video is noticeably slower . I am even experiencing a few instances where I appear to " drop " frames as it appears someone will freeze in place , and then appear a long way again when the video catches up . <p> I also converted to Jessie on this same test ( because what could be more fun than troubleshooting two major upgrades over a holiday weekend ! ( grin ) ) . To try and troubleshoot what I am seeing , I tested 2.4.1 on Jessie as well and I am not seeing the performance degradation . <p> So are there known degredations in 3.0 ? Anyone seeing things to watch out for , or things to program around ? <p> @ @ @ @ @ @ @ @ @ @ is that I 'll revert back to Jessie and 2.4.1 for our development environment . There are not any critical features needed in 3.0 ( yet ) , and its interesting to see a performance hit and I am not sure what 's causing it . <p> Hey Gary , thanks for the comment . As I mentioned in the post , I am still using OpenCV 2.4 in all of my production environments so I do n't  have any good benchmarks to compare the two ( yet ) . I personally cant justify this , so take it with a grain of salt , but Ive seen a few tweets and LinkedIn posts that also speculate that OpenCV 3 is a tad bit slower than 2.4 . Again , well need some real benchmarks to prove anything . <p> Also , keep in mind that OpenCV has many dependencies related to video and image I/O . It could very well be that a different version of the image or video I/O packages could be causing the slow down you are seeing . <p> Has anyone managed to figure out this problem @ @ @ @ @ @ @ @ @ @ 3 and python 2.7 . Its just cv2.imshow however , everything else is running at a reasonable pace , and Imshow works fine in opencv 2.4 <p> I am experimenting with OpencCV 3.1.0 and cascade classifiers . I am also using EmguCV3.1.0 and EmguCV2.4.xs on this project . From using EmguCV2.4.x I found out that HOG features could be used in cascade classifiers . <p> As I experimented with cascade training with OpenCV 3.1 , I found out that the HOG features are no longer supported by the cascade classifiers . So now I want to also install Open CV 2.4 , as I think HOG may be a better result for the type of objects I am detecting ( vehicles ) . <p> So my question is about your development environment . Have you installed both versions of OpenCV under the same machine ? If so , how did you achieve this ? <p> After looking at your ( excellent ) installation guide for OpenCV 3.1 , I thought a possible way would be to use python environments for this objective . Hence have an environment name cv24 and @ @ @ @ @ @ @ @ @ @ advise , if this approach ( or any other ) is possible . <p> Basically , the trick is to create a virtual environment for each of your OpenCV installs . You then download the OpenCV source code and create two builds one for OpenCV 3.1 and one for OpenCV 2.4 ; however , you do n't  run make install . You instead keep everything in your build/lib directory and delete the rest . You can then sym-link in the cv2.so bindings @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485184 @185184/ <h> Installing boost and boost-python on OSX with Homebrew <p> I really , really hope that someone finds this resource- useful . The amount of time I have wasted over the past few months ( passively ) trying to get boost and boost-python to install on my OSX machine- via Homebrew- has been nothing short of excruciating . <p> Do n't get me wrong , I love Homebrew . And if you are on an OSX machine and- are n't  using Homebrew , then I suggest you stop reading this post and install it right now . <p> Anyway , like I said , I hope that this post saves other people some time and hassle . And while this post is n't entirely- dedicated to computer vision , it is still very relevant if you are developing computer vision based applications using Python and OpenCV . <p> Update 4 May 2015 : - - Erik Bernhardsson- has released an update to Annoy that removes the dependency of Boost and Boost.Python from Annoy . You can now simply install Annoy using pip : pip install annoy- without any extra @ @ @ @ @ @ @ @ @ @ bindings so you can leverage the power of dlib from your Python shell . <p> Both Annoy and dlib are just two examples of packages that require the use of boost ( and boost-python if you want Python bindings ) . <p> Anyway , let 's go ahead and get this tutorial started I 've definitely wasted enough of my time working with this problem and I do n't  want to waste any of yours either ! <h> What is Homebrew ? <p> Homebrew is " the missing package manager for OSX " . It makes installing and managing packages not- installed by the default Apple installation a breeze , in the same manner that Debian apt-get- - does . <p> Note : Comparing Homebrew to apt-get is not entirely fair , but if this is the first time you are hearing of Homebrew , this comparison should suffice . <h> What is boost and boost-python ? <p> Boost is a collection of peer-reviewed ( i.e. very high quality ) C++ libraries that help programmers and developers not get caught up in reinventing the wheel . Boost provides implementations for linear algebra @ @ @ @ @ @ @ @ @ @ , just to name a few . <p> Again , these libraries are peer-reviewed and very high quality . A very large number of C++ applications , especially in the scientific space , rely on the Boost libraries in some way or another . <p> We also have- boost-python , which provides interoperability between the C++ and Python programming languages . <p> Note:This blog post was written- in January 2015 . Definitely head to the Homebrew homepage and use the latest install script provided by the Homebrew community . <h> Step 2 : Update Homebrew <p> Now that you have Homebrew installed , you need to update it and grab the latest package ( i.e. " formula " ) definitions . These formula are simply instructions on how to install a given library or package . <p> To update Homebrew , simply do : <p> Installing boost and boost-python on OSX with Homebrew <p> Shell <p> 1 <p> $brew update <h> Step 3 : Install Python <p> Its bad form to use the system Python as your main interpreter . And this is especially true if you intend on using @ @ @ @ @ @ @ @ @ @ install Python via brew : <p> Installing boost and boost-python on OSX with Homebrew <p> Shell <p> 1 <p> $brew install python <h> Step 4 : Installing boost <p> So far so good . But now its time to install boost . <p> And this is where you really need to start paying attention . <p> To install boost , execute the following command : <p> Installing boost and boost-python on OSX with Homebrew <p> Shell <p> 1 <p> $brew install boost--with-python <p> You see that --with-python- - flag ? <p> Yeah , do n't  forget that its important . <p> In my case , I figured that boost-python would already be installed , given the --with-python- - flag . <p> Apparently that 's not the case . You need to explicitly install boost-python as well . Otherwise , you 'll get the dreaded segfault error when you try to call a package from within Python that expects to find boost bindings . <p> Also , you might want to go take a nice little walk while boost downloads , compiles , and installs . Its a large library and if you @ @ @ @ @ @ @ @ @ @ like I am ) , then I highly suggest that you context switch and get some other work done . <h> Step 5 : Installing boost-python <p> Now that boost is installed , we can get boost-python installed as well : <p> Installing boost and boost-python on OSX with Homebrew <p> Shell <p> 1 <p> $brew install boost-python <p> The boost-python package should install a lot faster than boost , but you still might want to make yourself a cup of coffee , especially- if your system is slow . <h> Step 6 : Confirm boost and boost-python is installed <p> Make sure that both boost and boost-python are installed : <p> Installing boost and boost-python on OSX with Homebrew <p> Shell <p> 1 <p> 2 <p> 3 <p> $brew listgrep'boost ' <p> boost <p> boost-python <p> As you can see from my terminal output , both boost and boost-python have been successfully installed ( provided that you did n't  get any errors from the above steps , of course ) . <h> Already using Python- +- virtualenv ? Keep reading . <p> Oh , you thought we were done @ @ @ @ @ @ @ @ @ @ a mistake . <p> Because guess what ? If you already have Python installed and are using- virtualenv- ( and in my case , virtualenvwrapper ) , youve still got some work to do . <p> Note : If you are not already using virtualenv- - and virtualenvwrapper- - to manage your Python packages , this is something that- you should really look into . It makes your life substantially easier trust me . <h> New virtualenvs : <p> If you are creating a new virtualenv , you 'll be good to go . No extra work is required , everything will work smoothly out of the box . <h> Existing virtualenvs : <p> So let me tell you something you already know : When we construct a virtual environment , our Python executable , along with relevant libraries , includes , and site-packages are cloned and sequestered into their own independent environment . <p> And let me tell you something you might not know : If you already have your virtualenv setup- before compiling and installing boost and boost-python ( like I did ) , then you will not have @ @ @ @ @ @ @ @ @ @ best way to solve this problem ? <p> Honestly , I 'm not sure what the " best " way is . There has to be a more elegant method than what I 'm proposing . But here 's how I fixed the problem : <p> Generated a requirements.txt- - for my virtualenv <p> Deactivated and deleted my virtualenv <p> Recreated my- virtualenv <p> pip **26;7601;TOOLONG - that shit and be done with it <p> After you 've performed these steps your new virtualenv will have the boost-python bindings in place . And hopefully you wont have wasted as much time as I have . <h> An Annoy Example <p> Now that we have boost and boost-python installed , let 's take them for a test drive using the Annoy package . <p> Update 4 May 2015 : - - As I mentioned at the top of this post , - Erik Bernhardsson- has released an update to Annoy that removes the dependency of Boost and Boost.Python from Annoy . You can now simply install Annoy using pip without a having Boost or Boost.Python installed . <p> Now that our embedding is created , let 's @ @ @ @ @ @ @ @ @ @ first vector in our list : <p> Installing boost and boost-python on OSX with Homebrew <p> Python <p> 1 <p> 2 <p> **34;7629;TOOLONG <p> **35;7665;TOOLONG <p> We can also find nearest neighbors that are not already part of the index : <p> Installing boost and boost-python on OSX with Homebrew <p> Python <p> 1 <p> 2 <p> LONG ... <p> **39;7702;TOOLONG <p> So what would happen if you tried to execute this code without boost and boost-python installed ? <p> Your code would segfault during the getnnsbyitem- - and getnnsbyvector- - functions . And if you were using dlib , then you would segfault during the import . Definitely something to keep in mind . If you are segfault-ing during these functions , then something is funky with your boost and boost-python install . <h> Summary <p> In this blog post I reviewed how to install boost and boost-python on OSX using Homebrew . <p> Personally , - I wasted a ridiculous amount of time while passively working on this problem over- a few month period the goal of this article was to ( hopefully ) help you save time @ @ @ @ @ @ @ @ @ @ you know of a more elegant way to solve this problem , please let me know in the comments or shoot me an email ! <p> Hello ! The with-python switch to boost no longer does anything . Running just brew install boost-python will install both boost and Boost.Python for your hacking enjoyment , and they should run quite quickly since theyll give you precompiled binary packages by default now . <p> rpforest is really awesome = Although now annoy is 100% pip-installable . I think Eric ( the creator of annoy ) is planning on adding rpforest to his benchmark . My only gripe with rpforest is that it only allows for the cosine distance , not Euclidean which I find a little frustrating . <p> I personally do n't  use Boost or Boost.Python directly . I 've only used them in context of other libraries , such as annoy and dlib . That said , I 'll see if I can write a post on this topic in the future if there is enough interest . <p> Hi , I would like to know how does one link boost libraries @ @ @ @ @ @ @ @ @ @ MAC OS . I have attempted to install a radio astronomy data package but I need to link the boost libraries first before installing the package I am interested in . Please help <p> I have Ubuntu 16.04 LTS with ( default ) python interpreter 2.7.12 . I have also created a virtualenv ( OpenCV 3 + Python 3.5 ) using PyImageSearch tutorial ( http : //goo.gl/xMTMK7 ) , in which I have installed dlib . <p> I can import dlib in my virtualenv ( " cv " ) , without errors , but when I run a dlib example I get the following errors : <p> I have tried to follow Adrians advice on deleting and recreating the virtualenv , but because virtualenv is configured to use Python 3.5 ( instead of Ubuntu Python executable ) , the new virtualenv is always created with python 3.5 . So the new virtualenv gets the bindings for boost-python , but its using python 3.5 and dlib is expecting python 2.7 site-packages . <p> Hey Bogdan Based on the error message , it looks like dlib was ( likely ) compiled against @ @ @ @ @ @ @ @ @ @ import it . I would double-check your CMake command for dlib and ensure that you are compiling against the correct version of Python ( i.e. , the same one in @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485185 @185185/ <h> High-throughput feature extraction <p> High-throughput feature extraction <p> In our previous lesson , we got our first taste of running computer vision jobs on the Hadoop library using the MapReduce paradigm , Python , and the Hadoop Streaming API. - We also defined a- reusable project structure that can be utilized anytime we need to construct a MapReduce job . <p> To demonstrate the utility of our project structure , today we are going to take another look at running computer vision jobs on MapReduce this time , well apply feature extraction and quantify the contents of a set of images using keypoint detection and local invariant descriptors . <h> Objectives : <p> In this lesson , we will : <p> Define a MapReduce job that can be used to detect keypoints and extract local invariant descriptors . <h> High-throughput feature extraction <p> As we have seen throughout our CBIR module , keypoints and local invariant descriptors are critical for- building high accuracy and scalable image search engines . <p> However , the task of extracting features can be quite time consuming . Perhaps there is a way @ @ @ @ @ @ @ @ @ @ is ! <p> Feature extraction is inherently a task that can be made parallel . To start , you might want to use a multiprocessing library to split feature extraction across your system bus and multiple cores of your processors . <p> If that- still is n't enough speed , then you should consider setting up a Hadoop cluster to perform feature feature extraction. - There is a lot of overhead in running a MapReduce job but once you get the job started , it can process images extremely quickly . <p> In the remainder of this lesson , well review how to create an image description pipeline and run it on Hadoop . Specifically , well learn how to detect- Fast Hessian keypoints and extract- RootSIFT descriptors , just like we have done in our CBIR lessons only this time , well be leveraging the power of Hadoop and MapReduce . <h> Defining our project <p> To start , let 's briefly review our project structure : <p> Hadoop <p> 26 <p> 27 <p> ---deploy <p> ---pyimagesearch.zip <p> ---jobs <p> **28;7743;TOOLONG <p> ---output <p> ---pyimagesearch <p> ---init.py <p> ---descriptors <p> ---init.py <p> ---detectanddescribe.py <p> ---rootsift.py <p> ---hadoop <p> ---init.py <p> ---mapper <p> ---int.py <p> ---mapper.py <p> ---reducer <p> ---int.py <p> ---reducer.py <p> ---sbin <p> ---deploy.sh <p> ---startstack.sh <p> ---stopstack.sh <p> ---config.sh <p> **26;7773;TOOLONG <p> **28;7801;TOOLONG <p> **25;7831;TOOLONG <p> Notice how we are re-using many of the components of the framework that we defined in our previous lesson on- high-throughput face detection . In fact , the only- new additions to our project are : <p> descriptors- : A sub-module of pyimagesearch- containing the code we need to detect keypoints and extract features . <h> The feature extractor mapper <p> We are now ready to define the heart of our MapReduce job the **26;7858;TOOLONG . This file will accept input from the Hadoop Streaming utility , deserialize the image , detect keypoints , and extract local invariant descriptors , followed by emitting the output to @ @ @ @ @ @ 11 <p> 12 <p> # ! /usr/bin/env python 55203 @qwx675203 <p> importsys <p> # import the zipped packages and finish import packages <p> sys.path.insert ( 0 , " pyimagesearch.zip " ) <p> **31;7913;TOOLONG importMapper <p> **29;7946;TOOLONG importDetectAndDescribe <p> **29;7977;TOOLONG importRootSIFT 55219 @qwx675219 <p> importcv2 <p> Again , note the use of the shebang- at the top of our Python script to indicate that this is an executable file . We then import our pyimagesearch.zip- archive and subsequently import any required Python packages . <p> We are now ready to define our main job- method , used to read input from stdin and push output to stdout @ @ @ @ @ @ @ <p> # initialize the keypoint detector , local invariant descriptor , and the <p> # descriptor <p> # pipeline <p> **35;8035;TOOLONG " SURF " ) <p> descriptor=RootSIFT() <p> **30;8072;TOOLONG , descriptor ) <p> # loop over the lines of input <p> forline **30;8104;TOOLONG : <p> # parse the line into the image I 'd , path , and image <p> LONG ... <p> # describe the image and initialize the output list <p> image=cv2.cvtColor ( image , cv2.COLORBGR2GRAY ) <p> **26;8136;TOOLONG , width=320 ) <p> ( kps , descs ) =dad.describe(image) <p> output= <p> # loop over the keypoints and descriptors <p> for ( kp , vec ) inzip ( kps , descs ) : <p> # update the output list as a 2-tuple of the keypoint ( x , y ) -coordinates <h> Creating the MapReduce driver script <p> The last step is to define our driver shell script used to execute our feature extraction pipeline on Hadoop . <p> Let 's take a look at this <p> 20 <p> 21 <p> # ! /bin/sh <p> # grab the current working directory <p> BASE=$ ( pwd ) <p> # create the latest deployable package <p> sbin/deploy.sh <p> # change directory to where Hadoop lives <p> cd$HADOOPHOME <p> # ( potentially optional ) : turn off safe mode <p> bin/hdfs dfsadmin-safemode leave <p> # remove the previous output directory <p> bin/hdfs **33;8191;TOOLONG <p> # define the set of local files that need to be present to run the Hadoop <p> # job -- comma separate each file path <p> FILES= " **31;8226;TOOLONG , <p> **30;8259;TOOLONG " <p> Line 4 grabs the current working directory , which in this case should be the folder that contains our implementation of the feature extraction Hadoop project . We then construct the pyimagesearch.zip- archive on- Line 7 by calling deploy.sh- . <p> Next , we need to construct the set of local FILES- that need to be included with our MapReduce job . This comma separated list includes the **26;8291;TOOLONG file along with the archived pyimagesearch.zip- file . <p> Be sure @ @ @ @ @ @ @ @ @ @ feature extraction driver script executable <p> Python <p> 1 <p> **35;8319;TOOLONG <p> From there , we can execute the job ( provided that you have Hadoop and HDFS running , of course ) : <p> **25;8356;TOOLONG <p> Shell <p> 1 <p> **28;8383;TOOLONG <h> Verifying our results <p> After our MapReduce job has completed , we can verify the output by listing the directory contents on HDFS : <p> Checking the output of our feature extraction job <p> Shell <p> 1 <p> $bin/hdfs **31;8413;TOOLONG <p> Figure 1 : Viewing the output directory contents on HDFS . <p> As we can see , we have the SUCCESS- file , followed by many part-*- files . <p> From here , we could write a Python script to digest the keypoints and features into an HDF5 dataset , like we have done in our CBIR lessons . But for the time being , let 's look at a Python script that demonstrates we have successfully extracted the keypoints 16 <p> 17 <p> 18 55203 @qwx675203 <p> **29;8446;TOOLONG 55220 @qwx675220 55218 @qwx675218 55219 @qwx675219 <p> importrandom <p> importglob <p> importjson <p> importcv2 <p> # construct the argument parser and parse the arguments 55206 @qwx675206 <p> LONG ... to the features output directory " ) <p> LONG ... of features samples to use " ) 55208 @qwx675208 <p> # grab the list of parts files <p> LONG ... <p> Lines 2-9 import our required Python packages while- Lines 12-15 parse our required Python arguments . The first argument , --features-data- , is simply the path to the part-*- files residing locally on disk . The --sample- switch controls the number of images we are going to sample and visually verify . <p> From there , - Line 18 grabs the path to all part-*- files on disk @ @ @ @ @ # loop over the random number of images to sample <p> foriinrange ( 0 , args " sample " ) : <p> # randomly sample one of the features files , then load the output file <p> **27;8477;TOOLONG <p> **30;8506;TOOLONG ( " n " ) <p> # randomly sample a row and unpack it <p> row=random.choice(d) <p> LONG ... <p> **29;8538;TOOLONG <p> # load the image and resize it <p> image=cv2.imread(path) <p> **26;8569;TOOLONG , width=320 ) <p> orig=image.copy() <p> descs= <p> # loop over the keypoints and features <p> for ( kp , vec ) infeatures : <p> # draw the keypoint on the image , then update the list of descriptors <p> cv2.circle ( image , ( kp0 , kp1 ) , 3 , ( 0,255,0 ) , 2 ) <p> descs.append(vec) <p> # show the image <p> print ( " INFO : " . format ( path , np.array(descs).shape ) ) <p> cv2.imshow ( " Original " , orig ) <p> cv2.imshow ( " Keypoints " , image ) 55212 @qwx675212 <p> Line 21 randomly loops over a set of images to sample . We choose a part-*- @ @ @ @ @ @ @ @ @ @ ) . A row from the selected file is then randomly sampled and unpacked ( Lines 27-29 ) . <h> Summary <p> In this lesson , we learned how to use our Python + Hadoop Streaming API framework to detect keypoints and extract local invariant descriptors from a set of images . <p> In general , you 'll first want to parallelize- your feature extraction code across the system bus and multiple cores of your processors . Feature extraction is inherently a task that can be made parallel and it often runs very quickly on a single system with lots of available processor cores . <p> However , in the event that a single system is not fast enough , you should consider using big data tools such as Hadoop to parallelize feature extraction across multiple nodes in a Hadoop cluster . While there is a lot of overhead in actually- starting a Hadoop job ( hence why I recommend parallelizing across the system bus first ) , once the job is up and- running , it can chew through an entire dataset of images quickly . 
@@71485186 @185186/ <p> I am pleased to announce that both the Quickstart Bundle and- Hardcopy Bundle- of- Practical Python and OpenCV- now ship with a downloadable Raspbian . img file that comes with OpenCV- pre-configured and pre-installed . <p> This . img file is also- tiny , weighing in at only 1.7GB that 's approximately the same size as the official Raspbian release ! <p> All you have to do is download the . img file , flash it to your SD card , and boot your Pi . <p> From there , you 'll have a- complete Python + OpenCV development environment at your fingertips , all without the hassle of configuring , compiling , and installing OpenCV . <p> To learn more about the Raspbian . img file , - keep reading . <h> Raspbian + OpenCV out-of-the-box <p> I went back to my recent tutorials on installing OpenCV on the Raspberry Pi and computed the amount of time it takes to perform each step . <p> You know what I found ? <p> Even if you know- exactly what you are doing it can take over- 2.2 hours to compile @ @ @ @ @ @ @ @ @ @ then emailed a sample of novice readers who successfully installed OpenCV on their Pi and asked how long it took them to complete the compile and installation process . <p> Perhaps not surprisingly I found the amount of time for novice readers to install OpenCV on their Raspberry Pi jumped nearly- 4x to- over- 8.7 hours . <p> Clearly , the barrier to entry for many PyImageSearch readers trying to learn OpenCV and computer vision is getting OpenCV itself installed on their Raspberry Pi . <p> In an effort to help these readers get the most out of their Raspberry Pi , I have decided to release my own personal Raspbian . img file that has OpenCV- pre-configured and pre-installed . <p> By bundling the pre-configured Raspbian . img together with- Practical Python and OpenCV my goal is to : <p> If this does n't  sound like you , no worries , I totally understand I 'll be still be providing free tutorials to help you get OpenCV up and running on your Raspberry Pi . <h> Raspbian + OpenCV pre-configured and pre-installed <p> The rest of this document describes how @ @ @ @ @ @ @ @ @ @ included in your purchase of the- Quickstart Bundle or- Hardcopy Bundle of- Practical Python and OpenCV . <p> Note : The pre-configured Raspbian + OpenCV . img file is- not included in the Basic Bundle . <p> At the end of this guide you 'll also find answers to frequently asked questions regarding the Raspbian + OpenCV . img file . If you have a question that is not covered in FAQ , - please send me a message . <h> Booting your Pi for the first time <p> After writing the the Raspbian . img to your card , insert the card into your Pi and boot it up . On the first boot your Raspbian filesystem will be- auto-expanded to utilize the- entire- disk . <p> This means that you- do not have to run raspi-config=&gt;Expand Filesystem- manually . <p> After the auto-expansion has completed your Pi will reboot and you will be able to use it as normal ( the auto-expansion of the filesystem is only performed on the first boot ) . <p> Here is a screenshot of the disk utilization on my Pi after it has @ @ @ @ @ @ @ @ @ @ Raspberry Pi for the first time your filesystem will be expanded to utilize the entire disk . <p> Notice that my- entire 16GB card is being utilized . <h> Using Python and OpenCV on your Raspberry Pi <p> In order to get OpenCV 3 installed with- both Python 2.7 and Python 3 bindings I leveraged Python virtual environments . Each Python virtual environment is totally independent from one another ensuring there are no dependency or versioning issues . <p> In the remainder of this section I explain ( 1 ) what Python virtual environments are and ( 2 ) how to access the OpenCV + Python 2.7/Python 3 bindings . <h> What are Python virtual environments ? <p> At the very core , Python virtual environments allow us to create- isolated , independent environments for each of our Python projects . This implies that- each project can have its own set of- dependencies , - regardless of which dependencies another project has . <p> In the context of OpenCV , this allows us to have- one virtual environment for Python 2.7 and then- another virtual environment for Python 3 . @ @ @ @ @ @ @ @ @ @ and have their own set of dependencies for a given project . <p> For a detailed look at Python virtual environments please refer to this tutorial . <h> Accessing the Python 2.7 + OpenCV 3 environment <p> To access the Python 2.7 + OpenCV 3 virtual environment you need to run two commands : <p> Notice how the text ( py2cv3 ) - appears before your shell . This implies that you are in the py2cv3- virtual environment . This command only needs to be executed- once for each shell that you open up . <h> Accessing the Python 3 + OpenCV 3 environment <p> Similar to the section above , the Python 3 + OpenCV 3 environment can also be accessed using the source- and workon- commands : <p> Raspbian + OpenCV pre-configured and pre-installed <p> Shell <p> 1 <p> 2 <p> $source/.profile <p> $workon py3cv3 <p> You can also just use the following command located in your home directory : <p> Raspbian + OpenCV pre-configured and pre-installed <p> Shell <p> 1 <p> $source startpy3cv3.sh <p> This command only needs to be executed- once for each shell you open @ @ @ @ @ @ @ @ @ @ ( py3cv3 ) - now appears before the prompt indicating that you have access to Python 3 + OpenCV 3 : <p> You may also use your favorite SFTP/FTP client and transfer the code from your system to your Pi : <p> Figure 8 : Utilize a SFTP/FTP client to transfer the Practical Python and OpenCV code from your system to the Raspberry Pi . <p> Or you may want to manually write the code on the Pi using the built-in text editor as you follow along with the book : <p> Figure 9 : Using the built-in text editor that ships with the Raspberry Pi to write code . <p> I would suggest either downloading the- Practical Python and OpenCV source code via web browser or using SFTP/FTP as this also includes the- datasets utilized in the book as well . However , manually coding along is a- great way to learn and I highly recommend it ! <h> X11 forwarding with your Raspberry Pi <p> If you know how to use SSH then you can utilize X11 forwarding to access your Raspberry Pi . Simply provide the -X- @ @ @ @ @ @ @ @ @ @ <p> Raspbian + OpenCV pre-configured and pre-installed <p> Shell <p> 1 <p> $ssh-Xpi@youripaddress <p> You will then be able to execute Python examples that utilize cv2.imshow- and see the results on your laptop/desktop screen . <p> However , your X11 session will timeout/close so if you see the following error message : <p> Raspbian + OpenCV pre-configured and pre-installed . <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> WARNING**:Could notopenXdisplay <p> Unable toinit server:Could notconnect:Connection refused <p> Gtk-WARNING**:cannot open display:localhost:10.0 <p> Then simply close your SSH connection and re-connect . <p> Note : - Mac users may first need to install XQuartz- to utilize X11 forwarding . Windows users should check their respective SSH client documentation for instructions on how to use X11 forwarding . <h> Frequently Asked Questions ( FAQ ) <p> In this section I detail the answers to frequently asked questions regarding the Raspberry Pi . img file . <p> By all means , I encourage you to do so . Its a great exercise and you 'll learn- a lot about the Linux environment . I would suggest you follow one of my- many @ @ @ @ @ @ @ @ @ @ . <p> Again , this pre-configured Raspbian image is intended for readers- who want to skip the install process and jumpstart their education . <h> How long will it take to install Python + OpenCV by hand ? <p> I 've ran the numbers and even if you know- exactly what you are doing it will take a bare minimum of- 2.2 hours to compile and install OpenCV . <p> If you have never installed OpenCV before or you are not familiar with Linux-based environments that number can easily jump to- over 8.7 hours based on my survey of novice readers who successfully installed OpenCV on their Raspberry Pi . <p> It really comes down to- how much you value your time and- how quickly you want to get started learning computer vision . I always encourage you to use my free tutorials on installing OpenCV on the Raspberry Pi , but if you want to save yourself time ( and headaches ) , then definitely consider going with the pre-configured Raspbian . img . <h> Which- Practical Python and OpenCV bundles is the Raspbian image included in ? <p> The @ @ @ @ @ @ @ @ @ @ and- Hardcopy Bundle of- Practical Python and OpenCV . The pre-configured Raspbian image is- not included in the Basic Bundle . <h> After installing your distribution of Raspbian , how do I access Python + OpenCV ? <h> Is Wolframs Mathematica included in your Raspbian distribution ? <p> No , I am not legally allowed to distribute a modified version of Raspbian ( that is part of a product ) with Mathematica installed . <h> How did you reduce the size of the Raspbian image file ? <p> To start , I removed unneeded software such as Wolframs Mathematica and LibreOffice . Removing these two pieces of software alone saved nearly 1GB of space . <p> From there , the size of the main partition was reduced and was set to auto-expand on first boot . <h> What Python packages are installed on the Raspberry Pi ? <p> After accessing either the respective py2cv3- or py3cv3- virtual environments run pip freeze- to see a full list of Python packages installed . <p> In short , I have included- all necessary Python packages you will need to be successful executing the @ @ @ @ @ @ @ @ @ @ NumPy , SciPy , scikit-learn , scikit-image , mahotas , - and- many others . <h> Where can I learn more about Python virtual environments ? <p> My favorite resource and introduction to Python virtual environments can be found here . I also discuss them in the first half of this blog post . <h> 26 Responses to Raspbian + OpenCV pre-configured and pre-installed . <p> Excellent work Adrian , many thanks ! I am working with a group of students who are using a USB monochrome camera on a Pi-3 ; the camera supports 1394 protocol over USB-2 ( its a Point Grey Chameleon 2 ) , have you any examples using that interface with Python ? <p> Thanks Hugh . I do n't  have any examples that directly cover the firewire protocol , but I would instead look to see if the camera is compatible with the cv2.VideoCapture function . If it is , this is by far the easiest way to access frames from a video stream for peripheral cameras . <p> Dear Adrian thank you for your great explanation . I am about to start rasppery @ @ @ @ @ @ @ @ @ @ that raspian.zip file should we buy some kind of course to be able download it cause - could n't see a link to do so <p> I was waiting for this for some time . This is really helpful Adrian ! I have just finished installing OpenCV on a CHIP computer ( 9 bucks ) . Compiling takes even longer ( 9 h ) , so I preferred using apt-get , but I get only Opencv 2. x , which is fine ( at least for now ) . I wish Chip would have a way to backup and restore , so that I can just flash an *. img file like I can on RPi . Btw , do you think this img would run on RPi zero also ? <p> Let me boot up my Raspberry Pi Zero and test it out . I will edit this comment as soon as I get the result . <p> EDIT : Confirmed . I just booted up my Raspberry Pi Zero and confirmed that my Raspbian . img file will work with the Pi Zero as well . I have included @ @ @ @ @ @ @ @ @ @ demonstrating that Python can import the OpenCV bindings ) . <p> The Raspbian . img file is part of the of the Quickstart Bundle and Hardcopy Bundle of Practical Python and OpenCV . You would need to purchase one of these bundles to download the pre-configured Raspbian + OpenCV . img file . <p> If you would like to install compile + install OpenCV again you would need to download the source , make your modifications , and then re-run CMake and make . I would refer to my previous tutorial on installing OpenCV on the Pi for more information . <p> I have been following your blog for quite a while and have created an application which I want to run automatically when my raspberry PI boots . <p> Do you know if it is possible to get a virtual environment to run on power up ? I have created a script similar to your startpy2cv3.sh , source /. profile workon cv python /home/pi/homesurv.py conf conf.json <p> when the python script runs I get an error message along the lines of cant find cv2 implying that the virtual environment @ @ @ @ @ @ @ @ @ @ start with a clean image and install opencv manually , following one of @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485187 @185187/ <h> You can become a computer vision + OpenCV Guru . <p> " This course is the best investment in time and money that I 've made in a long time . " Skip Snaith <h> Learning computer vision &amp; OpenCV used to be hard . But not anymore . <h> PyImageSearch Gurus has one goal ... <h> ... to make developers , researchers , and students like yourself become awesome at solving real-world computer vision problems . <p> Inside PyImageSearch Gurus , you 'll find : <p> An actionable , real-world course on OpenCV and computer vision . Each lesson in PyImageSearch Gurus is taught in the same trademark , hands-on , easy-to-understand PyImageSearch style that you know and love . We 'll be using primarily Python + OpenCV throughout the course , but expect a few other libraries to be utilized as well , including scikit-learn , scikit-image , Caffe , and Keras . <p> The most comprehensive computer vision education online today . The PyImageSearch Gurus course covers 13 modules broken out into 168 lessons , with other 2,161 pages of content . You wo n't @ @ @ @ @ @ @ @ @ @ , I guarantee it . See below for the full list of topics to be covered in the course . <p> A community of like-minded developers , researchers , and students just like you , who are eager to learn computer vision and level-up their skills . <h> Computer vision is everywhere ... whether you realize it or not . <p> Have you been on Facebook recently ? If so , you may have noticed that Facebook not only can automaticallydetect faces in images but also recognize and tag them as well ! <p> Just like Facebook has seamlessly inserted face recognition into their online photo software , we can also apply computer vision to other areas of our lives : including automatic license plate identification , handwriting recognition , security , surgery , military , retail , and much more . <p> My goal in the PyImageSearch Gurus course is to teach you how to utilize computer vision to solve actual , real-world problems , such as the ones listed in the domains above . Join me , and give the course a try before you know it , @ @ @ @ @ @ @ @ @ @ projects of your own ! <h> Curious about deep learning ? I 'm here to help . I 've created an entire module dedicated to Neural Networks , Deep Belief Networks , and Convolutional Neural Networks , allowing you to get up to speed quickly and easily . <p> When I first started writing the PyImageSearch Gurus course , my original working title for this module was " Deep Learning Demystified " . You see , every morning when I opened my notebook and starting drafting lessons , I wanted that header in big , block letters as a constant reminder that deep learning does n't have to be complicated . <p> And in truth , it does n't have to be . Inside this module , you 'll start by learning the basics of neural networks and deep learning , including activation functions , convolutional layers , pooling layers , and common network architectures . <p> From there , you 'll apply popular deep learning libraries such as Caffe , Keras , and scikit-theano to train your own custom networks to classify the contents of images . <p> You @ @ @ @ @ @ @ @ @ @ able to apply it to your own datasets as well ! <h> You 've probably seen face recognition software before remember Facebook ? The real question is , how do they do it ? I 'll show you ! And then you 'll be able to apply face recognition to your own projects . <p> Let 's play a little game suppose I hold up a picture of your mother , best friend , or college roommate . I 'm willing to bet that you 'll be able to recognize the person in the image , right ? <p> Face recognition tends to be an easy task for humans . We are able to intuitively pick out distinguishing attributes of a person 's face . <p> But the same is not true for computers . In order for a computer to recognize people in images , we need to develop specialized techniques that can quantify facial characteristics . <p> Once you join the PyImageSearch Gurus course , I 'll show you how to build face recognition systems of your own using both images and video streams . <p> The PyImageSearch @ @ @ @ @ @ @ @ @ @ valuable investments for me . I work in a fashion oriented company where the analytics team focuses on computer vision - and this course is just a perfect fit . A major plus is the amazing quality of the code , available to download for every lesson . If you like computer vision , this is your place . <h> The Bag of Visual Words model . Tf-idf weighting . Inverted indexes . Sounds complicated , right ? It 's not . Let me show you how building image search engines is n't magic and how you can do it too . <p> Content-based Image Retrieval ( i.e. , image search engines ) is the reason why I started the PyImageSearch blog . Inside these lessons , you 'll discover my special techniques for building image search engines and making collections of images visually searchable . <p> Plus , you 'll gain access to my previously unreleased personal libraries that I 've spent years crafting and perfecting . Using these resources , you 'll be able to build image search engines of your own in no time . <h> Recognize @ @ @ @ @ @ @ @ @ @ applications that can detect and understand your actions in real-time . <p> Ever seen the movie Minority Report ? It features a scene with an amazing ( although fictional ) hand gesture recognition system . I ca n't promise you that we can build that ... but what I can promise is to teach you the basics of how hand gesture recognition works . <p> We 'll start with the basics of segmentation , and how we can segment the hand region from a video stream . Then , we 'll apply hand gesture recognition algorithms that can detect the number of fingers you are holding up . <p> From there , I 'm confident that you 'll be able to apply hand gesture recognition to your own applications ! <p> Serializing and concatenating text files makes sense ... but how is that possible with images ? I 'll show you . <p> From there , we 'll utilize MapReduce to quickly process large image datasets in bulk using my personal Hadoop + MapReduce project structure template . This is the exact same template that I use when working on @ @ @ @ @ @ @ @ @ @ in 2012 Google built an image classification system using 16,000 processors and months of computing time to recognize cats in YouTube videos . <h> I 'll show you the same tricks Google used to build their cat recognizer ... but in a fraction of the time and CPU power . <p> Image classification and image understanding are currently ( and will continue to be ) the most popular sub-field of computer vision for the next 10 years . We 'll see large companies like Google , Microsoft , and Baidu quickly snap up successful image understanding startup companies . We 'll also see more consumer applications on our smartphones that can interpret the contents of an image . <p> In order to be prepared for this , you need to understand image classification this is especially true if you 're an entrepreneur preparing to enter the computer vision space . <p> Inside this module , I 'll detail how to build your own image classifiers with lots of code examples . You 'll be developing and deploying your own image classification systems well before the PyImageSearch Gurus course completes . @ @ @ @ @ @ @ @ @ @ as computer vision . Your writing/teaching are very clear and I like the layout of the lessons . Love everything so far and I 'm so glad to be a part of it . I signed up for the instant access membership and already think it 's well worth your asking price . <p> The PyImageSearch Gurus course is one of the best education programs I have ever attended . No matter whether you are a beginner or advanced computer vision developer , you 'll definitely learn something new and valuable inside the course . I highly recommend PyImageSearch Gurus to anyone interested in learning computer vision . <h> Work with practical , hands-on Raspberry Pi projects . <p> Last winter , I went down to Mexico for a few days to get away from the unbearable frozen tundra of the north eastern United States . While I was gone , I set up a Raspberry Pi by my front door to perform both motion detection and face identification . If anything were to happen , I would receive a txt message alert to my smartphone ! <p> Using a @ @ @ @ @ @ @ @ @ @ API ) , I was able to build such a system in less than a day ! <p> Inside the PyImageSearch Gurus course , I 'll show you how to build Raspberry Pi + computer vision applications like this one ... and I have no doubt that you 'll be able to extend this project using your knowledge of computer vision gained throughout the rest of the course ! <h> Visually understand and quantify photos with image descriptors . <h> What 's in an image ? And how do you describe the contents of an image using a bunch of numbers ? The trick is to apply image descriptors and feature extraction . It 's easy , once you get the hang of it . <p> Before we can build custom object detectors , classify the contents of images , or construct large-scale image search engines , we first need to understand a critical component of computer vision image descriptors . <p> Image descriptors and feature extraction govern how and what parts of an image are quantified , effectively turning a matrix of pixels into a single list of numbers @ @ @ @ @ @ @ @ @ @ These techniques play a vital role in computer vision , so we spend lots of time dissecting and understanding feature extraction methods , including : Histogram of Oriented Gradients , Local Binary Patterns , Haralick texture , Zernike moments , SIFT , RootSIFT , SURF , ORB , BRISK , FREAK , and many more ! <h> Study real-world solutions to actual computer vision problems . <h> You 'll learn how to track multiple objects in a video , recognize handwriting , identify plant species , and much more . <p> One of the primary reasons I started the PyImageSearch blog is to create high-quality , easy-to-follow , and super hands-on computer vision tutorials . A major factor in this practicality is the ability to translate the lessons you learn on PyImageSearch to actual , real-world computer vision problems . <p> I 'm pleased to say that this same motivation has carried over to the PyImageSearch Gurus course . Even though we 'll be reviewing more advanced algorithms and topics no worries the same easy-to-follow explanations for every topic we review will still be there . And as always , @ @ @ @ @ @ @ @ @ @ Master the basics of computer vision and image processing . <h> The PyImageSearch Gurus course is my magnum opus on computer vision it 's for developers , researchers , and students just like yourself , who are looking for the complete computer vision experience . <p> We 'll start with the basics of computer vision and image processing , allowing you to get started quickly ( without being bogged down in lots of theory ) . In fact , some of our very first projects include Tetris block identification , recognizing tic-tac-toe pieces , and learning how to segment license plate characters from license plates . Not bad for your first few lessons , right ? <p> Like I said , this is a super practical and super hands-on computer vision course . <p> Whether you 're just getting started learning computer vision or you 're a seasoned veteran in the field , this course is for you . <h> Direct access to me . <h> The PyImageSearch Guru forums are my new home . <p> Each week I receive a ton of emails . To be honest , it @ @ @ @ @ @ @ @ @ @ you need access to me to help solve a computer vision problem or get advice in what direction to go , the PyImageSearch Gurus forum will be my new home . <p> I 'll be checking the forums and replying to topics and questions each and every day . <h> ... and share your computer vision accomplishments with your network . <p> If you 've ever taken an online course through popular learning platforms such as Coursera , Udemy , or edX , you 'll know that after completing a course , you can directly embed your hard-earned certificate on your LinkedIn profile . <p> The same is true for the PyImageSearch Gurus course ! <p> When you successfully complete the PyImageSearch Gurus course , you 'll be able to embed your Certificate of Completion on your LinkedIn page to share with your network . Once embedded , the certificate will link back to the PyImageSearch Gurus site , verifying your PyImageSearch Gurus graduation . <h> You might be wondering , " Is PyImageSearch Gurus right for me ? " <h> You bet it is ! <h> Regardless of whether @ @ @ @ @ @ @ @ @ @ 're already a seasoned computer vision pro , PyImageSearch Gurus has the tutorials and code for you . <p> Just getting started in computer vision ? Do n't worry ! You wo n't be bogged down with tons of theory and complex equations . We 'll start off with the basics of computer vision , image processing , and OpenCV so you have a solid foundation and understanding . You 'll learn in a fun , practical manner with lots of code . Using the proven , hands-on PyImageSearch Gurus teaching method , you 'll be able to graduate to the more advanced content in no time . <p> Already a seasoned computer vision pro ? This course is n't just for beginners there 's content in here for you , too . You 'll discover how to apply Deep Learning and Convolutional Neural Networks to classify the contents of images . You 'll construct a framework that can be used to train your own custom object detector with minimal effort . And you 'll build systems to automatically recognize license plates in images . These real-world solutions can be @ @ @ @ @ @ @ @ @ @ can not say it enough : PyImageSearch Gurus is amazing . I really enjoy myself and the way you set everything up ( blog , course , community , incredible response time ) . It tells a lot about how much you enjoy it too . <h> Apprentice Membership <p> $95 <p> Billed monthly <p> The Apprentice Membership is billed monthly . Every month a new set of lessons are released according to a preset schedule I have meticulously crafted to help you study computer vision in the most efficient , optimal manner . You can work through the course on your own time , independent from other students ' progress . Under this membership , it will take you as little as 6 months to complete the PyImageSearch Gurus course . <p> If you 're just testing the waters ( or , if budget is an issue ) , I 'd recommend going with the Apprentice Membership . You 'll still be getting the best , most comprehensive computer vision education online and you can always upgrade to an instant access membership once you 've given the course a @ @ @ @ @ @ @ @ @ @ <p> Preset course schedule : each month for 6 months ( based on your signup date ) a new set of lessons is released . <p> Access to the private PyImageSearch Gurus Community + Forums . <p> Access to PyImageSearch Jobs land your next position in the computer vision field ( coming soon ) . <h> Instant Access Membership <p> $995 <p> One-time Payment SAVE 15% ! <p> I 've designed the Instant Access Membership to be your all-access pass to the PyImageSearch Gurus course for a single , upfront one-time payment . Inside this membership , you 'll get instant access to every single lesson in PyImageSearch Gurus . This is the 100% , entirely self-paced option , allowing you to work through the course at your own speed , finish in less than 6 months , and focus on the modules that interest you the most . <p> 70% of PyImageSearch Gurus members choose this option and most members that initially go with the Apprentice Membership upgrade to instant access after they join . So why not start with the best ? <p> When you purchase the Instant @ @ @ @ @ @ @ @ @ @ the PyImageSearch Gurus course . <p> Instant access to every lesson in the PyImageSearch Gurus course for an upfront one-time payment . <p> 100% fully self-paced : work at your own speed , focus on the lessons that interest you the most . <p> Accelerate your progress and finish the course faster than 6 months . <p> Access to the private PyImageSearch Gurus Community + Forums . <p> Access to PyImageSearch Jobs land your next position in the computer vision field ( coming soon ) . <h> Questions ? <h> Which membership should I buy ? <p> This mostly depends on your budget . The instant access membership costs more upfront , but you 'll get access to every lesson inside the PyImageSearch Gurus course immediately plus , you 'll save 15% in the long run . However , if budget is a concern ( or if you want to try the course before committing to the instant access ) , then the monthly membership is still a great deal . In either case , you 're still getting the best , most comprehensive computer vision education online . <h> @ @ @ @ @ @ @ @ @ @ for me ? <p> Yes , absolutely . Let me put it this way I built PyImageSearch Gurus to be the course I wish I had when I started learning computer vision . Even though computer vision concepts seem complicated , overlapping and intertwining , I 've unweaved all the complexity , leaving you with an easy-to-follow roadmap to graduating the PyImageSearch Gurus course and becoming a computer vision guru . <h> PyImageSearch Gurus seems expensive ... <p> I 've found that charging for the PyImageSearch Gurus course dramatically improves the quality of content and people involved . And let 's be honest free content has its limitations : ( 1 ) Myself and other PyImageSearch Gurus are willing to divulge more information in a private setting than we should share on the open web . This means you get exclusive information that you wo n't get anywhere else . ( 2 ) Private communities lead to better relationships and outcomes . The PyImageSearch Gurus community is a group of committed developers , researchers , and students and I ensure that by charging a fee . ( 3 ) Finally @ @ @ @ @ @ @ @ @ @ free , nor should it be . Premium education , advice , and connections are valuable . If you do n't see it that way , that 's totally cool ... but you likely wo n't fit in here . <h> If I start with the monthly membership , can I upgrade to the instant access membership ? <p> Yes , you can absolutely start out with the monthly membership and then upgrade to the instant access membership later on all you need to do is email me , let me know that you would like to upgrade , and I 'll take care of the upgrade for you . <p> That said , all upgrades should take place within the first 30 days of joining the PyImageSearch Gurus course to take advantage of the 15% annual discount . So if you would like to upgrade , definitely let me know sooner rather than later ! <h> What happens after I buy this course ? <p> After purchasing either the instant access or monthly membership to PyImageSearch Gurus , you 'll immediately be able to login to the course and @ @ @ @ @ @ @ @ @ @ access the PyImageSearch Gurus community + forums . <p> If you purchase the instant access membership , you 'll have access to all lessons immediately . If you go with the apprentice membership , you 'll have access to the first month 's content . <h> How many hours per week should I commit to PyImageSearch Gurus ? <p> The number of hours you should spend per week on the PyImageSearch Gurus course varies from student to student , but in general , the more time you put into the course , the more value you 'll get out of it . I recommend spending 30-60 minutes per day on the course , or a total of 5 hours per week . Think of it this way : if you can spend the time it takes you to drink your morning coffee reading the lessons inside PyImageSearch Gurus , you 'll notice huge improvements in your computer vision knowledge . <h> Do I need any programming experience before enrolling in PyImageSearch Gurus ? <p> Yes , in order to be successful inside PyImageSearch Gurus , basic programming experience is a @ @ @ @ @ @ @ @ @ @ but is by no means a requirement . Simply put , if you know basic control statements such as if , while , and for , you likely already have enough experience to be successful inside the course . <h> Do I need to know Python before joining the course ? <p> No , you do not need to know Python before joining PyImageSearch Gurus . Experience in Python is recommended , but not a requirement . You 'll find that Python is a very easy language to learn , and even if you do not have experience in Python , I 'm confident that just by reviewing the code examples in the course , you 'll be able to pick up the language quickly . If at any point you decide you need extra help learning Python , exclusive discounts on the excellent RealPython.com course are offered inside PyImageSearch Gurus to help you level-up your Python coding skills . <h> Which versions of Python + OpenCV are used in the course ? <p> The PyImageSearch Gurus course provides code examples compatible with Python 2.7 and OpenCV 2.4 . The @ @ @ @ @ @ @ @ @ @ 's tried and true . It 's well documented . And it 's been the de facto standard computer vision library for the past 3 years . These reasons alone make it best suited to teach a large , comprehensive computer vision course . <h> Are you going to support OpenCV 3 ? <p> OpenCV 3 is still in its infancy . There are many more updates , bug fixes , and code reorganizations to come . Because of this , we 'll be using the battle-tested OpenCV 2.4 in the PyImageSearch Gurus course . That said , yes , the PyImageSearch Gurus course will support OpenCV 3 once it matures a bit ( likely somewhere in the next 1.5 years ) . However , for the time being , OpenCV 2.4 . X will be used inside the course . <h> I have another question . <h> How I 'll make sure you 're 100% satisfied with PyImageSearch Gurus . <p> PyImageSearch Gurus is the course I wish I had when I first started learning computer vision and OpenCV over 6+ years ago . <p> If you 're interested @ @ @ @ @ @ @ @ @ @ the knowhow to solve advanced computer vision problems ( and eventually land a job in the computer vision field ) , I designed this course for you . <p> I 've also taken steps to ensure that each customer is accountable and enjoys their time in PyImageSearch Gurus . I do n't want you to pay me and that be it . I want you to pay me and then make a significant return on your investment , so I 've included a followup accountability program to help keep you on track and motivated to finish the PyImageSearch Gurus course . <p> Of course , I recognize that there is n't a " one-size-fits-all " solution to anything . So if you do buy my course , go through it , do the work , and feel like you have n't learned anything , then I do n't want your money . Just reply to your purchase receipt email within 30 days for a full 100% money back guarantee . 
@@71485188 @185188/ <h> Archive Image Processing <p> We have now reached the final installment in our three part series on- measuring the size of objects in an image and- computing the distance between objects . Two weeks ago , we started this round- of tutorials by learning how to ( correctly ) order coordinates in a clockwise manner using Python and OpenCV . Then , last week , we discussed how to <p> Measuring the size of an object ( or objects ) in an image has been a- heavily requested tutorial on the PyImageSearch blog for some time now and it feels- great to get this post online and share it with you . Todays post is the second in a three part series on- measuring the size of objects in an <p> Today we are going to kick-off a three part series on- calculating the size of objects in images along with- measuring the- distances between them . These tutorials have been some of the most- heavily requested- lessons on the PyImageSearch blog . I 'm super excited to get them underway - and I 'm sure you are too . @ @ @ @ @ @ @ @ @ @ is the final post in our three part series on shape detection and analysis . Previously , we learned how to : Compute the center of a contour Perform shape detection &amp; identification Today we are going to perform both- shape detection and- color labeling on objects in images . At this point , we understand that regions of an image <p> This tutorial is the second post in our three part series on- shape detection and analysis . Last week we learned how to compute the center of a contour using OpenCV . Today , we are going to leverage contour properties to actually- label and identify shapes in an image , just like in the figure- at the top of this post . <p> Today , we are going to start a new 3-part series of tutorials on- shape detection and analysis . Throughout this series , well learn how to : Compute the center of a contour/shape region . Recognize various shapes , such as circles , squares , rectangles , triangles , and pentagons using only contour properties . Label the color of a shape . @ @ @ @ @ @ @ @ @ @ classic- algorithm used for segmentation and is especially useful when extracting- touching or- overlapping objects in images , such as the coins in the figure above . Using traditional image processing methods such as thresholding and contour detection , we would be unable to extract each individual coin from the image but by leveraging the <p> Todays blog post will build on what we learned from last week : how to construct an image scraper using Python + Scrapy to scrape 4,000 Time magazine cover images . So now that we have this dataset , what are we going to do with it ? Great question . One of my favorite visual analysis techniques to apply <p> Did you know that the human eye perceives color and luminance differently than the sensor on your smartphone or digital camera ? You see , - when- twice the number of photons hit the sensor of a digital camera , it receives- twice the signal ( a linear relationship ) . However , that 's not how our human eyes work . Instead , @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485189 @185189/ <p> Since I started PyImageSearch I 've received a handful of emails regarding generating movie barcodes , and awhile I normally do n't  cover- visualization methods , I ended up deciding to write a blog post on it . It is a pretty neat technique after all ! <p> In the remainder of this tutorial I 'll be demonstrating how to write your own Python + OpenCV application to generate movie barcodes of your own . <h> Generating movie barcodes with OpenCV and Python <p> In order to construct movie barcodes we need to accomplish three tasks : <p> Task #1 : Determine the number of frames in a video file . Computing the total number of frames in a- movie allows us to get a sense of how many frames we should be- including in the movie barcode visualization . Too many frames and our barcode will be- gigantic ; too little frames and the movie barcode will be aesthetically unpleasing . <p> Task #2 : Generating the movie barcode data . Once we know the total number of video- frames we want to include in the movie barcode , @ @ @ @ @ @ @ @ @ @ RGB average , maintaining- a list of averages as we go . This serves as our actual movie barcode data . <p> Task #3 : Displaying the movie barcode . Given the list of RGB averages for a set of frames , we can take this data and create the actual- movie barcode visualization that is displayed to our screen . <p> The rest of this post will demonstrate how to accomplish each of these tasks . <h> Movie barcode project structure <p> Before we get too far in this tutorial , let 's first discuss our project/directory structure detailed below : <p> Well require two command line arguments along with an optional switch , each of which are detailed below : <p> --video- : This is the path to our input video file that we are going to generate the movie barcode for . <p> --output- : Well be looping over the frames in the input video file and computing the RGB average for every- N-th frame . These RGB averages will be serialized to a JSON file so we can use this data for the actual movie barcode visualization @ @ @ @ @ @ @ @ @ @ controls the number of frames to- skip when processing the video . Why might we want to skip frames ? Consider the- Jurassic Park trailer above : There are over- 4,700 frames in a movie clip under 3m30s . If we used only- one pixel to visualize the RGB average for each frame , - our movie barcode would be over 4,700 pixels wide ! Thus , its important that we skip every- N-th barcode to reduce the output visualization size . <p> Notice here that 199- frames have been saved to disk using a frame skip of 25 . <p> Going back to the output of countframes.py- we can see that- 4,790 / 25 = 199- ( the calculation actually equals 192 , but that 's due to a discrepancy in countframes.py- to learn more about this behavior , please see last weeks blog post ) . <p> You should also now see a file named **25;8597;TOOLONG in your output- directory : <p> --height- : This parameter controls the- height of the movie barcode visualization . Well default this value to a height of 250 pixels . <p> --barcode-width- : @ @ @ @ @ @ @ @ @ @ the movie barcode needs to have a width in pixels . We set a default value of 1 pixel per bar , but we can change the width by supplying a different value for this command line argument . <p> We are now ready to visualize the barcode : <p> Generating movie barcodes with <p> 38 <p> 39 <p> # load the averages file and convert it to a NumPy array <p> avgs=json.loads ( open ( args " avgs " ) . read() ) <p> avgs=np.array ( avgs , dtype= " int " ) <p> # grab the individual bar width and allocate memory for <p> # the barcode visualization <p> bw=args " barcodewidth " <p> LONG ... <p> dtype= " uint8 " ) <p> # loop over the averages and create a single ' bar ' for <p> # each frame average in the list <p> @ @ @ @ @ @ @ @ @ @ ( barcode , ( i*bw , 0 ) , ( ( i+1 ) *bw , <p> args " height " ) , avg , -1 ) <p> # write the video barcode visualization to file and then <p> # display it to our screen <p> cv2.imwrite ( args " barcode " , barcode ) <p> cv2.imshow ( " Barcode " , barcode ) 55212 @qwx675212 <p> Lines 20 and 21 load the serialized RGB means from disk . <p> Lines 26 and 27 utilize the --height- switch along with the number of entries in the avgs- list and- the --barcode-width- to allocate memory for a NumPy array large enough to store the movie barcode . <p> For each of the RGB averages we loop over them individually ( Line 31 ) and use the cv2.rectangle- function to draw the each- bar in the movie barcode ( Lines 32 and 33 ) . <p> Finally , - Lines 37-39 write the movie barcode- to disk and display the visualization to our screen . <h> Movie barcode visualizations <p> To see the movie barcode visualization for the- Jurassic Park trailer , make @ @ @ @ @ @ @ @ @ @ using the- " Downloads " section of this post . From there , execute the following commands : <p> Generating movie barcodes with OpenCV and Python <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> $python **25;8624;TOOLONG **30;8651;TOOLONG <p> --output **39;8683;TOOLONG <p> $python **25;8724;TOOLONG **31;8751;TOOLONG <p> --barcode LONG ... <p> Figure 2 : Generating a movie- barcode for the Jurassic Park trailer . <p> The large green bars at the beginning of the barcode correspond to the green preview screen required by the Motion Picture Association of America , Inc . <p> The blues in the middle of the movie barcode refer to : <p> The heavy downpours and the bluish tint cast by the strong thunderstorm when things start to- go downhill for our park visitors . <p> Tims blue shirt while in the jeep . <p> Denny Nedry in the blue-tinted embryo cold storage room . <p> Let 's now visualize the movie trailer to- The Matrix , another one of my all-time favorite movies : <p> Below you can see the output from running countframes.py- and- generatebarcode.py- on the movie clip : <p> Figure 3 : @ @ @ @ @ @ @ @ @ @ generating the video barcode for The Matrix trailer . <p> And here follows the actual movie barcode : <p> Figure 4 : Visualizing the movie barcode using OpenCV and Python . <p> Perhaps unsurprisingly , this movie barcode is heavily dominated by the- The Matrix-style greens and blacks . <p> Finally , here is one last example from the movie trailer for- The Dark Knight : <p> Note : I used the website keepvid.com- to download the trailers to the movies mentioned above . I do not own the copyrights to these videos nor do I claim to this blog post is for educational and example purposes only . Please use wisely . <h> Summary <p> In todays blog post I demonstrated how to generate video barcodes using OpenCV and Python . <p> Generating video barcodes is normally used for design aesthetics and admittedly does n't  have a far-reaching computer vision/image processing purpose ( other than visualization itself ) . <p> That said , you could treat the movie barcodes as a form of " feature vector " and use them to- compare other movie barcodes/movie clips for similarity or @ @ @ @ @ @ @ @ @ @ . <p> I hope you enjoyed this blog post ! <p> Be sure to enter your email address in the form below to be notified when future blog posts are published . <h> Downloads : 55217 @qwx675217 <h> 5 Responses to Generating movie barcodes with OpenCV and Python <p> Is there a reason for not setting CAPPROPPOSFRAMES to be the frame you want to calculate the mean of i.e. indexing into the video at the desired frame number instead of reading entire video frame by frame and deciding if you want to capture its mean or not <p> Like you said , there is n't much of a use of the generated barcode as a good hashing technique by using movie name can get us good results to get unique I 'd for movies . However , I found the use of this barcode as a feature descriptor of that movie trailer . I wanted to know is this is something that one can seriously look for ? The reason I ask is that histograms are usually very good at capturing the image frame info , so does the barcode have a @ @ @ @ @ @ @ @ @ @ serve as a feature vector , that is certainly possible . However , just because two movies have similar color distributions does not guarantee they have similar contents . That said , for simple applications computing the Euclidean distance between two barcodes would serve @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485190 @185190/ <h> Face recognition for security <p> Face recognition for security <p> In this project , well learn how to perform face recognition on the Raspberry Pi 2 hardware and create a simple security system that can send us- text message alerts when intruders enter our video stream . <p> One of the most irritating sensations in the world is touching a greasy keyboard or mouse so if you have n't washed your hands , - stay away from my desk ! <p> But in all seriousness , this project will be directly applicable to any area that you want to monitor using face recognition . In the remainder of this lesson , I 'll show you exactly how to set up such a security cam using the Raspberry Pi , Amazon S3 , and the Twilio API . <h> Objectives : <p> In this lesson , we will : <p> Leverage our knowledge of face recognition and apply it to the Raspberry Pi . <p> Determine if an intruder has entered the view of the Raspberry Pi camera , and if so : <p> Upload a photo of the intruder to @ @ @ @ @ @ @ @ @ @ intruder to our smartphone . <h> Face recognition for security <p> The face is arguably the most defining characteristic of the human body . Every persons face is unique . And while we all have two eyes , two ears , a noise , and a mouth ( barring any birth defects or accidents ) - our facial structure can vary dramatically . Its within this variance which lies the distinguishing characteristics that be used to identify one person from another . <p> Inside our lessons on- face recognition , we learned how to apply- Local Binary Patterns and a bit of machine learning to automatically classify faces in images and video streams . Today , we are going to build upon our knowledge of face recognition and leverage the Raspberry Pi to build a simple security camera . <p> Before we get started , let 's look at our project structure : <p> @ @ <p> 21 <p> ---cascades <p> ---conf <p> ---alerts.json <p> ---output <p> ---classifier <p> ---faces <p> ---pyimagesearch <p> ---init.py <p> ---facerecognition <p> ---init.py <p> ---facedetector.py <p> ---facerecognizer.py <p> ---notifications <p> ---init.py <p> ---twilionotifier.py <p> ---utils <p> ---init.py <p> ---conf.py <p> ---gatherselfies.py <p> ---securitycam.py <p> ---trainrecognizer.py <p> Inside the cascades- directory , we will store the Haar cascades used by OpenCV to detect faces in images . <p> The conf- directory will store our JSON configuration file , used to store important variables such as our Amazon SWS , S3 , and Twilio authentication parameters . <p> Inside the pyimagesearch- module , we have the facerecognition- sub-module , which will implement all necessary logic to ( 1 ) train a face recognizer- and ( 2 ) identify faces in a video stream . <p> We then have the notifications- module , which stores our TwilioNotifier- class . This class is responsible for taking an image , uploading it to S3 , and then sending a text message notification via the- Twilio API . <p> To gather face examples , we first need to run gatherexamples.py- to collect training data for each person @ @ @ @ @ @ @ @ @ @ then be executed to train an LBP-based face recognizer on top of the face samples . <p> Finally , we can launch securitycam.py- to monitor our video stream for faces , and if one is found , we can attempt to identify it . - If the face- can not be identified , this script will handle passing the image to the TwilioNotifier- , where well send a text message notification to our smartphone . <h> Gathering face examples <p> The first step in building our face recognizer is to run gatherexamples.py- to collect examples of each face we want to identify . This script is identical to our previous lesson on- gathering selfies , but with one exception we have updated it to use the unified VideoStream- class ( that is compatible with both builtin/USB webcams along with the Raspberry Pi camera module ) - instead of the standard cv2.VideoCapture- method . You can read more about the VideoStream- class in this blog post on PyImageSearch.com . <p> Since the implementation of the gatherselfies.py- is essentially identical to our previous lesson , we wont be reviewing the source code @ @ @ @ @ @ @ @ @ @ section at the bottom of this tutorial to review the ( very small ) updates . <p> In any case , if you are using a USB camera connected to your Raspberry Pi to gather selfies , execute the following command : <p> You should now have the output classifier stored in your output/classifier- directory : <p> Figure 2 : Training our face classifier . <h> Twilio and Amazon S3 <p> In order to send text message notifications containing images of an intruder to our smartphone , well be using the Twilio API . The Twilio API is free ( with some minor restrictions ) and is very simple to use . <p> To start , head over to the Twilio website and register for an account . After registering , Twilio will automatically assign you a phone number that you can use for sending messages . You 'll also want to grab your AccountSID- and AuthToken- which are the credentials used to access the Twilio API . You can find the AccountSID- and AuthToken- on your Twilio Account page . <p> Notice how we need to supply a mediaurl- to @ @ @ @ @ @ @ @ @ @ public server ) that contains the image we want to send in our text message . <p> If you already have a public server , then you can certainly upload images to it programmatically . But if you do n't  have a public-facing server ( or would rather use a simple Python-based API ) , then you 'll want to set up Amazon S3 . <p> Amazon Simple Storage Service ( S3 ) is an online file storage service hosted by Amazon Web Services . Storage is organized into " buckets " , where each bucket can store a set of files . <p> Amazon S3- is not free , but is- extremely cheap , coming in at &lt; $0.15 per gigabyte of storage per month plus bandwidth costs . In short , you can easily run a custom home security system using Twilio + Amazon S3 for less than the cost of a pack of gum at your local convenience store . <p> Our send- function requires only a single parameter , the image- that we want to send via text message. - Lines 16 and 17 take this image- @ @ @ @ @ @ @ @ @ @ 20-22 then create a separate thread used to upload the image to S3 and then send it over the wire via the Twilio API . We use threading in this case , so we do n't  slow down our main video processing pipeline due to I/O latency . <p> Notice how the Thread- makes a call to a send- function . Let 's define it below : <p> --consec-frames- : Here , we can supply an integer specifying the minimum number of- consecutive frames containing an unknown face that must be met prior to sending an alert . <p> --picamera- : Finally , this value controls where we are using a USB camera or the Raspberry Pi camera module . <p> Our next step is to perform a series of initializations <p> 49 <p> 50 <p> # load the configuration file and @ @ @ @ @ @ @ @ @ @ " ) <p> tn=TwilioNotifier(conf) <p> # initialize the video stream and allow the camera sensor to warm up <p> print ( " INFO warming up camera ... " ) <p> LONG ... <p> time.sleep(2.0) <p> # initialize the face detector , load the face recognizer , and set the confidence <p> # threshold <p> fd=FaceDetector ( args " facecascade " ) <p> **27;8784;TOOLONG " classifier " ) <p> **30;8813;TOOLONG " confidence " ) <p> # initialize the number of consecutive frames list that will keep track of ( 1 ) the <p> # name of the face in the image and ( 2 ) the number of *consecutive* frames the face <p> # has appeared in <p> consec=None <p> # initialize the color of the bounding box used for the face and the last time <p> # we sent an MMS notification <p> color= ( 0,255,0 ) <p> lastSent=None <p> Lines 28 and 29 load our JSON configuration file from disk and use the configuration to instantiate a TwilioNotifier- object . We also initialize our VideoStream- and allow the camera sensor to warm up on- Lines 32-34 . <p> This @ @ @ @ @ @ @ @ @ @ frames from our video stream , poll them one by one , pre-process the frame , and then detect faces in the image . We also draw the current timestamp on the frame- . <p> Our next code block determines- if there is an intruder or not <p> 90 <p> 91 <p> # loop over the face bounding boxes <p> for ( i , ( x , y , w , h ) ) inenumerate(faceRects) : <p> # grab the face to predict and predict it <p> face=grayy:y+h , x:x+w <p> ( prediction , confidence ) =fr.predict(face) <p> # if the consecutive frames list is None , or the prediction does not match the <p> # name from the previous frame , re-initialize the list <p> ifconsec isNone : <p> consec=prediction , 1 <p> color= ( 0,255,0 ) <p> # if predicted face @ @ @ @ @ @ @ @ @ @ the <p> # total count <p> elifprediction==consec0 : <p> consec1+=1 <p> # if the prediction has been " unknown " for a sufficient number of frames , <p> # then we have an intruder <p> LONG ... <p> # change the color of the bounding box and text <p> color= ( 0,0,255 ) <p> intruder=True <p> Line 70 loops over the detected faces in the frame. - Lines 72 and 73 extract the face ROI and then pass the face on to our face identifier . <p> If the consec- variable is None- , we initialize it as a list , containing the name of the face and the number of consecutive frames the face has appeared in . <p> Otherwise , if the predicted face matches the name in consec- , then we update the consecutive frame count . <p> Line 88- makes the all important check - has an intruder entered our video stream ? - If the predicted face is Unknown- , and has been Unknown- for a sufficient number of frames , then an intruder has been detected . <p> Next , let 's draw the bounding @ @ @ @ @ @ @ @ @ @ followed by checking to see if ( 1 ) an intruder has been detected and ( 2 ) enough time has passed in between Twilio message sends <p> 105 <p> 106 <p> # display the text prediction on the image , followed by drawing a bounding box <p> You 'll need to fill in your own values wherever there is a value of XXXXX- . The awsaccesskeyid- and awssecretacesskey- can be generated/found by managing your authentication keys . The s3bucket- should be the name of a bucket you have created in your S3 account used to store images uploaded from your securitycam.py- script . <p> The twiliosid- and twilioauth- values can be found on your Twilio Account page . Finally , twilioto- and twiliofrom- are used to define the phone number of the- recipient and the- sender , respectively . <p> After updating your alerts.json- file and running both gatherselfies.py- and trainrecognizer.py- , we are ready to start our security camera service @ @ @ @ @ @ @ @ @ @ use the following command to kick-off the security camera : <p> In this case , I 've set up my Raspberry Pi to hide behind my laptop with an attached USB camera to monitor anyone who sits in front of my desk : <p> Figure- 3 : My face recognition setup . <p> I 've also included --confidence1- in my command to ensure that my face is marked as an " intruder " . Obviously , in your own scripts , you wo n't want to include such a slow confidence- but this is a good way to debug your script and ensure the proper actions are being triggered . <p> When I sit down in front of my laptop , my face is detected and is labeled as " unknown " : <p> Figure 4 : Detecting my face in a video stream . <p> However , after a sufficient number of frames have passed , I 'm marked as an " intruder " . Notice how the color of the bounding box and text has changed from- green to- red : <p> Figure 5 : Marking a person as an " intruder @ @ @ @ @ @ @ @ @ @ the following text message to my iPhone : <p> Figure 6 : Receiving a text message containing a photo of the intruder . <p> I have included a full video demonstration of the securitycam.py- script below : <h> Summary <p> In this lesson , we learned how to build a simple security system utilizing : <p> If a face entered the view of our security camera , and the face could not be identified , a text message notification containing a snapshot of the intruder was sent to our smartphone device . <p> In order to guard against false-positive detections , we ensured that the " intruder " was marked as " Unknown " for a sufficient number of frames prior to sending this text message . Whenever implementing code that requires processing a video stream and deriving information from the frames , its good practice to ensure that an event , activity , or identification is taking place for a preset number of frames prior to sending an alert or taking further action . 
@@71485191 @185191/ <h> Color Quantization with OpenCV using K-Means Clustering <p> The movie was shot digitally , but then an animated feel was given to it in the post-processing steps but it was a painstaking process . For each frame in the movie , animators traced over the original footage , frame by frame . <p> Just take a second to consider the number of frames in a movieand that each one of those frames had to be traced by hand . <p> Quite the undertaking , indeed . <p> So what if there was a way to create A Scanner Darkly- animation effect using computer vision ? <p> Is that possible ? <p> You bet . <p> In this blog post I 'll show you how to use k-means clustering and color quantization to create A Scanner Darkly- type effect in images . <p> But another approach is to explicitly quantize the image and reduce- the number of colors to say , 16 or 64 . This creates a substantially smaller space and ( ideally ) less noise and variance . <p> In practice , you can use this technique to @ @ @ @ @ @ @ @ @ @ the famous QBIC CBIR system ( one of the original CBIR systems that demonstrated image search engines were possible ) utilized quantized color histograms in the quadratic distance to compute similarity . <p> Now that we have an understanding of what color quantization is , let 's explore how we can utilize it to create A Scanner Darkly- type effect in images . <h> Color Quantization with OpenCV <p> Let 's get our hands dirty . <p> Open up a new file , call it quant.py , and start coding 10 <p> 11 <p> 12 55203 @qwx675203 <p> fromsklearn.cluster importMiniBatchKMeans 55220 @qwx675220 55218 @qwx675218 <p> importcv2 <p> # construct the argument parser and parse the arguments 55206 @qwx675206 <p> LONG ... to the image " ) <p> LONG ... <p> help= " # of clusters " ) 55208 @qwx675208 <p> The first thing well do is import our necessary packages on Lines 2-5 . Well use NumPy for numerical processing , arparse for parsing command @ @ @ @ @ @ @ @ @ @ Our k-means implementation will be handled by scikit-learn ; specifically , the MiniBatchKMeans class . <p> You 'll find that MiniBatchKMeans is substantially faster than normal K-Means , although the centroids may not be as stable . <p> This is because MiniBatchKMeans operates on small " batches " of the dataset , whereas K-Means operates on the population of the dataset , thus making the mean calculation of each centroid , as well as the centroid update loop , much slower . <p> In general , I normally like to start with MiniBatchKMeans and if ( and only if ) my results are poor do I switch over to normal K-Means . <p> Lines 7-12 then handle parsing our command line arguments . Well need two switches : --image , which is the path to the image we want to apply color quantization to , and --clusters , which is the number of colors that our output image is going to have . <p> Now the real interesting code starts : <p> Color <p> 44 <p> 45 <p> # load the image and grab its width and height <p> image=cv2.imread ( args " image " ) <p> ( h , w ) =image.shape:2 <p> # convert the image from the RGB color space to the L*a*b* <p> # color space -- since we will be clustering using k-means <p> # which is based on the euclidean distance , we 'll use the <p> # L*a*b* color space where the euclidean distance implies <p> # perceptual meaning <p> image=cv2.cvtColor ( image , cv2.COLORBGR2LAB ) <p> # reshape the image into a feature vector so that k-means <p> # can be applied <p> LONG ... <p> # apply k-means using the specified number of clusters and <p> # then create the quantized image based on the predictions <p> LONG ... <p> **28;8845;TOOLONG <p> LONG ... <p> # reshape the feature vectors to images <p> quant=quant.reshape ( ( h @ @ @ @ @ @ @ @ @ @ h , w , 3 ) ) <p> # convert from L*a*b* to RGB <p> quant=cv2.cvtColor ( quant , cv2.COLORLAB2BGR ) <p> image=cv2.cvtColor ( image , cv2.COLORLAB2BGR ) <p> # display the images and wait for a keypress <p> cv2.imshow ( " image " , np.hstack ( image , quant ) ) 55212 @qwx675212 <p> First , we load our image off disk on Line 15 and grab its height and width , respectively , on Line 16 . <p> Because in the L*a*b* color space the euclidean distance between colors has actual perceptual meaning this is not the case for the RGB color space . <p> Given that k-means clustering also assumes a euclidean space , were better off using L*a*b* rather than RGB . <p> In order to cluster our pixel intensities , we need to reshape our image on Line 27 . This line of code simply takes a ( M , N , 3 ) - image , ( M x N pixels , with three components per pixel ) and reshapes it into a ( M x N , 3 ) - feature vector . @ @ @ @ @ @ @ @ @ @ array , rather than a three dimensional image . <p> From there , we can apply our actual mini-batch K-Means clustering on Lines 31-33 . <p> Line 31 handles instantiating our MiniBatchKMeans class using the number of clusters we specified in command line argument , whereas Line 32 performs the actual clustering . <p> Besides the actual clustering , Line 32 handles something extremely important " predicting " what quantized color- each pixel in the original image- is going to be . This prediction is handled by determining which centroid the input pixel is closest to . <p> From there , we can take these predicted labels and create our quantized image on Line 33 using some fancy NumPy indexing . All this line is doing is using the predicted labels to lookup the L*a*b* color in the centroids array . <p> Lines 36 and 37 then handle reshaping our ( M x N , 3 ) - feature vector back into a ( M , N , 3 ) - dimensional image , followed by converting our images from the L*a*b* color space back to RGB . <h> Creating- @ @ @ @ @ @ @ @ @ @ can see our original image on the left and our quantized image on the right . <p> Clearly we can see that when using only k=4- colors that we have much lost of the color detail of the original image , although an attempt is made to model the original color space of the image the grass is still green , the soccer ball still has white in it , and the sky still has a tinge of blue . <p> But as we increase the k=8 and k=16 colors we can definitely start to see improved results . <p> Its really interesting to note that it only takes 16 colors to create a good representation of the original image , and thus our A Scanner Darkly- effect . <p> Let 's try another image : <p> Figure 2 : Apply color quantization using OpenCV to a nature scene . <p> Again , the original nature scene image is on the left and the quantized output on the right . <p> Just like the World Cup image , notice that as the number of quantized clusters increase , we are able @ @ @ @ @ @ @ @ @ @ 3 : Applying color quantization using OpenCV to Jurassic Park . <p> Finally , take a look at Figure 3 to see a screen capture from Jurassic Park . Notice how the number of colors have been reduced this is explicitly evident in Hammonds white shirt . <p> There is an obvious tradeoff between the number of clusters and the quality of the quantized image . <p> The first tradeoff is that as the number of clusters increases , so does the amount of time it takes to perform the clustering . <p> The second tradeoff is that as the number of clusters increases , so does the amount of memory it takes to store the output image ; however , in both cases , the memory footprint will still be smaller than the original image since you are working with a substantially smaller color palette . <h> Summary <p> In this blog post I showed you how to perform color quantization using OpenCV and k-means clustering to create A Scanner Darkly- type of effect in images . <p> While color quantization does not perfectly mimic the movie effect , @ @ @ @ @ @ @ @ @ @ in an image , you can create a more posterized , animated feel to the image . <p> Of course , color quantization has more practical applications than just visual appeal . <p> Color quantization is commonly used in systems where memory is limited or when compression is required . <p> In my own personal work , I find that color quantization is best used when building CBIR systems . In fact , QBIC , one of the seminal image search engines , demonstrated that by using quantized color histograms and the quadratic distance , that image search engines were indeed possible . <p> So take a second to use the form below to download the code . And then create some quantized images of your own and send them over to me . I 'm looking forward to seeing your results ! <h> Downloads : 55217 @qwx675217 <h> 13 Responses to Color Quantization with OpenCV using K-Means Clustering <p> I have a question that is a bit related to this post . For reasons , i have to implement this kmeans things in C++ . The thing is in python @ @ @ @ @ @ @ @ @ @ just two lines of code : <p> where img is the original image , center and labels are the k-means centers , labels respectively . <p> The thing i am stuck at : Is there an efficient way of doing the same thing in c++ without looping explicitly over all the pixels ? I am trying to find one but havent found yet . Can you hint on a strategy of doing this without the looping over all the pixels ( if such strategy would exist ) . Thanks . <p> Just to clarify : do you have to implement k-means yourself in C++ ? Or can you use a library that already has k-means implemented ? If you have to implement it yourself , then you 're going to need over the loop over the pixels individually , cluster them , and then loop over them again to quantize them . <p> I 'm not familiar with the rgb2ind function in MATLAB , but I assume its used to index the colors of an image ? If so , then yes , you can use a method such as k-means clustering @ @ @ @ @ @ @ @ @ @ , if you look at the . labels attribute of the k-means clustering object , you 'll find the indices of @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485192 @185192/ <h> Ordering coordinates clockwise with Python and OpenCV <p> The goal of this blog post is- two-fold : <p> The- primary purpose is to learn how to arrange the- ( x , y ) -coordinates associated with a rotated bounding box in top-left , top-right , bottom-right , and bottom-left order . Organizing bounding box coordinates in such an order is a prerequisite to performing operations such as perspective transforms or matching corners of objects ( such as when we compute the distance between objects ) . <p> The- secondary purpose is to address a subtle , hard-to-find bug in the orderpoints- method of the imutils package . By resolving this bug , our orderpoints- function will no longer be susceptible to a- debilitating bug . <p> All that said , let 's get this blog post started by reviewing the original , flawed method at ordering our bounding box coordinates in clockwise order . <h> The original ( flawed ) method <p> Before we can learn how to arrange a set of bounding box coordinates in ( 1 ) clockwise order and more specifically , ( 2 ) a @ @ @ @ @ @ @ @ @ @ we should first review the orderpoints- method detailed in the original 4 point getPerspectiveTransform blog post . <p> I have renamed the ( flawed ) orderpoints- method to orderpointsold- so we can compare our original and updated methods . To get started , open up a new file and name it ordercoordinates.py- : <p> 29 <p> 30 <p> 31 55203 @qwx675203 <p> **29;8875;TOOLONG <p> fromimutils importperspective <p> fromimutils importcontours 55220 @qwx675220 55218 @qwx675218 55219 @qwx675219 <p> importcv2 <p> deforderpointsold(pts) : <p> # initialize a list of coordinates that will be ordered <p> # such that the first entry in the list is the top-left , <p> # the second entry is the top-right , the third is the <p> # bottom-right , and the @ @ @ @ @ @ @ @ @ @ , dtype= " float32 " ) <p> # the top-left point will have the smallest sum , whereas <p> # the bottom-right point will have the largest sum <p> s=pts.sum(axis=1) <p> rect0=ptsnp.argmin(s) <p> rect2=ptsnp.argmax(s) <p> # now , compute the difference between the points , the <p> # top-right point will have the smallest difference , <p> # whereas the bottom-left will have the largest difference <p> diff=np.diff ( pts , axis=1 ) <p> rect1=ptsnp.argmin(diff) <p> rect3=ptsnp.argmax(diff) <p> # return the ordered coordinates <p> returnrect <p> Lines 2-8 handle importing our required Python packages for this example . Well be using the imutils- package later in this blog post , so if you do n't  already have it installed , be sure to install it via pip- : <p> Ordering coordinates clockwise with Python and OpenCV <p> Shell <p> 1 55204 @qwx675204 <p> Otherwise , if you- do have imutils- installed , you should upgrade to the latest version ( which has the updated orderpoints- implementation ) : <p> Ordering coordinates clockwise with Python and OpenCV <p> Shell <p> 1 <p> $pip install--upgrade imutils <p> Line 10 defines our orderpointsold- @ @ @ @ @ @ @ @ @ @ the set of points that we are going to arrange in top-left , top-right , bottom-right , and bottom-left order ; although , as well see , this method has some flaws . <p> We start on- Line 15 by defining a NumPy array with shape ( 4,2 ) - which will be used to store our set of four- ( x , y ) -coordinates . <p> What happens when the sum or difference of the two points is the- same ? <p> In short , tragedy . <p> If either the sum array s- or the difference array diff- have the same values , we are at risk of choosing the incorrect index , which causes a cascade affect on our ordering . <p> Selecting the wrong index implies that we chose the incorrect point from our pts- list . And if we take the incorrect point from pts- , then our clockwise top-left , top-right , bottom-right , bottom-left ordering will be be destroyed . <p> So how can we address this problem and ensure that it does n't  happen ? <p> To handle this problem , @ @ @ @ @ @ @ @ @ @ sound mathematic principles . And that 's exactly what well cover in the next section . <h> A better method to order coordinates clockwise with OpenCV and Python <p> Now that we have looked at a- flawed version of our orderpoints- function , let 's review an- updated , correct implementation . <p> The implementation of the orderpoints- function we are about to review can be found in the imutils package ; specifically , in the perspective.py file . I 've included the exact implementation in this blog post as a matter of completeness : <p> 29 <p> 30 <p> 31 55203 @qwx675203 <p> fromscipy.spatial importdistance asdist 55220 @qwx675220 <p> importcv2 <p> deforderpoints(pts) : <p> # sort the points based on their x-coordinates <p> **25;8906;TOOLONG : , @ @ @ @ @ @ @ @ @ @ right-most points from the sorted <p> # x-roodinate points <p> leftMost=xSorted:2 , : <p> rightMost=xSorted2 : , : <p> # now , sort the left-most coordinates according to their <p> # y-coordinates so we can grab the top-left and bottom-left <p> # points , respectively <p> LONG ... <p> ( tl , bl ) =leftMost <p> # now that we have the top-left coordinate , use it as an <p> # anchor to calculate the Euclidean distance between the <p> # top-left and right-most points ; by the Pythagorean <p> # theorem , the point with the largest distance will be <p> # our bottom-right point <p> LONG ... <p> ( br , tr ) =rightMostnp.argsort(D) : : -1 , : <p> # return the coordinates in top-left , top-right , <p> # bottom-right , and bottom-left order <p> returnnp.array ( tl , tr , br , bl , dtype= " float32 " ) <p> Again , we start off on- Lines 2-4 by importing our required Python packages . We then define our orderpoints- function on- Line 6 which requires only a single parameter the list of pts- that @ @ @ @ @ @ @ @ @ @ these pts- based on their- x-values . Given the sorted xSorted- list , we apply array slicing to grab the two left-most points along with the two right-most points ( Lines 12 and 13 ) . <p> The leftMost- points will thus correspond to the- top-left and- bottom-left points while rightMost- will be our- top-right and- bottom-right points - the trick is to figure out which is which . <p> Luckily , this is n't too challenging . <p> If we sort our leftMost- points according to their- y-value , we can derive the top-left and bottom-left points , respectively ( Lines 18 and 19 ) . <p> Then , to determine the bottom-right and bottom-left points , we can apply a bit of geometry . <p> Using the top-left point as an anchor , we can apply the Pythagorean theorem and compute the Euclidean distance- between the top-left and rightMost- points . By the definition of a triangle , the hypotenuse will be the largest side of a right-angled triangle . <p> Thus , by taking- the top-left point as our anchor , the bottom-right point will have the largest @ @ @ @ @ @ @ @ @ @ top-right points ( Lines 26 and 27 ) . <h> Testing our coordinate ordering implementations <p> Now that we have both the- original and- updated versions of orderpoints- , let 's continue the implementation of our ordercoordinates.py- script and give them both a try : <p> Ordering coordinates clockwise with 46 <p> 47 <p> 48 55202 @qwx675202 55206 @qwx675206 <p> LONG ... <p> help= " whether or not the new order points should should be used " ) 55208 @qwx675208 <p> # load our input image , convert it to grayscale , and blur it slightly <p> image=cv2.imread ( " example.png " ) 55215 @qwx675215 <p> **26;8933;TOOLONG , ( 7,7 ) , 0 ) <p> # perform edge detection , then perform a dilation + erosion to <p> # close gaps in between object edges <p> **28;8961;TOOLONG <p> edged=cv2.dilate ( edged , None , iterations=1 ) <p> edged=cv2.erode ( edged , None , iterations=1 ) <p> Lines 33-37 handle @ @ @ @ @ @ @ @ @ @ single argument , --new- , which is used to indicate whether or not the- new or the- originalorderpoints- function should be used . Well default to using the- original implementation . <p> From there , we load example.png- from disk and perform a bit of pre-processing by converting the image to grayscale and smoothing it with a Gaussian filter . <p> We continue to process our image by applying the Canny edge detector , followed by a dilation + erosion to close any gaps between outlines in the edge map . <p> After performing the edge detection process , our image should look like this : <p> Figure 1 : Computing the edge map of the input image . <p> As you can see , we have been able to determine the outlines/contours of the objects in the image . <p> Now that we have the outlines of the edge map , we can apply the cv2.findContours- function to actually- extract the outlines of the objects : <p> Ordering coordinates clockwise with Python and OpenCV <p> Python <p> 50 <p> 51 <p> 52 <p> 53 <p> 54 <p> 55 <p> @ @ @ @ @ @ @ @ @ @ the edge map <p> LONG ... 55211 @qwx675211 <p> **36;8991;TOOLONG <p> # sort the contours from left-to-right and initialize the bounding box <p> # point colors <p> ( cnts , ) **28;9029;TOOLONG <p> LONG ... <p> We then sort the object contours from left-to-right , which is n't a requirement , but makes it easier to view the output of our script . <p> The next step is to loop over each of the contours individually : <p> Ordering coordinates clockwise with <p> 74 <p> 75 <p> # loop over the contours individually <p> for ( i , c ) inenumerate(cnts) : <p> # if the contour is not sufficiently large , ignore it <p> **27;9059;TOOLONG : <p> continue <p> # compute the rotated bounding box of the contour , then <p> # draw the contours <p> box=cv2.minAreaRect(c) <p> LONG ... <p> box=np.array ( box , dtype= " int " ) <p> cv2.drawContours ( image , box , @ @ @ @ @ @ @ @ @ @ show the original coordinates <p> print ( " Object # : " . format(i+1) ) <p> print(box) <p> Line 61 starts looping over our contours . If a contour is not sufficiently large ( due to " noise " in the edge detection process ) , we discard the contour region ( Lines 63 and 64 ) . <p> Otherwise , - Lines 68-71 handle computing the rotated bounding box of the contour ( taking care to use cv2.cv.BoxPoints- if we are using OpenCV 2.4 or cv2.boxPoints- if we are using OpenCV 3 ) and drawing the contour on the image- . <p> Well also print the original rotated bounding box- so we can compare the results after we order the coordinates . <p> We are now ready to order our bounding box coordinates in a clockwise arrangement : <p> When utilizing perspective transforms ( or any other project that requires ordered coordinates ) , - make sure you use our updated implementation ! <h> Summary <p> In this blog post , we started a three part series on- calculating the size of objects in images and- measuring the distance @ @ @ @ @ @ @ @ @ @ to order the 4 points associated with the rotated bounding box of each object . <p> We 've already implemented such a function in a previous blog post ; however , as we discovered , this implementation has a fatal flaw it can return the wrong coordinates under very specific situations . <p> To resolve this problem , we defined a new , updated orderpoints- function and placed it in the imutils package . This implementation ensures that our points are always ordered correctly . <p> Now that we can order our- ( x , y ) -coordinates in a reliable manner , we can move on to measuring the size of objects in an image , which is exactly what I 'll be discussing in our next blog post . <p> Be sure to signup for the PyImageSearch Newsletter by entering your email address in the form below - you wo n't want to miss this series of posts ! <h> Downloads : 55217 @qwx675217 <p> Thanks for this post Adrian , but I 'm a little confused about one aspect of the new algorithm . <p> In the new orderpoints function , @ @ @ @ @ @ @ @ @ @ order of the y-value , which gives us the top-left &amp; bottom-left points . That makes perfect sense to me . <p> My question is , why was it not just as simple for the right side points ? Rather than the Euclidean distance calculation forming the basis of the sort in lines 26 &amp; 27 , why could the right most points not have also just been sorted by their y-value to determine the top-right &amp; bottom-right points ? <p> I expect there is a scenario where that wo n't work ( which is the reason for the more complicated solution ) , but I just ca n't think of what that scenario would be . <p> Yes , you are correct . However , after the previous bug from what looked like an innocent heuristic approach , I decided to go with the Euclidean distance , that way I always knew the bottom-right corner of the rectangle would be based on the properties of triangles ( and therefore the correct corner chosen ) , rather than running into another scenario where the orderpoints function broke ( and being left again @ @ @ @ @ @ @ @ @ @ more " preemptive strike " against any bugs that could arise . <p> I used the code above as video feed on rasPi inorder to zoom in the image containing a white text embedded on a red square , mounted on a drone as part of a project . <p> The algorithm works perfectly and the zoomed image appears perfectly upright text even when image is slightly skewed .. However when the square was turned completely on the l.h.s or r.h.s . The text appeared sideways too . <p> The same situation was when the text was facing bottom <p> Unfortunately my text recognition software ( pytesseract ) couldnt read the text sideways/bottomways There are other recog. engines that can deal with this but are not free <p> Is there a way i could modify the code so that my embedded text always upright . I did give myself a thinking but could n't go that far becoz i thought that for the case if the image is completely sideways i might say that the distance between top-left and top right corner is less than what was before and hence rotate @ @ @ @ @ @ @ @ @ @ for both situations i.e. completley l.h.s and r.h.s and i am absolutely clueless on how to solve when the text is facing bottom <p> is it possible to distinguish between real objects and floor lines ? I am trying to detect objects on a floor that has lines all over and I do n't  know how to separate the real objects from rectangles/squares on the floor . <p> In most cases , yes , this should be possible . Using the Canny edge detector , you can determine lines that run most of the width/height of the image . Furthermore , these lines should also be equally spaced and in many cases intersecting . You can use this information to help prune out the floro lines that you are not interested in . <p> If I am reading this right , another special case that may need to be accounted for is skewed four-sided objects where the order of the x values may not reliably give you lefts and rights . Take for example an object with the points ( 0,0 ) , ( 2,0 ) , ( 3,4 ) @ @ @ @ @ @ @ @ @ @ smaller than the bottom left x and the sort by xs will result in top left and top right being identified as top left and bottom left respectively . <p> It almost seemed that the bounding rectangle detected the rotation angle of the coin ( relative to the the head and neck of the President being upright ) . Just wanted to make sure this was coincidence and not a desired feature of the algorithm . <p> # now that we have the top-left coordinate , use it as an # anchor to calculate the Euclidean distance between the # top-left and right-most points ; # the point with the largest distance will be # our bottom-right point <p> This may be true for images of quadrilaterals but its not generally true . Imagine a square oriented with its edges horizontal and vertical and now deform it by sliding the right-hand vertical image straight upwards so that we get a series of parallelograms . At some point it consists of two equilateral triangles stuck together and now the two right-hand points are equidistant from the top-left corner . From now @ @ @ @ @ @ @ @ @ @ than is the lower-right corner . <p> I discovered this the hard way while trying to find the grid-lines on a go board . There were a lot of false lines from diagonals and they created some very skewed parallelograms . <p> I 'm now using code that finds the top-left and bottom-left points by your method but then calculates the angle bl-&gt;tl-&gt;r for r in the rightMost points and assigns tr and br accordingly . It seems to work . <p> weeks ago , we started this round- of tutorials by learning how to ( correctly ) order coordinates in a clockwise manner using Python and OpenCV . Then , last week , we discussed how to measure the- size of objects in an image- using a reference <p> few weeks ago , I demonstrated how to order the- ( x , y ) -coordinates of a rotated bounding box in a clockwise fashion an extremely useful skill that is critical in many @ @ @ @ @ 
@@71485196 @185196/ <p> You might already be familiar with some shape descriptors , such as Hu moments . Today I am going to introduce you to a more powerful shape descriptor Zernike moments , based on Zernike polynomials that are orthogonal to the unit disk . <p> Sound complicated ? <p> Trust me , its really not . With just a few lines of code I 'll show you how to compute Zernike moments with ease . <p> OpenCV and Python versions : This example will run on- Python 2.7- and OpenCV 2.4 . X. <h> Previous Posts <p> This post is part of an on-going series of blog posts on how to build a real-life Pokedex using Python , OpenCV , and computer vision and image processing techniques . If this is the first post in the series that you are reading , go ahead and read through it ( there is a lot of awesome content in here on how to utilize shape descriptors ) , but then go back to the previous posts for some added context . <p> At this point , we already have our database of @ @ @ @ @ @ @ @ @ @ downloaded our sprites , but now we need to quantify them in terms of their outline ( i.e. their shape ) . <p> Remember playing " Whos that Pokemon ? " as a kid ? That 's essentially what our shape descriptors will be doing for us . <p> For those who did n't  watch Pokemon ( or maybe need their memory jogged ) , the image at the top of this post is a screenshot from the Pokemon TV show . Before going to commercial break , a screen such as this one would pop up with the outline of the Pokemon . The goal was to guess the name of the Pokemon based on the outline alone . <p> This is essentially what our Pokedex will be doing playing Whos that Pokemon , but in an automated fashion . And with computer vision and image processing techniques . <h> Zernike Moments <p> Before diving into a lot of code , let 's first have a quick review of Zernike moments . <p> Image moments are used to describe objects in an image . Using image moments you can calculate values @ @ @ @ @ @ @ @ @ @ ( the center of the object , in terms of x , y coordinates ) , and information regarding how the object is rotated . Normally , we calculate image moments based on the contour or outline of an image , but this is not a requirement . <p> OpenCV provides the HuMoments function which can be used to characterize the structure and shape of an object . However , a more powerful shape descriptors can be found in the mahotas package zernikemoments . Similar to Hu moments , Zernike moments are used to describe the shape of an object ; however , since the Zernike polynomials are orthogonal to each other , there is no redundancy of information between the moments . <p> One caveat to look out for when utilizing Zernike moments for shape description is the scaling and translation of the object in the image . Depending on where the image is translated in the image , your Zernike moments will be drastically different . Similarly , depending on how large or small ( i.e. how your object is scaled ) in the image , your Zernike @ @ @ @ @ @ @ @ @ @ of the Zernike moments are independent of the rotation of the object , which is an extremely nice property when working with shape descriptors . <p> In order to avoid descriptors with different values based on the translation and scaling of the image , we normally first perform segmentation . That is , we segment the foreground ( the object in the image we are interested in ) from the background ( the " noise " , or the part of the image we do not want to describe ) . Once we have the segmentation , we can form a tight bounding box around the object and crop it out , obtaining translation invariance . <p> Finally , we can resize the object to a constant NxM pixels , obtaining scale invariance . <p> From there , it is straightforward to apply Zernike moments to characterize the shape of the object . <p> As we will see later in this series of blog posts , I will be utilizing scaling and translation invariance prior to applying Zernike moments . <p> As you may know from the Hobbits and Histograms @ @ @ @ @ @ @ @ @ @ descriptors as classes rather than functions . The reason for this is that you rarely ever extract features from a single image alone . Instead , you extract features from a dataset of images . And you are likely utilizing the exact same parameters for the descriptors from image to image . <p> For example , it would n't make sense to extract a grayscale histogram with 32 bins from image #1 and then a grayscale histogram with 16 bins from image #2 , if your intent is to compare them . Instead , you utilize identical parameters to ensure you have a- consistent representation across your entire dataset . <p> That said , let 's take this code apart : <p> Line 2 : - Here we are importing the mahotas package which contains many useful image processing functions . This package also contains the implementation of our Zernike moments . <p> Lines 5-8 : We need a constructor for our ZernikeMoments class . It will take only a single parameter the radius of the polynomial in pixels . The larger the radius , the more pixels will be included in @ @ @ @ @ @ @ @ @ @ likely have to tune it and play around with it to obtain adequately performing results if you use Zernike moments outside this series of blog posts . <p> Lines 10-12 : Here we define the describe method , which quantifies our image . This method requires an image to be described , and then calls the mahotas implementation of zernikemoments to compute the moments with the specified radius , supplied in- Line 5 . <p> Overall , this is n't much code . Its mostly just a wrapper around the mahotas implementation of zernikemoments . But as I said , I like to define my descriptors as classes rather than functions to ensure the consistent use of parameters . <p> Next up , well index our dataset by quantifying each and every Pokemon sprite in terms of shape . <h> Indexing Our Pokemon Sprites <p> Now that we have our shape descriptor defined , we need to apply it to every Pokemon sprite in our database . This is a fairly straightforward process so I 'll let the code do most of the explaining . Let 's open up our favorite editor , @ @ @ @ @ @ @ @ @ @ : <p> Indexing Pokemon Sprites in Python 19 <p> 20 <p> 21 55203 @qwx675203 <p> **32;9088;TOOLONG importZernikeMoments 55220 @qwx675220 55218 @qwx675218 <p> importcPickle <p> importglob <p> importcv2 <p> # construct the argument parser and parse the arguments 55206 @qwx675206 <p> LONG ... <p> help= " Path where the sprites will be stored " ) <p> ap.addargument ( " -i " , " --index " , required=True , <p> help= " Path to where the index file will be stored " ) 55208 @qwx675208 <p> # initialize our descriptor ( Zernike Moments with a radius <p> # of 21 used to characterize the shape of our pokemon ) and <p> # our index dictionary <p> desc=ZernikeMoments(21) <p> index= <p> Lines 2-8- handle importing the packages we will need . I put our ZernikeMoments class in the pyimagesearch sub-module for organizational sake . We will make use of @ @ @ @ @ @ @ @ @ @ line arguments , cPickle for writing our index to file , glob for grabbing the paths to our sprite images , and cv2 for our OpenCV functions . <p> Then , - Lines 10-15- parse our command line arguments . The --sprites switch is the path to our directory of scraped Pokemon sprites and --index points to where our index file will be stored . <p> Line 20 handles initializing our ZernikeMoments descriptor . We will be using a radius of 21 pixels . I determined the value of 21 pixels after a few experimentations and determining which radius obtained the best performing results . <p> Finally , we initialize our index on Line 21 . Our index is a built-in Python dictionary , where the key is the filename of the Pokemon sprite and the value is the calculated Zernike moments . All filenames are unique in this case so a dictionary is a good choice due to its simplicity . <p> Time to quantify our Pokemon sprites : <p> Indexing Pokemon Sprites in Python , OpenCV <p> 38 <p> 39 <p> # loop over the sprite images <p> forspritePath inglob.glob ( args " sprites " + " /*. png " ) : <p> # parse out the pokemon name , then load the image and <p> # convert it to grayscale <p> LONG ... <p> **28;9122;TOOLONG <p> image=cv2.cvtColor ( image , cv2.COLORBGR2GRAY ) <p> # pad the image with extra white pixels to ensure the <p> # edges of the pokemon are not up against the borders <p> # of the image <p> **42;9152;TOOLONG , <p> cv2.BORDERCONSTANT , value=255 ) <p> # invert the image and threshold it <p> **28;9196;TOOLONG <p> threshthresh&gt;0=255 <p> Now we are ready to extract Zernike moments from our dataset . Let 's take this code apart and make sure we understand what is going on : <p> Line 24 : We use glob to grab the paths to our all Pokemon sprite images . All our sprites have a file extension of . png . If you 've never used glob before , its an extremely easy way to grab @ @ @ @ @ @ @ @ @ @ or extensions . Now that we have the paths to the images , we loop over them one-by-one . <p> Line 27 : - The first thing we need to do is extract the name of the Pokemon from the filename . This will serve as our unqiue key into the index dictionary . <p> Line 28 and 29 : This code is pretty self-explanatory . We load the current image off of disk and convert it to grayscale . <p> Line 34 and 35 : - Personally , I find the name of the copyMakeBorder function to be quite confusing . The name itself does n't  really describe what it does . Essentially , the copyMakeBorder " pads " the image along the north , south , east , and west directions of the image . The first parameter we pass in is the Pokemon sprite . Then , we pad this image in all directions by 15 white ( 255 ) pixels . This step is n't necessarily required , but it gives you a better sense of the thresholding on- Line 38 . <p> Line 38 and 39 @ @ @ @ @ @ @ @ @ @ or mask ) of the Pokemon image prior to applying Zernike moments . In order to find the outline , we need to apply segmentation , discarding the background ( white ) pixels of the image and focusing only on the Pokemon itself . This is actually quite simply all we need to do is flip the values of the pixels ( black pixels are turned to white , and white pixels to black ) . Then , any pixel with a value greater than zero ( black ) is set to 255 ( white ) . <p> Take a look at our thresholded image below : <p> Figure 2 : Our Abra sprite is pictured on the top and the thresholded image on the bottom . <p> This process has given us the mask of our Pokemon . Now we need the outermost contours of the mask the actual outline of the Pokemon . <p> Indexing Pokemon Sprites in Python , OpenCV , and mahotas <p> Python <p> 41 <p> 42 <p> 43 <p> 44 <p> 45 <p> 46 <p> 47 <p> 48 <p> # initialize the outline image @ @ @ @ @ @ @ @ @ @ ) of the pokemone , then draw <p> # it <p> **28;9226;TOOLONG , dtype= " uint8 " ) <p> LONG ... 55211 @qwx675211 <p> LONG ... <p> cv2.drawContours ( outline , cnts , -1,255 , -1 ) <p> First , we need a blank image to store our outlines we appropriately a variable called outline on- Line 44 and fill it with zeros with the same width and height as our sprite image . <p> Then , we make a call to cv2.findContours on Line 45 . The first argument we pass in is our thresholded image , followed by a flag cv2.RETREXTERNAL telling OpenCV to find only the outermost contours . Finally , we tell OpenCV to compress and approximate the contours to save memory using the cv2.CHAINAPPROXSIMPLE flag . <p> As I mentioned , we are only interested in the largest contour , which corresponds to the outline of the Pokemon . So , - on- Line 47 we sort the contours based on their area , in descending order . We keep only the largest contour and discard the others . <p> Finally , we draw the @ @ @ @ @ @ @ @ @ @ The outline is drawn as a filled in mask with white pixels : <p> Figure 3 : Outline of our Abra . We will be using this image to compute our Zernike moments . <p> We will be using this outline image to compute our Zernike moments . <p> Computing Zernike moments for the outline is actually quite easy : <p> Indexing Pokemon Sprites in Python , OpenCV , and mahotas <p> Python <p> 50 <p> 51 <p> 52 <p> 53 <p> # compute Zernike moments to characterize the shape <p> # of pokemon outline , then update the index <p> **30;9256;TOOLONG <p> indexpokemon=moments <p> On- Line 52 we make a call to our describe method in the ZernikeMoments class . All we need to do is pass in the outline of the image and the describe method takes care of the rest . In return , we are given the Zernike moments used to characterize and quantify the shape of the Pokemon . <p> So how are we quantifying and representing the shape of the Pokemon ? <p> Let 's investigate : <p> Indexing Pokemon Sprites in Python , OpenCV @ @ @ @ @ @ @ @ @ @ **25;9288;TOOLONG <p> ( 25 , ) <p> Here we can see that our feature vector is of 25-dimensionality ( meaning that there are 25 values in our list ) . These 25 values represent the contour of the Pokemon . <p> We can view the values of the Zernike moments feature vector like this : <p> Indexing Pokemon Sprites in Python , OpenCV , and mahotas <p> Python <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> 6 <p> &gt;&gt;&gt;moments <p> LONG ... <p> LONG ... <p> LONG ... <p> LONG ... <p> 0.02457978 <p> So there you have it ! The Pokemon outline is now quantified using only 25 floating point values ! Using these 25 numbers we will be able to disambiguate between all of the original 151 Pokemon . <p> Finally on Line 53 , we update our index with the name of the Pokemon as the key and our computed features as our value . <p> The last thing we need to do is dump our index to file so we can use when we perform a search : <p> Indexing Pokemon Sprites in @ @ @ @ @ @ @ @ @ @ <p> 56 <p> 57 <p> 58 <p> # write the index to file <p> f=open ( args " index " , " w " ) <p> f.write ( cPickle.dumps ( index ) ) <p> f.close() <p> To execute our script to index all our Pokemon sprites , issue the following command : <p> Indexing Pokemon Sprites in Python , OpenCV , and mahotas <p> Shell <p> 1 <p> $python index.py--sprites sprites--index index.cpickle <p> Once the script finishes executing all of our Pokemon will be quantified in terms of shape . <p> Later in this series of blog posts , I 'll show you how to automatically extract a Pokemon from a Game Boy screen and then compare it to our index . <h> Summary <p> In this blog post we explored Zernike moments and how they can be used to describe and quantify the shape of an object . <p> In this case , we used Zernike moments to quantify the outline of the original 151 Pokemon . The easiest way to think of this is playing " Whos that Pokemon ? " as a kid . You are given the @ @ @ @ @ @ @ @ @ @ what the Pokemon is , using only the outline alone . We are doing the same thing only we are doing it automatically . <p> This process of describing an quantifying a set of images is called " indexing " . <p> Now that we have our Pokemon quantified , I 'll show you how to search and identify Pokemon later in this series of posts . <h> Downloads : 55217 @qwx675217 <p> I need to appreciate your kind sharing and wise approaches here . Actually , I am doing my PhD and on the way about one of my mini projects need to reconrtuct the images from Zernike moments extracted from Mahotas toolbox that you introduced . There is one code I found on the web but relate to matlab and am not very sure of that . I wonder if you could kindly give me an advice . Thank you very much . <p> Congrats on doing your PhD Hamid , that 's very exciting ! As for Zernike Moments , I would suggest looking at the source code directly of the Mahotas implementation . You 'll probably need to modify @ @ @ @ @ @ @ @ @ @ suggest sending a message on GitHub to Luis , the developer and maintainer of Mahtoas he is an awesome guy and knows a lot about CV . <p> While calculating the zernlike moments how to determine or end up using the the right radius value for the specific set of images ? <p> Could you elaborate on this item of your post pls <p> Lines 5-8 : We need a constructor for our ZernikeMoments class . It will take only a single parameter " the radius of the polynomial in pixels . The larger the radius , the more pixels will be included in the computation . This is an important parameter and you 'll likely have to tune it and play around with it to obtain adequately performing results if you use Zernike moments outside this series of blog posts . <p> The easiest way to do this is to compute the cv2.minEnclosingCircle of the contour . This will give you a radius that encapsulates the entire object . Or , if you have a priori knowledge about the problem you can hardcode it . I discuss this more @ @ @ @ @ @ @ @ @ @ to say your posts are so well written . Even though I learned something I did n't  know , I did not feel lost or confused for a moment . Instead of aversion masquerading as boredom , it is *exciting* and *fun* to read the next line . <p> You may be naturally gifted , but I suspect you 've put great energy and care into crafting these lessons . <p> One question , can you index multiple images for 1 pokemon ? If so , I can index the images of every generation ( gold/silver , black/white , ) and my pokedex will become smarter . This way it might also be able to process a random image of a Pokemon and still be accurate . <p> Thanks a a lot . I was searching for Zernik moments . I found no thing except one web site providing MATLAB codes which was unclear to me . You explain all things practically , specially where results of codes are provided . I appreciate you dear Adrian . <p> Could you explain what the content of cnts might be ? Why is @ @ @ @ @ @ @ @ @ @ For our case , the image contains only one pokemon sprite . Should it give more than one external contour ? Thanks for the tutorial . <p> This blog post assumed you are using OpenCV 2.4 ; however , you are using OpenCV 3 , where the cv2.findContours return signature changed . You can read more about this change here . As for sorting the contours , sometimes there is " noise " in our input image . Sorting just ensures we grab the largest region . <p> the web and built up a database of Pokemon . We 've indexed our database of Pokemon sprites using Zernike moments . Weve analyzed query images and found our Game Boy screen @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485197 @185197/ <p> I 'm taking some of my favorites from this list . Others from my own personal library . And I 'm bundling them together and giving them away FOR FREE . <p> If you 're interested in computer vision , machine learning , IPython , Raspberry Pi , Natural Language Processing , or even parallel computing , I- absolutely guarantee- that there is a book in this giveaway for you . <h> Are you going to spam me ? <p> No , I promise not to spam you . And I wo n't share your email address with anyone . <p> You 'll get access to my free PyImageSearch Newsletter which- includes computer vision and machine learning tips , tricks , and hacks that I 've stumbled across after 8 years of working in the field . These are actionable tips and tricks that I utilize to solve real-world- computer vision and machine learning problems . Whether you are just a hobbyist or a full-fledged scientist or engineer , I guarantee that you 'll find content in this newsletter that you can take straight to the bank . @ @ @ @ @ @ @ @ @ @ . You can unsubscribe at any time ( but definitely stick around for at least a few emails ! ) <h> How do I increase my chances of winning ? <p> First , head over to the giveaway page and enter your email address . Then , grab your lucky URL- and start sharing ! Share it to your Facebook , your Twitter , your Tumblr , your LinkedIn ! Post it wherever you can ! The more people who signup using your lucky URL , the more chances you have to win ! <h> I already own one ( or more ) of these books ! <p> No problem at all ! I 'll reimburse you for the current value of the book(s) if you win . <h> Are you giving away the printed or digital versions of the books ? <p> Some books are only available as digital . For all books that are- not digital only @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485198 @185198/ <h> Archive Deep Learning <p> A few months ago I wrote- a tutorial on how to classify images using Convolutional Neural Networks ( specifically , VGG16 ) pre-trained on the ImageNet dataset with Python and the Keras deep learning library . The pre-trained networks inside of Keras are capable of recognizing- 1,000 different object categories , similar to objects we encounter in our day-to-day lives with high <p> You may have heard me mention it in a passing comment on the PyImageSearch blog Maybe I even hinted at it in a 1-on-1 email Or perhaps you simply saw the writing on the wall due to the recent uptick in Deep Learning/Neural Network tutorials here on the blog But I 'm here today to tell <p> A few months ago I demonstrated how to install the Keras deep learning library with a- Theano backend . In todays blog post I provide detailed , step-by-step instructions to install Keras using a TensorFlow backend , originally developed by the researchers and engineers on the Google Brain Team . Ill also ( optionally ) demonstrate how you can integrate OpenCV into <p> In last @ @ @ @ @ @ @ @ @ @ first-order optimization algorithm that can be used to learn a set of classifier coefficients for parameterized learning . However , the " vanilla " implementation of gradient descent can be prohibitively slow to run on large datasets in fact , it can even be considered- computationally wasteful . Instead , we should apply- Stochastic <p> Every relationship has its building blocks . Love . Trust . Mutual respect . Yesterday , I asked my girlfriend of 7.5 years to marry me . She said yes . It was quite literally the happiest day of my life . I feel like the luckiest guy in the world , not only because I have her , but also because this incredible PyImageSearch <p> If you 've been following along with this series of blog posts , then you already know what a- huge fan I am of Keras . Keras is a super powerful , easy to use Python library for building neural networks and deep learning networks . In the remainder of this blog post , I 'll demonstrate how to build a simple neural <p> In previous tutorials , I 've discussed two @ @ @ @ @ @ @ @ @ @ loss ( which we usually refer to in conjunction with Softmax classifiers ) . In order to to keep our discussions of these loss functions straightforward , I purposely left out an important component : - regularization . While our loss function allows us to determine how well ( or poorly ) our <p> Normally , I only publish blog posts on Monday , - but I 'm so excited about this one that it could n't wait and I decided to hit the publish button early . You see , just a few days ago , - Fran+ois Chollet pushed three Keras models ( VGG16 , VGG19 , and ResNet50 ) online these networks- are- pre-trained- on the ImageNet dataset , meaning that they can recognize- 1,000 <p> In todays blog post , we are going to- implement our first Convolutional Neural Network ( CNN ) - LeNet - using Python and the Keras deep learning package . The LeNet architecture was first introduced by LeCun et al . in their 1998 paper , - Gradient-Based Learning Applied to Document Recognition. - As the name of the paper suggests , the @ @ @ @ @ @ @ @ @ @ start todays blog post by asking a series of questions which will then be addressed later in the tutorial : What are image convolutions ? What do they do ? Why do we use them ? How do we apply them ? And what role do convolutions play in deep learning ? The word " @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485200 @185200/ <h> Hands-on with the NVIDIA DIGITS DevBox for Deep Learning <p> I will be doing more Deep Learning and Convolutional Neural Network tutorials on the PyImageSearch blog over the coming months . <p> I 'm dead serious about this - and I 've put my money where my mouth is and- invested in some real hardware for deep learning . <p> To learn more about my investment , the NVIDIA DIGITS DevBox , and the new tutorials coming to the PyImageSearch blog , keep reading . <h> Hands-on with the NVIDIA DIGITS DevBox for Deep Learning <p> For anyone that is interested in the NVIDIA DIGITS DevBox for deep learning and perhaps more importantly , the rational that led- to me purchasing a pre-configured deep learning system instead of building my own Ive included my experience in working through the decision process , making the purchase , and unboxing the system . In future blog posts , I 'll be reviewing how Ive setup and configured the system for my own optimal setup . <p> But there is also a- third side that does n't  often get discussed on the PyImageSearch blog @ @ @ @ @ @ @ @ @ @ - the- entrepreneur- and ( occasional ) consultant . <p> Its becoming increasingly rare that I can take on- new consulting/contracting work , but when I do , I tend to be- very selective about what the project and budget is . And over the past few years , I 've noticed I 've been using- more and more deep learning within my projects ( both for contracting work and for personal/business projects ) . <p> This may appear in stark contrast ( and on the surface , perhaps a bit hypocritical ) to my blog post on- getting off the deep learning bandwagon - but the title of that article was n't the primary point . <p> Instead , the purpose of that ( controversial ) blog post was to drive a single detail home : <p> " Machine learning is n't a tool . It 's a methodology with a rational thought process that is entirely dependent on the problem we are trying to solve . We should n't blindly apply algorithms and see what sticks aside from spot-checking . We need to sit down , explore the feature space ( @ @ @ @ @ @ @ @ @ @ and then consider our best mode of action . " <p> Deep learning , just like Support Vector Machines , Random Forests , and other machine learning algorithms all have a rational process and assumptions to when we should use each particular model . - There is a time and a place where we use deep learning - you just need to be- mindful in your selection of algorithms for a particular program . <p> My hope in publishing deep learning tutorials on the PyImageSearch blog is to better illuminate- when and- where deep learning is appropriate for computer vision tasks . <h> So what does this have to do with the PyImageSearch blog ? <p> Great question . <p> I 've said it before in other blog posts , and I 'll say- it again here today : <p> Every day I get more and more requests for deep learning tutorials . And up until 2 months ago , - I was still writing the PyImageSearch Gurus course. - I simply did not have the time , energy , or attention span to start planning out deep learning tutorials which by @ @ @ @ @ @ @ @ @ @ ( in terms of thought process , computational effort , and experiments ) for me to create . <p> But- more importantly , I 've reclaimed a bunch of my- energy and- attention both of which are- critical in creating high-quality tutorials . Over the years I spent in graduate school , writing my dissertation , running the PyImageSearch blog , authoring- Practical Python and OpenCV , and creating the PyImageSearch Gurus course , Ive- mastered the ability to bang out 5,000+ words in a single sitting . Time is n't a problem for me when it comes to writing what- really matters are my energy and attention levels . <p> Over the next year , you can expect- more deep learning tutorials to be published on the PyImageSearch blog . It wont be an- immediate change , but it will slowly ramp up over the next 3-4 months as I start creating a backlog of posts . <p> The point is this : <p> If you 're interested in deep learning , specifically deep learning and computer vision , PyImageSearch is the place to be . <h> Running the numbers <p> Okay @ @ @ @ @ @ @ @ @ @ an NVIDIA DIGITS DevBox that 's not a small amount of money by any means . <p> So how did I justify this- huge number ? <p> As I mentioned above , I 'm an entrepreneur , a scientist , and at heart , a business person this implies there is ( some sort of ) logic behind my actions . If you show me the numbers , they work out , - and they align with my goals - I can justify the investment and plan accordingly . <p> I started out the assessment by looking at the facts : <p> Fact #1 : I am currently spending either $0.65 per hour on an Amazon EC2 g2.2xlarge instance- or- $2.60 per hour on an g2.8xlarge instance. - Most of my EC2 hours are spent using a g2.8xlarge instance . <p> Fact #2 : Both of these EC2 instances have- less RAM- than the NVIDIA DIGITS DevBox . And based on the way Amazon computes virtual-CPUs , the g2.8xlarge has only marginally better CPU ( again , if you can trust the vCPU allocation ) . <p> Fact #5 : 4GB alone @ @ @ @ @ @ @ @ @ @ computation- across multiple GPUs . That 's fine , but does n't  seem worth it in the long run . Ideally , I can run either- four experiments in- parallel- or- spread the computation across four cards , thereby decreasing the time it takes to train a given model . The Titan X is clearly the winner here . <p> Next , I ran the numbers to determine the intersection between the hourly rates of an g2.8xlarge instance and the $15,000 upfront investment : <p> Figure 1 : Plotting the intersection between the EC2 g2.8xlarge hourly rate and the upfront cost of $15,000 for the NVIDIA DIGITS DevBox . The break-even point is at approximately 240 days . <p> This gives me approximately 5,769 hours ( 240 days ) of compute time on an g2.8xlarge instance . <p> With a break even point of only- 240 days ( where it can take days to weeks for a single model to train ) , the decision started to become more clear . <p> Now , the next question I had to ask myself was : <p> " Do I order the hardware @ @ @ @ @ @ @ @ @ @ Or do I go with a pre-configured machine pay a bit of markup ? " <p> I 'll get a fair amount of negative feedback for this point , - but in my opinion , I tend to lean towards " done for you solutions " . <p> Why ? <p> Three reasons : I have limited- time , - energy , and- attention . <p> Anytime I can spend money to pay a professional to outsource a task that I am either ( 1 ) not good at , ( 2 ) do n't  like doing , or ( 3 ) is not worth my time/energy , I 'll tend to move the task off my plate - this is the exact rationale that enables me to work- smarter instead of- harder . <p> So , let 's presume that I could buy- the hardware to create a comparable system to the NVIDIA DIGITS DevBox for approximately $8,000 - that saves me $7,000 right ? <p> Well , not so fast . <p> I 'm not going to say what my hourly consulting rate is , but let 's ( for the sake of @ @ @ @ @ @ @ @ @ @ my time : - $7,000 / 250 per hour = 28 hours . <p> In order for this time-to-money tradeoff ( let alone the- attention and- energy it will take ) , within 28 hours of my own time , I need to : <p> Research the hardware I need . <p> Purchase it . <p> Get it all together in my office . <p> Assemble the machine . <p> Install the OS , software , drivers , etc . <p> Can I do- all this in 28 hours with- minimum- context switching ? <p> Honestly , I do n't  know . I- probably could . <p> But what if I 'm wrong ? <p> And the better question to ask is : <p> What if something breaks ? <p> I 'm not a hardware person , I do n't  enjoy working with hardware its just the way Ive always been . <p> If I build my own system , I 'm my own support staff . But if I go with NVIDIA , I have the entire DevBox team to help support troubleshoot , and resolve the issue . <p> So , @ @ @ @ @ @ @ @ @ @ 15 hours to order the hardware , put it together , install the required components , and ensure that its working properly . <p> That leaves me with- 28 15 = 13 hours of my time left to handle- any troubleshooting issues that occur over the lifetime of the machine . <p> Is that realistic ? <p> No , its not . <p> And from this perspective ( i.e. , my perspective ) , this investment makes sense . You may be in a totally different situation but between the projects I 'm currently working on , the companies I run , and the PyImageSearch blog , I 'll be utilizing deep learning- a lot more in the coming future . <p> Factor in the fact that I not only value my time , but also by- energy and- attention , this further justifies the upfront cost . Plus , this better enables me to create awesome deep learning tutorials for the PyImageSearch blog . <p> Note : I 'm not factoring in the increase of my utility bill which- will happen , but in the long run , this becomes- marginalized- due @ @ @ @ @ @ @ @ @ @ , and ship faster . <h> Ordering the NVIDIA DIGITS DevBox <p> Placing an order for the NVIDIA DIGITS DevBox is n't as simple as opening a webpage , entering your credit card information , and clicking the- " Checkout " button . Instead , I needed to contact NVIDIA- and fill out the access form . <p> Within 48 hours I was in talks with a representative where I created the PO ( Purchase Order ) and shipment . Once the terms were agreed upon , I cut a check and overnighted it to NVIDIA . Overall , it was quite the painless process . <p> However , I will note that if you 're not doing a lot of shipping/receiving , the actual- shipment of the DevBox- may be a bit confusing . I personally do n't  have much experience in logistics , but luckily , my father does . <p> I called him up for some clarity regarding- " Freight terms " and " EX-WORKS " . - <p> Essentially , the EX-WORKS comes down to : <p> EX-WORKS ( EXW ) is an international trade term that describes @ @ @ @ @ @ @ @ @ @ goods ready for pickup at his or her own place of business . All other transportation- costs and risks are assumed by the buyer . ( source ) <p> What this boils down to is simple : <p> NVIDIA will be putting together your system . <p> But once its boxed up and on their loading bay , - the responsibility is on you . <p> How did I handle this ? <p> I used my FedEx business account and shelled out extra cash for insurance on the shipment . Not a big deal . <p> The only reason I included this section in the blog post is to help out others who are in a similar situation who may be unfamiliar with the terms . <h> Unboxing the NVIDIA DIGITS DevBox <p> The DevBox ships in a hefty 55lb box , but the cardboard is- extremely thick and well constructed : <p> Figure 2 : The box the NVIDIA Digits DevBox ships in . <p> The DevBox is also very securely packed in a styrofoam container to prevent any damage to your machine . <p> While unboxing the DevBox @ @ @ @ @ @ @ @ @ @ product for the first time , its still quite the- pleasure . After unboxing , the NVIDIA DIGITS DevBox machine itself measures 18 ? in height , 13 ? in width , and 16.3 ? in depth : <p> Figure 3 : The NVIDIA DIGITS DevBox fully unboxed . <p> You 'll notice there are three hard drive slots on the front of the machine : <p> Figure 4 : Three hard drive slots on the front of the DevBox . <p> This is where you slide in the three 3TB hard drives included in the shipment of your DevBox : <p> Figure 6 : Slotting the drives into their respective bays is easy . After slotting , the drives are securely locked in place . <p> On first boot , you need to connect a monitor , keyboard , and mouse to configure the machine : <p> Figure 7 : Make sure you connect your monitor to the- first graphics card in the system . <p> Pressing the power button starts the magic : <p> Figure 8 : Booting the DevBox . <p> Next , you 'll see the boot sequence @ @ @ @ @ @ @ @ @ @ booting the NVIDIA DIGITS DevBox . <p> After the machine finishes booting , you 'll need to configure Ubuntu ( just like you would with a standard install ) . After setting your keyboard layout , timezone , username , password , etc. , the rest of the configuration is automatically taken care of . <p> Its also worth mentioning that the BIOS dashboard is quite informative and beautiful : <p> Figure 10 : The BIOS dashboard on the NVIDIA DIGITS DevBox . <p> Overall , I 'm quite pleased with the setup process . In under 30 minutes , I had the entire system setup and ready to go . <h> You 're not plugging that directly into the wall , are you ? <p> Protect your investment get a quality Uninterrupted Power Supply ( UPS ) . Ill detail exactly which UPS ( and rack ) I chose in the next blog post . The images gathered for this blog post were mainly for demonstration purposes during the initial unboxing . <p> In short , I would- not recommend having your DevBox sitting directly on top of carpet or plugged into @ @ @ @ @ @ @ @ @ @ for trouble . <h> Summary <p> Admittedly , this is a lengthy blog post , so if you 're looking for the TLDR , its actually quite simple : <p> Ill be doing- a lot more deep learning tutorials on the PyImageSearch blog in the next year . It will start with a slow ramp-up over the next 3-4 months with more consistency building towards the end of the year . <p> In order to facilitate the creation of better deep learning tutorials ( and for use with my own projects ) , I 've put my money where my mouth is and invested in an NVIDIA DIGITS DevBox . <p> The DevBox was a delight to setupalthough there are a few practical tips and tricks that I 'll be sharing in next- weeks blog- post . <p> If you 're interested in diving into the world of deep learning , the PyImageSearch blog is the place to be . <p> I 'm sure anyone with a keen interest in computer vision has stumbled on ML articles and thought " Damn that looks interesting " , I know I have . My ML knowledge is @ @ @ @ @ @ @ @ @ @ forward to the next bunch of articles ! <p> No , certainly not . Ill be detailing how to setup an Amazon EC2 system for deep learning soon . You 'll also be able to run more the " more simple " examples on your CPU . The main reason I 'm using the DevBox is for the speed , ability to work in parallel , and to facilitate tutorial creation . <p> Nice box ! Some thoughts : Figure 1 does not take into account the increase in computing power that will happen in the next 200 days . With the release of the Nvidia 1080 it reasonable to assume the cost of a g2.8xlarge will come down within the next 200 days . Similarly building a Nvidia 1080 based system now will be more cost effective than a Nvidia Titan X based system . Finally , running this box 200 days straight will also have an significant effect on your power bill and then there 's the noise , generated heath etc . Anyway , this seems to get really complex if you want to take everything into account . Enjoy @ @ @ @ @ @ @ @ @ @ The 1080 has only 8GB of memory versus the Titan X. Going from 4GB on the K520 is nice , but it does n't  compare to the 12GB of the Titan X. <p> 2 . In my opinion , the 1080 is more geared towards gamers . It will likely be another year or two before NVIDIA releases another GPU that is heavily targeted at deep learning researchers . <p> 3 . Yes , running this machine will increase my electrical bill , there is no doubt in that . However , this increase is less of an issue ( in my case ) because its simply a business expense . The money has to come from some where , sure , but its not that big of a deal in the long run . <p> 4 . I presume that the g2.8xlarge price will decrease in the future . But by how much ? And when ? These are total unknowns . <p> Nice post and I enjoyed it very much . I particularly like the way you explain why you come to certain decisions . <p> Just one @ @ @ @ @ @ @ @ @ @ you expect readers to have more powerful machines ? Currently , I 'm trying out many of your examples in a VM in a typical Mac or sometimes , in a Raspberry Pi . <p> BTW , I recently read a post about how Dan Goldin expand his scientific knowledge in a new area at 75 and build an AI startup . That amazed me very much . Anyway , look forward to learn more about deep learning . <p> Hey Rick most of my examples in future blog posts will assume that you have a GPU or an Amazon EC2 GPU instance . Some examples will be able to run on the CPU as well ( although only the most basic ones ) . In general , I would n't recommend using the Pi for the future tutorials . <p> I see how Crazy you are to dive deep into deep learning . Would love to see you work on one complex real world challenge . " I want you to design and train your model to detect and identify " Empty " and " Occupied " Parking spots <p> Nice @ @ @ @ @ @ @ @ @ @ still very much a beginner when it comes to Deep Learning , hopefully I can come up with a similar justification and logic for my own box in the future . Looking forward to your tutorials . <p> In the meantime I 'll be sticking with AWS , as I have found the spot pricing on the g2 instances 1/3 cost of the on-demand pricing to fit within my budget for now , and still being way faster than my laptop . Hopefully , AWS will introduce some updated GPU instance soon with newer GPUs . I 've also found some of the marketplace AMI such as this Tensorflow one pretty useful ( if there are other beginners here ) since I 'm not exactly a Linux installation wizard : LONG ... <p> Just thought I would live my two cents here . Great post and great work . I have followed your work for some time now . Keep it up . <p> In regards to your hardware , we build these systems in house for the work we done with machine learning , computer vision and drones . <p> I have @ @ @ @ @ @ @ @ @ @ the links to a few of these boxes we use . <p> At a fraction of the cost obviously .. <p> A lightweight simple workstation for basic stuff . The card that is on there now is not what we have in it this is our base configuration . But we have the same set up with a 980ti Hybrid from EVGA and works great ! <p> One thing we will add a note here on is PC Part Picker itself . Its a great website and tool for doing your research . Moverover , they already filter out what is compatible and what is not so that makes it a whole lot easier . And it links it straight to Amazon . <p> Each time we build a system it takes about 30-60 minutes top from start to finish . <p> This is a great resource , thanks for sharing Daniel . In my particular situation , I still would n't have gone this route , mainly because the time/cost tradeoff for me to setup the hardware ( when I do n't  like working with hardware ) would not have @ @ @ @ @ @ @ @ @ @ are looking to save money or enjoy the hardware aspect , this is a fantastic resource . <p> Hi Adrian ! Great Post ! Thanks for sharing . We have procured a dev-box recently ( not directly from NVIDIA though ) . We are 3-4 of us currently sharing this machine . Do you happen to know how we would go about setting up DIGITS for each user ? Currently , everyones models are getting dumped into the same folder . Unfortunately , the box did n't  come with good pre-configured permissions set up for multiple users . <p> I 'm honestly not sure about a per-user install of DIGITS . I 'm sure its possible via some PATH manipulation , but I would suggest posting on the official DIGITS GitHub @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485201 @185201/ <p> I 'm going to start this post by clueing you in on a piece of personal history that very few people know about me : as a kid in early high school , I used to spend nearly- every single Saturday at the local RC ( Remote Control ) track about 25 miles from my house . <p> You see , I used to race 1/10th scale ( electric ) RC cars on an amateur level . Being able to spend time racing every Saturday was definitely one of my favorite ( if not- the- favorite ) experience of my childhood . <p> I used to race the ( now antiquated ) TC3 from Team Associated . From stock engines , to 19-turns , to modified ( I was down to using a 9-turn by the time I stopped racing ) , I had it all . I even competed in a few big-time amateur level races on the east coast of the United States and placed well . <p> But as I got older , and as racing became more expensive , I started spending less time at @ @ @ @ @ @ @ @ @ @ , this was probably one of the smartest decisions that I 've ever made , even though I loved racing so much it seems quite unlikely that I could have made a career out of racing RC cars , but I certainly have made a career out of programming and entrepreneurship . <p> So perhaps it comes as no surprise , now that I 'm 26 years old , that I felt the urge to get back into RC . But instead of cars , I wanted to do something that I had never done before - drones and quadcopters . <p> Which leads us to the purpose of this post : - developing a system to automatically detect targets from a quadcopter video recording . <p> If you want to see the target acquisition in action , I wont keep you waiting . Here is the full video : <p> Otherwise , keep reading to see how- target detection in quadcopter and drone video streams is done using Python and OpenCV ! <p> OpenCV and Python versions : In order to run this example , you 'll need Python 2.7 and OpenCV @ @ @ @ @ @ @ @ @ @ While I have a ton of experience racing RC cars , I 've never flown anything in my life. - Given this , I decided to go with a nice entry level drone so I could learn the ropes of piloting without causing too much damage to my quadcopter or my back account . Based on the excellent recommendation from Marek Kraft , I ended up purchasing the Hubsan X4 : <p> Figure 1 : My Hubsan X4 quadcopter . Its tiny ( fits in the palm of my hand ) , inexpensive ( only $45 ) , great for learning to fly , and comes with a built-in camera . <p> I went with this quadcopter for 4 : reasons : <p> The Hubsan X4 is excellent for- first time pilots- who are just learning to fly . <p> Its very tiny you can fly it indoors and around your living room until you get used to the controls . <p> It comes with a camera that records video footage to a micro-SD card . As a computer vision researcher and developer , having a built in camera was critical @ @ @ @ @ @ @ @ @ @ only 0.3MP , but that was good enough for me to get started . Hubsan also has- another model that sports a 2MP camera , but its over double the price . Furthermore , you could always mount a smaller , higher resolution camera to the undercarriage of the X4 , so I could n't justify the extra price as a novice pilot . <p> Not only is the Hubsan X4- inexpensive ( only $45 ) , but- so are the replacement parts ! When you 're just learning to fly , you 'll be going through a lot of rotor blades . And for a pack of 10 replacement rotor blades you 're only looking at a handful of dollars . <h> Finding targets in drone and quadcopter video streams using Python and OpenCV <p> But of course , I am a computer vision developer and researcherso after I learned how to fly my quadcopter without crashing it into my apartment walls repeatedly , I decided I wanted to have some fun and apply my computer vision expertise . The rest of this blog post will detail how to find and detect targets @ @ @ @ @ @ @ @ @ @ The first thing I did was create the " targets " that I wanted to detect . These targets were simply the PyImageSearch logo . I printed a handful of them out , cut them out as- squares , and pasted them to my apartment cabinets and walls : <p> Figure 2 : The target marker we are going to detect in our drone video stream using Python + OpenCV . <p> The end goal will be to detect these targets in the video recorded by my Hubsan X4 : <p> Well , if you 're a regular follower of the PyImageSearch blog , you may know that I 'm a big fan of using contour properties to detect objects in images . <p> IMPORTANT : A clever use of contour properties can save you- from training complicated machine learning models . <p> Why make a problem more challenging than it needs to be ? <p> And when you think about it , detecting squares in images is n't too terribly challenging of a task when you consider the- geometry of a square . <p> Let 's think- about the properties of a square @ @ @ @ @ @ @ @ @ @ square has four vertices . <p> Property #2 : - A square will have ( approximately ) equal width and height . Therefore , the- aspect ratio , or more simply , the ratio of the width to the height of the square will be approximately 1 . <p> Leveraging these two properties ( along with two other contour properties ; the convex hull- and- solidity ) , well be able to detect our targets in an image . <p> Anyway , enough talk . Let 's get to work . Open up a file , name it drone.py- , and insert the following code : <p> Finding targets in drone and 29 <p> 30 <p> 31 55203 @qwx675203 55218 @qwx675218 <p> importcv2 55202 @qwx675202 @ @ @ @ @ @ @ @ @ @ --video " , help= " path to the video file " ) 55208 @qwx675208 <p> # load the video <p> **28;9315;TOOLONG " video " ) <p> # keep looping <p> whileTrue : <p> # grab the current frame and initialize the status text <p> ( grabbed , frame ) =camera.read() <p> status= " No Targets " <p> # check to see if we have reached the end of the <p> # video <p> ifnotgrabbed : <p> break <p> # convert the frame to grayscale , blur it , and detect edges <p> gray=cv2.cvtColor ( frame , cv2.COLORBGR2GRAY ) <p> **29;9345;TOOLONG , ( 7,7 ) , 0 ) <p> **31;9376;TOOLONG <p> # find contours in the edge map <p> LONG ... 55211 @qwx675211 <p> We start off by importing our necessary packages , argparse- to parse command line arguments and cv2- for our OpenCV bindings . <p> We then parse our command line arguments on- Lines 6-8 . Well need only a single switch here , --video- , which is the path to the video file our quadcopter recorded while in flight . In an ideal situation we could stream the @ @ @ @ @ @ @ @ @ @ but the Hubsan X4 does not have that capability . Instead , well just have to post-process the video , but if you were to stream the video , - the same principles would apply . <p> Line 11- opens our video file for reading and- Line 14 starts a loop , where the goal is to loop over each frame of the input video . <p> We grab the next frame of the video from the buffer on- Line 16 by making a call to camera.read()- . This function returns a tuple of 2 values . The first , grabbed- , is a boolean indicating whether or not the frame was successfully read . If the frame was not successfully read or if we have reached the end of the video file , we break from the loop on- Lines 21 and 22 . <p> The second value returned from camera.read()- is the frame- itself this frame is a NumPy array of size N x M pixels which well be processing and attempting to find targets in . <p> Well also initialize a status- string on- Line 17 which @ @ @ @ @ @ @ @ @ @ current frame . <p> The first few pre-processing steps are handled on- Lines 25-27 . Well start by converting the frame from RGB to grayscale- since we are not interested in the color of the image , just the intensity . Well then blur the grayscale frame to remove high frequency noise , allowing us to focus on the actual structural components of the frame . And well finally perform edge detection to reveal the outlines of the objects in the image . <p> Figure 4 : An example of one of the edge maps generated from a single frame . Notice how the outlines of the door , door handle , refrigerator , and targets are revealed . <p> These outlines of the " objects " could correspond to the outlines of a door , a cabinet , the refrigerator , or the target itself . To distinguish between these outlines , well need to leverage contour properties . <p> But first , well need to find the contours of these objects from the edge map . To do this , well make a call to cv2.findContours- on- Lines @ @ @ @ @ @ @ @ @ @ of contoured regions in the edge mapped image . <p> Now that we have the contours , let 's see how we can leverage them to find the actual targets in an image : <p> Finding targets in drone and quadcopter video streams using <p> 70 <p> 71 <p> # loop over the contours <p> forcincnts : <p> # approximate the contour <p> peri=cv2.arcLength ( c , True ) <p> **35;9409;TOOLONG , True ) <p> # ensure that the approximated contour is " roughly " rectangular <p> **39;9446;TOOLONG : <p> # compute the bounding box of the approximated contour and <p> # use the bounding box to compute the aspect ratio <p> @ @ @ @ @ @ @ @ @ @ <p> aspectRatio=w/float(h) <p> # compute the solidity of the original contour <p> area=cv2.contourArea(c) <p> **39;9514;TOOLONG ( c ) ) <p> **29;9555;TOOLONG <p> # compute whether or not the width and height , solidity , and <p> # aspect ratio of the contour falls within appropriate bounds <p> **26;9586;TOOLONG <p> **28;9614;TOOLONG <p> LONG ... <p> # ensure that the contour passes all our tests <p> ifkeepDims andkeepSolidity andkeepAspectRatio : <p> # draw an outline around the target and update the status <p> # text <p> LONG ... <p> status= " Target(s) Acquired " <p> # compute the center of the contour region and draw the <p> # crosshairs <p> M=cv2.moments(approx) <p> LONG ... <p> LONG ... <p> LONG ... <p> LONG ... <p> LONG ... <p> The snippet of code above- is where the real bulk of the target detection happens . <p> Well start by looping over each of the contours on- Line 34 . <p> And then for each of these contours , well apply- contour approximation. - As the name suggests , - contour approximation , is an algorithm for reducing the number of points in a curve @ @ @ @ @ @ @ @ @ @ approximation . This algorithm is commonly known as the Ramer-Douglas-Peucker algorithm , or simply the split-and-merge algorithm . <p> The general assumption of this algorithm is that a curve can be approximated by a series of short line segments . And we can thus- approximate a given number of these line segments to reduce the number of points it takes to construct a curve . <p> Well apply the same principles here . If our approximated contour has between 4 and 6 points ( Line 40 ) , then well consider the object to be rectangular and a candidate for further processing . <p> Note : Ideally , an approximated contour should have- exactly 4 vertices , but in the real-world this is not always the case due to sub-par image quality or noise introduced via motion blur , such as flying a quadcopter around a room . <p> Next up , well grab the bounding box of the contour on- Line 43- and use it to compute the- aspect ratio of the box ( Line 44 ) . The aspect ratio is defined as the ratio of the width @ @ @ @ @ @ @ @ @ @ box : <p> aspect ratio = width / height <p> Well also compute two more contour properties on- Lines 47-49 . The first is the simple area of the bounding box , or the number of- non-zero pixels inside the bounding box region divided by the- total number of pixels in the bounding box region . <p> Well also compute the area of the- convex hull , and finally use the area of the original bounding box and the area of the convex hull to compute the- solidity : <p> solidity- = original area / convex hull area <p> Since this is meant to be a super practical , hands-on post , I 'm not going to go into the details of the convex hull . But if contour properties really interest you , I have over 50+ pages worth of tutorials on the contour properties inside the PyImageSearch Gurus course if that sounds interesting to you , I would definitely reserve your spot in line for when the doors to the course open . <p> Anyway , now that we have our contour properties , we can use them to @ @ @ @ @ @ @ @ @ @ : <p> Line 53 : Our target should have a width- and height- of at least 25 pixels . This ensures that small , random artifacts are filtered from our frame . <p> Line 54 : - The target should have a solidity- value greater than- 0.9 . <p> Line 55 : - Finally , the contour region should have an aspectRatio- that is is between- 0.8 &lt;= aspect ratio &lt;= 1.2 , which will indicate that the region is approximately square . <p> Provided that all these tests pass on- Line 58 , we have found our target ! <p> Lines 61 and 62- draw the bounding box region of the approximated contour and update the status- text . <p> Then , - Lines 66-71- compute the center of the bounding box and use the center- ( x , y ) -coordinates to draw crosshairs on the target . <p> Let 's go ahead and finish up this script : <p> Finding targets in drone and quadcopter video streams using <p> 86 <p> 87 <p> # draw the status text on the frame <p> LONG ... <p> ( 0,0,255 ) , 2 ) <p> # show the frame and record if a key is pressed <p> cv2.imshow ( " Frame " , frame ) <p> **27;9644;TOOLONG <p> # if the ' q ' key is pressed , stop the loop <p> ifkey==ord ( " q " ) : <p> break <p> # cleanup the camera and close any open windows <p> camera.release() <p> cv2.destroyAllWindows() <p> Lines 74 and 75- then draw the status- text on the top-left corner of our frame , while- Lines 78-83- handle if the q- key is pressed , and if so , we break from the loop . <p> Finally , - Lines 86- and 87- perform cleanup , release the pointer to the video file , and close all open windows . <h> Target detection results <p> Even though that was less than 100 lines of code , including comments , that was a decent amount of work . Let 's see our target detection in action . <p> Open up a terminal , @ @ @ @ @ @ @ @ @ @ the following command : <p> Finding targets in drone and quadcopter video streams using Python and OpenCV <p> As you can see , our Python script has been able to successfully detect the targets ! <p> here 's another still image from a separate video of the PyImageSearch targets being detected : <p> Figure 6 : Our Python + OpenCV script has been able to successfully detect the targets in our video stream . <p> So as you can see , a our little target detection script has worked quite- well ! <p> For the full demo video of our quadcopter detecting targets in our video stream , be sure to watch the YouTube video at the top of this post . <h> Further work <p> If you watched the YouTube video at the top of this post , you may have noticed that sometimes the crosshairs and bounding box regions of the detected target tend to " flicker " . This is because many ( basic ) computer vision and image processing functions are very sensitive to noise , especially noise introduced due to motion a blurred square can easily @ @ @ @ @ @ @ @ @ @ our target detection tests can fail . <p> To combat this , we could use some more advanced computer vision techniques . For one , we could use adaptive correlation filters , which I 'll be covering in a future blog post . <h> Summary <p> This article detailed how to use simple contour properties to find targets in drone and quadcopter video streams using Python and OpenCV . <p> The video streams were captured using my Hubsan X4 , which is an excellent starter quadcopter that comes with a built-in camera . While the Hubsan X4 does not directly stream the video back to your system for processing , it does record the video feed to a micro-SD card so that you can post-process the video later . <p> However , if you had a quadcopter that included video stream properties , you could absolutely apply the same techniques proposed in this article . <p> Anyway , I hope you enjoyed this post ! And please consider sharing it on your Twitter , Facebook , or other social media ! <h> Downloads : <p> Just a quick note : The @ @ @ @ @ @ @ @ @ @ 350mb which might take a bit of time to download if you are on a slow connection . I also pay for all Amazon S3 hosting fees out of pocket , so if you enjoy the posts on the PyImageSearch blog , please consider supporting the blog by picking up a copy of my book , Practical Python and OpenCV + Case Studies . Otherwise , please enjoy the code and happy hacking ! 55217 @qwx675217 <p> This article is just amazing . I wish I had time to work on that kind of applications . I have a suggestion : do you think is easy to extend this application to the detection of colors ? I have played with your tutorial regarding color , but I have found that detecting colors is not so easy when you take into account that the background conditions can change , or when the targets are moving . <p> I find your job amazing , and you are the " Lionel Messi " of image processing ! ! ! <p> Wow , being mentioned in the same sentence as Lionel Messi , that 's @ @ @ @ @ @ @ @ @ @ German would n't that make me more of a Miroslav Klose ? = <p> Detecting colors in images can be tricky without controlled lighting conditions , its definitely not easy if you expect the lighting conditions to change . However , I do have a post on detecting colors in image and another one on detecting skin definitely take a look if you are interested ! <p> I converted this code to run on live video . The problem I have is the video only shows when it detects a target ( square ) . As soon as there are no more targets it freezes until the next target . I 've tried moving the cv2.imshow around and putting it in multiple times but it does n't  change the output . Any ideas ? <p> Hey Charlie , that definitely is a strange error . It sounds like your cv2.imshow call is likely in the wrong place . Keep it in the same place as the original code listing and everything should work fine . <p> Yes and no . I had the pyimagesearch logo on my other computer so I just @ @ @ @ @ @ @ @ @ @ realized almost any square object gets detected . It would be nice to detect that one specific py logo and no other square objects . I 'm sure that is significantly more advanced . <p> Hey Charlie , you 're right . Pretty much any square object will be detected . In followup posts I 'll explain how to detect all square targets and then filter them based on their contents . Its a little more challenging , but honestly not much . <p> Hey Charlie , I know its been some time since you posted this , but , if possible could you please share some insight as to how you converted the code to run on live video feed . I 'm using the Pi Camera Module . <p> I really appreciate the quick reply Adrian . I used the unified camera access as you suggested , along with the original target tracking code . The live feed tracking is now working great ! Thank you very much for these in depth tutorials . Theyre excellent ! <p> Hi Charlie , Could you help us shed some light on how you would @ @ @ @ @ @ @ @ @ @ , not from the Pi camera , but I imagine a drone video source via UDP , specified through a SDP H264 file ? I tried to specify the source as **26;9673;TOOLONG and opencv seems to connect to the source but returns chroma errors and the decoding stops after some frames . Any help would be fabulous . <p> Hi Adrian , this is impressive . Can this code be used in detecting leader car in roads in realtime ? Is is possible to measure the distance between the host and leader car with the integration of this code ? It would be very helpful if you please let me know . <p> Hey Amit , this code couldnt directly be used to detect cars in real-time . There would be too much motion going on and cars can have very different shapes , as opposed to a square target which is always square . You would need to use a bit of machine learning to train your own car detector classifier . As for detecting the distance between objects , or in this case , cars , I have @ @ @ @ @ @ @ @ @ @ and an object . Definitely take a look as it should be quite applicable to your problem . <p> nice tutorial ! if i 'm understand it correctly , you are not comparing the marker to predefined image . so what if i want to recognize several different markers ? <p> i was thinking to try this in real time video and let my robot act according the marker he see . maybe if i run the algorithm on several markers to get they solidity then use it later to identify the marker in real time .. did n't  test it yet should it work ? <p> Correct , I am not comparing to a pre-defined marker . If you wanted to use multiple markers you would need to ( 1 ) detect the presence of a marker and ( 2 ) identify which marker it is . Ill be doing a followup post in the future using multiple markers . It does n't  add too much complexity to the code . <p> I am actually working on the same type of project , and was wondering how do you identify @ @ @ @ @ @ @ @ @ @ background , without explicitly telling the drone to look for red color ? I am thinking using histogram and then finding the peaks in it . Will this method work ? Could you please advise on this ? <p> Its pretty hard to tell a computer what to look for without actually specifying what it looks like = There are many methods to detect objects in images . Some a very simple like color based methods . Others are shape based , like the one in the post you 're reading now . Some methods can use template matching . And even more advanced methods require the usage of machine learning . In either case , you 'll obtain much better results if you can instruct your CV system what to look for . Peak finding methods may work , but theyll also be extremely prone to error . <p> I am working on making a drone which will follow a object that is moving i will be attaching few common geometric figures , then moving one of them . Then will find the moved object and follow it . <p> Here @ @ @ @ @ @ @ @ @ @ shapes , using shape matching , then perform edge detection and threshing only on that part . 2 . Find the distance of centres of each geometric figures from each others centres . And is this centres distance changes then , the one that has moved will be detected . 3 . Then use geometry to calculate the distance travelled by object and apply corresponding PWM to the flight controller . <p> So is my approach correct ? This will surely detect geometric figures . <p> LIMITATION : 1 . This works only for objects which are flat ( since geometry is used to determine the distance moved . ) How do I determine distance to any target of arbitrary height ? <p> You 're project sounds quite interesting Sahil ! I would suggest looking into video stabilization which can actually be a really challenging topic . I do n't  have any OpenCV code for that , but I know the MATLAB tutorials have a good example of a feature-based stabilization algorithm . <p> Hi Adrian this code looks awesome ! however I am having trouble running it . When I @ @ @ @ @ @ @ @ @ @ python drone.py video FlightDemo.mp4 " ( without quotes ) it cmd seems to execute the command however nothing else happens . the video does n't  open . any ideas ? I 'm new to python and am working on a project where a drone will identify ground markers and fly to them and this code looks like a great starting point ! Thanks <p> Hey Chris it sounds like your installation of OpenCV was not compiled with video support , meaning that it can not decode the frames of the . mp4 file . I would suggest re-compiling and re-installing OpenCV with video support . I provide a bunch of tutorials on how to do this on this page . Also , the Ubuntu virtual machine included in Practical Python and OpenCV comes with OpenCV + Python pre-configured and pre-installed , so that might be worth looking into as well ! <p> Thanks for the great posts here ! Really helpful ! If i understand correctly , you did your video processing offline so not with the intention to control your quadcopter ? I 've tested your code on a RPi 2B @ @ @ @ @ @ @ @ @ @ is not really enough to control the copter . <p> That is correct the quadcopter captured the video feed , which was then process offline on my laptop . Depending on the type of image processing algorithms used on the Pi , it can be challenging to get near 30 FPS unless ( 1 ) you 're using basic techniques , ( 2 ) implement in C/C++ for an added speed boost , or ( 3 ) a combination of both . <p> These are all amazing tutorials , in which I am trying to build a search and rescue drone using OpenCV and a Raspberry PI . I was just wondering , is there any way for the code to be modified , so that the camera can see an object that has more than just horizontal and vertical lines ? I would like to be able to modify the code in order to see something other than a square , such as a triangle or a circle . <p> You can actually do that with this code as well . You just need to modify the contour approximation and @ @ @ @ @ @ @ @ @ @ and extent . Taken together you can differentiate triangles , circles , squares , etc . I plan on doing a blog post on this soon , but I also cover it inside the PyImageSearch Gurus course . <p> Thanks alot for your efforts . I like more your posts . I want you tell how can I start with cars detection and counting for build my smart traffic light system and hope you recommend for me some types of cameras that will help me with good performance for this project . <p> I would definitely start by reading up on the basics of motion detection . If you 're planning on doing a bit of machine learning to aid in the detection process , then understanding how HOG + Linear SVM works is a great idea as well . <p> If the target was painted on the wall ( instead of printed on a square piece of paper ) , what method would you use to detect it ? Would you reach for multi-scale template matching , or is there something else that might produce better results ? Thanks ! @ @ @ @ @ @ @ @ @ @ the target was painted on the wall ( provided you retained the square border ) . Otherwise , multi-scale template matching and object detection via HOG + Linear SVM would be a good approach . <p> I was doing some testing using your code above to see if there was any significant difference between Gaussian blurring and the bilateral filter to help preserve edges . I compared the following : cv2.GaussianBlur ( imagegrayrotated , ( 7,7 ) , 0 ) cv2.bilateralFilter ( imagegrayrotated , 7 , 75 , 75 ) <p> Both seemed to yield almost exactly the same results . I was wondering if you had any further research or if you had done any other studies comparing the two . Seems to be a wash to me but I thought maybe you had some further insight . Thanks ! <p> Bilateral filtering is normally used to blur an image while still preserving edges ( such as the edges between the target and the wall ) . However , bilateral filtering is also more computationally expensive . Bilateral filtering can yield different results depending on your parameter choices . @ @ @ @ @ @ @ @ @ @ the PyImageSearch Gurus course . <p> Hi Adrian , When I run your code above I get the following error for findContours : " too many values to unpack " . I made the following code change to address the problem . I thought I should let you know in case you want to update your code above so that others do n't  encounter the same issue . <p> Hey Edan make sure you read the comments . The comment by " Brian " above discusses this error and how to resolve it . You should also read this blog post on how the cv2.findContours function changed between OpenCV 2.4 and OpenCV 3 . <p> Your website has been an amazing help to my school projects . Thank you so much ! Please help me to cross this one last hurdle : <p> I am looking to stream the video from my pi-cam over WIFI to an IP address , say 192.168.1.2:8080 and then extracting the packets to OpenCV on my Mac OS for processing . How can I make that happen ? Or is other alternatives , or even @ @ @ @ @ @ @ @ @ @ a processor in itself ? <p> This looks great . something I 'm interested in adapting . I have a raspberry Pi with a pi cam . the cam is on a pan/tilt mount with a laser module attached . I want it to be able to " look " around for a black balloon and to fire the laser ( popping the balloon ) once its found and then centred the image . would you suggest this as a starting point or do you have something closer that you 've already done ? <p> I personally havent done any pan and tilt programming , but for actual tracking , I would start with this blog post . This could serve as input to the pan and tilt function ( since it allows you to track direction ) . <p> Sure , that 's absolutely possible . Just use the cv2.RETRCCOMP flag rather than cv2.RETREXTERNAL . Then , loop over the contours and find the two that ( 1 ) overlap and ( 2 ) have the desired properties as detailed in this post . <p> You can detect circles in images using @ @ @ @ @ @ @ @ @ @ get right . In general , I would n't recommend the technique discussed in this post for detecting circles , mainly because it uses contour approximation and vertices counting , which by definition does n't  make sense for a circle . <p> Hey Adrian , thanks for this very helpful and informative post . I was wondering if you have tried detecting a specific image using the video streams ? If so could you please point me in the direction of how to do this . Thanks again ! <p> There are multiple ways to detect a specific image in a video stream . I would start off with something like multi-scale template matching . Another method worth trying is to detect keypoints , extract local invariant descriptors , and apply keypoint matching . I detail how to apply this method to recognize book covers inside Practical Python and OpenCV . <p> Hi , Adrian Thank you for your posting . I just want to check one thing about this project . I am doing an univ project . But it is actually pretty similar but little bit different with yours @ @ @ @ @ @ @ @ @ @ can I do this ? If you can give me any advice , I will totally appreciate for it so much . Thank you ! <p> Marker detection is entirely dependent on your marker and the environment you are trying to detect it in . I would start by trying basic detection using thresholding/edge detection and contour properties . If your marker is complex or your environment is quite " noisy " , then keypoint detection and local invariant descriptor matching works well . Otherwise , you might have to train a custom object detector . <p> Very cool Adrian , thanks for spending the time to put together these howtos . <p> I 'm looking for a way to do precision landing with my drone , with an accuracy of +/- 2 inches . I do n't  really want to use any active systems on the ground , and so a printed marker + CV would be great . The drone needs to be able to see the target from about 20m above ground , and the conditions will be daylight . What image/pattern do you think would make for @ @ @ @ @ @ @ @ @ @ using the technique described in this post , or using color based detection , or maybe a hybrid with both ? <p> The problem with " natural " outdoor scenes is that your lighting conditions can vary dramatically and thus color based region detection is n't the best . You might be able to include color in your marker , but I would create a hybrid approach that uses color and other recognizable patterns . <p> Keypoint detection , local invariant descriptors , and keypoint matching would be a good approach here . Inside Practical Python and OpenCV I demonstrate how to use this approach to recognize the covers of books in images but you could also apply it to recognizing a target as well . <p> Thanks for the pointers I 'll take a look . When it comes to blurry images ( due to vibrations ) , any idea which method and marker will yield the best results ? Am I right to assume a sparse and well spaced pattern will do better than one with lots of small details ? The distance between the camera and the marker will @ @ @ @ @ @ @ @ @ @ be taken into account . <p> Blurry images can be a pain to work with it , so the first general piece of advice is to try to avoid that . However , in some situations this is unavoidable . When that is the case , I would try to apply blur detection to find frames that are " less blurry " then others . Once you find one of these frames , try to detect your marker . Then once you have it , you can try applying correlation-based tracking methods to continue tracking the marker . <p> Another great alternative is to try to find the marker in every frame , but then take the average of the marker position over N frames , so even if you can not find the marker in a given frame , your average will still help you " track " it until its found again . <p> Finally , larger , well defined patterns that do n't  rely on tiny details ( " big " details , if you will ) are likely to perform better here . <p> Good Morning @ @ @ @ @ @ @ @ @ @ pre-recorded video . With live , I 'm using the picamera , and I 'm using some short 10 second , high quality 720 mp4 clips . When using the clips , the frame is as large as my 42 inch TV , and the processing is very slow at what would seem like 1 FPS . How would I go about tweaking the way the video is imported and processed so it played back at the correct speed and in a smaller window ? Also , could you explain why we do n't  need to import numpy as np in this tutorial ? I just noticed that every other tut has the numpy imported . <p> Hey Jeremyn think of it this way : the larger your frame is , the more memory it takes up ( both in disk and RAM ) . Also , the larger the frame is , the longer it takes to decode the frame using whatever CODEC your video file is in . In this case , it seems like what 's actually slowing down your script is the decoding process . I would look at @ @ @ @ @ @ @ @ @ @ them to working in a different thread , and also look at more optimized methods to decoding your frames ( perhaps at the hardware level ) . <p> As for as NumPy , the reason we did n't  import it is because we never called any NumPy functions simple as that = <p> Hey Adrian ! Just thought I 'd post this update on my project . I 'm about halfway done with this project . Thanks for these tutorials , I 've really learned a lot and its even helped with my internship because I 'm using Python there too . Ill post another video when the semester is over and its complete . <p> hello Adrian , i am just not sure how to connect my drone video stream to the laptop , my drone is xpredators 510 and i am looking for a way to apply image processing on the video stream . could you help me please <p> I do n't  have any experience with that drone , but I would suggest researching how the video stream is sent over the network . You might be able to send it @ @ @ @ @ @ @ @ @ @ as I have not used that particular drone . <p> A square is a special case of a rectangle they both have 4 corners . The reason I use &gt;= 4 and &lt;= 6 is to handle any cases where the approximation is close but due to various noise in the image its not exactly 4 . If you are 100% sure that your images/video will contain 4 vertices after applying contour approximation you can simply change the line to : <p> Adrian , I would like to thank you again . Based on this tutorial , me and my son built a robot with a camera , that search for a target and try to get closer to it . we had lots of fun in the making process . Thanks for reminding this . G <p> Hey Adrian , great post.Suppose I have a circular logo , what would I have to put into approximate it into circle.You had square logo right , I am trying to detect a red ball.So I would need circular approximations . By the way , I read your each and every post @ @ @ @ @ @ @ @ @ @ You could try utilizing circle detection , although I 'm not a fan of circle detection as it can be a pain to get the parameters just right . Another option is to compute solidity and extent and use them to help filter the circle . The issue here is that since a circle has no real " edges " you 'll end up with a contour approximation with a lot of vertices . That 's fine , but you 'll also need to check the aspect ratio and solidity to see if you 're examining a circle . <p> Hey Adrian , what a post ! I am new to Python while I have learned C and C++ . I installed OpenCV 3.0 and Python 3.4 with your tutorial . When I run the drone.py , it tells me that ImportError : No module named cv2 . How can I make it rightn+ Many thanks ! <p> If the frame is not being grabbed , then the likely issue is that OpenCV was not compiled with video codec support enabled . I do n't  support Windows here on the PyImageSearch blog , so you 'll @ @ @ @ @ @ @ @ @ @ OpenCV by hand if you can . I personally recommend using a Unix-based OS for computer vision development . I have plenty of OpenCV install tutorials for a @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485202 @185202/ <h> Tag Archives segmentation <p> I 'll keep the introduction to todays post short , since I think the title of this post and GIF animation above speak for themselves . Inside this post , I 'll demonstrate how to attach- multiple cameras to your Raspberry Piand access all of them using a single Python script . Regardless if- your setup includes : Multiple USB webcams . Or the Raspberry <p> Wow , last weeks blog post on building a basic motion detection system was- awesome. - It was a lot of fun to write and the feedback I got from readers like yourself made it well worth the effort to put together . For those of you who are just tuning it , last weeks post on building a motion detection <p> That son of a bitch . I knew he took my last beer . These are words a man should never , ever- have to say . But I muttered them to myself in an exasperated sigh of disgust as I closed the door to my refrigerator . You see , - I had just spent over 12 @ @ @ @ @ @ @ @ @ @ huge influences in my life that made me want to become a scientist . The first was David A. Johnston , an American USGS volcanologist who died on May 18th , 1980 , the day of the catastrophic eruption of Mount St. Helens in Washington state . He was the first to report the eruption , exclaiming Vancouver ! <p> Tragic . Heartbreaking . Unbearable . These are the three words that I would use to describe my past week . About a week ago , a close childhood friend of mine passed away in a tragic car accident . I went to elementary school and middle school with him . We spent our summers skateboarding on my driveway and the winters <p> So last night I went out for a few drinks with my colleague , James , a fellow computer vision researcher who I have known for years . You see , James likes to party . He s a " go hard " type of guy . He- wanted to hit every single bar on Washington St. ( there 's a lot of them ) and then hit @ @ @ @ @ @ @ @ @ @ eyelid twitching ? One where your eyelid just wont stop spazzing out , no matter what you do ? That 's currently what 's going on with me and its been going on- for over two weeks . As you can imagine , I 'm not exactly happy . However , to combat this persistent @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485203 @185203/ <h> Accessing RPi.GPIO and GPIO Zero with OpenCV + Python <p> I cant believe this is the first time I am writing a blog post on GPIO and the Raspberry Pi . Its a pretty big mistake on my part . I should have written this post much earlier . <p> You see , on average , I receive 1-3 emails per week along the lines of : <p> When I use the cv virtual environment , I can access my OpenCV bindings . But then I ca n't import RPi.GPIO . When I switch out of the virtual environment ( and use the system Python ) , I can use RPi.GPIO , but I ca n't import OpenCV . What gives ? <p> The reason for this problem is that RPi.GPIO/GPIO Zero- were not installed into your Python virtual environment ! To fix this issue , all you need to do is use pip- to install them into your virtual environment from there , you 'll be good to go . <p> But to be honest with you , - I do n't  think that 's the real issue here ! <p> @ @ @ @ @ @ @ @ @ @ what Python virtual environments are and- why we use them . Utilizing Python virtual environments is a- best practice that you- need to become comfortable with . <p> In the remainder of this blog post , I 'll gently introduce the concept of Python virtual environments . And from there , well learn how to install RPi.GPIO and GPIO Zero into the same Python virtual environment as our OpenCV bindings , allowing us to access both OpenCV and RPi.GPIO/GPIO Zero- at the same time ! <p> However , it seems that I have n't done a good enough job explaining- what Python virtual environments are and- why we use them . The following section should help clear up any questions . <h> What are Python virtual environments ? <p> At the very core , Python virtual environments allow us to create- isolated , independent environments for- each of our- Python projects. - This implies that each project can have its own set of dependencies , - regardless of which dependencies another project has . <p> So why in the world would we want to create a virtual environment for each of our @ @ @ @ @ @ @ @ @ @ contractors and are hired by a company to develop- ProjectA . But before we have completed- ProjectA , a second company hires us to develop- ProjectB . We notice that both- ProjectA and- ProjectB have a dependency on- LibraryAbut the problem is that- ProjectA requires v1.0.0 of- LibraryA while- ProjectB requires v2.0.0 ! <p> This is a real issue- for Python because we can not install- two different versions of the- same library into the same site-packages- directory ( i.e. , where Python stores 3rd party libraries , such as the ones you download and install from pip- , GitHub , etc . ) . <p> So , what do we do ? <p> Do- we run out to the nearest Apple Store and buy a new MacBook Pro so we can use one laptop to develop- ProjectA and the other to develop- ProjectB ? - I really hope not . That would get expensive quick . <p> Do use a web host like Linode , Digital Ocean , or Amazon EC2 and spin-up a new instance for each project ? - This is a better solution , and is @ @ @ @ @ @ @ @ @ @ instance , its overkill . <p> Or do we use Python virtual environments ? <p> You guessed it - we go with Python virtual environments . <p> In this case , all we need to do is create a virtual environment for- each- project , that way there is a separate , isolated , and independent environment for all- projects : <p> Figure 1 : Each Python environment we create is separate and independent from the others . <p> This allows us to install completely different dependencies for- ProjectA and- ProjectB , ensuring we can finish development of both projects on the same computer . <h> You- probably installed RPi.GPIO and GPIO Zero incorrectly <p> " Incorrect " is the wrong word to use here but I needed to get your attention . When you went to install RPi.GPIO or GPIO Zero , I 'm willing to bet you used apt-get- . Your- command probably looked ( something ) like this : <p> Accessing RPi.GPIO and GPIO Zero with OpenCV + Python <p> Shell <p> 1 <p> $sudo apt-getinstall **30;9701;TOOLONG <p> This command will download RPi.GPIO and GPIO Zero from the @ @ @ @ @ @ @ @ @ @ . <p> The- problem is that apt-get- will install packages into the- system install of Python and- not your Python virtual environments . <p> And this is- exactly why you run into problems like : <p> I can import OpenCV in the cv virtual environment , but I ca n't import RPi.GPIO . On the other hand , I can import RPi.GPIO outside of the cv environment , but then I ca n't import cv2 . <p> How do you resolve this issue ? <p> You just need to install your libraries into your- virtual environment rather than the system version of Python , which can easily be accomplished by workon- and pip- . <p> This brings me to my next point : <p> I- do not recommend using apt-get- to install Python libraries . <p> You 're probably thinking , - " But Adrian , using apt-get- is so easy ! Its just one command and then I 'm done ! " <p> I 've got news for you using pip- is also one command . And its just as easy . <p> To get back to my point , not only will apt-get- @ @ @ @ @ @ @ @ @ @ your virtual environment ) , there is also the issue that apt-get- packages are normally a bit out of date . <p> And guess what happens when you want to install the newest version of a given library ? <p> Hint : You run into the exact problem detailed above you 'll be trying to install two different versions of the- same library into the- same site-packages- - directory , which simply can not happen ( due to how Python works ) . <p> Instead , you should be using the Python package manager , pip , to install your Python packages into virtual environments . For more information on pip- , how it works , and why we use it , please refer to this article . <h> Installing RPi.GPIO and GPIO Zero " correctly " <p> Let 's go ahead and get RPi.GPIO and GPIO zero installed - into our Python virtual environment . To start , first use the workon- command to enter your Python virtual environment : <p> Accessing RPi.GPIO and GPIO Zero with OpenCV + Python <p> Shell <p> 1 <p> $workon&lt;virtual environment name&gt; <p> Note : @ @ @ @ @ @ @ @ @ @ workon- command so that the virtual environment startup scripts are loaded . <p> You can tell you are in a Python virtual environment if- the name of the environment appears in parenthesis before your prompt : <p> Figure 2 : I can tell I am in the " cv " virtual environment because I can see the text " ( cv ) " before my prompt . <p> In this case , I have entered the cv- virtual environment and I can verify this because I see the text- " ( cv ) " before my prompt . <p> From there , we can let pip- install RPi.GPIO and GPIO Zero for us : <p> Accessing RPi.GPIO and GPIO Zero with OpenCV + Python <p> Shell <p> 1 <p> 2 <p> $pip install RPi.GPIO <p> $pip install gpiozero <p> Lastly , let 's test the install and ensure we can import RPi.GPIO , GPIO Zero , and OpenCV together : <p> Accessing RPi.GPIO and GPIO Zero with OpenCV + Python <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> $python <p> &gt;&gt;&gt;import RPi.GPIO <p> &gt;&gt;&gt;import gpiozero <p> @ @ @ @ @ @ @ @ @ @ assumption that the virtual environment you are using- already has OpenCV installed in it . My cv- virtual environment has OpenCV already installed , so by using pip- to install the RPi.GPIO- and gpiozero- to install the respective GPIO packages , I 'm able to access all three libraries from within the same environment . <p> Once you can import these libraries in the same environment , were ready to move on . <h> Hardware <p> For this blog post , I used my- Raspberry Pi 3 and the- TrafficHAT- board , a really cool module for the Raspberry Pi that allows you to get started quickly and easily with GPIO programming : <p> Figure 3 : The TrafficHAT module for the Raspberry Pi , which includes 3 LED lights , a buzzer , and push button , all of which are programmable via GPIO . <p> As you can see , the TrafficHAT includes 3 big LED lights , a push-button , and a buzzer . <p> Note : - Its called a " hat " because we simply need to set it on top of the GPIO pins no @ @ @ @ @ @ @ @ @ @ . <p> Once we have the TrafficHAT installed on the Raspberry Pi , we can program it using nearly any programming language ( provided the language can access the GPIO pins ) , but for the purposes of this blog post , well be using Python + RPi.GPIO and GPIO Zero . <h> Using RPi.GPIO + OpenCV <p> Let 's go ahead and write some code to access the TrafficHAT board using the RPi.GPIO library . Well also utilize OpenCV to load an image from file and display it to our screen . <p> Open up a new file , name it gpiodemo.py- , and insert the following code : <p> Accessing RPi.GPIO and GPIO 23 <p> 24 <p> 25 55203 @qwx675203 <p> importRPi.GPIO asGPIO <p> importtime <p> importcv2 <p> # load the input image and display it to @ @ @ @ @ @ @ @ @ @ and press any key to continue ... " ) <p> image=cv2.imread ( " hooverdam.jpg " ) <p> cv2.imshow ( " Image " , image ) 55212 @qwx675212 <p> print ( " moving on ... " ) <p> # set the GPIO mode <p> GPIO.setmode(GPIO.BCM) <p> # loop over the LEDs on the TrafficHat and light each one <p> # individually <p> foriin(22,23,24) : <p> GPIO.setup ( i , GPIO.OUT ) <p> GPIO.output ( i , GPIO.HIGH ) <p> time.sleep(3.0) <p> GPIO.output ( i , GPIO.LOW ) <p> # perform a bit of cleanup <p> GPIO.cleanup() <p> Lines 2-4 handle importing our required Python packages . We then load the input image from disk and display it to our screen on- Lines 8-10 . Our script will pause execution until we click on the active image window and press any key on our keyboard . <p> From there , we loop over each of the LEDs on the TrafficHAT ( Line 18 ) . For each of these lights , we : <p> Turn the LED light on . <p> Wait 3 seconds . <p> Turn the light off and continue @ @ @ @ @ @ @ @ @ @ are in the cv- virtual environment ( or whatever virtual environment you are using to store your OpenCV bindings + GPIO libraries ) by using the workon- command : <p> Accessing RPi.GPIO and GPIO Zero with OpenCV + Python <p> Shell <p> 1 <p> $workon cv <p> We can then run the gpiodemo.py- script : <p> Accessing RPi.GPIO and GPIO Zero with OpenCV + Python <p> Shell <p> 1 <p> $python gpiodemo.py <p> As the output image demonstrates , we can see that our hooverdam.jpg- image is displayed to the screen- and the green LED lights is shining brightly on the TrafficHAT : <p> Figure 4 : Loading an image to my screen using OpenCV and then lighting up the green LED using GPIO . <h> What about root ? <p> But what if we wanted to execute gpiodemo.py- as the root user ? What do we do then ? <p> We have two options here . <p> The first option is to use the sudo- command- inside our Python virtual environment , like this : <p> Accessing RPi.GPIO and GPIO Zero with OpenCV + Python <p> Shell <p> 1 @ @ @ @ @ @ @ @ @ @ are in your Python virtual environment- before executing your script with sudo- ; otherwise , you will not have access to the libraries installed in your virtual environment . <p> The second option is to launch a root shell , access our Python virtual environment , and then execute the script : <p> Accessing RPi.GPIO and GPIO Zero with OpenCV + Python <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> $sudo/bin/bash <p> $source/home/pi/.profile <p> $workon cv <p> $python gpiodemo.py <p> Which one is best ? <p> To be honest , it does n't  really matter . <p> There are some cases where using sudo- is easier . And there are others where its nice to simply have a root shell pulled up . Use whichever option you 're more comfortable with just be careful when executing commands as root ! <h> Using RPI Zero + OpenCV <p> Now that we 've explored RPi.GPIO , let 's re-create the same Python script , but this time using the GPIO Zero library . Open up a different file , name it gpiozerodemo.py- , and insert the following code : <p> Accessing RPi.GPIO @ @ 19 <p> 20 <p> 21 55203 @qwx675203 <p> fromgpiozero importTrafficHat <p> importtime <p> importcv2 <p> # load the input image and display it to our screen <p> print ( " click on the image and press any key to continue ... " ) <p> image=cv2.imread ( " hooverdam.jpg " ) <p> cv2.imshow ( " Image " , image ) 55212 @qwx675212 <p> print ( " moving on ... " ) <p> # initialize the TrafficHAT , then create the list of lights <p> th=TrafficHat() <p> LONG ... <p> # loop over the lights and turn them on one-by-one <p> forlight inlights : <p> light.on() <p> time.sleep(3.0) <p> light.off() <p> Lines 2-4 again handle importing our required packages . What 's really interesting here is that the gpiozero- library has a TrafficHat- class , which enables us to easily interface with the TrafficHAT module . <p> We can then initialize the TrafficHat- object and construct the @ @ @ @ @ @ @ @ @ @ Finally , - Lines 18-21 handle looping over each of the lights- , turning each on individually , waiting 3 seconds , and then turning the light- off before moving to the next one . <p> Just like the RPi.GPIO example , we- first need to access our Python virtual environment and then execute our script : <p> Accessing RPi.GPIO and GPIO Zero with OpenCV + Python <p> Shell <p> 1 <p> 2 <p> $workon cv <p> $python gpiozerodemo.py <p> As the script executes , - we can see an image displayed to our screen and the LEDs lighting up : <p> Figure 5 : A second example of using OpenCV to display an image and then utilizing GPIO to illuminate an LED . <p> Note : - To execute this script as root , follow the instructions detailed in the previous section . <h> Summary <p> In this blog post , I started by reviewing- what Python virtual environments are and- why we use them . Simply put , Python virtual environments allow us to create independent , isolated development environments for- each project we work on , ensuring @ @ @ @ @ @ @ @ @ @ , - virtual environments allow us to keep our system install of Python clean and tidy . <p> Once we understood the basics of Python virtual environments , I detailed how to ( correctly ) install RPi.GPIO and GPIO Zero such that we can access- both GPIO libraries- and OpenCV at the same time . <p> We then developed a simple Python script that loads an image from disk using OpenCV , displays it to our screen , and then lights up various LEDs on the TrafficHAT . <p> In the next blog post , well create a more advanced GPIO + OpenCV script , this time lighting up LEDs and buzzing the buzzer whenever a- pre-defined- visual action takes place . <p> Be sure to enter your email address in the form below to be notified when the next blog post goes live , you wo n't want to miss it ! <h> Downloads : 55217 @qwx675217 <h> 21 Responses to Accessing RPi.GPIO and GPIO Zero with OpenCV + Python <p> I think it would be awesome to talk about cloning virtualenvs ( maybe somthing along the lines of using @ @ @ @ @ @ @ @ @ @ virtualenv i can use as a base for other projects . Otherwise it takes a minute to get all the python bindings setup . <p> I am so glad I found your website , it has opened a whole new world of programming possibilities . I 'm currently attempting to use OpenCV3 and Python to scan , OCR and then upload the data to the web = <p> Hi Adrian , Thanks for the great tutorial . For some reason if I run the script as sudo gpiodemo.py I get " ImportError : No module named RPi.GPIO " . Even though going into the python shell and trying " import RPi.GPIO " shows no issues . Only if I use your second option to launch a root shell with " sudo /bin/bash , etc. " the GPIO routine works . Generally this would be ok , but I would like to write the program call into crontab so that the GPIOs are accessed in regular time intervals . Any idea what the problem could be ? Thanks for any suggestions . <p> If you would like to use crontab to execute @ @ @ @ @ @ @ @ @ @ next weeks blog post where I demonstrate how to use crontab to execute a Python + OpenCV script on reboot . The same principle can be applied to running cronjobs at regular intervals as well . <p> Thanks for all the great posts Adrian . Your blog is my goto place for all things related to Computer Vision . The first half of this post helped me understand how to install pygame on Ubuntu and saved my day ! <p> Hey Adrian . First of all , I 'd like to thank you for all your support throught your page . I had a problem and aI hope you could help me . I 'm developing a recogning face system using OpenCV and Raspberry Pi for my final project to graduate in Automation Engineering . Thus , I wrote a simple code in the final of my recogning code in order to use the GPIO , but when I execute using the Python environment , the following message showed : " RuntimeError : No access to /dev/mem . Try running as root ! " . So , I read your page and @ @ @ @ @ @ @ @ @ @ sudo " , as you adviced . But , appeared the message : " ImportError : No module named cv2 " . As you said , there is other way to run the code as root . Therefore , still in python environment , I typed " sudo /bin/bash " and the root changed for : " root@raspberrypi : " in write words . In following , I put " /home/pi/.profile " , but , when I typed " work cv " appeared the message : " ERROR : Environment cv does not exist . Create it with mkvirtualenv cv ' " . Would you have a way to help me ? I do n't  find anything that I could use to solve this problem . Thank you again . <p> If the method I suggested is n't working , the alternate method is to specify the full path to the Python binary inside the virtual environment directory . This will ensure the correct environment variables are used . For example , you can do this : <p> $ sudo /. virtualenvs//bin/python yourscript.py <p> Just replace envname with the name of @ @ @ @ @ @ @ @ @ @ rpi.gpio and gpiozero in libreelec but apt-get is disabled . I managed to install it from the default Programs by installing Raspberry Pi Tools but when I try to run the commands I get the Not Found message like if they did n't  get installed , any idea ? I 'm using @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485212 @185212/ <h> Tag Archives convolutional neural network <p> Normally , I only publish blog posts on Monday , - but I 'm so excited about this one that it could n't wait and I decided to hit the publish button early . You see , just a few days ago , - Fran+ois Chollet pushed three Keras models ( VGG16 , VGG19 , and ResNet50 ) online these networks- are- pre-trained- on the ImageNet dataset , meaning that they can recognize- 1,000 <p> In todays blog post , we are going to- implement our first Convolutional Neural Network ( CNN ) - LeNet - using Python and the Keras deep learning package . The LeNet architecture was first introduced by LeCun et al . in their 1998 paper , - Gradient-Based Learning Applied to Document Recognition. - As the name of the paper suggests , the authors implementation of LeNet was used <p> I 'm going to start todays blog post by asking a series of questions which will then be addressed later in the tutorial : What are image convolutions ? What do they do ? Why do we use them ? @ @ @ @ @ @ @ @ @ @ convolutions play in deep learning ? The word " convolution " sounds like a <p> So you 're interested in deep learning and Convolutional Neural Networks . But where do you start ? Which library do you use ? There are just so many ! Inside this blog post , I detail 9 of my- favorite Python deep learning libraries . This list is- by no means- exhaustive , its- simply a list of libraries that Ive used in my computer vision <p> I 've got a big announcement today : I will be doing more Deep Learning and Convolutional Neural Network tutorials on the PyImageSearch blog over the coming months . I 'm dead serious about this - and I 've put my money where my mouth is and- invested in some real hardware for deep learning . To learn more about my investment , the <p> A few weeks ago- I introduced bat-country , my implementation of a lightweight , extendible , easy to use Python package for deep dreaming and inceptionism . The reception of the library was very good , so- I decided that it would be interesting to do @ @ @ @ @ @ @ @ @ @ trippy images like on the <p> One of the main benefits of the bat-country Python package for deep dreaming and visualization is its ease of use , extensibility , and customization . And let me tell you , that customization really came in handy last Friday- when the Google Research team- released an update to their deep dream work , demonstrating a method to " guide " your input images <p> We cant stop here , this is bat country . Just a few days ago , the Google Research blog published a post demonstrating a unique , interesting , and perhaps even disturbing method to visualize what 's going inside the layers of a Convolutional Neural Network ( CNN ) . Note : - Before you go , I suggest taking a look at the images generated using <p> Disclaimer : This post is a bit cynical in tone . In all honesty , I support deep learning research , I support the findings , and I believe that by researching deep learning we can only further improve our classification approaches and develop better methods. - We all know that @ @ @ @ @ @ @ @ @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485213 @185213/ <p> An actionable , real-world 6-8 month course on OpenCV and computer vision . This course will be taught using Python and OpenCV ( along with a few other libraries ) . Each month a new set of lessons will be released . <p> A community of like-minded developers , programmers , and students- who are eager to learn computer vision and level-up their OpenCV skills . <p> An IPython Notebook development environment in the cloud. - Everything we study and all the projects we work on will be done in the the cloud and from your browser . There will be nothing to download , and nothing to install . <p> Direct access to me. - Each day I get 100s of emails and the count is rising every week . The PyImageSearch Gurus forums and Q&amp;A will be my new home and your place to get guaranteed access to me . <p> As a heads up , over the next 7 days I 'll be posting- a few more announcements that you wo n't want to miss , including : <h> Thursday , January 8th : <p> A sneak @ @ @ @ @ @ @ @ @ @ of what 's inside PyImageSearch Gurus . <h> Friday , January 9th : <p> The breakdown of topics well cover inside PyImageSearch Gurus. - From face recognition , to automatic license plate detection , deep learning , - and Raspberry Pi projects , there are a plethora of awesome computer vision topics we are going to cover . You wo n't want to miss this list ! <h> Monday , January 12th : <p> The full list of Kickstarter rewards , so you can plan ahead for which reward you want . <h> Wednesday , January 14th : <p> The Kickstarter campaign link- that you can use to claim your PyImageSearch Gurus spot- and grab your Kickstarter reward . <p> Hi Israel , I 'm still in the process of finalizing the Kickstarter reward levels , but I will have them posted online by Monday , January 12th . Be sure to signup for the Kickstarter notifications so you receive an email PyImageSearch Guru updates ! <h> Trackbacks/Pingbacks <p> A couple days ago ago I mentioned that on Wednesday , January 14th- I am launching a Kickstarter to fund my new project - @ @ @ @ @ @ @ @ @ @ development environment- dedicated to turning you @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485218 @185218/ <h> Tag Archives caffe <p> So you 're interested in deep learning and Convolutional Neural Networks . But where do you start ? Which library do you use ? There are just so many ! Inside this blog post , I detail 9 of my- favorite Python deep learning libraries . This list is- by no means- exhaustive , its- simply a list of libraries that Ive used in my computer vision <p> A few weeks ago- I introduced bat-country , my implementation of a lightweight , extendible , easy to use Python package for deep dreaming and inceptionism . The reception of the library was very good , so- I decided that it would be interesting to do a follow up post but instead of generating some really trippy images like on the <p> One of the main benefits of the bat-country Python package for deep dreaming and visualization is its ease of use , extensibility , and customization . And let me tell you , that customization really came in handy last Friday- when the Google Research team- released an update to their deep dream work , demonstrating @ @ @ @ @ @ @ @ @ @ We cant stop here , this is bat country . Just a few days ago , the Google Research blog published a post demonstrating a unique , interesting , and perhaps even disturbing method to visualize what 's going inside the layers of a Convolutional Neural Network ( CNN ) . Note : - Before you go , I suggest taking a look @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485221 @185221/ <h> Tag Archives deep learning <p> Disclaimer : This post is a bit cynical in tone . In all honesty , I support deep learning research , I support the findings , and I believe that by researching deep learning we can only further improve our classification approaches and develop better methods. - We all know that research is iterative . And sometimes we even @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485222 @185222/ <h> PyImageSearch Gurus member spotlight : Tuomo Hiippala <p> In todays blog post , I interview PyImageSearch Gurus member , - Tuomo Hiippala , who was recently awarded a- 28000G grant ( approximately $30,500 USD ) to research- how computer vision can be used to study visual culture , including social media images and photo archives . Well discuss his very unique area of research , which blends- linguistics with computer vision , and how his work is shaping modern day visual data analysis . <p> Tuomo has been a longtime reader of the PyImageSearch blog and was even one of the- very first backers of the PyImageSearch Gurus Kickstarter campaign . Its an honor to have him here today and its only fitting that he s the first person I 've interviewed on the blog . There is a lot of knowledge to be gained from this interview with Tuomo - especially how computer vision can be applied to real-world problems . <p> Ideally , I hope these types of- spotlight- and- interview posts can be more frequent on the PyImageSearch blog in the future. - While I try @ @ @ @ @ @ @ @ @ @ week , whats- really important is how you , the reader , take this knowledge and apply it to your own projects . <h> PyImageSearch Gurus member spotlight : Tuomo Hiippala <p> Adrian : Hey Tuomo ! Thanks for agreeing to do this interview . Its great to have you on the PyImageSearch blog . <p> Tuomo : As a long-time reader : my pleasure ! <p> Adrian : Let 's go ahead and kick off this interview . Where do you work/what is your job ? <p> Tuomo : I currently work as a post-doctoral researcher at the Centre for Applied Language Studies , University of Jyv+skyl+ , all the way up in Finland . As for my background , I got a PhD in English philology from the University of Helsinki , specializing in linguistics . I 'm a special kind of linguist though , because my interests extend way beyond language . <p> Basically , I do research on how language interacts with other modes of communication , such as photographs , diagrams , infographics you name it . Visiting a website or picking up the daily newspaper @ @ @ @ @ @ @ @ @ @ all these different modes together . They are closely coordinated and interact with each other all the time it 's an inherent feature of communication , but curiously enough , something that has only recently really caught on with researchers from different fields . <p> Adrian : How did you first become interested in computer vision ? <p> Tuomo : I had to check my notebook for this : I wrote down something during the 2014 Frankfurt Book Fair in Germany I remember vaguely checking out some of the exhibitioners at the trade fair , who advertised automatic document management solutions . That 's when the idea really hit me , although I was obviously already familiar with techniques such as Optical Character Recognition , which pretty much revolutionized linguistics back in the 1990s , removing the logjam of manual input . Anyhow , that 's when I decided to learn more about computer vision and get to know Python on the side . <p> Adrian : How do you use computer vision in your job ? <p> Tuomo : I 've got a few projects that I 'm working @ @ @ @ @ @ @ @ @ @ a semi-automatic document analysis system , which uses OpenCV , Tesseract and NLTK to examine document images and annotates their features into an XML database . Commercial OCR software is not really useful for this purpose , because it is mainly concerned with extracting the content . For studying how the different modes of communication work together , we need to know much more : the content , its placement in layout , logical organisation and semantic interrelations and this just gives us a minimal view of a document and its inner workings . I hope to present my ongoing work at the Association for Computational Linguistics annual meeting in Berlin next summer ! <p> I 'm also looking into how computer vision could be used to enable the researchers to get a stronger grip on large volumes of visual data . Many different fields in the humanities and social sciences are concerned with visual communication yet many researchers tend to do visual analysis by hand , which really does not allow them to seriously engage with the volumes of data typically found in historical photo archives or social media @ @ @ @ @ @ @ @ @ @ in open source libraries such as OpenCV and other tools that make programming more accessible IPython comes to mind as a good candidate for developing and sharing such tools . This is what I 'd like to contribute to , focusing on the applications and leaving the development of algorithms to computer scientists . I nevertheless try to learn about the algorithms as well ! <p> Adrian : You mentioned that you applied for a grant . Can you tell me more about that ? <p> Tuomo : Sure ! Finnish Cultural Foundation just awarded me a personal grant worth 28 000G ( approximately $35,000 USD ) to do research on how computer vision can be used to study visual culture , such as social media images and photo archives . I also got a 3500G grant from the City of Helsinki in 2015 to study the photographs shared by the tourists visiting Helsinki , that is , how they see the city , which is another long-term research interest of mine . I 'm currently waiting for the beginning of the tourist season to start collecting data ! <p> @ @ @ @ @ @ @ @ @ @ ? Be sure to give these two pages a read : <p> Adrian : How did the **27;9733;TOOLONG Gurus help you in obtaining the grant ? <p> Tuomo : What can I say ? The blog and the course have benefitted my work immensely : it has opened up a whole new area of research for me . Although I 've been learning computer vision for one and half years only , I 'm already confident enough to apply the techniques in research . And this is obviously essential for acquiring any funding at all . <p> Tuomo : Absolutely I think the course is of extremely high quality and worth every cent I think I was among the first five people to back it on Kickstarter ! What I find particularly appealing is that the course materials are extremely well-written and presented in a way that allows one to learn Python at the same time . They go nicely hand-in-hand , as do the course and the surrounding community ! <p> Adrian : Anything else you would like to share ? <p> Tuomo : I would certainly recommend learning @ @ @ @ @ @ @ @ @ @ , regardless of their professional or academic interests . I think the variety of projects that have been showcased inside the PyImageSearch Gurus course nicely illustrates the wealth of applications that computer vision can and will have in the future . And the best part is , most of these techniques are really accessible to a layperson , thanks to Python and the wealth of quality tutorials . Just dive in , it will definitely pay off ! <p> Adrian : If a PyImageSearch reader wants to chat , where is the best place to connect with you ? <p> The doors to the course open every 2-4 weeks and seats inside the course are- limited , so if you 're interested in joining , be sure to claim your spot now . <p> I 'll see you inside ! <h> Summary <p> In todays post , I interviewed- Tuomo Hiippala , who works for the- University of Jyv+skyl+ . His research blends two unique areas of study : - linguistics and- computer vision , where his work focuses on enabling researchers to better understand large volumes of visual data . <p> @ @ @ @ @ @ @ @ @ @ I suggest wrapping your computer vision app as an API and then accessing it via your programming language of choice ( and can afford to send the image over the network ) . However , this is only suitable for non-realtime applications . In the case where you need real-time performance , you should implement in Objective-C or Swift . <p> If you 're interested in the former approach ( using the computer vision API ) , take a look at PyImageSearch Gurus which has an entire module dedicated to using PhoneGap/Cordova to ( i.e. , JavaScript + HTML ) to build a mobile computer vision application @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485224 @185224/ <p> Originally published in the SIGGRAPH 2007 proceedings , I read this paper for the first time during my computational photography class as an undergraduate student . <p> This paper , along with the- demo video from the authors , - made the algorithm feel like magic , especially to a student who was just getting his feet wet in the world of computer vision and image processing . <p> These seams are then removed from the original image , allowing us to resize the image while preserving the most salient regions- ( the original algorithm also supports- adding seams , allowing us to- increase the image size as well ) . <p> In the remainder of todays blog post I 'll discuss the seam carving algorithm , how it works , and how to apply seam carving using Python , OpenCV , and sickit-image . <p> To learn more about this classic computer vision algorithm , just keep reading ! <h> Seam carving with OpenCV , Python , and scikit-image <p> The first part of this blog post will discuss- what- the seam carving algorithm is and- why we @ @ @ @ @ @ @ @ @ @ <p> From there I 'll demonstrate how to use seam carving using OpenCV , Python , and scikit-image . <p> Finally , I 'll wrap up this tutorial by providing a demonstration- of the seam carving algorithm in action . <h> The seam carving algorithm <p> Introduced by Avidan and Shimar in 2007 , the seam carving algorithm is used to resize ( both downsample and upsample ) an image by removing/adding- seams that have- low energy . <p> Seams are defined as connected pixels that flow from left-to-right or top-to-bottom provided that they traverse the- entire width/height of the image . <p> Thus , in order to perform seam carving we need two important inputs : <p> The original image . This is the input image that we want to resize . <p> The energy map . We derive the energy map from the original image . The energy map should represent the most salient regions of the image . Typically , this is either the gradient magnitude representation ( i.e. , output of Sobel , Scharr , etc. operators ) , entropy maps , or saliency maps . <p> These @ @ @ @ @ @ @ @ @ @ by their energy . Seams with- low energy- are placed at the front of the list while- high energy seams are placed at the back of the list . <p> To resize an image we either- remove seams with low energy to downsample an image or we- duplicate seams with low energy- to upsample the image . <p> Below is an example of taking the original image , finding the seams with the lowest energy , and then removing them to reduce the final size of the output image : <h> Why use traditional seam carving over traditional resizing ? <p> Keep in mind that the purpose of seam carving is to preserve the most salient ( i.e. , " interesting " ) regions of an image while still resizing the image itself . <p> Using traditional methods for resizing changes the dimensions of the entire image- no care is taken to determine- what part of the image is most or least important . <p> Seam carving instead applies heuristics/path finding derived from the energy map to determine- which regions of the image can be removed/duplicated to ensure ( 1 @ @ @ @ @ @ @ @ @ @ preserved and ( 2 ) this is done in an aesthetically pleasing way . <p> Note : Preserving the most interesting regions of an image in an aesthetically pleasing manner is a lot harder than it sounds . While seam carving may seem like magic , its actually not and it has its limitations . See the " Summary " section for more information on these limitations . <p> --image- : The path to the input image we want to apply seam carving to . <p> --direction- : The direction in which well apply seam carving . A value of vertical- will adjust the image width while a value of horizontal- will adjust the image height . We default the carving direction to vertical- . <p> On the- left you can see the original input image a photo of Bryce Canyon , one of the most beautiful National Parks- to visit in the United States . Then on the- right- we have the seam carved image . As you can see , we have- removed vertical seams from the image , thereby decreasing the image width . <p> We can @ @ @ @ @ @ @ @ @ @ Seam carving with OpenCV , Python , and scikit-image <p> Shell <p> 1 <p> $python seamcarving.py--image **26;9762;TOOLONG horizontal <p> Figure 9 : ( Left ) The original image . ( Right ) Removing horizontal seams from the image to decrease height . <p> I have also included a GIF animation below that demonstrates seam carving one pixel at a time to give you a better feel for the algorithm : <p> Figure 10 : Applying seam carving to an image using OpenCV , Python , and scikit-image . <h> Summary <p> In todays blog post I discussed the seminal seam carving algorithm used for content-aware resizing of images . <p> Inside the paper Avidan and Shimar demonstrate that seam carving can not only be used for reducing image size , but also for- increasing image size as well ; however , the scikit-image implementation currently only supports downsampling . <p> While this algorithm may have felt like " magic " to myself as an undergraduate , I eventually learned there is no such thing as magic in the computer vision world - every algorithm has its limitations . <p> For @ @ @ @ @ @ @ @ @ @ than visually appealing results where important semantic information of the image is partially destroyed or cut out entirely . A great example would be applying seam carving to an image that contains faces and seams from the faces are removed . <p> To get around this we can annotate our image to provide " hints " to the seam carving algorithm , ensuring the labeled regions are not cut during the seam carving process . In an ideal world , we could provide energy maps that better reflect the salient regions of the image we wish to keep , thereby requiring no modification to the actual seam carving algorithm . <p> Personally , Im interested to see the future landscape of seam carving . With deep learning algorithms used for tasks such as saliency detection , these salient maps can then be used for seam carving . We may eventually see an end-to-end seam carving network . <p> Anyway , I hope you enjoyed the discussion of this classic computer vision paper ! <p> To be notified when future blog posts go live , be sure to enter your email @ @ @ @ @ @ @ @ @ @ @qwx675217 <p> Visual artifacts can and will happen . The downside is that the artifacts are dependent on the input image . Dimensions that work for one image will not work for others . When this happens you can supply ROIs to the seam carving algorithm to specify " do n't  touch this area " or the algorithm can be updated to be non-greedy and instead " look ahead " and what removing a given seam may do to the aesthetics of the image . Refer to the original publication for details on this . <p> I have a question about the scikit-image installation . When I run pip install scikit-image inside of my virtual environment the download bar fills up but sklearn or scipy are not available when I try to import them . Although , when I am outside of my virtual environment I am able to import scipy and sklearn but not use opencv . Do you have any ideas why I would be encountering this problem ? Any help would be much appreciated . <p> Hey Dean it sounds like you have installed some packages into @ @ @ @ @ @ @ @ @ @ virtual environment . I would suggest you access your virtual environment and then install NumPy and SciPy followed by scikit-learn : <p> I was just wondering if seam carving combined with face/text recognition would have its advantages . Meaning , rather than storing comparative faces/objects/text in the original image would it improve recognition accuracy to conduct a post processing seam carving to remove low energy space . Then , when attempting recognition remove also remove low energy regions and then perform recognition on high energy images/frames only ? @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485225 @185225/ <h> OpenCV and Python Color Detection <p> So , here I am . Riding the Amtrak 158 train , coming home after a long business trip . <p> Its hot . The AC is barely working . A baby is screaming right next to me while the accompanying mother looks forlornly out the window , clearly questioning whether or not having a child was the right life decision . <p> And to top it all off , the Wi-Fi does n't  work . <p> Luckily , I brought along my Game Boy and collection of Pokemon games . <p> As I slid my trusty Blue version into my Game Boy , I thought to myself , instead of battling Gary Oak for the thousandth time , maybe I can do a little computer vision . <p> Honestly , would n't it be really cool to be able to segment each of the game cartridges using nothing but color ? <p> Grab yourself a nice cool glass of water to combat the failed AC and a pair of ear plugs to block out the wailing child . Because in this post @ @ @ @ @ @ @ @ @ @ perform color detection . <p> Lines 7-9 then handle parsing our command line arguments . Well need just a single switch , --image- , which is the path to where our image resides on disk . <p> Then , on Line 12 , we load our image off disk . <p> Now , here comes the interesting part . <p> We want to be able to detect each of the Game Boy cartridges in the image . That means well have to recognize- red , - blue , yellow , and- gray colors in the image . <p> Let 's go ahead and define this list of colors : <p> OpenCV and Python Color Detection <p> Python <p> 14 <p> 15 <p> 16 <p> 17 <p> 18 <p> 19 <p> 20 <p> # define the list of boundaries <p> boundaries= <p> ( 17,15,100 , 50,56,200 ) , <p> ( 86,31,4 , 220,88,50 ) , <p> ( 25,146,190 , 62,174,250 ) , <p> ( 103,86,65 , 145,133,128 ) <p> <p> All we are doing here is defining a list of boundaries- in the RGB color space ( or rather , BGR , @ @ @ @ @ @ @ @ @ @ ) , where each entry in the list is a tuple with two values : a list of lower limits and a list of- upper limits . <p> For example , let 's take a look at the tuple- ( 17,15,100 , 50,56,200 ) - . <p> Here , we are saying that all pixels in our image that have a R &gt;= 100 , B &gt;= 15 , and G &gt;= 17 along with R &lt;= 200 , B &lt;= 56 , and G &lt;= 50 will be considered- red . <p> Now that we have our list of boundaries , we can use the cv2.inRange- function to perform the actual color detection . <p> Let 's take a look : <p> OpenCV and <p> 34 <p> 35 <p> # loop over the boundaries <p> for ( lower , upper ) inboundaries : <p> # create NumPy arrays from the boundaries <p> lower=np.array ( lower , dtype= " uint8 " ) <p> @ @ @ @ @ @ @ @ @ @ # find the colors within the specified boundaries and apply <p> # the mask <p> mask=cv2.inRange ( image , lower , upper ) <p> **27;9790;TOOLONG , image , mask=mask ) <p> # show the images <p> LONG ... 55212 @qwx675212 <p> We start looping over our upper and lower boundaries- on Line 23 , then convert the upper and lower limits to NumPy arrays on- Line 25 and 26 . These two lines seem like they can be omitted , but when you are working with OpenCV Python bindings , OpenCV- expects these limits to be NumPy arrays . Furthermore , since these are pixel values that fall within the range- 0 , 256 we can use the unsigned 8-bit integer data type . <p> To perform the actual color detection using OpenCV , take a look at- Line 29 where we use the cv2.inRange- function . <p> The cv2.inRange- - function expects three arguments : the first is the image- - were we are going to perform color detection , the second is the lower- - limit of the color you want to detect , and the third argument @ @ @ @ @ @ @ @ @ @ to detect . <p> After calling cv2.inRange , - a binary mask is returned , where white pixels ( 255 ) represent pixels that fall into the upper and lower limit range and black pixels ( 0 ) do not . <p> Note : We are performing color detection in the RGB color space . But you can easily do this in the HSV or L*a*b* color space as well . You would simply need to adjust your upper and lower limits to the respective color space . <p> To create the output image , we apply our mask on- Line 31 . This line simply makes a call to cv2.bitwiseand , showing only pixels in the image- that have a corresponding white ( 255 ) value in the mask . <p> If your environment is configured correctly ( meaning you have OpenCV with Python bindings installed ) , you should see this as your output image : <p> Figure 1 : Detecting the color red in an image using OpenCV and Python . <p> As you can see , the Red Pokemon cartridge is easily detected ! <p> Now @ @ @ @ @ @ @ @ @ @ Detecting the color blue in an image using OpenCV and Python . <p> Nope , no problem there ! <p> And a similar story for Yellow version : <p> Figure 3 : Detecting the color yellow in an image using OpenCV and Python . <p> Lastly , the outline of the gray Game Boy cartridge is also found : <p> Figure 4 : Detecting gray in an image using OpenCV and Python . <h> Summary <p> In this blog post I showed you how to perform color detection using OpenCV and Python . <p> To detect colors in images , the first thing you need to do is define the- upper and- lower limits for your pixel values . <p> Once you have defined your upper and lower limits , you then make a call to the cv2.inRange- method which returns a mask , specifying which pixels fall into your specified upper and lower range . <p> Finally , now that you have the mask , you can apply it to your image using the cv2.bitwiseand- function . <p> My train is just a few stops away from home , @ @ @ @ @ @ @ @ @ @ you found it useful ! <p> And if you have any questions , as always , feel free to leave a comment or shoot me a message . <h> Downloads : 55217 @qwx675217 <p> The cv2.inRange function is extremely efficient , but has the caveat that you need to know the pixel intensity boundaries prior to applying it to the image . Methods such as Otsus thresholding and Adaptive thresholding can be used to help determine the threshold value in traditional thresholding . <p> We are trying to find the ratio of red and green glowing armbands from a video stream captured by a RasPi . The images of an outdoor square where the lighting conditions will be changing through an evening . Can you suggest a good method for picking out the red and green bands ? Ive been playing with your sample code and I 'm a bit stuck figuring out the ranges . I am clearly not defining them properly as the masked images come out barely showing the red and green . Ultimately the output needs to be the ratio of green:red to crowd-control a pong like @ @ @ @ @ @ @ @ @ @ advice you can offer . <p> I think this post on ball tracking will really help with your arm band project . Inside that post I make reference to a range-detector script inside the imutils package that will help you determine the appropriate color ranges for tracking . <p> Hey Ankit please see my previous reply . You need to use the range-detector script to determine your upper and lower limits . Its hard to define thresholds that will always work , mainly because colors may have different HSV values depending on the lighting conditions of your environment . <p> Ok . Thank you . Last question , if I may : do you think that segmenting an image according to its colors is a good idea to use in order to recolor the image with given desired colors for each segment ? Could this give a good result ? Regards from Begueradj <p> Great question ! The answer is yes , absolutely . This post on color transfer between images could be dramatically improved by performing the local color transfer within each individual segment rather than globally . The @ @ @ @ @ @ @ @ @ @ segments are the most similar for the source and target image . <p> Great work . I 've seen several implementations but yours is the most elegant I encountered so far . One thing though , if you wanted just to get a boolean flag like blueispresent or orangeispresent would you find some way yo compare the current array with the previous ones or go with object tracking instead ? <p> If you wanted to have a boolean flag indicating whether or not a color is present , I would loop over the color ranges , generate a mask for each one , and then count the number of non-zero pixels in the mask using cv2.countNonZero . If the number if non-zero pixels is above a given threshold , then you could say that a given color is present . I hope that helps ! <p> Actually I 'm using ColorPix.exe to determine the RGB value of the color , but I do n't  understand how can I set the boundaries . <p> For example , <p> R = 255 , G = 37 , B = 37 , <p> this is @ @ @ @ @ @ @ @ @ @ and i want to make a range ( lower and upper ) , now Im stuck i do n't  know how to determine the lower or upper . I tried subtracting 20 to RGB for the lower , and add 20 to upper , but it does n't  detect the color . <p> If you are trying to define " shades " of a color , its actually a lot easier to use the HSV color space . I would define your colors in HSV using your color picker tool , and then convert those colors back to RGB . <p> Hello Adrian . When trying to complete this exercise appears to me the following error : mask = cv2.inRange ( image , lower , upper ) error : ( 209 ) the lower bounary is neither an array of the same size and same type the src , not in scalar function cv : : inRange Ive given round and round but still with the same error . Yours faithfully , Francisco <p> Hi Franciso , I have actually heard about this error from one or two other PyImageSearch @ @ @ @ @ @ @ @ @ @ on my machine and the PyImageSearch virtual machine . The first thing I would do is make sure the path to your image is correct . You supply the image path via command line argument . If after ensuring that your path is correct , can you let me know which version of Python , OpenCV , and NumPy you are using ? And it would probably help to know which OS you are using as well . <p> On my Ubuntu virtual env I compile without any problem too , but when I port over to the RasPi3 , I get this same error as above ? Do you know what could be causing this ? The image is in the working directory , so I doubt its a path issue ? Also Ive tried running in Python3 , with no success but all else in the RasPi virtualEnv is compiling in Python 2.7.9 , so I guess I 'm forced to go with Python 2 for now . Will see if I can find out what the issue with Python 3 is ? ? ? <p> hello adrian . @ @ @ @ @ @ @ @ @ @ I click the f5 in python idle gives me error soon . already read on another website that the error may be due to this : np.uint8 instead dtype : " uint8 " . my version of python is 2.7.9 . opencv and numpy is recent . Greetings <p> I would definitely try to execute the script via command line instead of IDLE and see if you still get the error . Like I said , the problem could potentially be that the image is not being read of disk properly . Inspect the returned value of cv2.imread and if the value is None , then you 'll know that the image was not read properly . As for the versions of OpenCV and NumPy , it would be really beneficial if you could give the exact version numbers rather than just " recent " . <p> Hello , just the night I will try to run the program from the command line and give the versions of opencv and numpy . thank immense availability , you have had . another question : I 'm tempted to buy the premium course bundle @ @ @ @ @ @ @ @ @ @ it . this course will be available in the near future or has to date not be available ? greetings <p> Hey Mobin if you want to use the Raspberry Pi camera , I would suggest reading this post on accessing the Raspberry Pi camera . It will only take a few small modifications to this code to take an input image from the Raspberry Pi . <p> First of all , great work on the blog and the tutorials , I really appreciate your work . This has already helped me a lot with what I am trying to achieve = Marc already wanted to know , whether there is a way of determining the boundaries from a given colour . In my application , I take a photo of lego bricks with a raspberry pi and want to determine the bricks positions inside a grid and what colour they are . I know which brick colours there are in the image , but I have to account for bad lighting ( or at least differing lighting conditions ) . Do you think I could get satisfactory results using @ @ @ @ @ @ @ @ @ @ calculating the boundaries in hsv with some fixed value ? <p> Hey Iris , in general I think you are on the right track . But I would strongly encouraging you to work with your lighting conditions prior to writing a single line of code . The success of most computer vision applications starts before a single line of code is written it all starts with the quality of your images . The higher quality they are , the easier they are to work with . <p> It certainly is ! You 'll need to make use of the cv2.VideoCapture function to obtain access to the webcam . I actually detail exactly how to perform color based object tracking inside Practical Python and OpenCV + Case Studies . Definitely take a look ! <p> thank you Adrian . this will be very helpful . can i ask , if you have another one example for shape detection , like arrows or any other shapes ? ? i just think , that i might easily understand you 're example . than the others in the internet . <p> If you 're interested in doing @ @ @ @ @ @ @ @ @ @ good start . I use contours to recognize square targets in image the same methodology can be used to identify other shapes . <p> Pure green in the RGB color space is ( 0 , 255 , 0 ) . However , you 'll need to play with these values a bit to make them detect the particular shade of green you are interested . An alternative is to use the HSV color space where you may find it easier to define the color ranges . <p> Hey , Adrian . Good blog ! The fact that I will be able to learn for free through your blog is just exceptional ! You are my inspiration ! <p> I want to ask something . I want to track two objects in a video . The two objects can change colours between green and orange ( those two colours only ) . Example , if Object1 is green , there will be print out Object1 = On " and at the same time if Object2 is orange , there will be print out Object2 = Off . <p> I am thinking of @ @ @ @ @ @ @ @ @ @ of the frame into two to focus on those two things differently and applying the cv2.range to both of the sliced frame . If the cv2.range detect green it will returns mask ( means there is white pixels inside the sliced frame ) , so , from that the program will know if the object is green or orange . But how can I tell the program to check if the returns contain mask or not ? <p> Or do you have any other better workaround ? <p> Thanks in advance ! Proud to say to anyone that I am learning OpenCV from you blog = <p> Thanks for this tutorial . Similar to a few other users here , I am also getting the following error : " The lower bounary is neither an array of the same size and same type as src , nor a scalar " . <p> I am using python 2.7.10 and what I believe is OpenCV 2.4.11-7 on Windows through Python ( x , y ) Idle . Is my set up ok for running this code ? I like Python ( x @ @ @ @ @ @ @ @ @ @ with in one quick install . <p> I have been troubleshooting for a while but I ca n't figure out what is wrong . I really just want to segment an image by color do you know of any other simple methods to do this ? <p> Hey Adrian , thank you for checking this out for me . Indeed you are correct and I am receiving NONE when I try to print the image with opencv commands . After troubleshooting the imread command for a bit , I tried loading the image with scipy instead then continuing the work with cv2 and this works ! From a bit of internet browsing , it seems like others who had this problem fixed it by working around it like I did or updating to a newer version of openCV . Thanks for the help . <p> Works great ! ! ! But if i want to separate the image into regions of different colors without knowing what colors will be in the image beforehand , how do I do it ? For example , if I have an image that contains just @ @ @ @ @ @ @ @ @ @ ) , and want to draw contour around the images how do I do it ? Thanks a lot ! ! ! <p> PS . I saw your other post on labeling different colors in an image but will that work if some of the colored region is inside other region ? <p> The only piece of code that needs to change is to apply a " loop " that loops over an input set of images . Take a look at how I apply the listimages function in this blog post and it will give you a good idea on how to process multiple images . <p> Hello Adrian , Thanks for the amazing tutorials ! ! ! I just have a small question : i want to detect a Picture of apps on my Smartphone Screen : exactly I have a Webcam and i have to detect apps on my Smartphone Screen und recognize where they are . And i want to define the Coordinate of this . how can i do it ? please help me = ( I use python 3 and opencv3 with Raspberry pi @ @ @ @ @ @ @ @ @ @ my desk sir , and then i get an eror , can you help me resolve this ? <p> OpenCV Error : Sizes of input arguments do not match ( The lower bounary is neither an array of the same size and same type as src , nor a scalar ) in inRange mask = cv2.inRange ( image , lower , upper ) cv2.error : LONG ... error : ( -209 ) The lower bounary is neither an array of the same size and same type as src , nor a scalar in function inRange <p> If you are absolutely , 100% sure that your path to the image is valid , then OpenCV likely can not read the image type you are trying to pass in . Again , I 'm pretty sure that your image paths are incorrect . But if they are , try placing a print(image) call right after cv2.imread . If you get None back AND your image path is valid , then OpenCV can not read your image type . <p> SWT is a great algorithm , although Im unaware of a good Python implementation @ @ @ @ @ @ @ @ @ @ I still think simple thresholding can be used one . The first threshold can be used to detect light text against dark backgrounds . And a second round of thresholding can be used to detect dark text against light backgrounds . Furthermore , since this is a scoreboard , you " know " where in the ROI the text will be , allowing you to focus on the regions of the image that are most likely to contain text . <p> Hi Adrian , Thank you for the great blog ! I am learning Python and openCV at the same time . Because I am just learning , I am using an editor to help me with my Python syntax . Running you code within the IDE produces the same error than many have mentioned above : " OpenCV Error : Sizes of input arguments do not match " As you have correctly pointed out already it is a path issue to the image . I can run the code in terminal and it runs perfectly . The question I have is this , can I add a line of @ @ @ @ @ @ @ @ @ @ run and then edit the code in my IDE ? Thanks for your time ! ! <p> Are you executing your Python script from within your IDE ? Or via command line argument ? If you 're using an IDE , I normally recommend using the IDE to write the code but then use the terminal to execute the code so you have better control over the command line arguments . <p> Defining the valid color ranges can be pretty tricky , especially if you 're just getting started . I would start by using the range-detector script in the imutils package . This will help you tune the ranges . <p> That sounds like a perfect use-case for the cv2.inRange function . Have you tried defining the lower and upper boundaries for your green objects yet ? I would suggest using the range-detector script mentioned in this blog post as a starting point . <p> To define your own custom color range , just define the lower and upper boundary boundaries . You can do this by modifying the boundaries variable . It might take some trial and error to get @ @ @ @ @ @ @ @ @ @ masking . Create a new image with the same dimensions as the original one , only with a white rather than black background . Then apply bitwise masking to make the foreground show up on the background . I provide more information on masking and bitwise operations inside Practical Python and OpenCV . <p> Normally , we refer to pixel values in RGB order . However , OpenCV refers to them in BGR order ( NOT GBR order ) . Therefore , the pixel values supplied in this blog post are provided in BGR order , which is what OpenCV expects . <p> hey adrian , ineed your help , i want to combine your work about the color detection and shape detection , adding the position of the object , and the result in show in the terminal , i already success but the color and shape detection did n't  combine in the result output , can you help me ? <p> You would want to define color threshold ranges for each color in the camouflage . Then , apply each of these thresholds to the image and construct @ @ @ @ @ @ @ @ @ @ This will help you detect camouflage in your image . <p> To write an image to disk you would see cv2.imwrite . If you 're just getting started with learning computer vision and OpenCV , I would suggest that you go through Practical Python and OpenCV . Your question is addressed in the first few chapters . <p> Hey Adrian , i would like to use this to concept to build an app to help people with color vision deficiency ( usually known as color blindness ) . I wan na detect whether in an image there is blue , red , green color , or any of its combination to help those who have partial color blindness. but i also need to detect any colors in an image for those who have total color blindness . Could you help me about this ? what should i do to make it works ? <p> ( fyi i 'm a real newbie with a really basics knowledge in opencv and python , so if u could give a detailed explaination it would be a great help for me ) Thanks in advance ! @ @ @ @ @ @ @ @ @ @ balancing you would first need to calibrate your images using a color chart or a gray-level balancing card . From there you can obtain a consistent level of color representation for a given scene with your camera . This is definitely more of an advanced technique and not one I would recommend you taking on if you are just getting started with OpenCV and computer vision . If you 're just getting started in the world of computer vision and OpenCV , I would suggest you work through Practical Python and OpenCV before continuing . <p> Thanks for ur kind reply , but i havent set my goal far to do color balancing , i just meant to ask what should i do to detect all colors within an image without setting any boundaries of RGB color space ? Just like the code u share , but without setting any boundaries , could u please give me the answer ? I also wan na ask could we give a highlight or whatsoever to differentiate each region of colors in an image ? and how could we give a text lable of @ @ @ @ @ @ @ @ @ @ thanks before <p> Are you looking to detect and label various colors in an image without setting any color boundaries and without color balancing ? Simply put , that 's not possible , unless you train a classifier to label the colors of pixels based on real-world images . <p> Hi , your code looks interesting . I am working on a project in which I have to perform color change , not just color detection . As in , if I detect yellow , I have to display an image with white overlapped on the yellow blobs . Any suggestion on how I could do that ? <p> You could use OpenCVs cv2.drawContours function to draw the white overtop the detected yellow blobs . I discuss the basics of OpenCV and how to use cv2.drawContours inside my book , Practical Python and OpenCV I would highly suggest starting there . <p> I run my application in Python 2.7 , but the following error OpenCV Error : Sizes of input arguments do not match ( The lower bounary is neither an array of the same size and same type as src @ @ @ @ @ @ @ @ @ @ ( image , lower , upper ) please help me <p> It depends . Normally you define a set of colors that you want to recognize and then quantize the colors in the image to their closest corresponding color in your database . I would look into @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485226 @185226/ <p> Instead , you should be utilizing a simple extension to SIFT , called RootSIFT , that can be used to dramatically increase object recognition accuracy , quantization , and retrieval accuracy . <p> Whether you 're matching descriptors of regions surrounding keypoints , clusterings SIFT descriptors using k-means , or building a bag of visual words model , the RootSIFT extension can be used to improve your results . <p> Best of all , the RootSIFT extension sits on top of the original SIFT implementation and does not require changes to the original SIFT source code . <p> You do not have to recompile or- modify your favorite SIFT- implementation- to utilize the- benefits- of RootSIFT . <p> So if you 're using SIFT regularly in your computer vision applications , but have yet to level-up to RootSIFT , read on . <p> This blog post will show you how to implement RootSIFT in Python and OpenCV without ( 1 ) having to change a single line of code in the original OpenCV SIFT implementation and ( 2 ) without having to compile the entire library . <p> Sound interesting @ @ @ @ @ @ @ @ @ @ learn how to implement RootSIFT in Python and OpenCV . <p> OpenCV and Python versions : In order to run this example , you 'll need Python 2.7 and OpenCV 2.4 . X. <h> Why RootSIFT ? <p> It is well known that when comparing histograms the Euclidean distance often yields inferior performance than when using the chi-squared distance or the Hellinger kernel Arandjelovic et al . 2012 . <p> And if this is the case why do we often use the Euclidean distance to compare SIFT descriptors when matching keypoints ? Or clustering SIFT descriptors to form a codebook ? Or quantizing SIFT descriptors to form a bag of visual words ? <p> Remember , while the original SIFT papers discuss comparing descriptors using the Euclidean distance , SIFT is still a histogram itself and would n't other distance metrics offer greater accuracy ? <p> It turns out , the answer is yes . And instead of comparing SIFT descriptors using a different- metric we can instead modify the 128-dim descriptor returned from SIFT directly . <p> You see , Arandjelovic et al . suggests a simple algebraic extension to the @ @ @ @ @ @ @ @ @ @ descriptors to be " compared " using a Hellinger kernel but still utilizing the Euclidean distance . <p> UPDATE : From the- Arandjelovic et al.s paper and presentation , its a little ambiguous if the final L2 normalization should be performed or not . This step is not mentioned in their paper . But it is mentioned on- Slide 10 of their presentation. - As Chris pointed out in the comments section , - explicitly performing the L2 normalization is not needed . By taking the L1 norm , followed by the square-root , we have already L2 normalized the feature vector and further normalization is not needed . <p> That 's it ! <p> Its a simple extension . But this little- modification can dramatically improve results , whether you 're matching keypoints , clustering SIFT descriptors , of quantizing to form a bag of visual words , Arandjelovic et al . has shown that RootSIFT can easily be used in all scenarios that SIFT is , while improving results . <p> In the rest of this blog post I 'll show you how to implement RootSIFT using Python and OpenCV . @ @ @ @ @ @ @ @ @ @ into your own applications and improve your results ! <h> Implementing RootSIFT in Python and OpenCV <p> Open up your favorite editor , create a new file and name it rootsift.py- , and let 's get started : 23 <p> 24 <p> 25 55203 @qwx675203 55220 @qwx675220 <p> importcv2 <p> classRootSIFT : <p> definit(self) : <p> # initialize the SIFT feature extractor <p> LONG ... <p> defcompute ( self , image , kps , eps=1e-7 ) : <p> # compute SIFT descriptors <p> ( kps , descs ) **29;9819;TOOLONG , kps ) <p> # if there are no keypoints or descriptors , return an empty tuple <p> iflen(kps)==0 : <p> return ( , None ) <p> # apply the Hellinger kernel by first L1-normalizing and taking the <p> # square-root <p> descs/= ( descs.sum ( axis=1 , @ @ @ @ @ @ @ @ @ @ np.linalg.norm ( descs , axis=1 , ord=2 ) + eps ) <p> # return a tuple of the keypoints and descriptors <p> return ( kps , descs ) <p> The first thing well do is import our necessary packages . Well use NumPy for numerical processing and cv2- - for our OpenCV bindings . <p> The compute- - function on- Line 10 then handles the computation of the RootSIFT descriptor . This function requires two arguments and an optional third argument . <p> The first argument to the- compute- function is the image- - that we want to extract RootSIFT descriptors from . The second argument is the list of keypoints , or local regions , from where the RootSIFT descriptors will be extracted . And finally , an epsilon variable , eps- , is supplied to prevent any divide-by-zero errors . <p> From there , we extract the original SIFT descriptors on- Line 12 . <p> We make a check on- Lines 15 and 16- if there are no keypoints or descriptors , we simply return an empty tuple . <p> Converting the original SIFT descriptors to RootSIFT descriptors @ @ @ @ @ @ @ @ @ @ each vector in the descs- - array ( Line 20 ) . <p> From there , we take the square-root of each element in the SIFT vector ( Line 21 ) . <p> And we finally L2-normalize the resulting SIFT vectors ( Line 22 ) . <p> Lastly , - all we have to do is return the tuple of keypoints and RootSIFT descriptors to the calling function on- Line 25 . <h> Running RootSIFT <p> To actually see RootSIFT in action , open up a new file , name it driver.py- , and well explore how to extract SIFT and RootSIFT descriptors from images : 20 <p> 21 <p> 22 55203 @qwx675203 <p> fromrootsift importRootSIFT <p> importcv2 <p> # load the image we are going to extract descriptors from and convert <p> # it to grayscale <p> image=cv2.imread ( " example.png @ @ @ @ @ @ @ @ @ @ keypoints in the image <p> **35;9850;TOOLONG " SIFT " ) <p> **25;9887;TOOLONG <p> # extract normal SIFT descriptors <p> LONG ... <p> ( kps , descs ) =extractor.compute ( gray , kps ) <p> print " SIFT : kps=%d , descriptors=%s " % ( len(kps) , descs.shape ) <p> # extract RootSIFT descriptors <p> rs=RootSIFT() <p> ( kps , descs ) =rs.compute ( gray , kps ) <p> print " RootSIFT : kps=%d , descriptors=%s " % ( len(kps) , descs.shape ) <p> On Lines 1 and 2 we import- our RootSIFT- - descriptor along with our OpenCV bindings . <p> We then load our example image , convert it to grayscale , and detect Difference of Gaussian keypoints on Lines 7-12 . <p> From there , we extract the original SIFT descriptors on Lines 15-17 . <p> And we extract the RootSIFT descriptors on Lines 20-22 . <p> To execute our script , simply issue the following command : <p> Implementing RootSIFT in Python and OpenCV <p> Shell <p> 1 <p> $python driver.py <p> Your output should look like this : <p> Implementing RootSIFT in Python and OpenCV <p> @ @ @ @ @ @ @ @ @ @ 1006,128 ) <p> RootSIFT:kps=1006 , descriptors= ( 1006,128 ) <p> As you can see , we have extract 1,006 DoG keypoints . And for each keypoint we have extracted 128-dim SIFT and RootSIFT descriptors . <p> From here , you can take this RootSIFT implementation and apply it to your own applications , including keypoint and descriptor matching , clustering descriptors to form centroids , and quantizing to create a bag of visual words model all of which we will cover in future posts . <p> The RootSIFT extension- does not require you to modify the source of your favorite SIFT implementation it simply sits on top of the original implementation . <p> The simple 4-step 3-step process to compute RootSIFT is : <p> Step 1 : Compute SIFT descriptors using your favorite SIFT library . <p> Step 2 : L1-normalize each SIFT vector . <p> Step 3 : Take the square-root of each element in the SIFT vector . <p> Step 4 : - L2-normalize the resulting vector . <p> No matter if you are using SIFT to match keypoints , form cluster centers using k-means , or quantize @ @ @ @ @ @ @ @ @ @ you should definitely consider utilizing RootSIFT rather than the original SIFT to improve your object retrieval accuracy . <h> Downloads : 55217 @qwx675217 <p> If its segfaulting then its probably an issue with the way OpenCV was installed and compiled . Check your OpenCV version and ensure you are running something of the 2.4 . X flavor . Also try to remove one line at a time and re-run until you can pinpoint the line of code that is causing the segfault . <p> Alright , from the python interpreter I managed to dump out the version of both python and opencv which are : Python 2.7.6 ( default , Mar 22 2014 , 22:59:56 ) and &gt;&gt;&gt; from cv2 import version ; version 2.4.8 <p> I went ahead and requested your original scripts and its still giving me an error : segmentation fault ( core dumped ) <p> Hi Adrian and Johnny , The segmentation fault is because OpenCV was built without using the nonfree module . SIFT is in the nonfree module and hence the segmentation fault . I faced this problem some weeks back when I had @ @ @ @ @ @ @ @ @ @ repository . Hope it helps ! ! <p> Nice , thanks for the tip Bikram ! = But to my understanding , only OpenCV 3 ( which is in beta ) does not compile the nonfree module by default . The previous versions of OpenCV 2.4 . X still compiled all nonfree modules during installation time . <p> The color information ( i.e. the Red , Green , and Blue channels individually ) are not needed to detect keypoints . Furthermore , most keypoint detectors expect a grayscale image so we convert from RGB to grayscale and discard the color information . <p> Hi , Adrian , in the rootsift.py , you apply the Hellinger kernel by first L1-normalizing , taking the square-root , and then L2-normalizing . I have tested by first L2-normalizing , taking the square-root , and then L1-normalizing , just as the paper says : 1 ) L1-normailze the SIFT vector ( originally it has unit L2 norm ) ; 2 ) square root each element . I find the order of L2-normalizing and L1-normalizing does n't  effect the values of rootSIFT . But for understanding @ @ @ @ @ @ @ @ @ @ the paper says . <p> Hey Yong , take a look at Slide 10 of the presentation done by Arandjelovic . This slide details their implementation of RootSIFT where the first step is L1 normalization , the second is element-wise square root , and the final step is L2 normalization . Its interesting that the values were not effected though . <p> 1 ) When you do L1 normalization with axis=0 , are n't  you normalizing all columns of the set of descriptors ? I would think you would use desc /= ( desc.sum ( axis=1 , keepdims=True + eps ) if you wanted to normalize each SIFT descriptor <p> 2 ) Are you sure you are supposed to divide the normalized and squarerooted descriptor vectors by the L2 norm ? If you read the paper by Arandjelovic and Zimmerman , they do not do this . I feel like you did this because you saw that as a step in their presentation , but I think that in that slide they were saying that the descriptor is L2 normalized as a result of L1 normalizing and taking the square @ @ @ @ @ @ @ @ @ @ rocker , and thanks for introducing me to this cool trick ! <p> 1 . Thanks for pointing this out ! It looks like I have accidentally pushed a previous version of the RootSIFT code online from my git repo . This was certainly not my intention . Thanks a million for pointing this out . The code and blog post have been updated . <p> 2 . The original SIFT descriptor is L2 normalized , so while the paper does not explicitly state that square-rooted descriptor should be L2 normalized , I think its applied . Perhaps I am wrong , but that 's how I interpreted it . <p> Good point . It does seem like this step is unnecessary . I am going to run some benchmarks related to image retrieval accuracy on my system and see if anything changes . Technically , it shouldnt . But either way I 'll be posting an update on this article mentioning that the final L2 normalization is not necessary . <p> In section 5.1 , Implementation Details , the author mentions transforming SURFs using signed square rooting , and then references @ @ @ @ @ @ @ @ @ @ : Dense SURFS , which are transformed using signed square-rooting . " <p> Is this essentially a " RootSURF " , or am I oversimplifying it ? And can the Hellinger Kernel be used effectively with other feature extractors , like AKAZE ? <p> Feature vectors generated from AKAZE and KAZE are binary feature vectors so they are compared using a Hamming distance . The chi-squared distance does n't  make much sense here , unless you have constructed a bag-of-visual-words and are comparing the bag-of-visual-words histograms using the chi-squared distance . <p> Hi Adrian , I used your post on " Where did SIFT and SURF go " just a few days ago to get OpenCV 3.0.0 installed on my fresh install of Jessie ( RPI 2 ) . I admire your method as it parallels my own " How To " instructions ; replete with expected times for each step ! The install went great , but for some reason its not playing well with the code above . <p> For example , on line 11 you have : detector = **26;9914;TOOLONG " SIFT " ) which results in @ @ @ @ @ @ @ @ @ @ so I changed it to : detector = **28;9942;TOOLONG <p> On line 15 you have : extractor = **30;9972;TOOLONG " SIFT " ) which results in the same type of error ( module has no attribute ) <p> I tried playing around with an " xfeatures2d " version of that line of code without any luck . Documentation from OpenCV is also not up-to-date regarding 3.0.0 as several sites have pointed out . <p> hey man ! just wanted to ask how would I compute a matrix for all the descriptors if i am taking a large dataset of images , here you have taken one image in consideration . I could run a loop to read all the images but how should i store all the descriptors in a matrix ? <p> Hey Adrian , Really useful article . Thank you ! = But I have a conceptual based doubt regarding SIFT . Since I am a beginner , this question might come out to be a bit absurd . <p> I understand that cv2.KeyPoints Class offers an attribute pt for the x and y coordinates of each keypoint @ @ @ @ @ @ @ @ @ @ position after applying SIFT ? I have searched long and hard but have n't gotten an answer . I am really hoping I can find the answer here . <p> The DoG keypoint detector ( confusingly called " SIFT " in OpenCV which is also the name of the local invariant descriptor ) does indeed return a keypoint object with a . pt attribute . The SIFT descriptor takes this object and then describes the region surrounding it . I 'm not sure what you mean by " extract the pixel position after applying SIFT " because the pixel position has n't changed at all . Applying the SIFT descriptor does not change the ( x , y ) -coordinates of the keypoint . <p> I admit I might be even more confused with the concept than I thought . <p> I am currently working on something where I am required to apply SLIC and then SIFT on an input image . I am the trying to calculate the number of keypoints for each of the superpixels I obtain after SLIC . Upon implementing SLIC , I get a 2D numpy @ @ @ @ @ @ @ @ @ @ as the image . So I was hoping upon extracting the coordinates of the keypoints after SIFT , I can apply a simple conditional statement in the iteration of segments and use a " count " variable to calculate the total number of keypoints in that superpixel ? <p> What my question really means is : Are the coordinates returned using pt the pixel positions of the keypoint ( so as I can use them as stated above ) ? But I have noticed that the pt attribute returns float-like ( x , y ) value . This is where my real confusion arises . <p> I would suggest detecting keypoints on the image first . Then , apply SLIC and obtain your " segments " . Loop over each of these segments and then check to see if the ( x , y ) -coordinates of the keypoint . pt object falls inside the segment . This will allow you to assign each of the keypoints to a specific superpixel . <p> At the end of the day , the coordinates returned by . pt are the ( x @ @ @ @ @ @ @ @ @ @ the original image . <p> Make sure you use the " Downloads " section of this blog post to download the source code + project structure for the tutorial . You likely do not have the project structure setup correctly @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485227 @185227/ <h> Tag Archives object detection <p> Todays blog post is inspired from an email I received from Jason , a student at the University of Rochester . Jason is interested in building a custom object detector using the HOG + Linear SVM framework for his final year project . He understands the steps required to build the object detector well enough - but he is n't <p> Did you know that OpenCV can detect cat faces in imagesright out-of-the-box with- no extras ? I did n't  either . But after Kendrick Tan broke the story , I had to check it out for myselfand do a little investigative- work to see how this cat detector seemed to sneak its way into the OpenCV repository without me noticing ( much <p> Last week we discussed how to use OpenCV and Python to perform pedestrian detection . To accomplish this , we leveraged the built-in HOG + Linear SVM detector that OpenCV ships with , allowing us to detect people in images . However , one aspect of the HOG person detector we- did not discuss in detail is the detectMultiScale- function @ @ @ @ @ @ @ @ @ @ , uplifting- people over the years . My PhD advisor who helped get me through graduate school . My father who was always there for me as a kid and still is now . And- my girlfriend who has always been positive , helpful , and supportive ( even when I probably did n't  deserve it ) . I 've also <p> This past Saturday , I was caught in the grips of childhood nostalgia , so I busted out my PlayStation 1 and my original copy of Final Fantasy VII . As a kid in late middle school/early high school , I logged 70+ hours playing through this heartbreaking , inspirational , absolute masterpiece of an RPG . As a kid in middle <p> Today marks the- 100th blog post on PyImageSearch. 100 posts. - Its hard to believe it , but its true . When I started PyImageSearch back in January of 2014 , I had no idea what the blog would turn into . I did n't  know how it would evolve and mature . And I most certainly did not know how popular it would <p> So @ @ @ @ @ @ @ @ @ @ an image pyramid . And in todays article we are going to extend that example and introduce the concept of a sliding window . Sliding windows play an integral role in object classification , as they allow us to localize exactly- " where " in an image an object resides . <p> It 's too damn cold up in Connecticut- so cold that I had to throw in the towel and escape for a bit . Last week I took a weekend trip down to Orlando , FL just to escape . And while the weather was n't perfect ( mid-60 degrees Fahrenheit , cloudy , and spotty rain , as @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485228 @185228/ <p> Today I am- super excited- to share with you the- Kickstarter reward levels . <p> This reward list is- exclusive to PyImageSearch readers- and will not be publicly available until the Kickstarter campaign launches on- Wednesday , January 18th at 10AM EST . <p> So why am I sharing this reward list with you now ? <p> To give you an advantage , of course ! <p> When the- Deep Learning for Computer Vision with Python- Kickstarter campaign launches on Wednesday , there will be 1,000s of PyImageSearch readers ( not to mention , other Kickstarter users ) reading the campaign page and trying to decide which reward to select . <p> The problem is that- if you wait too long- to select the reward level you want , - you might miss out on it ! <p> Honestly , I can not emphasize this point enough : <p> There are only a handful of early bird spots available at reduced prices , so you 'll definitely want to- plan ahead- and- act quickly- if you want to claim your copy at a discounted price . <p> How quickly @ @ @ @ @ @ @ @ @ @ numbers from two big product sales/launches in PyImageSearch history to give you an idea : <p> 2015 PyImageSearch Gurus Kickstarter : - Two years ago I ran a Kickstarter to fund the PyImageSearch Gurus course . The campaign was funded in less than 30 minutes with- all early bird rewards sold out within- 8 minutes . <p> 2016 Black Friday sale : - This past Thanksgiving , I offered 25% OFF my book , - Practical Python and OpenCV . To the first 15 customers I offered a FREE Raspberry Pi + a hand-signed , hardcopy edition " - all 15 orders came in during the first- 93 seconds- of the sale . <p> The PyImageSearch community has grown- a lot- since my last Kickstarter in 2015 so I 'm projecting that- all early bird rewards will sell out within 5 minutes- of the Kickstarter campaign going live " - and to be totally honest with you , - they could go a lot faster . <p> Be sure to plan ahead and pick out the reward you want ! - Remember , other PyImageSearch readers and Kickstarter users will be @ @ @ @ @ @ @ @ @ @ <p> The main reward for this Kickstarter is the- Deep Learning for Computer Vision with Python- eBook offered at a- heavily discounted rate . <p> These rates are- exclusive- to the Kickstarter campaign and will not be available once- Deep Learning for Computer Vision with Python- officially launches . <p> I have broken down the book into three volumes called- bundles , - so you can decide which bundle is most appropriate for you based on : <p> All- source code listings- so you can run the examples from the book out-of-the-box . <p> A- downloadable- pre-configured- Ubuntu VirtualBox virtual machine- that ships with all the necessary Python + deep learning libraries you will need to be successful- pre-installed . <p> Access to the- Deep Learning for Computer Vision with Python- companion website- so you can further your knowledge , even when you 're done reading the book . <p> The- ImageNet Bundle- also includes a- hardcopy edition of- all three volumes- delivered to your doorstep " please note that this is the- only- bundle that includes the hardcopy edition . <p> After choosing a bundle , you should decide whether @ @ @ @ @ @ @ @ @ @ vision/OpenCV education- before- you begin your deep learning journey . <p> I- highly recommend- that you choose- at least one- of these add-ons to make the most out of- Deep Learning for Computer Vision with Python . <h> Add-on #1 : - Practical Python and OpenCV <p> Along with the- Deep Learning for Computer Vision with Python- book bundles , I 'm also offering a- Kickstarter-exclusive printing of my book , - Practical Python and OpenCV. - I will be- individually numbering- and- hand-signing each copy , - just for you . <p> Bottom Line : - You should choose the- Practical Python and OpenCV- reward add-on if you have zero ( or minimal ) experience in computer vision or OpenCV- and want to learn the basics in less than a weekend. - Please see- this page- for more details on my book . <h> Add-on #2 : PyImageSearch Gurus <p> PyImageSearch Gurus is an online- course- and- community- I have meticulously designed to take you from computer vision- beginner- to- expert. - Guaranteed . <p> This course is- super in-depth , so know that- you 'll be getting a HUGE deal @ @ @ @ @ @ @ @ @ @ the cost of- Deep Learning for Computer Vision with Python- is practically- free- once you build in the price of the Gurus course ) . You can learn more about PyImageSearch Gurus- here . <p> Bottom Line : - You should choose the- PyImageSearch Gurus add-on- if you want to study- computer vision- in the same level of depth that you 'll be studying- deep learning- once- Deep Learning for Computer Vision with Python- is released . <h> Pledge $170 or more : <h> Pledge $500 or more : <p> EARLY BIRD SPECIAL " Youll be getting the Starter Bundle , a hand-signed hardcopy of my book , Practical Python and OpenCV , and access to the PyImageSearch Gurus course , all for the exclusive early bird pricing- Limited ( 5 of 5 left ) <h> Pledge $245 or more : <h> Pledge $750 or more : <p> EARLY BIRD SPECIAL " Youll be getting the Practitioner Bundle , a hand-signed hardcopy of my book , Practical Python and OpenCV , and access to the PyImageSearch Gurus course , all for the exclusive early bird pricing. - Limited ( 5 of @ @ @ @ @ @ @ @ @ @ Pledge $995 or more : <p> EARLY BIRD SPECIAL " Youll be getting the ImageNet Bundle , a hand-signed hardcopy of my book , Practical Python and OpenCV , and access to the PyImageSearch Gurus course , all for the exclusive early bird pricing. - Limited ( 5 of 5 left ) <h> Pledge $1,245 or more : <h> Special Rewards <h> Pledge $2,500 or more : <p> Includes everything in the $1,245 reward . Plus , have dinner with me in the NYC area . During dinner , we can discuss your own computer vision/deep learning projects or talk about deep learning topics that interest you. - Limited ( 3 of 3 left ) <p> As you can see , there are- significant discounts- for funding the project early , - so be sure to pick out the reward level that you want- before- the Kickstarter campaign goes live ! <p> And remember , - Wednesday is the big day- " - you 'll receive an email from me at 10AM EST with a link to the Kickstarter page . <p> Please keep in mind , there are a- very @ @ @ @ @ @ @ @ @ @ so when the Kickstarter launches on Wednesday , - you 'll definitely want to @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485229 @185229/ <p> While OpenCV- itself does n't  play a critical role in deep learning , it- is used by- other deep learning libraries such as Caffe , specifically in " utility " programs ( such as building a dataset of images ) . Simply put , having OpenCV installed makes it easier to write code to facilitate the procedure of pre-processing images- prior to feeding them into deep neural networks . <p> Because of this , we should install OpenCV into the same environment as our deep learning libraries , to at the very least , make our lives easier . <p> Furthermore , in a GPU-enabled CUDA environment , there are a number of compile-time optimizations we can make to OpenCV , allowing it to take advantage of the GPU for faster computation ( but mainly for C++ applications , not so much for Python , at least at the present time ) . <p> Ill be making the assumption that you 'll be installing OpenCV into the- same environment as last weeks blog post in this case , I 'll be continuing my example of using the Ubuntu 14.04 g2.2xlarge @ @ @ @ @ @ @ @ @ @ of the PyImageSearch blog , then you 'll also know that I 'm a- big fan of using pip- , virtualenv- , and virtualenvwrapper- to create sequestered , independent Python virtual environments for each of our projects . You can install the virtual environment packages using the commands listed below ( or you can skip this step if you already have Python virtual environments setup on your machine ) : <p> Then , scroll down to the bottom of the file , append the following lines , and save + exit the editor : <p> Compiling OpenCV with CUDA support <p> Shell <p> 1 <p> 2 <p> 3 <p> # virtualenv and virtualenvwrapper <p> export **29;10004;TOOLONG <p> **41;10035;TOOLONG <p> At this point , we can create our cv- virtual environment : <p> Compiling OpenCV with CUDA support <p> Shell <p> 1 <p> 2 <p> 3 <p> $source/.bashrc <p> $mkvirtualenv cv <p> $pip install numpy <p> Note : Again , you 'll want to read the first half of this blog post to better understand Python virtual environments if this is your first time using them . I also explain them more thoroughly and @ @ @ @ @ @ @ @ @ @ on this website . <p> Now , let 's download and unpack OpenCV . If you 're using the default Amazon EC2 g2.2xlarge instance , then- I highly suggest that you download the OpenCV sources and do your compiling on /mnt- . <p> The default g2.2xlarge instance has only 8GB of space , which once you factor in the system files , NVIDIA drivers , etc. , is not enough room to compile OpenCV from source : <p> Figure 1 : The default disk size for the g2.2xlarge instance is only 8GB , which does n't  leave enough space to compile OpenCV from source . <p> However , the /mnt- volume has 64GB of space , which is- more than enough for our compile : <p> Figure 2 : However , if we use the /mnt volume instead , we have 64GB far more than what is required to compile OpenCV . <p> If you are indeed on an Amazon EC2 instance , be sure to change directory to /mnt- and create a directory specifically for your OpenCV compile prior to downloading the source : <p> Compiling OpenCV with CUDA support <p> @ @ @ @ @ @ @ @ @ @ $cd/mnt <p> $sudo mkdiropencvcompile <p> $sudo chown-Rubuntu opencvcompile <p> $cdopencvcompile <p> The above command will create a new directory named opencvcompile- in the /mnt- volume , followed by giving the ubuntu- user permission to modify it at their will . <p> Note : The /mnt- volume is what Amazon calls " ephemeral storage " . Any data put on this volume will be lost when your system is stopped/rebooted . You do n't  want to use /mnt- to store long-term data , but its perfectly fine to use /mnt- to compile OpenCV . Once OpenCV is compiled , it will be installed to the system drive your OpenCV- installation will not- disappear- between reboots . <p> For this tutorial , I 'll be using OpenCV 3.1 . But you could also use OpenCV 2.4 . X or OpenCV 3.0 . Use the following commands to download the source : <p> We are now ready to use cmake- to configure our build . Take special care when running this command , as I 'm introducing some configuration variables you may not be familiar with : <p> <p> 11 <p> 12 <p> $cdopencv-3.1.0 <p> $mkdirbuild <p> $cdbuild <p> **30;10078;TOOLONG <p> **31;10110;TOOLONG <p> -DWITHCUDA=ON <p> -DENABLEFASTMATH=1 <p> -DCUDAFASTMATH=1 <p> -DWITHCUBLAS=1 <p> **26;10143;TOOLONG <p> LONG ... <p> -DBUILDEXAMPLES=ON .. <p> To start , take note of the WITHCUDA=ON- flag . Technically , this flag will be set to ON- by default since CMake is smart enough to detect that the CUDA Toolkit is installed . But , just in case , well manually set the variable to WITHCUDA=ON- to ensure CUDA support is compiled . <p> From there , we add in a few more optimizations , mainly around using cuBLAS , an implementation of the BLAS ( Basic Linear Algebra Subprograms ) library in the CUDA runtime . <p> We also indicate that we want to utilize the " fast math " optimizations , a series of- extremely fast mathematical routines that are optimized for speed ( they are written in Assembly ) and essentially perform little-to-no error checking . Again , the FastMath libraries are geared towards pure speed and nothing else . <p> After running cmake- @ @ @ @ @ @ @ @ @ @ section it should look similar to mine , which I have included below : <p> Figure 3 : Examining the output of CMake to ensure OpenCV will be compiled with CUDA support . <p> Notice how CUDA support is going to be compiled using both cuBLAS and- " fast math " optimizations . <p> Provided that your own CMake command exited without error , you can now compile and install OpenCV : <p> Compiling OpenCV with CUDA support <p> Shell <p> 1 <p> 2 <p> 3 <p> $make-j8 <p> $sudo makeinstall <p> $sudo ldconfig <p> If all goes well , the make- command should run successfully : <p> Figure 4 : OpenCV with CUDA support has successfully compiled . <p> Again , assuming your compile finished without error , OpenCV should now be installed in **39;10171;TOOLONG . You can verify this using the ls- command : <p> Compiling OpenCV with CUDA support <p> Shell <p> 1 <p> 2 <p> 3 <p> **43;10212;TOOLONG <p> total2092 <p> -rw-r--r--1root **27;10257;TOOLONG <p> Note : Youll want to find and make note of where your cv2.so- file is on your system ! Whenever we create @ @ @ @ @ @ @ @ @ @ to explore various deep learning libraries ) , you 'll want to sym-link the cv2.so- file into the site-packages- directory of your Python virtual environment so you have access to OpenCV . <p> The last step is to sym-link the cv2.so- file ( our Python bindings ) into the cv- virtual environment : <p> Compiling OpenCV with CUDA support <p> Shell <p> 1 <p> 2 <p> LONG ... <p> LONG ... <p> To verify our installation , open up a new terminal , access the cv- virtual environment using the workon- command , fire up a Python shell , and then import OpenCV : <p> Compiling OpenCV with CUDA support <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> 6 <p> 7 <p> $cd <p> $workon cv <p> $python <p> &gt;&gt;&gt;import cv2 <p> &gt;&gt;&gt;cv2. version <p> ' 3.1.0 ' <p> &gt;&gt;&gt; <p> Finally , now that OpenCV is installed , let 's perform a bit of cleanup and remove the source files used for installation : <p> Compiling OpenCV with CUDA support <p> Shell <p> 1 <p> 2 <p> $cd/mnt <p> $sudo rm-rf opencvcompile <p> Again , I @ @ @ @ @ @ @ @ @ @ working with Python virtual environments , the site-packages- directory , and how to use symbolic links . I recommend the following tutorials to help understand each of them : <p> Furthermore , by installing OpenCV with CUDA support , we can take advantage of the GPU for further optimized operations ( at least from within C++ applications there is n't much support for Python + OpenCV + GPU , yet ) . <p> Next week , I 'll detail how to install the Keras Python package for deep learning and Convolutional Neural Networks from there , the real fun will start ! <p> Need to manually download ippicvlinux20151201.tgz and copy to LONG ... Else build will report error , said download failed . ( maybe because of poor net connection ? but when I manually download the file , the speed is fast ) <p> Correct , the GPU bindings with OpenCV are not available with Python , but you could call them within a Python script as a compiled C++ extension or you might be using CUDA support in another program . In either case , its important to see how @ @ @ @ @ @ @ @ @ @ .. its been a rough day with opencv cuda is installed and when i run nvcc -V it prints the cuda 7.5 that i am using .. then i tried to compile opencv with cuda by following this tutorial .. i had no problem and no errors and followed all the steps , cmake , make -j4 , and sudo make install .. all worked fine .. but when i try to import cv2 it seems that its not installed .. when i list the files that are in **38;10286;TOOLONG its zero no files are there .. so i ran this command to locate the cv2.so file and this was the results : naif@naif-Z170-D3H : $ sudo find / -name " *cv2.so* " sudo password for naif : LONG ... LONG ... naif@naif-Z170-D3H : $ <p> actually i did n't  set up a virtual environment so i did n't  need it , i reinstall and compile it , and got error is cmake , and found this in opencv docs that in some systems it might need to fix cmake macro , so i ran <p> 1 . Make sure @ @ @ @ @ @ @ @ @ @ importing cv2. 2 . Check the " build/lib " directory to ensure your Python bindings are compiled . 3 . Read through the @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485231 @185231/ <h> Archive Rants <p> Over the past three years running PyImageSearch.com I have received and answered tens of thousands of questions from readers- just like yourself who are interested in studying computer vision , OpenCV , and deep learning . Looking back on this time , I can say that the vast majority of the questions I have answered have been a- real pleasure to <p> Todays blog post is part one of a two-part series inspired by an email I received over the weekend from Aarav , a PyImageSearch reader who is interested in studying computer vision : SUBJECT : How do I ask good computer vision questions ? Hey Adrian , My name is Aarav . First , I just want to say how much I <p> Ever wonder what its like to work as a computer vision researcher or developer ? You 're not alone . Over the past few years running PyImageSearch I have received emails and inquiries that are " outside " traditional computer vision and OpenCV questions . They instead focus on something- much more personal - my daily life . PyImageSearch reader Jared @ @ @ @ @ @ @ @ @ @ post is a bit cynical in tone . In all honesty , I support deep learning research , I support the findings , and I believe that by researching deep learning we can only further improve our classification approaches and develop better methods. - We all know that research is iterative . And sometimes we even @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485232 @185232/ <h> Tag Archives hog <p> Todays blog post is inspired from an email I received from Jason , a student at the University of Rochester . Jason is interested in building a custom object detector using the HOG + Linear SVM framework for his final year project . He understands the steps required to build the object detector well enough - but he is n't <p> Last week we discussed how to use OpenCV and Python to perform pedestrian detection . To accomplish this , we leveraged the built-in HOG + Linear SVM detector that OpenCV ships with , allowing us to detect people in images . However , one aspect of the HOG person detector we- did not discuss in detail is the detectMultiScale- function ; specifically , <p> I 've met a lot of amazing , uplifting- people over the years . My PhD advisor who helped get me through graduate school . My father who was always there for me as a kid and still is now . And- my girlfriend who has always been positive , helpful , and supportive ( even when I probably did n't  @ @ @ @ @ @ @ @ @ @ weeks blog post we discovered how to construct an image pyramid . And in todays article we are going to extend that example and introduce the concept of a sliding window . Sliding windows play an integral role in object classification , as they allow us to localize exactly- " where " in an image an object resides . <p> It 's too damn cold up in Connecticut- so cold that I had to throw in the towel and escape for a bit . Last week I took a weekend trip down to Orlando , FL just to escape . And while the weather was n't perfect ( mid-60 degrees Fahrenheit , cloudy , and spotty rain , as you can see from the <p> I have issues - I cant stop thinking about object detection . You see , last night I was watching The Walking Dead- and instead of enjoying the zombie brutality , the forced- cannibalism , or the enthralling storyline , - all I wanted to do was build an object detection system to recognize zombies . Would it be very useful ? Probably not . @ @ @ @ @ @ @ @ @ @ Bowl this past weekend ? I did . Kind of . I spent Super Bowl Sunday ( which is practically a holiday in the United States ) at my favorite Indian bar . Pounding Kingfisher beers . Savoring a delicious dish of- Tandoori chicken all while hacking up a storm on my laptop and coding up some custom <p> Connecticut is cold . Very cold . Sometimes its hard to even get out of bed in the morning . And honestly , without the aide of copious amounts of pumpkin spice lattes and the beautiful sunrise over the crisp autumn leaves , I do n't  think I would leave my cozy bed . But I have work to do . And today <p> If you 've been paying attention to my Twitter account lately , youve probably noticed one or two teasers of what I 've been working on a Python framework/package to rapidly construct object detectors using Histogram of Oriented Gradients and Linear Support Vector Machines . Honestly , I really ca n't stand using the Haar @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485233 @185233/ <h> Tag Archives keras <p> A few months ago I wrote- a tutorial on how to classify images using Convolutional Neural Networks ( specifically , VGG16 ) pre-trained on the ImageNet dataset with Python and the Keras deep learning library . The pre-trained networks inside of Keras are capable of recognizing- 1,000 different object categories , similar to objects we encounter in our day-to-day lives with high <p> A few months ago I demonstrated how to install the Keras deep learning library with a- Theano backend . In todays blog post I provide detailed , step-by-step instructions to install Keras using a TensorFlow backend , originally developed by the researchers and engineers on the Google Brain Team . Ill also ( optionally ) demonstrate how you can integrate OpenCV into <p> If you 've been following along with this series of blog posts , then you already know what a- huge fan I am of Keras . Keras is a super powerful , easy to use Python library for building neural networks and deep learning networks . In the remainder of this blog post , I 'll demonstrate how to build @ @ @ @ @ @ @ @ @ @ posts on Monday , - but I 'm so excited about this one that it could n't wait and I decided to hit the publish button early . You see , just a few days ago , - Fran+ois Chollet pushed three Keras models ( VGG16 , VGG19 , and ResNet50 ) online these networks- are- pre-trained- on the ImageNet dataset , meaning that they can recognize- 1,000 <p> In todays blog post , we are going to- implement our first Convolutional Neural Network ( CNN ) - LeNet - using Python and the Keras deep learning package . The LeNet architecture was first introduced by LeCun et al . in their 1998 paper , - Gradient-Based Learning Applied to Document Recognition. - As the name of the paper suggests , the authors implementation of LeNet was used <p> The purpose of this blog post is to demonstrate how to install the Keras library for deep learning . The installation procedure will show how to install Keras : With GPU support , so you can leverage your GPU , CUDA Toolkit , cuDNN , etc. , for faster network training . Without- @ @ @ @ @ @ @ @ @ @ a GPU <p> So you 're interested in deep learning and Convolutional Neural Networks . But where do you start ? Which library do you use ? There are just so many ! Inside this blog post , I detail 9 of my- favorite Python deep learning libraries . This list is- by no means- exhaustive , its- simply a list of libraries that Ive @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485235 @185235/ <h> The 3 Types of Image Search Engines : Search by Meta-data , Search by Example , and Hybrid <p> What type of image search engine do you want to build ? Is your search engine going to rely on tags , keywords , and text associated with an image ? Then you 're probably building a search by meta-data image search engine . <p> Are you actually examining the image itself and trying to understand what the image contains ? Are you trying to quantify the image and extract a set of numbers to represent the color , texture , or shape of an image ? Then you are likely building a search by example image search engine . <p> Or are you combining the two methods above ? Are you relying on textual information related to the image and then quantifying the image itself ? Sounds like a hybrid image search engine to me . <p> Let 's go ahead and breakdown down each of these types of image search engines and try to understand them a little better . <h> Search by Meta-data <p> You go to Google @ @ @ @ @ @ @ @ @ @ a text box that you type your keywords into , and two buttons : " Google Search " and " I 'm Feeling Lucky " . This is what we have come to love and adore as a text search engine . Manually typing in keywords and finding relevant results . <p> In fact , a meta-data image search engine is only marginally different from the text search engine mentioned above . A search by meta-data image search engine rarely examines the actual image itself . Instead , it relies on textual clues . These clues can come from a variety of sources , but the two main methods are : <h> 1 . Manual Annotations : <p> In this case , an administrator or user provides tags and keywords suggesting the content of an image . For example , let 's take a look at a screencap from my all time favorite movie , Jurassic Park . <p> What types of tags or keywords would we associate with this image ? Well , we see there are two- dinosaurs , - but to be more precise , they are- velociraptors . @ @ @ @ @ @ @ @ @ @ but its not a kitchen like in your house or apartment . Everything is stainless steel and industrial grade this is clearly a- restaurant kitchen . Finally , we see Tim , a boy- looking quite scared . Just by looking at this image for a second or two , we have come up with six tags to describe the image : - dinosaurs , - velociraptors , kitchen , - industrial- kitchen , boy- and- scared . This is an example of a manual annotation of images . We are doing the work , we are supplying the computer with keywords hinting at the content of the image . <h> 2 . Contextual Hints : <p> Normally , contextual hints only apply to webpages . Unlike manual annotations , where we had to come up with tags by hand , contextual hinting automatically examines the text surrounding an image or the text that an image appears on . The downside to this approach is that we are assuming the content of the image is related to the text on the webpage . This may work for websites such as @ @ @ @ @ @ @ @ @ @ content of the article , but if I were to implement a search by meta-data algorithm on this blog , it would ( wrongly ) associated the Jurassic Park image above with a bunch of keywords related to image search engines . While I would personally find this quite amusing , it demonstrates the limitations of the contextual hinting approach . <p> By using text keywords ( whether manual annotations of contextual hints ) to characterize an image , we can actually frame an image search engine as a- text search engine and apply standard practices from information retrieval. - As I mentioned above , the best example of an image search engine implementing search by meta-data is your standard Google , Bing , or Yahoo search that utilizes text keywords father than the content of the image itself . Next , let 's examine image search engines that take into account the actual content of the image . <h> Search by Example <p> Imagine you are Google or TinEye . You have billions of images that are searchable . Are you going to manually label each and every image ? @ @ @ @ @ @ @ @ @ @ and expensive . What about contextual hints ? That 's an automatic method , right ? Sure , but remember the limitations I mentioned above . You could get some very strange results by relying only on the text on the same webpage an image appears on . <p> Instead , you can build a " search by example " image search engine . These types of image search engines try to quantify the image itself and are called Content Based Image Retrieval- ( CBIR ) systems . A crude example would be to characterize the color of an image by the mean , standard deviation , and skewness of the pixel intensities in the image . ( Quick note : If you are building a simple image search engine , in many cases , this approach actually works quite well ) . <p> Given a dataset of images , we would compute these moments over all images in our dataset and store them on disk . When we quantify an image we are describing an image and extracting- image features . These image features are an abstraction of the image @ @ @ @ @ @ @ @ @ @ The process of extracting features from a collection of images is called- indexing . <p> Okay , so now we have extracted features from each and every image in our dataset . How do perform a search ? Well , the first step is to provide our system with a- query image , an example of what we are looking for our in dataset . The query image is described in the exact same manner as our indexed images . We then use a distance function , such as the Euclidean distance , to compare our query features to the features in our indexed dataset . Results are then sorted in terms of relevancy ( where the smaller the Euclidean distance means more " relevant " ) and presented to us . <p> Examples of search by example image search engines include- TinEye , Incogna , and my own Chic Engine and I 'd My Pill . In all of these examples , features are extracted from the query image and are compared to a database of features . <h> Hybrid Approach <p> Let 's pretend that we are building an image @ @ @ @ @ @ @ @ @ @ images with your tweets . And of course , Twitter let 's you supply hashtags to your tweets . <p> What if we used the hashtags to build a search by meta-data image search engine and then analyzed and quantified the image itself to build a search by example image search engine ? If we took this approach , we would be building a- hybrid image search engine , that includes both text keywords along with features extracted from the images . <p> The best example I can think of such a hybrid approach is Google Image Search . Does Google Image Search actually analyze the image itself ? You bet it does . But Google was primarily a text search engine first , so it does allow you to search by meta-data as well . <h> Summary <p> If you are relying on tags and keywords supplied by actual people , you are building a search by meta-data image search engine . If your algorithm analyzes the image itself and quantifies the image by extracting features , then you are creating a search by example search engine and are performing @ @ @ @ @ @ @ @ @ @ are using both keyword hints and features together , you are building a @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485236 @185236/ <p> From a computer vision perspective , dlib has a number of state-of-the-art implementations , including : <p> Facial landmark detection <p> Correlation tracking <p> Deep metric learning <p> Over the next few weeks well be exploring some of these techniques ( especially facial landmark detection ) , so definitely take the time now to get dlib configured and installed on your system . <h> Step #1 : Install dlib prerequisites <p> The dlib library only has four primary prerequisites : <p> Boost : - Boost is a collection of peer-reviewed ( i.e. , very high quality ) C++ libraries that help programmers not get caught up in reinventing the wheel . Boost provides implementations for linear algebra , multithreading , basic image processing , and unit testing , just to name a few . <p> Boost.Python : - As the name of this library suggests , Boost.Python provides interoperability between the C++ and Python programming language . <p> CMake : - CMake is an open-source , cross-platform set of tools used to build , test , and package software . You might already be familiar with CMake if @ @ @ @ @ @ @ @ @ @ . <p> Hint : - You can check if Homebrew is already installed on your machine by executing the brew- command in your terminal . If you get a brew:command notfound- error , then Homebrew is not installed on your machine . <p> Now that Homebrew is installed , open up your /. bashprofile- file ( create it if it does n't  exist ) : <p> How to install dlib <p> Shell <p> 1 <p> $nano/.bashprofile <p> And update your PATH- variable to check for packages installed by Homebrew- before checking the rest of your system : <p> How to install dlib <p> Shell <p> 1 <p> 2 <p> # Homebrew <p> export PATH=/usr/local/bin : $PATH <p> After updating your /. bashprofile- file , it should look similar to mine : <p> Figure 1 : After updating your /. bashprofile file , yours should look similar to mine . <p> We now need to reload the contents of the /. bashprofile- file via the source- command : <p> How to install dlib <p> Shell <p> 1 <p> $source/.bashprofile <p> This command only needs to be executed- once . Alternatively , @ @ @ @ @ @ @ @ @ @ automaticallysource- the /. bashprofile- for you . <p> Next , let 's install Python 2.7 and Python 3 : <p> How to install dlib <p> 1 <p> 2 <p> $brew install python <p> $brew install python3 <p> We can then install CMake , Boost , and Boost.Python : <p> How to install dlib <p> Shell <p> 1 <p> 2 <p> 3 <p> $brew install cmake <p> $brew install boost <p> $brew install **26;10326;TOOLONG <p> The --with-python3- flag ensures that Python 3 bindings for Boost.Python are compiled as well Python 2.7 bindings are compiled by default . <p> Once you start the boost-python- install , consider going for a nice walk as the build can take a bit of time ( 10-15 minutes ) . <p> As a sanity check , I would suggest validating that you have both boost- and boost-python- installed before proceeding : <p> How to install dlib <p> Shell <p> 1 <p> 2 <p> 3 <p> $brew listgrep'boost ' <p> boost <p> boost-python <p> As you can see from my terminal output , both Boost and Boost.Python have been successfully installed . <p> The last step is to @ @ @ @ @ @ @ @ @ @ to X11 . XQuartz is easy to install just download the . dmg- and run the install wizard . After installing , make sure you logout and log back in ! <p> Fun Fact : XQuartz used to be installed by- default on OSX 10.5-10.7 . We now need to manually install it . <p> Now that we have our prerequisites installed , let 's continue to our next ( optional ) step . <h> Step #2 : Access your Python virtual environment ( optional ) <p> If you have followed any of my PyImageSearch tutorials on installing OpenCV , then you are likely using Python virtual environments . <p> Using Pythons virtualenv and virtualenvwrapper libraries , we can create- separate , independent Python environments for each project we are working on this is considered a- best practice when developing software in the Python programming language . <p> Note : - I 've already discussed Python virtual environments many times before on the PyImageSearch blog so I wont spend any more time discussing them here today if you would like to read more about them , please see any of my installing @ @ @ @ @ @ @ @ @ @ dlib into a- pre-existing Python virtual environment , use the workon- command : <p> How to install dlib <p> Shell <p> 1 <p> $workon&lt;your virtualenv name&gt; <p> For example , if I wanted to access a Python virtual environment named cv- , I would use the command : <p> How to install dlib <p> Shell <p> 1 <p> $workon cv <p> Notice how my terminal window has changed the text ( cv ) - now appears before my prompt , indicating that I am in the cv- Python virtual environment : <p> Figure 2 : I can tell that I am in the " cv " Python virtual environment by validating that the text " ( cv ) " appears before my prompt . <p> Otherwise , I can create an entirely separate virtual environment using the mkvirtualenv- command the command below creates a Python 2.7 virtual environment named py2dlib- : <p> How to install dlib <p> Shell <p> 1 <p> $mkvirtualenv py2dlib <p> While this command will create a Python 3 virtual environment named py3dlib- : <p> How to install dlib <p> Shell <p> 1 <p> $mkvirtualenv py3dlib-ppython3 <p> @ @ @ @ @ @ @ @ @ @ environments are- optional , but highly recommended if you are doing any type of Python development . <p> For readers that have followed my previous OpenCV install tutorials here on the PyImageSearch blog , please make sure you access your Python virtual environment before proceeding to Step #3 ( as you 'll need to install the Python prerequisites + dlib into your virtual environment ) . <h> Step #3 : Install dlib with Python bindings <p> The dlib library does n't  have any real Python prerequisites , but if you plan on using dlib for any type of computer vision or image processing , I would recommend installing : <p> I do not have any plans to create a Windows install tutorial nor support Windows on the PyImageSearch blog . Please refer to dlib official site for Windows instructions . When it comes to learning computer vision ( or most advanced science techniques ) , I would encourage you to use a Unix-based system . <p> I wish I could offer Windows support , but there are honestly too many logistical issues . I offer almost 200 free tutorials here on @ @ @ @ @ @ @ @ @ @ If you do n't  want to purchase any of the teaching products here on PyImageSearch , you do n't  have to no one is asking you to or forcing you to . Enjoy the content and I hope it helps you on your computer vision journey . But please keep in mind that I am just one person and there are limitations to what I can provide . I 've found that Unix-based environments are the most useful when building computer vision applications ( the exception being if you want to build a Windows app , of course ) . If you have a Windows-specific question , I am not the right person to ask . <p> Hello , why do n't  you work on a virtual machine ? I am doing so on my Mac , all the Python stuff works much better on a Linux 16.04 LTS VM on VirtualBox . Both products are free , you can get VirtualBox from https : **35;10354;TOOLONG . Just install it on your Windows system , create or download ( http : **25;10391;TOOLONG ) the Ubuntu VM and than follow the directions @ @ @ @ @ @ @ @ @ @ , its a breeze . <p> The dlib library ships with a facial landmark detector ? Are you asking me to demonstrate how to train that detector from scratch ? Keep in mind that most of the dlib functionality does n't  have Python bindings , only C++ . <p> I had installed dlib a few days back . I am working in a virtual environment using Anaconda and my experience was that there were a few incompatibilities between the boost in anaconda repository and dlib from pip . <p> After toiling for a few days , was able to get dlib working with Anaconda . Had to setup dlib and boost from source . <p> So if any one is facing difficulty in working with Anaconda and dlib , I might be able to help . <p> 99.9% of the time you 'll see a segmentation fault error when you try to import dlib into a different version of Python than it was compiled against . I would suggest compiling dlib from source rather than using pip . <p> The install is likely not " stuck " . It can take @ @ @ @ @ @ @ @ @ @ On a Raspberry Pi this number can jump to an hour . If you run top you 'll see that your system is busy compiling dlib . <p> Hello Adrian , I followed this tutorial to install dlib in python . Everything was going smoothly until the time of installing dlib . When i ran the command " pip install dlib " it gives two error " Failed building wheel for dlib " and " error : cmake configuration failed ! " . What could be the reason behind this ? Thanks in advance . <p> Ive been struggling with this for a couple of days trying to play catchup with your emails . I 'd love to know your config which allows it to install in 10 minutes . I 'm sure its to do with swap size or zRAM ( I havent tried this ) <p> Today I increased the size of the dphys-swapfile to 1Gb and moved it ( temporarily ) onto a USB hard drive . The Raspian default is 100Mb on the SD card ! Of course a 100Mb swap file on an SD card is going to @ @ @ @ @ @ @ @ @ @ takes place . <p> Anyway , whilst running the pip install dlib command I also fired off top swap used peaked at 500Mb so far no wonder it hung with the default swap setting . I noted that my Pi is a lot more responsive to user input during the install having done this ( previously I had to pull the plug to get it back ) . Yay , the install has just this second completed without error and probably in less than 20 minutes ( I did n't  time it exactly ) - now to follow your blog . <p> Having done that I 'll now restore the original swapfile and reboot . <p> For others with similar hanging issues this is what I did ( after a few hours of web searching ) . <p> Hi Brian Im actually detailing an entire blog post dedicated to installing dlib on the Raspberry Pi . Its already written and set to publish on Monday , May 1st at 10AM EST . It makes use of swap settings similar to yours . You actually do n't  even need the external drive @ @ @ @ @ @ @ @ @ @ to implement the paper " Max-Margin Object Detection " based on Structured SVM by Python , which is written by Davis King ? Can you give a example in your blog ? Maybe , it is a good topic for your course ! <p> Appreciate your blog on installing the dlib library . I do wish to ask you of an error as when I am installing the dlib it gets stuck while compiling the trainer.cpp as a result the installation of the dlib freezes . <p> I read on stackoverflow that we have to manually compile the cpp examples ? I am running on py2.7.9 virtual env . Any suggestions would be helpful . <p> I installed dlib in Ubuntu 16.04 following this guide and it works perfectly . Thanks for that ! My problem is that face detection is a really slow process and I want to speed-it up by activating the support for AVX extensions but I do n't  know how to modify the cmake in order to recompile it as initially I just did it through pip ( pip install dlib ) . <p> As far @ @ @ @ @ @ @ @ @ @ from source rather than a pip install . Ill be covering how to speedup the dlib library for face detection and facial landmarks in a future blog post . <p> thanks for your great tutorials . I followed this one on my macbook on Sierra , and succeeded in compiling without errors . However , when I import dlib in python3.6 , I get an error : illegal instruction : 4 . Could it be my processor is not up yo this ? <p> Hi Arpit thanks for the comment ; however , I do not support Windows here on the PyImageSearch blog . I highly recommend that you a Unix-based environment such as Linux or @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485237 @185237/ <p> Otherwise , you should upgrade to the latest version ( 0.3.6- at the time of this writing ) so you have the updated orderpoints function : <p> Measuring distance between objects in an image with OpenCV <p> Shell <p> 1 <p> $pip install--upgrade imutils <p> Lines 14-19- parse our command line arguments . We need two switches here : --image- , which is the path to the input image containing the objects we want to measure , and --width- , the width ( in inches ) of our reference object . <p> Next , we need to preprocess our image : <p> Measuring distance between objects in an <p> 41 <p> 42 <p> # load the image , convert it to grayscale , and blur it slightly <p> image=cv2.imread ( args " image " ) 55215 @qwx675215 <p> **26;10418;TOOLONG , ( 7,7 ) @ @ @ @ @ @ @ @ @ @ perform a dilation + erosion to <p> # close gaps in between object edges <p> **28;10446;TOOLONG <p> edged=cv2.dilate ( edged , None , iterations=1 ) <p> edged=cv2.erode ( edged , None , iterations=1 ) <p> # find contours in the edge map <p> LONG ... 55211 @qwx675211 <p> **36;10476;TOOLONG <p> # sort the contours from left-to-right and , then initialize the <p> # distance colors and reference object <p> ( cnts , ) **28;10514;TOOLONG <p> LONG ... <p> ( 255,0,255 ) ) <p> refObj=None <p> Lines 22-24 load our image from disk , convert it to grayscale , and then blur it using a Gaussian filter with a- 7 x 7 kernel . <p> Once our image has been blurred , we apply the Canny edge detector to detect edges in the image a dilation + erosion is then performed to close any gaps in the edge map ( Lines 28-30 ) . <p> A call to cv2.findContours- detects the outlines of the objects in the edge map ( Lines 33-35 ) while- Line 39 sorts our contours from left-to-right . Since we know that our US quarter ( i.e. @ @ @ @ @ @ @ @ @ @ object in the image , sorting the contours from left-to-right ensures that the contour corresponding to the reference object will always be the- first entry in the cnts- list . <p> We then initialize a list of colors- used to draw the distances along with the refObj- variable , which will store our bounding box , centroid , and pixels-per-metric value of the reference object . <p> Measuring distance between objects in an <p> 62 <p> 63 <p> # loop over the contours individually <p> forcincnts : <p> # if the contour is not sufficiently large , ignore it <p> **27;10544;TOOLONG : <p> continue <p> # compute the rotated bounding box of the contour <p> box=cv2.minAreaRect(c) <p> LONG ... <p> box=np.array ( box , dtype= " int " ) <p> # order the points in the contour such that they appear <p> # in top-left , top-right , bottom-right @ @ @ @ @ @ @ @ @ @ outline of the rotated bounding <p> # box <p> **32;10573;TOOLONG <p> # compute the center of the bounding box <p> cX=np.average ( box : , 0 ) <p> cY=np.average ( box : , 1 ) <p> On- Line 45 we start looping over each of the contours in the cnts- list . If the contour is not sufficiently large ( Lines 47 and 48 ) , we ignore it . <p> Otherwise , - Lines 51-53 compute the rotated bounding box of the current object ( using cv2.cv.BoxPoints- for OpenCV 2.4 and cv2.boxPoints- for OpenCV 3 ) . <p> A call to orderpoints- on- Line 59 rearranges the bounding box- ( x , y ) -coordinates in top-left , top-right , bottom-right , and bottom-left order , which as well see , is important when we go to compute the distance between object corners . <p> Lines 62 and 63 compute the center- ( x , y ) -coordinates of the rotated bounding box by taking the average of the bounding box in both the- x and- y direction . <p> The next step is to calibrate our refObj- : @ @ @ @ @ @ @ <p> 80 <p> 81 <p> # if this is the first contour we are examining ( i.e. , <p> # the left-most contour ) , we presume this is the <p> # reference object <p> ifrefObj isNone : <p> # unpack the ordered bounding box , then compute the <p> # midpoint between the top-left and top-right points , <p> # followed by the midpoint between the top-right and <p> # bottom-right <p> ( tl , tr , br , bl ) =box <p> ( tlblX , tlblY ) =midpoint ( tl , bl ) <p> ( trbrX , trbrY ) =midpoint ( tr , br ) <p> # compute the Euclidean distance between the midpoints , <p> # then construct the reference object <p> D=dist.euclidean ( ( tlblX , tlblY ) , ( trbrX , trbrY ) ) <p> refObj= ( box , ( cX , cY ) , D/args " width " ) <p> continue <p> @ @ @ @ @ @ @ @ @ @ then we need to initialize it . <p> We start by unpacking the ( ordered ) rotated bounding box coordinates and computing the midpoint between the top-left and bottom-left along with top-right and bottom-right points , respectively ( Lines 73-75 ) . <p> From there , we compute the Euclidean distance between the points , giving us our " pixels-per-metric " , allowing us to determine how many pixels fit into --width- inches . <p> Note : See last weeks post for a more detailed discussion of the " pixels-per-metric " variable . <p> The pixels-per-metric ratio that well be using to determine the distance between objects . <p> Our next code block handles drawing the contours around our reference object and the object we are currently examining , followed by constructing refCoords- and objCoords- such that ( 1 ) the bounding box coordinates and ( 2 ) the- ( x , y ) -coordinates of the of the centroid are included in the same arrays : <p> Measuring distance between objects in an image with OpenCV <p> Python <p> 83 <p> 84 <p> 85 <p> 86 <p> 87 <p> @ @ @ @ @ @ @ @ @ @ the contours on the image <p> orig=image.copy() <p> LONG ... <p> LONG ... <p> # stack the reference coordinates and the object coordinates <p> # to include the object center <p> **27;10607;TOOLONG , refObj1 ) <p> objCoords=np.vstack ( box , ( cX , cY ) ) <p> We are now ready to compute the distance between the respective corners and centroids of objects in our image : <p> We then draw a circle representing the- ( x , y ) -coordinates of the current points we are computing the distance between and draw a line to connect the points ( Lines 97-110 ) . <p> From there , - Line 105 computes the Euclidean distance between the reference location and the object location , followed by dividing the distance by the " pixels-per-metric " , giving us the final distance in inches between the two objects. - The computed distance is then drawn on our image ( Lines 106-108 ) . <p> Note : This distance computation is performed for- each of the top-left , top-right , bottom-right , bottom-left , and centroid coordinates for a total of- five distance @ @ @ @ @ @ @ @ @ @ display the output image to our screen . <h> Distance measurement results <p> To give our distance measurement script a try , download the source code and corresponding images to this post using the " Downloads " form at the bottom of this tutorial . Unarchive the . zip- file , change directory to the distancebetween.py- script , and then execute the following command : <p> Measuring distance between objects in an image with OpenCV <p> Shell <p> 1 <p> $python **25;10636;TOOLONG **32;10663;TOOLONG <p> Below follows a GIF animation demonstrating the output of our script : <p> Figure 2 : Computing the distance between objects in an image with OpenCV . <p> In each of these cases , our script matches the top-left- ( red ) , - top-right- ( purple ) , bottom-right- ( orange ) , - bottom-left- ( teal ) , and centroid- ( pink ) coordinates , followed by computing the distance ( in inches ) between the reference object and the current object . <p> Notice how the two quarters in the image are perfectly parallel to each other , implying that the distance between @ @ @ @ @ @ @ @ @ @ follows a second example , this time computing the distance between our reference object and a set of pills : <p> Measuring distance between objects in an image with OpenCV <p> Shell <p> 1 <p> $python **25;10697;TOOLONG **32;10724;TOOLONG <p> Figure 3 : Computing the distance between pills using OpenCV . <p> This example could be used as input to a pill sorting robot that automatically takes a set of pills and organizes them according to their size and distance from a pill container . <p> Our last example computes the distance between our reference object ( a 3.5in x 2in business card ) and a set of 7 ? vinyl records and an envelope : <p> Measuring distance between objects in an image with OpenCV <p> Shell <p> 1 <p> $python **25;10758;TOOLONG **30;10785;TOOLONG <p> Figure 4 : A final example of computing the distance between objects using OpenCV and computer vision . <p> As you can see , in each of these cases , we have successfully computed the distance ( in actual , measurable units ) between objects in an image . <h> Summary <p> In the third and @ @ @ @ @ @ @ @ @ @ we learned how to take two different objects in an image and compute the distance between them in- actual measurable units ( such as inches , millimeters , etc . ) . <p> Just as we found out in last weeks post , before we can ( 1 ) compute the size of an object or ( 2 ) measure the distance between two objects , we first need to compute the " pixels-per-metric " ratio , used to determine how many pixels " fit " into a given unit of measurement . <p> Once we have this ratio , computing the distance between objects is almost trivially easy . <p> Anyway , I hope you enjoyed this series of blog posts ! If you have any suggestions for a future series , please leave a comment on shoot me a message . <p> And before you go , - be sure to signup for the PyImageSearch Newsletter by entering your email address in the form below ! <h> Downloads : 55217 @qwx675217 <p> Can you elaborate on what you mean by " continuously varying " ? If you have @ @ @ @ @ @ @ @ @ @ you are looking at . This can be done using all sorts of methods including object detection , keypoint matching , template matching , etc . <p> In your tutorial about Ball Tracking with opencv , when we move the ball back and forth in front of camera , the size of the contour varies accordingly , So taking this as reference , how to find distance between two balls ? ? I have to detect the distance between the two head lights of a car and distance from the camera when car is moving . <p> You 'll need to combine two calibration methods . The first ( this blog post ) to measure the distance between objects in images . Then , you can use this blog post to measure the distance from the camera to the object . <p> That line of code handles if you are using OpenCV 2.4 or OpenCV 3 . There is a difference in the tuple returned by OpenCV between OpenCV 2.4 and OpenCV 3 . You can read more about the change to cv2.findContoursin this blog post . <p> The same general @ @ @ @ @ @ @ @ @ @ applied . You first need to " calibrate " the camera . From there , determining the distance between two objects in a video stream is the same as determining the distance between two objects in an image you just need to access the video stream and then process every frame of the stream . <p> Dear Dr. Adrian ; Thanks in advance for your reply I do it . Now i can save all result in a single image , but i have a problem : When i delet line 111- cv2.imshow ( " Image " , orig ) when i want to stop show result on the screen , for loop not work perfectly ! ! Just work for 3 object ( ( it save result just for 2 or 3 object ) ) not for all object Please if you can help me ? How i can cancel line 111 and run program with out error ? ? <p> You can certainly do that as well . The important part is that you find your reference object first . Your reference object allows you to compute the @ @ @ @ @ @ @ @ @ @ have this , you can compute the distance between two non-reference objects . <p> Yes , it absolutely is . In order to correct for this its common to calibrate our camera by computing the intrinsic properties of the camera . This allows us to help us correct for distortion . For these measurements to be correct you should have an ( approximately ) 90 degree , birds-eye-view of the objects . <p> I would suggest starting with this blog post to learn how to measure the distance between a camera and an object . Once you 've detected both objects you can apply the same triangle similarity technique to measure the distance between the objects . <p> The same techniques applied to single images can also be applied to video . I would suggest reading this blog post on accessing video streams . If you are new to OpenCV and Python , be sure to read through Practical Python and OpenCV as this book will give you the fundamentals you need to be successful porting the algorithm from single images to video . <p> Hi Adrian ! ! ! ! @ @ @ @ @ @ @ @ @ @ I wanted to know if i have a set of parallel lines , then can i get the distance between two consecutive lines instead of with reference to just the first one ? ? ? <p> Hey , I was working on a project which required me to measure the diameter of holes and length and width of a plate using the image of the plate and a rasp pi cam and ofc rasp pi . I was wondering if this has the ability to be accurate to .001 in @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485238 @185238/ <h> Tag Archives linear svm <p> Todays blog post is inspired from an email I received from Jason , a student at the University of Rochester . Jason is interested in building a custom object detector using the HOG + Linear SVM framework for his final year project . He understands the steps required to build the object detector well enough - but he is n't <p> Over the past few weeks , we 've started to learn more and more about machine learning and the role it plays in- computer vision , - image classification , and- deep learning . Weve seen how Convolutional Neural Networks ( CNNs ) such as LetNet can be used to classify handwritten digits from the MNIST dataset . We 've applied the k-NN algorithm to classify whether or <p> Well . Ill just come right out and say it. - Today is my 27th birthday . As a kid I was always- super excited about my birthday . It was another year closer to being able to drive a car . Go to R rated movies . Or buy alcohol . But now as an @ @ @ @ @ @ @ @ @ @ <p> Last week we discussed how to use OpenCV and Python to perform pedestrian detection . To accomplish this , we leveraged the built-in HOG + Linear SVM detector that OpenCV ships with , allowing us to detect people in images . However , one aspect of the HOG person detector we- did not discuss in detail is the detectMultiScale- function ; specifically , <p> I 've met a lot of amazing , uplifting- people over the years . My PhD advisor who helped get me through graduate school . My father who was always there for me as a kid and still is now . And- my girlfriend who has always been positive , helpful , and supportive ( even when I probably did n't  deserve it ) . I 've also <p> So in last weeks blog post we discovered how to construct an image pyramid . And in todays article we are going to extend that example and introduce the concept of a sliding window . Sliding windows play an integral role in object classification , as they allow us to localize exactly- " where " in an image @ @ @ @ @ @ @ @ @ @ cant stop thinking about object detection . You see , last night I was watching The Walking Dead- and instead of enjoying the zombie brutality , the forced- cannibalism , or the enthralling storyline , - all I wanted to do was build an object detection system to recognize zombies . Would it be very useful ? Probably not . I mean , its <p> Did you watch the Super Bowl this past weekend ? I did . Kind of . I spent Super Bowl Sunday ( which is practically a holiday in the United States ) at my favorite Indian bar . Pounding Kingfisher beers . Savoring a delicious dish of- Tandoori chicken all while hacking up a storm on my laptop and coding up some custom <p> Connecticut is cold . Very cold . Sometimes its hard to even get out of bed in the morning . And honestly , without the aide of copious amounts of pumpkin spice lattes and the beautiful sunrise over the crisp autumn leaves , I do n't  think I would leave my cozy bed . But I have work to do . @ @ @ @ @ @ @ @ @ @ Twitter account lately , youve probably noticed one or two teasers of what I 've been working on a Python framework/package to rapidly construct object detectors using Histogram of Oriented Gradients and Linear Support Vector Machines . Honestly , I really ca n't stand using the Haar @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485239 @185239/ <h> Archive Libraries <p> Todays blog post is part three in our current series on facial landmark detection and their applications to computer vision and image processing . Two weeks ago I demonstrated how to install the dlib library- which we are using for facial landmark detection . Then , last week I discussed how to use dlib to actually- detect facial landmarks in <p> Last week we learned how to install and configure dlib- on our system with Python bindings . Today we are going to use dlib and OpenCV to detect- facial landmarks in an image . Facial landmarks are used to localize and represent salient regions of the face , such as : Eyes Eyebrows Nose Mouth Jawline Facial landmarks have been successfully <p> Two weeks ago I interviewed Davis King , the creator and chief maintainer of the dlib library . Today I am going to demonstrate how to install dlib with Python bindings on both- macOS and- Ubuntu . I- highly encourage you to take the time to install dlib on your system well @ @ @ @ @ @ @ @ @ @ vision literature is- Seam Carving for Content-Aware Image Resizing by Avidan and Shamir from Mitsubishi Electric Research Labs ( MERL ) . Originally published in the SIGGRAPH 2007 proceedings , I read this paper for the first time during my computational photography class as an undergraduate student . This paper , along with <p> A few months ago I demonstrated how to install the Keras deep learning library with a- Theano backend . In todays blog post I provide detailed , step-by-step instructions to install Keras using a TensorFlow backend , originally developed by the researchers and engineers on the Google Brain Team . Ill also ( optionally ) demonstrate how you can integrate OpenCV into <p> The purpose of this blog post is to demonstrate how to install the Keras library for deep learning . The installation procedure will show how to install Keras : With GPU support , so you can leverage your GPU , CUDA Toolkit , cuDNN , etc. , for faster network training . Without- GPU support , so even if you do not have a GPU <p> So you 're interested in deep learning and @ @ @ @ @ @ @ @ @ @ Which library do you use ? There are just so many ! Inside this blog post , I detail 9 of my- favorite Python deep learning libraries . This list is- by no means- exhaustive , its- simply a list of libraries that Ive used in my computer vision <p> Over the past two weeks on the PyImageSearch blog , we have discussed how to use threading to increase- our FPS processing rate on- both built-in/USB webcams , along with the Raspberry Pi camera module . By utilizing threading , - we learned that we can substantially reduce the affects of I/O latency , leaving the main thread to run without being blocked as <p> Today is the second post in our three part series on milking- every last bit of performance out of your webcam- or Raspberry Pi camera . Last week we discussed how to : Increase the FPS rate of our video processing pipeline . Reduce the affects of I/O latency on standard USB and built-in webcams using threading . This week well- continue <p> Over the next few weeks , I 'll be doing a series of @ @ @ @ @ @ @ @ @ @ ( FPS ) from your webcam using Python , OpenCV , and threading . Using threading to handle I/O-heavy tasks ( such as reading frames from a camera sensor ) is a programming model that has existed for @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485240 @185240/ <h> Tag Archives threading <p> I said- it in last weeks blog post- and I 'll say it again here today - I am not , by any stretch of the imagination , a GUI developer . I think my aversion to GUI development started back in early-High school when I was teaching myself Java ; specifically , how to write Java applets ( remember what god damn nightmares applets <p> I 'll keep the introduction to todays post short , since I think the title of this post and GIF animation above speak for themselves . Inside this post , I 'll demonstrate how to attach- multiple cameras to your Raspberry Piand access all of them using a single Python script . Regardless if- your setup includes : Multiple USB webcams . Or the Raspberry <p> Over the past two weeks on the PyImageSearch blog , we have discussed how to use threading to increase- our FPS processing rate on- both built-in/USB webcams , along with the Raspberry Pi camera module . By utilizing threading , - we learned that we can substantially reduce the affects of I/O latency , leaving @ @ @ @ @ @ @ @ @ @ Today is the second post in our three part series on milking- every last bit of performance out of your webcam- or Raspberry Pi camera . Last week we discussed how to : Increase the FPS rate of our video processing pipeline . Reduce the affects of I/O latency on standard USB and built-in webcams using threading . This week well- continue <p> Over the next few weeks , I 'll be doing a series of blog posts on how to improve your frames per second ( FPS ) from your webcam using Python , OpenCV , and threading . Using threading to handle I/O-heavy tasks ( such as reading frames from a camera sensor ) is a programming model that has existed for @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485241 @185241/ <h> Generating art with guided deep dreaming . <p> One of the main benefits of the bat-country Python package for deep dreaming and visualization is its ease of use , extensibility , and customization . <p> And let me tell you , that customization really came in handy last Friday- when the Google Research team- released an update to their deep dream work , demonstrating a method to " guide " your input images to visualize the features of a target image . <p> The results were quite impressive so I decided to port the functionality to bat-country . <p> Truth be told , it only took 20 minutes from start-to-finish to get the code together . I honestly spent more time running the- Python scripts to gather example images and updating the documentation than I did updating the codebase . <p> The secret to this quick turnaround is the extensibility of the BatCountry- class where nearly every function and every method can be overridden and extended . <p> Want to change how each image is pre-processed or post-processed ? No problem . Define your own custom processor and @ @ @ @ @ @ @ @ @ @ ? Again , just define your own objective and you 're good to go . <p> In fact , defining your own custom objective function is the the exact route I took when extending- bat-country- . I simply defined a new objective function , allowing- the step function to be further customized , and were done ! <p> In the remainder of this blog post well play around with the new bat-country update to perform guided dreaming - and even use it to generate our own art using guided deep dreaming ! <h> Guided deep dreaming <p> Last Friday the Google Research team- posted an update to their deep dream working demonstrating it was possible to guide your dreaming process by supplying a seed image . This method passes your input image through the network in a similar manner , but this time using your seed image to guide and influence the output . <p> If you do n't  already have bat-country- installed , either pull down the code from the GitHub repo- or use pip to install it on your system : - pip install bat-country- or- pip install--upgrade bat-country- @ @ @ @ @ @ @ @ @ @ <p> What 's nice about this approach is that we can " guide " what the output image looks like . Here I use a seed image of Vincent van Goghs Starry Night and apply to an image of clouds : <p> Figure 1 : An example of applying guided dreaming using Starry Night and a cloud image . <p> As you can see , the output cloud image after applying guided dreaming appears to mimic many of the brush strokes of Van Goghs painting . <p> Which got me thinking what would happen if I took some well-known- paintings from extremely famous artists such as Andy Warhol , MC Escher , Pablo Picasso , Jackson Pollock , and Vincent van Gogh and used them as inputs and guides to- each other ? <p> What would the results look like ? Would the artistic style of each painting- be transferred to the other ? <p> In order to test this out , I collected images of the following pieces of work : <h> Andy Warhol Marilyn Monroe <h> Summary <p> In this blog post I reviewed the updates to the bat-country @ @ @ @ @ @ @ @ @ @ images : an input image that will be passed through the network , and a seed image that the network will use to " guide " the output . <p> I then took the updated code and used it to generate art from famous works , such as Jackson Pollocks Energy Made Visible , Pablo Picassos Guernica , and Vincent van Goghs Wheat Field with Cypresses . <p> Definitely consider installing the bat-country- package on your system and giving deep dreaming a try ! Its strangely addictive ( and not to mention , a lot of fun ) to play around and generate your own images . <p> Finally , if you 're interested in deep learning , deep dreaming , and computer vision , think about signing up for the PyImageSearch Newsletter by entering your email address in the form below . I send out regular updates on the PyImageSearch blog , each filled with actionable , real-world computer vision projects . <h> 9 Responses to Generating art with guided deep dreaming . <p> I believe that they are using neurons as if they 're dopped with LSD , seriously . @ @ @ @ @ @ @ @ @ @ as if they where on LSD . I could be wrong ; though , I thought I @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485242 @185242/ <h> Watermarking images with OpenCV and Python <p> A few weeks ago , I wrote a blog post on creating transparent overlays with OpenCV . This post was meant to be a gentle introduction to a neat little trick you can use to improve the aesthetics of- your processed image(s) , such as creating a Heads-up Display ( HUD ) on live video streams . <p> But there 's another , more practical reason that I wanted to introduce transparent overlays to you - watermarking images. - Watermarking an image or video is called digital watermarking , and is the process of embedding a- unique and- identifying pattern onto the image itself . <p> For example , professional photographers tend to watermark digital proofs sent to clients ( including relevant information such as their name and/or design studio ) until the client agrees to purchase the photos , at which the original , unaltered images are released . This allows the photographer to distribute demos and samples of their work , without actually " giving away " the original compositions . <p> We also see digital watermarks in copyrighted video @ @ @ @ @ @ @ @ @ @ frame of the video , thereby crediting the original producer- of the work . <p> In both of these cases , the goal of watermarking is to create a unique and identifiable pattern on the image , giving attribution to the original creator , but- without destroying the contents of the image itself . <p> To learn how to utilize OpenCV to watermark your own dataset of images , keep reading . <h> Watermarking images with OpenCV and Python <p> The goal of this blog post is to demonstrate- how to add watermarks to images using OpenCV and Python . To get started , well need a watermark , which for the purposes of this tutorial , Ive chosen to be the PyImageSearch logo : <p> Figure 1 : Our example watermark image the PyImageSearch logo . <p> This watermark is a PNG image- with- four channels : a Red channel , a Green channel , a Blue channel , and an- Alpha channel used to control the- transparency of each of the pixels in the image . <p> Values in our alpha channel can range- 0 , 255 , @ @ @ @ @ @ @ @ @ @ , not transparent at all ) while a value of- 0 is- 100% transparent . Values that fall between 0 and 255 have varying levels of transparency , where the smaller the alpha value , - the more transparent the pixel is . <p> In the above figure , all pixels that- are not part of the- white " PyImageSearch " logo are fully transparent , meaning that you can " see through them " to the background of what the image is laid on top of . In this case , I 've set the image to have a- blue background so we can visualize the logo itself ( obviously , you would not be able to see the white PyImageSearch logo if I placed it on a white background hence using a blue background for this example ) . <p> Once we actually overlay the watermark on our image , the watermark will be semi-transparent , allowing us to ( partially ) see the background of the original image . <p> Now that we understand the process of watermarking , let 's go ahead and get started . <h> Creating @ @ @ @ @ @ @ @ @ @ , name it watermarkdataset.py- , and let 's get started : <p> Watermarking 18 <p> 19 <p> 20 55203 @qwx675203 <p> fromimutils importpaths 55220 @qwx675220 55218 @qwx675218 <p> importcv2 <p> importos 55202 @qwx675202 55206 @qwx675206 <p> LONG ... <p> help= " path to watermark image ( assumed to be transparent PNG ) " ) <p> ap.addargument ( " -i " , " --input " , required=True , <p> help= " path to the input directory of images " ) <p> LONG ... <p> help= " path to the output directory " ) <p> LONG ... <p> help= " alpha transparency of the overlay ( smaller is more transparent ) " ) <p> LONG ... <p> help= " flag used to handle if bug is displayed or not " ) 55208 @qwx675208 <p> Lines 2-6 import our required Python packages . Well be making use of the imutils package here @ @ @ @ @ @ @ @ @ @ , let pip- install it for you : <p> Watermarking images with OpenCV and Python <p> Shell <p> 1 55204 @qwx675204 <p> Lines 9-20 then handle parsing our required command line arguments . We require three command line arguments and can supply two additional ( optional ) ones . A full breakdown of each of the command line arguments can be found below : <p> --watermark- : Here we supply the path to the image we wish to use as the watermark . We presume that ( 1 ) this image is a- PNG image with- alpha transparency and ( 2 ) our watermark is smaller ( in terms of both width and height ) then- all images in the dataset we are going to apply the watermark to . <p> --input- : This is the path to our input directory of images we are going to watermark . <p> --alpha- : The optional --alpha- value controls the level of transparency of the watermark . A value of- 1.0 indicates that the watermark should be 100% opaque ( i.e. , not transparent ) . A value of- 0.0 indicates that @ @ @ @ @ @ @ @ @ @ to tune this value for your own datasets , but I 've found that a value of 25% works well for most circumstances . <p> --correct- : Finally , this switch is used to control whether or not we should preserve a " bug " in how OpenCV handles alpha transparency . The only reason I 've included this switch is for a matter of education regarding the OpenCV library . Unless you want to investigate this bug yourself , you 'll likely be leaving this parameter alone . <p> Now that we have parsed our command line arguments , we can load our watermark image from disk : <p> Watermarking images with OpenCV and Python <p> Python <p> 22 <p> 23 <p> 24 <p> 25 <p> # load the watermark image , making sure we retain the 4th channel <p> # which contains the alpha transparency <p> LONG ... <p> ( wH , wW ) =watermark.shape:2 <p> Line 24 loads our watermark- image from disk using the cv2.imread- function . Notice how we are using the cv2.IMREADUNCHANGED- flag this value is supplied so we can read the alpha transparency channel of the @ @ @ @ @ @ @ @ @ @ , and Blue channels ) . <p> Line 25 then grabs- the spatial dimensions ( i.e. , height and width ) of the watermark- image . <p> The next code block addresses some strange issues Ive encountered when working with alpha transparency and OpenCV : <p> Watermarking images with <p> 36 <p> 37 <p> # split the watermark into its respective Blue , Green , Red , and <p> # Alpha channels ; then take the bitwise AND between all channels <p> # and the Alpha channels to construct the actaul watermark <p> # NOTE : I 'm not sure why we have to do this , but if we do n't , <p> # pixels are marked as opaque when they should n't be <p> ifargs " correct " &gt;0 : <p> ( B , G , R , A ) =cv2.split(watermark) <p> B=cv2.bitwiseand ( B , B , mask=A ) <p> G=cv2.bitwiseand ( G , G , mask=A ) <p> R=cv2.bitwiseand ( R , R @ @ @ @ @ @ @ @ @ @ R , A ) <p> When I first implemented this example , I noticed some- extremely strange behavior on the part of cv2.imread- and PNG filetypes with alpha transparency . <p> To start , I noticed that- even with the cv2.IMREADUNCHANGED- flag , the transparency values in the alpha channel- were not respected by- any of the Red , Green , or Blue channels these channels would appear to be either- fully opaque or- semi-transparent , but never the- correct level of transparency that I presumed they would be . <p> However , upon investigating the alpha channel itself , I noticed there were no problems with the alpha channel directly - the alpha channel was loaded and represented perfectly . <p> Therefore , to ensure that each of the Red , Green , and Blue channels respected the alpha channel , I took the bitwise AND- between the individual color channels and the alpha channel , treating the alpha channel as a mask ( Lines 33-37 ) - this resolved the strange behavior and allowed me to proceed with the watermarking process . <p> I 've included the --correct- flag @ @ @ @ @ @ @ @ @ @ do not apply this type of correction ( more on in this the- " Watermarking results " section ) . <p> Next , let 's go ahead and process our dataset of images : <p> Watermarking images with <p> 60 <p> 61 <p> # loop over the input images <p> forimagePath inpaths.listimages ( args " input " ) : <p> # load the input image , then add an extra dimension to the <p> # image ( i.e. , the alpha transparency ) <p> **27;10817;TOOLONG <p> ( h , w ) =image.shape:2 <p> LONG ... <p> # construct an overlay that is the same size as the input <p> # image , ( using an extra dimension for the alpha transparency ) , <p> # then add the watermark to the overlay in the bottom-right <p> # corner <p> overlay=np.zeros ( ( h , @ @ @ @ @ @ @ @ @ @ <p> overlayh-wH-10:h-10 , w-wW-10:w-10=watermark <p> # blend the two images together using transparent overlays <p> output=image.copy() <p> LONG ... <p> # write the output image to disk <p> LONG ... <p> p=os.path.sep.join ( ( args " output " , filename ) ) <p> cv2.imwrite ( p , output ) <p> On- Line 40 we start looping over each of the images in our --input- directory . For each of these images , we load it from disk and grab its width and height . <p> Its important to understand that each image- is represented as a NumPy array with shape- ( h , w , 3 ) , where the- 3 is the number of channels in our image one for each of the Red , Green , and Blue channels , respectively . <p> However , since we are working with alpha transparency , we need to add a- 4th dimension to the image to store the alpha values ( Line 45 ) . This alpha channel has the same spatial dimensions as our original image and all values in the alpha channel are set to- 255 , indicating @ @ @ @ @ @ @ @ @ @ <p> Lines 51 and 52 construct the overlay- for our watermark . Again , the overlay- has the exact same width and height of our input image . <p> Lines 59-61 then take our output- image and write it to the --output- directory . <h> Watermarking results <p> To give our watermarkdataset.py- script a try , download the source code and images associated with this post- using the- " Downloads " form at the bottom of this tutorial . Then , navigate to the code directory and execute the following command : <p> Watermarking images with OpenCV and Python <p> Shell <p> 1 <p> 2 <p> $python **30;10846;TOOLONG **26;10878;TOOLONG <p> --input input--output output <p> After the script finishes executing , your output- directory should contain the following five images : <p> Figure 2 : The output from our watermarking script . <p> You can see each of the watermarked images below : <p> Figure 3 : Watermarking images with OpenCV and Python . <p> In the above image , you can see the white PyImageSearch logo has been added as a- watermark to the original image . <p> Below follows @ @ @ @ @ @ @ @ @ @ Again , notice how the PyImageSearch logo appears ( 1 ) semi-transparent and ( 2 ) in the bottom-right corner of the image : <p> Figure 4 : Creating watermarks with OpenCV and Python . <p> About a year ago , I went out to Arizona to enjoy the Red Rocks . Along the way , I stopped at the Phoenix Zoo to pet and feed a giraffe : <p> I 'm also a- huge fan of mid-century modern architecture , so I had to visit Taliesin West : <p> Figure 6 : Watermarking images with OpenCV . <p> Finally , here 's a beautiful photo of the Arizona landscape ( even if it was a bit cloudy that day ) : <p> Figure 7 : Creating watermarks with OpenCV is easy ! <p> Notice how in each of the above images , the " PyImageSearch " logo has been placed in the bottom-right corner of the output image . Furthermore , this watermark is- semi-transparent , allowing us to see the contents of the background image through the foreground watermark . <h> Strange behavior with alpha transparency <p> So , remember @ @ @ @ @ @ @ @ @ @ with alpha transparency can happen if we do n't  take the bitwise AND- between each respective Red , Green , and Blue channel and the alpha channel ? <p> Let 's take a look at this- strangeness . <p> Execute the watermarkdataset.py- script again , this time supplying the --correct0- flag to skip the bitwise AND- step : <p> Then , opening an output image of your choosing , you 'll see something like this : <p> Figure 8 : ( Incorrectly ) creating a watermark with OpenCV . <p> Notice how the- entire watermark image is treated as being semi-transparent- instead of only the corresponding alpha pixel- values ! <p> Baffling , right ? <p> I 'm honestly not sure why this happens and I could n't find any information on the behavior in the OpenCV documentation . If anyone has any extra details on this issue , please leave a note in the comments section- at the bottom of this post . <p> Otherwise , if you utilize alpha transparency in any of your own OpenCV image processing pipelines , make sure you take special care to mask each Red , Green @ @ @ @ @ @ @ @ @ @ <h> Summary <p> In this blog post we learned how to watermark an image dataset using OpenCV and Python . Using digital watermarks , you can overlay your own name , logo , or company brand overtop your original work , thereby protecting the content and attributing yourself as the original creator . <p> In order- to create these digital watermarks with OpenCV , we leveraged PNG with alpha transparency . Utilizing alpha transparency can be quite tricky , - especially since it appears that OpenCV does not automatically mask transparent pixels for each channel . <p> Instead , you 'll need to- manually perform this masking yourself by taking the bitwise AND- between the input channel and the alpha mask ( as demonstrated in this blog post ) . <p> Anyway , I hope you enjoyed this tutorial ! <p> And before you go , be sure to enter your email address in the form below to be notified when new blog posts are published ! <h> Downloads : 55217 @qwx675217 <h> 13 Responses to Watermarking images with OpenCV and Python <p> I have a problem with cv2.bitwiseand nothing show @ @ @ @ @ @ @ @ @ @ ( B , A , mask=A ) G .. now this work like your example . Did you know why ? , I do n't  understand what 's happend. cv2 only show a black image when I use **25;10906;TOOLONG to view it . <p> Found my error , I use a watermark whit transparent background and black text . Only change text to white color and all works fine . Thanks for your blogs . <p> Hello , first of all thank you so much for this great tutorial , your blog/website is a lifesaver . <p> I am working on a project where I need to overlay an image inside a circle , the circle does not have a fixed size and position , it is always moving . <p> My question is , how can I overlay the image ( watermark ) inside the moving circle ? I am clueless as to how dictate the position of where the watermark is to be placed . How can I specify the ( x , y ) coordinates ? <p> If you 're just getting started learning about OpenCV and Python , @ @ @ @ @ @ @ @ @ @ and OpenCV to help you build strong fundamentals . Inside the book I detail how to specify the ( x , y ) -coordinates of an image . In this case , its handled on Line 52 via NumPy array slicing . <p> I made a watermark that is white with a black border in an attempt to make it visible on any background . When I apply the watermark , any black parts are completely transparent/invisible . Is there a way to avoid this ? <p> Hello Adrian , I cant say how helpfull your tutorials and work are to me . A big THANK YOU to you and your team . My question is : could it be possible to watermark a camera capture with a video stream ? Could it be possible to watermark with a second video capture ? What would be the prerequisite apart from the 2 cameras ? Thank you for your @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485244 @185244/ <h> ( Faster ) Non-Maximum Suppression in Python <p> You see , last night I was watching The Walking Dead- and instead of enjoying the zombie brutality , the forced- cannibalism , or the enthralling storyline , - all I wanted to do was build an object detection system to recognize zombies . <p> Would it be very useful ? Probably not . <p> I mean , its quite obvious if a zombie is coming after you . The stench alone should be a dead ( hey , look at that pun ) giveaway , let alone the gnashing of teeth and outstretched arms . And we might as well throw in guttural " brainnnnsss " moans as well . <p> Like I said , it would be pretty obvious if a zombie was after you you certainly would n't need a computer vision system to tell you that . But this is just an example of the type of stuff that runs through my head on a day-to-day basis . <p> To give you some context , two weeks ago I posted about using Histogram of Oriented Gradients and @ @ @ @ @ @ @ @ @ @ system . Sick of using the OpenCV Haar cascades and obtaining poor performance , and not to mention , extremely long training times , I took it upon myself to write my own Python object detection framework . <p> So far , its gone extremely well and its been a lot of fun to implement . <p> But there 's an- unescapable issue you must handle when building an object detection system - overlapping bounding boxes . Its going to happen , there 's no way around it . In fact , its actually a good sign that your object detector is firing properly so I would n't even call it an " issue " exactly . <p> To handle the removal overlapping bounding boxes ( that refer to the same object ) we can either use non-maximum suppression on the Mean-Shift algorithm . While Dalal and Triggs- prefer Mean-Shift , I find Mean-Shift to give sub-par results . <p> The implementation from last week required an extra inner for- - loop to compute the size of bounding regions and compute the ratio of overlapped area . <p> Instead , Dr. Malisiewicz replaced @ @ @ @ @ @ @ @ @ @ is how we are able to achieve substantially faster speeds when applying non-maximum suppression . <p> Instead of repeating myself and going line-by-line through the code like I did last week , let 's just examine the important parts . <p> Lines 6-22- of our faster non-maximum suppression function are essentially the same as they are from last week . We start by grabbing the- ( x , y ) coordinates of the bounding boxes , computing their area , and sorting the indexes into the boxes- - list according to their bottom-right y-coordinate of each box . <p> Instead of using an inner for- - loop to loop over each of the individual boxes , we instead vectorize the code using the np.maximum- - and np.minimum- - functions this allows us to find the maximum and minimum values across the- axis rather than just individual scalars . <p> Note : You- have- to use the np.maximum- - and np.minimum- - functions here they allow you to mix scalars and vectors . The np.max- - and np.min- - functions do not , and if you use them , you will find @ @ @ @ @ @ @ @ @ @ . It took me quite awhile to debug this problem when I was porting the algorithm from MATLAB to Python . <p> Lines 47 and 48 are also vectorized here we compute the width and height of each of the rectangles to check . Similarly , computing the overlap- - ratio on Line 51- is also vectorized . From there , we just delete all entries from our idx- - list that are greater than our supplied overlap threshold . Typical values for the overlap threshold normally fall in the range 0.3-0.5 . <p> The Malisiewicz et al . method is essentially identical to the- Felzenszwalb et al . method , but by using vectorized code we are able to reach a reported 100x speed-up in non-maximum suppression ! <h> Faster Non-Maximum Suppression in Action <p> Let 's go ahead and explore a few examples . Well start with the creepy little girl zombie that we saw at the top of this image : <p> Figure 1 : Detecting three bounding boxes in the image , but non-maximum suppression suppresses two of the overlapping boxes . <p> Its actually really funny @ @ @ @ @ @ @ @ @ @ human faces generalizes to zombie faces as well . Granted , they are still " human " faces , but with all the blood and disfigurement , I would n't be surprised to see some strange results . <p> Figure 2 : It looks like our face detector did n't  generalize as well the teeth of this zombie looks like a face to the detector . <p> Speaking of strange results , it looks like our face detector detected the mouth/teeth region of the zombie on the right . Perhaps if I had explicitly trained the HOG + Linear SVM face detector on zombie images the results would be better . <p> Figure 3:Six bounding boxes are detected around the face , but by applying fast non-maximum suppression we are correctly able to reduce the number of bounding boxes to one . <p> In this last example we can again see that our non-maximum suppression algorithm is working correctly even though six original bounding boxes were detected by the HOG + Linear SVM detector , applying non-maximum suppression has correctly suppressed five of the boxes , leaving us with the final @ @ @ @ @ @ @ @ @ @ I 'm not sure why that would happen , I have not encountered that problem before . It sounds like it might be a divide by zero error . Check the area array and see if there are any bounding boxes that somehow have zero area . <p> Hmm . I 've implemented your algorithm on top of a set of over 122k bounding boxes and I 'm getting some very strange results . <p> First , the more I raise the percentage , the less bounding boxes I end up with in the end . You would think the higher percentage lowers the supression and therefore would raise the amount of resulting bounding boxes but I 'm not finding this to be the case at all . <p> Second , the bounding boxes were generated using a few different algorithms and the results I 'm getting seem very small- for 122k bounding boxes , I 'm getting numbers in the low double digits to single digits . <p> I tried pre-sorting the bounding boxes by area before passing them into the algorithm . Strangely enough , sorting by reverse order and regular ordering seem to @ @ @ @ @ @ @ @ @ @ indeed , but I question the correctness . I would love to use it though- it is certainly several orders of magnitude faster than having to use nested for loops . <p> In this implementation the bounding boxes are sorted by their size ( area ) . However , if you have the probabilities associated with each bounding box that is much more preferable . In that case you would sort the bounding boxes according to their confidence ( largest to smallest ) . This would require modifying the actual function . <p> Maybe I 'm missing something . I see the bounding boxes sorted by their y2 component . My bounding boxes are the X1 , Y1 , X2 , Y2 in euclidean space . This to me would imply that the bounding boxes are being sorted only by their y2 value in euclidean space unless there 's something going on outside of the scope of this source code which is sorting them by their actual area ? <p> The only other thing I can think of is that maybe X2 and Y2 are really width and height and not the @ @ @ @ @ @ @ @ @ @ still emans that the bounding boxes are only being sorted by their heights . <p> I am a java developer that moved to python only recently and specifically to numpy so its possible I am missing something here that 's throwing off my understnading . I would love to get this working because it is worlds faster . I do have probabilities associated with my bounding boxes but I ran three different algorithms to propose the boxes and they are all going to contain three different probabilities . It seems like trying to sort by those probabilities would n't be the best idea across algorithms . I suppose I can run the NMS separately for each set of bounding boxes from each algorithm . <p> Correct , the bounding boxes are being sorted by their y2 coordinate . This implementation makes the assumption that you do not have the probabilities associated with each bounding box . If you do n't  have the probabilities associated with each bounding box then you need to sort on a coordinate . <p> In your case since you have the probabilities I would modify this function to @ @ @ @ @ @ @ @ @ @ . This will suppress bounding boxes with low probabilities that overlap with bounding boxes that have higher predicted probabilities . <p> Yeah I also think that this implementation of nms is flawed . ie : if I 'm understand this right , it builds in a systematic error which lends preference for bounding boxes near the bottom right corner of an image . Am I right about the systematic error , or did I misunderstand something in the way the algorithm works ? If so , there 's definitely a better way to do this . <p> That said , thank you once again for your work , its well-written and nicely expository as always = <p> As I mentioned in previous comments , if you do n't  have a probability/confidence associated with a bounding box you need to define some method to sort on . In this case its arbitrarily the lower-right corner of the bounding boxes . If you instead have the probabilities associated with each prediction you should instead sort on these ( with higher probabilities at the front of the list ) . <p> Hi Adrian , I @ @ @ @ @ @ @ @ @ @ , and I have already finished them all . The case studies are great , but I just noticed that almost every detection project in case study is implemented through opencv haar cascade classifier . I was wondering how I can write my own object detection framework , like you have mentioned in this blog . Is there any tutorials or courses for this ? <p> Hi Zeyu thank you for picking up a copy of Practical Python and OpenCV . This book is meant to take you the fundamentals of computer vision and image processing using the OpenCV library . If you 're interested in more advanced object detection , be sure to take a look at the PyImageSearch Gurus course where I demonstrate how to implement the HOG + Linear SVM framework from scratch . <p> What is the purpose of adding 1 when calculating the area and the width and height ? Wouldnt that make the calculation of the area and width inaccurate ? e.g. say we have a square where the top left and bottom right coordinates of the corners were ( 2,2 ) and ( 4,4 @ @ @ @ @ @ @ @ @ @ square is 4 . Using area = ( x2-x1+1 ) * ( y2-y1+1 ) would give an area of 9 . Am I missing something here ? <p> If i do have the probability of the boxes , the only thing i would have to change in the code would be the argsort to scores , and nothing else ? Your previous replies did n't  quite cleared my doubt , thanks ! <p> Thank you for your interesting posts on nms , I learned a lot from them . <p> I did find some peculiarities in your code . Do you mind clarifying them ? <p> Your text states It is absolutely critical that we sort according to the bottom-right corner as we 'll need to compute the overlap ratio of other bounding boxes later in this function Since you 're sorting on y2 , I deduct that the coordinate pair ( x2 , y2 ) is the bottom right and the pair ( x1 , y1 ) the top left . This means that y2 is the minimal y of a certain box and x2 the maximal x of a @ @ @ @ @ @ @ @ @ @ Xmax ) This results in y1 being the maximal y , x1 being the minimal X. ( y1 = Ymax , x1 = Xmin ) Since Ymin &lt;= Ymax , the calculated area on line 26 ( Xmax Xmin ) * ( Ymin Ymax ) will be &lt;= 0 . Adding a constant 1 does n't prevent the second factor and thus the area from being 0 . <p> When sorting on y2 with argssort ( line 27 ) and taking the last one ( line 34 ) , I think it will favour the highest bottom right corner since argsort sorts ascending . Sorting on probs with argssort and taking the last one works correctly , as I see it , as it favours the highest prob . <p> On lines 38-44 , you select the largest coordinates for the start of the bounding box and the smallest for the end . I assume the start is the Top Left and the end is the Bottom Right . Then if you want the largest box , I 'd take the minimum of x1 ( left most of top left @ @ @ @ @ @ @ @ @ @ ) for the top left and the maximum of x2 ( rightmost of bottom right ) and minimum of y2 ( lowest of bottom right ) . <p> On lines 46-48 you calculate width and height the same way as you did for the area on line 26 . <p> From my point of view , the overlap calculation on line 51 can result in a divide by zero when y2 = y1 1 on line 26 . <p> Even if I did n't  work much with OpenCV ( just starting in fact ) , I 'm pretty sure , that given a picture , of let 's say 800x600px , ( 0/0 ) is in the upper left corner and ( 799/599 ) is the lower right . Like on every computer screen . So y2 is the maximum of every box . So all you 're following conclusions are wrong . And since Adrian is adding a positive constant to both factors and the coordinates are zero or positive , there 's no way to get an area of size zero . Nevertheless , I would change it to area = ( @ @ @ @ @ @ @ @ @ @ to reduce the calculation steps by one . <p> By the way , from a complexity theory point of view , this algorithm and the Felszenszwalb have the same time complexity . The speedup comes from the optimized implementation of numpy . At the end of the day , numpy uses loops for the vector calculations , so on another platform or another python implementation ( cython , ironpython , etc. pp ) there might be no difference . <p> If I understand the algorithm right , its a simple line sweep . I will take a closer look if I find the time . Maybe you can gain a speedup if you build a B-Tree or a similiar structure instead of sorting and doing all the calculations . It might be even better if you already get the boxes in a Tree from the Detector @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485245 @185245/ <h> Archive Raspberry Pi <p> A few weeks ago I did a blog post on how to install the dlib library on Ubuntu and macOS . Since Raspbian , the operating system that ( most ) Raspberry Pi users run is Debian-based ( as is Ubuntu ) , the- same install instructions can be used for Raspbian as Ubuntu however , there 's a catch . The Raspberry Pi 3 ships <p> Ever since I wrote the first PyImageSearch tutorial on installing OpenCV + Python on the Raspberry Pi B+ back in February 2015 it has been my dream to offer a- downloadable , pre-configured Raspbian . img file with OpenCV pre-installed . Today this dream has become a reality . I am pleased to announce that both the Quickstart Bundle and- Hardcopy <p> Todays blog post will take a short diversion from our recent trend of Deep Learning tutorials here on the PyImageSearch blog and instead focus on a topic that I 've been receiving a ton of- emails about lately - common errors when using the Raspberry Pi camera module . I want to start this post by mentioning @ @ @ @ @ @ @ @ @ @ blog post- and I 'll say it again here today - I am not , by any stretch of the imagination , a GUI developer . I think my aversion to GUI development started back in early-High school when I was teaching myself Java ; specifically , how to write Java applets ( remember what god damn nightmares applets <p> here 's a common question I get asked on the PyImageSearch blog : How do I make a Python + OpenCV script start as soon as my system boots up ? There are many ways to accomplish . By my favorite is to use crontab and the @reboot option . The main reason I like this method so much is <p> Last week we learned a bit about Python virtual environments and how to access the RPi.GPIO and GPIO Zero libraries- along with OpenCV . Today we are going to build on that knowledge and create an " alarm " that triggers both an LED light to turn on- and a buzzer to go off whenever a- specific visual action takes place . <p> I cant believe this is the first time @ @ @ @ @ @ @ @ @ @ Raspberry Pi . Its a pretty big mistake on my part . I should have written this post much earlier . You see , on average , I receive 1-3 emails per week along the lines of : When I use the cv <p> Can you believe its been over- four years since the original Raspberry Pi model B was released ? Back then the Pi Model B shipped with only 256MB- of RAM and a 700MHz single core processor . Just over- one year ago the Raspberry Pi 2- was unleashed on the world . And man , for something called a- " Pi " , this beast made an <p> One of my favorite parts of running the PyImageSearch blog is a being able to link together previous blog posts and create a solution to a particular problem in this case , real-time panorama and image stitching with Python and OpenCV . Over the past month and a half , we 've learned how to increase the FPS <p> I 'll keep the introduction to todays post short , since I think the title of this post and GIF animation @ @ @ @ @ @ @ @ @ @ demonstrate how to attach- multiple cameras to your Raspberry Piand access all of them using a single Python script . Regardless if- your setup includes : Multiple USB @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485246 @185246/ <h> Main Menu <h> Start here : Learn computer vision &amp; OpenCV <p> Welcome to PyImageSearch.com ! On this page , I- have detailed the optimal path to learning computer vision and OpenCV utilizing the PyImageSearch website . <h> Who is this guide for ? <p> This guide is for- you. - Regardless of whether youre- just getting started learning computer vision and OpenCV , or- you 're a seasoned veteran ( or at least aspire to be an expert one day ) , I 've organized this page to be your- blueprint- for utilizing the PyImageSearch blog to level up- your computer vision and OpenCV skills . <h> Why did you create guide ? <p> I 'm a true believer in education . I love teaching . I love writing . And running the PyImageSearch blog is honestly one of the highlights of my day . <p> But with over 150+ published tutorials on PyImageSearch.com , the number of lessons alone can be quite overwhelming. - Not to mention , a number of free resource guides and email crash courses as well as- few paid computer vision education products . <p> @ @ @ @ @ @ @ @ @ @ be overwhelming- to get yourself oriented and figure out where you should even start ! <p> That 's exactly why I created this guide : to help- you . <p> Utilizing this guide , you 'll be able to : <p> Assess and determine where you are on your computer vision journey . <p> Follow my- exact blueprint to take the next steps . <p> Level up your computer vision skills . <h> Be sure to bookmark this page ! <p> Youll be able to refer back to this page as you progress through- your computer vision journey . <p> I have organized this guide into three sections to help you get started learning computer vision and OpenCV : <p> Get Yourself Oriented <p> Learn the- Fundamentals <p> Master Computer Vision <p> To determine which section is most relevant to- you , Ive designed- the following three questions to ( 1 ) determine your familiarity with the PyImageSearch blog and ( 2 ) assess your current skill level/where you would like to end up : <p> Is this your first time visiting the PyImageSearch blog ? If so , regardless of your @ @ @ @ @ @ @ @ @ @ Get Yourself Oriented section to familiarize yourself with the site . I would also recommend reading this guide- in its entirety , so you can better understand my blueprint for- learning computer vision . <p> Are you- just getting started learning computer vision , image processing , and OpenCV/looking to level up your skills ? If this is your first exposure to computer vision and OpenCV , or if you want to ensure that you have a firm grasp on the fundamentals , then the- Learn the Fundamentals- section will be the most relevant . <p> Below I have included a graphical version of my blueprint to learning computer vision . Based on your answers to the questions above , you can use this flowchart to determine which section is most relevant to you and then follow the outlined sub-steps : <p> If you already have some- previous experience in computer vision , whether from the workplace , a classroom , a hobby , or a project that utilized OpenCV , then you 're likely looking to level up your skills . In that case , you 'll want to pay attention @ @ @ @ @ @ @ @ @ @ Computer Vision " sections . <p> Finally , if you 're looking to- master computer vision and- tackle more advanced concepts such as Deep Learning , Automatic License Plate Recognition ( ANPR ) , and Face Recognition , you 'll definitely want to read the- " Master Computer Vision " section of this guide but be sure to read the other sections as well , as the steps there might still be relevant to your computer vision journey . <h> Get Yourself Oriented <p> If this is your first visit to the PyImageSearch blog or if you have zero ( or very little ) experience with computer vision and OpenCV , then you should start with this section . <p> Here , I detail my free guides and mini email crash courses to help you get started on your path to computer vision mastery . <h> Discover my hand-picked computer vision resources <p> The- first step you should take on the PyImageSearch blog is to sign up for my ( FREE ) 11-page Computer Vision and Image Search Engine Resource Guide PDF . <h> Take a free email crash course <p> I @ @ @ @ @ @ @ @ @ @ courses on computer vision . <p> Note : I offer two free email crash courses on the PyImageSearch blog . Feel free to take them both but I highly suggest that you only take- one at a time so you do n't  become overwhelmed with content . <p> The first email crash course I offer is a free 21-day course that covers- the basics of computer vision and image search engines . You can sign up for this course by entering your email address in the green slider form- at the bottom-right corner of this page : <p> Ive hand-tailored this crash course to be the best introduction to computer vision you can get . <p> When you sign up , each day for 21 days you 'll receive a new email from me direct to- your inbox this email will contain lessons , suggestions , and tips to help you : <p> If you 're looking to jumpstart your OpenCV education , its hard to beat this virtual machine , since it allows you to skip the OpenCV compile + installation process this will save you a ton of time ( @ @ @ @ @ @ @ @ @ @ you can always come back and install OpenCV on your native system at a later date . The critical step now is to simply get started . <p> This book is your- guaranteed jumpstart guide to learning the fundamentals of computer vision and image processing . In fact , many PyImageSearch readers have gone through my book in a- single weekend , making it the ultimate guide to not only learning OpenCV , - but learning it efficiently and effectively as well . <p> Best of all , you 'll learn the basics of computer vision and OpenCV by working on- actual , real-world problems such as : <p> Face detection <p> Object tracking in video <p> Handwriting recognition <p> and much more ! <p> All chapters in the book come with lots of examples , code , and detailed walkthroughs . <p> Simply put : I wrote this book for you for developers , researchers , and students who are interested in computer vision and image processing , but still need to learn the fundamentals . <p> If you 're just getting started in computer vision ( or want to level @ @ @ @ @ @ @ @ @ @ a look at- Practical Python and OpenCV . You can use the following link to read more about my book and even grab a free sample chapter : <p> Here are just some of the topics I cover inside the PyImageSearch Gurus course : 55209 @qwx675209 <p> Deep Learning and Convolutional Neural Networks 55214 @qwx675214 <p> Training Custom Object Detectors <p> Image Search Engines <p> Hand Gesture Recognition <p> Hadoop + Big Data for Computer Vision <p> Image Classification and Machine Learning <p> and much more ! <p> Like I said , the PyImageSearch Gurus course is- extremely comprehensive , and it goes into- much more detail than the- Practical Python and OpenCV book . If you 're looking for the most complete computer vision education you can find , you just cant @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485247 @185247/ <h> Tag Archives template matching <p> Today 's blog post is a continuation of our recent series on Optical Character Recognition ( OCR ) and computer vision . In a previous blog post , we learned how to install the Tesseract binary and use it for OCR . We then learned how to cleanup images using basic image processing techniques to improve the output of Tesseract OCR . <p> This past weekend Ive been really sick with the flu . I have n't done much besides lay on my couch , sip chicken noodle soup from a coffee mug , and marathon gaming sessions of Call of Duty . Its honestly been- years since I 've spent a weekend relentlessly playing Call of Duty . Getting online @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485248 @185248/ <h> Tag Archives webcam <p> Todays blog post will take a short diversion from our recent trend of Deep Learning tutorials here on the PyImageSearch blog and instead focus on a topic that I 've been receiving a ton of- emails about lately - common errors when using the Raspberry Pi camera module . I want to start this post by mentioning Dave Jones , <p> One of my favorite parts of running the PyImageSearch blog is a being able to link together previous blog posts and create a solution to a particular problem in this case , real-time panorama and image stitching with Python and OpenCV . Over the past month and a half , we 've learned how to increase the FPS <p> I 'll keep the introduction to todays post short , since I think the title of this post and GIF animation above speak for themselves . Inside this post , I 'll demonstrate how to attach- multiple cameras to your Raspberry Piand access all of them using a single Python script . Regardless if- your setup includes : Multiple USB webcams . Or the Raspberry <p> Over the @ @ @ @ @ @ @ @ @ @ discussed how to use threading to increase- our FPS processing rate on- both built-in/USB webcams , along with the Raspberry Pi camera module . By utilizing threading , - we learned that we can substantially reduce the affects of I/O latency , leaving the main thread to run without being blocked as <p> Today is the second post in our three part series on milking- every last bit of performance out of your webcam- or Raspberry Pi camera . Last week we discussed how to : Increase the FPS rate of our video processing pipeline . Reduce the affects of I/O latency on standard USB and built-in webcams using threading . This week well- continue <p> Over the next few weeks , I 'll be doing a series of blog posts on how to improve your frames per second ( FPS ) from your webcam using Python , OpenCV , and threading . Using threading to handle I/O-heavy tasks ( such as reading frames from a camera sensor ) is a programming model that has existed for decades . For example , <p> Over the past year the PyImageSearch blog has @ @ @ @ @ @ @ @ @ @ clustering to find the dominant colors in an image was ( and still is ) hugely popular . One of my personal favorites , building a kick-ass mobile document scanner- has been the most popular PyImageSearch article for months . And @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485249 @185249/ <h> Tag Archives pyimagesearch gurus <p> In todays blog post , I interview- Saideep Talari , a PyImageSearch Gurus graduate who was recently hired as a computer vision engineer at a startup in India . Saideeps story holds a special place in my heart as its so incredibly- sincere , - genuine , and- heartfelt . You see , Saideep comes- from a very low income- Indian family . They did not have much money . In <p> In todays blog post , I interview PyImageSearch Gurus member , - Tuomo Hiippala , who was recently awarded a- 28000G grant ( approximately $30,500 USD ) to research- how computer vision can be used to study visual culture , including social media images and photo archives . Well discuss his very unique area of research , which blends- linguistics with computer vision , and how his work is <p> Todays blog post comes straight out of the PyImageSearch Gurus course . Inside PyImageSearch Gurus we have a community page ( much like a combination of forums + Q&amp;A + StackOverflow ) where we discuss a variety of computer vision topics @ @ @ @ @ @ @ @ @ @ learning computer vision and image processing . This post was <p> This past Friday the PyImageSearch Gurus Kickstarter came to a close . The campaign was a huge success with 253 backers joining in . It was extremely humbling to see the support from you , the PyImageSearch readers , and the Kickstarter viewers . I feel extremely lucky to be able to wake up every day and do what I <p> Were getting close to the end of the PyImageSearch Gurus Kickstarter " and we just hit our 2nd stretch goal ! This means that well be learning all about hand gesture recognition inside the PyImageSearch Gurus computer vision course ! Anyway , over the past few days I have been visiting family . Its amazing how much Ive missed <p> I need to apologize for this post- " if there are some glaringly obvious typos on grammatical errors , I 'm sorry . You see , I just pulled an all-nighter . My eyes are bloodshot and glazed over like something out of The Walking Dead . My brain feels like mashed potatoes beaten with a sledge hammer @ @ @ @ @ @ @ @ @ @ the Super Bowl this past weekend ? I did . Kind of . I spent Super Bowl Sunday ( which is practically a holiday in the United States ) at my favorite Indian bar . Pounding Kingfisher beers . Savoring a delicious dish of- Tandoori chicken all while hacking up a storm on my laptop and coding up some custom <p> Wow , I 'm speechless at the moment . You guys are incredible PyImageSearch has the best , most supportive readers possible on the face of this earth . The PyImageSearch Gurus Kickstarter campaign went live today at 10am EST . By 10:25am EST , the project- was 100% funded ! It took less than 25 minutes to 100% fund the project ! I <p> The PyImageSaerch Gurus Kickstarter is officially LIVE ! You can back the PyImageSearch Gurus Kickstarter campaign using this link : LONG ... Remember , there are only a handful of early bird spots available at reduced prices &amp; early entry- - you 'll definitely want to act now if you want to claim your spot ! Thank you so much for being supportive <p> So over @ @ @ @ @ @ @ @ @ @ . And Ive given you an exclusive sneak preview . And Ive even detailed the full list of topics to be covered inside PyImageSearch Gurus . But now I have something even more special Today I am super excited- to share with you the @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485250 @185250/ <h> Tag Archives motion detection <p> One of my favorite parts of running the PyImageSearch blog is a being able to link together previous blog posts and create a solution to a particular problem in this case , real-time panorama and image stitching with Python and OpenCV . Over the past month and a half , we 've learned how to increase the FPS <p> I 'll keep the introduction to todays post short , since I think the title of this post and GIF animation above speak for themselves . Inside this post , I 'll demonstrate how to attach- multiple cameras to your Raspberry Piand access all of them using a single Python script . Regardless if- your setup includes : Multiple USB webcams . Or the Raspberry <p> Wow , last weeks blog post on building a basic motion detection system was- awesome. - It was a lot of fun to write and the feedback I got from readers like yourself made it well worth the effort to put together . For those of you who are just tuning it , last weeks post on building a motion detection @ @ @ @ @ @ @ @ @ @ took my last beer . These are words a man should never , ever- have to say . But I muttered them to myself in an exasperated sigh of disgust as I closed the door to my refrigerator . You see , - I had just spent over 12 hours @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485251 @185251/ <p> Instead of starting off by parsing arguments , loading images , and taking care of other normal procedures , let 's skip these steps for the time being and jump immediately in to defining our sortcontours- - function which will enable us to sort our contours . <p> The actual sortcontours- - function is defined on Line 6 and takes two arguments . The first is cnts- , the list of contours that the we want to sort , and the second is the sorting method- , which indicates the direction in which we are going to sort our contours ( i.e. left-to-right , top-to-bottom , etc . ) . <p> From there well initialize two important variables on Lines 8 and 9 . These variables simply indicate the sorting order ( ascending or descending ) and the index of the bounding box we are going to use to perform the sort ( more on that later ) . Well initialize these variables to sort in ascending order and along to the x-axis location of the bounding box of the contour . <p> If we are sorting right-to-left or @ @ @ @ @ @ @ @ @ @ according to the location of the contour in the image ( Lines 12 and 13 ) . <p> Similarly , on Lines 17 and 18 we check to see if we are sorting from top-to-bottom or bottom-to-top . If this is the case , then we need to sort according to the y-axis value rather than the x-axis ( since we are now sorting vertically rather than horizontally ) . <p> The actual sorting of the contours happens on Lines 22-24 . <p> We first compute the bounding boxes of each contour , which is simply the starting ( x , y ) -coordinates of the bounding box followed by the width and height ( hence the term " bounding box " ) . ( Line 22 ) <p> The boundingBoxes- - enable us to sort the actual contours , which we do on Line 23-24 using some Python magic that sorts two lists together . Using this code we are able to sort both the contours and bounding boxes according to the criteria that we provided . <p> Finally , we return the ( now sorted ) list of @ @ @ @ @ @ @ @ @ @ 27 . <p> This function simply computes the center ( x , y ) -coordinate of the supplied contour c- - on Lines 32-34 and then uses the center coordinates to draw the contour I 'd , i- , on Lines 37 and 38 . <p> Finally , the passed in image- - is returned to the calling function on Line 41 . <p> Again , this is simply a helper function that well leverage to draw contour I 'd numbers on our actual image so we can visualize the results of our work . <p> Now that the helper functions are done , let 's put the driver code in place to take our actual image , detect contours , and sort them : <p> Sorting Contours using <p> 61 <p> 62 <p> # construct the argument parser and parse the arguments 55206 @qwx675206 <p> LONG ... to the input image @ @ @ @ @ @ @ @ @ @ <p> # load the image and initialize the accumulated edge image <p> image=cv2.imread ( args " image " ) <p> LONG ... <p> # loop over the blue , green , and red channels , respectively <p> forchan incv2.split(image) : <p> # blur the channel , extract edges from it , and accumulate the set <p> # of edges for the image <p> **28;10933;TOOLONG <p> **28;10963;TOOLONG <p> **35;10993;TOOLONG , edged ) <p> # show the accumulated edge map <p> cv2.imshow ( " Edge Map " , accumEdged ) <p> Lines 44-47 are n't  very interesting " they simply parse our command line arguments , --image- - which is the path to where our image resides on disk , and --method- - which is a text representation of the direction in which we want to sort our contours . <p> From there we load our image off disk on Line 50 and allocate memory for the edge map on Line 51 . <p> Constructing the actual edge map happens on Lines 54-59 , where we loop over each Blue , Green , and Red channel of the image ( Line 54 @ @ @ @ @ @ @ @ @ @ noise ( Line 57 ) , perform edge detection , ( Line 58 ) , and update the accumulated edge map on Line 59 . <p> We display the accumulated edge map on line 62 which looks like this : <p> Figure 1 : ( Left ) Our original image . ( Right ) The edge map of the Lego bricks . <p> As you can see , we have detected the actual edge outlines of the Lego bricks in the image . <p> Now , let 's see if we can ( 1 ) find the contours of these Lego bricks , and then ( 2 ) sort them : <p> Sorting Contours using <p> 86 <p> 87 <p> # find contours in the accumulated image , keeping only the largest <p> # ones <p> LONG ... 55211 @qwx675211 <p> LONG @ @ @ @ @ @ @ @ @ @ ) contours and draw them <p> for ( i , c ) inenumerate(cnts) : <p> orig=drawcontour ( orig , c , i ) <p> # show the original , unsorted contour image <p> cv2.imshow ( " Unsorted " , orig ) <p> # sort the contours according to the provided method <p> LONG ... <p> # loop over the ( now sorted ) contours and draw them <p> for ( i , c ) inenumerate(cnts) : <p> drawcontour ( image , c , i ) <p> # show the output image <p> cv2.imshow ( " Sorted " , image ) 55212 @qwx675212 <p> Quite obviously , the first step here is to find the actual contours in our accumulated edge map image on Line 66 and 67 . We are looking for the external contours of the Lego bricks , which simply corresponds to their outlines . <p> Based on these contours , we are now going to sort them according to their size by using a combination of the Python sorted- - function and the cv2.contourArea- - method " this allows us to sort our contours according to their @ @ @ @ @ @ @ @ @ @ <p> We take these sorted contours ( in terms of size , not location ) , loop over them on Line 72 and draw each individual contour on Line 73 using our drawcontour- - helper function . <p> This image is then displayed to our screen on Line 76 . <p> However , as you 'll notice , our contours have been sorted only according to their size " no attention has been paid to their actual location in the image . <p> We address this problem on Line 79 where we make a call to our custom sortcontours- - function . This method accepts our list of contours along with sorting direction method ( provided via command line argument ) and sorts them , returning a tuple of sorted bounding boxes and contours , respectively . <p> Finally , we take these sorted contours , loop over them , draw each individual one , and finally display the output image to our screen ( Lines 82-87 ) . <h> Results <p> Let 's put our hard work to the test . <p> Open up a terminal , navigate to your source @ @ @ @ @ @ @ @ @ @ left we have our original unsorted contours . Clearly , we can see that the contours are very much out of order the first contour is appearing at the very bottom and the second contour at the very top ! <p> However , by applying our sortedcontours- - function we were able to sort our Lego bricks from top-to-bottom . <p> As you can see , there 's nothing to it were simply leveraging the bounding box of each object in the image to sort the contours by direction using Python and OpenCV . <p> In the future all you need is our trusty sortedcontours- - function and you 'll always be able to sort contours in terms of direction without a problem . <h> Summary <p> In this blog article we learned how to sort contours from left-to-right , right-to-left , top-to-bottom , and bottom-to-top . <p> Realistically , we only needed to leverage two key functions . <p> The first critical function was the cv2.boundingRect- - method which computes the bounding box region of the contour . And based on these bounding boxes , we leveraged some Python magic and @ @ @ @ @ @ @ @ @ @ " sort " these bounding boxes in the direction we want . <p> And that 's all there is to it ! <p> If you would like to play with the examples included in this post , just enter your email address in the form below and I 'll email you the code instantly . <h> Downloads : 55217 @qwx675217 <h> 30 Responses to Sorting Contours using Python and OpenCV <p> Hey ! Can you do an article on sorting of points in a grid ? I have a point grid on a wall and I make a picture of that wall and now I have the set of points , after a perspective transform and lens distortions , but no assignment of points to grid locations . <p> Hi Chris , thanks for the comment . I 'm not sure I fully understand your comment , but are you trying to ( 1 ) find the corners of the grid and ( 2 ) sort them according to some criterion ? Finding the corners of the grid should be very simple the Harris or GFTT corner detector will easily make quick work @ @ @ @ @ @ @ @ @ @ by sorting the actual locations ? <p> Hi Adrian , interesting post . I like the idea of being able to sort the contours . However , I had 2 questions : 1 ) its a little difficult to tell which criteria is being used to do the actual sorting . From lines 23-24 , it looks like it is the boundingBoxes starting points . Is that correct ? 2 ) While it depends on the application , itd seem better to sort the contours based upon their centroid or the first moment . In this case , all contours are roughly the same size so it may not matter much . Your thoughts ? <p> 1 . Correct , the we are sorting on either the x or y coordinate of the starting location of the bounding box . <p> 2 . As you suggested , it all depends on your application . In most cases when I am using bounding boxes , such as automatic license plate recognition or handwriting recognition , sorting from left-to-right based on the starting x-coordinate is more than sufficient . In other cases @ @ @ @ @ @ @ @ @ @ that 's the beauty of the approach suggested you can sort however you want by modifying the lambda function . <p> Hey Adrian , I want to use part of your script in a programm to sort contours in a video stream . However it crashs all the time when I remove the contours out of the camera screen with : " ValueError : need more than 0 values to unpack " refering to line 24 of your code . How can I tell the programm to wait untill there is at least one contour , or to just do n't  " unpack " untill one contour appears . <p> i have several grid based images which has start point ( small green colored region located anywhere within image ) and stop point ( small red region ) . i want to find shortest path between start point and end point . what should I do <p> The first step is to find the two points . Methods like edge detection and thresholding can help you accomplish this . From there , you simply compute the Euclidean distance , which is @ @ @ @ @ @ @ @ @ @ <p> this was a great one but i have one qustion i dad detected more than one object and i want to sort the Coordinates of this objects in array instead of printing out them like ( push-pop ) -sorting method <p> Not easily , but yes . If you know the number of items there are in a row , you first from top-to-bottom . Extract the rows , then sort the rows left-to-right . Ill be demonstrating how to do this @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485252 @185252/ <h> Tag Archives svm <p> A couple weeks ago , we discussed the concepts of both- linear classification- and- parameterized learning . This type of learning allows us to take a set of input data and class labels , and actually learn a function that- maps the input to the output predictions , simply by defining a set of parameters and optimizing over them . Our linear classification tutorial focused <p> Over the past few weeks , we 've started to learn more and more about machine learning and the role it plays in- computer vision , - image classification , and- deep learning . Weve seen how Convolutional Neural Networks ( CNNs ) such as LetNet can be used to classify handwritten digits from the MNIST dataset . We 've applied the k-NN algorithm to classify whether or <p> Well . Ill just come right out and say it. - Today is my 27th birthday . As a kid I was always- super excited about my birthday . It was another year closer to being able to drive a car . Go to R rated movies . Or buy alcohol @ @ @ @ @ @ @ @ @ @ care too much for my <p> Last week we discussed how to use OpenCV and Python to perform pedestrian detection . To accomplish this , we leveraged the built-in HOG + Linear SVM detector that OpenCV ships with , allowing us to detect people in images . However , one aspect of the HOG person detector we- did not discuss in detail is the detectMultiScale- function ; specifically , <p> I 've met a lot of amazing , uplifting- people over the years . My PhD advisor who helped get me through graduate school . My father who was always there for me as a kid and still is now . And- my girlfriend who has always been positive , helpful , and supportive ( even when I probably did n't  deserve it ) . I 've also <p> I have issues - I cant stop thinking about object detection . You see , last night I was watching The Walking Dead- and instead of enjoying the zombie brutality , the forced- cannibalism , or the enthralling storyline , - all I wanted to do was build an object detection system to @ @ @ @ @ @ @ @ @ @ not . I mean , its <p> Did you watch the Super Bowl this past weekend ? I did . Kind of . I spent Super Bowl Sunday ( which is practically a holiday in the United States ) at my favorite Indian bar . Pounding Kingfisher beers . Savoring a delicious dish of- Tandoori chicken all while hacking up a storm on my laptop and coding up some custom <p> Connecticut is cold . Very cold . Sometimes its hard to even get out of bed in the morning . And honestly , without the aide of copious amounts of pumpkin spice lattes and the beautiful sunrise over the crisp autumn leaves , I do n't  think I would leave my cozy bed . But I have work to do . And today <p> If you 've been paying attention to my Twitter account lately , youve probably noticed one or two teasers of what I 've been working on a Python framework/package to rapidly construct object detectors using Histogram of Oriented Gradients and Linear Support Vector Machines . Honestly , I really ca n't stand using the Haar @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485253 @185253/ <h> Archive Object Detection <p> Todays blog post is inspired from an email I received from Jason , a student at the University of Rochester . Jason is interested in building a custom object detector using the HOG + Linear SVM framework for his final year project . He understands the steps required to build the object detector well enough - but he is n't <p> Did you know that OpenCV can detect cat faces in imagesright out-of-the-box with- no extras ? I did n't  either . But after Kendrick Tan broke the story , I had to check it out for myselfand do a little investigative- work to see how this cat detector seemed to sneak its way into the OpenCV repository without me noticing ( much <p> Last week we discussed how to use OpenCV and Python to perform pedestrian detection . To accomplish this , we leveraged the built-in HOG + Linear SVM detector that OpenCV ships with , allowing us to detect people in images . However , one aspect of the HOG person detector we- did not discuss in detail is the detectMultiScale- function ; @ @ @ @ @ @ @ @ @ @ uplifting- people over the years . My PhD advisor who helped get me through graduate school . My father who was always there for me as a kid and still is now . And- my girlfriend who has always been positive , helpful , and supportive ( even when I probably did n't  deserve it ) . I 've also <p> This past Saturday , I was caught in the grips of childhood nostalgia , so I busted out my PlayStation 1 and my original copy of Final Fantasy VII . As a kid in late middle school/early high school , I logged 70+ hours playing through this heartbreaking , inspirational , absolute masterpiece of an RPG . As a kid in middle <p> Today marks the- 100th blog post on PyImageSearch. 100 posts. - Its hard to believe it , but its true . When I started PyImageSearch back in January of 2014 , I had no idea what the blog would turn into . I did n't  know how it would evolve and mature . And I most certainly did not @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485254 @185254/ <p> And while it still looks like the same image to us , to a Restricted Boltzmann Machine , this translation could spell trouble . <h> Raw Pixel Intensities as Feature Vectors <p> Did you know that there is a subtle , but critical- issue when using raw pixels as feature vectors , which is commonly done in image-based deep learning classification tasks ? <p> If you 're not careful and do n't  take the appropriate precautions , - small , 1 pixel shifts- in your input image can dramatically- hurt the performance of your classifier . <p> And were only talking about a one pixel shift . A shift this small is barely ( if at all ) noticeable to the human eye . <p> But if you 've trained your Restricted Boltzmann Machine on raw pixel features , you might be in for a surprise . <p> Sure , convolutional neural networks help alleviate this translation issue . Actually , alleviate- is too strong of a word . They are able to tolerate- translations in the image . The convolutional nets ( in general ) are still susceptible to @ @ @ @ @ @ @ @ @ @ most recent papers , Intriguing properties of neural nets- suggests , small changes in the input image can dramatically alter the overall classification of the network . <p> The authors call these types of images " adversarial images " do to the fact that they are , for all intents and purposes , identical to their original images according to the human eye . <p> However , these adversarial images were constructed in a far more complex method than simple one pixel translations of the input image . In fact , these images were constructed by manipulating pixel values by a very small amount in order to maximize the error of the deep learning network . <p> But it does demonstrate how subtle changes in an image , which are totally undetectable to the human eye , can lead to a misclassification when using raw pixel features . <h> How 1 Pixel Shifts in Images Can Kill Your RBM Performance <p> In order to demonstrate ( on a substantially smaller scale ) some of the issues associated with deep learning nets based on raw pixel feature vectors , I 've decided @ @ @ @ @ @ @ @ @ @ testing set of images and shift each image one pixel up , down , left , and right , will performance decrease ? <p> Again , for all intents and purposes , these one pixel shifts will be unnoticeable to the human eyebut what about a Restricted Boltzmann Machine ? <p> So here 's what were going to do : <p> Construct a training and testing split using the raw pixel features of a sample of the MNIST dataset . <p> Apply a single Restricted Boltzmann Machine to learn an unsupervised feature representation from the MNIST sample . <p> Train a Logistic Regression classifier on top of the learned features . <p> Evaluate the classifier using the test set to obtain a baseline . <p> Perturb the testing set by shifting the images one pixel up , down , left , and right . <p> Re-evaluate our classification pipeline and see if accuracy decreases . Again , these one pixel shifts are virtually unnoticeable to the human eye but they will end up hurting the overall performance of the system . <p> And here 's what were NOT- going to do : @ @ @ @ @ @ @ @ @ @ that these results are incriminating of all raw pixel based approaches . They 're not . I 'm only going to use a single RBM here . So there wont be any stacking and thus there wont be any deep learning . <p> Claim that researchers and developers should abandon raw pixel based approaches . There are ways to fix the problems I am suggesting in this post . The most common way is to apply deformations to the images at training time ( i.e. generating more training data by artificially transforming the image ) to make the neural net more robust . The second way is to sub-sample regions of the input image . <p> But what I am going to do- is show you how small , one pixel shifts in images- can dramatically- decrease the accuracy of your RBM if the appropriate precautions are n't  taken . <p> Hopefully this series of blog posts , with Python and scikit-learn code included , will aide some students and researchers who are just exploring neural nets and mapping raw pixel feature vectors to outputs . <h> Preliminary Results <p> The code @ @ @ @ @ @ @ @ @ @ will come next post ) , but I wanted to show some preliminary results : <p> Python <p> 14 <p> 15 <p> **39;11030;TOOLONG <p> **30;11071;TOOLONG <p> 00.950.980.97196 <p> 10.970.960.97245 <p> 20.920.950.94197 <p> 30.930.910.92202 <p> 40.920.950.94193 <p> 50.950.860.90183 <p> 60.950.950.95194 <p> 70.930.910.92212 <p> 80.910.900.91186 <p> 90.860.900.88192 <p> **25;11103;TOOLONG <p> The first thing I did was take a sample of the MNIST dataset ( 2000 data points ; roughly uniformly distributed per class label ) and constructed a 60/40 split 60% of the data for training and 40% for validation . <p> Then I used Bernoulli Restricted Boltzmann Machine to learn an unsupervised feature representation from the training data , which was then fed into a Logistic Regression classifier . <p> All relevant parameters were grid-searched and cross-validated to help ensure optimal values . <p> Then , I decided to " nudge " the testing test , by shifting each image in the testing set one pixel up , @ @ @ @ @ @ @ @ @ @ set four times larger than the original . <p> These shifted images , while nearly identical to the human eye , provided to be a challenge for the pipeline as accuracy dropped 5% . <p> Python <p> 14 <p> 15 <p> **37;11130;TOOLONG <p> **30;11169;TOOLONG <p> 00.940.930.94784 <p> 10.960.890.93980 <p> 20.870.910.89788 <p> 30.850.850.85808 <p> 40.880.920.90772 <p> 50.860.800.83732 <p> 60.900.910.90776 <p> 70.860.900.88848 <p> 80.800.850.82744 <p> 90.840.790.81768 <p> **25;11201;TOOLONG <p> On a small scale , we can see the issue with using raw pixels as feature vectors . Slight translations in the image , tiny rotations , and even noise during the image capturing process can reduce accuracy when fed into the net . <p> While these results are by no means conclusive , they at least demonstrate the general intuition that using raw pixel feature vectors can be prone to error without significant preprocessing beforehand . <h> Summary <p> In this blog post I introduced the notion that small @ @ @ @ @ @ @ @ @ @ Boltzmann Machine performance if you ca n't careful . <p> I then provided a " teaser " set of results to demonstrate that the one pixel translations in the images , while nearly identical to the human eye , can lead to a reduction in accuracy . <h> Next Up : <p> In my next blog post , I 'll show you my Python code to apply deep learning and a Restricted Boltzmann Machine to the MNIST dataset . <p> Be sure to signup for the newsletter below to receive an update when the post goes live ! You wo n't want to miss it <p> also , i 'm assuming you mentioned presumably rbm stacking as a way to would address image translation ( which is the point of the thought experiment ) . how would a rbm stack accomplish translation invariance ? <p> Hey Vin your intuition is correct : the output of one RBM feeds into another . As for RBMs achieving translation invariance , it does n't  . That 's why we have Convolutional Neural Networks . That said , give this post on getting started with deep learning a read @ @ @ @ @ @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485255 @185255/ <h> Tag Archive <p> Histogram of Oriented Gradients , or HOG for short , are descriptors mainly used in computer vision and machine learning for object detection . However , we can also use HOG descriptors for quantifying and representing both shape- and texture . HOG features were first introduced by Dalal and Triggs in their CVPR 2005 paper , Histogram of Oriented Gradients for Human Detection . In their work , Dalal Read More <p> In the previous section we learned about a few simple contour properties such as area , perimeter , and bounding boxes . These properties actually build upon each other and we can use them to define more advanced contour properties . The properties we are about to inspect are more- powerful than our previous properties , as they allow us to discriminate between and recognize various Read More <p> In our previous lesson , we learned how to- localize license plates in images using basic image processing techniques , such as morphological operations and contours . These license plate regions are called- license plate- candidates it is our job to take these candidate @ @ @ @ @ @ @ @ @ @ plate characters from the- background of the license plate . On the surface , this Read More 
@@71485257 @185257/ <h> Skin Detection : A Step-by-Step Example using Python and OpenCV <p> So last night I went out for a few drinks with my colleague , James , a fellow computer vision researcher who I have known for years . <p> You see , James likes to party . He s a " go hard " type of guy . He- wanted to hit every single bar on Washington St. ( there 's a lot of them ) and then hit em again on the way back . <p> Its safe to say that by the end of the night that James was blotto . Wasted . Lights out . <p> Anyway , it was getting late . 1:47am , to be exact . I 'm tired . I just want to go to bed . Were at the last bar . I 'm signing my name on the tab and heading for the door , looking forward to getting some sleep . <p> And James , in his clearly drunken and intoxicated state , walks ( or rather , stumbles like a newborn foal ) up to this blonde who looks @ @ @ @ @ @ @ @ @ @ says : " You knowif you take your shirt off I have a Python script that can detect how much skin you 're showing . Wan na see ? " <p> Now tell me , - what do you think happens next ? <p> That 's right . James gets a top shelf Manhattan thrown in his face by the South Beach girl . Honestly , he s just lucky that he did n't  catch a well deserved fist to the nose . <p> Shocked , and fairly appalled , I led the drunken James back to my apartment to sleep it off . Not to mention have a conversation with him regarding how to treat women the next morning . <p> So here 's the deal <p> In this blog post Im going to show you- how to detect skin in images using computer vision . <p> But use these superpowers for good , okay ? Do n't  be like James . Play it cool . <p> Read on and find out how simple it really is to detect skin in images using Python and OpenCV . <p> OpenCV and Python versions @ @ @ @ @ @ @ @ @ @ OpenCV 2.4 . X/OpenCV 3.0+ . <h> Detecting Skin in Images &amp; Video Using Python and OpenCV <p> You know the drill . Open up your favorite editor , create a new file , name it skindetector.py- , and let 's get to work : <p> Example : Detecting 14 <p> 15 <p> 16 55203 @qwx675203 <p> frompyimagesearch importimutils 55220 @qwx675220 55218 @qwx675218 <p> importcv2 55202 @qwx675202 55206 @qwx675206 <p> ap.addargument ( " -v " , " --video " , <p> help= " path to the ( optional ) video file " ) 55208 @qwx675208 <p> # define the upper and lower boundaries of the HSV pixel <p> # intensities to be considered ' skin ' <p> lower=np.array ( 0,48,80 , dtype= " uint8 " ) <p> **25;11228;TOOLONG , dtype= " uint8 " ) <p> On Lines 2-5 we import the packages that well need . Well use NumPy for some numerical processing , argparse- @ @ @ @ @ @ @ @ @ @ - for our OpenCV bindings . <p> Well also use a package called imutils- - which contains a bunch of " convenience " image processing functions for resizing , rotating , etc . You can read more about the imutils- - package in my Basics of Image Manipulations post . <p> From there , - Lines 8-11 parse our command line arguments . We have only a single ( optional ) switch , --video- , which allows you to detect skin in a pre-supplied video . However , you could just as easily omit this switch from the command and use your webcam to detect skin in images . If you do n't  have a webcam attached to your system , then you 'll have to supply a video . <p> From there , we- define the- lower and- upper boundaries for pixel intensities to be considered skin on Lines 15 and 16 . Its important to note that these boundaries are for the- HSV color space , - NOT the RGB color space . <p> If you havent had a chance to take a look at my post on Color @ @ @ @ @ @ @ @ @ @ a good time to do so- well be building off the fundamentals presented in the color detection post and extending our method to detect skin in images . <p> Alright , now let 's write some code to grab a reference to the webcam or video : <p> Example : Detecting Skin in Images using Python and OpenCV <p> Python <p> 18 <p> 19 <p> 20 <p> 21 <p> 22 <p> 23 <p> 24 <p> 25 <p> # if a video path was not supplied , grab the reference <p> # to the gray <p> ifnotargs.get ( " video " , False ) : <p> **26;11255;TOOLONG <p> # otherwise , load the video <p> else : <p> **28;11283;TOOLONG " video " ) <p> On Line 20 we make a check to see if the --video- - switch was supplied . If it was not , then we grab reference to the webcam by passing a value of 0- to the cv2.VideoCapture- - function . <p> However , if the --video- - switch was supplied , then well pass the path to the video to cv2.VideoCapture- - on Lines 24 and 25 @ @ @ @ @ @ @ @ @ @ <p> Example : Detecting Skin in Images using <p> 63 <p> 64 <p> # keep looping over the frames in the video <p> whileTrue : <p> # grab the current frame <p> ( grabbed , frame ) =camera.read() <p> # if we are viewing a video and we did not grab a <p> # frame , then we have reached the end of the video <p> ifargs.get ( " video " ) andnotgrabbed : <p> break <p> # resize the frame , convert it to the HSV color space , <p> # and determine the HSV pixel intensities that fall into <p> # the speicifed upper and lower boundaries <p> **26;11313;TOOLONG , width=400 @ @ @ @ @ @ @ @ @ @ ) <p> # apply a series of erosions and dilations to the mask <p> # using an elliptical kernel <p> LONG ... <p> LONG ... <p> LONG ... <p> # blur the mask to help remove noise , then apply the <p> # mask to the frame <p> **34;11373;TOOLONG , ( 3,3 ) , 0 ) <p> LONG ... <p> # show the skin in the image along with the mask <p> cv2.imshow ( " images " , np.hstack ( frame , skin ) ) <p> # if the ' q ' key is pressed , stop the loop <p> **31;11409;TOOLONG " q " ) : <p> break <p> # cleanup the camera and close any open windows <p> camera.release() <p> cv2.destroyAllWindows() <p> We start looping over the frames in the video on Line 28 and grab the next frame on Line 30 using the camera.read()- - method . <p> The camera.read()- - function returns a tuple , consisting of grabbed- - and frame- . The grabbed- - variable is simply a boolean flag , indicating if the frame was successfully read or not . The frame- - is the @ @ @ @ @ @ @ @ @ @ check on- Line 34 to see if the frame- - was not read . If it was not , and we are reading a video from file , we assume we have reached the end of the video and we can safely break from the loop . Otherwise , we keep on looping . <p> In order to speed up the skin detection process , we use our imutils.resize- - convenience function to resize our frame to have a width of 400 pixels on- Line 40 . <p> The actual skin detection takes place on- Line 41 and 42 . First , we convert the image from the RGB color space to the HSV color space . Then , we apply the cv2.inRange- - function , supplying our HSV frame , and our lower and upper boundaries as arguments , respectively . <p> The output of the cv2.inRange- - function is our mask This mask is- a single channel image , has the same width and height as the frame , and is of the 8-bit unsigned integer data type . <p> Pixels that are- white ( 255 ) in @ @ @ @ @ @ @ @ @ @ . Pixels that are- black ( 0 ) in the mask represent areas- that are notskin . <p> However , we may detect many small false-positive skin regions in the image . To remove these small regions , take a look at- Lines 46-48 . First , we create an elliptical structuring kernel . Then , we use this kernel to perform two iterations of erosions and dilations , respectively . These erosions and dilations will help remove the small false-positive skin regions in the image . <p> From there , we smooth- the mask slightly using a Gaussian blur on- Line 52 . This smoothing step , while not critical to the skin detection process , produces a much cleaner mask . <p> We then apply the skin mask to our frame on- Line 53 . <p> Line 56 shows us a side-by-side view of the original frame along with the frame with skin detected in it . <p> We wait for a keypress on- Line 59 , and if its the q- - key , then we break from the loop . <p> Finally , - Lines 63 @ @ @ @ @ @ @ @ @ @ any open windows . <h> Running our Skin Detector <p> To run our skin detector , open up a terminal and navigate to where our source code is stored . <p> If you are using the example video provided with the code downloads for this post ( or an example video of your own ) , then issue the following command : <p> Example : Detecting Skin in Images using Python and OpenCV <p> Shell <p> 1 <p> $python skindetector.py--video video/skinexample.mov <p> Otherwise , if you want to use your own webcam , execute this command : <p> Example : Detecting Skin in Images using Python and OpenCV <p> Shell <p> 1 <p> $python skindetector.py <p> If all goes well , you should see my detected skin , as shown by the following figure : <p> Figure 1 : Detecting the skin of my face using Python and OpenCV . <p> And here is another example image from the video supplied with the code download : <p> Figure 2 : Detecting both the skin of my and and the skin of my face using Python and OpenCV . <p> Pretty @ @ @ @ @ @ @ @ @ @ were able to build a fairly rudimentary skin detection algorithm that 's still able to obtain decent results ! <h> Limitations <p> There are some pretty obvious limitations and drawbacks to this approach . <p> The main drawback is that we are framing skin detection as a " color detection " problem . This assumes that we can easily specify the HSV values for ranges of pixel intensities that are considered skin . <p> However , under different lighting conditions , this approach might not perform as well and we would likely have to continue to tweak the HSV value ranges . <p> The HSV value ranges we supplied worked fairly well for this example postbut what about for other ethnicities ? <p> For example , I 'm a white male . And surely the same HSV values used to detect my skin could not be used to detect someone from Asia or Africa . This implies that we have some- a priori knowledge regarding the skin tone of who we want to detect . <p> Of course , more robust approaches can be applied . A ( highly simplified ) example- @ @ @ @ @ @ @ @ @ @ determine the color of the skin on their face , and then use that model to detect the rest of the skin on their body . <p> Not a bad approach , but as you can imagine , its definitely a little more complicated . <p> In the meantime , as long as you can specify the HSV values of the skin tone you want to detect , framing skin detection as a color detection problem should work quite well for you . <h> Summary <p> In this blog post I showed you how to- detect skin in images using Python and OpenCV . <p> We were able to supply upper and lower ranges of pixel intensities in the HSV color space to detect skin in images . <p> While this is not a perfect or robust approach , the simplicity of our skin detection algorithm makes it a very good starting point to build more robust solutions . <p> But remember our promise , okay ? <p> Do n't  be like James . Use our skin detection superpowers for good , not evil . <h> Downloads : 55217 @qwx675217 @ @ @ @ @ @ @ @ @ @ from the bottom of the page ? Be sure to click the " Download the Code ! " button at the bottom of this page and the pyimagesearch module will be in the . zip file . Alternatively , shoot me an email and I 'll be happy to pass it along to you . <p> Hey Gol , I 'm not sure I understand your question there is no facedetector module inside the skin detection . zip . If you are using code from the Case Studies book , then you 'll need to copy the facedetector into the pyimagesearch directory of the skin detection example . <p> I did a couple of your tutorials the last few days So I Bought your book today , I am looking forward to starting my journey . <p> I do have some experience with OpenCV , numpy , scipy , scikit image and scikit learn . I used them in a couple of projects and homeworks so I learned using some tutorials from here and there , so the learning process was n't structures . <p> So I decided to start with your book as @ @ @ @ @ @ @ @ @ @ beginning , as it will consolidate the skills much better I hope . <p> Anytime you see a call to cv2.VideoCapture and you want to use the code with the Raspberry Pi , you 'll need to convert it to use the picamera module , like in this post or use the V4L2 driver . I really recommend converting the code . Its much easier than getting the driver working . <p> As for the comment , that 's simply a typo . If a video is not supplied , then we will grab the webcam reference . <p> I managed to get the Raspberry Pi camera working on my Pi 2 for this example , but the frame rate is only about 5 frames/sec on my output and the video is delayed about 1-2 seconds . Is this expected because of the processing power of the Pi ? Just curious if there is anyway to make it smoother/faster . Thanks ! <p> Very nice , congrats on getting your Pi 2 working for this email ! 5 FPS sounds a tad low for this example though , even given the limited @ @ @ @ @ @ @ @ @ @ to resize the image even further , perhaps having a maximum width of 200-300 pixels . <p> Based on your terminal , I assume you are using the Raspberry Pi . Are you using a USB webcam or the Pi camera module ? I 'm assuming the Pi camera module . In that case , you 'll need to update the code to access the Raspberry Pi camera rather than use the cv2.VideoCapture function . <p> Under the pure definition of the HSV color space , yes , that 's correct . However , OpenCVs cv2.cvtColor function allows the Hue to be in the range 0 , 180 and both the saturation to be in the range 0 , 255 . <p> Thank you for this wonderful blog . I only started looking at the a few posts , but so far Ive learned a lot more than the last 2 weeks that I was randomly looking at different sources . <p> I have a question regarding this post . I have a video of hand washing and I want to detect the act of hand washing . One step is to detect @ @ @ @ @ @ @ @ @ @ on my videos , it can not detect the hands at all . Instead in some of the videos , parts of the wall , that have a light pink color , are detected , and in some other videos nothing is detected at all . <p> My question is does this code work well when there is fast movement of hands like in the case of hand washing ? <p> Another thing that I should mention is that the sink area has also a light color ; can this cause a problem as well ? <p> Do you think this algorithm still works for me by performing some changes or I should try something else ? <p> This method of skin detection relies on color thresholding , which can work well in some situations and fail in others . It mainly depends on lighting conditions and the color of your own skin . You might need to tweak the values to cv2.inRange to obtain a better segmentation of the skin . <p> That said , I do n't  think this method is best for your particular application . Presuming @ @ @ @ @ @ @ @ @ @ use simple motion detection to detect the hands as they go under the water stream . <p> Hi Adrian , first of all , thanks for the tutorial . I am using the Raspberry Pi and the Pi camera module . in other reply , you said it-s necessary to update the code to access the Raspberry Pi camera rather than use the cv2.VideoCapture function , but what code I have to add or change in skindetector.py ? <p> You 'll need to change the camera initialization and a function call that actually reads a frame from the video stream . I suggest using the VideoStream class which can access both a builtin/USB webcam and the Raspberry Pi camera module with minimal changes . <p> There are many different ways to perform segmentation , and yes , Bayesian methods can be applied for this . However , I do not have any tutorials on this particular method I cant promise I 'll cover it in the future , but I can certainly look into it . <p> Hello Adrian , Thanks for the tutorial . I have 2 questions for you . @ @ @ @ @ @ @ @ @ @ and excellent marketing jobs . 2- I have sent you an e-mail about buying your book but maybe for a specific tutorial ( object recognition and feature detection ) <p> Hi Adrian , I 'm pretty new in cv , anyway I 'm very excited of such field , thanks a lot for your wonderful blog . I have a question about the code showed above , I know that dilate is the complementary operation of erode , so what is the meaning of those two lines of code ? Why they do n't  elude each other ? <p> Performing an erosion will remove small speckles of noise from your binary map . But then you apply a dilation to grow back the regions of actual objects you 're interested in . In essence , an erosion followed by a dilation will remove noise from your images . <p> Certainly , there are many methods that can be used to detect hands in images . Some methods rely on machine learning and treat the problem as an object detection task ( like Haar cascades , HOG + Linear SVM detectors , or CNN @ @ @ @ @ @ @ @ @ @ detection such as stereo vision and 3D cameras it really just depends on the actual problem and the setup . <p> I do n't  have any experience with fingernail detection but I would n't rely on color thresholding due to varying skin tones . Have you considered training a custom object detector ? With a fixed camera this should be fairly straightforward . <p> I was wondering is this face detection possible with help of neural-networks do you any books ot tutorials for that like your Practical Python and OpenCV ? If there are any would really appreciate if you could mail me some links or blogs for some reference .. <p> You can certainly build a face detector using neural networks ; however , most current methods that you see in OpenCV or other computer vision libraries rely on either Haar cascades ( covered in Practical Python and OpenCV ) or HOG + Linear SVM detectors ( covered in the PyImageSearch Gurus course ) . <p> Ill be announcing a new book soon which will cover neural networks and deep learning so be sure to be on the lookout for @ @ @ @ @ @ @ @ @ @ ! But is there any other reference you have anything that can get me a jump-start for Neural Networks , with a level of your explanation xD ? Also , I tried using your range-detector script mentioned in this blog , I was using webcam , 2 windows pop for output one is original and other is thres .. But thres is always blank can you tell me exactly how to use it .. I am kinda new to all this but suprisingly ComputerVision is quite interesting ! Thank you <p> Hi Adrian- when I downloaded skindetection.zip , it landed in downloads . I unzipped it there , but I 'm pretty sure I needed to extract it to some other spot , because the program ca n't find the pyimagesearch module . Could not locate this problem in the other replies , so I 'm bugging you for some direction . Where @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485258 @185258/ <h> Tag Archives color detection <p> This is the final post in our three part series on shape detection and analysis . Previously , we learned how to : Compute the center of a contour Perform shape detection &amp; identification Today we are going to perform both- shape detection and- color labeling on objects in images . At this point , we understand that regions of an image <p> This past Saturday , I was caught in the grips of childhood nostalgia , so I busted out my PlayStation 1 and my original copy of Final Fantasy VII . As a kid in late middle school/early high school , I logged 70+ hours playing through this heartbreaking , inspirational , absolute masterpiece of an RPG . As a kid in middle <p> Today marks the- 100th blog post on PyImageSearch. 100 posts. - Its hard to believe it , but its true . When I started PyImageSearch back in January of 2014 , I had no idea what the blog would turn into . I did n't  know how it would evolve and mature . And I most certainly @ @ @ @ @ @ @ @ @ @ here I am . Riding the Amtrak 158 train , coming home after a long business trip . Its hot . The AC is barely working . A baby is screaming right next to me while the accompanying mother looks forlornly out the window , clearly questioning whether or not having a child was the right @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485259 @185259/ <p> The numerator of this equation computes the distance between the vertical eye landmarks while the denominator computes the distance between horizontal eye landmarks , weighting the denominator appropriately since there is only one set of horizontal points but- two sets of vertical points . <p> Why is this equation so interesting ? <p> Well , as well find out , the eye aspect ratio is approximately constant while the eye is open , but will rapidly fall to zero when a blink is taking place . <p> Using this simple equation , we can- avoid image processing techniques and simply rely on the- ratio of eye landmark distances to determine if a person is blinking . <p> To make this more clear , consider the following figure from- Soukupov+ and -ech : <p> Figure 5:Top-left : A visualization of eye landmarks when then the eye is open . Top-right : Eye landmarks when the eye is closed . Bottom : Plotting the eye aspect ratio over time . The dip in the eye aspect ratio- indicates a blink ( Figure 1 of Soukupov+ and -ech ) . <p> @ @ @ @ @ @ @ @ @ @ open the eye aspect ratio here would be large(r) and relatively constant over time . <p> The- bottom figure plots a graph of the eye aspect ratio over time for a video clip . As we can see , the eye aspect ratio is constant , then rapidly drops close to zero , then increases again , indicating a single blink has taken place . <p> In our next section , well learn how to implement the eye aspect ratio for blink detection using facial landmarks , OpenCV , Python , and dlib . <h> Detecting blinks with facial landmarks and OpenCV <p> To get started , open up a new file and name it detectblinks.py- . From there , insert the following code : <p> Eye blink detection with OpenCV 9 <p> 10 <p> 11 55203 @qwx675203 <p> fromscipy.spatial importdistance asdist <p> fromimutils.video importFileVideoStream <p> fromimutils.video importVideoStream <p> fromimutils importfaceutils 55220 @qwx675220 55218 @qwx675218 55219 @qwx675219 <p> importtime <p> importdlib <p> importcv2 <p> To access @ @ @ @ @ @ @ @ @ @ built-in webcam/USB camera/Raspberry Pi camera module ( VideoStream- ) , well need to use my imutils library , a set of convenience functions to make working with OpenCV easier . <p> If you do not have imutils- installed on your system ( or if you 're using an older version ) , make sure you install/upgrade using the following command : <p> Eye blink detection with OpenCV , Python , and dlib <p> Shell <p> 1 <p> $pip install--upgrade imutils <p> Note : If you are using Python virtual environments ( as all of my OpenCV install tutorials do ) , make sure you use the workon- command to access your virtual environment first and then install/upgrade imutils- . <p> Otherwise , most of our imports are fairly standard the exception is dlib , which contains our implementation of facial landmark detection . <p> Finally , Line 24 combines both the numerator and denominator to arrive at the final eye aspect ratio , as described in- Figure 4- above . <p> Line 27 then returns the eye aspect ratio to the calling function . <p> Let 's go ahead and parse our @ @ @ @ @ @ @ @ @ @ , Python , and dlib <p> Python <p> 29 <p> 30 <p> 31 <p> 32 <p> 33 <p> 34 <p> 35 55202 @qwx675202 55206 @qwx675206 <p> LONG ... <p> help= " path to facial landmark predictor " ) <p> LONG ... <p> help= " path to input video file " ) 55208 @qwx675208 <p> Our detectblinks.py- script requires a single command line argument , followed by a second optional one : <p> --shape-predictor- : This is the path to dlibs pre-trained facial landmark detector . You can download the detector along with the source code + example videos to this tutorial using the- " Downloads " section of the bottom of this- blog post . <p> --video- : This optional switch controls the path to an input video file residing on disk . If you instead want to work with a- live video stream , simply omit this switch when executing the script . <p> We now need to set two important constants that you may need to tune for your own implementation , along with initialize two other important variables , - so be sure to pay attention to @ @ @ @ @ @ @ @ @ @ Python , and dlib <p> Python <p> 37 <p> 38 <p> 39 <p> 40 <p> 41 <p> 42 <p> 43 <p> 44 <p> 45 <p> # define two constants , one for the eye aspect ratio to indicate <p> # blink and then a second constant for the number of consecutive <p> # frames the eye must be below the threshold <p> EYEARTHRESH=0.3 <p> EYEARCONSECFRAMES=3 <p> # initialize the frame counters and the total number of blinks <p> COUNTER=0 <p> TOTAL=0 <p> When determining- if a blink is taking place in a video stream , we need to calculate the eye aspect ratio . <p> If the eye aspect ratio falls below a certain threshold and then rises above the threshold , then well register a " blink " the EYEARTHRESH- is this threshold value . We default it to a value of 0.3- as this is what has worked best for my applications , but you may need to tune it for your own application . <p> We then have an important constant , EYEARCONSECFRAME- this value is set to 3- to indicate that three- successive frames with an @ @ @ @ @ @ @ @ @ @ for a blink to be registered . <p> Again , depending on the frame processing throughput rate of your pipeline , you may need to raise- or lower this number for your own implementation . <p> Lines 44 and 45 initialize two counters . COUNTER- is the total number of successive frames that have an eye aspect ratio less than EYEARTHRESH- while TOTAL- is the total number of blinks that have taken place while the script has been running . <p> Now that our imports , command line arguments , and constants have been taken care of , we can initialize dlibs face detector and facial landmark detector : <p> Using our array slicing techniques from earlier in this script , we can extract the- ( x , y ) -coordinates for both the left and right eye , respectively ( Lines 94 and 95 ) . <p> From there , we compute the eye aspect ratio for each eye on- Lines 96 and 97 . <p> Following the suggestion of- Soukupov+ and -ech , we average the two eye aspect ratios together to obtain a better blink estimate ( @ @ @ @ @ @ @ @ @ @ the same time , of course ) . <p> At this point we have computed our ( averaged ) eye aspect ratio , but we have n't actually determined if a blink has taken place this is taken care of in the next section : <p> Eye blink detection with OpenCV , Python <p> 122 <p> 123 <p> # check to see if the eye aspect ratio is below the blink <p> # threshold , and if so , increment the blink frame counter <p> ifear&lt;EYEARTHRESH : <p> COUNTER+=1 <p> # otherwise , the eye aspect ratio is not below the blink <p> # threshold <p> else : <p> # if the eyes were closed for a sufficient number of <p> # then increment the total number of blinks <p> **31;11442;TOOLONG : <p> TOTAL+=1 <p> # reset the eye frame counter <p> COUNTER=0 <p> Line 111 makes a check to see if the eye aspect ratio is below our blink @ @ @ @ @ @ @ @ @ @ consecutive frames that indicate a blink is taking place ( Line 112 ) . <p> Otherwise , - Line 116 handles the case where the eye aspect ratio- is not below the blink threshold . <p> In this case , we make another check on- Line 119 to see if a sufficient number of- consecutive frames contained an eye blink ratio below our pre-defined threshold . <p> If the check passes , we increment the TOTAL- number of blinks ( Line 120 ) . <p> We then reset the number of consecutive blinks COUNTER- ( Line 123 ) . <p> Our final code block simply handles drawing the number of blinks on our output frame , as well as displaying the current eye aspect ratio : <p> Eye blink detection with OpenCV , Python <p> 141 <p> 142 <p> # draw the total number of blinks on the frame along with <p> # @ @ @ @ @ @ @ @ @ @ ( frame , " Blinks : " . format(TOTAL) , ( 10,30 ) , <p> **26;11475;TOOLONG , ( 0,0,255 ) , 2 ) <p> cv2.putText ( frame , " EAR : : .2f " . format(ear) , ( 300,30 ) , <p> **26;11503;TOOLONG , ( 0,0,255 ) , 2 ) <p> # show the frame <p> cv2.imshow ( " Frame " , frame ) <p> **27;11531;TOOLONG <p> # if the q key was pressed , break from the loop <p> ifkey==ord ( " q " ) : <p> break <p> # do a bit of cleanup <p> cv2.destroyAllWindows() <p> vs.stop() <p> To see our eye blink detector in action , proceed to the next section . <h> Blink detection results <p> Before executing any of these examples , be sure to use the- " Downloads " section of this guide to download the source code + example videos + pre-trained dlib facial landmark predictor . From there , you can unpack the archive and start playing with the code . <p> Over this past weekend I was traveling out to Las Vegas for a conference . While I was waiting @ @ @ @ @ @ @ @ @ @ gate and put together the code for this blog post this involved recording a simple video of myself that I could use to evaluate the blink detection software . <p> To apply- our blink detector to the example video , just execute the following command : <p> Eye blink detection with OpenCV , Python , and dlib <p> Shell <p> 1 <p> 2 <p> 3 <p> $python detectblinks.py <p> --shape-predictor **33;11560;TOOLONG <p> --video blinkdetectiondemo.mp4 <p> And as you 'll see , we can successfully count the number of blinks in the video using OpenCV and facial landmarks : <p> Later , at my hotel , I recorded a live stream of the blink detector in action and turned it into a screencast . <p> To access my built-in webcam I executed the following command ( taking care to uncomment the correct VideoStream- class , as detailed above ) : <p> Eye blink detection with OpenCV , Python , and dlib <p> Shell <p> 1 <p> 2 <p> $python detectblinks.py <p> --shape-predictor **33;11595;TOOLONG <p> Here is the output of the live blink detector along with my commentary : <h> Improving our blink @ @ @ @ @ @ @ @ @ @ eye aspect ratio as a quantitative metric to determine if a person has blinked in a video stream . <p> However , due to noise in a- video stream , subpar facial landmark detections , or fast changes in viewing angle , a simple threshold on the eye aspect ratio could produce a false-positive detection , reporting that a blink had taken place when in reality the person had not blinked . <p> To make our blink detector more robust to these challenges , - Soukupov+ and -ech recommend : <p> Soukupov+ and -ech report that the combination of the temporal-based feature vector and SVM classifier helps reduce false-positive blink detections and improves the overall accuracy of the blink detector . <h> Summary <p> In this blog post I demonstrated how to build a blink detector using OpenCV , Python , and dlib . <p> The first step in building a blink detector is to perform facial landmark detection- to- localize the eyes in a given frame from a video stream . <p> Once we have the facial landmarks for both eyes , we compute the- eye aspect ratio- for- @ @ @ @ @ @ @ @ @ @ relating the distances between the vertical eye landmark points to the distances between the horizontal landmark points . <p> Once we have the eye aspect ratio , we can threshold it to determine if a person is blinking - the eye aspect ratio will remain approximately constant when the eyes are open and then will rapidly approach zero during a blink , then increase again as the eye opens . <p> Of course , a natural extension of blink detection is- drowsiness detection which well be covering in the next two weeks here on the PyImageSearch blog . <p> To be notified when the drowsiness detection tutorial is published , be sure to enter your email address in the form below ! <h> Downloads : 55217 @qwx675217 <p> Hi Adrian . I have the same issue with the blink detection : INFO loading facial landmark predictor INFO starting video stream thread and then the prompt . ( The code for " **26;11630;TOOLONG " works fine ) <p> Hi Adrian . Thanks for your great job ! ! I found the cause of the issue . I missed to uncomment two @ @ @ @ @ @ @ @ @ @ 68 : fileStream = False ( So we can use the built-in webcam or USB cam , as you say in the blog ( in your instructions are lines 63 and 64 but in the downloaded code are 66 and 68 ) I Hope this could help HAILI and RAVIVARMAN RAJENDIRAN Thanks a lot for this blog <p> Great article , this is really impressive . In my case , my eye aspect ratio never goes much above 0.3 no matter how wide I open my eyes . Also , I missed some blinks with the 3 frame setting , maybe your frame rate is higher than mine . This is what works better on my system : <p> It is interesting to note that the green outlines around my eye always seem to show both eyes are roughly the same amount open , even when one eye is completely wide open and the other eye is entirely shut . Looks like the facial landmark detector ( HOG ) is making some assumption that the face should be symmetric , so both eyes should be about the same . <p> @ @ @ @ @ @ @ @ @ @ is NOT HOG-based . Instead it interprets these landmarks as probabilities and attempts to fit a model to it . You can read more about the facial landmark detector here . <p> Hi Adrian , thanks for the detailed tutorial ! I downloaded the zip file and executed the required command in terminal . I got this error : RuntimeError : Unable to open **33;11686;TOOLONG I 'm using a macOS Sierra . can you please help me figure this out . <p> Hi Wallace I do n't  cover Windows here on the PyImageSearch blog . I recommend using Unix-based operating systems such as macOS and Ubuntu for computer vision development . IF you would like to install dlib on Windows , please refer to the official dlib site . <p> Ill be covering YOLO along with Faster R-CNNs and SSDs inside Deep Learning for Computer Vision with Python . Object detection with deep learning is still a very volatile field of research . Ill be discussing how these frameworks work and how to use them , but a pure Python implementation will be outside the scope of the book . <p> @ @ @ @ @ @ @ @ @ @ object detections are based on forks of libraries like Caffe or mxnet . The authors then implement custom layers . It will likely be a few years until we see these types of object detectors ( or even the building blocks ) naturally existing in a stable state inside Keras , mxnet , etc . <p> These are the individual indexes of the ( x , y ) -coordinates of the eyes . These indexes map to the equation in Figure 4 above ( keep in mind that the equation is one-indexed while Python is zero-indexed ) . Furthermore , you can read more about the individual facial landmarks in this post . <p> Great post ! Tested this one with mobile phone ( iPhone ) and it works okay . dlib detection is a bit problematic on iphone camera so sometimes it does n't  detect blinks because of bad lighting . <p> I like the simple idea behind it . I tried to apply this idea to " eyebrow raise " detection mechanism but the ratio is not changing as drastically as eye ratio . Do you maybe have @ @ @ @ @ @ @ @ @ @ raising or any other facial gesture ? <p> Keep in mind that the eyebrow facial landmarks are only represented by 5 points each they do n't  surround the eyebrow like the facial landmarks do for the eye so the aspect ratio does n't  have much meaning here . I would monitor the ( x , y ) -coordinates of the eyebrows , but otherwise you might want to look into other facial landmark models that can detect more points and encapsulate the entire eyebrow . <p> Hi Adrian , thanks for this guide . As of now , I 'm trying to use this system for wink detection . Right now I 'm gathering data , but what I 've determined is that both EARs decrease by something like 40% no matter which eye winks . Then I determine which EAR decreased more , and which decreased less . I may hard code it , if I can generalize it to a ratio indicating a wink , then a difference indicating WHICH eye is winking . From there , I need to make sure it does n't  give me false positives for blinks @ @ @ @ @ @ @ @ @ @ out ) . <p> Hi ! Great tutorials on the site ! Just to give you a suggestion ; Could you also use an IR lighting rig to light up the subject at night time ? Because most webcams have the capability to detect IR . Thanks ! <p> Thank you so much for sharing and I followed all your steps to create a blink rate monitoring as well as a head tilting monitoring using some other data among the 68 points . All works pretty well but I found that the predictor will fail to predict the accurate eye contour when the object is wearing optics . Do you have any suggestion for that ? Cause I know in openCV haar cascade there seems to be a specific classifier for eyes with glasses . Will there be a specific predictor built for this case ? <p> That 's a great question . If the driver is wearing glasses and the eyes can not be localized properly , you would likely need to create a specific predictor . Another option might be to try an IR camera , but again , @ @ @ @ @ @ @ @ @ @ . I 've never tried this method @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485260 @185260/ <p> Are you ready to start building your first image search engine ? Not so fast ! Let 's first go over some basic image processing and manipulations that will come in handy along your image search engine journey . If you are already an image processing guru , this post will seem pretty boring to you , but give it a read none-the-less you might pick up a trick or two . <p> OpenCV and Python versions : This example will run on- Python 2.7- and OpenCV 2.4 . X/OpenCV 3.0+ . <p> For this introduction to basic image processing , I 'm going to assume that you have basic knowledge of how to create and execute Python scripts . I 'm also going to assume that you have OpenCV installed . If you need help installing OpenCV , check out the quick start guides on the OpenCV website . <p> Go ahead and download this image to your computer . You 'll need it to start playing with some of the Python and OpenCV sample code . <p> Ready ? Here we go . <p> First , let 's load the image @ @ @ @ @ @ @ @ @ @ an Image <p> Python <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> 6 <p> 7 55203 @qwx675203 <p> importcv2 <p> # load the image and show it <p> LONG ... <p> cv2.imshow ( " original " , image ) 55212 @qwx675212 <p> Executing this Python snippet gives me the following result on my computer : <p> Figure 2 : Loading and Displaying the Jurassic Park tour jeep . <p> As you can see , the image is now displaying . Let 's go ahead and break down the code : <p> Line 1 : The first line is just telling the Python interpreter to import the OpenCV package . <p> Line 5 : We are now loading the image off of disk . The imread functions returns a NumPy array , representing the image itself . <p> Line 6 and 7 : A call to imshow displays the image on our screen . The first parameter is a string , the " name " of our window . The second parameter is a reference to the image we loaded off disk on Line 5 . Finally , a @ @ @ @ @ @ @ @ @ @ we press a key on our keyboard . Using a parameter of " 0 " indicates that any keypress will un-pause the execution . <p> Just loading and displaying an image is n't very interesting . Let 's resize this image and make it much smaller . We can examine the dimensions of the image by using the shape attribute of the image , since the image is a NumPy array after-all : <p> Getting the Dimensions of the Image <p> Python <p> 1 <p> printimage.shape <p> When executing this code , we see that ( 388 , 647 , 3 ) is outputted to our terminal . This means that the image has 388- rows , 647- columns , and 3 channels ( the RGB components ) . - When we write matrices , it is common to write them in the form- ( # of rows x # of columns ) which is the same way you specify the matrix size in NumPy . <p> However , when working with images this can become a bit confusing since we normally specify images in terms of- width x height . Looking @ @ @ @ @ @ @ @ @ @ that our image is- 388 pixels wide and- 647 pixels tall . However , this would be incorrect . Our image is actually- 647 pixels wide and- 388 pixels tall , implying that the height is the first entry in the shape and the width is the- second. - This little may be a bit confusing if you 're just getting started with OpenCV and is important to keep in mind . <p> Since we know that our image is 647 pixels wide , let 's resize it and make it 100 pixels wide : <p> <p> 9 <p> 10 <p> # we need to keep in mind aspect ratio so the image does <p> # not look skewed or distorted -- therefore , we calculate <p> # the ratio of the new image to the old image <p> r=100.0/image.shape1 <p> dim= ( 100 , int(image.shape0*r) ) <p> # perform the actual resizing of the image and show it <p> LONG ... <p> cv2.imshow ( " resized @ @ @ @ @ @ @ @ @ @ we can now see that the new resized image is only 100 pixels wide : <p> Figure 3 : We have now resized the image to 100px wide . <p> Let 's breakdown the code and examine it : <p> Line 4 and 5 : We have to keep the aspect ratio of the image in mind , which is the proportional relationship of the width and the height of the image . In this case , we are resizing the image to have a 100 pixel width , therefore , we need to calculate r , the ratio of the new width to the old width . Then , we construct the new dimensions of the image by using 100 pixels for the width , and r x the old image height . Doing this allows us to maintain the aspect ratio of the image . <p> Line 8-10 : The actual resizing of the image happens here . The first parameter is the original image that we want to resize and the second argument is the our calculated dimensions of the new image . The third parameter tells us @ @ @ @ @ @ @ @ @ @ about that for now . Finally , we show the image and wait for a key to be pressed . <p> Resizing an image was n't so bad . Now , let 's pretend that we are the Tyrannosaurus- Rex from the Jurassic Park movie let 's flip this jeep upside down : <p> <p> 9 <p> 10 <p> # grab the dimensions of the image and calculate the center <p> # of the image <p> ( h , w ) =image.shape:2 <p> center= ( w/2 , h/2 ) <p> # rotate the image by 180 degrees <p> **41;11721;TOOLONG <p> **28;11764;TOOLONG , M , ( w , h ) ) <p> cv2.imshow ( " rotated " , rotated ) 55212 @qwx675212 <p> So what does the jeep look like now ? You guessed it flipped upside down . <p> Figure 4 : The jeep has now been flipped upside . We could have also rotated the jeep by any arbitrary angle . <p> This is the most involved @ @ @ @ @ @ @ @ @ @ it down : <p> Line 3 : For convenience , we grab the width and height of the image and store them in their respective variables . <p> Line 4 : Calculate the center of the image we simply divide the width and height by 2 . <p> Line 7 : Compute a matrix that can be used for rotating ( and scaling ) the image . The first argument is the center of the image that we computed . If you wanted to rotate the image around any arbitrary point , this is where you would supply that point . The second argument is our rotation angle ( in degrees ) . And the third argument is our scaling factor in this case , 1.0 , because we want to maintain the original scale of the image . If we wanted to halve the size of the image , we would use 0.5 . Similarly , if we wanted to double the size of the image , we would use 2.0 . <p> Line 8 : Perform the actual rotation , by suppling the image , the rotation matrix @ @ @ @ @ @ @ @ @ @ as Dennis Nedry in Python and OpenCV . All we are doing is slicing arrays . We first supply the- startY and- endY coordinates , followed by the- startX and- endX coordinates to the slice . That 's it . We 've cropped the image ! <p> As a final example , let 's save the cropped image to disk , only in PNG format ( the original was a JPG ) : <p> Saving an Image to Disk using Python and OpenCV <p> Python <p> 1 <p> 2 <p> # write the cropped image to disk in PNG format <p> cv2.imwrite ( " thumbnail.png " , cropped ) <p> All we are doing here is providing the path to the file ( the first argument ) and then the image we want to save ( the second argument ) . Its that simple . <p> Figure 6 : We have now saved thumbnail.png to disk . <p> As you can see , OpenCV takes care of changing formats for us . <p> And there you have it ! Basic image manipulations in Python and OpenCV ! Go ahead and play around with @ @ @ @ @ @ @ @ @ @ Jurassic Park images . <p> If you have any questions , either leave them in the comments or send me an email I will be happy to help out . <h> Downloads : 55217 @qwx675217 <p> I believe numpy.shape returns you number of rows and then number of columns and not the other way round ( which is consistent with how we write matrices ) . Otherwise , 647 would denote the height , not the width . <p> Hi Rich , thanks for the comment . I was just trying to make the point that we normally think of images as width x height . But if you look at the NumPy shape , you 're actually seeing the height first , followed by the width . It is consistent with writing matrices and for numerical processing , but can be a bit confusing if you 're first starting to use OpenCV . <p> Hi Jay , if you want to provide both the width and the height , just change the dim tuple . For example , let 's say I wanted to hardcode that an image should be resized to @ @ @ @ @ @ @ @ @ @ would change my code to be : resized = cv2.resize ( puzzle , ( 32 , 32 ) , interpolation = cv2.INTERAREA ) <p> Hi Interesting page . I ve just signed up for your crash course . Quick question though . What if I want to crop an image not based on a rectangle but on a -say- circle ? ? How can this be done with opencv ? <p> Hi Dave , there are two ways to crop an image based on a circle . The first method would be to take the area of the circle and convert it to a rectangle ( based on the radius ) and crop the image that way . However , a better approach would be to take your circle and apply a bitwise and using the cv2.bitwiseand function . Check out this post and see how the cv2.bitwiseand function is used to crop arbitrary shaped regions from the image . <p> Hi , In the image.shape part , you wrote that the image has 388 columns , 647 rows , but it looks like the image has 388 rows and @ @ @ @ @ @ @ @ @ @ rows , cols , channels ? <p> Hello , thanks a lot ! You make opencv so easy and I have enrolled for the crash course . I believe it will really be of benefit to me . Meanwhile , I have a question , I have a 12 MB image that I want to reduce size , not making it smaller in terms of height and width but rather in terms of disk space . How do I do that with opencv ? Thank you again <p> That all depends on the format of your image . Files such as BMP are lossless but do n't  apply any compression , making them very large . PNG images are also lossless , but apply compression , making them a good choice between file size and image quality . In your case , you might want to convert your image to a JPEG file . <p> Thank you , thank you . I 'm deep into your premium bundle , your crash course , and greatly appreciate this blog . I 'm segmenting overlapping characters from ancient handwritten Chinese family records ( jiapu @ @ @ @ @ @ @ @ @ @ The library has millions of camera images that need to be computer indexed through OCR for online computer searching for family names and histories . I have tried 2d fft to extract features with limited success , and will next try HOG . I have also been reading about Haar cascades . <p> You cant really " crop " a circular region from an image . A bounding box , by definition , is rectangular . If you would like to extract a circular region , you 'll need to first extract the bounding box , then use a circular mask . But again , the image will still appear to be rectangular . <p> Hey Akif you can take a look at this post for an example of masking . You can do the same thing using the cv2.circle function . Otherwise , you should take a look at Practical Python and OpenCV where I discover masking in more detail ( including how to extract a circular area mask ) . <p> Hello , Love the website ! I like to compare various sources and have found your to be @ @ @ @ @ @ @ @ @ @ Thanks for the resource . <p> On the rotate , I think that you took the easy manipulation , 180 degrees . I needed 90 and was surprised that cv2.imshow() does n't  recognize the rotation in its dimensions . One change was forced by transposing the width and height in the cv2.warpAffine() . Yet , the image was offset by half width and half height . Does this need to be re-manipulated in the cv2.imshow() ? <p> Also , if you let the rotation handle the resize , you end up with another scenario entirely . <p> I am just learning openCV and python and understand the conveniences of the resources . So , if this is just part of the system , I 'll accept that . But , how would you handle this ? <p> Thanks for the kind words . And regarding the cv2.imshow and cv2.warpAffine method , I think there is a bit of confusion . The cv2.imshow does n't  care how the image is rotated , how its distorted or what its contents are it simply takes the contents of the NumPy array and displays it @ @ @ @ @ @ @ @ @ @ need to rotate an image and still have all parts of it fit into the size of the allocated NumPy array ( sort of like allocating an array in C/C++ ) , then you 'll need to handle this yourself . If you need to rotate 90 degrees ( or any other degree ) , simply specify the size of the image after the rotation to the cv2.warpAffine method and it will work . See this StackOverflow question for more information . <p> For my project I want to crop and scale video , is it smart to do it the same as mentioned above frame by frame ? I want to reduce the video to run other processes ( for which I 'm using the video ) much faster <p> For this type of application , I would n't suggest OpenCV . If you 're trying to develop a GUI to zoom in on a particular point , OpenCV is overkill and the GUI functionality is n't the best . You would need to create your own custom mouse listeners to detect a click , determine the ROI , and then zoom in on @ @ @ @ @ @ @ @ @ @ work to build with OpenCV , and again , not something I recommend . <p> Hey Sumit can you elaborate more on what you 're trying to do ? If you do n't  know the starting and ending coordinates , you need to " find " them in your image . There are lots of algorithms to do this , but it really depends on what you 're trying to accomplish . <p> You ca n't " crop " a non-rectangular shape from an image . All cropping ROIs must be specified in terms of ( x , y ) -coordinates . That said , you can use masks to define irregular shapes of an image but again , by definition of an image , all croppings are rectangular . <p> I 'm not sure what you mean " crop each face at one time " , but we normally crop a bounding box for each of the faces in the images . That means if you detect 2 faces in an image , you end up with 2 bounding boxes , and therefore 2 cropped regions . Is that what you are trying @ @ @ @ @ @ @ @ @ @ question . <p> LOVE the website ! Instructions are very clear , thanks for spending the time and effort to help people . <p> But I had a problem when following the steps , I used enthought canopy with opencv packages ( 2.4.9-4 ) and I stumble with issue that the python always crash and the window freeze ( deadly spinning beach ball ) when I use the imshow() method . Here is my simple code : <p> Hey Ryan I have n't used Enthought Canopy before so I cant comment on how they have setup their OpenCV distributions . I would instead recommend that you follow my tutorials and install OpenCV from source . <p> An image in OpenCV + Python is simply a NumPy array . Each NumPy array has a . shape . For multi-channel images the shape is a tuple of ( height , width , depth ) . Since we do n't  need the depth we use slicing to only grab the height and width . <p> It depends on your actual application . In most cases , you can treat this as object detection such @ @ @ @ @ @ @ @ @ @ are a number of techniques you can apply to obtain the ( x , y ) -coordinates of the bounding box , but again , this is highly dependent on what you are trying to build . I discuss these different types of method inside Practical Python and OpenCV and inside the PyImageSearch Gurus course . <p> hey Adrian , your blog is too much helpful . Thanks for providing the help . can you please provide the same operations on video ? how to process with videos like separating the frames and background subtraction etc ? <p> 1 . I have one source/original scanned image file , 2 . Also there are scanned copies for same template . 3. but other scanned copies(point:2) are of different size and skewed , I need to deskew and resize same . 4 . I am able to deskew it using your one of post , my query is how to resize scanned copies in to original scan copy size . <p> These are required for OMR . I would appreciate your suggestion , Thanks <h> Trackbacks/Pingbacks <p> you 're wondering about imutils on- @ @ @ @ @ @ @ @ @ @ Image Manipulations in Python and OpenCV , where I go over resizing , rotating , and translating . The imutils.py file in the pyimagesearch <p> Well also be using the imutils- - module , which contains convenience functions for resizing , rotating , and cropping images . You can read more about imutils- - in @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485261 @185261/ <h> Archive OpenCV 3 <p> A few weeks ago , I wrote a blog post on creating transparent overlays with OpenCV . This post was meant to be a gentle introduction to a neat little trick you can use to improve the aesthetics of- your processed image(s) , such as creating a Heads-up Display ( HUD ) on live video streams . But there 's another , more practical <p> Can you believe its been over- four years since the original Raspberry Pi model B was released ? Back then the Pi Model B shipped with only 256MB- of RAM and a 700MHz single core processor . Just over- one year ago the Raspberry Pi 2- was unleashed on the world . And man , for something called a- " Pi " , this beast made an <p> Last weeks blog post taught us how to write videos to file using OpenCV and Python. - This is a great skill to have , but it also raises the question : How do I write video clips containing- interesting events to file rather than the entire video ? In this case @ @ @ @ @ @ @ @ @ @ , distilling <p> Let me just start this blog post by saying that writing to video with OpenCV can be a- huge pain in the ass . My intention with this tutorial is to- help you get started writing videos to file with OpenCV 3 , provide ( and explain ) some boilerplate code , and detail how I got video writing to work on <p> This is the final post in our three part series on shape detection and analysis . Previously , we learned how to : Compute the center of a contour Perform shape detection &amp; identification Today we are going to perform both- shape detection and- color labeling on objects in images . At this point , we understand that regions of an image <p> This tutorial is the second post in our three part series on- shape detection and analysis . Last week we learned how to compute the center of a contour using OpenCV . Today , we are going to leverage contour properties to actually- label and identify shapes in an image , just like in the figure- at the top of this @ @ @ @ @ @ @ @ @ @ a new 3-part series of tutorials on- shape detection and analysis . Throughout this series , well learn how to : Compute the center of a contour/shape region . Recognize various shapes , such as circles , squares , rectangles , triangles , and pentagons using only contour properties . Label the color of a shape . While todays post is <p> One of my favorite parts of running the PyImageSearch blog is a being able to link together previous blog posts and create a solution to a particular problem in this case , real-time panorama and image stitching with Python and OpenCV . Over the past month and a half , we 've learned how to increase the FPS <p> In todays blog post , I 'll demonstrate how to perform image stitching and panorama construction- using Python and OpenCV . Given two images , well " stitch " them together to create a simple panorama , as seen in the example above . To construct our image panorama , well utilize computer vision and image processing techniques such as : keypoint detection and local invariant <p> In this blog post @ @ @ @ @ @ @ @ @ @ Pi Zero . Since I 've covered how to install OpenCV on the Raspberry Pi in- multiple , previous blog posts , I 'll keep this post on the shorter side and detail only the relevant commands necessary to get OpenCV up and @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485262 @185262/ <h> NOW HIRING : Computer Vision and OpenCV Correspondence Coordinator <p> The PyImageSearch blog has grown a lot since I published that- first post back in January 2014 . Its been an incredible journey and I have you , the reader , to thank for all the support over the past 2.5 years . <p> However , due to the whirlwind growth of PyImageSearch , I now receive 100+ emails per day- and its gotten to the point where I cant keep up with them all . <p> Believe it or not , I have been the only employee of PyImageSearch for- the past 2.5 years - and Ive personally answered each and every email I 've received during that time . <p> Interacting with you is honestly one of the highlights of my day . <p> For example : <p> Have you emailed me a question , no matter how basic or advanced , regarding computer vision ? <h> BONUS POINTS IF : <p> You have earned a college-level degree with coursework/final project/thesis involving computer vision , or similar real world experience . <p> You have an awesome StackOverflow @ @ @ @ @ @ @ @ @ @ . <h> AM I RIGHT FOR THIS JOB ? <p> You are the right person for this job if : <p> You enjoy computer vision and like helping others . <p> You want to work with a successful entrepreneur in a small company- and help shape the future of PyImageSearch . <p> Taking this job is more than answering questions regarding computer vision , image processing , and OpenCV . Its a chance to work with me- and learn from me . You do n't  have to be an entrepreneur yourself to apply for this job , but I would expect that you 're interested ( at some level ) in startups and small business . <h> WHEN DOES THIS JOB START ? <p> As soon as possible- I am targeting mid-August as a start date , but I will be flexible if you have circumstances that prevent you from starting then ( you have to be the right fit for the job , of course ) . <h> HOW DO I APPLY ? <p> Ill be accepting applications until July 31st at 11:59PM EST , so make sure you get @ @ @ @ @ @ @ @ @ @ I APPLY ? <p> After you apply , I 'll be personally reviewing your application . <p> From there , I 'll follow up with you over email , ask any additional questions , and if necessary , set up a formal interview , etc. , normally within 3-4 days . <p> Ill also be asking you to answer a few sample emails to get a feel for your writing style and computer vision/OpenCV knowledge . Do n't  worry , there are n't  any " right " or " wrong " responses to these examples I just need to get a feeling of how you write and interact with others . If you applied , be sure to keep an eye on your inbox for this next series of emails ! <p> I have great desire to work as Computer Vision and OpenCV Correspondence Coordinator . I have developed several Computer Vision projects on Raspberry Pi . I have been regular reader of the PyImageSearch blog and I have read through the Practical Python and OpenCV book . I was also a member of PyImageSearch Gurus course . I have published several @ @ @ @ @ @ @ @ @ @ I have also severed as reviewers for several Science Citation Indexed international journals.I have developed infant detection in car using OpenCV and Raspberry Pi . I have already mailed you about my interest , please consider my request . <p> I wish I could apply for coordinator , having lots of experience with Raspberry Pi and OpenCV but I am too much busy with PCL now a days . It feels really great by helping others to learn and share your wisdom . I wish you best of luck and we all are looking forward to see a mini-Adrian in coming days = <p> Hi , Adrian it was nice to hear from you and I would be glad working with you . I have worked with open source computer vision and you have helped me with your tutorials . Thanks to you that I have completed the projects related to openCV . Looking forward to work with you . <p> Hi , Adrian . July 30 , 2016 I have applied for the part time job through the website by the link you 've provided and sent my resume via @ @ @ @ @ @ @ @ @ @ hearing from you . Thanks . <p> I know I have missed the deadline but please consider this . I am quite a late bloomer when it comes to your website . Unlucky me ! <p> I would like to volunteer and work along with your team in this awesome adventure . Right now , I am doing my MS in United States in Signal Processing . I am a computer vision and machine learning enthusiast . I love python and raspberry pie . <p> Please consider my application as I really want to volunteer . I need no payment and will work 2-4 hrs @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485265 @185265/ <p> Notice how we are reusing the shapedetector.py- and ShapeDetector- class from our previous blog post . Well also create a new file , colorlabeler.py- , that will be used to tag image regions with a- text label of a color . <p> Finally , the detectcolor.py- driver script will be used to glue all the pieces together . <p> Before you continue working through this post , make sure that you have the imutils Python package installed on your system : <p> Determining object color with OpenCV <p> Shell <p> 1 55204 @qwx675204 <p> Well be using various functions inside this library through the remainder of the lesson . <h> Labeling colors in images <p> The first step in this project is to create a Python class that can be used to label shapes in an image with their associated color . <p> To do this , let 's define a class named ColorLabeler- in the colorlabeler.py- file : <p> We then dive into the constructor on- Line 8 . To start , we need to initialize a colors dictionary ( Lines 11-14 ) that specifies the mapping of @ @ @ @ @ @ @ @ @ @ to the- RGB tuple ( the- value of the dictionary ) . <p> From there , we allocate memory for a NumPy array to store these colors , followed by initializing the list of color names ( Lines 18 and 19 ) . <p> The next step is to loop over the colors- dictionary , followed by updating the NumPy array and the colorNames- list , respectively ( Lines 22-25 ) . <p> Well , in order to actually label and tag regions of an image as containing a certain color , well be computing the Euclidean distance between our dataset of- known colors ( i.e. , the lab- array ) and the- averages of a particular image region . <p> The known color that minimizes the Euclidean distance will be chosen as the color identification . <p> And unlike HSV and RGB color spaces , the Euclidean distance between L*a*b* colors has- actual perceptual meaning- hence well be using it in the remainder of this post . <p> The next step is to define the- label- method : <p> Determining object <p> 53 <p> 54 <p> deflabel ( self , image , c ) : <p> # construct a mask for the contour , then compute the <p> # average L*a*b* value for the masked region <p> **27;11794;TOOLONG , dtype= " uint8 " ) <p> cv2.drawContours ( mask , c , -1,255 , -1 ) <p> mask=cv2.erode ( mask , None , iterations=2 ) <p> mean=cv2.mean ( image , mask=mask ) : 3 <p> # initialize the minimum distance found thus far <p> minDist= ( np.inf , None ) <p> # loop over the known L*a*b* color values <p> for ( i , row ) inenumerate(self.lab) : <p> # compute the distance between the current L*a*b* <p> # color value and the mean of the image <p> d=dist.euclidean ( row0 , mean ) <p> # if the distance is smaller than the current distance , <p> # then update the bookkeeping variable <p> ifd&lt;minDist0 : <p> minDist= ( d , @ @ @ @ @ @ @ @ @ @ with the smallest distance <p> **29;11823;TOOLONG <p> The label- method requires two arguments : the L*a*b* image- containing the shape we want to compute color channel statistics for , followed by c- , the contour region of the image- we are interested in . <p> Lines 34 and 35 construct a mask for contour region , an example of which we can see below : <p> Figure 1 : - ( Right ) The original image . ( Left ) The mask image for the blue pentagon at the bottom of the image , indicating that we will only perform computations in the " white " region of the image , ignoring the black background . <p> Notice how the- foreground region of the mask- is set to white , while the- background is set to black . Well only perform computations within the masked ( white ) region of the image . <p> We also erode the mask slightly to ensure statistics are- only being computed for the masked region and that- no background is accidentally included ( due to a non-perfect segmentation of the shape from the original @ @ @ @ @ @ @ @ @ @ the mean ( i.e. , average ) for each of the L* , a* , and *b* channels of the image- for only the mask- ed region . <p> Finally , - Lines 43-51 handles looping over each row of the lab- array , computing the Euclidean distance between each known color and the average color , and then returning the name of the color with the smallest Euclidean distance . <h> Defining the color labeling and shape detection process <p> Now that we have defined our ColorLabeler- , let 's create the detectcolor.py- driver script . Inside this script well be combining- both our ShapeDetector- class from last week and the ColorLabeler- from todays post . <p> Lines 9-12- then parse our command line arguments . Like the other two posts in this series , we only need a single argument : the --image- path where the image we want to process lives on disk . <p> Next up , we can load the image and process it : <p> Determining object <p> 33 <p> 34 <p> # load the image and resize it to a smaller factor so that <p> # the shapes can be approximated better <p> image=cv2.imread ( args " image " ) <p> **28;11854;TOOLONG , width=300 ) <p> **40;11884;TOOLONG <p> # blur the resized image slightly , then convert it to both <p> # grayscale and the L*a*b* color spaces <p> **32;11926;TOOLONG , ( 5,5 ) , 0 ) <p> **25;11960;TOOLONG , cv2.COLORBGR2GRAY ) <p> lab=cv2.cvtColor ( blurred , cv2.COLORBGR2LAB ) <p> LONG ... <p> # find contours in the thresholded image <p> LONG ... 55211 @qwx675211 <p> **36;11987;TOOLONG <p> # initialize the shape detector and color labeler <p> sd=ShapeDetector() <p> cl=ColorLabeler() <p> Lines 16-18 handle loading the image from disk and then creating a resized- version of it , keeping track of the ratio- of the original height to the resized height . We resize the image so that our contour approximation is more accurate for shape identification . Furthermore , the smaller the image is , the less data there is @ @ @ @ @ @ @ @ @ @ <p> Lines 22-25- apply Gaussian smoothing to our resized image , converting to grayscale and L*a*b* , and finally thresholding to reveal the shapes in the image : <p> Figure 2 : Thresholding is applied to segment the background from the foreground shapes . <p> We find the contours ( i.e. , outlines ) of the shapes on- Lines 29-30 , taking care of to grab the appropriate tuple value of cnts- based on our OpenCV version . <p> We are now ready to detect both the- shape and- color of each object in the image : <p> Determining object <p> 60 <p> 61 <p> # loop over the contours <p> forcincnts : <p> # compute the center of the contour <p> M=cv2.moments(c) <p> cX=int ( ( M " m10 " /M " m00 " ) *ratio ) <p> @ @ @ @ @ @ @ @ @ @ " ) *ratio ) <p> # detect the shape of the contour and label the color <p> shape=sd.detect(c) <p> color=cl.label ( lab , c ) <p> # multiply the contour ( x , y ) -coordinates by the resize ratio , <p> # then draw the contours and the name of the shape and labeled <p> # color on the image <p> c=c.astype ( " float " ) <p> c*=ratio <p> c=c.astype ( " int " ) <p> text= " " . format ( color , shape ) <p> cv2.drawContours ( image , c , -1 , ( 0,255,0 ) , 2 ) <p> cv2.putText ( image , text , ( cX , cY ) , <p> **26;12025;TOOLONG , ( 255,255,255 ) , 2 ) <p> # show the output image <p> cv2.imshow ( " Image " , image ) 55212 @qwx675212 <p> We start looping over each of the contours on- Line 38 , while- Lines 40-42- compute the center of the shape . <p> Using the contour , we can then detect the shape- of the object , followed by determining its color- on- Lines 45 and 46 @ @ @ @ @ @ @ @ @ @ outline of the current shape , followed by the color + text label on the output image . <p> Lines 60 and 61- display the results to our screen . <h> Color labeling results <p> To run our shape detector + color labeler , just download the source code to the post using the form at the bottom of this tutorial and execute the following command : <p> Determining object color with OpenCV <p> Shell <p> 1 <p> $python detectcolor.py--image exampleshapes.png <p> Figure 3 : Detecting the shape and labeling the color of objects in an image . <p> As you can see from the GIF above , each object has been correctly identified both in terms of- shape and in terms of- color . <h> Limitations <p> One of the primary drawbacks to using the method presented in this post to- label colors is that due to lighting conditions , along with various hues and saturations , colors- rarely look like- pure red , green , blue , etc . <p> You can often identify small sets of colors using the L*a*b* color space and the Euclidean distance , @ @ @ @ @ @ @ @ @ @ return incorrect results depending on the complexity of your images . <p> So , that being said , how can we- more reliably label colors in images ? <p> Perhaps there is a way to- " learn " what colors " look like " in the real-world . <p> Indeed , there is . <p> And that 's exactly what I 'll be discussing in a future blog post . <h> Summary <p> Today is the final post in our three part series on shape detection and analysis . <p> While this method works for small color sets in semi-controlled lighting conditions , it will likely not work for larger color palettes in less controlled environments . As I hinted at in the " Limitations " section of this post , there is actually a way for us to " learn " what colors " look like " in the real-world . Ill save the discussion of this method for a future blog post . <p> So , what did you think of this series of blog posts ? Be sure to let me know in the comments section . <p> And @ @ @ @ @ @ @ @ @ @ form below to be notified when new posts go live ! <h> Downloads : 55217 @qwx675217 <h> 33 Responses to Determining object color with OpenCV <p> I 've worked with RGB and HSV quite a bit , but I never used LAB . Knowing now that it makes Euclidean distance meaningful in perceptual space means that I 'll stop trying to shoehorn RGB or HSV Euclidean distance into tasks for which they 're ill suited . Thanks ! <p> Ive been working for detecting seven colors and I want to do it in less controlled environments . I 've tried my best , but I cant find a solution . Then I found your blog post . I 'm very looking forward to your method to solve this problem . Thanks for sharing ! <p> I have been working on my school project on detecting object and color concurrently using live feed video , would that be fine for you to guide me through to get my code done ? Your help will be much appreciated . <p> In general , yes , the L*a*b* color space is better at handling lighting condition variations . @ @ @ @ @ @ @ @ @ @ based on what you are trying to accomplish . Since I do n't  know that I would suggest experimenting with and without the Value component in your Euclidean distance and look at the results . <p> hey , i just had a minor doubt . You used cvtColor command to convert BGR and RGB to LAB . But , as mentioned in the official documentation , it actually returns 2.55*L , a+128 , b+128 and not Lab . Does the euclidean distance have meaning in this space , or else , should n't the values returned by this command be converted to get the actual Lab values ? <p> The color ranges you should use are dependent on your lighting conditions . I would suggest using the range-detector script to help you narrow down on the proper color thresholding values . You should also read this blog post as well . <p> I will use this code as a subroutine in my main python code . How can I call this code in my main python code ? For example , in main python code , I want to say like @ @ @ @ @ @ @ @ @ @ for this . <p> You would need to define your own function that encapsulates this code . I would highly recommend that you spend a little time brushing up on Python and how to define your own functions before continuing , otherwise you may find yourself running into many errors and unsure how to solve them . Let me know if you need any good resources to learn the Python programming language . <p> Hey Julien can you elaborate more on what you mean by " color name of an ROI " ? This blog post demonstrates how to take a region of an image and determine the color name . I 'm not sure how this is different from what you want to accomplish ? <p> I just started to learn python programming + OpenCV , aiming an Engineering Final project for my university , and your examples and tutorials posted here are helping a lot ! I was wondering where can i download the image you utilized here in this tutorial ? <p> Hi Fiona I have not used Windows in over 10+ years and do not officially support @ @ @ @ @ @ @ @ @ @ PyImageSearch reader can help you out , otherwise I highly recommend that you use a Unix-based development environment such as Linux or macOS for computer vision development . <p> Hey Adrian , thanks for your efforts I need to define a scale for colors , i.e between ( 255,0,0 ) and ( 255,255,0 ) I could n't find a way for this " ColorLabeler " to transform it . Also no other post on your blog satisfies it . Can @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485266 @185266/ <h> Image Pyramids with Python and OpenCV <p> It 's too damn cold up in Connecticut- so cold that I had to throw in the towel and escape for a bit . <p> Last week I took a weekend trip down to Orlando , FL just to escape . And while the weather was n't perfect ( mid-60 degrees Fahrenheit , cloudy , and spotty rain , as you can see from the photo above ) , it was exactly 60 degrees warmer than it is in Connecticut- and that 's all that mattered to me . <p> While I did n't make it to Animal Kingdom or partake in any Disney adventure rides , I did enjoy walking Downtown Disney and having drinks at each of the countries in Epcot . <p> Sidebar : Perhaps I 'm biased since I 'm German , but German red wines are perhaps some of the most under-appreciated wines there are . Imagine having the full-bodied taste of a Chianti , but slightly less acidic . Perfection . If you 're ever in Epcot , be sure to check out the German wine @ @ @ @ @ @ @ @ @ @ to fly back from the warm Florida paradise to the Connecticut tundra , I started thinking about what the next blog post on PyImageSearch was going to be . <p> Really , it should not have been that long ( or hard ) of an exercise , but it was a 5:27am flight , I was still half asleep , and I 'm pretty sure I still had a bit of German red wine in my system . <p> After a quick cup of ( terrible ) airplane coffee , I decided on a 2-part blog post : <p> You see , a few months ago I wrote a blog post on utilizing the Histogram of Oriented Gradients image descriptor and a Linear SVM to detect objects in images . This 6-step framework can be used to easily train object classification models . <p> A critical aspect of this 6-step framework involves image pyramids and sliding windows . <p> Today we are going to review two ways to create image pyramids using Python , OpenCV , and sickit-image . And next week we 'll discover the simple trick to create @ @ @ @ @ @ @ @ @ @ we can start to glue together the pieces of our HOG + Linear SVM framework so you can build object classifiers of your own ! <h> What are image pyramids ? <p> Figure 1 : An example of an image pyramid . At each layer of the pyramid the image is downsized and ( optionally ) smoothed ( image source ) . <p> An " image pyramid " is a multi-scale representation of an image . <p> Utilizing an image pyramid allows us to find objects in images at different scales of an image . And when combined with a sliding window we can find objects in images in various locations . <p> At the bottom of the pyramid we have the original image at its original size ( in terms of width and height ) . And at each subsequent layer , the image is resized ( subsampled ) and optionally smoothed ( usually via Gaussian blurring ) . <p> The image is progressively subsampled until some stopping criterion is met , which is normally a minimum size has been reached and no further subsampling needs to take place @ @ @ @ @ @ @ @ @ @ OpenCV <p> In fact , this is the exact same image pyramid implementation that I utilize in my own projects ! <p> Let 's go ahead and get this example started . Create a new file , name it helpers.py- , and insert the following code : 18 <p> 19 <p> 20 55203 @qwx675203 55219 @qwx675219 <p> defpyramid ( image , scale=1.5 , minSize= ( 30,30 ) ) : <p> # yield the original image <p> yieldimage <p> # keep looping over the pyramid <p> whileTrue : <p> # compute the new dimensions of the image and resize it <p> **25;12081;TOOLONG <p> **26;12108;TOOLONG , width=w ) <p> # if the resized image does not meet the supplied minimum <p> # size , then stop constructing the pyramid <p> LONG ... <p> break <p> # yield the next image in the pyramid <p> yieldimage <p> We start by @ @ @ @ @ @ @ @ @ @ image processing convenience functions that are commonly used such as resizing , rotating , translating , etc . You can read more about the- imutils- - package here . You can also grab it off my GitHub . The package is also pip-installable : <p> Image Pyramids with Python and OpenCV <p> Shell <p> 1 55204 @qwx675204 <p> Next up , we define our pyramid- - function on Line 4 . This function takes two arguments . The first argument is the scale- , which controls by how much the image is resized at each layer . A small scale- - yields more layers in the pyramid . And a larger scale- - yields less layers . <p> Secondly , we define the minSize- , which is the minimum required- width and height- of the layer . If an image in the pyramid falls below this minSize- , we stop constructing the image pyramid . <p> Line 6 yields the original image in the pyramid ( the bottom layer ) . <p> From there , we start looping over the image pyramid on Line 9 . <p> Lines 11 and @ @ @ @ @ @ @ @ @ @ next layer of the pyramid ( while preserving the aspect ratio ) . This scale is controlled by the scale- - factor . <p> On Lines 16 and 17 we make a check to ensure that the image meets the minSize- - requirements . If it does not , we break from the loop . <p> Finally , Line 20 yields our resized image . <p> But before we get into examples of using our image pyramid , let 's quickly review the second method . <h> Method #2 : Image pyramids with Python + scikit-image <p> The second method to image pyramid construction utilizes Python and scikit-image . The scikit-image library already has a built-in method for constructing image pyramids called pyramidgaussian- , which you can read more about here . <p> here 's an example on how to use the pyramidgaussian- - function in scikit-image : 8 <p> 9 <p> # METHOD #2 : Resizing + Gaussian smoothing . <p> LONG ... <p> # if the @ @ @ @ @ @ @ @ @ @ LONG ... <p> break <p> # show the resized image <p> cv2.imshow ( " Layer " . format(i+1) , resized ) 55212 @qwx675212 <p> Similar to the example above , we simply loop over the image pyramid and make a check to ensure that the image has a sufficient minimum size . Here we specify downscale=2- - to indicate that we are halving the size of the image at each layer of the pyramid . <h> Image pyramids in action <p> Now that we have our two methods defined , let 's create a driver script to execute our code . Create a new file , name it pyramid.py- , and let 's get to work : 32 <p> 33 @ @ @ @ @ @ @ @ @ @ 55218 @qwx675218 <p> importcv2 <p> # construct the argument parser and parse the arguments 55206 @qwx675206 <p> LONG ... to the image " ) <p> Next up , we need to parse some command line arguments on Lines 9-11 . Our script requires only two switches , --image- , which is the path to the image we are going to construct an image pyramid for , and --scale- , which is the scale factor that controls how the image will be resized in the pyramid . <p> Line 14 loads then our image from disk . <p> We can start utilize our image pyramid Method #1 ( my personal method ) on Lines 18-21 where we simply loop over each layer of the pyramid and display it on screen . <p> The scikit-image pyramid generated 4 layers since it reduced the image by 50% at each layer . <p> Now , let 's change the scale factor to 3.0- - and see how the results change : <p> Image Pyramids with Python and OpenCV <p> Shell <p> 1 <p> $python pyramid.py--image **34;12163;TOOLONG <p> And the resulting pyramid now looks like : @ @ @ @ @ @ @ @ @ @ to 3.0 has reduced the number of layers generated . <p> Using a scale factor of 3.0- , only 3 layers have been generated . <p> In general , there is a tradeoff between performance and the number of layers that you generate . The smaller your scale factor is , the more layers you need to create and process but this also gives your image classifier a better chance at localizing the object you want to detect in the image . <p> A larger scale factor will yield less layers , and perhaps might hurt your object classification performance ; however , you will obtain much higher performance gains since you will have less layers to process . <h> Summary <p> In this blog post we discovered how to construct image pyramids using two methods . <p> The first method to image pyramid construction used Python and OpenCV and is the method I use in my own personal projects . Unlike the traditional image pyramid , this method does not smooth the image with a Gaussian at each layer of the pyramid , thus making it more acceptable for @ @ @ @ @ @ @ @ @ @ to pyramid construction utilized Python + scikit-image and did apply Gaussian smoothing at each layer of the pyramid . <p> So which method should you use ? <p> In reality , it depends on your application . If you are using the HOG descriptor for object classification you 'll want to use the first method since smoothing tends to hurt classification performance . <p> If you are trying to implement something like SIFT or the Difference of Gaussian keypoint detector , then you 'll likely want to utilize the second method ( or at least incorporate smoothing into the first ) . <h> Downloads : 55217 @qwx675217 <p> Hi Oliver , thanks for the comment . However , as mentioned in the Summary section of this article , Method #1 is intended to be used for object classification using HOG + Linear SVM . As demonstrated by Dalal and Triggs in their Histogram of Oriented Gradients for Human Detection paper , applying Gaussian smoothing ( even to remove high frequency noise ) prior to extracting HOG features actually hurts classification performance . So when using HOG , its actually best to just @ @ @ @ @ @ @ @ @ @ to agree with Adrian . Gaussian smooth during downscaling seems like a good idea when you read the signal processing literature , but for features like HOG it does n't  really matter . <p> With HOG its all about speed . The fewer operations your perform before the classifier hits the pixels , the better off you 'll be . <p> If you implement your own HOG ( only loosely following Navneet Dalals recipe ) , you should build a test suite which measures your computation time and descriptor/classifier performance , and iterate . I can envision a variant of HOG that benefits from smoothing , but experiments should have the final word . <p> Just to clarify , the resizing operation used in the imutils library does perform some level of smoothing to avoid aliasing effects ( Moire patterns in the case of images ) . They call it interpolation , which only means it uses a different type of smoothing , not as strong as when using a Gaussian filter . Too much smoothing will destroy edge information which has a big impact on gradient computation . Depending on @ @ @ @ @ @ @ @ @ @ use digital filters with sharper transition bands to preserve edge information as much as possible . Downsampling without filtering would be as bad as oversmoothing for HOGs . <p> Hi ! , that 's exactly what I am looking for , thank you so much for doing this post . I was reading about the use of Gaussian smooth during downscaling and now i am wondering if for haar-like features it should be used or not . thank you again . <p> Haar features are normally scaled in the feature space rather than the image space to avoid ( unnecessarily ) recomputing the integral images . I would refer to the original Viola-Jones paper on Haar cascades to read more about their sampling scheme . <p> OpenCVs implementation of pyramids do n't  give enough control . For example , one of my primary use cases for image pyramids is for object detection using Histogram of Oriented Gradients . Its well known that applying Gaussian blurring prior to extracting HOG feature vectors can actually hurt performance which is something OpenCVs implementation of pyramids do , hence the scikit-image implementation is a better @ @ @ @ @ @ @ @ @ @ The scale simply controls the number of levels that are ultimately generated by the image pyramid . The smaller the scale , more layers in the image pyramid are generated . The larger the scale , the less pyramids are generated . I would suggest downloading the source code to this post and running the examples with varying scale parameters to convince yourself of this . <p> Hi , I have been using your tutorial to build a gesture classifier and localiser . I was successful in doing so . However , I do not understand what to do with image pyramid . I understand why it is used but i donot get what we have to do once we obtain the pyramid . Suppose I obtained the pyramid , ran my localiser on each image of the pyramid and then obtained a list of sliding windows which my localising classifier suggest is a window of interest . What do I do now ? How do I combine the results ? How do I get the best bounding box from all the scaled images ? Thanks in advance ! Your @ @ @ @ @ @ @ @ @ @ your reply ! Okay so I did run sliding window on each layer of the pyramid , that gave me a list of windows most likely to be my object . I took the one of highest probability from each layer and then took the maximum probability one from that final set . I am guessing this is not the correct way to use Image Pyramid . <p> You said : run NMS after running sliding window on all layers . NMS algo takes as input a set of top left and bottom right coordinates , correct ? Now , when i run sliding window on all layers , i will obtain coordinates of boxes in different scales of images . Should I give these coordinates as input directly to the NMS algo or do I have to rescale them somehow ? <p> Your intuition is correct . When you append a bounding box to your set of possible predictions , scale the bounding box by the ratio of the current pyramid level to the original scale of the image . This will give you the coordinates of the bounding @ @ @ @ @ @ @ @ @ @ You then apply NMS to these bounding boxes . <p> In HOG , before I fuse the overlap windows I have to know the coordinates of window in the original image . Should I construct a mapping between original image and resized image ? Or this kind of mapping already exist some where ? <p> You are correct , you would want to map the coordinates from the resized image to the original image . This can be accomplished by computing the ratio of the new image width to the old image width . I cover this in more detail inside the PyImageSearch Gurus course . <p> Simply compute the ratio of the original image dimensions to the current dimensions of the layer . Multiply the coordinates by this ratio and you 'll obtain the coordinates in relation to the original image . I cover this , and the rest of the object detection framework , @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485269 @185269/ <h> Advanced contour properties <p> Advanced contour properties <p> In the previous section we learned about a few simple contour properties such as area , perimeter , and bounding boxes . These properties actually build upon each other and we can use them to define more advanced contour properties . The properties we are about to inspect are more- powerful than our previous properties , as they allow us to discriminate between and recognize various shapes in images . <p> Unlike the previous section where we explored a small piece of code for each contour property , I 'm instead going to start by providing definitions- for our advanced contour properties : - aspect ratio , - extent , - convex hull , and- solidity . <p> From there , we are going to take these more advanced contour properties and build two Python scripts . The first script is going to be used to recognize the Xs and Os on a tic-tac-toe board . And the second script were going to build will be used to recognize the different kinds of Tetris blocks . <p> Like I said , @ @ @ @ @ @ @ @ @ @ sure to pay attention to this section . <h> Objectives : <p> We are going to build on our simple contour properties and expand them to more advanced contour properties , including : <p> Aspect ratio <p> Extent <p> Convex hull <p> Solidity <p> Well then take these advanced contour properties and create a two- computer vision applications that will : <p> Identify the Xs and Os on a tic-tac-toe board . <p> Identify Tetris blocks . <h> Advanced Contour Properties <p> Contour properties can take you a long , - long way when building computer vision applications , especially when you are first getting started . It takes a bit of creative thinking and a lot of discipline not to jump to more advanced techniques such as machine learning and training your own object classifier but by paying attention to contour properties we can actually perform object identification for simple objects quite easily . <p> Let 's go ahead and get started reviewing some of the more advanced contour properties , beginning- with- aspect ratio . <h> Aspect Ratio <p> The first " advanced " contour property well discuss is @ @ @ @ @ @ @ @ @ @ that complicated at all , hence why I 'm putting the term " advanced " in quotations . But despite its simplicity , it can be very powerful . <p> For example , I 've personally used aspect ratio to distinguish between squares and rectangles and detect handwritten digits in images and prune them from the rest of the contours . <p> The actual definition of the a contours- aspect ratio- is as follows : <p> aspect ratio = image width / image height <p> Yep . Its really that simple . The aspect ratio is simply the ratio of the image width to the image height . <p> Shapes- with an aspect ratio &lt; 1- have a height that is greater than the width these shapes will appear to be more " tall " and elongated . For example , most digits and characters on a license plate- have an aspect ratio that is less than 1 ( since most characters on a license plate- are taller than they are wide ) . <p> And shapes with an aspect ratio &gt; 1- have a width that is greater than the height @ @ @ @ @ @ @ @ @ @ object that will have an aspect ratio greater than 1 since the width of a physical license plate is always greater than the height . <p> Finally , shapes with an aspect ratio = 1- ( plus or minus some of course ) , have approximately the same width and height . Squares and circles are examples of shapes that will have an aspect ratio of approximately 1 . <h> Extent <p> The- extent of a shape or contour is the ratio of the contour area to the bounding box- area : <p> extent = shape area / bounding box area <p> Recall that the area of an actual- shape is simply the number of pixels inside the contoured region . On the other hand , the rectangular area of the contour is determined by its bounding box , therefore : <p> bounding box area = bounding box width x bounding box height <p> In all cases the- extent- will be less than 1 this is because the number of pixels inside the contour can not possibly be larger the number of pixels in the bounding box of the shape @ @ @ @ @ @ @ @ @ @ trying to distinguish between various shapes in images is entirely dependent on your application . And furthermore , you 'll have to manually inspect the values of the extent to determine which ranges are good for distinguishing between shapes and later in this section I 'll show you exactly how to perform this inspection . <h> Convex Hull <p> I like to think of a convex hull as a super elastic rubber band to bundle together a bunch of mail envelopes . I can place however many envelopes I want inside this rubber band , regardless of their size . And no matter what , this super elastic rubber band will surround these envelopes and hold them together . This super elastic rubber band never leaves any extra space or any extra slack it requires the minimum amount of space to enclose all my envelopes . <p> A convex hull is almost like a mathematical rubber band . More formally , given a set of- X points in the Euclidean space , the convex hull is the smallest possible convex set that contains these- X points . <p> In the example image @ @ @ @ @ @ @ @ @ @ the convex hull in action : <p> Figure 1 : The convex hull of a simple polygon . Notice how the rubber band encloses all points ( source ) . <p> On the- left we have our original shape . And in the- center- we have the convex hull of original shape . Notice how the rubber band has been stretched to around all extreme points of the shape , but leaving- no extra space along the contour thus the convex hull is the minimum enclosing polygon of all points of the input shape , which can be seen on the- right . <p> Another important aspect of the convex hull that we should discuss is the- convexity . Convex curves are curves that appear to " bulged out " . If a curve is not bulged out , then we call it a- convexity defect . <p> For example , take a look at the following hand-drawn image of a hand taken from the OpenCV documentation : <p> Figure 2 : Black arrows represent convexity defects of the hand image . <p> The gray outline of the hand in @ @ @ @ @ @ @ @ @ @ line is the convex hull of the hand . And the black arrows , such as in between the fingers , are where the convex hull is " bulged in " rather than " bulged out " . Whenever a region is " bulged in " , such as in the hand image above , we call them- convexity defects . <p> Perhaps not surprisingly , the convex hull and convexity defects play a major role in hand gesture recognition , as it allows us to utilize the convexity defects of the hand to count the number of fingers . Well be exploring hand gesture recognition and convexity defects in- Module 13 of this course . <p> But hand gesture recognition is not the only thing the convex hull is good for . We also use it when computing another important contour property : - solidity . <h> Solidity <p> The last advanced contour I want to discuss is the- solidity of a shape . The solidity of a shape is the area of the contour area divided by the area of the convex hull : <p> solidity = contour @ @ @ @ @ @ @ @ @ @ possible to have a solidity value greater than 1 . The number of pixels inside a shape can not possibly outnumber the number of pixels in the convex hull , because by definition , the convex hull is the smallest possible set of pixels enclosing the shape . <p> Just as in the extent of a shape , when using the solidity to distinguish between various objects you 'll need to manually inspect the values of the- solidity to determine the appropriate ranges . For- example ( and as well see in the next sub-section ) , the solidity of a shape is actually perfect for distinguishing between the Xs and Os on a tic-tac-toe board . <h> Getting Our Hands Dirty <p> That 's great but the real question is : - How do we put these contour properties to work for us ? <p> In the upcoming two sections well be utilizing our contour properties to distinguish between Xs and Os on a tic-tac-toe board and how to recognize different Tetris blocks. - Both of these example problems are situations in which Ive seen other computer vision programmers and developers @ @ @ @ @ @ @ @ @ @ solve these problems that is simply overkill and is absolutely not necessary . <p> If anything , I want the key takeaway from this section to be : - always consider contours before more advanced computer vision and machine learning methods . <p> Often ( and as you 'll see below ) , a clever use of contour properties can enable us solve problems that may appear challenging on the surface , but are actually quite easy . <h> Distinguishing Between Xs and Os <p> Let 's get started by recognizing the Xs and Os on a tic-tac-toe board . <p> If you are unfamiliar with the game , the playing board looks like this : <p> Figure 3 : - A tic-tac-toe game board . <p> Tic-tac-toe is a two player game . One player is the " X " - and the other player is the- " O. " - Players alternate turns placing their respective Xs and Os on the board , with the goal of getting three of their symbols in a row , either horizontally , vertically , or diagonally . Its very simple game to play @ @ @ @ @ @ @ @ @ @ competitive games . <p> Interestingly , tic-tac-toe is a- solvable game . When played optimally , you are guaranteed at best to win , at and at worst to draw ( i.e. tie ) . <p> While we are n't  going to dive into the game mechanics of optimal tic-tac-toe play , we are going to write a Python script that leverages computer vision and contour properties to recognize the Xs and Os on the board . Using this script , you could then take the output and feed it into a tic-tac-toe solver to give you the optimal set of steps to play the game . <p> Line 9 handles finding the actual contours in the image . Notice how we are using the cv2.RETREXTERNAL- - flag in the cv2.findContours- - function to indicate that we want the- external-most contours only . <p> Had we not provided this flag , we would have ended up detecting the inner circle of the " O " character as well as the outer circle ! And if you do n't  believe me , I would take a second a review the Finding @ @ @ @ @ @ @ @ @ @ contours in an image . Remember when we detected the contour of the inner-ovular region of the rectangle ? The same premise applies here if we were to detect all contours , we would end up finding the inner circle of the " O ; " hence we are only interested in the external contours . <p> From there we start looping over each individual contour on Line 12 . <p> And here is where we start computing the actual properties of our contour . We start off by computing the area of the contour on- Line 15 . Again , remember that the cv2.contourArea- - is not giving us the area of the contour . Instead , its giving us the number of pixels that reside inside the contour ( well be computing the rectangular area of the contour later ) . <p> Line 16 then grabs the- bounding box- of the contour , which gives us the starting- ( x , y ) -coordinates of the contour , followed by the width and height of the bounding box . <p> Given these two simple contour properties , let 's @ @ @ @ @ @ @ @ @ @ the- solidity. - The actual convex hull of the shape is computed on- Line 21- and the- area of the convex hull is then computed on- Line 22 . Now that we have both the- area of the contour along with the- area of the convex hull we can compute the- solidity , which we defined above. - <p> So how are we going to put these properties to work for us ? Let 's take a look <p> 47 <p> 48 <p> # initialize the character text <p> char= " ? " <p> # if the solidity is high , then we are examining an O <p> ifsolidity&gt;0.9 : <p> char= " O " <p> # otherwise , if the solidity it still reasonable high , we <p> # are examining an X <p> elifsolidity&gt;0.5 : <p> char= " X " @ @ @ @ @ @ @ @ @ @ it <p> ifchar ! = " ? " : <p> cv2.drawContours ( image , c , -1 , ( 0,255,0 ) , 3 ) <p> LONG ... <p> ( 0,255,0 ) , 4 ) <p> # show the contour properties <p> print " %s ( Contour #%d ) -- solidity=%.2f " % ( char , i+1 , solidity ) <p> # show the output image <p> cv2.imshow ( " Output " , image ) 55212 @qwx675212 <p> Well start by initializing our char- - variable to indicate the character that we are looking at in this case , we initialize it to be a ? indicating that the character is unknown . <p> Lines 29 and 30 check to see if the character is an- O we hardcode a rule that if the solidity &gt; 0.9 then , then character should be marked as an- O. <p> And similarly , if the solidity &gt; 0.5 ( Lines 34 and 35 ) then we are examining an- X. <p> Finally , if we are able to identify the character , we then draw the contour and the character on our image @ @ @ @ @ @ @ @ @ @ . The letter- X has four large and obvious convexity defects one for each of the four Vs that form the X. On the other hand , the O has nearly no convexity defects , and the ones that it has are substantially less dramatic than the letter X. Therefore , the letter O is going to have a larger solidity than the letter X. <p> So how did I arrive that the solidity values of 0.9- and- 0.5 ? <p> It was n't magic I simply examined the solidity of each character on- Line 44 , which prints the solidity value and current contour I 'd to the screen . <p> Looking at the output of my terminal you can clearly see the ranges of valid values to distinguish between Xs and Os : <p> When we look at this output , we can see that the letter O has a solidity of- 0.98 for all three times it appears on the board . Similarly , the letter X has a solidity of- 0.6 . The last solidity value of- 0.28- refers to the actual game board itself . <p> Like @ @ @ @ @ @ @ @ @ @ trivial to define the solidity value ranges to distinguish between Xs and Os on- Lines 28-35 . <p> The actual output on a game board is thus : <p> Figure 5 : The final output of identifying Xs and Os on a tic-tac-toe board . <p> In this figure its substantially more clear that our algorithm is functioning as expected . We are able to identify all of the Xs and Os without a problem while totally ignoring the actual game board itself . And we accomplished all of this by examining the- solidity of each contour . No fancy computer vision algorithms . No fancy machine learning . Just leveraging contour properties to our advantage . <h> Identifying Tetris Blocks <p> Distinguishing between Xs and Os in a tic-tac-toe game is a great introduction to the power of contour properties . But in the previous example we used only the- solidity perform our identification . In some cases , such as in identifying the various types of Tetris blocks , we need to utilize more than one contour property . Specifically , well be using- aspect ratio , - @ @ @ @ @ @ @ @ @ @ with each other to perform our brick identification . <p> Open up a new file , name it contourproperties2.py- , and let 's start coding : <p> Again , we start with our standard practice of importing the necessary packages and loading our Tetris block image off disk . <p> Here is what our Tetris image looks like : <p> Figure 6 : The Tetris blocks that we will be identifying . <p> The aqua piece- is known as a Rectangle . The blue and orange blocks are called- L-pieces . The yellow shape is obviously a- Square . And the green and red bricks on the bottom are called- Z-pieces . <p> Our goal here is to extract contours from- each of these shapes and then identify which shape each of the blocks are . <p> To accomplish this , well need to create a- binary image so we can extract the contours of the image . We apply thresholding on- Line 9 , which we covered in- Module 1.8- to create a binary image , where the background pixels are- black and the- foreground pixels ( i.e. the Tetris @ @ @ @ @ @ @ @ @ @ contours in our thresholded image on- Line 17 and allocate- a NumPy array with the same shape as our input image on- Line 18- so we can visualize the convex hull of each shape . Visualizing the convex hull is- not a required step in identifying the various Tetris blocks ; however , I thought it would be interesting to include in this example so you could see what the contour of a convex hull looked like . <p> Let 's continue with our example <p> 45 <p> 46 <p> # loop over the contours <p> for ( i , c ) inenumerate(cnts) : <p> # compute the area of the contour along with the bounding box <p> # to compute the aspect ratio <p> area=cv2.contourArea(c) <p> ( x , y , w , h ) @ @ @ @ @ @ @ @ @ @ , which is simply the width <p> # divided by the height of the bounding box <p> aspectRatio=w/float(h) <p> # use the area of the contour and the bounding box area to compute <p> # the extent <p> extent=area/float(w*h) <p> # compute the convex hull of the contour , then use the area of the <p> # original contour and the area of the convex hull to compute the <p> # solidity <p> hull=cv2.convexHull(c) <p> **30;12199;TOOLONG <p> **29;12231;TOOLONG <p> # visualize the original contours and the convex hull and initialize <p> # the name of the shape <p> **26;12262;TOOLONG , hull , -1,255 , -1 ) <p> cv2.drawContours ( image , c , -1 , ( 240,0,159 ) , 3 ) <p> shape= " " <p> On Line 21 we start looping over each of the contours individually so we can compute our contour properties . <p> Well start off by computing the area- and the- bounding box of the contour on- Lines 24 and 25 . <p> We then compute the aspect ratio , which is simply the ratio of the width to the height of the bounding box @ @ @ @ @ @ @ @ @ @ ratio of a shape will be &lt; 1- if the height is greater than the width . The aspect ratio will be- &gt; 1 if the width is larger than the height . And the aspect ratio will be- approximately 1 if the width and height are equal . <p> With this in mind , do you have any guesses as to what- well use the- aspect ratio- for ? <p> If you guessed discriminating between the square and rectangle pieces , you would be correct . But more on that later . <p> Line 33 then computes the- extent of the current contour , which is the area ( i.e. number of pixels that reside within the contour ) divided by the true rectangular ( area = width x height ) area of the bounding box . <p> Finally , - Lines 38-40 compute the- convex hull and the- solidity- in the same manner as we saw in the tic-tac-toe example . <p> We then visualize the convex hull , draw the current contour , and initialize the name of our shape on- Lines 44-46 . <p> Now that @ @ @ @ @ @ @ @ @ @ define the actual rules and if- - statements that will allow us to discriminate between the various Tetris blocks <p> 74 <p> 75 <p> # if the aspect ratio is approximately one , then the shape is a square <p> LONG ... <p> shape= " SQUARE " <p> # if the width is 3x longer than the height , then we have a rectangle <p> elifaspectRatio&gt;=3.0 : <p> shape= " RECTANGLE " <p> # if the extent is sufficiently small , then we have a L-piece <p> elifextent&lt;0.65 : <p> shape= " L-PIECE " <p> # if the solidity is sufficiently large enough , then we have a Z-piece <p> elifsolidity&gt;0.80 : <p> shape= " Z-PIECE " <p> # draw the shape name on the image <p> LONG ... <p> ( 240,0,159 ) @ @ @ @ @ @ @ @ @ @ print " Contour #%d -- aspectratio=%.2f , extent=%.2f , solidity=%.2f " % ( <p> i+1 , aspectRatio , extent , solidity ) <p> # show the output images <p> cv2.imshow ( " Convex Hull " , hullImage ) <p> cv2.imshow ( " Image " , image ) 55212 @qwx675212 <p> As I hinted at above , if the aspect ratioof- a shape is approximately 1 , then the width and height of the shape are roughly equal . And if this is the case , we know that in context of our Tetris blocks , if the width and height are equal , then we must be examining a square ( Lines 49 and 50 ) . <p> Similarly , if the- aspect ratio is large , then the width- is much greater than the height . And in the context of identifying Tetris pieces , this must mean we are examining a horizontal rectangle ( Lines 53 and 54 ) . <p> However , if the rectangle was vertical rather than horizontal , the aspect ratio would then be very small since the height would be substantially larger than @ @ @ @ @ @ @ @ @ @ the code above and I 'll leave it as an exercise for- you to convince yourself of this point ( its good to have a little homework ) . <p> Finally , - Lines 57 and 58 examine the extent to see if we are looking at an L-piece , and- Lines 61 and 62- check to see if we are examining at Z-piece . Again , just like in the tic-tac-toe board , I- manually investigated the values of the aspect ratio , extent , and solidity to define the valid ranges to distinguish between the Tetris block pieces and if you were developing your own shape identification script , you would have to perform the same type of investigation . This will become very obvious once we look at the output of our Python script . <p> To execute this script , simply navigate to our source code directory and execute the following command : <p> contourproperties2.py <p> Python <p> 1 <p> $python contourproperties2.py <p> Once the script executes , you 'll see the first image popup on your screen : <p> Figure 7 : Identifying the first Z-piece using @ @ @ @ @ @ @ @ @ @ original input image . Below that we have our- thresholded image- ( Line 9 ) where the actual Tetris blocks we want to identify appear as white on a black background . And then on the- bottom we have our convex hull image for the Z-piece notice how this shape almost looks like a skewed hexagon . This is because the convex hull is accounting for the two convexity defects ( i.e. where the Z-piece is " bulged in " ) . <p> Then , looking at the output of the terminal we see that the- solidity is- &gt;= 0.8 according to our rule , this must be a Z-piece . <p> Let 's look at the next shape : <p> Figure 8 : Identifying the second Z-piece . <p> Again , we are applying the same thought process as in the previous figure . We can see that the solidity of the shape is also- &gt;= 0.8 , thus we are once again examining a Z-piece . <p> Figure 9 : Identifying the rectangle Tetris block . <p> Identifying the rectangle Tetris block is very easy since the block is @ @ @ @ @ @ @ @ @ @ height . This means that the- aspect ratio must be quite larger . In this case the- aspect ratio = 3.76 , allowing our rule on- Line 53 to correctly identify the shape as a rectangle . <p> Figure 10 : Identifying a shape using the aspect ratio is trivially easy . <p> Just like identifying the rectangle in- Figure 9- above was easy , recognizing the square is even easier . Since a square has ( approximately ) the same width and height , we know that if the aspect ratio = 1 , ( plus or minus some , of course ) the shape must be a square . <p> Also take a look at the visualization of the convex hull for both the rectangle and the square . Since both the rectangle and square shapes have no convexity defects , the convex hull is actually no different than the original shape . <p> Figure 12 : Identifying the first L-piece using the extent . <p> Take a look at the output of our terminal . Up until Contour #5 , has the- extent of a shape dipped @ @ @ @ @ @ @ @ @ @ not . Thus , the- extent is a good property to distinguish the L-piece . <p> The same is true for the second L-piece below : <p> Figure 13 : Identifying the second L-piece using the extent . <p> As you can see , using nothing more than the aspect ratio , - extent , and- solidity of a shape we were able to distinguish between the four different types of Tetris blocks . <p> Of course , there is more than one way to skin this cat . We could have also used methods to quantify- and represent the shapes , - like Hu moments or Zernike moments , which well cover later in this course . <p> But at the same time , why bother ? <p> Using simple contour properties , we were able to recognize Xs and Os on a tic-tac-toe board . And we were also able to recognize the various types of Tetris blocks . Again , these contour properties , which are very simple on the surface , can enable us to identify various shapes we just need to take a step back @ @ @ @ @ @ @ @ @ @ of each of our contour properties to construct rules to identify each shape . <h> Summary <p> In this lesson we built upon our previous simple contour properties and learned about more advanced contour properties such as- aspect ratio , - extent , - convex hull , and- solidity . <p> Using these contour properties then enabled us to distinguish between Xs and Os on a tic-tac-toe board and recognize various Tetris block pieces . Again , these shape identifications were accomplished using- only contour properties . We did not have to apply any type of advanced techniques such as machine learning or training our own object classifier to obtain our shape classifications . 
@@71485270 @185270/ <p> But I got out of bed . I braved the morning , and I took the ice cold floor on my feet like a champ . <p> Why ? <p> Because Im excited . <p> Excited to share something very special with you today <p> You see , over the past few weeks Ive gotten some really great emails from fellow PyImageSearch readers . These emails were short , sweet , and to the point . They were simple " thank yous " for posting actual , honest-to-goodness Python and OpenCV code that you could take and use to solve your own computer vision and image processing problems . <p> And upon reflection last night , I realized that I 'm not doing a good enough job sharing the libraries , packages , and code that I have developed for myself for everyday use so that 's exactly what I 'm going to do today . <p> In this blog post Im going to show you the functions in my transform.py- module . I use these functions whenever I need to do a 4 point **28;12290;TOOLONG - using OpenCV . <p> @ @ @ @ @ @ @ @ @ @ interestingand you 'll even be able to utilize it in your own projects . <h> 4 Point OpenCV getPerspectiveTransform Example <p> In that post I mentioned how you could use a perspective transform to obtain a top-down , " birds eye view " of an image provided that you could find reference points , of course . <p> This post will continue the discussion on the top-down , " birds eye view " of an image . But this time I 'm going to share with you personal code- that I use every single time- I need to do a 4 point perspective transform . <p> So let 's not waste any more time . Open up a new file , name it transform.py , and let 's get started 24 <p> 25 <p> 26 55203 @qwx675203 55220 @qwx675220 <p> @ @ @ @ @ @ @ @ @ @ coordinates that will be ordered <p> # such that the first entry in the list is the top-left , <p> # the second entry is the top-right , the third is the <p> # bottom-right , and the fourth is the bottom-left <p> rect=np.zeros ( ( 4,2 ) , dtype= " float32 " ) <p> # the top-left point will have the smallest sum , whereas <p> # the bottom-right point will have the largest sum <p> s=pts.sum(axis=1) <p> rect0=ptsnp.argmin(s) <p> rect2=ptsnp.argmax(s) <p> # now , compute the difference between the points , the <p> # top-right point will have the smallest difference , <p> # whereas the bottom-left will have the largest difference <p> diff=np.diff ( pts , axis=1 ) <p> rect1=ptsnp.argmin(diff) <p> rect3=ptsnp.argmax(diff) <p> # return the ordered coordinates <p> returnrect <p> Well start off by importing the packages well need : NumPy for numerical processing and cv2- - for our OpenCV bindings . <p> Next up , let 's define the orderpoints- function on Line 5 . This function takes a single argument , pts- , which is a list of four points specifying the ( x , @ @ @ @ @ @ @ @ @ @ . <p> It is absolutely crucial that we have a consistent ordering of the points in the rectangle . The actual ordering itself can be arbitrary , as long as it is consistent- throughout the implementation . <p> Personally , I like to specify my points in top-left , top-right , bottom-right , and bottom-left order . <p> Well start by allocating memory for the four ordered points on Line 10 . <p> Then , well find the top-left point , which will have the smallest x + y sum , and the bottom-right point , which will have the largest x + y sum . This is handled on Lines 14-16 . <p> Of course , now well have to find the top-right and bottom-left points . Here well take the difference ( i.e. x y ) between the points using the np.diff- function on Line 21 . <p> The coordinates associated with the smallest difference will be the top-right points , whereas the coordinates with the largest difference will be the bottom-left points ( Lines 22 and 23 ) . <p> Finally , we return our ordered functions @ @ @ @ @ @ @ @ @ @ , I ca n't stress again how important it is to maintain a consistent ordering of points . <p> And you 'll see exactly why in this next function : <p> <p> 63 <p> 64 <p> **27;12320;TOOLONG , pts ) : <p> # obtain a consistent order of the points and unpack them <p> # individually <p> rect=orderpoints(pts) <p> ( tl , tr , br , bl ) =rect <p> # compute the width of the new image , which will be the <p> # maximum distance between bottom-right and bottom-left <p> # x-coordiates or the top-right and top-left x-coordinates <p> LONG ... <p> LONG ... <p> maxWidth=max ( int ( widthA ) , int(widthB) ) <p> @ @ @ @ @ @ @ @ @ @ will be the <p> # maximum distance between the top-right and bottom-right <p> # y-coordinates or the top-left and bottom-left y-coordinates <p> LONG ... <p> LONG ... <p> maxHeight=max ( int ( heightA ) , int(heightB) ) <p> # now that we have the dimensions of the new image , construct <p> # the set of destination points to obtain a " birds eye view " , <p> # ( i.e. top-down view ) of the image , again specifying points <p> # in the top-left , top-right , bottom-right , and bottom-left <p> # order <p> dst=np.array ( <p> 0,0 , <p> maxWidth-1,0 , <p> maxWidth-1 , maxHeight-1 , <p> 0 , maxHeight-1 , dtype= " float32 " ) <p> # compute the perspective transform matrix and then apply it <p> **34;12349;TOOLONG , dst ) <p> LONG ... <p> # return the warped image <p> returnwarped <p> We start off by defining the fourpointtransform- function on Line 28 , which requires two arguments : image- and pts- . <p> The image- variable is the image we want to apply the perspective transform to . And the pts- list is @ @ @ @ @ @ @ @ @ @ the image we want to transform . <p> We make a call to our orderpoints- function on Line 31 , which places our pts- variable in a consistent order . We then unpack these coordinates on Line 32 for convenience . <p> Now we need to determine the dimensions of our new warped image . <p> We determine the width of the new image on Lines 37-39 , where the width is the largest distance between the bottom-right and bottom-left x-coordinates or the top-right and top-left x-coordinates . <p> In a similar fashion , we determine the height of the new image on Lines 44-46 , where the height is the maximum distance between the top-right and bottom-right y-coordinates or the top-left and bottom-left y-coordinates . <p> Note : - Big thanks to Tom Lowell who emailed in and made sure I fixed the width and height calculation ! <p> So here 's the part where you really need to pay attention . <p> Remember how I said that we are trying to obtain a top-down , " birds eye view " of the ROI in the original image ? And @ @ @ @ @ @ @ @ @ @ four points representing the ROI is crucial ? <p> On Lines 53-57 you can see why . Here , we define 4 points representing our " top-down " view of the image . The first entry in the list is ( 0,0 ) - indicating the top-left corner . The second entry is ( maxWidth-1,0 ) - which corresponds to the top-right corner . Then we have ( maxWidth-1 , maxHeight-1 ) - which is the bottom-right corner . Finally , we have ( 0 , maxHeight-1 ) - which is the bottom-left corner . <p> The takeaway here is that these points are defined in a consistent ordering representation and will allow us to obtain the top-down view of the image . <p> To actually obtain the top-down , " birds eye view " of the image well utilize the **28;12385;TOOLONG - function on Line 60 . This function requires two arguments , rect- , which is the list of 4 ROI points in the original image , and dst- , which is our list of transformed points . The **28;12415;TOOLONG - function returns M- , which is the @ @ @ @ @ @ @ @ @ @ on Line 61 using the cv2.warpPerspective- - function . We pass in the image- , our transform matrix M- , along with the width and height of our output image . <p> The output of cv2.warpPerspective- - is our warped- image , which is our top-down view . <p> We return this top-down view on Line 64 to the calling function . <p> Now that we have code to perform the transformation , we need some code to drive it and actually apply it to images . <p> Open up a new file , call transformexample.py- , and let 's finish this up 27 <p> 28 <p> 29 55203 @qwx675203 <p> **27;12445;TOOLONG importfourpointtransform 55220 @qwx675220 55218 @qwx675218 <p> importcv2 55202 @qwx675202 55206 @qwx675206 <p> ap.addargument ( " -i " , " @ @ @ @ @ @ @ @ @ @ " ) <p> ap.addargument ( " -c " , " --coords " , <p> help= " comma seperated list of source points " ) 55208 @qwx675208 <p> # load the image and grab the source coordinates ( i.e. the list of <p> # of ( x , y ) points ) <p> # NOTE : using the ' eval ' function is bad form , but for this example <p> # let 's just roll with it -- in future posts I 'll show you how to <p> # automatically determine the coordinates without pre-supplying them <p> image=cv2.imread ( args " image " ) <p> LONG ... <p> # apply the four point tranform to obtain a " birds eye view " of <p> # the image <p> **31;12474;TOOLONG , pts ) <p> # show the original and warped images <p> cv2.imshow ( " Original " , image ) <p> cv2.imshow ( " Warped " , warped ) 55212 @qwx675212 <p> The first thing well do is import our fourpointtransform- function on Line 2 . I decided put it in the pyimagesearch- sub-module for organizational purposes . <p> Well then @ @ @ @ @ @ @ @ @ @ command line arguments , and cv2- for OpenCV bindings . <p> We parse our command line arguments on Lines 8-12 . Well use two switches , --image- , which is the image that we want to apply the transform to , and --coords- , which is the list of 4 points representing the region of the image we want to obtain a top-down , " birds eye view " of . <p> We then load the image on Line 19 and convert the points to a NumPy array on Line 20 . <p> Now before you get all upset at me for using the eval- function , please remember , this is just an example . I do n't  condone performing a perspective transform this way . <p> And , as you 'll see in next weeks post , I 'll show you how to automatically- determine the four points needed for the perspective transform- no manual work on your part ! <p> Next , we can apply our perspective transform on Line 24 . <p> Finally , let 's display the original image and the warped , top-down view of the image @ @ @ @ @ @ @ @ @ @ OpenCV getPerspectiveTranform example to obtain a birds eye view of the image . <p> As you can see , we have successfully obtained a top-down , " birds eye view " of the notecard ! <p> In some cases the notecard looks a little warped this is because the angle the photo was taken at is quite severe . The closer we come to the 90-degree angle of " looking down " on the notecard , the better the results will be . <h> Summary <p> In this blog post I provided an OpenCV **28;12507;TOOLONG - example using Python . <p> I even shared code from my personal library on how to do it ! <p> But the fun does n't  stop here . <p> You know those iPhone and Android " scanner " apps that let you snap a photo of a document and then have it " scanned " into your phone ? <p> That 's right I 'll show you how to use the 4 point OpenCV getPerspectiveTransform example code to build one of those document scanner apps ! <p> I 'm definitely excited about it , I hope you @ @ @ @ @ @ @ @ @ @ for the PyImageSearch Newsletter to hear when the post goes live ! <h> Downloads : 55217 @qwx675217 <h> 92 Responses to 4 Point OpenCV getPerspective Transform Example <p> Hello Adrian , This was really a wonderful post it gave me a very insightful knowledge of how to apply the perspective transform . I just have a very small question about the part where you were finding the maxHeight and maxWidth . For maxHeight ( just considering heightA ) you wrote <p> np.sqrt ( ( ( tr1 br1 ) ** 2 ) + ( ( tr1 br1 ) ** 2 ) ) <p> but i think that the height should be np.absolute ( tr1 br1 ) <p> because you know this gives us the difference in the Y coordinate but the equation that you wrote gives us <p> Hi Vivek . The equation is utilizing the sum of squared differences ( Euclidean distance ) , whereas the equation you proposed is just the absolute value of the differences ( Manhattan distance ) . Try converting to the code to use the np.absolute function and let me know how the results look @ @ @ @ @ @ @ @ @ @ think so . The dst array assumes the ordering that I mentioned above and its important to maintain that order . If the order was not maintained then the results from applying the perspective transform would not be correct . <p> Python arrays are zero-indexed , so we start counting from zero . Furthermore , the top-left corner of the image is located at point ( 0,0 ) . For more information on the basics of image processing , including the coordinate system , I would definitely look at Practical Python and OpenCV . I have an entire chapter dedicated to learning the coordinate system and image basics . <p> Great sample . My question is regarding the transformation matrix . Could it be used to tranform only a small region from the original image to a new image instead of warping the entire image ? Say you used the Hello ! example above but you wanted to only relocate the exclamation mark from the original image to a new image to end up with exactly the same output you have except without the " Hello " part , just @ @ @ @ @ @ @ @ @ @ you can use the TM directly without using the warping function . <p> Hi Ken , the transformation matrix M is simply a matrix . On its own , it can not do anything . Its not until you plug it into the transformation function that the image gets warped . <p> As for only warping part of an image , you could certainly only transform the exclamation point . However , this would require you to find the four point bounding box around the exclamation point and then apply the transform which is exactly what we do in the blog post . And that point you 're better off transforming the entire index card and cropping out the exclamation point from there . <p> Hi Wiem , I 'm not sure I understand what you 're asking ? If you want to get the angle of rotation for a bounding box , you might want to look into the cv2.minAreaRect function . I cover it a handful of times on the PyImageSearch blog , but you 'll want to lookup the actual documentation on how to get the angle . <p> Hey , @ @ @ @ @ @ @ @ @ @ angle of rotation of the bounding box just use the cv2.minAreaRect function , which you can read more about here . Notice how the angle of rotation is returned for the bounding box . <p> Hey Adrian , I believe your orderpoints algorithm is not ideal , there are certain perspective in which it will fail , giving non contiguous points , even for a rectangular subject . A better approach is to find the top 2 points and 2 bottom points on the y axis , then sort these pairs on the x axis . <p> Thanks for the quick reply.Okay i downloaded the file on my old laptop running windows 7 what do i do with it . Sorry for the stupid question I am a newbee and I am also old so i have two strikes against me , but if you learn from mistakes I should be a genius in no time ! <p> Thanks so much for the code . I have tried your code and running well on my MacBook OSX Yosemite , Python 2.7.6 and openCV 3.0 . <p> I am just wondering @ @ @ @ @ @ @ @ @ @ four points . So , the variable input will be only the image . = Should it be possible ? What will be the algorithm to auto-detect the points ? <p> Thank you for the tutorial . Have you got any tutorial how to transform perspective for the whole image ? So user chooses 4 reference points and then the whole image is transformed ( in result image there are some black fragments added ) . I know that I should calculate appropriate points for input image , but I have no idea how to do it . Can you help ? <p> Hi , Adrian I 'm passing by just to say this post is really helpful and thank you very much for it . I 'm starting studying Computer Vision and your blog it is really helping my development . Well done and keep going . = ) <p> I 'm a complete newbie to OpenCV but should n't warping perspective off a Mat using the output of minAreaRect be a one-line command ? I mean , you clearly have extracted some of these things out as utils and a nice importable @ @ @ @ @ @ @ @ @ @ we all thank you but do n't  you think that if it were so " regularly used by devs for their image processing tasks " , they better lie in vanilla OpenCV ? To be *really really* honest , my " duckduckgo-ing " about warping off a rect perspective led me to this post of yours among the very first results and I *knew* the code presented obviously works but I did n't  immediately start using it *ONLY AND ONLY* because I *believed* there would be something to the effect of <p> warpedMat = **31;12537;TOOLONG , areaToWarp ) <p> Ultimately , on asking my colleague on how to do it , she suggested goin ahead with your written utils only ! = <p> Adrian , Thank you so much for your awesome tutorials ! ! Ive been learning how to use the raspberry pi the past few weeks in order to make an automated testing system , and your tutorials have been so thorough and helpful . Thank you so so much for making these ! ! <p> In order to apply a perspective transform ( and get reasonable results @ @ @ @ @ @ @ @ @ @ four corners of the object . If you can not , your results will look very odd indeed . Please see this post for more details on applying perspective transforms to rectangular objects . <p> Hi Adrian . sorry for my English . = I 'm newbie in opencv. thank you so much for awesome tut = i want crop this image https : **33;12570;TOOLONG with original on left and i want crop image on right . may u help me ? thanks in advance <p> thanks your tut . it very excited ! in this image , i want to get the road , and outside will be black , white or other color , because i 'm researching about raspberry pi , and i want it process least as possible . do you have any idea ? <p> I gave the input image of a irregular polygon formed after applying convex hull function to a set of points , supplying the four end pints of the polygon in the order you mentioned . However the output I get is a blank screen . No polygon in it . Can you @ @ @ @ @ @ @ @ @ @ the above code . <p> Its hard to say without seeing an example , but I imagine your issue is that you did not supply the coordinates in top-left , top-right , bottom-left , and bottom-right order prior to applying the transformation . <p> Adrian , great post . I was trying to build this for a visiting card . My problem is that the card could be aligned in any arbitrary angle with respect to the desk as well as with respect to the camera . <p> When I use this logic , there are alignments at which the resultant image is like a rotated and shrieked one . In your case , the image is rotated towards the right to make it look correct . However , if the original card was rotated clockwise 90 degrees , then the logic of top right , top left does not work correctly . <p> I tried using the " width will always be greater than height " approach but that too fails at times . <p> Hello Adrain , I am quit new in opencv. i am working in Imageprocessing @ @ @ @ @ @ @ @ @ @ during cropping of Image . I am working in diffferent Images having some black side background . The Picture is taken by camera maulally so the Image is also different in size . I want to crop the Image and separte it from balck Background . could you suggest how can i make a program that can detect the Image first and crop automatically . Thanks <p> I have a doubt when thinking about the generalization of this example regarding the destiny points . This example is specifically aimed to quadrangular objects , right ? I mean , you get the destiny image points because you simple say " hey , sure it will be a box , let 's get the max height , max width and that 's it " . <p> But would n't be so easy if the original object would have had a different shape , right ? <p> When the rectangular dimensions of the source target are known , the result is much better if you input x , y values for the destination image that conform to the x/y ratio of the source target . The @ @ @ @ @ @ @ @ @ @ perfect if the target is perpendicular to the viewpoint . This is why the distortion increases in proportion to the angle(s) away from perpendicular . Otherwise , you have to know the focal length of the camera , focus distance , etc. ( much more complicated calculations ) to estimate the " real " proportions of the targets dimensions ( or x/y ratio ) . <p> As an example , the page size in your samples looks like " legal " 8.5 x 14 paper . Once this is established , if you replace the maxHeight calculation with " maxHeight = ( maxWidth * 8 ) / 14 " , the output image(s) are much better looking as far as the x/y ratio is concerned ( no apparent distortion on the last sample ) . Of course , one must know the targets x/y ratio <p> Good point , thanks for sharing Rene . If the aspect ratio is know then the output image can be much better . There are methods that can attempt to ( automatically ) determine the aspect ratio but they are outside the scope of @ @ @ @ @ @ @ @ @ @ them in the future . <p> Thanks a lot for the post , this is great for the app I 'm trying to build . The thing is that I 'm translating all your code to Java and I do n't  know if everything is correctly translated because the image I get after the code is rotated 90- and flipped I 'm investigating what could be happening but maybe you think of something that could be happening . Thanks again for the post . <p> This page provides a nice discussion on affine vs. perspective transforms . An affine transform is a special case of a perspective transform . In this case perspective transforms are more appropriate we can make them even better if we can estimate the aspect ratio of the object we are trying to transform . See the link for more details . <p> There is no " pyimagesearch " module available on PyPI , hence you error when installing via " pip " . You need to use the " Downloads " section at the bottom of this page , download the code , and put the " pyimagesearch @ @ @ @ @ @ @ @ @ @ are executing . Alternatively , you could place it in the site-packages directory of your Python install , @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485271 @185271/ <p> So as a natural followup to this guide , I wanted to demonstrate how the GPU could be used to speedup the training of Deep Belief Networks ( previously , only the CPU had been used ) . <p> However , I had never setup my MacBook Pro to utilize the GPU for computational processing . That 's all fine and good I thought . This will make for a great tutorial , with me experimenting through trial and error and chronicling my steps along the way with the hope that someone else would find the tutorial useful . <p> The short of it is this : - I did succeed- in installing The CUDA Toolkit , drivers , and CUDAMat . <p> However . <p> For reasons I can not understand , training my Deep Belief Network with nolearn on my GPU takes- over a full minute longer than on the CPU . Truthfully . I 've tried every option that I can think of . And no dice . <p> Is it just because I 'm on a MacBook Pro the performance of my GeForce GT 750m just is n't @ @ @ @ @ @ @ @ @ @ ? Am I just missing a switch somewhere ? <p> Honestly , I 'm not sure . <p> So if anyone can she 'd some light on this problem , that would be great ! <p> But in the meantime I 'll go ahead and writeup the steps that I took to install the CUDA Toolkit , drivers , and CUDAMat . <h> My Experience with CUDAMat , Deep Belief Networks , and Python on OSX <p> So before you can even think about using your graphics card to speedup your training time , - you need to make sure you meet all the pre-requisites for the latest version of the CUDA Toolkit ( at the time of this writing , v6.5.18 is- the latest version ) , including : <p> Mac OSX 10.8 or later <p> The gcc or Clang compiler ( these should already be setup and installed if you have Xcode on your system ) <p> A CUDA-capable GPU <p> The NVIDIA CUDA Toolkit <p> The first two items on this checklist are trivial to check . The last item , the NVIDIA Cuda Toolkit is a simple download and @ @ @ @ @ @ @ @ @ @ To check if you have a CUDA-capable GPU on OSX , its a simple process of clicking the Apple icon at the top-left portion of your screen , selecting " About this Mac " , followed by " More info " , then " System Report " , and lastly the " Graphics/Display " tab . <p> It should look something like this : <p> Under my " Chipset Model " I see that I am using a NVIDIA GeForce GT 750m graphics card . <p> I like to put these in my . bashprofile- - file so that my paths are set each time I open up a new terminal : <p> Once you have edited your . bashprofile- file , close your terminal and open up a new one so that your changes are read . Alternatively . you could execute- source/.bashprofile- - to reload your settings . <p> To verify that CUDA the driver was indeed installed correctly , I executed the following command : <p> You 'll want to uncheck- the Automatic Graphic Switch ( which ensures that your GPU will always be utilized ) and also @ @ @ @ @ @ @ @ @ @ : - I honestly played around with both of these settings . Neither of them improved performance for me . <p> Now , let 's go ahead and install the CUDA Samples so that we can ensure that the CUDA Toolkit is functioning and that the driver is working . Installing the samples is just a simple shell script with should be available on your PATH- - provided that you have reloaded your . bashprofile- . <p> Installing the CUDA Samples <p> Shell <p> 1 <p> **28;12605;TOOLONG <p> This script installed the CUDA samples in my home directory . The output of the script below is n't very exciting , just proof that it succeeded : <p> From there , I compiled the deviceQuery- - and deviceBandwith- - examples with the following commands : <p> Compiling the CUDA Examples <p> Shell <p> 1 <p> 2 <p> **29;12635;TOOLONG <p> **31;12666;TOOLONG <p> Both examples compiled without any errors . <p> Running the compiled deviceQuery- - program I was able to confirm that the GPU was being picked up : <p> Similarly , the output of deviceBandwith- - demonstrates that GPU is accessible and is @ @ @ @ @ @ @ @ @ @ to feel pretty good . <p> The CUDA Toolkit , drivers , and samples were installed without a problem . I was able to compile and execute the samples with no issue . <p> So then it was time to move on to CUDAMat , which allows matrix calculations to take place on the GPU via Python , leading to ( hopefully ) a dramatic speedup in training time of neural networks . <p> I cloned down CUDAMat to my system and compiled it without a hitch using the supplied Makefile- : <p> I then ran testcudamat.py- - to ensure everything was working smoothly indeed it appeared that everything was working fine . <p> Now I was really excited ! <p> The CUDA Toolkit was installed , CUDAMat compiled without a hitch , and all the CUDAMat tests passed . I would be reaping the GPU speedups in no time ! <h> The Ugly Results <p> However , in my case , utilizing the GPU- was a minute slower- than using the CPU . <p> In the benchmarks- reported below , I was utilizing the nolearn implementation of a Deep @ @ @ @ @ @ @ @ @ @ . My network included an input layer of 784 nodes ( one for each of the input pixels of the- 28 x 28 pixel image ) , a hidden layer of 300 nodes , and an output layer of 10 nodes , one for each of the possible digits . I allowed the network to train for 10 epochs . <p> Hopefully someone with more expertise in utilizing the CUDA Toolkit , OSX , and the GPU can guide me in the right direction . <h> Summary <p> In this blog post I attempted to demonstrate how to utilize the GPU on OSX to speedup training times for Deep Belief Networks . <p> Unfortunately , my results demonstrated that the GPU yielded- slower training times than the CPU this makes absolutely no sense to me and it is completely counter-intuitive . <p> Is there something wrong with my configuration ? <p> Did I miss a step along the way ? <p> Surely I made a mistake or my intuition is off . <p> If you 're reading this post and thinking- " Hey Adrian , you 're an idiot . You forgot @ @ @ @ @ @ @ @ @ @ then please shoot me an email- with the subject- You 're an idiot and set me straight . It would be much appreciated . <p> At the very least I hope this post is a solid chronicle- of my experience and that someone , somewhere finds it helpful . <h> Update : <p> I have found my redemption ! To find out how I ditched my MacBook Pro and moved to the Amazon EC2 GPU , just click here . <p> Not any kind of expert , but my intuition is that your demo is n't wide enough to take advantage of the GPUs massive parallel architecture . CPU is still much much faster for linear sequence . Try doubling your width or finding a tool to see how many and how active your CUDA cores are . You 'll need more granularity than something like MSI Afterburner will tell you <p> I did not read through the entire post , but as a GPU user for training nets , here is my observation . The biggest bottleneck to GPU computing is actually a GPU/CPU data transfer . Everytime that happens there is @ @ @ @ @ @ @ @ @ @ bus . If your data set is small and your network is small , and you frequently keep swapping between CPU and GPU data structures there will be no perceived advantage <p> Thank you for the clarification and advice ! There definitely seems to be a common theme here and that my network is far too small for the GPU speedups to be realized . I will have to train on a much larger GPU and see if the timings improve . <p> Adrian , Do CudaMat or Deep belief networks rely on double precision , floating point operations ? If so that might be a reason . I 'm quite disappointed with the GT 750M capability in this regard . Take a look at http : **29;12699;TOOLONG . <p> Thanks for the link , that really helps clarify the performance of the MacBook Pro GPU . I 've written a follow up post that will go live later today ( Monday , Oct 13th ) that details my experiments with the GPU on the Amazon EC2 cloud . The results are substantially better so my previous poor results are definitely due @ @ @ @ @ @ @ @ @ @ a newbie to NNs and have questions about implementation . Using images as the example , after training with a NN better representations will are generated than say , manual feature engineering . What do these representations look like ? What is the process to take the representations and to deliver a search engine ? <p> There was actually a very recent paper published called Deep Learning for Content-Based Image Retrieval : A Comprehensive Study that reviews this process . Normally , if you wanted to apply a NN to build a CBIR system , you would use some sort of Deep Learning , stack some unsupervised RBMs together , and learn some feature representations . You then store these feature representations in your database and query against them later . <p> I ma about to try deep reinforcement learning ( atari games ) on my MacBook Pro equipped with NVIDIA GeForce GT 750M . First I was concerned by your article , but now reading the comments it seems that I was only a matter of data set size and net size . So does it mean that reinforcement @ @ @ @ @ @ @ @ @ @ certainly give you a performance gain over your CPU , but realistically , for any type of current research using deep learning you need a more powerful GPU than what is provided in your MacBook Pro . I would consider spinning @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485272 @185272/ <p> About a week ago , a close childhood friend of mine passed away in a tragic car accident . <p> I went to elementary school and middle school with him . We spent our summers skateboarding on my driveway and the winters snowboarding and sledding in my backyard . <p> Since last week I 've spent my time traveling and attending his viewing and service . Every moment has been utterly agonizing . I find it extremely hard to focus , my mind always wandering back to memories now over a decade old . <p> To be honest , it 's a wonder I am even writing this blog post at all . <p> But if there is anything I believe in , it 's that life should be celebrated . And there is no better form of celebration than the nostalgic memories of a loved one . <p> Back in middle school , him and I used to go through CCS Catalogs- ( yes , actual catalogs ; they did n't list all their products online , and even if they did , our internet was dialup @ @ @ @ @ @ @ @ @ @ would fantasize about which skateboards we were going to buy next or rather , which ones we were going to ask our parents to purchase for us as birthday presents . <p> Little memories like leafing through CCS Catalogs before , during , and after school bring a smile to my face . And they make the days a little easier to get through . <p> So in this post , I 'm going to show how to perform basic image segmentation using Python and OpenCV . <p> And we 'll give it a little skateboarding theme as well , just to pay homage to a friend whose memory weighs heavy on my mind . <p> OpenCV and Python versions : This example will run on- Python 2.7/Python 3.4+ and OpenCV 2.4 . X/OpenCV 3.0+ . <h> The cv2.threshold Function <p> Let 's start by taking a look at the cv2.threshold function signature : <p> ( T , threshImage ) = cv2.threshold ( src , thresh , maxval , type ) <p> The first parameter is our source image , or the image that we want to perform thresholding on @ @ @ @ @ @ @ @ @ @ parameter , thresh , is the threshold value which is used to classify the pixel intensities in the grayscale image . <p> The third parameter , maxval , - is the pixel value used if any given pixel in the image passes the thresh- test . <p> Finally , the fourth parameter is the thresholding method to be used . The type- value can be any of : <p> cv2.THRESHBINARY <p> cv2.THRESHBINARYINV <p> cv2.THRESHTRUNC <p> cv2.THRESHTOZERO <p> cv2.THRESHTOZEROINV <p> Sound complicated ? Its not I 'll show you examples for each of these thresholding types . <p> The cv2.threshold- - then returns a tuple of two values . The first value , T- , is the value that was used for the thresholding . In our case , this will be the same value as thresh- - that we pass into the cv2.threshold- - function . <h> Thresholding : Simple Image Segmentation using OpenCV <p> But in the beginning , there was only the most basic type of image segmentation : thresholding . <p> Let 's discover how to perform simple image segmentation using OpenCV . Open up your favorite @ @ @ @ @ @ @ @ @ @ 's get started : <p> Thresholding : 28 <p> 29 <p> 30 55203 @qwx675203 55218 @qwx675218 <p> importcv2 <p> # construct the argument parser and parse the arguments 55206 @qwx675206 <p> ap.addargument ( " -i " , " --image " , required=True , <p> help= " Path to the image to be thresholded " ) <p> LONG ... <p> help= " Threshold value " ) 55208 @qwx675208 <p> # load the image and convert it to grayscale <p> image=cv2.imread ( args " image " ) 55215 @qwx675215 <p> # initialize the list of threshold methods <p> methods= <p> ( " THRESHBINARY " , cv2.THRESHBINARY ) , <p> ( " THRESHBINARYINV " , cv2.THRESHBINARYINV ) , <p> ( " THRESHTRUNC " , cv2.THRESHTRUNC ) , <p> ( " THRESHTOZERO @ @ @ @ @ @ @ @ @ @ , cv2.THRESHTOZEROINV ) <p> # loop over the threshold methods <p> for ( threshName , threshMethod ) inmethods : <p> # threshold the image and show it <p> LONG ... <p> cv2.imshow ( threshName , thresh ) 55212 @qwx675212 <p> Well start by importing the two packages that well need , argparse- - and cv2- on- Lines 2 and 3 . <p> From there , well parse our command line arguments on- Lines 6-11 . Here well require two parameters . The first , --image- , is the path to the image that we want to threshold . The second , --threshold- , is the threshold value that will be passed into the cv2.threshold- - function . <p> From there , well load the image from disk and convert it to grayscale on- Lines 14 and 15 . We convert to grayscale since cv2.threshold- - expects a single channel image . <p> Lines 18-23 defines our list of thresholding methods . <p> We loop over our thresholding methods starting on- Line 26 . <p> From there , we apply the actual threshold method on- Line 28 . We pass our @ @ @ @ @ @ @ @ @ @ supplied threshold value as the second argument , 255 ( white ) as our value for when the threshold test passes as our third argument , and finally the threshold method itself as the final parameter . <p> Finally , the thresholded image is displayed on- Lines 29 and 30 . <p> Let 's look at some results . Open up your terminal , navigate to our code directory , and execute the following command : <p> Thresholding : Simple Image Segmentation using OpenCV <p> Shell <p> 1 <p> $python threshold.py--image **40;12730;TOOLONG <p> In this example we are using a value of 245 for our threshold test . If a pixel in the input image passes the threshold test , it will have the value set to 255 . <p> Now , let 's take a look at the results : <p> Figure 1 : Applying cv2.threshold with cv2.THRESHBINARY . <p> Using cv2.THRESHBINARY- - the skateboards are segmented to be black with a white background . <p> To invert the colors , just use cv2.THRESHBINARYINV- , as seen below : <p> Figure 2 : Applying cv2.threshold with cv2.THRESHBINARYINV . <p> Not bad @ @ @ @ @ @ @ @ @ @ : Applying cv2.threshold with cv2.THRESHTRUNC . <p> Using cv2.THRESHTRUNC- - leaves the pixel intensities as they are if the source pixel is not greater than the supplied threshold . <p> Then we have cv2.THRESHTOZERO- - which sets the source pixel to zero if the source pixel is not greater than the supplied threshold : <p> Figure 4 : Applying cv2.threshold with cv2.THRESHTOZERO . <p> Finally , we can invert this behavior as well using cv2.THRESHTOZEROINV- : <p> Figure 5 : Applying cv2.threshold with cv2.THRESHTOZEROINV . <p> Nothing to it ! <h> Summary <p> In this blog post I showed you how to perform the most basic form of image segmentation : thresholding . <p> Most importantly , be sure to play around with the thresh- - value as it will give different results depending on what value you supply . <p> In future posts , I 'll show you how to- automatically- determine the threshold value , no parameter tuning required ! @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485273 @185273/ <h> Installing OpenCV on your Raspberry Pi Zero <p> Since I 've covered how to install OpenCV on the Raspberry Pi in- multiple , previous blog posts , I 'll keep this post on the shorter side and detail only the relevant commands necessary to get OpenCV up and running . For a more thorough discussion on how to install OpenCV 3 on your Pi ( along with a 22-minute video- installation guide ) , please refer to this post . <p> Ill also be making the following assumptions in this installation guide : <p> You are using Raspberry Pi Zero hardware ( so the timings supplied with each command will match up ) . <p> Again , I have already covered installing OpenCV on multiple Raspberry Pi platforms and Raspbian flavors the primary goal of this tutorial is to get OpenCV up and running on your Pi Zero so you can get started learning about computer vision , image processing , and the OpenCV library . <h> Installing OpenCV on your Raspberry Pi Zero <p> If you have n't seen the Raspberry Pi Zero yet , its a really cool piece @ @ @ @ @ @ @ @ @ @ processor . 512mb of RAM. - And its smaller than a credit card . <p> But the best part ? <p> Its only $5 ! <p> While the Pi Zero is n't quite fast enough for advanced video processing , its still a great tool that you can use to learn the basics of computer vision and OpenCV . <h> Step #1 : Expand filesystem <p> If you 're using a- brand new install of Raspbian Jessie , then the first thing you should do is ensure your filesystem has been expanded to include all available space on your micro-SD card : <p> After rebooting , your filesystem will have be expanded to include all available space on your micro-SD card . <h> Step #2 : Install dependencies <p> I 've discussed each of these dependencies are in previous posts , so Ill just provide a brief description , the command(s) themselves , along with the- amount of time it takes- to execute each command so you can plan your OpenCV install accordingly ( the compilation of OpenCV alone takes 9+ hours ) . <p> The only requirement to build Python + OpenCV @ @ @ @ @ @ @ @ @ @ pip- to install NumPy for us : <p> Installing OpenCV on your Raspberry Pi Zero <p> Shell <p> 1 <p> $pip install numpy <p> Timing : 35m 4s <h> Step #5 : Compile and install OpenCV for the Raspberry Pi Zero <p> We are now ready to compile and install OpenCV . Make sure you are in the cv- virtual environment by using the workon- command : <p> Installing OpenCV on your Raspberry Pi Zero <p> Shell <p> 1 <p> $workon cv <p> And then setup the build using CMake : <p> Installing OpenCV <p> 8 <p> 9 <p> $cd/opencv-3.0.0/ <p> $mkdirbuild <p> $cdbuild <p> **30;12772;TOOLONG <p> **31;12804;TOOLONG <p> -DINSTALLCEXAMPLES=ON <p> **26;12837;TOOLONG <p> LONG ... <p> -DBUILDEXAMPLES=ON .. <p> Timing : 4m 29s <p> Now that the build is all setup , run make- to start the compilation process ( this is going to take awhile , so you might want to let this run overnight ) : <p> Installing OpenCV on your Raspberry Pi Zero <p> Shell <p> @ @ @ @ @ @ @ @ @ @ OpenCV compiled without error , you can install it on your Raspberry Pi Zero using : <p> Installing OpenCV on your Raspberry Pi Zero <p> Shell <p> 1 <p> 2 <p> $sudo makeinstall <p> $sudo ldconfig <p> Timing : 2m 31s <h> Step #6 : Finishing the install <p> Provided you completed Step #5 without an error , your OpenCV bindings should now be installed in **39;12865;TOOLONG : <p> Installing OpenCV on your Raspberry Pi Zero <p> Shell <p> 1 <p> 2 <p> 3 <p> **43;12906;TOOLONG <p> total1640 <p> -rw-r--r--1root **27;12951;TOOLONG <p> All we need to do now is sym-link the cv2.so- file ( which are our actual Python + OpenCV bindings ) into the site-packages- directory of the cv- virtual environment : <p> Installing OpenCV on your Raspberry Pi Zero <p> Shell <p> 1 <p> 2 <p> LONG ... <p> LONG ... <h> Step #7 : Verifying your OpenCV install <p> All that 's left to do now- is verify that OpenCV has been correctly installed on your Raspberry Pi Zero . <p> Whenever you want to use OpenCV , first make sure you are in the cv- virtual environment @ @ @ @ @ @ @ @ @ @ Shell <p> 1 <p> $workon cv <p> And from there you can fire up a Python shell and import the OpenCV bindings : <p> Installing OpenCV on your Raspberry Pi Zero <p> Shell <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> 6 <p> $workon cv <p> $python <p> &gt;&gt;&gt;import cv2 <p> &gt;&gt;&gt;cv2. version <p> ' 3.0.0 ' <p> &gt;&gt;&gt; <p> Or you can execute a Python script that imports OpenCV . <p> Once OpenCV has been installed , you can remove both the opencv-3.0.0- and opencvcontrib-3.0.0- directories , freeing up a bunch of space on your filesystem : <p> Installing OpenCV on your Raspberry Pi Zero <p> Shell <p> 1 <p> $rm-rf **31;12980;TOOLONG <p> But be cautious before you run this command ! Make sure OpenCV has been properly installed on your system before blowing away these directories , otherwise you will have to start the ( long , 9+ hour ) compile- all over again ! <h> Troubleshooting <p> If you ran into any error installing OpenCV 3 on your Raspberry Pi Zero , I would suggest consulting the- Troubleshooting section of this post which goes @ @ @ @ @ @ @ @ @ @ post also includes a complete 22 minute video where I demonstrate how to run each command to flawlessly get OpenCV 3 installed on your Raspberry Pi : <h> So , what 's next ? <p> But I 'm also willing to bet that you 're just getting started learning computer vision and OpenCV , and you 're probably feeling a bit confused and overwhelmed on where exactly to start . <p> Personally , I 'm a big fan of learning by example , so a good first step would be to read this blog post on accessing your Raspberry Pi Camera with the picamera module . This tutorial details the exact steps you need to take to ( 1 ) capture photos from the camera module and ( 2 ) access the raw video stream . <p> And if you 're really interested in leveling-up your computer vision skills , you should definitely check out my book , Practical Python and OpenCV + Case Studies . My book not only covers the basics of computer vision and image processing , but also teaches you how to solve real world computer vision problems including face detection in @ @ @ @ @ @ @ @ @ @ and handwriting recognition . <p> All code examples covered in the book are guaranteed to run on the Raspberry Pi 2 and Pi 3 as well ! Most programs will also run on the B+ and Zero models , but might be a bit slow due to the limited computing power of the B+ and Zero . <p> So let 's put your fresh install of OpenCV on your Raspberry Pi to good use " just click here to learn more about the real-world projects you can solve using your Raspberry Pi + Practical Python and OpenCV . <h> Summary <p> This post detailed how to install OpenCV 3 on your Raspberry Pi Zero . The purpose of this blog post was to provide accurate timings that you can use when planning your own install of OpenCV on your Pi Zero . <p> In order to get OpenCV up and running , I made the following assumptions : <p> You are running- Raspbian Jessie on your Raspberry Pi Zero . <p> You are installing OpenCV v3 . <p> You want to use Python 2.7 with your OpenCV bindings . <p> If @ @ @ @ @ @ @ @ @ @ bindings , please consult this post , where I have elaborated more on each step , provided more detailed information , and included a 22 minute video that walks you through step-by-step on installing OpenCV 3 on your Raspberry Pi . <p> Hi Adrian , cool site , I 'm wondering about the hardware setup for PiZero you have there . I see it has two micro usbs one will be used for power I think- . To me its a pity they did not drop the hdmi in favor of another microusb , I do n't  understand how you can use it . You attach the usb/network camera either to the other microusb but then how do you control it you either need a keyboard attached ( and the cool looking small hdmi monitor you have ) or a usb-&gt;network adapter or usb wifi . Do you have an usb hub ? <p> Indeed , it has two micro-USB ports . The first one is for power . And the second one is used for any peripherals . I ended up plugging in a USB hub , which then allows @ @ @ @ @ @ @ @ @ @ and webcam . The HDMI is also min-HDMI . <p> I have installed OpenCV because Im interested in exploring the possibilities of robot vision . The tests and demos work fine . There is , however , one thing I would like to be clarified on . When doing Python stuff I like to work with IDLE . I have started it from the command line in the virtual cv environment with IDLE &amp; . Import numpy works fine , but when I try to import cv2 I get " Import error : No module named cv2 . When I start Python from the command line importing cv2 works fine . But then , of course I miss all the advantages of IDLE . Is there a way of fixing this ? I already tried sudo apt-get install python from within the virtual environment , but the message I get says the the latest version is already installed . Thank you <p> I do n't  personally use IDLE ( other than the command line version ) , so I 'm not sure about this problem . If you like using the @ @ @ @ @ @ @ @ @ @ Notebooks , that way you can code similar IDLE , only in your browser . Plus , this has the advantage of working with the Python virtual environment . <p> Thank you very much for your reply . Meanwhile I investigated the matter a little bit further and found out that putting the symbolic link ln -s **45;13013;TOOLONG cv2.so in the /usr/bin/ directory because that 's where the Python executable is . This solves the problem and I am now able to use IDLE within the virtual environment . <p> Is it possible to have an image of raspbian + opencv for raspberry pi zero , 2 and 3 ready to download and flash it on the SD please ? I did try to install open cv following your tutorial on raspberry pi 2 and zero but there is always something wrong somewhere . For example , right now I have issues on cmake , it failed for many reasons and it will take huge amount of time for a newbie to understand what 's wrong . <p> I have n't tried cross-compiling before for the Raspberry Pi , it might be something @ @ @ @ @ @ @ @ @ @ on the Pi Zero , I 'm not sure what the end goal would be ? That would n't impact performance . <p> Hi Adrian . Thanks for the post . I suggest people compiling it on a headless pi ( using SSH ) to call make as : $make &gt; progress.txt 2&gt;&amp;1 &amp; It runs make in background , writing the stdout and stderr output to a file avoiding the typical issues with the SSH connections and allowing to close the session . To check the progress , open a new SSH and call : $tail -f progress.txt Kind regards <p> I tried installing opencv along with opencvcontrib , i did not run into any issue and it compiled , i am able to import opencv in my python environment but the modules from the opencvcontrib ( xfeatures2d etc ) are not available . I followed all the procedures listed here exactly . Do i need to add something extra in cmake statement . Any ideas as to how i can resolve this ? <p> Double-check your CMake and specifically check the " Modules to be Compiled " section . You 'll @ @ @ @ @ @ @ @ @ @ the time that is due to your path to the opencvcontrib/modules directory being incorrect . <p> Thanks for putting this up , and your other posts as well I am really digging the tutorial/ " learning by example " approaches . This is excellent stuff ! <p> Just curious , beyond object detection , have you used CV for " objects in motion " detection ? I know you have done some stuff with people/facial recognition , but I am gearing towards controlling a small quad-copter ( orange cheerson cx-10 ) . I am completely new to CV and still have a lot to learn , but I am trying to figure out the best way to determine 3d movement from a 2d image stream obviously , directions parallel to the plane of viewing is pretty clear ( up , down , left , right ) , but along the plane of view seems more of a challenge ( forward , backward ) . Have you tackled anything like that ? <p> Oh I came across a similar situation in tracking objects and associating a control logic for the motors @ @ @ @ @ @ @ @ @ @ simple . For the depth ( forward and backward ) , what worked for me was the area of ROI rectangle . What I mean <p> The recognized object can encapsulated in a rectangle ( see Adrian previous blogs ) cv2.rectangle from where one is getting x , y , w , h values . w*h provides the area of the rectangle . So as the object moves away from the camera the area of rectangle decreases and as the camera moves closer the area of the rectangle increases . Also at each depth plane the value of this area is unique . This value of the area can be interpreted as Z value and combined with X and Y values can provide variables to do design a control logic . <p> I have yet to play with DLIB library and correlation tracking etc and maybe that will be better . But the above mentioned approach works for me just fine . <p> Do you have comments/adivce on overclocking the Zero for a better Open CV performance ? I am just using for Face Recog using LBPHFace Detector . If @ @ @ @ @ @ @ @ @ @ for the same ? <p> I just followed this post and successfully compiled opencv-3.0.0 on C.H.I.P headless 4.4 . The final build took a little bit more than 5 hours . I did n't  need an external USB drive as the build only needed @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485274 @185274/ <p> Today I am super excited- to share with you the Kickstarter reward levels . <p> This reward list is exclusive to PyImageSearch blog readers and will not be available anywhere else until the Kickstarter campaign launches on January 14th at 10am EST . <p> So why am I sharing the reward list with you now ? To give you the advantage , of course ! <p> When the PyImageSearch Gurus Kickstarter campaign launches on January 14th , there will be 100s of PyImageSearch readers ( not to mention , other Kickstarter users ) reading the campaign page and trying to decide which reward to select . <p> The problem is that if you wait too long- to select the reward level that you want , you might miss out on it ! <p> You see , there are only a handful of early bird spots available at reduced prices &amp; early entry so you 'll definitely want to plan ahead and act quickly if you want to claim your spot ! <p> But since you 're a loyal PyImageSearch reader I wanted you to have this- Kickstarter reward list ahead @ @ @ @ @ @ @ @ @ @ decide which reward level you would like to back . Deciding now will save you time on Wednesday- and help ensure your spot is claimed ! <p> Below you can find the full breakdown of the Kickstarter rewards . Be sure to plan ahead and pick out the reward you want ! - Remember , other PyImageSearch readers and Kickstarter users will be trying to claim these spots too ! <h> Kickstarter Rewards <p> The main reward is a Kickstarter-exclusive- early access pass to PyImageSearch Gurus at either a significantly reduced monthly rate- or a heavily discounted yearly membership . Remember , this course is entirely self-paced so you will be able to work through the lessons at your leisure . However , these monthly and yearly rates are exclusive to the Kickstarter campaign and will not be available once PyImageSearch Gurus officially launches in August . <p> I am also offering a Kickstarter-exclusive printing of my book , Practical Python and OpenCV . Previously , this book only existed as an eBook . But for this Kickstarter campaign I will be having physical copies made , and individually numbering @ @ @ @ @ @ @ @ @ @ be sure to check out this reward , I 'm not sure if my book will ever be offered in print again ! <p> Below is the access timeline for PyImageSearch Gurus based on the reward level you choose : <h> Pledge $120 or more : <p> An individually numbered and hand signed exclusive Kickstarter printing of my book , Practical Python and OpenCV + access to PyImageSearch Gurus after the first 3 waves are in + lock in membership rate of $55/month . UNLIMITED <h> Yearly Memberships Rewards <h> Pledge $250 or more : <p> EARLY BIRD SPECIAL - 1 year access to PyImageSearch Gurus . Be part of the 1st wave inside + an individually numbered and hand signed exclusive Kickstarter printing of my book , Practical Python and OpenCV . Limited ( 3 left of 3 ) <h> Pledge $350 or more : <p> 1 year access to PyImageSearch Gurus . Be part of the 2nd wave inside + an individually numbered and hand signed exclusive Kickstarter printing of my book , Practical Python and OpenCV . Limited ( 5 left of 5 ) <h> Pledge $400 @ @ @ @ @ @ @ @ @ @ . Be part of the 3rd wave inside + an individually numbered and hand signed exclusive Kickstarter printing of my book , Practical Python and OpenCV . Limited ( 8 left of 8 ) <h> Pledge $500 or more : <p> 1 year access to PyImageSearch Gurus . Access to PyImageSearch Gurus after the first 3 waves are in + an individually numbered and hand signed exclusive Kickstarter printing of my book , Practical Python and OpenCV . UNLIMITED <h> A la carte Rewards <h> Pledge $125 or more : <p> A LA CARTE : Give PyImageSearch Gurus a try . This reward will give you access to ONE module of your choice once PyImageSearch Gurus launches . UNLIMITED <h> Pledge $300 or more : <p> A LA CARTE 3 PACK : Take PyImageSearch Gurus for a spin . This reward will give you access to THREE modules of your choice once PyImageSearch Gurus launches . UNLIMITED <h> Special Rewards <h> Pledge $1,000 or more : <p> Hop on a 60 minute long Skype call with me + 2 year membership + 1st wave access + hand signed copy of @ @ @ @ @ @ @ @ @ @ own computer vision projects or talk about PyImageSearch and topics you would like me to cover inside PyImageSearch Gurus . Limited ( 5 left of 5 ) <h> Pledge $2,500 or more : <p> 2 year membership + 1st wave access + hand signed copy of my book + have dinner with me in the NYC area . During dinner we can discuss your own computer vision projects or talk about PyImageSearch Gurus and the topics that interest you . Limited ( 3 left of 3 ) <p> As you can see , there are significant discounts for funding the project early , so be sure to pick out the reward level that you want before the Kickstarter campaign goes live ! <p> And remember , Wednesday- at 10am EST is the big day ! <p> Do n't  forget , there are a very limited number of rewards available for early access at significantly reduced prices , so when you the Kickstarter launches on Wednesday , you 'll definitely want to claim your reward immediately ! <p> I will be posting a link to the Kickstarter on this blog tomorrow , @ @ @ @ @ @ @ @ @ @ the Kickstarter launches @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485276 @185276/ <h> Multiple cameras with the Raspberry Pi and OpenCV <p> I 'll keep the introduction to todays post short , since I think the title of this post and GIF animation above speak for themselves . <p> Inside this post , I 'll demonstrate how to attach- multiple cameras to your Raspberry Piand access all of them using a single Python script . <p> Regardless if- your setup includes : <p> Multiple USB webcams . <p> Or the Raspberry Pi camera module + additional USB cameras <p> the code detailed in this post will allow you to access- all of your video streams - and perform motion detection on each of them ! <p> Best of all , our implementation of multiple camera access with the Raspberry Pi and OpenCV is capable of running in- real-time ( or near real-time , depending on the number of cameras you have attached ) , making it perfect for creating your own multi-camera home surveillance system . <h> Multiple cameras with the Raspberry Pi and OpenCV <p> The Raspberry Pi board has only- one camera port , so you will not be able to @ @ @ @ @ @ @ @ @ @ to perform some extensive hacks to your Pi ) . So in order to attach multiple cameras to your Pi , you 'll need to leverage- at least one ( if not more ) USB cameras . <p> That said , in order to build my own multi-camera Raspberry Pi setup , I ended up using : <p> A Logitech C920 webcam that is plug-and-play compatible with the Raspberry Pi . We can access this camera using either the cv2.VideoCapture- function built-in to OpenCV or the VideoStream- class from this lesson . <p> You can see an example of my setup below : <p> Figure 1 : My multiple camera Raspberry Pi setup . <p> Here we can see my Raspberry Pi 2 , along with the Raspberry Pi camera module ( sitting on top of the Pi 2 ) and my Logitech C920 webcam . <p> The Raspberry Pi camera module is pointing towards my apartment door to monitor anyone that is entering and leaving , while the USB webcam is pointed towards the kitchen , observing any activity that may be going on : <p> Figure 2 : The @ @ @ @ @ @ @ @ @ @ up to my Raspberry Pi , but are monitoring different areas of the room . <p> Ignore the electrical tape and cardboard on the USB camera this was from a previous experiment which should ( hopefully ) be published on the PyImageSearch blog soon . <p> Finally , you can see an example of both video feeds displayed to my Raspberry Pi in the image below : <p> In the remainder of this blog post , well define a simple motion detection class that can detect if a person/object is moving in the field of view of a given camera. - Well then write a Python driver script that instantiates our two video streams and performs motion detection in both of them . <p> As well see , by using the threaded video stream capture classes ( where one thread per camera is dedicated to perform I/O operations , allowing the main program thread to continue unblocked ) , we can easily get our motion detectors for- multiple cameras to run in- real-time on the Raspberry Pi 2 . <p> Let 's go ahead and get started by defining the simple @ @ @ @ @ @ @ @ @ @ <p> In this section , well build a simple Python class that can be used to detect motion in a field of view of a given camera . <p> For efficiency , this class will assume there is only- one object moving in the camera view at a time in future blog posts , well look at more advanced motion detection and background subtraction methods to track multiple objects . <p> accumWeight- : The floating point value used for the taking the weighted average between the current frame and the- previous set of frames . A larger accumWeight- will result in the background model having less " memory " and quickly " forgetting " what previous frames looked like . Using a high value of accumWeight- is useful if you except lots of motion in a short amount of time . Conversely , smaller values of accumWeight- give more weight to the background model than the current frame , allowing you to detect larger changes in the foreground . Well use a default value of 0.5- in this example , just keep in mind- that this is a tunable parameter @ @ @ @ @ @ @ @ @ @ After computing the difference between the current frame and the background model , well need to apply thresholding to find regions in a frame that contain motion this deltaThresh- value is used for the thresholding . Smaller values of deltaThresh- will detect more- motion , while larger values will detect less motion . <p> minArea- : After applying thresholding , well be left with a binary image that we extract contours from . In order to handle noise and ignore small regions of motion , we can use the minArea- parameter . Any region with &gt;minArea- is labeled as " motion " ; otherwise , it is ignored . <p> Finally , - Line 17- initializes avg- , which is simply the running , weighted average of the previous frames the BasicMotionDetector- has seen . <p> Let 's move on to our update- method : <p> Multiple cameras with the Raspberry <p> 32 <p> 33 <p> defupdate ( self , @ @ @ @ @ @ @ @ @ @ containing motion <p> locs= <p> # if the average image is None , initialize it <p> ifself.avg isNone : <p> self.avg=image.astype ( " float " ) <p> returnlocs <p> # otherwise , accumulate the weighted average between <p> # the current frame and the previous frames , then compute <p> # the pixel-wise differences between the current frame <p> # and running average <p> LONG ... <p> LONG ... <p> The update- function requires a single parameter the image we want to detect motion in . <p> Line 21 initializes locs- , the list of contours that correspond to motion locations- in the image . However , if the avg- has not been initialized ( Lines 24-26 ) , we set avg- to the current frame and return from the method . <p> Otherwise , the avg- has already been initialized so we accumulate the running , weighted average between the previous frames and the current frames , using the accumWeight- value supplied to the constructor ( Line 32 ) . Taking the absolute value difference between the current frame and the running average yields regions of the image that @ @ @ @ @ @ @ @ @ @ However , in order to actually detect regions in our delta image that contain motion , we first need to apply thresholding and contour detection : <p> Multiple cameras with the Raspberry <p> 54 <p> 55 <p> # threshold the delta image and apply a series of dilations <p> # to help fill in holes <p> LONG ... <p> cv2.THRESHBINARY ) 1 <p> thresh=cv2.dilate ( thresh , None , iterations=2 ) <p> # find contours in the thresholded image , taking care to <p> # use the appropriate version of OpenCV <p> LONG ... 55211 @qwx675211 <p> cnts=cnts0ifself.isv2 elsecnts1 <p> # loop over the contours <p> forcincnts : <p> # only add the contour to the locations list if it <p> # exceeds the minimum area <p> **36;13060;TOOLONG : <p> locs.append(c) <p> # return the set of locations <p> returnlocs <p> Calling cv2.threshold- using the supplied value @ @ @ @ @ @ @ @ @ @ which we then find contours in ( Lines 37-45 ) . <p> Note : - Take special care when examining- Lines 43-45 . As we know , the cv2.findContours- method return signature changed between OpenCV 2.4 and 3 . This codeblock allows us to use cv2.findContours- in- both OpenCV 2.4 and 3 without having to change a line of code ( or worry about versioning issues ) . <p> Finally , - Lines 48-52 loop over the detected contours , check to see if their area is greater than the supplied minArea- , and if so , updates the locs- list . <p> The list of contours containing motion are then returned to calling method on- Line 55 . <h> Accessing multiple cameras on the Raspberry Pi <p> Now that our BasicMotionDetector- class has been defined , we are now ready to create the multicammotion.py- driver script to access the- multiple cameras with the Raspberry Pi and apply motion detection to each of the video streams . <p> In the case that you- do not want to use the Raspberry Pi camera module and instead want to leverage two USB @ @ @ @ @ @ @ @ @ @ <p> Multiple cameras with the Raspberry Pi and OpenCV <p> Python <p> 1 <p> 2 <p> **32;13098;TOOLONG ( ) <p> **32;13132;TOOLONG ( ) <p> Where the src- parameter controls the index of the camera on your machine . Also note that you 'll have to replace webcam- and picam- with webcam1- and webcam2- , respectively throughout the rest of this script as well . <p> Finally , - Lines 19 and 20 instantiate two BasicMotionDetector- s , one for the USB camera and a second for the Raspberry Pi camera module . <p> We are now ready to perform motion detection in both video feeds : <p> Multiple cameras with the Raspberry <p> 44 <p> 45 <p> # loop over frames from the video streams <p> whileTrue : <p> # initialize the list of frames that have been processed <p> frames= <p> # loop @ @ @ @ @ @ @ @ @ @ ... <p> # read the next frame from the video stream and resize <p> # it to have a maximum width of 400 pixels <p> frame=stream.read() <p> **26;13166;TOOLONG , width=400 ) <p> # convert the frame to grayscale , blur it slightly , update <p> # the motion detector <p> gray=cv2.cvtColor ( frame , cv2.COLORBGR2GRAY ) <p> **26;13194;TOOLONG , ( 21,21 ) , 0 ) <p> locs=motion.update(gray) <p> # we should allow the motion detector to " run " for a bit <p> # and accumulate a set of frames to form a nice average <p> iftotal&lt;32 : <p> frames.append(frame) <p> continue <p> On- Line 24 we start an infinite loop that is used to constantly poll frames from our ( two ) camera sensors . We initialize a list of such frames- on- Line 26 . <p> Then , - Line 29 defines a for- loop that loops over each of the video stream and motion detectors , respectively . We use the stream- to read a frame- from our camera sensor and then resize the frame to have a fixed width of 400 pixels . <p> Further- pre-processing @ @ @ @ @ @ @ @ @ @ frame to grayscale and applying a Gaussian smoothing operation to reduce high frequency noise . Finally , the processed frame is passed to our motion- detector where the actual motion detection is performed ( Line 39 ) . <p> However , its important to let our motion detector " run " for a bit so that it can obtain an accurate running average of what our background " looks like " . Well allow 32 frames to be used in the average background computation- before applying any motion detection ( Lines 43-45 ) . <p> After we have allowed 32 frames to be passed into our BasicMotionDetectors , we can check to see if any motion was detected : <p> Multiple cameras with the Raspberry <p> 65 <p> 66 <p> # otherwise , check to see if motion was detected <p> iflen(locs)&gt;0 : <p> # initialize the minimum and @ @ @ @ @ @ @ @ @ @ respectively <p> ( minX , minY ) = ( np.inf , np.inf ) <p> ( maxX , maxY ) = ( -np.inf , -np.inf ) <p> # loop over the locations of motion and accumulate the <p> # minimum and maximum locations of the bounding boxes <p> forlinlocs : <p> ( x , y , w , h ) =cv2.boundingRect(l) <p> ( minX , maxX ) = ( min ( minX , x ) , max ( maxX , x+w ) ) <p> ( minY , maxY ) = ( min ( minY , y ) , max ( maxY , y+h ) ) <p> # draw the bounding box <p> cv2.rectangle ( frame , ( minX , minY ) , ( maxX , maxY ) , <p> ( 0,0,255 ) , 3 ) <p> # update the frames list <p> frames.append(frame) <p> Line 48 checks to see if motion was detected in the frame- of the current video stream- . <p> Provided that motion was detected , we initialize the minimum and maximum- ( x , y ) -coordinates associated with the contours ( i.e. , locs- ) . @ @ @ @ @ @ @ @ @ @ to determine the smallest bounding box that encompasses- all contours ( Lines 51-59 ) . <p> The bounding box is then drawn surrounding the motion region on- Lines 62 and 63 , followed by our list of frames- updated on- Line 66 . <p> Again , the code detailed in this blog post assumes that there is only- one object/person moving at a time in the given frame , hence this approach will obtain the desired result . However , if there are- multiple moving objects , then well need to use more advanced background subtraction and tracking methods future blog posts on PyImageSearch will cover how to perform multi-object tracking . <p> The last step is to display our frames- to our screen : <p> Multiple cameras with the Raspberry <p> 91 <p> 92 <p> # increment the @ @ @ @ @ @ @ @ @ @ current timestamp <p> total+=1 <p> **33;13222;TOOLONG <p> ts=timestamp.strftime ( " %A %d %B %Y %I : %M : %S%p " ) <p> # loop over the frames a second time <p> LONG ... <p> # draw the timestamp on the frame and display it <p> cv2.putText ( frame , ts , ( 10 , frame.shape0-10 ) , <p> **27;13257;TOOLONG , ( 0,0,255 ) , 1 ) <p> cv2.imshow ( name , frame ) <p> # check to see if a key was pressed <p> **27;13286;TOOLONG <p> # if the q key was pressed , break from the loop <p> ifkey==ord ( " q " ) : <p> break <p> # do a bit of cleanup <p> print ( " INFO cleaning up ... " ) <p> cv2.destroyAllWindows() <p> webcam.stop() <p> picam.stop() <p> Liens 70-72 increments the total- number of frames processed , followed by grabbing and formatting the current timestamp . <p> We then loop over each of the frames- we have processed for motion on- Line 75- and display them to our screen . <p> Finally , - Lines 82-86 check to see if the q- key is pressed @ @ @ @ @ @ @ @ @ @ loop. - Lines 89-92 then perform a bit of cleanup . <h> Motion detection on the Raspberry Pi with multiple cameras <p> To see our multiple- camera motion detector run on the Raspberry Pi , just execute the following command : <p> Multiple cameras with the Raspberry Pi and OpenCV <p> Shell <p> 1 <p> $python multicammotion.py <p> I have included a series of " highlight frames " in the following GIF that demonstrate our multi-camera motion detector- in action : <p> Figure 4 : An example of applying motion detection to multiple cameras using the Raspberry Pi , OpenCV , and Python . <p> Notice how I start in the kitchen , open a cabinet , reach for a mug , and head to the sink to fill the mug up with water this series of actions and motion are detected- on the first camera . <p> Finally , I head to the trash can to throw out a paper towel before exiting- the frame view- of the second camera . <p> A full video demo of multiple camera access using the Raspberry Pi can be seen below : @ @ @ @ @ @ @ @ @ @ how to access multiple cameras using the Raspberry Pi 2 , OpenCV , and Python . <p> When accessing multiple cameras on the Raspberry Pi , you have two choices when constructing your setup : <p> Either use multiple USB webcams . <p> Or using a single Raspberry Pi camera module and- at least one USB webcam . <p> Since the Raspberry Pi board has only one camera input , you can not leverage multiple Pi camera boards atleast without extensive hacks to your Pi . <p> In order to provide an interesting implementation of multiple camera access with the Raspberry Pi , we created a simple motion detection class that can be used to detect motion in the frame views of- each camera connected to the Pi . <p> While basic , this motion detector demonstrated that multiple camera access is capable of being executed in real-time on the Raspberry Pi - especially with the help of our threaded PiVideoStream- and VideoStream- classes implemented in blog posts a few weeks ago . <p> If you are interested in learning more about using the Raspberry Pi for computer vision , @ @ @ @ @ @ @ @ @ @ to OpenCV , - be sure to signup for the PyImageSearch Newsletter using the form at the bottom of this post . <p> See you next week ! <h> Downloads : 55217 @qwx675217 <p> I am personally interested in , and think that of common general use to everyone would be if you could show this with ofcourse a hardware ip-camera , asswell as a ipcamera App running on an Android and/or iphone device like IP Webcam / IP webcam pro with wich anyone can turn a android smartphone into a ip camera . <p> Hi Adrian , thanks for another great tutorial . Up until now , I 've been running OpenCV on my Raspberry Pi , logged into the GUI . I just tried booting the Pi to the console instead and on running any OpenCV project which uses imread , I get a GTK error gtk warning can not open display . Ive read that this is something to do with the X11 server . Have you tried OpenCV when booted into the console instead of the GUI ? Basically I would like to be able to start @ @ @ @ @ @ @ @ @ @ figured it would be a waste of resources having the GUI running in the background . <p> Indeed , anytime you want to use the cv2.imshow method , you 'll need to have a window server running ( such as X11 ) . If you want to start a Python script at boot and have it run in the background , just comment out all of your cv2.imshow and cv2.waitKey calls and your program will run just fine . <p> Xming forwards Xwindow over a remote connection without having to run Xwindow on the host . It works with Putty . But it is very slow . Running FPStest for the RPi camera over Putty , I get , 27.7 FPS / Threaded 215.7 FPS . Enabling a display -d 1 , and using Xming , I get , 2.7 FPS / Threaded 95.0 FPS . Running in xwindow GUI directly on the RPi , I get , 24.2 FPS / Threaded 216.1 FPS . Enabling display -d 1 , I get , 13.3 FPS / Threaded 56.0 FPS . Really a great testament to Adrians VideoStream class or PiVideoStream in @ @ @ @ @ @ @ @ @ @ camera . Regards Jon <p> Thanks for your response , I ran the code you had published for long time and it worked fine , thought I am seeing the message " Select time out " it does not seems to be impacting the function ( may be dropping frames but not sure ) still it working fine with Two Logictec C170 Webcams . I do not have Pi Cameras . ( I am not sure why you are not seeing this message . ) <p> Once again , great work , fantastic post , thanks a lot for sharing your code , I will run the code with more time integrate my own image processing routines and see how it goes <p> Stil I see the message " Select Timeout " my wild guess it may be due to the OS or the USB/Webcam drivers running on my RPi , can you share which model of RPi you are using which Linux image you are using , so that I can replicate the exact setup you have and give it a try <p> Another difference I can think of @ @ @ @ @ @ @ @ @ @ this will make a difference or not <p> I ran the program , everything went smooth . Nonetheless , when I press the q button the program terminated , but one of my webcams did not stop working , and the terminal did not show the &gt;&gt;&gt; anymore . It seemed working on an infinite loop . <p> Any idea what is going wrong ? <p> I am using two usb-webcams ( and I have already modified your code so that it can work well with two usb-webcams ) , and my OS is windows 10 . <p> Please continue with these blogs I am finding them very educational . My question , you make a reference to a multi-object tracking tutorial coming in the future . I would like to add a + to that article in hopes that it will land higher on your priority list . To that end , do you have an idea when you will be releasing such an article ? <p> Hey Mike thanks for suggesting multi-object tracking . I will do a tutorial on it , but to be honest , I 'm @ @ @ @ @ @ @ @ @ @ sure to keep you in the loop ! Comments like these help me prioritize posts , so thanks for that = <p> That 's quite strange , I 'm not sure what the problem is . It seems like the cameras might be drawing too much power and the Pi is shutting down ? You might want to post on the official Raspberry Pi forums and see if they have any suggestions . <p> My context is n't exactly the same since I use the C++ interface of OpenCV , and I am using Linux on a PC ( but I plan to go on Raspberry Pi after ) . I have a problem using multiple cameras though and I hoped that you would have some clues on the cause for that . <p> The problem is that I can not open 2 USB cameras at the same time without having an error from video4linux ( the Linuxs API for webcams , which OpenCV relies on , or so I understand ) . <p> Hey William , thanks for the comment . I 've never tried to use the C++ interface to access multiple @ @ @ @ @ @ @ @ @ @ error is . However , it seems like the same logic should apply . You should be able to create two separate pointers , where each points to the different USB camera src . <p> I plan to do the visual SLAM subject by using raspi computer board connected two usb camera Logitech C920 , but i do n't  know to get the two image and stream frame at the same time , can you give me some practical advice ? look forward your response ! <p> I would suggest using using this blog post as a starting point . You 'll need to combine GPIO code with OpenCV code , which may seem tricky , but once you see my example , its exactly pretty straightforward . <p> Adrian , would you be so kind as to point me in the direction of using just ONE camera ( the PiCam ( IR ) ) and being able to save the output motion capture mpeg ( OR have the ability to save the output motion capture as PNG files ) to a NAS on the same network of the raspberry pi @ @ @ @ @ @ @ @ @ @ processed on the Pi that does what i just mentioned . i am a python novice , but i am pretty sure that i can follow how things are being processed ( like you have in this blog post .. which is very awesome .. almost exactly what i am trying to achieve .. ) <p> If you 're trying to save video clips that contain motion ( or any other " key event " ) , I would recommend reading this tutorial where I explain how to do exactly that . <p> As for your second question , changing the motion box becomes a " drawing " problem at that point . Its not exactly hard , but its not exactly easy either . You 'll need to use built in OpenCV drawing functions to create arrows , rectangles , etc . Its a bit of a pain in the ass , but certainly possible . <p> If you want to draw just the motion field , I would get rid of the bounding box and just call cv2.drawContours on the region instead . <p> I personally have never tried with @ @ @ @ @ @ @ @ @ @ at that point should become power draw . 3 cameras is a lot for the Pi to power . Consider using a secondary power source . The Pi also might not be fast enough for process all of those frames . <p> Anytime you see an error related to an image or frame being NoneType its because either ( 1 ) the image was not read properly from disk or ( 2 ) the frame was not read properly from the video stream . In this case its the latter . Double check that you can access the video streams from your system . Ill try to do a more detailed blog post on this in the future . <p> Hey Jeff unfortunately , I 'm not entirely sure what the error is in this case . If you can access them both individually then your system can certainly open pointers to the cameras . Can you try running both individual cameras at the same time from separate Python scripts ? I 'd be curious if the USB hub does n't  have enough power to run both cameras . <p> It looks @ @ @ @ @ @ @ @ @ @ to run them in separate scripts I get the same error . I 'm going to try a couple other solutions , such as moving the experiment to a laptop where each port is individually powered . Ill post my findings . Thanks <p> That 's very interesting a couple of questions here .. 1 ) What happens if you would also save the video stream from 2 , 3 or 4 cameras ? Would that work or will the performance ( framerate , framedropsetc. ) drop significantly 2 ) What is the limit here ? For example does using 4 USB cameras + Pi Camera work ? Or will that fry the pi : ) ! ? 3 ) Is it possible to save the video stream from multiple cameras + show the cameras output live ? <p> I 've never tried with more than 2 cameras . The issue would become power draw . The Pi by itself likely would n't have enough power to run the USB cameras . You would likely need a USB hub that draws its own power . In either case , you would notice a performance @ @ @ @ @ @ @ @ @ @ Thanks for sharing . I want to know more about the cameras synchronization . Can you tell if they are both shooting at the same time ? Or which is the time difference ? What was the FPS on your project , using two cameras ? Thanks in advance ! <p> I actually do n't  have the FPS recorded when both cameras were running otherwise I would be happy to share . As for synchronization , there is no guarantee that the frames grabbed for each camera were captured at exactly the same time down to the micro-second . That is outside of the capabilities of this project . <p> and first , thank you so much for this tutorial . I-m sooo newbie with Pi things and that-s why I thought to ask you if you can help me . I-ve planned to use 2 x Pi 3 boards with cam &amp; ir sensors installed each other . Do you think it-s possible to use in some way your tutorial to use third Pi 3 board as NVR with external USB disk to these 2 x Pi boards ? @ @ @ @ @ @ @ @ @ @ with Pi 3 boards with cam &amp; ir sensors ? <p> Hmm , I 'm not sure I understand your exact question . Is your goal to have each of these Pis deployed somewhere and then networked together ? The point of this blog post is to connect multiple cameras to a single Pi , not have multiple Pis each using one camera . <p> Hi Adrian , Thank you for your tutorials they are great , i learned a lot , keep up the good work . I have a question , Is it possible , to save the videos from the cameras to be saved in some external storage ( SD or HDD ) and name the videos with current date-stamp ? If yes , how do i implement it in LONG ... Thank you . <p> I demonstrate how to save videos captured from cameras to disk in this blog post . Simply specify the file path to be one of your external storage devices . The filename can be created by using a number of Python libraries . I would use datetime . <p> Hi Adrian Started @ @ @ @ @ @ @ @ @ @ wing UAV for Search &amp; Rescue . My first tinkerings were on a PC , Win 10 and C++ . A week ago I received a RPi3 and camera , and it all seems to be python ! So your posts have been most informative and helpful , thank you . Just a few questions if you can find the time . Can the resolutions set in the VideoStream Class if the imutils module be changed in a program ? How do I " placed the BasicMotionDetector class inside the pyimagesearch module for organizational purposes " ? Occasionally when running a python program accessing a web Cam I get an error message VIDIOCDQBUF : No such device , it even happened on the threading FPS test after about 5 runs , then when I look at /dev , video0 is gone and video1 is there . Do you know anything about this ? Keep up the great work Regards Jon <p> Hi Adrian I was constantly getting the " VIDIOCDQBUF : No such device " fault when running the Web Cam FPS test with the display activated using Putty and @ @ @ @ @ @ @ @ @ @ test and the cam would become /dev/video1 . I changed the device number in the program to 1 , run the FPStest with -d 1 and about half way through I again got the " VIDIOCDQBUF : No such device " fault , in /dev , video1 was gone and video0 was back ! Unpluging/pluging in the cam would also give me back /dev/video0 . I have now used a powered USB hub to connect the WebCam and been running the FPS WebCam test for over an hour with no problems . Just thought I 'd let you know . PS Web cam FPS test results are : Over Putty 8.4 FPS / Threaded 55.7 FPS Over Putty using Xming and display enabled -d 1 , 2.3 FPS / Threaded 38.5 FPS Love your threading program . Regards Jon <p> I tried to make it work with 2 USB cams but without all those motion stuff . Just wanted 2 streams in 2 windows . So I tried to remove all motion related stuff . Did n't  work . Moved from one error to the other . Any suggestions or some @ @ @ @ @ @ @ @ @ @ Hey Pavel its hard to say what your error is without knowing the error message . My main suggestion here would be to delete the motion detector classes using an IDE like PyCharm so it will " highlight " any code that will throw an error by deleting these classes . <p> One question : When the RPi detects movement , I want to save a few seconds of video to a . h264 video file . I cant seem to find any examples of this online . Is there some keyword I should be looking for , or do you have a tutorial on this ? <p> It depends on what your camera sensors are possible at recording/retrieving frames at . I do n't  have any specific tutorials on adjusting the actual physical FPS Of the camera , but I will try to cover this in the future . The problem is that the function calls required to do this do n't  work on every camera due to driver issues . But to start , you should read up on the documentation of your camera to see if it @ @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485278 @185278/ <h> OpenCV panorama stitching <p> In todays blog post , I 'll demonstrate how to perform image stitching and panorama construction- using Python and OpenCV . Given two images , well " stitch " them together to create a simple panorama , as seen in the example above . <p> Well encapsulate all four of these steps- inside panorama.py- , where- well define a Stitcher- class used to construct our panoramas . <p> The Stitcher- class will rely on the imutils Python package , so if you do n't  already have it installed on your system , you 'll want to go ahead and do that now : <p> OpenCV panorama stitching <p> Shell <p> 1 55204 @qwx675204 <p> Let 's go ahead and get started by reviewing panorama.py- : <p> OpenCV panorama stitching <p> Python <p> 1 <p> 2 <p> 3 <p> 4 <p> 5 <p> 6 <p> 7 <p> 8 <p> 9 55203 @qwx675203 55220 @qwx675220 55219 @qwx675219 <p> importcv2 <p> classStitcher : <p> definit(self) : <p> # determine if we are using OpenCV v3.X <p> **25;13315;TOOLONG <p> We start off on- Lines 2-4- by importing our necessary packages . @ @ @ @ @ @ @ @ @ @ a set of OpenCV convenience methods , and finally cv2- for our OpenCV bindings . <p> The stitch- method requires only a single parameter , images- , which is the list of ( two ) images that we are going to stitch together to form the panorama . <p> We can also optionally supply ratio- , used for David Lowes ratio test when matching features- ( more on this ratio test later in the tutorial ) , reprojThresh- which is the maximum pixel " wiggle room " allowed by the RANSAC algorithm , and finally showMatches- , a boolean used to indicate if the keypoint matches should be visualized or not . <p> Line 15 unpacks the images- list ( which again , we presume to contain only two images ) . The ordering to the images- list is important : we expect images to be supplied in- left-to-right order . If images are- not supplied in this order , then our code will still run but our output panorama will only contain one image , not both . <p> Once we have unpacked the images- list , we make @ @ @ @ @ @ @ @ @ @ 17 . This method simply detects keypoints and extracts local invariant descriptors ( i.e. , SIFT ) from the two images . <p> Given the keypoints and features , we use matchKeypoints- ( Lines 20 and 21 ) to match the features in the two images . Well define this method later in the lesson . <p> If the returned matches M- are None- , then not enough keypoints were matched to create a panorama , so we simply return to the calling function ( Lines 25 and 26 ) . <p> Otherwise , we are now ready to apply the perspective transform : <p> <p> 44 <p> 45 <p> # otherwise , apply a perspective warp to stitch the images <p> # together <p> ( matches , H , status ) =M <p> **33;13342;TOOLONG , H , <p> LONG ... <p> LONG ... <p> # check to see if the keypoint matches @ @ @ @ @ @ @ @ @ @ status ) <p> # return a tuple of the stitched image and the <p> # visualization <p> return ( result , vis ) <p> # return the stitched image <p> returnresult <p> Provided that M- is not None- , we unpack the tuple on- Line 30 , giving us a list of keypoint matches- , the homography matrix H- derived from the RANSAC algorithm , and finally status- , a list of indexes to indicate which keypoints in matches- were successfully spatially verified using RANSAC . <p> Given our homography matrix H- , we are now ready to stitch the two images together . First , we make a call to cv2.warpPerspective- which requires three arguments : the image we want to warp ( in this case , the- right- image ) , the- 3 x 3- transformation matrix ( H- ) , and finally the shape out of the output image . We derive the shape out of the output image by taking the sum of the widths of both images and then using the height of the second- image . <p> Line 30 makes a check to see @ @ @ @ @ @ @ @ @ @ so , we make a call to drawMatches- and return a tuple of both the panorama and visualization to the calling method ( Lines 37-42 ) . <p> Otherwise , we simply returned the stitched image ( Line 45 ) . <p> Now that the stitch- method has been defined , let 's look into some of the helper methods that it calls . Well start with detectAndDescribe- : <p> <p> 71 <p> 72 <p> **25;13377;TOOLONG , image ) : <p> # convert the image to grayscale 55215 @qwx675215 <p> # check to see if we are using OpenCV 3 . X <p> ifself.isv3 : <p> # detect and extract features from the image <p> **39;13404;TOOLONG <p> LONG ... <p> # otherwise , we are using OpenCV 2.4 . X <p> else : <p> # detect keypoints in @ @ @ @ @ @ @ @ @ @ <p> # extract features from the image <p> LONG ... <p> ( kps , features ) =extractor.compute ( gray , kps ) <p> # convert the keypoints from KeyPoint objects to NumPy <p> # arrays <p> kps=np.float32 ( kp.pt forkp inkps ) <p> # return a tuple of keypoints and features <p> return ( kps , features ) <p> As the name suggests , the detectAndDescribe- method accepts an image , then detects- keypoints and extracts- local invariant descriptors . In our implementation we use the Difference of Gaussian ( DoG ) keypoint detector and the SIFT feature extractor . <p> On- Line 52 we check to see if we are using OpenCV 3 . X. If we are , then we use the **27;13509;TOOLONG function to instantiate both our DoG keypoint detector and SIFT feature extractor . A call to detectAndCompute- handles extracting the keypoints and features ( Lines 54 and 55 ) . <p> Its important to note that you- must have compiled OpenCV 3 . X with opencvcontrib support enabled . If you did not , you 'll get an error such as AttributeError : ' module'objecthas no @ @ @ @ @ @ @ @ @ @ over to my OpenCV 3 tutorials page where I detail how to install OpenCV 3 with opencvcontrib- support enabled for a variety of operating systems and Python versions . <p> Lines 58-65 handle if we are using OpenCV 2.4 . The **26;13538;TOOLONG function instantiates- our keypoint detector ( DoG ) . A call to detect- returns our set of keypoints . <p> From there , we need to initialize **30;13566;TOOLONG using the SIFT- keyword to setup our SIFT feature extractor- . Calling the compute- method of the extractor- returns a set of feature vectors- which quantify the region surrounding each of the detected keypoints in the image . <p> Finally , our keypoints are converted from KeyPoint- objects to a NumPy array ( Line 69 ) and returned to the calling method- ( Line 72 ) . <p> Next up , let 's look at the matchKeypoints- method : <p> <p> 86 <p> 87 <p> LONG ... <p> ratio , reprojThresh ) @ @ @ @ @ @ @ @ @ @ list of actual <p> # matches <p> LONG ... <p> LONG ... <p> matches= <p> # loop over the raw matches <p> forminrawMatches : <p> # ensure the distance is within a certain ratio of each <p> # other ( i.e. Lowe 's ratio test ) <p> LONG ... <p> matches.append ( ( m0.trainIdx , m0.queryIdx ) ) <p> The matchKeypoints- function requires four arguments : the keypoints and feature vectors associated with the first image , followed by the keypoints and feature vectors associated with the second image . David Lowes ratio- test variable and RANSAC re-projection threshold are- also be supplied . <p> Matching features- together is actually a fairly straightforward process . We simply loop over the descriptors from both images , compute the distances , and find the smallest distance for each pair of descriptors . Since this is a very common practice in computer vision , OpenCV has a built-in function called **28;13598;TOOLONG that constructs the feature matcher for us . The BruteForce- value indicates that we are going to exhaustively compute the Euclidean distance between all feature vectors from both images and find the @ @ @ @ @ @ @ @ @ @ A call to knnMatch- on- Line 79 performs k-NN matching between the two feature vector sets using- k=2- ( indicating the top two matches for each feature vector are returned ) . <p> The reason we want the top two matches rather than just the top one match is because we need to apply David Lowe 's ratio test for false-positive match pruning . <p> Again , - Line 79 computes the rawMatches- for each pair of descriptors - but there is a chance that some of these pairs are false positives , meaning that the image patches are not actually true matches . In an attempt to prune these false-positive matches , we can loop over each of the rawMatches- individually ( Line 83 ) and apply Lowe 's ratio test , which is used to determine high-quality feature matches . Typical values for Lowes ratio are normally in the range- 0.7 , 0.8 . <p> Once we have obtained the matches- using Lowes ratio test , we can compute the homography between the two sets of keypoints : <p> <p> 103 <p> 104 <p> # computing a homography requires at least 4 matches <p> iflen(matches)&gt;4 : <p> # construct the two sets of points <p> ptsA=np.float32 ( kpsAifor ( , i ) inmatches ) <p> ptsB=np.float32 ( kpsBifor ( i , ) inmatches ) <p> # compute the homography between the two sets of points <p> LONG ... <p> reprojThresh ) <p> # return the matches along with the homograpy matrix <p> # and status of each matched point <p> return ( matches , H , status ) <p> # otherwise , no homograpy could be computed <p> returnNone <p> Computing a homography between two sets of points requires- at a bare minimum an initial set of four matches . For a more reliable homography estimation , we should have substantially more than just four matched points . <p> Finally , the last method in our Stitcher- method , drawMatches- is used to visualize keypoint correspondences between two images : <p> <p> 124 <p> 125 <p> LONG ... <p> # initialize the output visualization image <p> ( hA , wA ) =imageA.shape:2 <p> ( hB , wB ) =imageB.shape:2 <p> LONG ... <p> vis0:hA,0:wA=imageA <p> vis0:hB , wA : =imageB <p> # loop over the matches <p> LONG ... <p> # only process the match if the keypoint was successfully <p> # matched <p> ifs==1 : <p> # draw the match <p> LONG ... <p> LONG ... <p> cv2.line ( vis , ptA , ptB , ( 0,255,0 ) , 1 ) <p> # return the visualization <p> returnvis <p> This method requires that we pass in the two original images , the set of keypoints associated with each image , the initial matches after applying Lowes ratio test , and finally the status- list provided by the homography calculation . Using these variables , we can visualize the " inlier " keypoints by drawing a straight line from keypoint N in the first image to keypoint M in the second image . <p> @ @ @ @ @ @ @ @ @ @ move on to creating the stitch.py- driver script 11 <p> 12 <p> 13 55203 @qwx675203 <p> **26;13628;TOOLONG importStitcher 55218 @qwx675218 55219 @qwx675219 <p> importcv2 55202 @qwx675202 55206 @qwx675206 <p> ap.addargument ( " -f " , " --first " , required=True , <p> help= " path to the first image " ) <p> LONG ... <p> help= " path to the second image " ) 55208 @qwx675208 <p> We start off by importing our required packages on- Lines 2-5 . Notice how we 've placed the panorama.py- and Stitcher- class into the pyimagesearch- module just to keep our code tidy . <p> Note : - If you are following along with this post and having trouble organizing your code , please be sure to download the source code using the form at the bottom of this post . The . zip of the code download will run out of the box without any errors . <p> From there , - Lines 8-14- parse our @ @ @ @ @ @ @ @ @ @ to the first image in our panorama ( the left-most image ) , and --second- , the path to the second image in the panorama ( the right-most image ) . <p> Remember , these image paths need to be suppled in left-to-right order ! <p> The rest of the stitch.py- driver script simply handles loading our images , resizing them ( so they can fit on our screen ) , and constructing our panorama : <p> <p> 30 <p> 31 <p> # load the two images and resize them to have a width of 400 pixels <p> # ( for faster processing ) <p> imageA=cv2.imread ( args " first " ) <p> imageB=cv2.imread ( args " second " ) <p> **28;13656;TOOLONG , width=400 ) <p> **28;13686;TOOLONG , width=400 ) <p> # stitch the images together to create a panorama <p> stitcher=Stitcher() <p> LONG ... <p> # show the images <p> cv2.imshow ( " Image @ @ @ @ @ @ @ @ @ @ B " , imageB ) <p> cv2.imshow ( " Keypoint Matches " , vis ) <p> cv2.imshow ( " Result " , result ) 55212 @qwx675212 <p> Once our images are loaded and resized , we initialize our Stitcher- class on Line 23 . We then call the stitch- method , passing in our two images ( again , in left-to-right order ) and indicate that we would like to visualize the keypoint matches between the two images . <p> Finally , - Lines 27-31- display our output images to our screen . <h> Panorama stitching results <p> In mid-2014 I took a trip out to Arizona and Utah to enjoy the national parks . Along the way I stopped at many locations , including Bryce Canyon , Grand Canyon , and Sedona . Given that these areas contain beautiful scenic views , I naturally took a bunch of photos some of which are perfect for constructing panoramas . I 've included a sample of these images in todays- blog to demonstrate panorama stitching . <p> All that said , let 's give our OpenCV panorama stitcher a try . Open up @ @ @ @ @ @ @ @ @ @ panorama stitching <p> Shell <p> 1 <p> 2 <p> $python stitch.py--first images/bryceleft01.png <p> --second images/bryceright01.png <p> Figure 1 : ( Top ) The two input images from Bryce canyon ( in left-to-right order ) . ( Bottom ) The matched keypoint correspondences between the two images . <p> At the top of this figure , we can see two input images ( resized to fit on my screen , the raw . jpg files are a much- higher resolution ) . And on the bottom , we can see the matched keypoints between the two images . <p> Using these matched keypoints , we can apply a perspective transform and obtain the final panorama : <p> Figure 2 : Constructing a panorama from our two input images . <p> As we can see , the two images have been successfully stitched together ! <p> Note : - On many of these example images , you 'll often see a- visible- " seam " running through the- center of the stitched images . This is because I shot many of photos using either my iPhone or a digital camera with autofocus turned @ @ @ @ @ @ @ @ @ @ shot. - Image stitching and panorama construction work best when you use the same focus for every photo . I never intended to use these vacation photos for image stitching , otherwise I would have taken care to adjust the camera sensors . In either case , just keep in mind the seam is due to varying sensor properties at the time I took the photo and was not intentional . <p> In the above input images we can see heavy overlap between the two input images . The main addition to the panorama is towards the right side of the stitched images- where we can see more of the " ledge " is added- to the output . <p> here 's another example from the Grand Canyon : <p> OpenCV panorama stitching <p> Shell <p> 1 <p> 2 <p> $python stitch.py--first **28;13716;TOOLONG <p> --second **29;13746;TOOLONG <p> Figure 5 : Using image stitching to build a panorama using OpenCV and Python . <p> From this example , we can see that more of the- huge expanse of the Grand Canyon has been added to the panorama . <p> Finally , let 's @ @ @ @ @ @ @ @ @ @ from Sedona , AZ : <p> OpenCV panorama stitching <p> Shell <p> 1 <p> 2 <p> $python stitch.py--first images/sedonaleft01.png <p> --second images/sedonaright01.png <p> Figure 6 : One final example of applying image stitching . <p> Personally , I find the red rock country of Sedona to be one of the most beautiful areas I 've ever visited . If you ever have a chance , definitely stop by you wont be disappointed . <p> So there you have it , image stitching and panorama construction- using Python and OpenCV ! <h> Summary <p> In this blog post we learned how to perform image stitching and panorama construction using OpenCV. - Source code was provided for image stitching for- both OpenCV 2.4 and OpenCV 3 . <p> While simple , this algorithm works well in practice when constructing panoramas for two images . In a future blog post , well review how to construct panoramas and perform image stitching- for more than two images . <p> Anyway , I hope you enjoyed this post ! Be sure to use the form below to download the source code and give it a try @ @ @ @ @ @ @ @ @ @ . As for stitching Aerial View imagery mainly for mapping purpose , can it be used to stitch second image that " overlap " on below part of the first image ( not the left-to-right , but bottom-to-upper part of image ) ? <p> Yes , you can use it to stitch bottom-to-top images as well , but you 'll need to change Lines 31-33 to handle allocating an image that is tall rather than wide and then update the array slices to stack the images on top of each other . But again , yes , its totally possible . <p> Unfortunately , the stitcher functionality in OpenCV 3.1 or more precisely the HomographyBasedEstimator and the BunderAdjuster used by the Stitcher class will only estimate camera rotations ( no camera translations ) , i.e. it assumes that all camera centers are approximately equal . <p> If the camera experience translations ( like aerial shots ) or translations in general , the obtained results are usually not that great even though the images can be matched given good keypoints . <p> A very good topic you have covered in this post @ @ @ @ @ @ @ @ @ @ regarding an OCR problem , i have first version of your book where you have described digit recognition using HOG features , that algorithm works on contour detection ( blob based ) , my question is what may be the other way to approach the problem where i cant get individual contours for each digit or character ( Segmentation is not possible ) , thanks for your suggestion in advance . and a big thank you for writing a very easy to understand book . <p> Are you referring to cursive handwriting where the characters are not individually segment-able ? If so , I think R-CNNs are a likely candidate if you have enough training data . Otherwise , you might want to look at the Street View House Numbers Dataset and the relevant papers associated with high accuracy results . <p> Hi Adrian thanks for your reply , actually i am working something like vehicle registration data extraction through registration card into a json file , where there are some fixed fields like name and address and their respective variables . i have captured a few images of registration @ @ @ @ @ @ @ @ @ @ and in some cases minor orientation changes also there , a big advantage here is there is no hand written letters or digits so variability of data is less , and all alphabets are in upper case , but at the time of segmentation ( image Thresholding ) some letters got merged in a single blob ( or contour ) so i cant extract each letter individually . so i cant apply blob based analysis , i have tried few pre-processing steps to separate the blobs but it results in some useful structural information loss , what should i do here . <p> I would suggest sending me an email so we can chat more offline about it . This post is about panorama construction , not digit extraction , so it would be great if we could keep the comments thread on topic . <p> Great post . Ive been wanting to try use OpenCV for orthophoto stitching of aerial photos from drones . <p> In terms of the seam that 's surely to do with different exposures and not focusing . Typically for landscape photos the focus will be @ @ @ @ @ @ @ @ @ @ used in the past has a feature to try and equalize the exposures so that the seam is n't visible . Sounds like potentially a topic for another OpenCV blog post = <p> Great point Sean ! I ran a few tests just using images around the apartment captured with my iPhone with a fixed exposure . This got rid of the seam . But of course , it requires that your environment does n't  change dramatically through the Panorama . <p> An alternative is to apply a blending mask along the seam , this way you avoid having to adjust exposure between images . If the difference in exposure is small between the neighbouring images , it hides the seam nicely . <p> Its more intuitive for us to think of a panorama being rendered from left-to-right ( which is an assumption this code makes ) . We reverse the unpacking in the stitch method for the actual matching of the keypoint detection and local invariant descriptors . Again , order does matter when it comes to the stitching of the images . <p> hi adrian i 'm vijay ive configured @ @ @ @ @ @ @ @ @ @ ! still to do some project does we need the any ide like eclipes ? ive just stucked in here will you please help me .. thanks in advance .. <p> Hey Adrian , First thanks for your blogpost , really well explained ! As for my problem , I am trying to make your code work for three images and I cant seem to obtain a good result ( the right side of the image gets deformed and pixelized ) . I did not get to creative I just conduct the exact same operation you did on 2 pictures . First I stitch picture A and B ( call the result R1 ) , then picture B and C ( R2 ) and finally I stitch R1 and R2 . You said you 'll post how to do panorama with +2 images , do you have a better algorithm in mind ? <p> When you say " get rid of " the black borders , do you mean simply setting the border to white ? If so , first create the result image using np.ones and fill them with ( @ @ @ @ @ @ @ @ @ @ ) rather than 0 ( black ) . <p> Keep in mind that an image is always rectangular . You cant set the pixels to " null " since they are part of the image matrix . You can change their actual color ( such as making them black or white ) , but you cant " remove " the pixels from the image . <p> Hi Adrian , Is it possible to save the output image without the black excess ? I know its because it computed based on the width of the two images , is there a way to save the image without it ? If so , how do i do this ? Thank You <p> I personally havent done this , but yes , it is possible . You basically need to find where the black excess is surrounding the image . A hacky way to do this would be to apply thresholding and find the contour of the image itself . A better approach would be to examine the homography/warping matrix and figure out the coordinates of where the valid stitched image is . @ @ @ @ @ @ @ @ @ @ , but will remove the black excess . Ill try to do a blog post on this topic in the future . <p> By intersection I mean that I only want the parts image portion that is present in both images . I tried using bitwiseand ( with the result of your cv2.warpPerspective in the stitch function and with the other image from the panorama ) , but the image it outputs has its colors all messed up . I do n't  really know how to create a mask correctly in order to achieve what I want . <p> Instead of creating a mask , the best option is to explore the ( x , y ) -coordinates of the matched feature vectors . Find the ( x , y ) -coordinates of the matched keypoints that correspond to the top-left , top-right , bottom-right , and bottom-left corners . From there , you can crop out the overlapping ROI . <p> Hello , really enjoying your tutorials but Ive run into a little snag . I have been trying to get this one to work but I keep getting @ @ @ @ @ @ @ @ @ @ , vis ) = stitcher.stitch ( imageA , imageB , showMatches=True ) . I even downloaded the source files too and they do n't  seem to work . Pretty new to python so I am not sure why this is happening . Any ideas ? <p> The stitch method will return None if not enough keypoints are matched to compute the homography . What images are you using ? Are they the same images as in this post or ones of your own ? If they are ones of your own , then there are not enough matched keypoints to stitch the images together . In that case , I would try different combinations of keypoint detectors and feature descriptors . It could also be the case that the images simply can not be stitched together . <p> Ah okay , that 's good to know ! I guess that the images do n't  have enough keypoints then . And I did get the posts pictures to work , I was using two of the left images instead of a left and right Looks like it was mainly human error . @ @ @ @ @ @ @ @ @ @ becomes very problematic because at least one of the images needs to act as a " reference point " . This is why panorama apps normally tend to make sure you move from left-to-right or down-to-up . Without setting an initial reference point , you have to resort to heuristics , which often fail . <p> So I 've being looking at this for a bit now , and have managed to find something that you may be interested in . I plotted all the matched points ( called " matches " from line 30 in your code ) of when images are inputted left to right and when inputted right to left . I found out that these points make roughly a line , and it is possible to calculate the slope of such a line . The slope of the left to right instance should always be smaller than the right to left instance . Therefore its possible to throw in a simple if statement and make a swap of variables . I have tested this theory with many images , and it seems to work very well . @ @ @ @ @ @ @ @ @ @ cases ( like being equal for example ) . Just thought you or some reader would be interested to know . <p> Thanks for sharing , and great investigative work ! This is certainly an interesting heuristic however , its a heuristic that is easily broken . For certain situations though , this might work well enough . <p> Stitching &gt; 2 images together is substantially harder than stitching 2 images together . The code has to change dramatically . I will have to write a separate blog post on this , but I 'm honestly not sure when I 'll be able to . <p> Hi Adrian , I 'm so grateful for that brilliant Tutorial but I have a problem ! when I try to run the code from Terminal nothing will be shown on screen although it gives NO error and first/second parameters are set perfectly .. I have the the latest version of OpenCV , is there any suggestion to help me , please my project should be finished in less than a week wainting for ur reply thanks again = <p> If nothing is getting displayed to your @ @ @ @ @ @ @ @ @ @ being computed ( in which case there are not enough keypoint matches ) . I would suggest inserting some print statements to help you debug where the code is existing . <p> You are calling stitch.py with the left image as first argument , that becomes imageA , and you warp it with warpPerspective . But what you say in the text is that warpPerspective gets as input the image we want to warp : the right image . <p> Very informative post . Thanks for it . I just wanted to know can you direct me to some post which is about spherical stitching . Actually i have 7 images from gopros ( Including top and bottom ) I want to create a spherical panorama from it . Any idea how that can be done . <p> Hi Adrian , excellent work with this example . I 'm trying to implement image stitching from a video using this example , but I can not make it work , none of images retrieves from video have successfuly stitched together . I 've searched a lot of examples using OpenCV with Java , @ @ @ @ @ @ @ @ @ @ some highlights to accomplish image stitching from a video it would be great ! ! thanks in advance = <p> Hey Adrian , I was wondering if it would be possible to take multiple images of a slide and stitch these together to create a very high resolution image of the whole slide . If so I was wondering why when I run the code it says segmentation fault ( core dumped ) <p> You 're likely trying to stitch together very large images and thus detecting TON of keypoints and local invariant descriptors ( hence the memory error ) . Instead , load your large images into memory , but then resize them to be be a maximum of 600 pixels along their largest dimension . Perform keypoint detection and matching on the smaller images ( we rarely process images larger than 600 pixels along their maximum dimension ) . <p> Once you have your homography matrix , apply it to your original high resolution images . <p> Great Tutorial this is awesome ! I had an interesting idea . I was wondering if it was possible to combine photos @ @ @ @ @ @ @ @ @ @ project I want to use a video to create a panorama . Using OpenCV to parse through the frames I would stitch one photo to the combined strip . This would mean that the left/first photo would be a lot wider than the right/second photo . Just looking for your opinion . Also your book is great I have been using it for my research . <p> Your images certainly do n't  have to be the same size or from the same camera sensor to be stitched together provided enough keypoints match ; however , I think you 're going to get mixed results that may not be aesthetically pleasing . If one image has a different exposure than the other then you 'll need to correct the final image by applying image blending . <p> How can I evaluate quantitatively different feature descriptors and extractors performance on the same image pair ? Which parameters can tell me about the accuracy of feature matching using different descriptors and extractors ? Is Lowe ratio or repError doing that also ? <p> If I understand your question right you are trying to determine which @ @ @ @ @ @ @ @ @ @ for a given set of image ? The Lowe ratio test is used to avoid false positive matches . On the other hand , the RANSAC method is used to actually spatially verify these matches . <p> I was wondering if you happened to do the other blog post where you stitched multiple images together ? I modified ur code from this example to linearly stitch images but am struggling to find a way to stitch images regardless of orientation . Any help is appreciated . <p> Also If you know of any tutorials on how to build the opencvconrib modules for opencv 3 on windows that would be a god send . <p> Hey Chris , I have n't had a chance to write up another blog post on image stitching . Most of my current posts have been related to deep learning and updated install tutorials for Ubuntu and Mac ( I do n't  support Windows on this blog ) . Ill try to circle back to the image stitching post , but I honestly cant say when that might be . <p> First of all , thanks A @ @ @ @ @ @ @ @ @ @ using a lot since I got an interest in image processing with Python ! <p> I enjoy a lot both the quality and the pedagogy of your guides &amp; solutions = <p> I have a question about this one I cant find the answer of on my own : I 'm trying to get a " reduction factor " of the homographies I compute . Which would mean something like : if a segment measures 10 pixels before warping , how long is it after warping . Because of deformation , there 's no unique value but I guess it could be possible to have the value range ? I was thinking maybe calculate the total distance between all matching key points on image A then the total distance between all matching key points on image B and calculate the ratio of those 2 values ? If there 's enough and well-reparted matching points that should give me an average reduction factor shouldnt it ? Unless there 's something built-in and more reliable with a cv2 function ? <p> Hmmm , this is a good question . I 'm not sure there is a built-in @ @ @ @ @ @ @ @ @ @ perspective transform is going to cause the images to be warped . I guess my question is what are you trying to accomplish by computing this value ? <p> Thanks again for all your effort into building this OpenCV community . I had a question regrading the photos themselves . You implementation requires a certain order for the images to be piped into your program . TO my understanding this is due to the warp perspective and the way you link the images from line 31-33 in panorama.py . I was wondering if there is a way to modify that section of code to allow for any order of images to be passed through ( I 've been trying my own thing to no avail ) ? <p> You are correct , we assume a specific order passed to the function this is by far the easiest method . For multiple images you actually keypoint matching on all the frames and then define a " path finding " algorithm that can order the frames . But for two images , I think its much easier to define a function that expects @ @ @ @ @ @ @ @ @ @ look at Dijkstras algorithm and dynamic programming to start . I do n't  know of any examples off the top of my head that implement this explicitly for image stitching . But again , this will ( ideally ) be a topic that I 'll cover in a future PyImageSearch post , I 'm just not sure when . <p> Thank you for your response ! I actually did find that blog post later , and you 'll notice that I made a comment on there as well , I 'm curious about how one might go about streaming the stitched video to something like a VR headset , or just streaming it in general . Just looking for some ideas . <p> I do n't  do any work with VR headsets ; however , there are a number of different streaming protocols . I would suggest doing research on your particular headset and see if its possible to stream the frames from the headset itself . Regarding streaming in general , I plan to cover that in a future blog post , although I 'm not entirely sure when that will be . <p> @ @ @ @ @ @ @ @ @ @ wondering if you could help me a bit further how to stitch four images together ( 2+2 grid ) , or guide me in the right direction . Basically I have four cameras in a grid filming a large space and which overlap by 1/5th of the space on the sides and top/bottom . I got your script working on two side-by-side images , but how could I adapt your script to stitch all four images together ? Is there an easy way to adapt your script by first stitching the top 2 and then the bottom 2 and then stitching those new top and bottom images together ? Any help is appreciated . I will then actually try and use the stiching parameters to use in stitching frames from the video together . Thanks again ! <p> We are also looking for a way to stitch video from multiple cameras together . IN our arrangment , the cameras are pointing from the edge of the space towards the center of the space , the opposite from most rigs today . This arrangment is basically bullet time . If we @ @ @ @ @ @ @ @ @ @ example put one on each of four walls in a room at the same x/y on each wall ) can we stitch the video together ? <p> Thanks , I had n't  seen you reply hence the delay . Yes the cameras are fixed and non-moving . I calibrated them to account for any distortion of the lenses . But then using your script on just the top two cameras it does warp the right camera based on the left . I would like all four to be stitched with all having the ability to be warped . Do you have any further tips , also what you mean regarding calibrating the cameras ? I appreciate it ! <p> I was wondering how may I perform a cylindrical/inverse cylindrical projection before of the candidate images to be stitched together . This will help me to stitch multiple images taken from a rotating base . <p> The segmentation fault is helpful , its likely that this is a local invariant descriptor issue . I would start to debug the problem by inserting print statements throughout your code until you can narrow @ @ @ @ @ @ @ @ @ @ Also , make sure you use the " Downloads " section of the post to download the code ( if you have n't done so already ) rather than copying and pasting . This will ensure you are using the same codebase and project structure . <p> I still do n't  quite get it . Can you tell me what to do step by step after downloading the zip file ? When and if you get a chance . It would be really helpful . I personally do not know much of python since i was taught on Java and a little bit of C++ . <p> I got on the command line and went to the file ( i put it on my desktop and did " cd desktop =&gt; panorama-stitching " and tried to run stitch.py ) and had that trouble . and doing " stitch.py bryceleft02.png bryceright02.png " had the same result . Thanks . <p> Hi Enkhbold while I 'm happy to help point readers in the right direction , the PyImageSearch blog does assume you 've had some basic programming experience and are comfortable working with the command @ @ @ @ @ @ @ @ @ @ execute a Python script . I would recommend you go through my list of recommended Python resources to help you learn the basics first . <p> Was able to make it work ( found the comment on top of the stitch.py ) i was trying " python stitch.py first images/bryceleft01.png second images/bryceright01.png " from the picture above and what made it not work was that backslash . Now i want to stitch multiple images can you provide the page you talked about that if you made it ? Me and my teacher are trying to make a program that constantly reads images from a video and saves it into a big panoramic picture to make it easier for watchers to see what was on a few secs or way back whenever they want. ( the idea came from MIT lectures and because the video can only show so much , or it would make things impossible to read since its too small ) So again , Is there a multiple image stitching method you made that i can take a look at ? <p> Hi , There is a reason @ @ @ @ @ @ @ @ @ @ and not the gray one ? ( OpenCV3 option ) <p> attached : ( kps , features ) = **33;13777;TOOLONG , None ) <p> instead of ( kps , features ) = **32;13812;TOOLONG , None ) <p> I 'm trying to do something similar with videos , I have two cameras , one PTZ and one wide and I 'm drawing a rectangular on the wide one of what the PTZ is showing and its really slow , I tried to use threads but still not close to real time . any suggestion ? <p> Thank you for pointing out this typo we typically detect keypoints and extract local invariant descriptors from grayscale rather than multi-channel images . The gray image should be used instead , but in most cases you wo n't notice any changes in performance . <p> I want to control a servo depending on the overlap between two pictures .. is no overlap I will rotate the servo till there is .. I tried to push two pictures of different places and it still finds keypoints .. how can I prevent that or at least increase the occuracy <p> @ @ @ @ @ @ @ @ @ @ overlap is to match the keypoints . Once you have the ( x , y ) -coordinates of the matched keypoints in both images you can determine how much of image #1 overlaps with image #2 . <p> Btw , I wonder about stitching not in panorama way , but match and stack two image with different size , for example : image1 size = 500+500 , image2 size = 100+100. image2 is part of spatial area on image1 so it should have high matches keypoint . But when I try with your code , It was said : " could not broadcast input array from shape ( ) into shape ( ) . <p> I knew this error was already asked before but how about this specific problem ? <p> fyi , I try to make image2 size same as image1 with framing it with black pixel , but image2 always behind on image1 . I try to change the black frame into alpha channel too , but after I try to match it , the alpha channel frame become bright red . <p> Hi Evan this is a @ @ @ @ @ @ @ @ @ @ the size of the new , resulting image manually . Take the coordinates of the matched region , compute the overlap , and subtract from image2 . This will give you the new dimensions that you need to " pad " the output image by in order to apply the array slicing . <p> Hi Adrian , thanks for replying The compute the overlap part I still did n't  know . Is overlap calculation is a kind of image registration method ? It seems overlap calculation is dragging reference image into sensed image . <p> The area of overlap is determined by detecting keypoints , extracting local invariant descriptors , and applying the RANSAC algorithm to determine keypoint correspondences . If you are interested in learning more about this technique , I cover it in both Practical Python and OpenCV and inside the PyImageSearch Gurus course . <p> I 'm working on a dual fisheye camera stitching project , that is to stitch the two equirectangular projections of two fisheye images . So I tried to apply your solution here as the stitching method . But the result I got was @ @ @ @ @ @ @ @ @ @ too much . Its probably because the distortion made by equirectangular projection affects the homography matrix . Do you have any idea to deal with this ? Or , are there any stitching method having better performance on fisheye stitching ? <p> Great post and a great blog overall ! It was very easy to understand , even as a beginner . I tried to modify this code to stitch multiple images ( not the best way to do it , but it kinda works ) . I 'm cropping out all the black regions that are left out after the stitching is done and then go on and stitch the next image alongside . However , after stitching a few images , it starts going out of the plane , resulting in a completely stretched out image . <p> I 've gone thru all the comments and the topic of multiple image stitching keeps coming up again and again . It would really help all of us if you could do a tutorial on that . I did find some other tutorials on this topic but they 're nowhere as close to @ @ @ @ @ @ @ @ @ @ this request soon ! <p> I understand that this line is slicing the result array or in simpler terms , we are cropping the image . But what I do n't  understand is the flow of the code in this line . Are we taking a slice of that array and equating that to imageB , thereby superimposing imageB on top of the result , which completes the stitching ? Please correct me if I 'm wrong . <p> Also , can you please explain why does slicing a numpy array like this result in broadcasting errors ? <p> Instead of trying to detail every aspect of " broadcasting " and how it turns up in error messages , I think its best that you review the NumPy documentation on broadcasting . In this case , it seems that the output dimensions of the image can not hold the slice . <p> The result of the code 0:imageB.shape0 starts from y=0 to y=imageB ( height ) . The second slice , 0:imageB.shape1 starts from x=0 to x=imageB ( width ) . Then , imageB is stored in this slice of the @ @ @ @ @ @ @ @ @ @ taking input from them rather than using argparse . Rest of the steps are as you have mentioned above . I have checked and both img1 and img2 are initialised ( I used imshow ) . Can you please help me out <p> I know you mentioned validating the images via cv2.imshow , but I would double and triple check this . It really sounds like that one or both of the images you are trying to stitch are not being properly read from the webcam(s) . I discuss these types of NoneType errors in this blog post . <p> That is indeed quite strange behavior . I 'm not sure why this may happen off the top of my head . I would start inserting more print and cv2.imshow statements into the code until you can see @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485279 @185279/ <h> Tag Archives pixels per metric <p> We have now reached the final installment in our three part series on- measuring the size of objects in an image and- computing the distance between objects . Two weeks ago , we started this round- of tutorials by learning how to ( correctly ) order coordinates in a clockwise manner using Python and OpenCV . Then , last week , we discussed how to <p> Measuring the size of an object ( or objects ) in an image has been a- heavily requested tutorial on the PyImageSearch blog for some time now and it feels- great to get this post online and share it with you . Todays post is the second in a three part series on- measuring the @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485280 @185280/ <h> Tag Archives multiprocessing <p> I 'll keep the introduction to todays post short , since I think the title of this post and GIF animation above speak for themselves . Inside this post , I 'll demonstrate how to attach- multiple cameras to your Raspberry Piand access all of them using a single Python script . Regardless if- your setup includes : Multiple USB webcams . Or the Raspberry <p> Over the past two weeks on the PyImageSearch blog , we have discussed how to use threading to increase- our FPS processing rate on- both built-in/USB webcams , along with the Raspberry Pi camera module . By utilizing threading , - we learned that we can substantially reduce the affects of I/O latency , leaving the main thread to run without being blocked as <p> Today is the second post in our three part series on milking- every last bit of performance out of your webcam- or Raspberry Pi camera . Last week we discussed how to : Increase the FPS rate of our video processing pipeline . Reduce the affects of I/O latency on standard USB and built-in webcams @ @ @ @ @ @ @ @ @ @ next few weeks , I 'll be doing a series of blog posts on how to improve your frames per second ( FPS ) from your webcam using Python , OpenCV , and threading . Using threading to handle I/O-heavy tasks ( such as reading frames from a camera sensor ) is a programming model that has existed for @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485281 @185281/ <h> Tag Archives matplotlib <p> I think a better title for this blog post might be : - How I lost a day of productivity to Ubuntu , virtual- environments , matplotlib , and rendering backends . Over the weekend I was playing around with deep learning on my Ubuntu system and went to plot the accuracy scores of my classifier . I coded up a quick Python script <p> You know what 's a really good feeling ? Contributing to the open source community . PyPI , the Python Package Index repository is a wonderful thing . It makes downloading , installing , and managing Python libraries and packages a breeze . And with all that said , - I have pushed my own personal imutils package online . I use this package nearly every single <p> So we know that matplotlib is awesome for generating graphs and figures . But what if we wanted to display a simple RGB image ? Can we do that with matplotlib ? Of course ! This blog post will show you how to display a Matplotlib RGB image in only a few @ @ @ @ @ @ @ @ @ @ even halfway through 2014 yet , but there have been some really amazing Python books released this year that have not been getting much attention . Some of these books are related to computer vision , some to machine learning and statistical analysis , and others to parallel computing . While not all- of @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485282 @185282/ <h> Archive dlib <p> Continuing our series of blog posts on facial landmarks , today we are going to discuss face alignment , the process of : Identifying the geometric structure of faces in digital images . Attempting to obtain a canonical alignment of the face based on translation , scale , and rotation . There are many forms of face alignment . Some methods try to <p> My- Uncle John is a long haul tractor trailer truck driver . For each new assignment , he picks his load up from a local company early in the morning and then sets off on a lengthy , enduring cross-country trek across the United States that takes him- days to complete . John is a nice , outgoing guy , who carries a <p> A few weeks ago I did a blog post on how to install the dlib library on Ubuntu and macOS . Since Raspbian , the operating system that ( most ) Raspberry Pi users run is Debian-based ( as is Ubuntu ) , the- same install instructions can be used for Raspbian as Ubuntu however , there 's @ @ @ @ @ @ @ @ @ @ last weeks blog post , I demonstrated how to perform facial landmark detection in real-time in video streams . Today , we are going to build upon this knowledge and develop a computer vision application that is capable of- detecting and counting blinks in video streams using facial landmarks and OpenCV . To build our blink detector , well be <p> Over the past few weeks we have been discussing- facial landmarks and the role they play in computer vision and image processing . We 've started off by learning how to detect facial landmarks in an image . We then discovered how to label and annotate- each of the facial regions , such as eyes , eyebrows , nose , mouth , and jawline . Today <p> Todays blog post is part three in our current series on facial landmark detection and their applications to computer vision and image processing . Two weeks ago I demonstrated how to install the dlib library- which we are using for facial landmark detection . Then , last week I discussed how to use dlib to actually- detect facial landmarks in <p> Last week @ @ @ @ @ @ @ @ @ @ system with Python bindings . Today we are going to use dlib and OpenCV to detect- facial landmarks in an image . Facial landmarks are used to localize and represent salient regions of the face , such as : Eyes Eyebrows Nose Mouth Jawline Facial landmarks have been successfully <p> Two weeks ago I interviewed Davis King , the creator and chief maintainer of the dlib library . Today I am going to demonstrate how to install dlib with Python bindings on both- macOS and- Ubuntu . I- highly encourage you to take the time to install dlib on your system over the next couple of days @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485283 @185283/ <h> Local Binary Patterns with Python &amp; OpenCV <p> Well . Ill just come right out and say it. - Today is my 27th birthday . <p> As a kid I was always- super excited about my birthday . It was another year closer to being able to drive a car . Go to R rated movies . Or buy alcohol . <p> But now as an adult , I do n't  care too much for my birthday I suppose its just another reminder of the passage of time and how it ca n't be stopped . And to be totally honest with you , I guess I 'm a bit nervous about turning the " Big 3-0 " in a few short years . <p> In order to rekindle some of that " little kid excitement " , I want- to do something special with todays post . Since today is both a- Monday ( when new PyImageSearch blog posts are published ) and- my birthday ( two events that will not coincide again until 2020 ) , I 've decided to put together a really great tutorial on- texture and @ @ @ @ @ @ @ @ @ @ this blog post I 'll show you how to use the Local Binary Patterns image descriptor ( along with a bit of machine learning ) to automatically- classify and- identify textures and patterns in images ( such as the texture/pattern of wrapping paper , cake icing , or candles , for instance ) . <p> Read on to find out more about Local Binary Patterns and how they can be used for texture classification . <h> PyImageSearch Gurus <p> The majority of this blog post on texture and pattern recognition is based on the Local Binary Patterns lesson inside the- PyImageSearch Gurus course . <p> While the lesson in PyImageSearch Gurus goes into- a lot more detail than what this tutorial does , I still wanted to give you a taste of what PyImageSearch Gurus my- magnum opus- on computer vision has in store for you . <p> At the time of this writing , the PyImageSearch Gurus course also covers an additional- 166 lessons and- 1,291 pages including computer vision topics such as- face recognition , - deep learning , - automatic license plate recognition , and- training your own @ @ @ @ @ @ @ @ @ @ <p> Unlike Haralick texture features that compute a- global representation of texture based on the Gray Level Co-occurrence Matrix , LBPs instead compute a- local representation of texture . This local representation- is constructed by comparing each pixel with its surrounding neighborhood of pixels . <p> The first step in constructing the LBP texture descriptor is to convert the image to grayscale . For each pixel in the grayscale image , we select a neighborhood of size- r surrounding the center pixel . A LBP value is then calculated for this center pixel and stored in the output 2D array with the same width and height as the input image . <p> For example , let 's take a look at the original LBP descriptor which operates on a fixed- 3 x 3 neighborhood of pixels just like this : <p> Figure 1 : The first step in constructing a LBP is to take the 8 pixel neighborhood surrounding a center pixel and threshold- it to construct a set of 8 binary digits . <p> In the above figure we take the center pixel ( highlighted in red ) and threshold @ @ @ @ @ @ @ @ @ @ intensity of the center pixel is greater-than-or-equal to its neighbor , then we set the value to- 1 ; otherwise , we set it to- 0 . With 8 surrounding pixels , we have a total of- 2 8 = 256 possible combinations of LBP codes . <p> From there , we need to calculate the LBP value for the center pixel . We can start from any neighboring pixel and work our way clockwise or counter-clockwise , but our ordering must be kept- consistent for all pixels- in our image and all images in our dataset . Given a- 3 x 3 neighborhood , we thus have 8 neighbors that we must perform a binary test on . The results of this binary test are stored in an 8-bit array , which we then convert to decimal , like this : <p> Figure 2 : Taking the 8-bit binary neighborhood of the center pixel and converting it into a decimal representation . ( Thanks to Bikramjot of- Hanzra Tech for the inspiration on this visualization ! ) <p> In this example we start at the top-right point and work @ @ @ @ @ @ @ @ @ @ along . We can then convert this binary string to decimal , yielding a value of 23 . <p> This value is stored in the output LBP 2D array , which we can then visualize below : <p> Figure 3 : The calculated LBP value is then stored in an output array with the same width and height as the original image . <p> This process of thresholding , accumulating binary strings , and storing the output decimal value in the LBP array is then repeated for each pixel in the input image . <p> Here is an example of computing and visualizing a full LBP 2D array : <p> Figure 4 : An example of computing the LBP representation ( right ) from the original input image ( left ) . <p> Figure 5 : Finally , we can compute a histogram that tabulates the number of times each LBP pattern occurs . We can treat this histogram as our feature vector . <p> A primary benefit of this original LBP implementation is that we can capture extremely fine-grained details in the image . However , being able to @ @ @ @ @ @ @ @ @ @ biggest drawback to the algorithm we can not capture details at varying scales , only the fixed- 3 x 3- scale ! <p> To handle this , an extension to the original LBP implementation was proposed by Ojala et al . to handle variable neighborhood sizes . To account for variable neighborhood sizes , two parameters were introduced : <p> The number of points- p in a circularly symmetric neighborhood to consider ( thus removing relying on a square neighborhood ) . <p> The radius of the circle- r , which allows us to account for different scales . <p> Lastly , its important that we consider the concept of LBP- uniformity . A LBP is considered to be uniform if it has- at most two 0-1 or- 1-0 transitions . For example , the pattern 00001000- ( 2 transitions ) and 10000000- ( 1 transition ) are both considered to be- uniform patterns since they contain at most two- 0-1 and- 1-0 transitions . The pattern 01010010- ) on the other hand is- not considered a uniform pattern since it has six- 0-1 or- 1-0 transitions . <p> The @ @ @ @ @ @ @ @ @ @ completely dependent on the number of points- p . As the value of- p increases , so will the dimensionality of your resulting histogram . Please refer to the original Ojala et al . paper for the full explanation on deriving the number of patterns and uniform patterns based on this value . However , for the time being simply keep in mind that given the number of points- p in the LBP there are- p + 1- uniform patterns . The final dimensionality of the histogram is thus- p + 2 , where the added entry tabulates all patterns that are- not uniform . <p> So why are uniform LBP patterns so interesting ? Simply put : they add an extra level of- rotation and grayscale invariance , hence they are commonly used when extracting LBP feature vectors from images . <h> Local Binary Patterns with Python and OpenCV <p> Local Binary Pattern implementations can be found in both the scikit-image and mahotas packages . OpenCV also implements LBPs , but strictly in the context of face recognition the underlying LBP extractor is not exposed for raw LBP histogram @ @ @ @ @ @ @ @ @ @ scikit-image implementation of LBPs as they offer more control of the types of LBP histograms you want to generate . Furthermore , the scikit-image implementation also includes variants of LBPs that improve rotation and grayscale invariance . <p> Before we get started extracting Local Binary Patterns from images and- using them for classification , we first need to create a dataset of textures . To form this dataset , earlier today I took a walk through my apartment and collected 20 photos of various textures and patterns , including an- area rug : <p> Figure 7 : Example images of the area rug texture and pattern . <p> Notice how the area rug images have a geometric design to it . <p> I also gathered a few examples of- carpet : <p> Figure 8 : Four examples of the carpet texture . <p> Notice how the carpet has a distinct pattern with- a coarse texture . <p> I then snapped a few photos of the- keyboard sitting on my desk : <p> Figure 9 : Example images of my keyboard . <p> Notice how the keyboard has little texture but @ @ @ @ @ @ @ @ @ @ silver metal spacing in between them . <p> Finally , I gathered a few final examples of- wrapping paper ( since it is my birthday after all ) : <p> Figure 10 : Our final texture we are going to classify wrapping paper . <p> The wrapping paper has a very smooth texture to it , but also demonstrates a unique pattern . <p> Let 's go ahead and get this demonstration- started by defining the directory structure for our project : <p> Local Binary Patterns with Python &amp; OpenCV <p> Shell <p> 1 <p> 2 <p> 3 <p> ---pyimagesearch <p> **25;13846;TOOLONG <p> ---recognize.py <p> Well be creating a pyimagesearch- module to keep our code organized . And within the pyimagesearch- module well create localbinarypatterns.py- , which as the name suggests , is where our Local Binary Patterns implementation will be stored . <p> We start of by importing the feature- sub-module of scikit-image which contains the implementation of the Local Binary Patterns descriptor . <p> Line 5- defines our constructor for our LocalBinaryPatterns- class . As mentioned in the section- above , we know that LBPs require two parameters : @ @ @ @ @ @ @ @ @ @ along with the- number of points along the outer radius . Well store both of these values on- Lines 8 and 9 . <p> From there , we define our describe- method on- Line 11 , which accepts a single required argument the image we want to extract LBPs from . <p> The actual LBP computation is handled on- Line 15 using our supplied radius and number of points . The uniform- method indicates that we are computing the rotation and grayscale invariant form of LBPs . <p> However , the lbp- variable returned by the localbinarypatterns- function is not directly usable as a feature vector . Instead , lbp- is a 2D array with the same width and height as our input image each of the values inside lbp- ranges from- 0 , numPoints + 2 , a value for each of the possible- numPoints + 1- possible rotation invariant prototypes ( see the discussion of- uniform patterns- at the top of this post for more information ) along with an extra dimension for all patterns that- are not- uniform , yielding a total of- numPoints + 2 unique @ @ @ @ @ @ @ @ @ @ feature vector , we need to make a call to np.histogram- which counts the number of times each of the LBP prototypes appears . The returned histogram is- numPoints + 2-dimensional , an integer count for each of the prototypes . We then take this histogram and normalize it such that it sums to- 1 , and then return it to the calling function . <p> Now that our LocalBinaryPatterns- descriptor is defined , let 's see how we can use it to recognize textures and patterns . Create a new file named recognize.py- , and let 's get coding : <p> Local 18 <p> 19 <p> 20 55203 @qwx675203 <p> **37;13873;TOOLONG **25;13912;TOOLONG <p> fromsklearn.svm importLinearSVC <p> fromimutils importpaths 55218 @qwx675218 <p> importcv2 55202 @qwx675202 55206 @qwx675206 <p> LONG ... <p> help= " path to the training images " ) <p> LONG ... <p> help= " path to the @ @ @ @ @ @ @ @ @ @ local binary patterns descriptor along with <p> # the data and label lists <p> **30;13939;TOOLONG <p> data= <p> labels= <p> We start off on- Lines 2-6- by importing our necessary command line arguments . Notice how we are importing the LocalBinaryPatterns- descriptor from the pyimagesearch- sub-module that we defined above . <p> From there , - Lines 9-14 handle parsing our command line arguments . Well only need two switches here : the path to the --training- data and the path to the --testing- data . <p> In this example , we have partitioned our textures into two sets : a training set of 4 images per texture ( 4 textures x 4 images per texture = 16 total images ) , and a testing set of one image per texture ( 4 textures x 1 image per texture = 4 images ) . The training set of 16 images will be used to " teach " our classifier and then well evaluate performance on our testing set of 4 images . <p> In order to store the LBP feature vectors and the label names associated with each of the @ @ @ @ @ @ @ @ @ @ to store the feature vectors and labels- to store the names of each texture ( Lines 19 and 20 ) . <p> Now its time to extract LBP features from our set of training images : <p> Local Binary Patterns with <p> 35 <p> 36 <p> # loop over the training images <p> forimagePath inpaths.listimages ( args " training " ) : <p> # load the image , convert it to grayscale , and describe it <p> **27;13971;TOOLONG 55215 @qwx675215 <p> hist=desc.describe(gray) <p> # extract the label from the image path , then update the <p> # label and data lists <p> **29;14000;TOOLONG ( " / " ) -2 ) <p> data.append(hist) <p> # train a Linear SVM on the data <p> model=LinearSVC ( C=100.0 , randomstate=42 ) <p> model.fit ( data , labels ) <p> We start looping over our training images on- Line 23 . For each of these images , we load them from disk , @ @ @ @ @ @ @ @ @ @ features . The label ( i.e. , texture name ) is then extracted from the image path and both our labels- and data- lists are updated , respectively . <p> Once we have our features and labels extracted , we can train our Linear Support Vector Machine- on- Lines 35 and 36 to learn the difference between the various texture classes . <p> Once our Linear SVM is trained , we can use it to classify subsequent texture images : <p> Local Binary Patterns with <p> 50 <p> 51 <p> # loop over the testing images <p> forimagePath inpaths.listimages ( args " testing " ) : <p> # load the image , convert it to grayscale , describe it , <p> # and classify it <p> **27;14031;TOOLONG 55215 @qwx675215 <p> hist=desc.describe(gray) <p> **31;14060;TOOLONG <p> # display the image and the prediction <p> LONG ... <p> 1.0 , ( 0,0,255 ) , 3 ) <p> cv2.imshow ( " Image " , image @ @ @ @ @ @ @ @ @ @ training images on- Line 22 to gather data to train our classifier , we now loop over the- testing images on- Line 39 to test the performance and accuracy of our classifier . <p> Again , all we need to do is load our image from disk , convert it to grayscale , extract Local Binary Patterns from the grayscale image , and then pass the features onto our Linear SVM for classification ( Lines 42-45 ) . <p> Lines 48-51- show the output classification to our screen . <h> Results <p> Let 's go ahead and give our texture classification system a try by executing the following command : <p> Figure 13 : Classifying the keyboard pattern is also easy for our method . <p> Finally , we are able to recognize the texture and pattern of the wrapping paper as well : <p> Figure 14 : Using Local Binary Patterns to classify the texture of an image . <p> While this example was quite small and simple , it was still able to demonstrate that by using Local Binary Pattern features and a bit of machine learning , we @ @ @ @ @ @ @ @ @ @ an image . <h> Summary <p> In this blog post we learned how to extract Local Binary Patterns from images and use them ( along with a bit of machine learning ) to perform- texture and pattern recognition . <p> If you enjoyed this blog post , be sure to take a look at the PyImageSearch Gurus course where the majority this lesson was derived from . <h> Downloads : 55217 @qwx675217 <p> No problem , I got it to work , great job . I was especially curious how well it would do with different types of keyboards ( or carpets etc ) and it worked amazingly . <p> One thing though you should use the os.path module instead of splitting by " / " ( specifically this line : **29;14093;TOOLONG ( " / " ) -2 ) as it does n't  work like you expect on windows , that uses as path delimiters . <p> ( for completeness , I changed the above line to LONG ... ) <p> The original implementation of SVMs were only intended for binary classification ( i.e. , two classes ) ; however @ @ @ @ @ @ @ @ @ @ used in this course ) can handle multi-class data without a problem . <p> I would suggest downloading the source code under the " Downloads " section of this post . The source code download will properly explain how to create the PyImageSearch module . All you need to do is create a pyimagesearch directory and then place a init.py file inside of it . <p> Thanks Adrian , very nice tutorial as usual , one thing i found is the histogram returned from class LocalBinaryPatterns , if set bins=np.arange ( 0 , self.numPoints + 2 ) in np.histogram() , the number of bins returned will be only self.numPoints+1 rather than self.numPoints+2 , as np.arange ( 0 , self.numPoints+2 ) will generate 0 , 1 , , self.numPoints+1 , which generates bins 0,1 ) , 1,2 ) , , self.numPoints , self.numPoints+1 for np.histogram() . <p> Either just use bins=self.numPoints + 2 or use bins=np.arange ( 0 , self.numPoints+3 ) will return self.numPoints+2 bins <p> Wow , you 're the best ! Adrian , what would be the best solution to work with the counting crowd in small place . For @ @ @ @ @ @ @ @ @ @ camera on the ceiling and out of the train ) . The input data would be the overhead and / or shoulders . <p> You 'll want to train your classifier using static images/frames . But classifying with a video can easily be accomplished by modifying the code to access the video stream . I recommend using this blog post as a starting point . <p> Are you referring to pixels being on " border " of the image and therefore not having a true neighborhood ? If so , just pad image with zeros such that you have pixels to fit the neighborhood . Other types of padding into replication , where you " replicate " the borders along the border to create the neighborhood . You can also " wrap around " and use the pixel values from the opposite side of the image . But in general , zero padding is normally used . <p> Hi again , The code is working as expected ; however , the following warning is thrown : LONG ... DeprecationWarning : Passing 1d arrays as data is deprecated in 0.17 and willraise @ @ @ @ @ @ @ @ @ @ ( -1 , 1 ) if your data has a single feature or X.reshape ( 1 , -1 ) if it contains a single sample . so in the future , it will start throwing exceptions ! ? Any idea how to avoid that ? <p> Hi Adrian great set of tutorials keep continuing please just one doubt , i have applied LBP on a set of faces and have extracted a histogram but how do you get the face , as you have shown before the tutorial above . Kindly guide <p> I have one additional question . I do n't  want to classify pictures , but extract small area with texture , calculate lbp histogram and then try to match histograms and find similar textures in the entire image . Something similar to Opencv Back Projection for color histograms . And actually I am trying to play with calcBackProject() function , but I have trouble with data types and cant make it work . <p> Other solution on my mind is to calculate the lbp histogram on the template image , and then manually iterate through picture ( @ @ @ @ @ @ @ @ @ @ for every region , compare that with template histogram using compareHist() and Chi-Square , and declare similarity . But that would be pretty coarse . Any other option ? <p> This certainly sounds like a texture matching problem , which I admittedly do n't  have much experience in . Using a combination of image pyramids and sliding windows you can get an scale and location independent method to matching textures , but this by definition is very computationally expensive . I would suggest starting with this method and seeing how far it gets you . I would also suggest treating the problem as a texture connected-component labeling problem as well . <p> One more complication is the fact that I want to do this for a live video feed , on an ARM processor = I implemented naive method : getting lbp hist for template region , and then manually iterating through patches of the image , calculating lbp hist for them , comparing histograms , and then setting whole region to 0 or 255 depending on the Chi Square distance . Result is not great : 1 ) manual @ @ @ @ @ @ @ @ @ @ but there has to be some way to vectorize that operation ) ; 2 ) result is coarse ( I am using 10+10 blocks on a 240+320 image ) and kinda looks like an edge detector <p> Of well . Ill try to play with it a bit more before discarding the idea . <p> Correct , this method will be painfully slow due to having to loop over the entire image . In some cases , you might be able to speed this up by implementing the function in C/C++ and then calling the method from Python . <p> Hi , Adrian Thanks for the lovely post . I downloaded your code and ran it . i can only see the arearug classified in the output . The rest images are not showing . I also get a warning after i run the scripts which is same as someone else pointed out that is : " DeprecationWarning : Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19 . Reshape your data either using X.reshape ( -1 , 1 ) if your data has a @ @ @ @ @ @ @ @ @ @ it contains a single sample . " <p> That is very strange regarding only the " area rug " class being utilized that should not happen . I have n't heard of that happening before either . As for the DeprecationWarning that can be resolved by wrapping the LBP hist as a list before prediction : <p> ok i 'm sorry it was my bad its working fine now ! One more question , so i 'm working on this college project and i need to extract only eyes and lips using Local binary patterns . Can you give me a lead as to how i can do that and in what format are they stored after extraction ? <p> LBP features are simply NumPy arrays . You can write them to file using cPickle , HDF5 , or simple CSV . For what its worth , I demonstrate how to train custom object detectors inside the PyImageSearch Gurus course . <p> Hey Adrian , Tanks for the great post . Regarding the numpy histogram , i am not sure about your code . Shouldnt the number of bins be equal to p @ @ @ @ @ @ @ @ @ @ are p + 1 bins for uniform patterns and 1 bin for non uniform patterns ( total p + 2 bins ) . And why is range equals to 0 , p + 2 and not the number of pixels in the image ? <p> Also do you get number of uniform patterns equals p + 1 because its rotation invariant ? Otherwise it will be p * ( p 1 ) + 2 ( equals 58 for p=8 ) <p> The code can be a bit confusing due to subtleties in the range and np.histogram function . <p> To start , the range function is not inclusive on the upper bound , therefore we have to use p + 3 instead of p + 2 to get the proper number of bins . Open up a Python shell and play with the range function to confirm this for yourself . <p> The rangeparameter to np.histogram is p + 2 because there are p + 1 uniform patterns . We then need to add an extra bin to the histogram for all non-uniform patterns . <p> Hey Suresh make sure @ @ @ @ @ @ @ @ @ @ the " Downloads " section in this post . The . zip archive of the code includes the exact directory and project structure to ensure the code works out-of-the-box . My guess is that your project structure is incorrect/does not include a init.py file in the pyimagesearch directory . <p> Hi Adrian , Just a note , If you are using localbinarypattern from skimage , the value assigned to the centre pixel is the opposite of what you are describing in the blog . In skimage it is : " If the intensity of the center pixel is greater-than-or-equal to its neighbor , then we set the value to 0 ; otherwise , we set it to 1 " . You might want to keep everything uniform . <p> DeprecationWarning : Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19 . Reshape your data either using X.reshape ( -1 , 1 ) if your data has a single feature or X.reshape ( 1 , -1 ) if it contains a single sample . DeprecationWarning ) <p> Hey Adrian , first of all great post @ @ @ @ @ @ @ @ @ @ I am stuck with this error : " ValueError : Found array with 0 feature(s) ( shape= ( 1,0 ) ) while a minimum of 1 is required " . How do I get rid of this error ? <p> Hay , Iam working on my code and I use the function " lbp = **28;14124;TOOLONG , npoints , radius , method= " Uniform " ) " to display just image lbp , so how should I put the numPoints and radius settings <p> just to help you a bit i followed your tut . Ubuntu 16.04 : How to install OpenCV and then i downloaded the code and it did n't  run i installed opencv 3.1 as you guided in addition to python 3 thanks for your time <p> I followed your code and upgraded it a bit , but i have noticed that if i add another image the code will miss identify it , i searched and saw that cheeking confidence value will help to print unknown on the low confidence values images , and also found that OpenCV3 is n't supporting confidence any more , what can @ @ @ @ @ @ @ @ @ @ build a further work on its value ? <p> i used this and it did n't  work , whenever i remove conf it works so what can i do then ? ? ? Thanks again <p> Hello Adrian , I 'm working on a Rasberry Pi using Python . I want to use LBP for face recognition , I read your earlier comment that it was in your Guru book , how would I go about accessing that specific module ? <p> Also , I just tried running the first code on this post , but I get the error that the module skimage does n't  exits . I have already install scikit-image and matplots succesfully . Can you think of any other reason why that would be the case ? <p> The LBP for face recognition is part of the Face Recognition Module inside PyImageSearch Gurus . Computer vision topics tend to overlap and intertwine ( you would need to understand feature extractors and a bit of machine learning first before applying face recognition ) so I would suggest working through the entire course . <p> As for the scikit-image @ @ @ @ @ @ @ @ @ @ when installing them ? Perhaps you installed them outside of the Python virtual environment where you normally access OpenCV . <p> The dataset used in this blog post really is n't large enough to apply 10 fold cross validation . I would suggest using a larger dataset , extracting features from each image , and then use scikit-learns cross-validation methods to help you accomplish this . <p> You commented in the post that LBP implementations can be found in scikit-image and mahotas packages ( or in OpenCV more specifically in the context of facial recognition ) . Is there any other package that contains LBP implementations or just those same ones ? <p> A CNN will learn various filters to discriminate amongst object classes . These filters could learn color blobs , edges , contours , and eventually higher-level , more abstract features . CNNs and LBPs are not the same . If you 're interested in learning more about feature extraction and CNNs , take a look at the PyImageSearch Gurus course and Deep Learning for Computer Vision with Python . <p> No , a CNN will learn its own filters @ @ @ @ @ @ @ @ @ @ would then feed these features into a standard machine learning classifier like an SVM , Random Forest , etc . A CNN is an end-to-end classifier . An image comes in as input and classifications at the output . You would n't use LBPs as @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485284 @185284/ <p> OpenCV and Python versions : This example will run on- Python 2.7/Python 3.4+ and OpenCV 2.4 . X/OpenCV 3.0+ . <h> The cv2.HoughCircles Function <p> In order to detect circles in images , you 'll need to make use of the cv2.HoughCircles function . Its definitely not the easiest function to use , but with a little explanation , I think you 'll get the hang of it . <p> Take a look at the function signature below : <p> cv2.HoughCircles ( image , method , dp , minDist ) <p> image : 8-bit , single channel image . If working with a color image , convert to grayscale first . <p> method : Defines the method to detect circles in images . Currently , the only implemented method is cv2.HOUGHGRADIENT , which corresponds to the Yuen et al . paper . <p> dp : This parameter is the inverse ratio of the accumulator resolution to the image resolution ( see Yuen et al. - for more details ) . Essentially , the larger the dp gets , the smaller the accumulator array gets . <p> minDist : Minimum distance @ @ @ @ @ @ @ @ @ @ detected circles . If the minDist is too small , multiple circles in the same neighborhood as the original may be ( falsely ) detected . If the minDist is too large , then some circles may not be detected at all . <p> param2 : Accumulator threshold value for the cv2.HOUGHGRADIENT method . The smaller the threshold is , the more circles will be detected ( including false circles ) . The larger the threshold is , the more circles will potentially be returned . <p> But I will say this be ready to play around with the parameter values from image to image . The minDist parameter is especially important to get right . Without an optimal minDist value , you may end up missing out on some circles , or you may detecting many false circles . <h> Detecting Circles in Images using OpenCV and Hough Circles <p> Ready to apply the cv2.HoughCircles function to detect circles in images ? <p> Great . Let 's jump into some code : <p> Detecting Circles in Images using OpenCV <p> Python <p> 1 <p> 2 <p> 3 <p> 4 <p> @ @ @ @ @ @ @ @ @ @ @qwx675203 55220 @qwx675220 55218 @qwx675218 <p> importcv2 <p> # construct the argument parser and parse the arguments 55206 @qwx675206 <p> LONG ... to the image " ) <p> Then , on Lines 7-9 we parse our command line arguments . Well need only a single switch , --image , which is the path to the image we want to detect circles in . <p> Let 's go ahead and load the image : <p> Detecting Circles in Images using OpenCV <p> Python <p> 11 <p> 12 <p> 13 <p> 14 <p> # load the image , clone it for output , and then convert it to grayscale <p> image=cv2.imread ( args " image " ) <p> output=image.copy() 55215 @qwx675215 <p> We load our image off disk on Line 12 and create a copy of it on Line 13 so we can draw our detected circles without destroying the original image . <p> As well see , the cv2.HoughCircles function requires an 8-bit , single channel image , so well go ahead and convert from the RGB color space to grayscale on Line 14 . <p> Okay , time to detect the @ @ @ @ @ @ <p> 32 <p> 33 <p> # detect circles in the image <p> LONG ... <p> # ensure at least some circles were found <p> ifcircles isnotNone : <p> # convert the ( x , y ) coordinates and radius of the circles to integers <p> **25;14154;TOOLONG , : ) . astype ( " int " ) <p> # loop over the ( x , y ) coordinates and radius of the circles <p> for ( x , y , r ) incircles : <p> # draw the circle in the output image , then draw a rectangle <p> # corresponding to the center of the circle <p> cv2.circle ( output , ( x , y ) , r , ( 0,255,0 ) , 4 ) <p> LONG ... <p> # show the output image <p> LONG ... 55212 @qwx675212 <p> Detecting the circles is handled by the cv2.HoughCircles function on Line 17 . We pass in the @ @ @ @ @ @ @ @ @ @ , the circle detection method as the second argument ( currently , the cv2.cv.HOUGHGRADIENT method is the only circle detection method supported by OpenCV and will likely be the only method for some time ) , an accumulator value of 1.5 as the third argument , and finally a minDist of 100 pixels . <p> A check is made on Line 20 to ensure at least one circle was found in the image . <p> The cv2.HoughCircles function was able to detect only seven of the circles instead of all- eight , leaving out the one in the center . <p> Why did this happen ? <p> Its due to the minDist parameter . The center- ( x , y ) - coordinates for the large outer circle are identical to the center inner circle , thus the center inner circle is discarded . <p> Unfortunately , there is not a way around this problem unless we make minDist unreasonably small , and thus generating many " false " circle detections . <h> Summary <p> In this blog post I showed you how to use the cv2.HoughCircles function in OpenCV @ @ @ @ @ @ @ @ @ @ or rectangles in images , detecting circles is substantially harder since we can not reply on approximating the number of points in a contour . <p> To help us detect circles in images , OpenCV has supplied the cv2.HoughCircles function . <p> While the cv2.HoughCircles method may seem complicated at first , I would argue that the most important parameter to play with is the minDist , or the minimum distance between the center ( x , y ) coordinates of detected circles . <p> If you set minDist too small , you may end up with many falsely detected circles . On the other hand , if minDist is too large , then you may end up missing some circles . Setting this parameter definitely takes some fine tuning . <h> Have a Question ? <p> Do you have a question about OpenCV and Python ? Just send me a message . And I 'll do my best to answer it on this blog . <h> Downloads : 55217 @qwx675217 <p> FINALLY ! I 'm guessing my search foo was lacking , but this is the first place I 've found a @ @ @ @ @ @ @ @ @ @ MinDist impact for the past 24 hours I 've been trying random values to deduce how all this was working . <p> Is there an intuitive way to understand the meaning of param1 , param2 , and dp ? <p> Moreover , I am confused because : my images have many circle-like objects , but if I increase the range of radii by increasing maxRadius , I sometimes get fewer circles . Is this something you have seen and could explain ? <p> Hi Sofia , if you have many circle like objects and you increase the maxRadius , then you will certainly find fewer circles . The maxRadius parameter controls the maximum radius of a circle . So if a circle has a radius greater than maxRadius , then it will not be detected . I would also take a look at these slides for a more in-depth review of circle detection . <p> The " int " or " float " is not the issue . The problem is you may have mixed tabs and spaces in your indentation of the code . More information on this type of @ @ @ @ @ @ @ @ @ @ you can spare the time , brushing up on a bit of Python will definitely help you avoid these types of errors . <p> Sir i am doing my project on human computer interaction.In that i plan to do roak paper sscissor game and arithmetic operation.So i have an hand image which capture from webcam.From that captured image i have to fix center point and contour point.So that only i can detect fingers.Sir can you tell me how to fix these points and how to detect finger from the hand image ? . Thank you ! <p> There are different ways to approach a problem like this . You could try more advanced techniques of ellipse detection , those would probably help . Personally , I would just find the contours of each circle , compute the center of the circle , and from there , its simple to determine the radius . <p> You certainly can , but you 'll have to tune the parameters of the cv2.HoughCircles function , which is not always the easiest task . I would suggest starting by reading this post on accessing the Raspberry @ @ @ @ @ @ @ @ @ @ here using the raspberry pi camera . What if I wanted to take a photo using the raspberry pi camera and then use cv2.HoughCircles to determine whether or not a circle was present in the picture taken ? Is that an easier task than simply detecting circles from a raw stream ? Thanks Steve <p> A video stream is just a collection of frames . Each frame can be considered an individual image . A video stream can be slightly more complicated due to motion blur as objects move , but the same general process applies . Technically applying circle detection to a single image ( provided there is no blur ) is easier , but if you can guarantee that in a video stream , its just as easy . <p> Hello adrian , is it possible to detect an oval object as a circle too ? i mean the object is not a perfect circle . ive implemented your tutorial but it can not detect the non-perfect-circle object . or maybe which params that i need to change ? thanks a lot before <p> You can indeed detect @ @ @ @ @ @ @ @ @ @ , but its a bit more challenging ( especially for the algorithm itself ) . Take a look at the circular and elliptical Hough transforms of scikit-image for more information . <p> You are doing a fantastic job . Your blog has now become my preferred destination to search for any python + opencv feature . I was wondering if you plan to write an article on multiprocessing of images for increasing speed anytime soon . So I want to batch process some images from a given folder and save the output in another folder . I tried using Pool from multiprocessing library but am running into errors . Any pointers ? <p> Hey Pranav thanks for putting this back on my radar . I was planning on doing a series of posts on Hadoop and image processing in the future , but I should start with just the basics of multiprocessing . As for other libraries , you might want to give pp a try . <p> If you 're trying to detect your iris , then I 'm not sure circle detection is your best bet . I would try @ @ @ @ @ @ @ @ @ @ between your iris and the whites of your eyes ) first . <p> Thanks for the tutorials . Im having trouble with detecting circles using OpenCV 3 with Python3 . I can load and image and video stream from the camera however when I try to use cv2.cv.CVHOUGHGRADIENT I get an error <p> module object has no attribute cv . <p> Changing this to cv2.CVHOUGHGRADIENT I get a different error <p> module object has no attribute CVHOUGHGRADIENT . <p> I am working within the virtual environment ( if that 's the correct terminology ) . <p> Hi , I have got another question . I am trying to do iris recognition system.For the last step , I need to encode the normalized image into binary and then , match the encoded images . For encoding , I used cv2.imencode , is this right thing to do ? For matching , I could not find what to do . Thank you <p> I can detect circles when I use the sample images you provided but when I try to read other images like this image LONG ... , the hough circles return @ @ @ @ @ @ @ @ @ @ The parameters I used were 1.2 as dp and 75 as minDist . Am I doing something wrong ? <p> Thanks in advance <p> Hi again , <p> I think I found a solution . It had something to do with the image size . After resizing the image to 300+300 , I was able to detect some circles . <p> I have another image with dimension 640+480 . I tried resizing it to 300+300 but I believe the circles in the original image would become ellipses once they 're compressed . Is there any other way I can do this ? Also does the input image have to be a perfect square dimension image ? I saw that the sample images you provided all have perfect square dimensions . <p> Hough circles is not a good method for real-time video processing . The motion blur makes it very challenging to reliably detect the circles . If possible , you might want to try color based methods or using HOG + Linear SVM . <p> i will add some agenda , 1 . i am following a ball with this method @ @ @ @ @ @ @ @ @ @ the " hit " more accurate i need good detection ( parameters 1 &amp;2 ) . <p> 2. i try to run it in realtime , the loop time is 180 ms with resolution of 640X480. to reduce the loop time i am cropping the image according to last recognition , that 's make the loop time 40 ms . BUT when running the SAME picture the biggest picture find the circle just fine and with the cropped image it sometimes find bigger radius detection ( with the same parameters ) . do you have an idea why the processing is different ? ? ? thanks again <p> It sounds like you want to increase your FPS processing rate . If so , take a look at this post , as well as the posts it links to . Inside I detail how to speedup camera I/O substantially this will help with your first problem . <p> As for your second question , you 're running Hough circles on a smaller , cropped ROI ? This actually does make sense due to the accumulator gradient . See the paper referenced in the @ @ @ @ @ @ @ @ @ @ this helpful tutorial ! I am using this code you have provided to detect petri dishes for a project my lab is working on . I am having a few problems where the code is detecting a circle but it is off center from what I expect and does n't  match the contours of the petri dish . It also detects some circles that do n't  exist even though my min distance is 1000 , which I think is quite high . I am new to image analysis and opencv and generally do n't  have much clue as to what I am doing , so I would really appreciate some help ! <p> Its hard to say what the exact issue could be without seeing your images , but in general , it can be hard to determine the parameters to Hough circles . You might want to investigate simple contour methods instead using the cv2.findContours function and contour properties to identify circle-like regions . You can also try the scikit-image implementation for finding circles . <p> If you hardcoded your image path to cv2.imread and your image is None @ @ @ @ @ @ @ @ @ @ supplied to cv2.imread and ensure that its correct . Based on the error message , I can almost guarantee that the path to the image is not valid . <p> Hi , after detecting circles with houghcircle , i need to verify if all circles ( wanted to be detected ) is detected as a first solution is to add reference image which contained all circles must be detected and try to color the circle detected in the reference image i asked if its possible to do this with opencv.If , yes i need some keywords or helpful tutoriel If , no there is other solution and thanks to reply <p> Hey Adrian .. thanks for the tutorial . Im having problem with executing the code . i encountered on 20th line circles = cv2.HoughCircles ( gray , cv2.cv.CVHOUGHGRADIENT , 1.2 , 100 ) where it shows AttributeError : module object has no attribute cv i tried with the change of cv to cv2 since i 'm using opencv3 .. kindly give solution to this problem .. Thanks in advance <p> Hi , when I try to run this code I @ @ @ @ @ @ @ @ @ @ cv . I have openCV installed . Also , I tried the suggestion on stackoverflow to write import cv2.cv as cv and then use cv instead of cv2.cv but it still does n't  work . Your help would be highly appreciated . Thanks in advance . <p> I would like to detect a circle in an image and after that , would have to convert the circle into a rectangular image using a polar conversion tool for rectangular . I 'm working on C ++ , Visual Studio 2012 , opencv 2.4.13 . The circle I 'm managing to find the image but not can do the conversion , someone would have an idea of how to do this ? Thank you very much in advance . <p> Your articles have been helping me teach in Australia via LinuxCircle.com At the moment we are working on a ball-following robot , in which Raspberry Pi , webcam , OpenCV3 , Python 3 and 4WD chassis are used . We want to detect any colour and shape , as long as it is circular object . Further question : 1 . How do we @ @ @ @ @ @ @ @ @ @ 2 . Will image smoothing such as blurring method help in segregating the ball with the background ? 3 . What is the role of light in Hough method ? If the robot light a LED torch towards the ball will it increase the chance of being detected ? <p> You can put the image wherever you like it you just need to supply the path to the image via command line argument . I would suggest using the " Downloads " section of this tutorial to download the code + example images and using that as your starting point . <p> If you are getting an error related to " NoneType " right after cv2.imread is being called , then 99% of the time is because you supplied an invalid path to cv2.imread . Double check that the path to your input image is correct . <p> Hi Adrian , These are awesome tutorials . I 'm trying to detect droplets in an image . But to start with , I tried executing your code , it worked fairly well for your sample images . But , for my images @ @ @ @ @ @ @ @ @ @ to the next blank command line . I tried resizing the image size as well . Would you be able to help me with this ? <p> It sounds like your image was not loaded from disk correctly , perhaps due to your command line arguments ( i.e. , path to an invalid file ) . Please take a look at this blog post for more information on resolving " NoneType " errors . <p> I 'm doing a project on smar fuel dispenser . Idea is to develop auto adjusting fuel dispenser . So can a get a code that will detect the fuel tank opening ( circular ) and which can give the position of it ( X Y Z co ordinates ) plz help me with this . Thank you <p> HI Adrian , been enjoying you Practical Python and OpenCV book . It was a great starter . Now I 'm trying to combine it with some of your blog examples . In the soda can example above , how would you create a mask for the outside of the circle ? I could probably use some sort @ @ @ @ @ @ @ @ @ @ to be able to create a mask so I can switch the mask on and off . <p> is there any difference if i read image , apply blur , canny edge then houghcircle ? I am not getting importance of param1 for providing internal canny ? When i used internal canny i was not able to detect circle matrix . When i applied canny first then hough circle , false rejection was reduced . What does it make a sense ? <p> It really depends on your input images . If your images are fairly " clean " already you can skip the blurring step . Applying the Canny edge detector further helps clean up the image , giving you only the binary edges . If your edges are well defined , it will improve the accuracy of the circle detector . <p> Sir , I just wanted to ask , if it is possible to draw a small circle of a fixed radius on all the contours that I have detected ? I do not want to find the bounding rectangle or circles for the contours . Pleas @ @ @ @ @ @ @ @ @ @ for the tutorial . I 'm working on a project that requires the detection of a specific circle in an image that could potentially have multiple circles . I set the minimum distance between center points ( minDist ) to be equal to the length of the image so that the function would only be allowed to find a single circle despite the image having multiple circles in it . So I was wondering how the HoughCircles function decides which circle to output . Does it take the one that most closely resembles a circle , or maybe the circle with the greatest radius ? The 8 circle example is a good example of what I 'm asking . I understand why the function only returned 7 circles but why did it choose to return the outside circle instead of the inside circle ? <p> Hey Adrian , Thank you for this tutorial . I am trying to detect ellipse shaped structures from images . I know there is function for ellipse detection in skimage and I also read your tutorial where you detect different shapes in an image . But my @ @ @ @ @ @ @ @ @ @ exact ellipse ( like deformed ellipse ) . These ellipses are cells from some patient data and every patient image will have different size and shape of ellipses . Can it be still detected by hough transform ? <p> I tried using houghellipse but does not work well . I also tried k-means clustering but do not get convincing results . Do you have some suggestions or tutorial wherein I can detect arbitrary shapes ? Regards Pranita <p> If the objects you are trying to detect are " deformed ellipses " , then I would suggest extracting contours and then computing the aspect ratio , extent , solidity , etc . These can be used to filter your shapes using a series of " if " statements . An example of using solidity/extent/aspect ratio to filter @qwx675223 55200 @qwx675200 55221 @qwx675221 
@@71485285 @185285/ <h> Tag Archives non maximum suppression <p> I have issues - I cant stop thinking about object detection . You see , last night I was watching The Walking Dead- and instead of enjoying the zombie brutality , the forced- cannibalism , or the enthralling storyline , - all I wanted to do was build an object detection system to recognize zombies . Would it be very useful ? Probably not . I mean , its <p> Connecticut is cold . Very cold . Sometimes its hard to even get out of bed in the morning . And honestly , without the aide of copious amounts of pumpkin spice lattes and the beautiful sunrise over the crisp autumn leaves , I do n't  think I would leave my cozy bed . But I have work to do . And today <p> If you 've been paying attention to my Twitter account lately , youve probably noticed one or two teasers of what I 've been working on a Python framework/package to rapidly construct object detectors using Histogram of Oriented Gradients and Linear Support Vector Machines . Honestly , I really ca n't @ @ @ @ 55223 @qwx675223 55200 @qwx675200 55221 